# Comparing `tmp/etiket_client-0.2.11-py3-none-any.whl.zip` & `tmp/etiket_client-0.2.12-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,18 +1,18 @@
-Zip file size: 112730 bytes, number of entries: 146
--rw-r--r--  2.0 unx      216 b- defN 24-Apr-17 08:46 etiket_client/__init__.py
+Zip file size: 117497 bytes, number of entries: 149
+-rw-r--r--  2.0 unx      216 b- defN 24-May-17 07:49 etiket_client/__init__.py
 -rw-r--r--  2.0 unx      256 b- defN 23-Dec-13 10:03 etiket_client/exceptions.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Dec-13 10:03 etiket_client/GUI/__init__.py
 -rw-r--r--  2.0 unx     1845 b- defN 24-Mar-14 16:06 etiket_client/GUI/app.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Dec-13 10:03 etiket_client/GUI/models/__init__.py
 -rw-r--r--  2.0 unx     1362 b- defN 23-Dec-13 10:03 etiket_client/GUI/models/login_mgmt.py
 -rw-r--r--  2.0 unx     1399 b- defN 23-Dec-13 10:03 etiket_client/GUI/models/schema_mgmt.py
 -rw-r--r--  2.0 unx     2112 b- defN 24-Mar-18 07:04 etiket_client/GUI/models/scope_mgmt.py
 -rw-r--r--  2.0 unx     1314 b- defN 24-Mar-18 07:03 etiket_client/GUI/models/sync_def_scope.py
--rw-r--r--  2.0 unx     8139 b- defN 24-Apr-19 07:17 etiket_client/GUI/models/sync_mgmt.py
+-rw-r--r--  2.0 unx     8340 b- defN 24-Apr-26 10:18 etiket_client/GUI/models/sync_mgmt.py
 -rw-r--r--  2.0 unx     1195 b- defN 24-Jan-11 16:31 etiket_client/GUI/models/sync_proc_mgmt.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Dec-13 10:03 etiket_client/GUI/qml/__init__.py
 -rw-r--r--  2.0 unx     2748 b- defN 23-Dec-13 10:03 etiket_client/GUI/qml/login.qml
 -rw-r--r--  2.0 unx      921 b- defN 23-Dec-13 10:03 etiket_client/GUI/qml/main.qml
 -rw-r--r--  2.0 unx      970 b- defN 23-Dec-13 10:03 etiket_client/GUI/qml/settings_index.qml
 -rw-r--r--  2.0 unx        0 b- defN 23-Dec-13 10:03 etiket_client/GUI/qml/icons/__init__.py
 -rw-r--r--  2.0 unx      300 b- defN 23-Dec-13 10:03 etiket_client/GUI/qml/icons/delete.svg
@@ -20,129 +20,132 @@
 -rw-r--r--  2.0 unx        0 b- defN 23-Dec-13 10:03 etiket_client/GUI/qml/settings_pages/__init__.py
 -rw-r--r--  2.0 unx     7366 b- defN 23-Dec-13 10:03 etiket_client/GUI/qml/settings_pages/scope_settings.qml
 -rw-r--r--  2.0 unx    18481 b- defN 24-Mar-14 16:52 etiket_client/GUI/qml/settings_pages/sync_add_new_source.qml
 -rw-r--r--  2.0 unx     7175 b- defN 24-Mar-24 09:35 etiket_client/GUI/qml/settings_pages/sync_settings.qml
 -rw-r--r--  2.0 unx        0 b- defN 23-Dec-13 10:03 etiket_client/GUI/resources/__init__.py
 -rw-r--r--  2.0 unx     1434 b- defN 23-Dec-13 10:03 etiket_client/GUI/resources/resource_rc.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Dec-13 10:03 etiket_client/local/__init__.py
--rw-r--r--  2.0 unx      966 b- defN 24-Mar-13 10:57 etiket_client/local/database.py
+-rw-r--r--  2.0 unx     1213 b- defN 24-May-09 07:04 etiket_client/local/database.py
 -rw-r--r--  2.0 unx     1144 b- defN 23-Dec-13 10:03 etiket_client/local/exceptions.py
--rw-r--r--  2.0 unx     6129 b- defN 24-Apr-16 14:03 etiket_client/local/model.py
+-rw-r--r--  2.0 unx     6109 b- defN 24-May-16 13:13 etiket_client/local/model.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Dec-13 13:03 etiket_client/local/types.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Dec-13 10:03 etiket_client/local/alembic/__init__.py
 -rw-r--r--  2.0 unx     2239 b- defN 24-Mar-13 10:58 etiket_client/local/alembic/env.py
 -rw-r--r--  2.0 unx      850 b- defN 24-Apr-16 18:25 etiket_client/local/alembic/versions/24b0115d4cc7_adding_delete_cache_table.py
 -rw-r--r--  2.0 unx     8187 b- defN 23-Dec-13 13:01 etiket_client/local/alembic/versions/4000a8b9703a_qdrive_v_0_1_0.py
 -rw-r--r--  2.0 unx      802 b- defN 23-Dec-13 10:03 etiket_client/local/alembic/versions/6e8777d1c9f3_etiket_client_v_0_1_1.py
 -rw-r--r--  2.0 unx     3913 b- defN 24-Apr-04 06:47 etiket_client/local/alembic/versions/716ce37a1bc1_etiket_client_v_0_1_3.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Dec-13 10:03 etiket_client/local/alembic/versions/__init__.py
 -rw-r--r--  2.0 unx     4611 b- defN 24-Apr-10 12:54 etiket_client/local/alembic/versions/bec34208dd87_update_uniqueness_of_alt_uuid.py
+-rw-r--r--  2.0 unx     1272 b- defN 24-May-15 09:36 etiket_client/local/alembic/versions/c077798e06b9_adding_sync_attr_to_files.py
 -rw-r--r--  2.0 unx     1289 b- defN 24-Jan-12 10:00 etiket_client/local/alembic/versions/c5d93c68feda_etiket_client_v0_1_5.py
+-rw-r--r--  2.0 unx     1812 b- defN 24-May-16 14:39 etiket_client/local/alembic/versions/c867b432fc6a_convert_time_to_utc.py
 -rw-r--r--  2.0 unx     4790 b- defN 24-Mar-24 09:35 etiket_client/local/alembic/versions/fe8c0d015ba2_update_sync_tables.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Dec-13 10:03 etiket_client/local/dao/__init__.py
--rw-r--r--  2.0 unx     2041 b- defN 23-Dec-13 10:03 etiket_client/local/dao/base.py
--rw-r--r--  2.0 unx     9682 b- defN 24-Apr-16 19:19 etiket_client/local/dao/dataset.py
--rw-r--r--  2.0 unx     3297 b- defN 24-Apr-18 11:29 etiket_client/local/dao/file.py
+-rw-r--r--  2.0 unx     2060 b- defN 24-May-03 11:27 etiket_client/local/dao/base.py
+-rw-r--r--  2.0 unx    10377 b- defN 24-May-09 10:33 etiket_client/local/dao/dataset.py
+-rw-r--r--  2.0 unx     3790 b- defN 24-May-14 14:50 etiket_client/local/dao/file.py
 -rw-r--r--  2.0 unx     2153 b- defN 23-Dec-13 10:03 etiket_client/local/dao/schema.py
--rw-r--r--  2.0 unx     5932 b- defN 23-Dec-13 10:03 etiket_client/local/dao/scope.py
+-rw-r--r--  2.0 unx     6113 b- defN 24-May-09 07:05 etiket_client/local/dao/scope.py
 -rw-r--r--  2.0 unx     2397 b- defN 23-Dec-13 10:03 etiket_client/local/dao/user.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Dec-13 10:03 etiket_client/local/models/__init__.py
--rw-r--r--  2.0 unx     2537 b- defN 23-Dec-13 10:03 etiket_client/local/models/dataset.py
--rw-r--r--  2.0 unx     1595 b- defN 23-Dec-13 13:04 etiket_client/local/models/file.py
--rw-r--r--  2.0 unx      925 b- defN 23-Dec-13 10:03 etiket_client/local/models/schema.py
--rw-r--r--  2.0 unx      844 b- defN 23-Dec-13 10:03 etiket_client/local/models/scope.py
--rw-r--r--  2.0 unx      629 b- defN 23-Dec-13 10:03 etiket_client/local/models/user.py
--rw-r--r--  2.0 unx      577 b- defN 23-Dec-13 10:03 etiket_client/local/models/user_base.py
+-rw-r--r--  2.0 unx     3441 b- defN 24-May-16 12:29 etiket_client/local/models/dataset.py
+-rw-r--r--  2.0 unx     2904 b- defN 24-May-16 12:47 etiket_client/local/models/file.py
+-rw-r--r--  2.0 unx     1427 b- defN 24-May-16 12:43 etiket_client/local/models/schema.py
+-rw-r--r--  2.0 unx     1342 b- defN 24-May-16 12:47 etiket_client/local/models/scope.py
+-rw-r--r--  2.0 unx      900 b- defN 24-May-16 12:46 etiket_client/local/models/user.py
+-rw-r--r--  2.0 unx     1292 b- defN 24-May-16 12:47 etiket_client/local/models/user_base.py
+-rw-r--r--  2.0 unx      453 b- defN 24-May-16 14:00 etiket_client/local/models/utility.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Dec-13 10:03 etiket_client/python_api/__init__.py
--rw-r--r--  2.0 unx     1151 b- defN 24-Mar-05 12:52 etiket_client/python_api/dataset.py
+-rw-r--r--  2.0 unx     1028 b- defN 24-May-15 15:05 etiket_client/python_api/dataset.py
 -rw-r--r--  2.0 unx       51 b- defN 23-Dec-13 10:03 etiket_client/python_api/exceptions.py
 -rw-r--r--  2.0 unx      465 b- defN 23-Dec-13 10:03 etiket_client/python_api/schema.py
 -rw-r--r--  2.0 unx      739 b- defN 24-Mar-13 10:58 etiket_client/python_api/scopes.py
 -rw-r--r--  2.0 unx      227 b- defN 23-Dec-13 10:03 etiket_client/python_api/user.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Dec-13 10:03 etiket_client/python_api/dataset_model/__init__.py
--rw-r--r--  2.0 unx     3232 b- defN 23-Dec-13 10:03 etiket_client/python_api/dataset_model/dataset.py
--rw-r--r--  2.0 unx    11510 b- defN 24-Apr-05 15:40 etiket_client/python_api/dataset_model/files.py
+-rw-r--r--  2.0 unx     3280 b- defN 24-May-09 08:38 etiket_client/python_api/dataset_model/dataset.py
+-rw-r--r--  2.0 unx    11710 b- defN 24-May-09 09:03 etiket_client/python_api/dataset_model/files.py
 -rw-r--r--  2.0 unx     2060 b- defN 23-Dec-13 10:03 etiket_client/python_api/dataset_model/utility.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Dec-13 10:03 etiket_client/remote/__init__.py
 -rw-r--r--  2.0 unx     1943 b- defN 24-Apr-09 10:33 etiket_client/remote/authenticate.py
--rw-r--r--  2.0 unx     6447 b- defN 24-Apr-09 09:24 etiket_client/remote/client.py
+-rw-r--r--  2.0 unx     6517 b- defN 24-May-15 07:27 etiket_client/remote/client.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Dec-13 10:03 etiket_client/remote/endpoints/__init__.py
 -rw-r--r--  2.0 unx     1467 b- defN 24-Apr-02 16:03 etiket_client/remote/endpoints/dataset.py
--rw-r--r--  2.0 unx     1550 b- defN 24-Apr-09 09:19 etiket_client/remote/endpoints/file.py
--rw-r--r--  2.0 unx      664 b- defN 23-Dec-13 10:03 etiket_client/remote/endpoints/schema.py
+-rw-r--r--  2.0 unx     2069 b- defN 24-May-15 07:27 etiket_client/remote/endpoints/file.py
+-rw-r--r--  2.0 unx      664 b- defN 24-May-08 12:39 etiket_client/remote/endpoints/schema.py
 -rw-r--r--  2.0 unx      813 b- defN 23-Dec-13 10:03 etiket_client/remote/endpoints/scope.py
 -rw-r--r--  2.0 unx      257 b- defN 23-Dec-13 10:03 etiket_client/remote/endpoints/user.py
 -rw-r--r--  2.0 unx      206 b- defN 23-Dec-13 10:03 etiket_client/remote/endpoints/version.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Dec-13 10:03 etiket_client/remote/endpoints/models/__init__.py
 -rw-r--r--  2.0 unx     3035 b- defN 24-Apr-04 14:51 etiket_client/remote/endpoints/models/dataset.py
--rw-r--r--  2.0 unx     2108 b- defN 24-Apr-04 14:51 etiket_client/remote/endpoints/models/file.py
+-rw-r--r--  2.0 unx     2203 b- defN 24-May-15 11:48 etiket_client/remote/endpoints/models/file.py
 -rw-r--r--  2.0 unx      708 b- defN 23-Dec-13 10:03 etiket_client/remote/endpoints/models/schema.py
 -rw-r--r--  2.0 unx     1722 b- defN 24-Apr-04 14:45 etiket_client/remote/endpoints/models/schema_base.py
 -rw-r--r--  2.0 unx     1490 b- defN 24-Apr-04 14:42 etiket_client/remote/endpoints/models/scope.py
 -rw-r--r--  2.0 unx     1375 b- defN 24-Apr-16 14:48 etiket_client/remote/endpoints/models/types.py
 -rw-r--r--  2.0 unx     1399 b- defN 24-Apr-04 14:54 etiket_client/remote/endpoints/models/user.py
 -rw-r--r--  2.0 unx     1270 b- defN 24-Apr-09 10:36 etiket_client/remote/endpoints/models/user_base.py
--rw-r--r--  2.0 unx      433 b- defN 24-Apr-10 12:54 etiket_client/remote/endpoints/models/utility.py
+-rw-r--r--  2.0 unx      463 b- defN 24-May-16 12:41 etiket_client/remote/endpoints/models/utility.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Dec-13 10:03 etiket_client/remote/tools/__init__.py
--rw-r--r--  2.0 unx     2345 b- defN 23-Dec-13 10:03 etiket_client/remote/tools/file_download.py
+-rw-r--r--  2.0 unx     2560 b- defN 24-May-09 09:02 etiket_client/remote/tools/file_download.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Dec-13 10:03 etiket_client/settings/__init__.py
--rw-r--r--  2.0 unx     1117 b- defN 23-Dec-13 10:03 etiket_client/settings/folders.py
+-rw-r--r--  2.0 unx     1289 b- defN 24-May-13 15:06 etiket_client/settings/folders.py
 -rw-r--r--  2.0 unx     1639 b- defN 24-Jan-11 16:31 etiket_client/settings/logging.py
--rw-r--r--  2.0 unx     1580 b- defN 23-Dec-13 10:03 etiket_client/settings/saver.py
+-rw-r--r--  2.0 unx     1647 b- defN 24-May-09 09:41 etiket_client/settings/saver.py
 -rw-r--r--  2.0 unx      758 b- defN 23-Dec-13 10:03 etiket_client/settings/schema_settings.py
--rw-r--r--  2.0 unx      660 b- defN 24-Jan-11 16:31 etiket_client/settings/user_settings.py
+-rw-r--r--  2.0 unx      691 b- defN 24-May-09 08:51 etiket_client/settings/user_settings.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Dec-13 10:03 etiket_client/sync/__init__.py
--rw-r--r--  2.0 unx     2977 b- defN 24-Jan-12 14:47 etiket_client/sync/proc.py
--rw-r--r--  2.0 unx     6905 b- defN 24-Apr-17 08:37 etiket_client/sync/run.py
+-rw-r--r--  2.0 unx     3067 b- defN 24-May-16 14:39 etiket_client/sync/proc.py
+-rw-r--r--  2.0 unx     7890 b- defN 24-May-16 14:31 etiket_client/sync/run.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Dec-13 09:03 etiket_client/sync/backends/__init__.py
 -rw-r--r--  2.0 unx     1633 b- defN 24-Mar-13 11:27 etiket_client/sync/backends/sources.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Dec-13 09:03 etiket_client/sync/backends/core_tools/__init__.py
 -rw-r--r--  2.0 unx      171 b- defN 24-Mar-13 15:06 etiket_client/sync/backends/core_tools/core_tools_config_class.py
--rw-r--r--  2.0 unx     5612 b- defN 24-Apr-21 15:43 etiket_client/sync/backends/core_tools/core_tools_sync_class.py
+-rw-r--r--  2.0 unx     6919 b- defN 24-May-17 07:43 etiket_client/sync/backends/core_tools/core_tools_sync_class.py
 -rw-r--r--  2.0 unx        0 b- defN 24-Apr-21 15:02 etiket_client/sync/backends/core_tools/data_getters/__init__.py
--rw-r--r--  2.0 unx      442 b- defN 24-Apr-21 15:42 etiket_client/sync/backends/core_tools/data_getters/get_gates.py
+-rw-r--r--  2.0 unx      570 b- defN 24-Apr-29 09:29 etiket_client/sync/backends/core_tools/data_getters/get_gates.py
 -rw-r--r--  2.0 unx     6693 b- defN 24-Apr-21 15:43 etiket_client/sync/backends/core_tools/data_getters/get_pulses.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Dec-13 10:03 etiket_client/sync/backends/core_tools/real_time_sync/__init__.py
 -rw-r--r--  2.0 unx      485 b- defN 24-Apr-03 09:02 etiket_client/sync/backends/core_tools/real_time_sync/core_tools_m_param.py
 -rw-r--r--  2.0 unx     4730 b- defN 24-Apr-17 07:27 etiket_client/sync/backends/core_tools/real_time_sync/measurement_sync.py
 -rw-r--r--  2.0 unx     8112 b- defN 24-Apr-17 08:21 etiket_client/sync/backends/core_tools/real_time_sync/qcodes_parameters.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Dec-13 09:03 etiket_client/sync/backends/native/__init__.py
--rw-r--r--  2.0 unx     6194 b- defN 24-Apr-16 14:52 etiket_client/sync/backends/native/sync_agent.py
--rw-r--r--  2.0 unx     2945 b- defN 24-Apr-02 16:02 etiket_client/sync/backends/native/sync_scopes.py
+-rw-r--r--  2.0 unx     6474 b- defN 24-May-16 12:12 etiket_client/sync/backends/native/sync_agent.py
+-rw-r--r--  2.0 unx     2945 b- defN 24-May-17 07:53 etiket_client/sync/backends/native/sync_scopes.py
 -rw-r--r--  2.0 unx      971 b- defN 24-Apr-16 14:34 etiket_client/sync/backends/native/sync_user.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Dec-13 09:03 etiket_client/sync/backends/qcodes/__init__.py
 -rw-r--r--  2.0 unx      129 b- defN 24-Apr-02 10:03 etiket_client/sync/backends/qcodes/qcodes_config_class.py
--rw-r--r--  2.0 unx     4863 b- defN 24-Apr-09 13:31 etiket_client/sync/backends/qcodes/qcodes_sync_class.py
+-rw-r--r--  2.0 unx     5590 b- defN 24-May-17 07:49 etiket_client/sync/backends/qcodes/qcodes_sync_class.py
 -rw-r--r--  2.0 unx     4230 b- defN 24-Apr-17 08:43 etiket_client/sync/backends/qcodes/real_time_sync.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Dec-13 09:03 etiket_client/sync/backends/quantify/__init__.py
--rw-r--r--  2.0 unx     5195 b- defN 24-Mar-14 14:04 etiket_client/sync/backends/quantify/quantify_sync_class.py
+-rw-r--r--  2.0 unx     5617 b- defN 24-May-03 12:37 etiket_client/sync/backends/quantify/quantify_sync_class.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Dec-13 09:03 etiket_client/sync/base/__init__.py
--rw-r--r--  2.0 unx     1260 b- defN 24-Apr-02 09:37 etiket_client/sync/base/sync_source_abstract.py
--rw-r--r--  2.0 unx     8356 b- defN 24-Apr-09 09:07 etiket_client/sync/base/sync_utilities.py
+-rw-r--r--  2.0 unx     1268 b- defN 24-May-16 09:55 etiket_client/sync/base/sync_source_abstract.py
+-rw-r--r--  2.0 unx     8848 b- defN 24-May-14 14:47 etiket_client/sync/base/sync_utilities.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Dec-13 09:03 etiket_client/sync/base/checksums/__init__.py
 -rw-r--r--  2.0 unx      208 b- defN 24-Mar-14 09:53 etiket_client/sync/base/checksums/any.py
 -rw-r--r--  2.0 unx     1379 b- defN 24-Mar-14 09:58 etiket_client/sync/base/checksums/hdf5.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Dec-13 09:03 etiket_client/sync/database/__init__.py
 -rw-r--r--  2.0 unx     1998 b- defN 24-Mar-13 08:30 etiket_client/sync/database/dao_scope_mappings.py
 -rw-r--r--  2.0 unx     8349 b- defN 24-Mar-24 09:35 etiket_client/sync/database/dao_sync_items.py
 -rw-r--r--  2.0 unx     1690 b- defN 24-Mar-13 11:36 etiket_client/sync/database/dao_sync_sources.py
--rw-r--r--  2.0 unx     2417 b- defN 24-Mar-13 10:57 etiket_client/sync/database/models_db.py
--rw-r--r--  2.0 unx     2505 b- defN 24-Apr-09 15:17 etiket_client/sync/database/models_pydantic.py
+-rw-r--r--  2.0 unx     2486 b- defN 24-May-16 13:12 etiket_client/sync/database/models_db.py
+-rw-r--r--  2.0 unx     2416 b- defN 24-May-13 07:55 etiket_client/sync/database/models_pydantic.py
 -rw-r--r--  2.0 unx      563 b- defN 24-Mar-13 08:29 etiket_client/sync/database/start_up.py
 -rw-r--r--  2.0 unx      299 b- defN 24-Mar-08 14:37 etiket_client/sync/database/types.py
 -rw-r--r--  2.0 unx        1 b- defN 23-Dec-13 10:03 etiket_client/sync/uploader/__init__.py
--rw-r--r--  2.0 unx     2170 b- defN 24-Mar-12 16:00 etiket_client/sync/uploader/file_uploader.py
+-rw-r--r--  2.0 unx     3297 b- defN 24-May-15 12:49 etiket_client/sync/uploader/file_uploader.py
 -rw-r--r--  2.0 unx        0 b- defN 24-Apr-05 10:59 etiket_client/testing/__init__.py
 -rw-r--r--  2.0 unx       60 b- defN 24-Mar-14 15:05 etiket_client/testing/test_gui.py
 -rw-r--r--  2.0 unx     1174 b- defN 24-Mar-13 14:01 etiket_client/testing/test_live_sync.py
 -rw-r--r--  2.0 unx        0 b- defN 24-Apr-05 11:00 etiket_client/testing/live_data_generation/__init__.py
 -rw-r--r--  2.0 unx     4907 b- defN 24-Apr-16 18:39 etiket_client/testing/live_data_generation/core_tools_LD.py
 -rw-r--r--  2.0 unx    12577 b- defN 24-Apr-09 08:03 etiket_client/testing/live_data_generation/parameters.py
 -rw-r--r--  2.0 unx     4766 b- defN 24-Apr-16 18:40 etiket_client/testing/live_data_generation/qcodes_LD.py
--rw-r--r--  2.0 unx     6552 b- defN 24-Apr-09 14:54 etiket_client/testing/live_data_generation/qdrive_LD.py
--rw-r--r--  2.0 unx    16695 b- defN 24-Apr-22 11:47 etiket_client-0.2.11.dist-info/LICENCE
--rw-r--r--  2.0 unx     1065 b- defN 24-Apr-22 11:47 etiket_client-0.2.11.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Apr-22 11:47 etiket_client-0.2.11.dist-info/WHEEL
--rw-r--r--  2.0 unx       14 b- defN 24-Apr-22 11:47 etiket_client-0.2.11.dist-info/top_level.txt
--rw-r--r--  2.0 unx        1 b- defN 24-Mar-25 08:37 etiket_client-0.2.11.dist-info/zip-safe
--rw-rw-r--  2.0 unx    14486 b- defN 24-Apr-22 11:47 etiket_client-0.2.11.dist-info/RECORD
-146 files, 335631 bytes uncompressed, 88900 bytes compressed:  73.5%
+-rw-r--r--  2.0 unx     6594 b- defN 24-May-15 07:27 etiket_client/testing/live_data_generation/qdrive_LD.py
+-rw-r--r--  2.0 unx    16695 b- defN 24-May-17 11:19 etiket_client-0.2.12.dist-info/LICENCE
+-rw-r--r--  2.0 unx     1101 b- defN 24-May-17 11:19 etiket_client-0.2.12.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-May-17 11:19 etiket_client-0.2.12.dist-info/WHEEL
+-rw-r--r--  2.0 unx       14 b- defN 24-May-17 11:19 etiket_client-0.2.12.dist-info/top_level.txt
+-rw-r--r--  2.0 unx        1 b- defN 24-Mar-25 08:37 etiket_client-0.2.12.dist-info/zip-safe
+-rw-rw-r--  2.0 unx    14848 b- defN 24-May-17 11:19 etiket_client-0.2.12.dist-info/RECORD
+149 files, 352493 bytes uncompressed, 93065 bytes compressed:  73.6%
```

## zipnote {}

```diff
@@ -105,17 +105,23 @@
 
 Filename: etiket_client/local/alembic/versions/__init__.py
 Comment: 
 
 Filename: etiket_client/local/alembic/versions/bec34208dd87_update_uniqueness_of_alt_uuid.py
 Comment: 
 
+Filename: etiket_client/local/alembic/versions/c077798e06b9_adding_sync_attr_to_files.py
+Comment: 
+
 Filename: etiket_client/local/alembic/versions/c5d93c68feda_etiket_client_v0_1_5.py
 Comment: 
 
+Filename: etiket_client/local/alembic/versions/c867b432fc6a_convert_time_to_utc.py
+Comment: 
+
 Filename: etiket_client/local/alembic/versions/fe8c0d015ba2_update_sync_tables.py
 Comment: 
 
 Filename: etiket_client/local/dao/__init__.py
 Comment: 
 
 Filename: etiket_client/local/dao/base.py
@@ -153,14 +159,17 @@
 
 Filename: etiket_client/local/models/user.py
 Comment: 
 
 Filename: etiket_client/local/models/user_base.py
 Comment: 
 
+Filename: etiket_client/local/models/utility.py
+Comment: 
+
 Filename: etiket_client/python_api/__init__.py
 Comment: 
 
 Filename: etiket_client/python_api/dataset.py
 Comment: 
 
 Filename: etiket_client/python_api/exceptions.py
@@ -414,26 +423,26 @@
 
 Filename: etiket_client/testing/live_data_generation/qcodes_LD.py
 Comment: 
 
 Filename: etiket_client/testing/live_data_generation/qdrive_LD.py
 Comment: 
 
-Filename: etiket_client-0.2.11.dist-info/LICENCE
+Filename: etiket_client-0.2.12.dist-info/LICENCE
 Comment: 
 
-Filename: etiket_client-0.2.11.dist-info/METADATA
+Filename: etiket_client-0.2.12.dist-info/METADATA
 Comment: 
 
-Filename: etiket_client-0.2.11.dist-info/WHEEL
+Filename: etiket_client-0.2.12.dist-info/WHEEL
 Comment: 
 
-Filename: etiket_client-0.2.11.dist-info/top_level.txt
+Filename: etiket_client-0.2.12.dist-info/top_level.txt
 Comment: 
 
-Filename: etiket_client-0.2.11.dist-info/zip-safe
+Filename: etiket_client-0.2.12.dist-info/zip-safe
 Comment: 
 
-Filename: etiket_client-0.2.11.dist-info/RECORD
+Filename: etiket_client-0.2.12.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## etiket_client/GUI/models/sync_mgmt.py

```diff
@@ -140,20 +140,26 @@
                 dao_sync_sources.add_new_source(syncSource, session)
         return errorstring
     
     @pyqtSlot(str, str, str, str, str, str, result = str)
     def evaluateCoreToolsData(self, name, database, user, password, port, host):
         errorstring = ""
         
-        for data in self.__get_data():
-            if data.type is SyncSourceTypes.coretools:
-                if data.name == name:
-                    errorstring = f"The name '{name}' already exists.\n"
-                if data.config_data['dbname'] == database and data.config_data['host'] == host:
-                    errorstring = f"Already added the database '{database}' on the host '{host}'.\n"
+        try:
+            import core_tools
+        except ImportError:
+            errorstring = "Please install the core_tools package."
+        
+        if errorstring == "":
+            for data in self.__get_data():
+                if data.type is SyncSourceTypes.coretools:
+                    if data.name == name:
+                        errorstring = f"The name '{name}' already exists.\n"
+                    if data.config_data['dbname'] == database and data.config_data['host'] == host:
+                        errorstring = f"Already added the database '{database}' on the host '{host}'.\n"
         
         if errorstring == "":
             # check if it connects.
             cred = CoreToolsConfigData(dbname=database, user=user, password=password, port=int(port), host=host)
             try :
                 import psycopg2
                 conn = psycopg2.connect(**dataclasses.asdict(cred))
```

## etiket_client/local/database.py

```diff
@@ -1,21 +1,28 @@
 from etiket_client.settings.folders import get_sql_url
 
-from sqlalchemy import create_engine
+from sqlalchemy import create_engine, event
 from sqlalchemy.orm import sessionmaker
+from sqlalchemy.engine import Engine
 
 from etiket_client.local.model import Base
 from etiket_client.sync.database.models_db import SyncBase
 from etiket_client.sync.database.start_up import start_up
 
 from alembic.config import Config
 from alembic import command
 
 import os, etiket_client
 
+@event.listens_for(Engine, "connect")
+def set_sqlite_pragma(dbapi_connection, connection_record):
+    cursor = dbapi_connection.cursor()
+    cursor.execute("PRAGMA journal_mode=WAL")
+    cursor.close()
+
 engine = create_engine(get_sql_url(), echo=False)
 Session = sessionmaker(engine)
 
 # TODO user resources lib!!
 with engine.begin() as connection:
     etiket_client_directory = os.path.dirname(os.path.dirname(etiket_client.__file__))
     alembic_cfg = Config(os.path.join(etiket_client_directory, 'alembic.ini'))
```

## etiket_client/local/model.py

```diff
@@ -17,16 +17,16 @@
     user_id: Mapped[int] = mapped_column(ForeignKey("users.id"), primary_key=True) 
 
 class Scopes(Base):
     __tablename__ = "scopes"
 
     id: Mapped[int] = mapped_column(primary_key=True)
     uuid: Mapped[UUID] = mapped_column(unique=True)
-    created : Mapped[datetime] = mapped_column(types.DateTime(timezone=True), default=datetime.now())
-    modified: Mapped[datetime] = mapped_column(types.DateTime(timezone=True), default=datetime.now(), onupdate=func.current_timestamp())
+    created : Mapped[datetime] = mapped_column(types.DateTime(timezone=True), default=func.now())
+    modified: Mapped[datetime] = mapped_column(types.DateTime(timezone=True), default=func.now(), onupdate=func.current_timestamp())
     name : Mapped[str]
     description : Mapped[str]
     archived : Mapped[bool] = mapped_column(default=False)
     schema_id : Mapped[Optional[int]] = mapped_column(ForeignKey("schemas.id")) 
     
     schema : Mapped["Schemas"] = relationship(back_populates="scopes")
     users : Mapped[List["Users"]] = relationship(back_populates="scopes", secondary="scope_user_link")
@@ -34,30 +34,30 @@
 class Schemas(Base):
     __tablename__ = "schemas"
     
     id: Mapped[int] = mapped_column(primary_key=True)
     uuid: Mapped[UUID] =mapped_column(unique=True)
     name : Mapped[str]
     description : Mapped[str]
-    created : Mapped[datetime] = mapped_column(types.DateTime(timezone=True), default=datetime.now())
-    modified: Mapped[datetime] = mapped_column(types.DateTime(timezone=True), default=datetime.now(), onupdate=func.current_timestamp())
+    created : Mapped[datetime] = mapped_column(types.DateTime(timezone=True), default=func.now())
+    modified: Mapped[datetime] = mapped_column(types.DateTime(timezone=True), default=func.now(), onupdate=func.current_timestamp())
     schema : Mapped[dict] = mapped_column(types.JSON)
     
     scopes : Mapped[List["Scopes"]] = relationship(back_populates="schema")
 
 class Users(Base):
     __tablename__ = "users"
 
     id: Mapped[int] = mapped_column(primary_key=True)
     username: Mapped[str] = mapped_column(unique=True)
     firstname: Mapped[str]
     lastname: Mapped[str]
     email: Mapped[str] = mapped_column(unique=True)
-    created : Mapped[datetime] = mapped_column(types.DateTime(timezone=True), default=datetime.now())
-    modified: Mapped[datetime] = mapped_column(types.DateTime(timezone=True), default=datetime.now(), onupdate=func.current_timestamp())
+    created : Mapped[datetime] = mapped_column(types.DateTime(timezone=True), default=func.now())
+    modified: Mapped[datetime] = mapped_column(types.DateTime(timezone=True), default=func.now(), onupdate=func.current_timestamp())
     active: Mapped[bool] = True
     disable_on: Mapped[Optional[datetime]]
     user_type: Mapped[UserType] 
 
     scopes: Mapped[List["Scopes"]] = relationship(back_populates="users", secondary="scope_user_link")
 
 class DsAttrLink(Base):
@@ -71,16 +71,16 @@
     __table_args__ = (UniqueConstraint('uuid', name = 'datasets_uuid_unique'),
                       UniqueConstraint('alt_uid', 'scope_id', name = 'datasets_alt_uid_scope_id_unique'),)
     
     id: Mapped[int] = mapped_column(primary_key=True)
     uuid : Mapped[UUID] = mapped_column(index=True)
     alt_uid : Mapped[Optional[str]] = mapped_column(index = True)
     collected: Mapped[datetime] = mapped_column(TIMESTAMP)
-    created : Mapped[datetime] = mapped_column(types.DateTime(timezone=True), default=datetime.now())
-    modified : Mapped[datetime] = mapped_column(types.DateTime(timezone=True), default=datetime.now(), onupdate=func.current_timestamp())
+    created : Mapped[datetime] = mapped_column(types.DateTime(timezone=True), default=func.now())
+    modified : Mapped[datetime] = mapped_column(types.DateTime(timezone=True), default=func.now(), onupdate=func.current_timestamp())
     name: Mapped[str]
     scope_id : Mapped[int] = mapped_column(ForeignKey("scopes.id"), nullable=False, index=True)
     creator : Mapped[str]
     description : Mapped[Optional[str]]
     notes : Mapped[Optional[str]]
     keywords : Mapped[List[str]] = mapped_column(types.JSON, nullable=False)
     search_helper : Mapped[str]
@@ -116,27 +116,27 @@
     creator : Mapped[str]
     type : Mapped[FileType]
     
     scope_id : Mapped[int] = mapped_column(ForeignKey("scopes.id"))
     dataset_id : Mapped[int] = mapped_column(ForeignKey("datasets.id"))
     
     collected: Mapped[datetime]
-    created : Mapped[datetime] = mapped_column(types.DateTime(timezone=True), default=datetime.now())
-    modified : Mapped[datetime] = mapped_column(types.DateTime(timezone=True), default=datetime.now(), onupdate=func.current_timestamp())
+    created : Mapped[datetime] = mapped_column(types.DateTime(timezone=True), default=func.now())
+    modified : Mapped[datetime] = mapped_column(types.DateTime(timezone=True), default=func.now(), onupdate=func.current_timestamp())
     
     etag : Mapped[Optional[str]] = None
     size :  Mapped[int]
     status : Mapped[FileStatusLocal]
 
     local_path : Mapped[Optional[str]]
     S3_link : Mapped[Optional[str]]
     S3_validity : Mapped[Optional[datetime]]
     last_accessed : Mapped[Optional[datetime]]
     ntimes_accessed : Mapped[int] = 0
-    synchronized : Mapped[bool] = False
+    synchronized : Mapped[bool] = mapped_column(index=True)
     
 class FileDeleteQueue(Base):
     __tablename__ = "file_delete_queue"
     
     id: Mapped[int] = mapped_column(primary_key=True)
     local_path : Mapped[str]
     delete_after : Mapped[datetime] = mapped_column(types.DateTime(timezone=True))
```

## etiket_client/local/dao/base.py

```diff
@@ -1,9 +1,8 @@
-from sqlalchemy import select, update, delete
-
+from sqlalchemy import select, delete
 class dao_base:
     @staticmethod
     def _create(create_schema, model, session):
         model_db = model(**create_schema.model_dump(by_alias=True))
         session.add(model_db)
         session.commit()
         return model_db
@@ -16,15 +15,16 @@
         result = session.execute(stmt).scalars().all()
         return [read_schema.model_validate(res) for res in result]
 
     @staticmethod
     def _update(model, update_schema, session, exclude = None):
         update_schema_dict = update_schema.model_dump(exclude_unset=True, by_alias=True, exclude=exclude)
         for k, v in update_schema_dict.items():
-            if v: setattr(model, k, v)
+            if v is not None:
+                setattr(model, k, v)
         session.commit()
         return model
     
     @staticmethod
     def _delete(model, condition, session):
         stmt = delete(model).where(condition)
         session.execute(stmt)
```

## etiket_client/local/dao/dataset.py

```diff
@@ -1,21 +1,23 @@
+from etiket_client.remote.endpoints.models.types import FileStatusLocal, FileType
 from etiket_client.settings.user_settings import user_settings
 from etiket_client.local.exceptions import DatasetAlreadyExistException,\
     DatasetAltUIDAlreadyExistException, DatasetNotFoundException
 
-from etiket_client.local.model import Scopes, Datasets, DatasetAttr, DsAttrLink
+from etiket_client.local.model import Files, Scopes, Datasets, DatasetAttr, DsAttrLink
 from etiket_client.local.models.dataset import  DatasetCreate, DatasetRead,\
     DatasetUpdate, DatasetSearch, DatasetSelection
     
 from etiket_client.local.dao.base import dao_base
-from etiket_client.local.dao.scope import _get_scope_raw, dao_scope
+from etiket_client.local.dao.scope import _get_scope_raw, dao_scope, _get_user_scope_ids
 
 from sqlalchemy.orm import Session, selectinload
-from sqlalchemy import select, func, delete, Date
+from sqlalchemy import select, func, delete, Date, and_, or_
 from sqlalchemy.sql.selectable import Select
+
 from typing import List, Dict, Optional
 from uuid import UUID
 
 class dao_dataset(dao_base):
     @staticmethod
     def create(datasetCreate : DatasetCreate, session : Session):
         if not dao_dataset._unique(Datasets, Datasets.uuid == datasetCreate.uuid, session):
@@ -26,18 +28,21 @@
         
         dataset_db = Datasets(**datasetCreate.model_dump( by_alias=True, exclude=["scope_uuid", "attributes"]),
                       search_helper=_gen_search_helper(datasetCreate))
         
         scope = _get_scope_raw(datasetCreate.scope_uuid, session)
         dataset_db.scope = scope
         
+        attr = []
         for k,v in datasetCreate.attributes.items():
-            attr = _get_or_create_attr(k,v, scope, session)
-            dataset_db.attributes.append(attr)
-
+            attr += [_get_or_create_attr(k,v, scope, session)]
+        
+        for a in attr:
+            dataset_db.attributes.append(a)
+        
         session.add(dataset_db)
         session.commit()
         return DatasetRead.model_validate(dataset_db)
     
     @staticmethod
     def read(ds_uuid : 'UUID | str', session : Session) -> DatasetRead:
         dataset_db = _get_ds_by_uuid(ds_uuid, session)
@@ -58,16 +63,16 @@
                     if attribute.value == datasetUpdate.attributes[attribute.key]:
                         datasetUpdate.attributes.pop(attribute.key)
                         continue
                 attr_to_delete.append(attribute.id)
                 dataset_db.attributes.remove(attribute)
             session.commit()
 
-            dao_dataset.__assign_attributs(dataset_db, datasetUpdate.attributes, session)
-            dao_dataset.__remove_unlinked_attributes(attr_to_delete, session)       
+            dao_dataset.__assign_attributes(dataset_db, datasetUpdate.attributes, session)
+            dao_dataset.__remove_unlinked_attributes(attr_to_delete, session)     
         
     @staticmethod
     def delete(ds_uuid : UUID, session : Session):
         ds = _get_ds_by_uuid(ds_uuid, session)
         attr_id = [attr.id for attr in ds.attributes]
         session.delete(ds)
         session.commit()
@@ -110,35 +115,46 @@
         for res in result:
             out[res[0]].append(res[1])
         
         return out
 
     @staticmethod
     def get_unsynced_datasets(session : Session):
-        # TODO add scope check.
-        stmt = select(Datasets.uuid).where(Datasets.synchronized == False)
+        scopes_ids = _get_user_scope_ids(user_settings.user_name , session=session)
+
+        stmt = select(Files.dataset_id).where(and_(Files.synchronized == False, Files.status == FileStatusLocal.complete, Files.type != FileType.HDF5_CACHE))
+        stmt = stmt.group_by(Files.dataset_id)
+        ds_ids = session.execute(stmt).scalars().all()
+        
+        stmt = select(Datasets.uuid).where(or_(Datasets.synchronized is False, Datasets.id.in_(ds_ids)))
+        stmt = stmt.where(Datasets.scope_id.in_(scopes_ids))
         stmt = stmt.order_by(Datasets.collected)
         return session.execute(stmt).scalars().all()
-    
+        
     @staticmethod
     def get_number_of_datasets(session : Session):
         stmt = select(func.count(Datasets.id))
         return session.execute(stmt).scalar_one()
     
     @staticmethod
     def get_scope_uuid_from_ds_uuid(dataset_uuid : UUID, session: Session):
         ds = _get_ds_by_uuid(dataset_uuid, session)
         return ds.scope.uuid
         
     @staticmethod
-    def __assign_attributs(model, attributes, session):
+    def __assign_attributes(model, attributes, session):
+        new_attr = []
         for k,v in attributes.items():
-            attr = _get_or_create_attr(k,v, model.scope, session)
+            new_attr.append(_get_or_create_attr(k,v, model.scope, session))
+        
+        for attr in new_attr:
             model.attributes.append(attr)
-            
+        
+        session.commit()
+        
     @staticmethod
     def __remove_unlinked_attributes(attr_id_list : List[int], session : Session):
         to_delete =[]
         for i in attr_id_list:
             stmt = select(func.count("*")).where(DsAttrLink.dataset_attr_id == i)
             if session.execute(stmt).scalar_one() == 0:
                 to_delete.append(i)
@@ -192,15 +208,15 @@
 
 def _gen_search_helper(model : Datasets):
     search_helper = f"{model.name} {model.description} "
     for kw in model.keywords:
         search_helper += f" {kw}"
     return search_helper
 
-def _get_ds_by_uuid(uuid : 'UUID | str', session : Session) -> Datasets:
+def  _get_ds_by_uuid(uuid : 'UUID | str', session : Session) -> Datasets:
     condition = Datasets.alt_uid==str(uuid)
     try:
         if not isinstance(uuid, UUID):
             uuid = UUID(uuid)
         condition = condition | (Datasets.uuid == uuid)
     except:
         pass
@@ -215,12 +231,12 @@
 
 def _get_or_create_attr(key, value, scope : Scopes, session:Session):
     stmt = select(DatasetAttr).where(DatasetAttr.scope_id == scope.id)
     stmt = stmt.where(DatasetAttr.key == key)
     stmt = stmt.where(DatasetAttr.value == value)
     
     result = session.execute(stmt).scalar_one_or_none()
-    
+
     if result is None:
         return DatasetAttr(key=key, value=value, scope_id = scope.id)
     
     return result
```

## etiket_client/local/dao/file.py

```diff
@@ -5,29 +5,29 @@
 
 from sqlalchemy import select
 from sqlalchemy.orm import Session
 
 from etiket_client.local.dao.base import dao_base
 from etiket_client.local.dao.dataset import _get_ds_by_uuid
 
-from typing import Optional
+from typing import Optional, List
 from uuid import UUID
 
 import datetime, os, logging
 
 logger = logging.getLogger(__name__)
 
-
 class dao_file(dao_base):
     @staticmethod
     def create(fileCreate : FileCreate, session : Session):
         ds = _get_ds_by_uuid(fileCreate.ds_uuid, session)
         file = Files(**fileCreate.model_dump(by_alias=True, exclude=["ds_uuid"]),
                       scope_id=ds.scope.id, dataset_id=ds.id)
-        session.add(file)
+        ds.files.append(file)
+        ds.modified = datetime.datetime.now(tz=datetime.timezone.utc)
         session.commit()
         return file
         
     @staticmethod
     def read(fileSelect : FileSelect, session : Session):
         files = _get_File_raw(fileSelect.uuid, fileSelect.version_id, session)
         return [FileRead.model_validate(file) for file in files]
@@ -44,16 +44,23 @@
         files = _get_File_raw(fileSelect.uuid, fileSelect.version_id, session)
         for file in files:
             if file.local_path is not None:
                 # put the file in a delete queue, such that no program currently using the file is interrupted.
                 dao_file_delete_queue.add_file(file.local_path, session)
             session.delete(file)
         session.commit()
-
-
+    
+    @staticmethod
+    def get_file_by_name(datasetUUID : UUID, file_name : str, session : Session) -> 'List[Files]':
+        ds = _get_ds_by_uuid(datasetUUID, session)
+        files = []
+        for file in ds.files:
+            if file.name == file_name:
+                files.append(file)
+        return files
 class dao_file_delete_queue(dao_base):
     @staticmethod
     def add_file(file_path : str, session : Session) -> None:
         delete_after = (datetime.datetime.now(tz=datetime.timezone.utc) +
                                     datetime.timedelta(days=5))
         fileToDelete = FileDeleteQueue(local_path=file_path, delete_after=delete_after)
         logger.info("Adding file %s to delete queue.", file_path)
@@ -62,21 +69,25 @@
     
     @staticmethod
     def clean_files(session : Session):
         stmt = select(FileDeleteQueue).where(FileDeleteQueue.delete_after < datetime.datetime.now(tz=datetime.timezone.utc))
         files = session.execute(stmt).scalars().all()
         for file in files:
             logger.info("Deleting file %s", file.local_path)
-            os.remove(file.local_path)
+            try:
+                os.remove(file.local_path)
+            except FileNotFoundError:
+                pass
             session.delete(file)
         session.commit()
         
-def _get_File_raw(file_uuid : UUID, version : Optional[int], session : Session):
+def _get_File_raw(file_uuid : UUID, version : Optional[int], session : Session) -> 'List[Files]':
     try:
         stmt = select(Files).where(Files.uuid == file_uuid)
         if version:
             stmt = stmt.where(Files.version_id == version)
         files = session.execute(stmt).scalars().all()
         if not files : raise Exception
         return files
     except:
-        raise FileNotAvailableException
+        raise FileNotAvailableException
+
```

## etiket_client/local/dao/scope.py

```diff
@@ -120,15 +120,19 @@
             return scope_uuids
         else:
             return scope_uuids_user
 
 def _get_user_scope_UUIDs(username : str, user_type:UserType, session:Session):
     scopes = dao_scope.read_all(username=username, session=session)
     return [scope.uuid for scope in scopes]
-       
+
+def _get_user_scope_ids(username : str, session:Session):
+    stmt = select(Scopes.id).join(Users.scopes).where(Users.username==username)
+    return session.execute(stmt).scalars().all()
+
 def _get_scope_raw(scope_uuid : UUID, session:Session, lazy=True) -> Scopes:
     try:
         stmt = select(Scopes).where(Scopes.uuid == scope_uuid)
         if lazy == False:
             stmt.options(selectinload(Scopes.users))
         return session.execute(stmt).scalars().one()
     except:
```

## etiket_client/local/models/dataset.py

```diff
@@ -1,12 +1,13 @@
 from etiket_client.local.models.file import FileRead
 from etiket_client.local.models.scope import ScopeRead
 from etiket_client.remote.endpoints.models.types import datasetstr
 
-from pydantic import BaseModel, Field, model_validator, ConfigDict, field_validator
+from etiket_client.local.models.utility import convert_time_from_local_to_utc, convert_time_from_utc_to_local
+from pydantic import BaseModel, Field, model_validator, ConfigDict, field_validator, field_serializer
 from typing import Optional, List, Dict
 
 import datetime, uuid
 
 class DatasetBase(BaseModel):
     uuid : uuid.UUID
     alt_uid : Optional[str] = Field(default=None)
@@ -26,14 +27,18 @@
     
     @model_validator(mode='after')
     def same_identifier_check(self):
         if self.alt_uid != str(self.uuid):
             return self
         raise ValueError("The uuid and alternative uid need to be different.")
 
+    @field_serializer('collected')
+    def collected_serialzer(self, collected: datetime, _info):
+        return convert_time_from_local_to_utc(collected)
+    
 class DatasetRead(DatasetBase):
     model_config = ConfigDict(from_attributes=True)
     
     created : datetime.datetime
     modified : datetime.datetime
 
     scope : ScopeRead
@@ -43,14 +48,30 @@
     @field_validator('attributes', mode='before')
     @classmethod
     def convert_orm_type_to_dict(cls, attributes):
         out = {}
         for attr in attributes:
             out[attr.key]= attr.value
         return out
+    
+    @field_validator('created', mode='before')
+    @classmethod
+    def convert_created_time_utc_to_local(cls, created : datetime.datetime):
+        return convert_time_from_utc_to_local(created)
+    
+    @field_validator('modified', mode='before')
+    @classmethod
+    def convert_modified_time_utc_to_local(cls, modified : datetime.datetime):
+        return convert_time_from_utc_to_local(modified)
+
+    @field_validator('collected', mode='before')
+    @classmethod
+    def convert_collected_time_utc_to_local(cls, collected : datetime.datetime):
+        return convert_time_from_utc_to_local(collected)
+    
 
 class DatasetUpdate(BaseModel):
     alt_uid : Optional[str] = Field(default=None)
     name:  Optional[datasetstr] = Field(default=None)
     description : Optional[str] = Field(default=None)
     notes : Optional[str] = Field(default=None)
     keywords :  Optional[List[str]] = Field(default=None)
```

## etiket_client/local/models/file.py

```diff
@@ -1,14 +1,16 @@
 from etiket_client.remote.endpoints.models.types import filestr, FileType, FileStatusLocal
 
 from typing import Optional, List
-from pydantic import BaseModel, Field, ConfigDict
+from pydantic import BaseModel, Field, ConfigDict, field_serializer, field_validator
 
 import datetime, uuid
 
+from etiket_client.local.models.utility import convert_time_from_local_to_utc, convert_time_from_utc_to_local
+
 class FileBase(BaseModel):
     name : filestr
     filename : str
     uuid : uuid.UUID
     creator : str
     collected : datetime.datetime
     size : int
@@ -21,40 +23,68 @@
     
     local_path : str
     ntimes_accessed : int = 0
     synchronized : bool = Field(default=False)
 
 class FileCreate(FileBase):
     ds_uuid : uuid.UUID
+    
+    @field_serializer('collected')
+    def collected_serialzer(self, collected: datetime, _info):
+        return convert_time_from_local_to_utc(collected)
 
 class FileRead(FileBase):
     model_config = ConfigDict(from_attributes=True)
     
     created: datetime.datetime
     modified: datetime.datetime
     
     last_accessed : Optional[datetime.datetime]
     ntimes_accessed : int
     synchronized : bool
     
+    @field_validator('collected', mode='before')
+    @classmethod
+    def convert_collected_time_utc_to_local(cls, collected : datetime.datetime):
+        return convert_time_from_utc_to_local(collected)
+    
+    @field_validator('created', mode='before')
+    @classmethod
+    def convert_created_time_utc_to_local(cls, created : datetime.datetime):
+        return convert_time_from_utc_to_local(created)
+    
+    @field_validator('modified', mode='before')
+    @classmethod
+    def convert_modified_time_utc_to_local(cls, modified : datetime.datetime):
+        return convert_time_from_utc_to_local(modified)
+    
 class FileUpdate(BaseModel):
     file_generator : Optional[str] = None
     size : Optional[int] = None
     type : Optional[FileType] = None
     etag: Optional[str] = None
     status : Optional[FileStatusLocal] = None
     
     local_path : Optional[str] = None
     s3_link : Optional[str] = None
     s3_validity : Optional[datetime.datetime] = None
     last_accessed : Optional[datetime.datetime] = None
     ntimes_accessed : Optional[int] = None
     synchronized : bool = False
+    
+    @field_serializer('last_accessed')
+    def last_accessed_serialzer(self, last_accessed: Optional[datetime.datetime], _info):
+        return convert_time_from_local_to_utc(last_accessed)
+    
+    @field_serializer('s3_validity')
+    def s3_validity_serialzer(self, s3_validity: Optional[datetime.datetime], _info):
+        return convert_time_from_local_to_utc(s3_validity)
 
 class FileSelect(BaseModel):
     uuid : uuid.UUID
     version_id : Optional[int] = Field(default=None) 
 
 class FileSignedUploadLinks(BaseModel):
     upload_id : str
     part_size : int
-    presigned_urls : List[str]
+    presigned_urls : List[str]
+
```

## etiket_client/local/models/schema.py

```diff
@@ -1,13 +1,15 @@
 from etiket_client.remote.endpoints.models.schema_base import SchemaData, SchemaAttributes
-from pydantic import BaseModel, Field, ConfigDict
+from pydantic import BaseModel, Field, ConfigDict, field_validator
 from typing import Optional
 
 import datetime, uuid
 
+from etiket_client.local.models.utility import convert_time_from_utc_to_local
+
 class SchemaBase(BaseModel):
     model_config = ConfigDict(populate_by_name=True)
 
     uuid : uuid.UUID
     name : str
     description: str = Field(default='')
     schema_ : SchemaData = Field(alias='schema')
@@ -16,14 +18,24 @@
     pass
 
 class SchemaRead(SchemaBase):
     model_config = ConfigDict(from_attributes=True, populate_by_name=True)
 
     created: datetime.datetime
     modified: datetime.datetime
+    
+    @field_validator('created', mode='before')
+    @classmethod
+    def convert_created_time_utc_to_local(cls, created : datetime.datetime):
+        return convert_time_from_utc_to_local(created)
+    
+    @field_validator('modified', mode='before')
+    @classmethod
+    def convert_modified_time_utc_to_local(cls, modified : datetime.datetime):
+        return convert_time_from_utc_to_local(modified)
 
 class SchemaUpdate(BaseModel):
     model_config = ConfigDict(populate_by_name=True)
 
     name : Optional[str] = Field(default=None)
     description : Optional[str] = Field(default=None)
     schema_ : Optional[SchemaData] = Field(alias='schema', default=None)
```

## etiket_client/local/models/scope.py

```diff
@@ -1,16 +1,18 @@
 from etiket_client.local.models.schema import SchemaRead
 from etiket_client.local.models.user_base import UserRead
 from etiket_client.remote.endpoints.models.types import scopestr
 
 from typing import Optional, List
-from pydantic import BaseModel, Field, ConfigDict
+from pydantic import BaseModel, Field, ConfigDict, field_validator
 
 import datetime, uuid
 
+from etiket_client.local.models.utility import convert_time_from_utc_to_local
+
 
 class ScopeBase(BaseModel):
     name : scopestr
     uuid : uuid.UUID
     description: str
     archived: bool
 
@@ -26,9 +28,19 @@
     
     created: datetime.datetime
     modified: datetime.datetime
     archived: bool
     
     schema_: Optional["SchemaRead"] = Field(alias="schema")
 
+    @field_validator('created', mode='before')
+    @classmethod
+    def convert_created_time_utc_to_local(cls, created : datetime.datetime):
+        return convert_time_from_utc_to_local(created)
+    
+    @field_validator('modified', mode='before')
+    @classmethod
+    def convert_modified_time_utc_to_local(cls, modified : datetime.datetime):
+        return convert_time_from_utc_to_local(modified)
+
 class ScopeReadWithUsers(ScopeRead):
     users : List["UserRead"]
```

## etiket_client/local/models/user.py

```diff
@@ -1,17 +1,25 @@
+import datetime
+
 from etiket_client.remote.endpoints.models.types import UserType
 
 from etiket_client.local.models.user_base import UserBase, UserRead
 from etiket_client.local.models.scope import ScopeRead
 
 from typing import Optional, List
-from pydantic import BaseModel, EmailStr, Field
+from pydantic import BaseModel, EmailStr, Field, field_serializer
+
+from etiket_client.local.models.utility import convert_time_from_local_to_utc
 
 class UserCreate(UserBase):
     pass
+
+    @field_serializer('disable_on')
+    def collected_serialzer(self, collected: datetime, _info):
+        return convert_time_from_local_to_utc(collected)
         
 class UserReadWithScopes(UserRead):    
     scopes : List[ScopeRead]
 
 class UserUpdate(BaseModel):
     firstname: Optional[str] = Field(default=None)
     lastname: Optional[str] = Field(default=None)
```

## etiket_client/local/models/user_base.py

```diff
@@ -1,22 +1,39 @@
 from etiket_client.remote.endpoints.models.types import usernamestr, UserType
 
-from pydantic import BaseModel, EmailStr, Field, ConfigDict
+from pydantic import BaseModel, EmailStr, Field, ConfigDict, field_validator
 from typing import Optional
 
 import datetime
 
+from etiket_client.local.models.utility import convert_time_from_utc_to_local
+
 class UserBase(BaseModel):
     username: usernamestr
     firstname: str
     lastname: str
     email: EmailStr
     user_type: UserType = Field(default=UserType.standard_user)
 
     disable_on: Optional[datetime.datetime] = Field(default=None)
 
 class UserRead(UserBase):
     model_config = ConfigDict(from_attributes=True)
     
     created: datetime.datetime
     modified: datetime.datetime
+    
+    @field_validator('created', mode='before')
+    @classmethod
+    def convert_created_time_utc_to_local(cls, created : datetime.datetime):
+        return convert_time_from_utc_to_local(created)
+    
+    @field_validator('modified', mode='before')
+    @classmethod
+    def convert_modified_time_utc_to_local(cls, modified : datetime.datetime):
+        return convert_time_from_utc_to_local(modified)
+    
+    @field_validator('disable_on', mode='before')
+    @classmethod
+    def convert_disable_on_time_utc_to_local(cls, disable_on : datetime.datetime):
+        return convert_time_from_utc_to_local(disable_on)
```

## etiket_client/python_api/dataset.py

```diff
@@ -1,34 +1,34 @@
-from etiket_client.local.dao.dataset import dao_dataset, DatasetCreate, DatasetUpdate
+import uuid
+
+from etiket_client.local.dao.dataset import dao_dataset, DatasetCreate
 from etiket_client.local.database import Session
 
 from etiket_client.remote.endpoints.dataset import dataset_read as dataset_read_remote
 
-from etiket_client.python_api.dataset_model.dataset import dataset_model
 from etiket_client.local.exceptions import DatasetNotFoundException
 
-import uuid
 
-def dataset_create(datasetCreate:DatasetCreate):
+def dataset_create_raw(datasetCreate : DatasetCreate):
     with Session() as session:
         ds = dao_dataset.create(datasetCreate, session)
-    return dataset_model(ds, None)
+    return ds
 
-def dataset_read(dataset_uuid:'uuid.UUID | str'):
+def dataset_read_raw(dataset_uuid:'uuid.UUID | str'):
     local_ds = None
     with Session() as session:
         try :
             local_ds = dao_dataset.read(dataset_uuid, session)
-        except DatasetNotFoundException as e:
+        except DatasetNotFoundException:
             pass
         except Exception as e:
             raise e
     
     try:
         remote_ds = dataset_read_remote(dataset_uuid)
-    except Exception as e:
+    except Exception:
         remote_ds = None
     
     if local_ds is None and remote_ds is None:
         raise DatasetNotFoundException(f"Dataset with uuid {dataset_uuid} not found")
     
-    return dataset_model(local_ds, remote_ds)
+    return local_ds, remote_ds
```

## etiket_client/python_api/dataset_model/dataset.py

```diff
@@ -21,19 +21,19 @@
     keywords = ds_property_mgr(modifiable = True)
     ranking = ds_property_mgr(modifiable = True)
     
     scope = ds_property_mgr(modifiable = False)
     attributes = ds_property_mgr(modifiable = True)
     
     def __init__(self, local_dataset : Optional[DatasetReadLocal],
-                 remote_dataset : Optional[DatasetReadRemote]):
+                 remote_dataset : Optional[DatasetReadRemote], verbose = True):
         self._l_ds = local_dataset
         self._r_ds = remote_dataset
         
-        self.files = file_manager(self)
+        self.files = file_manager(self, verbose=verbose)
     
     def __repr__(self) -> str:
         repr_string = f"Contents of dataset :: {self.name}"
         repr_string += "\n" + "="*len(repr_string) + "\n\n"
         repr_string += f"uuid :: {self.uuid}\n"
         if self.alt_uid:
             repr_string += f"Alternative identifier :: {self.alt_uid}\n"
@@ -62,19 +62,19 @@
     def _create_local(self):
         if not self._l_ds:
             datasetCreate = DatasetCreate(**self._r_ds.model_dump(exclude=['files', 'scope']),
                                           scope_uuid=self._r_ds.scope.uuid)
             with Session() as session:
                 self._l_ds = dao_dataset.create(datasetCreate, session)
 
-    def _update_local(self, datasetUpdate:DatasetUpdate):
+    def _update_local(self, datasetUpdate : DatasetUpdate):
         if not self._l_ds:
             self._create_local()
         
         if self._r_ds:
             if self._l_ds.modified < self._r_ds.modified :
-                du = DatasetUpdate.model_validate(self._r_ds)
+                du = DatasetUpdate.model_validate(self._r_ds.model_dump())
                 with Session() as session:
                     dao_dataset.update(self.uuid, du, session)
         
         with Session() as session:
             dao_dataset.update(self.uuid, datasetUpdate, session)
```

## etiket_client/python_api/dataset_model/files.py

```diff
@@ -19,30 +19,31 @@
 
 @dataclass
 class FileData:
     local: Optional[FileReadLoc] = None
     remote: Optional[FileReadRem] = None
 
 class file_manager(dict):
-    def __init__(self, dataset):
+    def __init__(self, dataset, verbose):
         self.dataset = dataset
+        self.verbose = verbose
         
         file_contents = {}
         files = []
         if self.dataset._l_ds : files += self.dataset._l_ds.files
         if self.dataset._r_ds : files += self.dataset._r_ds.files
         
         # TODO guarantee uniqueness of the file name?
         for file in files:
             if file.name not in file_contents.keys():
                 file_contents[file.name] = []
             file_contents[file.name].append(file)   
         
         for key, data_items in file_contents.items():
-            self[key] = file_object(key, dataset, data_items)
+            self[key] = file_object(key, dataset, data_items, verbose=self.verbose)
     
     def add_new_file(self, name, file_path, filetype, file_status = FileStatusLocal.complete, file_generator="unknown"):
         if name in self.keys():
             raise ValueError(f"File with the name {name} already exists. Please update the file instead.")
         
         file_uuid = uuid.uuid4()
         version_id = generate_version_id()
@@ -52,15 +53,15 @@
             fc = FileCreate(name = name, filename=os.path.basename(expected_path), uuid=file_uuid, 
                         creator=user_settings.user_name, collected=datetime.datetime.now(),
                         file_generator=file_generator, status=file_status,
                         type=filetype, size=os.path.getsize(expected_path),
                         ds_uuid=self.dataset.uuid, version_id=version_id, local_path=expected_path)
             dao_file.create(fc, session)
             new_file_record = dao_file.read(FileSelect(uuid=fc.uuid), session)
-            self[name] = file_object(name, self.dataset, new_file_record)
+            self[name] = file_object(name, self.dataset, new_file_record, verbose=self.verbose)
         return self[name]
 
     def __repr__(self):
         if len(self) == 0:
             return "No files present in the current dataset."
         else:            
             headers = ["name", "type", "selected version number (version_id)", "Maximal version number"]
@@ -69,42 +70,43 @@
                 tabular_data.append([name, str(content.current.type),
                         f"{content.versions.index(content.current.version_id)} ({content.current.version_id})" , len(content) -1])
             
             message = tabulate.tabulate(tabular_data, headers)
             return message
 
 class file_object:
-    def __init__(self, name, dataset, data_items : List['FileReadLoc | FileReadRem']):
+    def __init__(self, name, dataset, data_items : List['FileReadLoc | FileReadRem'], verbose):
         self.name = name
         self.dataset = dataset
+        self.verbose = verbose
         self.__build(data_items)
 
     def __build(self, data_items : List['FileReadLoc | FileReadRem']):
         file_version_data = {}
         for file_data in data_items:
             if file_data.version_id not in file_version_data.keys():
                 file_version_data[file_data.version_id] = FileData()
             if isinstance(file_data, FileReadLoc):
                 file_version_data[file_data.version_id].local = file_data
             else:
                 file_version_data[file_data.version_id].remote = file_data
         
         self.__file_versions = {}
         for key, fileData in file_version_data.items():
-            self.__file_versions[key] = file_version(self.dataset, fileData)
+            self.__file_versions[key] = file_version(self.dataset, fileData, self.verbose)
             
         self.__current_version_id = sorted(self.__file_versions.keys())[-1]
     
     def __rebuild(self):
         files = []
         with Session() as session:
             files += dao_file.read(FileSelect(uuid=self.current.uuid), session)
         try : 
             files += file_read(FileSelectRem(uuid=self.current.uuid))
-        except:
+        except Exception:
             pass
         
         self.__build(files)
     
     def __len__(self):
         return len(self.__file_versions)
     
@@ -159,23 +161,23 @@
             self.__current_version_id = self.__file_versions[version_id].version_id
         else:
             raise ValueError(f"The requested version {version_id} does not exist. (The following versions are available {str(self.__file_versions.keys())}).")
     
     def set_prev_version(self):
         current = self.versions.index(self.current.version_id)
         if current < 0:
-            raise ValueError(f"The current version is the first version of the file.")
+            raise ValueError("The current version is the first version of the file.")
         self.set_version_number(current -1)
     
     def set_next_version(self):
         current = self.versions.index(self.current.version_id)
         self.set_version_number(current +1)
     
     def __repr__(self) -> str:
-        output = f"File object information\n=======================\n"
+        output = "File object information\n=======================\n"
         output += f"Name : {self.name}\nSelected File version : {self.current.version_id}\n"
         output += f"File versions ({len(self)}) : \n"
         for version in self.versions:
             if version == self.current.version_id:
                 output += f"\t* {version} "
             else:
                 output += f"\t  {version} "
@@ -191,16 +193,17 @@
     collected = ds_file_mgr()
     filename = ds_file_mgr()
 
     size = ds_file_mgr()
     type = ds_file_mgr()
     status = ds_file_mgr()
     
-    def __init__(self, dataset, fileData:FileData):
+    def __init__(self, dataset, fileData:FileData, verbose):
         self.dataset = dataset
+        self.verbose = verbose
         self.local_version = fileData.local
         self.remote_version = fileData.remote
     
     @property
     def local(self) -> bool:
         if self.local_version:
             return True
@@ -220,15 +223,15 @@
                 dao_file.update(fs, fu, session)
         else:
             raise ValueError("This action can only be performed on a local file.")
         
     @property
     def path(self) -> str:
         if not self.local_version:
-            local_path = file_download(self.dataset.scope.uuid, self.dataset.uuid, self.remote_version)
+            local_path = file_download(self.dataset.scope.uuid, self.dataset.uuid, self.remote_version, self.verbose)
 
             self.dataset._create_local()
             fc = FileCreate(status=FileStatusLocal.complete, local_path=local_path,
                             synchronized=True, ds_uuid=self.dataset.uuid, 
                             **self.remote_version.model_dump(exclude=["status"]))
             
             with Session() as session:
```

## etiket_client/remote/client.py

```diff
@@ -9,14 +9,16 @@
 # QDL_URL = "https://0.0.0.0:8000"
 
 PREFIX = "/api/v2"
 logger = logging.getLogger(__name__)
 
 
 class client:
+    session = requests.Session()
+
     @staticmethod
     def url():
         return f"{QDL_URL}{PREFIX}"
     
     @staticmethod
     def __validate():
         user_settings.load() # reload to check if other process already refreshed the credentials
@@ -37,15 +39,15 @@
     @staticmethod
     def _login(username : str, password : str):
         logger.info(f"Attemp log in for the user {username}")
         
         data = {"grant_type": "password",
                 "username": username,
                 "password": password}
-        response = requests.post(f"{QDL_URL}{PREFIX}/token", data=data)
+        response = client.session.post(f"{QDL_URL}{PREFIX}/token", data=data)
 
         if response.status_code != 200:
             logger.error(f"Log in failed for {username}\n Server message : {response.json()['detail']} ")
             message = f"Log in failed, please try again. \n\tdetails : {response.json()['detail']}\n"
             raise LoginFailedException(message)
         
         logger.info(f"Log in succesful for {username}\n")
@@ -81,15 +83,15 @@
             lock =  filelock.FileLock(get_user_data_dir() + 'token_refresh.lock', 2)
             with lock.acquire(0):
                 if refresh_token is None:
                     refresh_token = user_settings.refresh_token
                 
                 data = {"grant_type" : "refresh_token",
                         "refresh_token": refresh_token}
-                response = requests.post(f"{QDL_URL}{PREFIX}/token", data=data)
+                response = client.session.post(f"{QDL_URL}{PREFIX}/token", data=data)
 
                 if response.status_code != 200:
                     logger.info(f"Token refresh failed. Details : {response.json()['detail']}\n")
                     message = f"Token refresh failed, you will need to login again. \n\tdetails : {response.json()['detail']}\n"
                     raise TokenRefreshException(message)
                 
                 logger.info(f"Refreshing token for {user_settings.user_name} successfull!")
@@ -104,42 +106,42 @@
         except Exception as e:
             logger.error(f"Token refresh failed for {user_settings.user_name} :/ ({str(e)})\n")
             raise e         
     
     @staticmethod 
     def post(url, data = None, json_data=None, params=None, headers = None):
         headers = client.__gen_auth_header(headers)
-        response = requests.post(f'{client.url()}{url}',params=params, data=data, json=json_data, headers=headers)
+        response = client.session.post(f'{client.url()}{url}',params=params, data=data, json=json_data, headers=headers)
         if response.status_code >=400 and response.status_code <500:
             raise RequestFailedException(response.json()["detail"])
         if response.status_code >=500:
             raise RequestFailedException(f"Failed with error code :: {response.status_code}")
         return response.json(),
 
     @staticmethod
     def get(url, params = None, data = None, json_data=None, headers = None):
         headers = client.__gen_auth_header(headers)
-        response = requests.get(f'{client.url()}{url}', params=params, data=data, json=json_data, headers=headers)
+        response = client.session.get(f'{client.url()}{url}', params=params, data=data, json=json_data, headers=headers)
         
         if response.status_code >=400:
             raise RequestFailedException(response.json()["detail"])
         return response.json()
     
     @staticmethod
     def put(url, data = None, params=None, headers = None):
         headers = client.__gen_auth_header(headers)
-        response = requests.put(f'{client.url()}{url}', data=data, params=params, headers=headers)
+        response = client.session.put(f'{client.url()}{url}', data=data, params=params, headers=headers)
         if response.status_code >=400:
             raise ValueError
         return response.json()
     
     @staticmethod
     def patch(url, data = None, json_data=None, params=None, headers = None):
         headers = client.__gen_auth_header(headers)
-        response = requests.patch(f'{client.url()}{url}', data=data, json=json_data, headers=headers, params=params)
+        response = client.session.patch(f'{client.url()}{url}', data=data, json=json_data, headers=headers, params=params)
         if response.status_code >=400:
             raise ValueError
         return response.json()
     
     @staticmethod
     def __get_auth_header():
         client.__validate()
```

## etiket_client/remote/endpoints/file.py

```diff
@@ -1,33 +1,40 @@
 from etiket_client.remote.client import client
 from etiket_client.remote.endpoints.models.file import FileCreate, FileValidate,\
-    FileRead, FileSelect, FileSignedUploadLinks
+    FileRead, FileSelect, FileSignedUploadLinks, FileSignedUploadLink
 
 import typing, uuid
 
 def file_create(fileCreate : FileCreate):
     client.post("/file/", json_data=fileCreate.model_dump(mode="json"))
 
-def file_generate_presigned_upload_link(file_uuid : uuid.UUID, version_id : int) -> FileSignedUploadLinks:
+def file_generate_presigned_upload_link_multi(file_uuid : uuid.UUID, version_id : int) -> FileSignedUploadLinks:
     params = {"file_uuid" : str(file_uuid), "version_id" : str(version_id)}
     data = client.get("/file/presigned_link/", params=params)
     return FileSignedUploadLinks.model_validate(data)
 
+def file_generate_presigned_upload_link_single(file_uuid : uuid.UUID, version_id : int) -> FileSignedUploadLink:
+    params = {"file_uuid" : str(file_uuid), "version_id" : str(version_id)}
+    data = client.get("/file/presigned_link/single_part/", params=params)
+    return FileSignedUploadLink.model_validate(data)
+
 def file_read(fileSelect : FileSelect) -> typing.List[FileRead]:
     data = client.get("/file/", json_data=fileSelect.model_dump(mode="json"))
     return [FileRead.model_validate(d) for d in data]
 
 def file_read_by_name(dataset_uuid : uuid.UUID, name : str) -> typing.List[FileRead]:
     params = {"dataset_uuid" : str(dataset_uuid), "name" : name}
     data = client.get("/file/by_name/", params=params)
     return [FileRead.model_validate(d) for d in data]
 
-
 def mark_immutable(file_uuid : uuid.UUID, version_id : int):
     client.post("/file/mark_immutable/",
                 json_data={"file_uuid" : str(file_uuid), "version_id" : version_id})
 
-def file_validate_upload(fileValidate : FileValidate):
+def file_validate_upload_multi(fileValidate : FileValidate):
     client.post("/file/validate_upload/", json_data=fileValidate.model_dump(mode="json"))
     
+def file_validate_upload_single(fileValidate : FileValidate):
+    client.post("/file/validate_upload/single_part/", json_data=fileValidate.model_dump(mode="json"))
+    
 def file_abort_upload(fileValidate : FileValidate):
     client.post("/file/abort_upload/", json_data=fileValidate.model_dump(mode="json"))
```

## etiket_client/remote/endpoints/models/file.py

```diff
@@ -51,23 +51,28 @@
     @field_validator('modified', mode='before')
     @classmethod
     def convert_modified_time_utc_to_local(cls, modified : datetime.datetime):
         return convert_time_from_utc_to_local(modified)
 
 class FileSelect(BaseModel):
     uuid : uuid.UUID
-    version_id : Optional[int] = Field(default=None) 
+    version_id : Optional[int] = Field(default=None)
 
 class FileSignedUploadLinks(BaseModel):
     uuid : uuid.UUID
     version_id : int
     upload_id : str
     part_size : int
     presigned_urls : List[str]
 
+class FileSignedUploadLink(BaseModel):
+    uuid : uuid.UUID
+    version_id : int
+    url : str
+
 class FileValidate(BaseModel):
     uuid : uuid.UUID
     version_id : int
     
     md5_checksum : str
     upload_id : str
     etags: List[str]
```

## etiket_client/remote/endpoints/models/utility.py

```diff
@@ -1,13 +1,14 @@
 
 from datetime import datetime, timezone
+from typing import Optional
 from dateutil.parser import isoparse
 
-def convert_time_from_utc_to_local(data_time : 'datetime | None'):
+def convert_time_from_utc_to_local(data_time : Optional[datetime]):
     if data_time is None:
         return None
     return isoparse(data_time).astimezone().replace(tzinfo=None)
 
-def convert_time_from_local_to_utc(data_time : 'datetime | None'):
+def convert_time_from_local_to_utc(data_time : Optional[datetime]):
     if data_time is None:
         return None
     return data_time.astimezone(tz = timezone.utc).isoformat()
```

## etiket_client/remote/tools/file_download.py

```diff
@@ -12,15 +12,15 @@
 
 class DownloadProgressBar(tqdm):
     def update_to(self, b=1, bsize=1, tsize=None):
         if tsize is not None:
             self.total = tsize
         self.update(b * bsize - self.n)
 
-def file_download(scope_uuid, dataset_uuid, fileRead : FileRead):
+def file_download(scope_uuid, dataset_uuid, fileRead : FileRead, verbose):
     if fileRead.S3_validity is None:
         raise ValueError("File is not available on the server. The file does not seem to be uploaded yet.")
     
     if fileRead.S3_validity > datetime.now().timestamp() + 3600: # make sure the link is at least valid for 1h
         fileRead = file_read(FileSelect(uuid = fileRead.uuid, version_id=fileRead.version_id))[0]
     
     print(f"Downloading {fileRead.name} -- version_id :: {fileRead.version_id})")
@@ -41,16 +41,21 @@
             
     logger.info('Starting file download.')
     
     with requests.get(fileRead.S3_link, stream=True) as r:
         r.raise_for_status()
         tot_size = int(r.headers.get('content-length', 0))
 
-        with DownloadProgressBar(total=tot_size, unit='iB', unit_scale=True) as t:
+        if verbose is True:
+            with DownloadProgressBar(total=tot_size, unit='iB', unit_scale=True) as t:
+                with open(file_path, 'wb') as f:
+                    for chunk in r.iter_content(chunk_size=65536): 
+                        t.update(len(chunk))
+                        f.write(chunk)
+        else:
             with open(file_path, 'wb') as f:
                 for chunk in r.iter_content(chunk_size=65536): 
-                    t.update(len(chunk))
                     f.write(chunk)
     
     logger.info('File download complete.')
     
     return file_path
```

## etiket_client/settings/folders.py

```diff
@@ -1,35 +1,41 @@
+import platform
 import platformdirs as pd
 import os, sys, hashlib
 
 # TODO :: general question, should data and sql by linked to a folder on the system rather than a user folder?
 
+def __get_base_dir():
+    if platform.system() == 'Darwin':
+        return os.path.expanduser("~/Library/Containers/com.qdrive.dataQruiser/Data/qdrive")
+    return f"{pd.user_data_dir()}/qdrive"
+
 def get_sql_url():
-    path  = f"{pd.user_data_dir()}/qdrive/sql/" 
+    path  = f"{__get_base_dir()}/sql/"
     if not os.path.exists(path):
         os.makedirs(path)
     return f"sqlite+pysqlite:///{path}etiket_db.sql"
 
 def get_data_dir():
-    path  = f"{pd.user_data_dir()}/qdrive/data/" 
+    path  = f"{__get_base_dir()}/data/"
     if not os.path.exists(path):
         os.makedirs(path)
     return path
 
 def create_file_dir(scope_uuid, dataset_uuid, file_uuid, version_id):
     fpath = f'{get_data_dir()}{scope_uuid}/{dataset_uuid}/{file_uuid}/{version_id}/'            
     if not os.path.exists(fpath):
         os.makedirs(fpath)
     return fpath
 
 def get_user_data_dir():
     python_env = hashlib.md5(sys.prefix.encode('utf-8')).hexdigest()
-    path  = f"{pd.user_data_dir()}/qdrive/user_data/{python_env}/" 
+    path  = f"{__get_base_dir()}/user_data/{python_env}/"
     if not os.path.exists(path):
         os.makedirs(path)
     return path
 
 def get_log_dir():
-    path  = f"{pd.user_data_dir()}/qdrive/logs/" 
+    path  = f"{__get_base_dir()}/logs/"
     if not os.path.exists(path):
         os.makedirs(path)
     return path
```

## etiket_client/settings/saver.py

```diff
@@ -9,14 +9,16 @@
                 yaml.dump(
                     clean_data_for_saving(dataclasses.asdict(settings), dataclasses.fields(cls)) ,
                     file)
             return settings
         else:
             with open(cls._config_file, 'r') as file:
                 config = yaml.safe_load(file)
+                if config is None:
+                    config = {}
 
             fields = dataclasses.fields(cls)
             kwargs = {k.name : None for k in fields}
             for field in fields:
                 if field.name in config.keys():
                     if issubclass(field.type, enum.Enum):
                         setattr(cls, field.name , field.type(config[field.name]))
```

## etiket_client/settings/user_settings.py

```diff
@@ -10,14 +10,16 @@
     access_token : str = None
     access_token_expiration : int = None
     refresh_token : str = None
     
     system_is_measurement_PC : bool = None
     measurement_PC : str = None
     API_token  : str = None
+    
+    verbose : bool = True
 	
     current_scope : str = None
     default_attributes : dict = None
 
     sync_PID : int = None
 
     _config_file : ClassVar[str] =  f"{get_user_data_dir()}settings.yaml"
```

## etiket_client/sync/proc.py

```diff
@@ -1,20 +1,19 @@
 import logging, psutil, sys, subprocess, platform
-import re
 
 from etiket_client.settings.user_settings import user_settings
 from typing import List
 
 logger = logging.getLogger(__name__)
 
 DETACHED_PROCESS = 0x00000008
 CREATE_NEW_PROCESS_GROUP = 0x00000200
 
 def start_sync_agent():
-    logger.info(f'Trying to start a new sync agent.')
+    logger.info('Trying to start a new sync agent.')
     name ='etiket_sync'
     module_name =  'etiket_client.sync.run'
     cmd = [sys.executable, '-m', module_name, '--detached' ]
     
     running, procs = _is_running(name , module_name, use_settings=False)
     if not running:
         if platform.system() == 'Windows':
@@ -44,34 +43,35 @@
 def kill_sync_agent():
     name ='etiket_sync'
     module_name =  'etiket_client.sync.run'
     running, procs = _is_running(name, module_name, use_settings=False)
     if running:
         for proc in procs: proc.kill()
                     
-def _is_running(name, module_name, use_settings=True) -> [bool, List[psutil.Process]]:
-    logger.info(f'Checking if sync agent is running, use_settings = {use_settings}.')
+def _is_running(name, module_name, use_settings=True) -> 'List[bool, List[psutil.Process]]':
+    logger.info('Checking if sync agent is running, use_settings = %s.', use_settings)
+
     if use_settings:
         user_settings.load()
         if user_settings.sync_PID:
             try:
                 proc = psutil.Process(user_settings.sync_PID)
                 if proc.name().startswith('python') or proc.name().startswith('Python'):
-                    if module_name in proc.cmdline():
-                        logger.info(f'Sync agent is running (proc name :: {name}, with module name : {module_name} and PID {proc.pid}).')
+                    if module_name in proc.cmdline() or 'Python qdrive sync' in proc.cmdline():
+                        logger.info('Sync agent is running (proc name :: %s, with module name : %s and PID %s).', name, module_name, proc.pid)
                         return True, [proc]
             except (psutil.AccessDenied, psutil.ZombieProcess, psutil.NoSuchProcess):
                 pass
     else:
         procs = []
         for proc in psutil.process_iter(['name', 'cmdline', 'pid']):
             if proc.name().startswith('python') or proc.name().startswith('Python'):
                 try:
-                    if module_name in proc.cmdline():
-                        logger.info(f'Sync agent is running (proc name :: {name}, with module name : {module_name} and PID {proc.pid}).')
+                    if module_name in proc.cmdline() or 'Python qdrive sync' in proc.cmdline():
+                        logger.info('Sync agent is running (proc name :: %s, with module name : %s and PID %s).', name, module_name, proc.pid)
                         procs.append(proc)
                 except (psutil.AccessDenied, psutil.ZombieProcess, psutil.NoSuchProcess):
                     continue
         if procs: return True, procs
 
-    logger.info(f'No sync agent is active.')
+    logger.info('No sync agent is active.')
     return False, []
```

## etiket_client/sync/run.py

```diff
@@ -6,19 +6,21 @@
 from etiket_client.remote.endpoints.models.types import FileType
 from etiket_client.sync.backends.native.sync_agent import run_native_sync
 from etiket_client.sync.backends.sources import SyncSource
 from etiket_client.sync.database.dao_sync_items import dao_sync_items
 from etiket_client.sync.database.dao_sync_sources import dao_sync_sources,\
     sync_source_update
 
+from etiket_client.sync.database.models_pydantic import sync_item
 from etiket_client.sync.database.types import SyncSourceStatus, SyncSourceTypes
 
 from etiket_client.local.database import Session 
 
 import logging, time
+from setproctitle import setproctitle
 
 logger = logging.getLogger(__name__)
 
 
 def sync_loop():
     running = True
     while running == True:
@@ -46,58 +48,66 @@
             continue
         else:
             try:
                 logger.info("Syncing %s, of type %s", sync_source.name, sync_source.sync_class.SyncAgentName)
                 get_new_sync_items(sync_source, session)
                 
                 sync_item = dao_sync_items.read_sync_item(sync_source.id, offset = 0, session=session)
+                
+                if sync_item is not None:
+                    liveDS = sync_source.sync_class.checkLiveDataset(sync_source.sync_config, sync_item)
+                    # this is needed to not get stuck in a loop of live datasets (though not the cleanest way)
+                    liveDS_already_tried = check_live_DS_already_tried(sync_item, session)
+                    if (liveDS and sync_source.sync_class.LiveSyncImplemented is False) or liveDS_already_tried:
+                        # assume only one live dataset at a time.
+                        sync_item = dao_sync_items.read_sync_item(sync_source.id, offset = 1, session=session)
+                        liveDS = False
+                
                 if sync_item is None:
+                    ssu = sync_source_update(status=SyncSourceStatus.synchronized)
+                    dao_sync_sources.update_status(sync_source.id, ssu, session)
                     logger.info("No new items to sync from %s.", sync_source.name)
                     continue
-                
-                liveDS = sync_source.sync_class.checkLiveDataset(sync_source.sync_config, sync_item)
-                if liveDS and sync_source.sync_class.LiveSyncImplemented is False:
-                    # assume only one live dataset at a time.
-                    sync_item = dao_sync_items.read_sync_item(sync_source.id, offset = 1, session=session)
-                    liveDS = False
-                    
-                # assumption there is only one live dataset at a time.
-                if liveDS is False:
+
+
+                liveSync_successful = False
+                if liveDS is True:
+                    try:
+                        logger.info("Syncing live dataset, %s from %s.", sync_item.dataIdentifier, sync_source.name)
+                        sync_source.sync_class.syncDatasetLive(sync_source.sync_config, sync_item)
+                        liveSync_successful = True
+                        logger.info("Synced live dataset, %s from %s.", sync_item.dataIdentifier, sync_source.name)
+                        # do not mark as successfull -- upload the final version of the files using the normal sync.
+                    except Exception:
+                        logger.exception("Failed to synchronize %s from %s.", sync_item.dataIdentifier, sync_source.name)
+                        dao_sync_items.mark_failed_attempt(sync_source.id, sync_item.dataIdentifier,session)
+                                            
+                if liveDS is False or liveSync_successful is True:
                     try:
                         logger.info("Syncing %s from %s.", sync_item.dataIdentifier, sync_source.name)
                         sync_source.sync_class.syncDatasetNormal(sync_source.sync_config, sync_item)
-                        n_syncs += 1
-                        logger.info("Synced %s from %s.", sync_item.dataIdentifier, sync_source.name)
                         
-                        # remove any dataset caches:
-                        try:
+                        try: # remove any dataset caches:
                             dataset_local = dao_dataset.read(sync_item.datasetUUID, session)
-                        except DatasetNotFoundException:
-                            dataset_local = None
-                            
-                        if dataset_local:
                             for file in dataset_local.files:
                                 if file.type == FileType.HDF5_CACHE:
                                     fs = FileSelect(uuid=file.uuid, version_id=file.version_id)
                                     dao_file.delete(fs, session)
+                        except DatasetNotFoundException:
+                            pass
+                        except Exception as e:
+                            raise e                            
                         
-                        dao_sync_items.mark_successful_sync(sync_source.id, sync_item.dataIdentifier ,session)
-                    except Exception:
-                        logger.exception("Failed to synchronize %s from %s.", sync_item.dataIdentifier, sync_source.name)
-                        dao_sync_items.mark_failed_attempt(sync_source.id, sync_item.dataIdentifier,session)                    
-                else:
-                    try:
-                        logger.info("Syncing live dataset, %s from %s.", sync_item.dataIdentifier, sync_source.name)
-                        sync_source.sync_class.syncDatasetLive(sync_source.sync_config, sync_item)
                         n_syncs += 1
-                        logger.info("Synced live dataset, %s from %s.", sync_item.dataIdentifier, sync_source.name)
-                        # do not mark as successfull -- upload the final version of the files using the normal sync.
+                        logger.info("Synced %s from %s.", sync_item.dataIdentifier, sync_source.name)
+
+                        dao_sync_items.mark_successful_sync(sync_source.id, sync_item.dataIdentifier ,session)
                     except Exception:
                         logger.exception("Failed to synchronize %s from %s.", sync_item.dataIdentifier, sync_source.name)
-                        dao_sync_items.mark_failed_attempt(sync_source.id, sync_item.dataIdentifier,session)
+                        dao_sync_items.mark_failed_attempt(sync_source.id, sync_item.dataIdentifier,session)   
                 
                 # update statistics and state of the sync source.
                 n_items = dao_sync_items.count_items(sync_source.id, session)
                 n_items_failed = dao_sync_items.count_items_failed(sync_source.id, session)
                 n_items_synced = dao_sync_items.count_items_synchronized(sync_source.id, session)
                 n_items_skipped = dao_sync_items.count_items_not_in_scope(sync_source.id, session)
                 
@@ -117,14 +127,26 @@
         time.sleep(1)
 
 def get_new_sync_items(sync_source : SyncSource, session):
     last_identifier = dao_sync_items.get_last_identifier(sync_source.id, session)
     new_items = sync_source.sync_class.getNewDatasets(sync_source.sync_config, last_identifier)
     logger.info("Found %s new items to sync.", len(new_items))
     dao_sync_items.write_sync_items(sync_source.id, new_items, session)
-    
+
+def check_live_DS_already_tried(sync_item : sync_item, session):
+    try:
+        dataset = dao_dataset.read(sync_item.datasetUUID, session)
+        for file in dataset.files:
+            if file.type == FileType.HDF5_CACHE:
+                return True
+    except DatasetNotFoundException:
+        return False
+    return False
+
 if __name__ == '__main__':
     import etiket_client
     from etiket_client.settings.logging import set_up_sync_logger
 
+    setproctitle('Python qdrive sync')
+    
     logger = set_up_sync_logger(etiket_client.__name__)
     sync_loop()
```

## etiket_client/sync/backends/core_tools/core_tools_sync_class.py

```diff
@@ -1,24 +1,27 @@
-import typing, time
+from os import sync
+import typing, time, logging
 
 from etiket_client.sync.backends.core_tools.core_tools_config_class import CoreToolsConfigData
 from etiket_client.sync.backends.core_tools.data_getters.get_gates import get_gates_formatted
 from etiket_client.sync.backends.core_tools.data_getters.get_pulses import get_AWG_pulses
 from etiket_client.sync.backends.core_tools.real_time_sync.measurement_sync import live_measurement_synchronizer
 from etiket_client.sync.base.sync_source_abstract import SyncSourceAbstract
 from etiket_client.sync.base.sync_utilities import file_info, sync_utilities,\
     dataset_info, sync_item, new_sync_item, FileType
 
+logger = logging.getLogger(__name__)
+
 try:
     import psycopg2
-    from core_tools.data.ds.data_set import load_by_id
+    from core_tools.data.ds.data_set import load_by_id, data_set
     from core_tools.data.ds.ds2xarray import ds2xarray
     from core_tools.data.SQL.connect import SQL_conn_info_local
 except ImportError:
-    print("Core-tools not installed, will not be able to use core tools sync")
+    logger.warning("Core-tools not installed, will not be able to use core tools sync")
 
 
 class CoreToolsSync(SyncSourceAbstract):
     SyncAgentName = "core-tools"
     ConfigDataClass = CoreToolsConfigData
     MapToASingleScope = False
     LiveSyncImplemented = True
@@ -31,76 +34,101 @@
         lastIdentifier = 0 if lastIdentifier is None else int(lastIdentifier)
 
         conn = psycopg2.connect(database=configData.dbname, user=configData.user, password=configData.password,
                                 host=configData.host, port=configData.port)
         cur = conn.cursor()
         stmt = "SELECT id, project FROM global_measurement_overview WHERE id > %s ORDER BY id ASC"
         cur.execute(stmt, (lastIdentifier,))
-        
         newSyncIdentifiers = [new_sync_item(dataIdentifier = str(row[0]), scopeIdentifier=row[1]) for row in cur.fetchall()]
-
+        conn.close()
+        
         return newSyncIdentifiers
     
     @staticmethod
     def checkLiveDataset(configData: CoreToolsConfigData, syncIdentifier: sync_item):
         ds_ct = load_by_id(int(syncIdentifier.dataIdentifier))
+        # select max id from the database
+        conn = psycopg2.connect(database=configData.dbname, user=configData.user, password=configData.password,
+                                host=configData.host, port=configData.port)
+        
+        cur = conn.cursor()
+        stmt = "SELECT MAX(id) FROM global_measurement_overview"
+        cur.execute(stmt)
+        max_id = cur.fetchone()[0]
+        conn.close()
+        # assume only one measurement happens at a time.
+        if int(syncIdentifier.dataIdentifier) < max_id:
+            return False
+        
         return not ds_ct.completed
     
     @staticmethod
     def syncDatasetNormal(configData: CoreToolsConfigData, syncIdentifier: sync_item):
         ds_ct = create_ds_from_core_tools(configData, syncIdentifier, False)
-        if ds_ct.snapshot:
-            f_info = file_info(name = 'snapshot', created = ds_ct.run_timestamp,
-                               fileName = 'snapshot.json',
-                               fileType= FileType.JSON,
-                               file_generator = "core-tools")
-            sync_utilities.upload_JSON(ds_ct.snapshot, syncIdentifier, f_info)
-            
-            pulses = get_AWG_pulses(ds_ct.snapshot)
-            if pulses:
-                f_info = file_info(name = 'AWG pulses', created = ds_ct.run_timestamp,
-                                   fileName = 'pulses.hdf5',
-                                   fileType= FileType.HDF5_NETCDF,
-                                   file_generator = "core-tools")
-                sync_utilities.upload_xarray(pulses, syncIdentifier, f_info)
-                
-            gates = get_gates_formatted(ds_ct.snapshot)
-            if gates:
-                f_info = file_info(name = 'gates', created = ds_ct.run_timestamp,
-                                   fileName = 'gates.json',
-                                   fileType= FileType.JSON,
-                                   file_generator = "core-tools")
-                sync_utilities.upload_JSON(gates, syncIdentifier, f_info)
-        
-        if ds_ct.metadata:
-            f_info = file_info(name = 'metadata', 
-                               fileName = 'metadata.json',
-                               fileType= FileType.JSON,
-                               created = ds_ct.run_timestamp, file_generator = "core-tools")
-            sync_utilities.upload_JSON(ds_ct.metadata, syncIdentifier, f_info)
-            
+        upload_meta_data(ds_ct, syncIdentifier)
+        
         ds_xarray = ds2xarray(ds_ct)
         f_info = file_info(name = "measurement",
                            fileName = 'measured_data.hdf5',
                            fileType= FileType.HDF5_NETCDF,
                            created = ds_ct.run_timestamp, file_generator = "core-tools")
         sync_utilities.upload_xarray(ds_xarray, syncIdentifier,f_info)
         
     @staticmethod
     def syncDatasetLive(configData: CoreToolsConfigData, syncIdentifier: sync_item):
-        create_ds_from_core_tools(configData, syncIdentifier, True)
+        ds_ct = create_ds_from_core_tools(configData, syncIdentifier, True)
+        logger.info(f"Created dataset with id : {ds_ct.id}")
+        upload_meta_data(ds_ct, syncIdentifier)
+        logger.info(f"Uploaded metadata for dataset with id : {ds_ct.id}")
+        logger.info(f"Starting live sync for dataset with id : {ds_ct.id}")
         lms = live_measurement_synchronizer(int(syncIdentifier.dataIdentifier), syncIdentifier.datasetUUID)
         try:
             while lms.is_complete() is not True:
                 lms.sync()
                 time.sleep(0.2)
         except Exception as e:
             raise e
         finally:
             lms.complete()
+        logger.info(f"Live sync for dataset with id : {ds_ct.id} is complete")
+
+
+def upload_meta_data(ds_ct : 'data_set', syncIdentifier: sync_item):
+    if ds_ct.snapshot:
+        f_info = file_info(name = 'snapshot', created = ds_ct.run_timestamp,
+                            fileName = 'snapshot.json',
+                            fileType= FileType.JSON,
+                            file_generator = "core-tools")
+        sync_utilities.upload_JSON(ds_ct.snapshot, syncIdentifier, f_info)
+        logger.info(f"Uploaded snapshot for dataset with id : {ds_ct.id}")
+        
+        pulses = get_AWG_pulses(ds_ct.snapshot)
+        if pulses:
+            f_info = file_info(name = 'AWG pulses', created = ds_ct.run_timestamp,
+                                fileName = 'pulses.hdf5',
+                                fileType= FileType.HDF5_NETCDF,
+                                file_generator = "core-tools")
+            sync_utilities.upload_xarray(pulses, syncIdentifier, f_info)
+        logger.info(f"Uploaded pulses for dataset with id : {ds_ct.id}")
+        gates = get_gates_formatted(ds_ct.snapshot)
+        if gates:
+            f_info = file_info(name = 'gates', created = ds_ct.run_timestamp,
+                                fileName = 'gates.json',
+                                fileType= FileType.JSON,
+                                file_generator = "core-tools")
+            sync_utilities.upload_JSON(gates, syncIdentifier, f_info)
+        logger.info(f"Uploaded gates for dataset with id : {ds_ct.id}")
+    
+    if ds_ct.metadata:
+        f_info = file_info(name = 'metadata', 
+                            fileName = 'metadata.json',
+                            fileType= FileType.JSON,
+                            created = ds_ct.run_timestamp, file_generator = "core-tools")
+        sync_utilities.upload_JSON(ds_ct.metadata, syncIdentifier, f_info)
+        logger.info(f"Uploaded metadata for dataset with id : {ds_ct.id}")
 
 def create_ds_from_core_tools(configData: CoreToolsConfigData, syncIdentifier: sync_item, live : bool):
     ds_ct = load_by_id(int(syncIdentifier.dataIdentifier))
     
     description = f'database : {configData.dbname} | id : {ds_ct.id} | ct_uuid : {ds_ct.exp_uuid}'
     
     ds_info = dataset_info(name = ds_ct.name, datasetUUID = syncIdentifier.datasetUUID,
```

## etiket_client/sync/backends/core_tools/data_getters/get_gates.py

```diff
@@ -1,13 +1,17 @@
 from typing import Dict
 
 import logging
+
 logger = logging.getLogger(__name__)
 
 def get_gates_formatted(snapshot : Dict) -> Dict:
     gates_dic = None
-    try:
-        gates = snapshot['station']['instruments']['gates']['parameters']
-        gates_dic = {name: f"{g['value']} {g['unit']}" for name, g in gates.items() if name!='IDN'}
-    except Exception:
-        logger.exception("Error while parsing gates from snapshot")
+    if snapshot is None:
+        try:
+            gates = snapshot['station']['instruments']['gates']['parameters']
+            gates_dic = {name: f"{g['value']} {g['unit']}" for name, g in gates.items() if name!='IDN'}
+        except KeyError:
+            logger.warning("No gates found in snapshot")
+        except Exception:
+            logger.exception("Error while parsing gates from snapshot")
     return gates_dic
```

## etiket_client/sync/backends/native/sync_agent.py

```diff
@@ -1,22 +1,22 @@
 from etiket_client.local.dao.dataset import dao_dataset, DatasetUpdate as DatasetUpdateLocal
 from etiket_client.local.dao.file import dao_file, FileUpdate as FileUpdateLocal
 from etiket_client.local.models.file import FileSelect, FileStatusLocal, FileType
 
 from etiket_client.remote.endpoints.dataset import dataset_read, dataset_create, dataset_update
-from etiket_client.remote.endpoints.file import file_create, file_generate_presigned_upload_link
+from etiket_client.remote.endpoints.file import file_create, file_generate_presigned_upload_link_single
 
 from etiket_client.remote.endpoints.models.dataset import DatasetCreate, DatasetUpdate
 from etiket_client.remote.endpoints.models.file import FileCreate, FileRead
 
 from etiket_client.sync.database.types import SyncSourceStatus
 from etiket_client.sync.database.dao_sync_sources import dao_sync_sources
 from etiket_client.sync.database.models_pydantic import sync_source_update
 from etiket_client.sync.base.sync_utilities import md5
-from etiket_client.sync.uploader.file_uploader import upload_new_file
+from etiket_client.sync.uploader.file_uploader import upload_new_file_single
 from etiket_client.sync.backends.native.sync_scopes import sync_scopes
 from etiket_client.sync.backends.sources import SyncSource
 
 from sqlalchemy.orm import Session
 
 import logging, typing, uuid
 
@@ -75,24 +75,29 @@
                     logger.info("Synchronizing file with name %s, uuid %s and version_id %s", file.name, file.uuid, file.version_id)
                     fs = FileSelect(uuid=file.uuid, version_id=file.version_id)
 
                     file_remote = get_remote_file(dataset_remote.files, file.uuid, file.version_id)
                     if file_remote:
                         logger.info("File record already present on the remote server, updating details.")
                         # TODO update details.
+                        if file_remote.md5_checksum is not None:
+                            fu = FileUpdateLocal(synchronized=True)
+                            dao_file.update(fs, fu, session)
+                            continue
                     else:
                         logger.info("Creating file record on the remote server.")
                         fc = FileCreate(**file.model_dump(), ds_uuid=dataset_local.uuid)
                         file_create(fc)
                         logger.info("File record created.")
                     
+                    
                     logger.info("Starting upload of the file.")
-                    upload_info = file_generate_presigned_upload_link(file.uuid, file.version_id)
+                    upload_info = file_generate_presigned_upload_link_single(file.uuid, file.version_id)
                     md5_checksum = md5(file.local_path)
-                    upload_new_file(file.local_path, upload_info, md5_checksum)
+                    upload_new_file_single(file.local_path, upload_info, md5_checksum)
                     logger.info("Upload finished.")
                 
                     fu = FileUpdateLocal(synchronized=True)
                     dao_file.update(fs, fu, session)
             
             du = DatasetUpdateLocal(synchronized=True)
             dao_dataset.update(dataset_local.uuid, du,session)
```

## etiket_client/sync/backends/qcodes/qcodes_sync_class.py

```diff
@@ -32,41 +32,55 @@
             cursor.execute(get_newer_guid_query, (int(lastIdentifier),))
             rows = cursor.fetchall()
             
             newSyncIdentifiers += [new_sync_item(dataIdentifier=str(row[0])) for row in rows]
         return newSyncIdentifiers
     
     @staticmethod
-    def checkLiveDataset(configData: QCoDeSConfigData, syncIdentifier: sync_item):        
+    def checkLiveDataset(configData: QCoDeSConfigData, syncIdentifier: sync_item):
+        # assume only one measurement happens at a time.
+        with sqlite3.connect(configData.database_directory) as conn:
+            res = conn.execute("SELECT MAX(run_id) FROM runs")
+            max_id = res.fetchone()[0]
+        
+        if int(syncIdentifier.dataIdentifier) < max_id:
+            return False
+            
         ds_qc = load_by_id(int(syncIdentifier.dataIdentifier))
         return not ds_qc.completed
     
     @staticmethod
     def syncDatasetNormal(configData: QCoDeSConfigData, syncIdentifier: sync_item):
         ds_qc = create_ds_from_qcodes(configData, syncIdentifier, False)
 
-        created_time = datetime.fromtimestamp(ds_qc.completed_timestamp_raw)
+        created_time = datetime.fromtimestamp(ds_qc.run_timestamp_raw)
         if ds_qc.snapshot:
-            f_info = file_info(name = 'Snapshot', fileName = 'snapshot_qc.json', 
+            f_info = file_info(name = 'Snapshot', fileName = 'snapshot.json',
                                fileType= FileType.JSON,
                                created = created_time, file_generator = "QCoDeS")
             sync_utilities.upload_JSON(ds_qc.snapshot, syncIdentifier, f_info)
         
         ds_xr = ds_qc.to_xarray_dataset()
-        f_info = file_info(name = 'measurement', fileName = 'measured_data_qc.hdf5',
+        f_info = file_info(name = 'measurement', fileName = 'measured_data.hdf5',
                            fileType= FileType.HDF5_NETCDF,
                            created = created_time, file_generator = "QCoDeS")
         sync_utilities.upload_xarray(ds_xr, syncIdentifier, f_info)
 
     @staticmethod
     def syncDatasetLive(configData: QCoDeSConfigData, syncIdentifier: sync_item):
-        create_ds_from_qcodes(configData, syncIdentifier, True)
+        ds_qc = create_ds_from_qcodes(configData, syncIdentifier, True)
+        if ds_qc.snapshot:
+            created_time = datetime.fromtimestamp(ds_qc.run_timestamp_raw)
+            f_info = file_info(name = 'Snapshot', fileName = 'snapshot.json',
+                               fileType= FileType.JSON,
+                               created = created_time, file_generator = "QCoDeS")
+            sync_utilities.upload_JSON(ds_qc.snapshot, syncIdentifier, f_info)
+            
         QCoDeS_live_sync(int(syncIdentifier.dataIdentifier), str(configData.database_directory), syncIdentifier.datasetUUID)
 
-
 def create_ds_from_qcodes(configData: QCoDeSConfigData, syncIdentifier: sync_item, live : bool):
     ds_qc = load_by_id(int(syncIdentifier.dataIdentifier))
 
     collected_time = datetime.fromtimestamp(ds_qc.run_timestamp_raw)
     
     ranking = 0
     if 'inspectr_tag' in ds_qc.metadata.keys():
```

## etiket_client/sync/backends/quantify/quantify_sync_class.py

```diff
@@ -40,22 +40,29 @@
         
         return newSyncIdentifiers
     
     @staticmethod
     def checkLiveDataset(configData: QuantifyConfigData, syncIdentifier: sync_item):
         # There does not seem to be anything in a dataset to indicate that it has been completed or not :/
         # TODO : check if this is too slow?
-        otherDatasets = QuantifySync.getNewDatasets(configData, syncIdentifier.dataIdentifier)
-        if len(otherDatasets) != 0:
+        other_datasets = QuantifySync.getNewDatasets(configData, syncIdentifier.dataIdentifier)
+        if len(other_datasets) != 0:
             return False
         
         # check the last time the file is modified:
-        path = os.path.join(configData.quantify_directory, syncIdentifier.dataIdentifier, "dataset.hdf5")
-        modTime = pathlib.Path(path).stat().st_mtime
-        if datetime.now() - datetime.fromtimestamp(modTime) < timedelta(minutes=2):
+        dir_content = os.listdir(os.path.join(configData.quantify_directory, syncIdentifier.dataIdentifier))
+        m_files = [content for content in dir_content if content.endswith(".hdf5") or content.endswith(".h5")]
+        
+        if len(m_files) == 0:
+            return False
+
+        m_file = max(m_files, key=lambda f: os.path.getmtime(os.path.join(configData.quantify_directory, syncIdentifier.dataIdentifier, f)))
+        path = os.path.join(configData.quantify_directory, syncIdentifier.dataIdentifier, m_file)
+        mod_time = pathlib.Path(path).stat().st_mtime
+        if datetime.now() - datetime.fromtimestamp(mod_time) < timedelta(minutes=2):
             return True
         return False
     
     @staticmethod
     def syncDatasetNormal(configData: QuantifyConfigData, syncIdentifier: sync_item):
         create_ds_from_quantify(configData, syncIdentifier, False)
         path = os.path.join(configData.quantify_directory, syncIdentifier.dataIdentifier)
```

## etiket_client/sync/base/sync_source_abstract.py

```diff
@@ -23,15 +23,15 @@
     @staticmethod
     @abstractmethod
     def getNewDatasets(configData: Type[dataclass], lastIdentifier : str) -> 'List[new_sync_item] | None':
         raise NotImplementedError
     
     @staticmethod
     @abstractmethod
-    def checkLiveDataset(configData: Type[dataclass], syncIdentifier : sync_item):
+    def checkLiveDataset(configData: Type[dataclass], syncIdentifier : sync_item) -> bool:
         pass
     
     @staticmethod
     @abstractmethod
     def syncDatasetNormal(configData: Type[dataclass], syncIdentifier : sync_item):
         pass
```

## etiket_client/sync/base/sync_utilities.py

```diff
@@ -1,22 +1,23 @@
 from etiket_client.remote.endpoints.dataset import dataset_create, dataset_read
-from etiket_client.remote.endpoints.file import file_create, file_generate_presigned_upload_link, file_read_by_name
+from etiket_client.remote.endpoints.file import file_create, file_generate_presigned_upload_link_single, file_read_by_name
 from etiket_client.remote.endpoints.models.dataset import DatasetCreate
 from etiket_client.remote.endpoints.models.types import FileStatusRem, FileType
 from etiket_client.remote.endpoints.models.file import FileCreate
 
 from etiket_client.local.database import Session
 from etiket_client.local.dao.dataset import dao_dataset
+from etiket_client.local.dao.file import dao_file
 from etiket_client.local.models.dataset import  DatasetCreate as DatasetCreateLocal
 
 from etiket_client.sync.base.checksums.hdf5 import md5_netcdf4
 from etiket_client.sync.base.checksums.any import md5
 
 from etiket_client.sync.database.dao_sync_items import dao_sync_items
-from etiket_client.sync.uploader.file_uploader import upload_new_file
+from etiket_client.sync.uploader.file_uploader import upload_new_file_multi, upload_new_file_single
 from etiket_client.sync.database.models_pydantic import sync_item, new_sync_item
 
 from etiket_client.python_api.dataset_model.files import generate_version_id
 
 from typing import List, Dict, Optional, Set
 
 import os, json, tempfile, logging, uuid, dataclasses, datetime
@@ -108,17 +109,17 @@
                 file_raw.write(content.encode())
                 file_raw.flush()
             
             sync_utilities.upload_file(file_path, s_item, f_info)
     
     @staticmethod
     def upload_file(file_path, s_item : sync_item, f_info : file_info):
-        files = []
+        r_files = []
         try:
-            files = file_read_by_name(s_item.datasetUUID, f_info.name)
+            r_files = file_read_by_name(s_item.datasetUUID, f_info.name)    
         except Exception:
             # TODO : check on the type of exception
             logger.info("File %s not found on remote server, creating new file.", f_info.name)
         
         if f_info.fileType is None:
             if file_path.endswith('.h5') or file_path.endswith('.hdf5') or file_path.endswith('.nc'):
                     f_info.fileType = FileType.HDF5_NETCDF
@@ -134,28 +135,34 @@
                 logger.warning("Could not calculate md5 checksum for file %s, of dataset with uuid : %s. This file will be considered as a normal H5 file.", f_info.name, s_item.datasetUUID)
                 f_info.fileType = FileType.HDF5
                 md5_checksum = md5(file_path)
         else:
             md5_checksum = md5(file_path)
         # TODO fix definition of collected and created on the server
         # create the file entry (if needed)        
-        version_id = generate_version_id()
-        
+        version_id = generate_version_id(f_info.created)
         fc = FileCreate(name = f_info.name, filename=f_info.fileName,
                         creator=f_info.creator, uuid =uuid.uuid4(), collected = f_info.created,
                         size = os.stat(file_path).st_size, type = f_info.fileType,
                         file_generator = f_info.file_generator, version_id = version_id,
                         ds_uuid = s_item.datasetUUID)
         
-        if len(files) == 0:
+        if len(r_files) == 0:
+            try :
+                with Session() as session :
+                    l_files = dao_file.get_file_by_name(s_item.datasetUUID, f_info.name, session)
+                    if len(l_files) > 0:
+                        fc.uuid = l_files[0].uuid
+            except Exception:
+                pass
             file_create(fc)
         else:
-            fc.uuid = files[0].uuid
+            fc.uuid = r_files[0].uuid
             
-            f_dict = {file.version_id : file for file in files}
+            f_dict = {file.version_id : file for file in r_files}
 
             if version_id in f_dict.keys():
                 if f_dict[version_id].md5_checksum == md5_checksum:
                     return
                 elif f_dict[version_id].status is FileStatusRem.pending:
                     # TODO : remove upload_id of the current upload and recheck status?
                     pass
@@ -170,12 +177,13 @@
                     last_mod_time = datetime.datetime.fromtimestamp(os.path.getmtime(file_path))
                     fc.version_id = generate_version_id(last_mod_time)
                     file_create(fc)
             else:
                 file_create(fc)
 
         # upload the file
-        upload_info = file_generate_presigned_upload_link(fc.uuid, version_id)                
-        upload_new_file(file_path, upload_info, md5_checksum)
+        upload_info = file_generate_presigned_upload_link_single(fc.uuid, version_id)                
+        # upload_new_file_multi(file_path, upload_info, md5_checksum)
+        upload_new_file_single(file_path, upload_info, md5_checksum)
 
         logger.info("File %s uploaded to remote server.", f_info.name)
```

## etiket_client/sync/database/models_db.py

```diff
@@ -1,10 +1,10 @@
 from etiket_client.sync.database.types import SyncSourceTypes, SyncSourceStatus
 
-from sqlalchemy import ForeignKey, Index, UniqueConstraint, func
+from sqlalchemy import ForeignKey, Index, UniqueConstraint, func, types
 from sqlalchemy.orm import Mapped, mapped_column, DeclarativeBase
 from sqlalchemy.types import JSON
 
 from datetime import datetime
 
 import uuid, typing
 
@@ -18,15 +18,15 @@
     name : Mapped[str] = mapped_column(unique=True)
     type : Mapped[SyncSourceTypes]
     status : Mapped[SyncSourceStatus]
     items_total : Mapped[int] 
     items_synchronized : Mapped[int]
     items_failed : Mapped[int]
     items_skipped : Mapped[int]
-    last_update : Mapped[datetime] = mapped_column(server_default=func.now(), onupdate=func.now())
+    last_update : Mapped[datetime] = mapped_column(types.DateTime(timezone=True), server_default=func.now(), onupdate=func.now())
     config_data : Mapped[dict] = mapped_column(JSON)
     
     auto_mapping : Mapped[bool]
     default_scope : Mapped[typing.Optional[uuid.UUID]]
     
 
 class SyncScopeMappingsSQL(SyncBase):
@@ -51,8 +51,8 @@
     dataIdentifier: Mapped[int] = mapped_column(index = True)
     scopeIdentifier : Mapped[typing.Optional[str]]
     datasetUUID  : Mapped[uuid.UUID] = mapped_column(unique=True)
     
     synchronized : Mapped[bool] = mapped_column(default = False)
     attempts : Mapped[bool] = mapped_column(default = 0)
     
-    last_update : Mapped[datetime] = mapped_column(server_default=func.now(), onupdate=func.now())
+    last_update : Mapped[datetime] = mapped_column(types.DateTime(timezone=True), server_default=func.now(), onupdate=func.now())
```

## etiket_client/sync/database/models_pydantic.py

```diff
@@ -1,8 +1,7 @@
-from etiket_client.remote.endpoints.models.utility import convert_time_from_utc_to_local
 from etiket_client.sync.database.types import SyncSourceStatus, SyncSourceTypes
 
 from pydantic import BaseModel, ConfigDict, Field, field_serializer, field_validator
 from typing import List, Optional, Tuple, Dict
 
 import uuid, datetime, pathlib
```

## etiket_client/sync/uploader/file_uploader.py

```diff
@@ -1,52 +1,71 @@
-from etiket_client.remote.endpoints.file import file_validate_upload
-from etiket_client.remote.endpoints.models.file import FileValidate, FileSignedUploadLinks
+from etiket_client.remote.endpoints.file import file_validate_upload_multi, file_validate_upload_single
+from etiket_client.remote.endpoints.models.file import FileValidate, FileSignedUploadLink, FileSignedUploadLinks
+from etiket_client.remote.client import client
+
 from etiket_client.exceptions import RequestFailedException
 
-import requests, logging
+import requests, logging, os, time
 
 logger = logging.getLogger(__name__)
 
+def upload_new_file_single(file_raw_name, upload_info : FileSignedUploadLink, md5_checksum, ntries = 0):
+    timeout = os.stat(file_raw_name).st_size/100_000
+    if timeout < 10 :
+        timeout = 10
+
+    with open(file_raw_name, 'rb') as file:
+        for n_tries in range(3):
+            response = client.session.put(upload_info.url, data=file, timeout=timeout)
+            success = True
+            if response.status_code >= 400:
+                logging.warning('Failed to upload a file with name (%s).\nRAW JSON response :: %s', file_raw_name, response.reason)
+                success = False
+            if n_tries == 2 and success is False:
+                raise RequestFailedException('Failed to upload file.')
+            if success is True:
+                break
+    file_validate_upload_single(FileValidate(uuid=upload_info.uuid, version_id=upload_info.version_id, upload_id='', md5_checksum=md5_checksum, etags=[]))
+
 # TODO on the server side, make sure only one client can upload.
-def upload_new_file(file_raw_name, upload_info  : FileSignedUploadLinks, md5_checksum, ntries = 0):
+def upload_new_file_multi(file_raw_name, upload_info  : FileSignedUploadLinks, md5_checksum, ntries = 0):
     try:
         n_parts = len(upload_info.presigned_urls)
-        chunck_size = upload_info.part_size
+        chunk_size = upload_info.part_size
         etags = []
-        
         with open(file_raw_name, 'rb') as file:
             for i in range(n_parts):
-                file.seek(i * chunck_size)
-                data = file.read(chunck_size)
+                file.seek(i * chunk_size)
+                data = file.read(chunk_size)
 
                 for n_tries in range(3):
-                    success, response = upload_chunck(upload_info.presigned_urls[i], data)
-                    if n_tries == 2 and success == False:
+                    success, response = upload_chunk(upload_info.presigned_urls[i], data)
+                    if n_tries == 2 and success is False:
                         raise RequestFailedException('Failed to upload file.')
-                    if success == True:
+                    if success is True:
                         break
 
                 etags.append(str(response.headers['ETag']))
-
+        
         fv = FileValidate(uuid=upload_info.uuid, version_id=upload_info.version_id,
                             upload_id=upload_info.upload_id, md5_checksum=md5_checksum,
                             etags=etags)
-        file_validate_upload(fv)
-    
+        file_validate_upload_multi(fv)
+
     except Exception as e:
         if ntries < 3:
-            logger.warning(f'Failed to upload file with name {file_raw_name}.\n Error message :: {e}, try {ntries} (and trying again).\n)')
-            upload_new_file(file_raw_name, upload_info, ntries+1)
+            logger.warning('Failed to upload file with name %s.\n Error message :: %s, try %s (and trying again).\n', file_raw_name, e, ntries)
+            upload_new_file_multi(file_raw_name, upload_info, ntries+1)
         else :
-            logger.exception(f'Failed to upload file with name {file_raw_name}.\n')
+            logger.exception('Failed to upload file with name %s.\n', file_raw_name)
             raise e
     
-def upload_chunck(url, data):
-    response = requests.put(url, data=data)
+def upload_chunk(url, data):
+    response = client.session.put(url, data=data, timeout=400) # assume that the upload speed is > 100KB/s
 
     if response.status_code >=400:
         response_json = None
         if response:
             response_json = response.json()
-        logging.warning(f'Failed to upload a chunk to url with hash ({hash(url)}).\nRAW JSON resonse :: {response_json}')
+        logging.warning('Failed to upload a chunk to url with hash (%s).\nRAW JSON response :: %s', hash(url), response_json)
         return False, response
     return True, response
```

## etiket_client/testing/live_data_generation/qdrive_LD.py

```diff
@@ -35,14 +35,16 @@
     ch1_values = np.linspace(400, 800, n_x_vals)
     ch2_values = np.linspace(200, 450, n_y_vals)
     
     for i in ch1_values:
         for j in ch2_values:
             dc.add_data({param : param.get(), dac.ch1 : i, dac.ch2 : j})
             time.sleep(t_exp/(n_x_vals*n_y_vals))
+    
+    dc.complete()
 
 def _3D_Sweep_0D_param_test():
     n_x_vals = 20
     n_y_vals = 50
     n_z_vals = 30
     
     param = TEST_0D_3D_graph_alike_param(n_x_vals, n_y_vals, n_z_vals)
@@ -56,14 +58,16 @@
     
     for i in ch1_values:
         for j in ch2_values:
             for k in ch3_values:
                 dc.add_data({param : param.get(), dac.ch1 : i, dac.ch2 : j, dac.ch3 : k})
                 time.sleep(t_exp/(n_x_vals*n_y_vals*n_z_vals))
 
+    dc.complete()
+
 def _1D_Sweep_1D_array_param_test():
     n_x_vals = 100
     param = TEST_1D_array_parameter_1D(n_x_vals)
     
     ds = dataset.create("1D_Sweep_1D_array_param_test")
     dc = data_collector(ds)
     dc += from_QCoDeS_parameter(param, [dac.ch1], dc)
```

## Comparing `etiket_client-0.2.11.dist-info/LICENCE` & `etiket_client-0.2.12.dist-info/LICENCE`

 * *Files identical despite different names*

## Comparing `etiket_client-0.2.11.dist-info/METADATA` & `etiket_client-0.2.12.dist-info/METADATA`

 * *Files 11% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: etiket_client
-Version: 0.2.11
+Version: 0.2.12
 Summary: A client to interact with the etiket web framework.
 Author: QDrive team
 Requires-Python: >=3.7
 License-File: LICENCE
 Requires-Dist: PyQt5 >=5.15.0
 Requires-Dist: SQLAlchemy >=2.0.0
 Requires-Dist: alembic >=1.6.0
@@ -15,14 +15,15 @@
 Requires-Dist: tabulate >=0.8.0
 Requires-Dist: filelock >=3.4.0
 Requires-Dist: platformdirs >=4.0.0
 Requires-Dist: psutil >=5.8.0
 Requires-Dist: numpy >=1.16.0
 Requires-Dist: PyYAML >=6.0.0
 Requires-Dist: email-validator <=2.1.0
+Requires-Dist: setproctitle >=1.3.3
 Requires-Dist: python-dateutil >2.8.0
 Requires-Dist: qcodes >0.3.0
 Requires-Dist: xarray <=0.20.2,>=0.15.0 ; python_version < "3.8"
 Requires-Dist: h5py <=3.8.0,>=3.0.0 ; python_version < "3.8"
 Requires-Dist: netCDF4 <=1.5.8,>=1.5.0 ; python_version < "3.8"
 Requires-Dist: xarray >=0.20.2 ; python_version >= "3.8"
 Requires-Dist: h5py >=3.8.0 ; python_version >= "3.8"
```

## Comparing `etiket_client-0.2.11.dist-info/RECORD` & `etiket_client-0.2.12.dist-info/RECORD`

 * *Files 4% similar despite different names*

```diff
@@ -3,15 +3,15 @@
 etiket_client/GUI/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 etiket_client/GUI/app.py,sha256=A5gGtXvZ1M5ms5MB6l0RiOtPPCgm6S1Sz5WKWl5NtRk,1845
 etiket_client/GUI/models/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 etiket_client/GUI/models/login_mgmt.py,sha256=BLde15chZThFv0gCcstFg7kkijNn5SCvreC7Ri3pU_k,1362
 etiket_client/GUI/models/schema_mgmt.py,sha256=Obag2eR1agR9fUCjBBVKsQtVNlV3ZTbZhkL2VYUjzYs,1399
 etiket_client/GUI/models/scope_mgmt.py,sha256=kdB_vzFD7DlKJvlBqqB_gkmPVquzCThK3O1654f9U2A,2112
 etiket_client/GUI/models/sync_def_scope.py,sha256=B-pAtJzRoYgxPuvWPbRSHJwGheft0QXVjbP-mZII5Aw,1314
-etiket_client/GUI/models/sync_mgmt.py,sha256=Dq59mI7dOPpdsrWzcAI1JqYSCmZmWTr645OFQ88hphM,8139
+etiket_client/GUI/models/sync_mgmt.py,sha256=powx6Bn5FBPbYvlvU1OwNYxxZsIlEo68JxHAtWSQjCE,8340
 etiket_client/GUI/models/sync_proc_mgmt.py,sha256=Eex-vihsmwqqryIn1KYOcDqB74w0RkN-kCbB7TZucgo,1195
 etiket_client/GUI/qml/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 etiket_client/GUI/qml/login.qml,sha256=vrfPQ8NOeZp0eCnsHkdFPS3F2CJbPANZ-pei45-01tg,2748
 etiket_client/GUI/qml/main.qml,sha256=pgT-xZnhiItFe457-wSW92Y3u_wpeUJMv88hPPyfpUk,921
 etiket_client/GUI/qml/settings_index.qml,sha256=F6r73KG81iaYTWfjfAWwd5SxjYizHUw5Nfqio51siLY,970
 etiket_client/GUI/qml/icons/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 etiket_client/GUI/qml/icons/delete.svg,sha256=7HXnVq9ON6kmovZO-PWerIUnamWPyKeu8MwSxehx6XE,300
@@ -19,128 +19,131 @@
 etiket_client/GUI/qml/settings_pages/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 etiket_client/GUI/qml/settings_pages/scope_settings.qml,sha256=7yY5nmBdYm9PcnYkgzeUaTj7aqnbws58KJEzFT3Jyoo,7366
 etiket_client/GUI/qml/settings_pages/sync_add_new_source.qml,sha256=Yp4udI--QTNbB9ciO6c1KEg3_MIDKAYeiKhlTm6uQvI,18481
 etiket_client/GUI/qml/settings_pages/sync_settings.qml,sha256=HnhSyl5qkGlmad0Z2JskXAcMporXB3wQxdNw2ls_MTY,7175
 etiket_client/GUI/resources/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 etiket_client/GUI/resources/resource_rc.py,sha256=mX5GMKlYtRO4evpeswFokedjeh6VGfHm5qNgiW3BuV4,1434
 etiket_client/local/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-etiket_client/local/database.py,sha256=AG5QTcSbSJWvnnnoi670Vm3LrNSDgsPmit374tnivTk,966
+etiket_client/local/database.py,sha256=T6RA5ygauKG82FW8EheiT9dqqCqv2251KjTVTG66xnk,1213
 etiket_client/local/exceptions.py,sha256=D5yaTw0ndwFYDmDl-8ngKIB33E4Yoc_kZFNmfBy-SKY,1144
-etiket_client/local/model.py,sha256=vNUmCuJd1tVuWr0YDUeh3GRB6u2preG_vEJsDiLXalU,6129
+etiket_client/local/model.py,sha256=HQFf4EgyJzeP0CktEYMBn29HXYN7PjPhTDgqtcrhc34,6109
 etiket_client/local/types.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 etiket_client/local/alembic/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 etiket_client/local/alembic/env.py,sha256=T4wmyp6pfdNn8qfY6YQpZXLXmXKNz-2TTNmZTNd82xE,2239
 etiket_client/local/alembic/versions/24b0115d4cc7_adding_delete_cache_table.py,sha256=k4hrypvmyJmJhlP5IxgAPoA8yOlGXUZUv7nRdakB6L8,850
 etiket_client/local/alembic/versions/4000a8b9703a_qdrive_v_0_1_0.py,sha256=TTdvTObM3aZO4D7pj2vbuZQZEjrCCAtu-jt90z7MvzU,8187
 etiket_client/local/alembic/versions/6e8777d1c9f3_etiket_client_v_0_1_1.py,sha256=XJFQrTmJU1ENc7HqBiObuxCF2hxd7tjcI19d15b3RPs,802
 etiket_client/local/alembic/versions/716ce37a1bc1_etiket_client_v_0_1_3.py,sha256=V7RgRWlwb1qeFrSmUAlWUZm7oTA7tD4C7QtVwAlfGAI,3913
 etiket_client/local/alembic/versions/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 etiket_client/local/alembic/versions/bec34208dd87_update_uniqueness_of_alt_uuid.py,sha256=Euh5hZw9TfYMG-WlgnLPllKO2PrTx-BYtpLd581MLeo,4611
+etiket_client/local/alembic/versions/c077798e06b9_adding_sync_attr_to_files.py,sha256=ZpyzhYAatiX-IHtoLBIjA1p5C_SVdFZUuPCWcTTiq-Q,1272
 etiket_client/local/alembic/versions/c5d93c68feda_etiket_client_v0_1_5.py,sha256=68MfKSfgLZmKvUWTr3A6xu3Cl8kTYZeaM4sb8SFZ3bE,1289
+etiket_client/local/alembic/versions/c867b432fc6a_convert_time_to_utc.py,sha256=7TDMzA3ToPhpFFFAEgFPvq9D0VFKQygmoSN4NcdmpmI,1812
 etiket_client/local/alembic/versions/fe8c0d015ba2_update_sync_tables.py,sha256=Dtx0GEgJ0Wrm6r1u_DEXs3HPzdzhLGc7IAeKrv8MMO8,4790
 etiket_client/local/dao/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-etiket_client/local/dao/base.py,sha256=_fPUZMdKetwOi4J-8FAWh9Zkv3GraWzUgbIJFyyydig,2041
-etiket_client/local/dao/dataset.py,sha256=WaRWCjeRmdzKx7oAYrrMm8hBcL9Tf3iOvCIRKTzRz64,9682
-etiket_client/local/dao/file.py,sha256=mvUcFwC7G_UymTHARSYKuQ1DZr0sL_618lmrp3PxUeU,3297
+etiket_client/local/dao/base.py,sha256=UDa-pio52orIrbqkCMBdreAHUjHO5kAofWPKnSRjPuo,2060
+etiket_client/local/dao/dataset.py,sha256=gmOuhjulb_2yan2iT5Wxt-PxCuzFv2chMdDpMrqjM68,10377
+etiket_client/local/dao/file.py,sha256=kuk0N10f8AE5jgpf3Az4L4bbeKQRL21scsrFx4XP8Js,3790
 etiket_client/local/dao/schema.py,sha256=NTqNru47Gjj---3gfNiUs44OB_4b3ACwkG4d_fgFsCo,2153
-etiket_client/local/dao/scope.py,sha256=bZ0_meh1GYvr_HTFzEA_GBCBT7gvnSdgASKZ-To43hA,5932
+etiket_client/local/dao/scope.py,sha256=P_GAplsCy09pfcjxoCLjH54VZPKDZz2O9s3yU7HlD4c,6113
 etiket_client/local/dao/user.py,sha256=PfPROEIc0RQgKGOc4cHQyskA0V2Q0WYxEBqgqjyK-JI,2397
 etiket_client/local/models/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-etiket_client/local/models/dataset.py,sha256=b-X-D8rd1y-VRdXW-3kzllK2vvPbU_RrnQtQlR2UW9g,2537
-etiket_client/local/models/file.py,sha256=X-aFAEDm06QwRrjoRWFQZq-YYhZmpleVoF8Pjwx1Lc4,1595
-etiket_client/local/models/schema.py,sha256=k3fMLaYDvsX30kDV8ENbloghYa_WmfSM313m2JrNQhc,925
-etiket_client/local/models/scope.py,sha256=gHYbwExKGr-XejfNJmS1caJrIepxcknu0mGAhYelMrU,844
-etiket_client/local/models/user.py,sha256=-5Yui1SQ8qs3mHRYF500OI1x45vh780Wz3flSdYGUSE,629
-etiket_client/local/models/user_base.py,sha256=lHupgo_C0OxslEyoBJ9xMKtsJ9HJJoe7b292a_PM1q4,577
+etiket_client/local/models/dataset.py,sha256=1mfgBdMWa5dQnKuOKnCQAbBHKiQGeIN-dZUgVCGa-7k,3441
+etiket_client/local/models/file.py,sha256=DtVdRM3AP5ad13AtCmONGnIpSqbgbaDPUPivPa1kycI,2904
+etiket_client/local/models/schema.py,sha256=avkIZgHtuO-N7nkK6RIMvFjmEu7RNiAbfYIbblJeKfM,1427
+etiket_client/local/models/scope.py,sha256=lJPvQlCFjEDz3GQVLAYHFvf0290pb6KqmzyX0iroM2A,1342
+etiket_client/local/models/user.py,sha256=VeFzx4igCtI4r4J2qJFT8aEkz8zFUrlxvp0xXYXQ5P8,900
+etiket_client/local/models/user_base.py,sha256=B8ww8deiv6LKmIwG9MuMLy3AzUq8uO_IkDNw9Vwk57k,1292
+etiket_client/local/models/utility.py,sha256=HAAzudb8weTRswYmaOC7IhwM5pgFXs1noxIVdPit1GU,453
 etiket_client/python_api/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-etiket_client/python_api/dataset.py,sha256=Q8H7smCF3S92Xs1h3EcTLsfg7YmfSXMJkiYmZvRL_GU,1151
+etiket_client/python_api/dataset.py,sha256=tbyQ8XbtQaeBSaDhX40vpRJNxleCBTUO2EmSaA5Q5G4,1028
 etiket_client/python_api/exceptions.py,sha256=bFL2df4VhsiUeKryLGFpIig_0OSPSRQKpzTMIUTBf2g,51
 etiket_client/python_api/schema.py,sha256=00N_TRZSigbNkcP2pD2g6T_SXFnz9PL6bCm1lsMKyBE,465
 etiket_client/python_api/scopes.py,sha256=6Y76YbsPGAgIZPIrSzYlOIwQdCqv7boRbxjB5sxNjoU,739
 etiket_client/python_api/user.py,sha256=IS0TaL9Yc4OGLTIWW3tyD2L9XWY1xp7F2CxzUT6ShZI,227
 etiket_client/python_api/dataset_model/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-etiket_client/python_api/dataset_model/dataset.py,sha256=-SuAast2o_Sm3yB9pAC2O9QK9uC49fQQS3L8J-Mp2Wg,3232
-etiket_client/python_api/dataset_model/files.py,sha256=URdZo8uesQu1B8d64ec10RNSuIMqKfh7wHNZ5I8Bzf0,11510
+etiket_client/python_api/dataset_model/dataset.py,sha256=2I70i8ZvQjoP9aHQ9EfFcAcDllZTQE5HVd188jOv8zc,3280
+etiket_client/python_api/dataset_model/files.py,sha256=UuU64d4S3oe-QS0FqUTlSDtqf_TXiApzwiEpK9PJD2I,11710
 etiket_client/python_api/dataset_model/utility.py,sha256=EvJ5lragZ9EL6BIZPpIsHj2svdXwQ7v-YPZzIIMV0IY,2060
 etiket_client/remote/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 etiket_client/remote/authenticate.py,sha256=zrnxKQCVJk0Pm4TFjsQuH2Os9G8GXkiBqTc5Ma_t6pw,1943
-etiket_client/remote/client.py,sha256=UXRGNQBV8sJm5QAWvRnlE-0N6gGZyptNLvp8eXqoG0o,6447
+etiket_client/remote/client.py,sha256=lLILVswIbO1erx5O4G-CXEaLw2ryKUEVcL6Z6DVqMKQ,6517
 etiket_client/remote/endpoints/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 etiket_client/remote/endpoints/dataset.py,sha256=Xak5XhveTVNr_P6O07SAaTggR-tZL2TlxlG-EGavJmI,1467
-etiket_client/remote/endpoints/file.py,sha256=--yejK5jrYZL015uh7VV8IXK2-_5aVnhG5sxEqkQNxc,1550
+etiket_client/remote/endpoints/file.py,sha256=oFmwiHr2vSqJZhyCXBh5_Csi0WHqw2v2AFPhAryI_RQ,2069
 etiket_client/remote/endpoints/schema.py,sha256=FmSaWAi2qPAVGPjhI5Mc3UiLcCiXbe_uiTo-6-X7ECU,664
 etiket_client/remote/endpoints/scope.py,sha256=by_jnt2erITX93LS7mJVeICa8tRw6GNOzDebLjYxkv4,813
 etiket_client/remote/endpoints/user.py,sha256=e6y1-Rirt7EJwdiKb23qCTlgcWaN7YkRn0JWswo7arg,257
 etiket_client/remote/endpoints/version.py,sha256=8p2TL7m5Q21BmATc8Gam-O2D6GVHRjHREQk0XypnSL4,206
 etiket_client/remote/endpoints/models/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 etiket_client/remote/endpoints/models/dataset.py,sha256=iFZ1nxeu2xIufhN5dHvkWqRJ7HIWj0eiwMwmbqAZlfI,3035
-etiket_client/remote/endpoints/models/file.py,sha256=UJv-D40f3vuBkVksfhuMnjlZYkKjW58w3weZrZgkjxA,2108
+etiket_client/remote/endpoints/models/file.py,sha256=AvdDlvDID-_uZpPiM-eqQ2jayNyeebtRYhwFHTUfmPI,2203
 etiket_client/remote/endpoints/models/schema.py,sha256=vGQv3whPmZAhLr8XsTN8GJmXqXQxFrPLeID21lCNAdM,708
 etiket_client/remote/endpoints/models/schema_base.py,sha256=_DTaNc7UVTMjZR5pJyGmX38b50-kYLBT6VZm03NuwQg,1722
 etiket_client/remote/endpoints/models/scope.py,sha256=_xhjEMhaCkNpFi7yQIED8IoCpT79X3ee4SS53p8-cy4,1490
 etiket_client/remote/endpoints/models/types.py,sha256=oWr5XFxLy1h3Wy5CQGV6dXM5m0AXqSis4o201r9H9T4,1375
 etiket_client/remote/endpoints/models/user.py,sha256=B08By0kNwJcAeFfYVJhevFN46-psZzQM7rNFJ1yx7FQ,1399
 etiket_client/remote/endpoints/models/user_base.py,sha256=RUcNX6F1mfgwiYU6Uqn_-mK2Yo6eWV0WASGtD3Wg96k,1270
-etiket_client/remote/endpoints/models/utility.py,sha256=RUGeIWDr8l-oZhN4wJsMSvyVxeXzaEan58-r6M0fxcA,433
+etiket_client/remote/endpoints/models/utility.py,sha256=brmqZipSmFLZXYuwGd1azIeYTVgx-8xNnY6uGPFC0l8,463
 etiket_client/remote/tools/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-etiket_client/remote/tools/file_download.py,sha256=jaXvOdTbpHxiV_6O7TH6gkL7JUvDhuF4_qHP5jOseo4,2345
+etiket_client/remote/tools/file_download.py,sha256=0hVxWsEeW279qJyOwHzn5T8fq8LXcAW2sPxxS1zUb64,2560
 etiket_client/settings/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-etiket_client/settings/folders.py,sha256=xm89IgHP5n--o8Qtepz3OXzCRoSAzA3Xr5ge2BjuRKo,1117
+etiket_client/settings/folders.py,sha256=VbAOmki1agxvoQPS8cjgtBH-RNqFX0lQUTx0g0LAd-A,1289
 etiket_client/settings/logging.py,sha256=JE3fDaRPz6LbeUEpDYvVuAeEmZKtBC0z0dvFrtL4IDA,1639
-etiket_client/settings/saver.py,sha256=fHXlyc6II2kZJ0nX9UEPpRLKNKdr1YKnUIlkRcZUgYQ,1580
+etiket_client/settings/saver.py,sha256=5UFg95XEC4UDi8DZJmp_KnizP9VlYBnA3Qaf968RiaE,1647
 etiket_client/settings/schema_settings.py,sha256=jzB6VYV7fJYHYlVKMx_QpObxAZ10x6BPxVrk6x0LOxg,758
-etiket_client/settings/user_settings.py,sha256=nWOpUNDizU0Id1axIYnH206US0BSR3zpX_I6KPLN_FE,660
+etiket_client/settings/user_settings.py,sha256=Ud_XIJeBT0FLFxY6RMUwLI29HemZOvNcuD0EZVgil6E,691
 etiket_client/sync/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-etiket_client/sync/proc.py,sha256=V3e_Bj1zdpscZIozi0YI4q1Q4ZNp2lE75mFFvGL_Kdg,2977
-etiket_client/sync/run.py,sha256=uuVFtqV79JJDBVEkpYhHRxd0Gxffyjtd-kp7x7Z5uVI,6905
+etiket_client/sync/proc.py,sha256=DwmGblv4thtxLFLgiVSHnw7ywYHHOiMkpGnDW1Nb8_A,3067
+etiket_client/sync/run.py,sha256=uItKplcyT0hTeLk48M5tBnreNgWcn1e3E6-Au4BSpLA,7890
 etiket_client/sync/backends/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 etiket_client/sync/backends/sources.py,sha256=R-se09PUBJ2BV2OyjrsQkEjFSHgNFxGrYFhxYyCCkoE,1633
 etiket_client/sync/backends/core_tools/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 etiket_client/sync/backends/core_tools/core_tools_config_class.py,sha256=XN_yCG-K2AKGY1DvdsfqKvwPFJUqmLq4uDLII9Gz5gE,171
-etiket_client/sync/backends/core_tools/core_tools_sync_class.py,sha256=bt3H8_0_e4EnAp9TtVKYIgXbJJab6mrBCbVCp3zdLko,5612
+etiket_client/sync/backends/core_tools/core_tools_sync_class.py,sha256=g0nao6qbnZ2XEP4LZJgBfvTAz80W-v7cWMPvtYLyKrs,6919
 etiket_client/sync/backends/core_tools/data_getters/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-etiket_client/sync/backends/core_tools/data_getters/get_gates.py,sha256=ajhvKyUW-haBAdFEi6ZcN9Zbplov-YpPnNDMA60ZlSs,442
+etiket_client/sync/backends/core_tools/data_getters/get_gates.py,sha256=QXNBAa9KaE4g4WTh5FC65BqXXEdvqYE5EIqhiHozI3o,570
 etiket_client/sync/backends/core_tools/data_getters/get_pulses.py,sha256=Hi7RfH2k7MamJOrVJCPXkq07FBP44suT7sek6ERR5-Q,6693
 etiket_client/sync/backends/core_tools/real_time_sync/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 etiket_client/sync/backends/core_tools/real_time_sync/core_tools_m_param.py,sha256=XGh95z5Og5dj677SAMVlB6ttTm4pfHYyQBtVH1YKJy0,485
 etiket_client/sync/backends/core_tools/real_time_sync/measurement_sync.py,sha256=M020UrZ_313OssF6Soq_NfEe7jPPCN7a94nXUS0YMl0,4730
 etiket_client/sync/backends/core_tools/real_time_sync/qcodes_parameters.py,sha256=9Tpof8jdl49HqCsTAaUCD6mpS2QA2zbsayXpAaXHzsA,8112
 etiket_client/sync/backends/native/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-etiket_client/sync/backends/native/sync_agent.py,sha256=KTRkQ92ROnNn7Y2_PbWb4KC4U4GyEKhbfsBDE1rEjPU,6194
+etiket_client/sync/backends/native/sync_agent.py,sha256=dADgDyjrpsU1DlbbeGScmVZD4vx-cUkc6w8wGnWHkJA,6474
 etiket_client/sync/backends/native/sync_scopes.py,sha256=u1bw8Jk2xYbNBt1zpUi1tPk9mzwlmvQtWkdG1wPem8c,2945
 etiket_client/sync/backends/native/sync_user.py,sha256=9AEcG9SEK--A_bGgEnMuNWLO_Dagjofe7gDU5dBEayw,971
 etiket_client/sync/backends/qcodes/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 etiket_client/sync/backends/qcodes/qcodes_config_class.py,sha256=0w7sNNW-TBib8-drNP464smaSUMbLQVhg6IfpkznTXI,129
-etiket_client/sync/backends/qcodes/qcodes_sync_class.py,sha256=CI6KEQBUbIRF6cwmNzwrPTt9jZK3lPwu5OTsg7-lGaQ,4863
+etiket_client/sync/backends/qcodes/qcodes_sync_class.py,sha256=ZsCnRxgilmA2u7qn3m4fxoVRa8FBo23QzCr8XD1ov20,5590
 etiket_client/sync/backends/qcodes/real_time_sync.py,sha256=i5Im84Rr3vnktOEYkhRaCyjSJj9usrvhOgXFy4xWd6U,4230
 etiket_client/sync/backends/quantify/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-etiket_client/sync/backends/quantify/quantify_sync_class.py,sha256=RmsteMG7BH-XvbOSll_R0g21edDxOyysSOUAv1O80rg,5195
+etiket_client/sync/backends/quantify/quantify_sync_class.py,sha256=5ChlrGB1h80TypRV7cwWojWUjh1fWTpg5n2rbKT5r-A,5617
 etiket_client/sync/base/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-etiket_client/sync/base/sync_source_abstract.py,sha256=i0B3aEKCxuH-W1BxW1Nxww0lDWUo70GesN9h2l7FhrA,1260
-etiket_client/sync/base/sync_utilities.py,sha256=tOUrR8CGI243RIxHlrrw0bHgwp9NPeK19FtFdbfqWfU,8356
+etiket_client/sync/base/sync_source_abstract.py,sha256=7M9vO5iCOQKyKjNDCozied1It6-DkPE54aE13WKLOPg,1268
+etiket_client/sync/base/sync_utilities.py,sha256=8lpB6Za1VL80aOXI0CtHmoSPcZs6ylYaoyEKc5ExvUk,8848
 etiket_client/sync/base/checksums/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 etiket_client/sync/base/checksums/any.py,sha256=aAVSOp9mQb7yf9QttP5Ki8saUnd3fDr8oGZSgsuLJKM,208
 etiket_client/sync/base/checksums/hdf5.py,sha256=nB0R-7ILqFpOkMNjqONsdaCizlQzHooUxb7Wv6GdvMI,1379
 etiket_client/sync/database/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 etiket_client/sync/database/dao_scope_mappings.py,sha256=pht7g_I8MufVgdBZydA2eDhqtnhan87dJqvBXXHqqbY,1998
 etiket_client/sync/database/dao_sync_items.py,sha256=MfzB_rPwNqIfocBoewd8R9szObciXkuc6NuzIB0ltyc,8349
 etiket_client/sync/database/dao_sync_sources.py,sha256=fmxa2_bZNwh8ivoEluiRSLO5zBEEuuqrnuGDtkifRIw,1690
-etiket_client/sync/database/models_db.py,sha256=SIWNGQnz-daOY8dLmN-wiMSGCYa_7xIVCYJVk6lBrXs,2417
-etiket_client/sync/database/models_pydantic.py,sha256=sECVV_Et7tdL-5Y56UDf0Xjf_AnGAA1eLGAhc8Q_lcw,2505
+etiket_client/sync/database/models_db.py,sha256=hEQxhi8L1wQVqxNIU9Vp7EorYQHXG9UE7yH6Ff3Wugg,2486
+etiket_client/sync/database/models_pydantic.py,sha256=8ze8LGqTBeN3GANdChm2mOqmtlzZfUifzVjz323Byg8,2416
 etiket_client/sync/database/start_up.py,sha256=D4OEGlf7yINGIyfiGKyJveBoicudVG6NpO7stFXlniw,563
 etiket_client/sync/database/types.py,sha256=-B1bm4xzPd0Gn2cRV9yNus4FxZQQYNk4VPLvdGkisMw,299
 etiket_client/sync/uploader/__init__.py,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
-etiket_client/sync/uploader/file_uploader.py,sha256=SjSQC7epfXrt6V1hj8nE_aqauV0AhQPmJOMtDH65ZOQ,2170
+etiket_client/sync/uploader/file_uploader.py,sha256=55tx1uoRjNCShuwaAHTm4riAfueYGYc-lRUwGm-jQHA,3297
 etiket_client/testing/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 etiket_client/testing/test_gui.py,sha256=jcfXkukTqeiPFZ1KeZIwXENNyMkrHOGXTo7oznthNuA,60
 etiket_client/testing/test_live_sync.py,sha256=kCL1O7Qs_gevWZOQyzpXhKGLIOxJEp21WeZ9NGX8Mh0,1174
 etiket_client/testing/live_data_generation/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 etiket_client/testing/live_data_generation/core_tools_LD.py,sha256=RnL5ggi0a03AkHRhsUGLigyumWQBP_R-bv9pHOf4358,4907
 etiket_client/testing/live_data_generation/parameters.py,sha256=LPntIRK9Z2CqWmbcGdb-4AbQxhov8driBEOwBqky1uc,12577
 etiket_client/testing/live_data_generation/qcodes_LD.py,sha256=xaQlgRmKX4Ouf8pZxkRLpXecoccKzGLMTCb9DIRY8J0,4766
-etiket_client/testing/live_data_generation/qdrive_LD.py,sha256=0REzvqa1OVsuDk5Z-CX0AZgtzS_0f02SaKT8ASW4gXU,6552
-etiket_client-0.2.11.dist-info/LICENCE,sha256=BNXChWe7aLPGR3QkVckAkeLqK2cI2idnmInh-A3YOO0,16695
-etiket_client-0.2.11.dist-info/METADATA,sha256=THIxKRRnxhFIiiq21D3giFToub7MtR3GyFSEJfS0_40,1065
-etiket_client-0.2.11.dist-info/WHEEL,sha256=oiQVh_5PnQM0E3gPdiz09WCNmwiHDMaGer_elqB3coM,92
-etiket_client-0.2.11.dist-info/top_level.txt,sha256=sQ0oXCO6SoO7HAFfJLohl1RDPxH9VbkQaa1Rwnd0kIc,14
-etiket_client-0.2.11.dist-info/zip-safe,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
-etiket_client-0.2.11.dist-info/RECORD,,
+etiket_client/testing/live_data_generation/qdrive_LD.py,sha256=OM3txBYXllVFGjeSnqb0yp17lmy0T_2RsbncUyVayKY,6594
+etiket_client-0.2.12.dist-info/LICENCE,sha256=BNXChWe7aLPGR3QkVckAkeLqK2cI2idnmInh-A3YOO0,16695
+etiket_client-0.2.12.dist-info/METADATA,sha256=7Pt2qGPxlNjeaZLiOndUvNNWTSsSr3qSJZ6mMQVdw1g,1101
+etiket_client-0.2.12.dist-info/WHEEL,sha256=oiQVh_5PnQM0E3gPdiz09WCNmwiHDMaGer_elqB3coM,92
+etiket_client-0.2.12.dist-info/top_level.txt,sha256=sQ0oXCO6SoO7HAFfJLohl1RDPxH9VbkQaa1Rwnd0kIc,14
+etiket_client-0.2.12.dist-info/zip-safe,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
+etiket_client-0.2.12.dist-info/RECORD,,
```

