# Comparing `tmp/ixdat-0.2.9.dev2.tar.gz` & `tmp/ixdat-0.2.9.dev3.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "ixdat-0.2.9.dev2.tar", last modified: Thu Feb  8 10:59:43 2024, max compression
+gzip compressed data, was "ixdat-0.2.9.dev3.tar", last modified: Sun Feb 18 22:07:21 2024, max compression
```

## Comparing `ixdat-0.2.9.dev2.tar` & `ixdat-0.2.9.dev3.tar`

### file list

```diff
@@ -1,81 +1,82 @@
-drwxrwxr-x   0 scott     (1000) scott     (1000)        0 2024-02-08 10:59:43.363828 ixdat-0.2.9.dev2/
--rw-rw-r--   0 scott     (1000) scott     (1000)     1062 2023-11-06 13:44:18.000000 ixdat-0.2.9.dev2/LICENSE.txt
--rw-rw-r--   0 scott     (1000) scott     (1000)     5749 2024-02-08 10:59:43.363828 ixdat-0.2.9.dev2/PKG-INFO
--rw-rw-r--   0 scott     (1000) scott     (1000)     5270 2024-01-31 16:25:04.000000 ixdat-0.2.9.dev2/README.rst
--rw-rw-r--   0 scott     (1000) scott     (1000)       30 2023-11-06 13:44:18.000000 ixdat-0.2.9.dev2/pyproject.toml
--rw-rw-r--   0 scott     (1000) scott     (1000)      158 2024-02-08 10:59:43.363828 ixdat-0.2.9.dev2/setup.cfg
--rw-rw-r--   0 scott     (1000) scott     (1000)     1992 2023-11-06 13:44:18.000000 ixdat-0.2.9.dev2/setup.py
-drwxrwxr-x   0 scott     (1000) scott     (1000)        0 2024-02-08 10:59:43.355828 ixdat-0.2.9.dev2/src/
-drwxrwxr-x   0 scott     (1000) scott     (1000)        0 2024-02-08 10:59:43.359828 ixdat-0.2.9.dev2/src/ixdat/
--rw-rw-r--   0 scott     (1000) scott     (1000)      678 2024-02-08 10:37:21.000000 ixdat-0.2.9.dev2/src/ixdat/__init__.py
-drwxrwxr-x   0 scott     (1000) scott     (1000)        0 2024-02-08 10:59:43.359828 ixdat-0.2.9.dev2/src/ixdat/backends/
--rw-rw-r--   0 scott     (1000) scott     (1000)      931 2023-11-06 13:44:18.000000 ixdat-0.2.9.dev2/src/ixdat/backends/__init__.py
--rw-rw-r--   0 scott     (1000) scott     (1000)     2304 2023-11-06 13:44:18.000000 ixdat-0.2.9.dev2/src/ixdat/backends/backend_base.py
--rw-rw-r--   0 scott     (1000) scott     (1000)    10205 2023-11-06 13:44:18.000000 ixdat-0.2.9.dev2/src/ixdat/backends/directory_backend.py
--rw-rw-r--   0 scott     (1000) scott     (1000)     1546 2023-11-06 13:44:18.000000 ixdat-0.2.9.dev2/src/ixdat/backends/memory_backend.py
--rw-rw-r--   0 scott     (1000) scott     (1000)     8874 2024-01-31 16:25:04.000000 ixdat-0.2.9.dev2/src/ixdat/config.py
--rw-rw-r--   0 scott     (1000) scott     (1000)     4621 2023-11-06 13:44:18.000000 ixdat-0.2.9.dev2/src/ixdat/constants.py
--rw-rw-r--   0 scott     (1000) scott     (1000)    15755 2024-01-31 16:25:04.000000 ixdat-0.2.9.dev2/src/ixdat/data_series.py
--rw-rw-r--   0 scott     (1000) scott     (1000)    22036 2024-02-04 14:08:08.000000 ixdat-0.2.9.dev2/src/ixdat/db.py
--rw-rw-r--   0 scott     (1000) scott     (1000)      968 2023-11-06 13:44:18.000000 ixdat-0.2.9.dev2/src/ixdat/exceptions.py
-drwxrwxr-x   0 scott     (1000) scott     (1000)        0 2024-02-08 10:59:43.359828 ixdat-0.2.9.dev2/src/ixdat/exporters/
--rw-rw-r--   0 scott     (1000) scott     (1000)      186 2024-02-08 08:20:40.000000 ixdat-0.2.9.dev2/src/ixdat/exporters/__init__.py
--rw-rw-r--   0 scott     (1000) scott     (1000)     8417 2024-02-08 08:20:40.000000 ixdat-0.2.9.dev2/src/ixdat/exporters/csv_exporter.py
--rw-rw-r--   0 scott     (1000) scott     (1000)     1935 2023-11-06 13:44:18.000000 ixdat-0.2.9.dev2/src/ixdat/exporters/ec_exporter.py
--rw-rw-r--   0 scott     (1000) scott     (1000)     2653 2023-11-06 13:44:18.000000 ixdat-0.2.9.dev2/src/ixdat/exporters/ecms_exporter.py
--rw-rw-r--   0 scott     (1000) scott     (1000)     1652 2024-02-08 08:20:40.000000 ixdat-0.2.9.dev2/src/ixdat/exporters/ms_exporter.py
--rw-rw-r--   0 scott     (1000) scott     (1000)     2859 2024-02-08 08:20:40.000000 ixdat-0.2.9.dev2/src/ixdat/exporters/sec_exporter.py
--rw-rw-r--   0 scott     (1000) scott     (1000)     5990 2024-02-08 08:20:40.000000 ixdat-0.2.9.dev2/src/ixdat/exporters/spectrum_exporter.py
--rw-rw-r--   0 scott     (1000) scott     (1000)    67940 2024-02-04 12:15:14.000000 ixdat-0.2.9.dev2/src/ixdat/measurements.py
-drwxrwxr-x   0 scott     (1000) scott     (1000)        0 2024-02-08 10:59:43.359828 ixdat-0.2.9.dev2/src/ixdat/plotters/
--rw-rw-r--   0 scott     (1000) scott     (1000)       74 2023-11-06 13:44:18.000000 ixdat-0.2.9.dev2/src/ixdat/plotters/__init__.py
--rw-rw-r--   0 scott     (1000) scott     (1000)     6984 2024-02-08 10:40:05.000000 ixdat-0.2.9.dev2/src/ixdat/plotters/base_mpl_plotter.py
--rw-rw-r--   0 scott     (1000) scott     (1000)     9743 2024-02-08 10:35:47.000000 ixdat-0.2.9.dev2/src/ixdat/plotters/ec_plotter.py
--rw-rw-r--   0 scott     (1000) scott     (1000)    13560 2023-11-06 13:44:18.000000 ixdat-0.2.9.dev2/src/ixdat/plotters/ecms_plotter.py
--rw-rw-r--   0 scott     (1000) scott     (1000)    39912 2024-02-04 11:32:26.000000 ixdat-0.2.9.dev2/src/ixdat/plotters/ms_plotter.py
--rw-rw-r--   0 scott     (1000) scott     (1000)     3483 2023-11-06 13:44:18.000000 ixdat-0.2.9.dev2/src/ixdat/plotters/plotting_tools.py
--rw-rw-r--   0 scott     (1000) scott     (1000)    17992 2024-01-31 16:25:04.000000 ixdat-0.2.9.dev2/src/ixdat/plotters/sec_plotter.py
--rw-rw-r--   0 scott     (1000) scott     (1000)    13333 2024-01-31 16:25:04.000000 ixdat-0.2.9.dev2/src/ixdat/plotters/spectrum_plotter.py
--rw-rw-r--   0 scott     (1000) scott     (1000)    46270 2023-11-06 13:44:18.000000 ixdat-0.2.9.dev2/src/ixdat/plotters/tpms_plotter.py
--rw-rw-r--   0 scott     (1000) scott     (1000)     1798 2023-11-06 13:44:18.000000 ixdat-0.2.9.dev2/src/ixdat/plotters/value_plotter.py
-drwxrwxr-x   0 scott     (1000) scott     (1000)        0 2024-02-08 10:59:43.359828 ixdat-0.2.9.dev2/src/ixdat/projects/
--rw-rw-r--   0 scott     (1000) scott     (1000)        0 2023-11-06 13:44:18.000000 ixdat-0.2.9.dev2/src/ixdat/projects/__init__.py
--rw-rw-r--   0 scott     (1000) scott     (1000)      561 2023-11-06 13:44:18.000000 ixdat-0.2.9.dev2/src/ixdat/projects/lablogs.py
--rw-rw-r--   0 scott     (1000) scott     (1000)      483 2023-11-06 13:44:18.000000 ixdat-0.2.9.dev2/src/ixdat/projects/samples.py
-drwxrwxr-x   0 scott     (1000) scott     (1000)        0 2024-02-08 10:59:43.363828 ixdat-0.2.9.dev2/src/ixdat/readers/
--rw-rw-r--   0 scott     (1000) scott     (1000)     1872 2024-01-31 16:25:04.000000 ixdat-0.2.9.dev2/src/ixdat/readers/__init__.py
--rw-rw-r--   0 scott     (1000) scott     (1000)     2687 2023-11-06 13:44:18.000000 ixdat-0.2.9.dev2/src/ixdat/readers/autolab.py
--rw-rw-r--   0 scott     (1000) scott     (1000)     5145 2023-11-06 13:44:18.000000 ixdat-0.2.9.dev2/src/ixdat/readers/avantage.py
--rw-rw-r--   0 scott     (1000) scott     (1000)    22963 2024-01-31 16:25:04.000000 ixdat-0.2.9.dev2/src/ixdat/readers/biologic.py
--rw-rw-r--   0 scott     (1000) scott     (1000)     1603 2023-11-06 13:44:18.000000 ixdat-0.2.9.dev2/src/ixdat/readers/chi.py
--rw-rw-r--   0 scott     (1000) scott     (1000)     9784 2023-11-06 13:44:18.000000 ixdat-0.2.9.dev2/src/ixdat/readers/cinfdata.py
--rw-rw-r--   0 scott     (1000) scott     (1000)    16026 2023-11-06 13:44:18.000000 ixdat-0.2.9.dev2/src/ixdat/readers/cinfdata_db.py
--rw-rw-r--   0 scott     (1000) scott     (1000)     4033 2024-01-31 16:25:04.000000 ixdat-0.2.9.dev2/src/ixdat/readers/ec_ms_pkl.py
--rw-rw-r--   0 scott     (1000) scott     (1000)     6008 2023-11-06 13:44:18.000000 ixdat-0.2.9.dev2/src/ixdat/readers/ivium.py
--rw-rw-r--   0 scott     (1000) scott     (1000)    15536 2024-02-08 08:20:40.000000 ixdat-0.2.9.dev2/src/ixdat/readers/ixdat_csv.py
--rw-rw-r--   0 scott     (1000) scott     (1000)    11371 2024-02-08 08:20:40.000000 ixdat-0.2.9.dev2/src/ixdat/readers/msrh_sec.py
--rw-rw-r--   0 scott     (1000) scott     (1000)     3198 2023-11-06 13:44:18.000000 ixdat-0.2.9.dev2/src/ixdat/readers/pfeiffer.py
--rw-rw-r--   0 scott     (1000) scott     (1000)     3970 2023-11-06 13:44:18.000000 ixdat-0.2.9.dev2/src/ixdat/readers/qexafs.py
--rw-rw-r--   0 scott     (1000) scott     (1000)     7457 2024-01-31 16:25:04.000000 ixdat-0.2.9.dev2/src/ixdat/readers/reading_tools.py
--rw-rw-r--   0 scott     (1000) scott     (1000)     1587 2023-11-06 13:44:18.000000 ixdat-0.2.9.dev2/src/ixdat/readers/rgasoft.py
--rw-rw-r--   0 scott     (1000) scott     (1000)     2016 2023-11-06 13:44:18.000000 ixdat-0.2.9.dev2/src/ixdat/readers/xrdml.py
--rw-rw-r--   0 scott     (1000) scott     (1000)    32357 2024-02-08 08:20:40.000000 ixdat-0.2.9.dev2/src/ixdat/readers/zilien.py
--rw-rw-r--   0 scott     (1000) scott     (1000)    35693 2024-02-08 08:20:40.000000 ixdat-0.2.9.dev2/src/ixdat/spectra.py
-drwxrwxr-x   0 scott     (1000) scott     (1000)        0 2024-02-08 10:59:43.363828 ixdat-0.2.9.dev2/src/ixdat/techniques/
--rw-rw-r--   0 scott     (1000) scott     (1000)     1806 2024-02-08 08:20:40.000000 ixdat-0.2.9.dev2/src/ixdat/techniques/__init__.py
--rw-rw-r--   0 scott     (1000) scott     (1000)     7430 2023-11-06 13:44:18.000000 ixdat-0.2.9.dev2/src/ixdat/techniques/analysis_tools.py
--rw-rw-r--   0 scott     (1000) scott     (1000)    15956 2024-01-31 16:25:04.000000 ixdat-0.2.9.dev2/src/ixdat/techniques/cv.py
--rw-rw-r--   0 scott     (1000) scott     (1000)     9262 2023-11-06 13:44:18.000000 ixdat-0.2.9.dev2/src/ixdat/techniques/deconvolution.py
--rw-rw-r--   0 scott     (1000) scott     (1000)    15432 2023-11-06 13:44:18.000000 ixdat-0.2.9.dev2/src/ixdat/techniques/ec.py
--rw-rw-r--   0 scott     (1000) scott     (1000)    19028 2024-02-08 08:20:40.000000 ixdat-0.2.9.dev2/src/ixdat/techniques/ec_ms.py
--rw-rw-r--   0 scott     (1000) scott     (1000)    56401 2024-02-08 08:20:40.000000 ixdat-0.2.9.dev2/src/ixdat/techniques/ms.py
--rw-rw-r--   0 scott     (1000) scott     (1000)    11427 2024-02-04 11:32:05.000000 ixdat-0.2.9.dev2/src/ixdat/techniques/reactor.py
--rw-rw-r--   0 scott     (1000) scott     (1000)    12910 2024-01-31 16:25:00.000000 ixdat-0.2.9.dev2/src/ixdat/techniques/spectroelectrochemistry.py
--rw-rw-r--   0 scott     (1000) scott     (1000)    11008 2024-01-31 16:25:04.000000 ixdat-0.2.9.dev2/src/ixdat/tools.py
--rw-rw-r--   0 scott     (1000) scott     (1000)      461 2023-11-06 13:44:18.000000 ixdat-0.2.9.dev2/src/ixdat/units.py
-drwxrwxr-x   0 scott     (1000) scott     (1000)        0 2024-02-08 10:59:43.359828 ixdat-0.2.9.dev2/src/ixdat.egg-info/
--rw-r--r--   0 scott     (1000) scott     (1000)     5749 2024-02-08 10:59:43.000000 ixdat-0.2.9.dev2/src/ixdat.egg-info/PKG-INFO
--rw-rw-r--   0 scott     (1000) scott     (1000)     2077 2024-02-08 10:59:43.000000 ixdat-0.2.9.dev2/src/ixdat.egg-info/SOURCES.txt
--rw-rw-r--   0 scott     (1000) scott     (1000)        1 2024-02-08 10:59:43.000000 ixdat-0.2.9.dev2/src/ixdat.egg-info/dependency_links.txt
--rw-rw-r--   0 scott     (1000) scott     (1000)       59 2024-02-08 10:59:43.000000 ixdat-0.2.9.dev2/src/ixdat.egg-info/requires.txt
--rw-rw-r--   0 scott     (1000) scott     (1000)        6 2024-02-08 10:59:43.000000 ixdat-0.2.9.dev2/src/ixdat.egg-info/top_level.txt
+drwxrwxrwx   0        0        0        0 2024-02-18 22:07:21.711961 ixdat-0.2.9.dev3/
+-rw-rw-rw-   0        0        0     1083 2023-11-10 11:45:00.000000 ixdat-0.2.9.dev3/LICENSE.txt
+-rw-rw-rw-   0        0        0     5917 2024-02-18 22:07:21.711961 ixdat-0.2.9.dev3/PKG-INFO
+-rw-rw-rw-   0        0        0     5409 2024-02-05 20:16:49.000000 ixdat-0.2.9.dev3/README.rst
+-rw-rw-rw-   0        0        0       32 2023-11-10 11:45:00.000000 ixdat-0.2.9.dev3/pyproject.toml
+-rw-rw-rw-   0        0        0      171 2024-02-18 22:07:21.711961 ixdat-0.2.9.dev3/setup.cfg
+-rw-rw-rw-   0        0        0     2063 2023-11-10 11:45:00.000000 ixdat-0.2.9.dev3/setup.py
+drwxrwxrwx   0        0        0        0 2024-02-18 22:07:21.622126 ixdat-0.2.9.dev3/src/
+drwxrwxrwx   0        0        0        0 2024-02-18 22:07:21.641750 ixdat-0.2.9.dev3/src/ixdat/
+-rw-rw-rw-   0        0        0      699 2024-02-18 22:03:19.000000 ixdat-0.2.9.dev3/src/ixdat/__init__.py
+drwxrwxrwx   0        0        0        0 2024-02-18 22:07:21.661769 ixdat-0.2.9.dev3/src/ixdat/backends/
+-rw-rw-rw-   0        0        0      955 2023-11-10 11:45:00.000000 ixdat-0.2.9.dev3/src/ixdat/backends/__init__.py
+-rw-rw-rw-   0        0        0     2361 2023-11-10 11:45:00.000000 ixdat-0.2.9.dev3/src/ixdat/backends/backend_base.py
+-rw-rw-rw-   0        0        0    10450 2023-11-10 11:45:00.000000 ixdat-0.2.9.dev3/src/ixdat/backends/directory_backend.py
+-rw-rw-rw-   0        0        0     1588 2023-11-10 11:45:00.000000 ixdat-0.2.9.dev3/src/ixdat/backends/memory_backend.py
+-rw-rw-rw-   0        0        0     9126 2024-02-05 20:16:49.000000 ixdat-0.2.9.dev3/src/ixdat/config.py
+-rw-rw-rw-   0        0        0     4783 2023-11-10 11:45:00.000000 ixdat-0.2.9.dev3/src/ixdat/constants.py
+-rw-rw-rw-   0        0        0    16183 2024-02-05 20:16:49.000000 ixdat-0.2.9.dev3/src/ixdat/data_series.py
+-rw-rw-rw-   0        0        0    22526 2024-02-05 20:19:35.000000 ixdat-0.2.9.dev3/src/ixdat/db.py
+-rw-rw-rw-   0        0        0     1005 2023-11-10 11:45:00.000000 ixdat-0.2.9.dev3/src/ixdat/exceptions.py
+drwxrwxrwx   0        0        0        0 2024-02-18 22:07:21.671655 ixdat-0.2.9.dev3/src/ixdat/exporters/
+-rw-rw-rw-   0        0        0      190 2024-02-15 14:34:48.000000 ixdat-0.2.9.dev3/src/ixdat/exporters/__init__.py
+-rw-rw-rw-   0        0        0     8615 2024-02-15 14:34:48.000000 ixdat-0.2.9.dev3/src/ixdat/exporters/csv_exporter.py
+-rw-rw-rw-   0        0        0     1978 2023-12-21 10:41:58.000000 ixdat-0.2.9.dev3/src/ixdat/exporters/ec_exporter.py
+-rw-rw-rw-   0        0        0     2719 2023-12-21 10:41:58.000000 ixdat-0.2.9.dev3/src/ixdat/exporters/ecms_exporter.py
+-rw-rw-rw-   0        0        0     1696 2024-02-15 14:34:48.000000 ixdat-0.2.9.dev3/src/ixdat/exporters/ms_exporter.py
+-rw-rw-rw-   0        0        0     2929 2024-02-15 14:34:48.000000 ixdat-0.2.9.dev3/src/ixdat/exporters/sec_exporter.py
+-rw-rw-rw-   0        0        0     6129 2024-02-15 14:34:48.000000 ixdat-0.2.9.dev3/src/ixdat/exporters/spectrum_exporter.py
+-rw-rw-rw-   0        0        0    69429 2024-02-05 21:43:44.000000 ixdat-0.2.9.dev3/src/ixdat/measurements.py
+drwxrwxrwx   0        0        0        0 2024-02-18 22:07:21.681841 ixdat-0.2.9.dev3/src/ixdat/plotters/
+-rw-rw-rw-   0        0        0       76 2023-11-10 11:45:00.000000 ixdat-0.2.9.dev3/src/ixdat/plotters/__init__.py
+-rw-rw-rw-   0        0        0     7177 2024-02-18 20:09:14.000000 ixdat-0.2.9.dev3/src/ixdat/plotters/base_mpl_plotter.py
+-rw-rw-rw-   0        0        0     9989 2024-02-18 20:09:14.000000 ixdat-0.2.9.dev3/src/ixdat/plotters/ec_plotter.py
+-rw-rw-rw-   0        0        0    13865 2023-11-10 11:45:00.000000 ixdat-0.2.9.dev3/src/ixdat/plotters/ecms_plotter.py
+-rw-rw-rw-   0        0        0    40918 2024-02-05 20:19:35.000000 ixdat-0.2.9.dev3/src/ixdat/plotters/ms_plotter.py
+-rw-rw-rw-   0        0        0     3579 2023-11-10 11:45:00.000000 ixdat-0.2.9.dev3/src/ixdat/plotters/plotting_tools.py
+-rw-rw-rw-   0        0        0    18433 2024-02-05 20:16:49.000000 ixdat-0.2.9.dev3/src/ixdat/plotters/sec_plotter.py
+-rw-rw-rw-   0        0        0    13693 2024-02-05 20:16:49.000000 ixdat-0.2.9.dev3/src/ixdat/plotters/spectrum_plotter.py
+-rw-rw-rw-   0        0        0    47396 2023-11-10 11:45:00.000000 ixdat-0.2.9.dev3/src/ixdat/plotters/tpms_plotter.py
+-rw-rw-rw-   0        0        0     1853 2023-11-10 11:45:00.000000 ixdat-0.2.9.dev3/src/ixdat/plotters/value_plotter.py
+drwxrwxrwx   0        0        0        0 2024-02-18 22:07:21.681841 ixdat-0.2.9.dev3/src/ixdat/projects/
+-rw-rw-rw-   0        0        0        0 2023-11-10 11:45:00.000000 ixdat-0.2.9.dev3/src/ixdat/projects/__init__.py
+-rw-rw-rw-   0        0        0      581 2023-11-10 11:45:00.000000 ixdat-0.2.9.dev3/src/ixdat/projects/lablogs.py
+-rw-rw-rw-   0        0        0      503 2023-11-10 11:45:00.000000 ixdat-0.2.9.dev3/src/ixdat/projects/samples.py
+drwxrwxrwx   0        0        0        0 2024-02-18 22:07:21.701722 ixdat-0.2.9.dev3/src/ixdat/readers/
+-rw-rw-rw-   0        0        0     2010 2024-02-18 20:09:31.000000 ixdat-0.2.9.dev3/src/ixdat/readers/__init__.py
+-rw-rw-rw-   0        0        0     2770 2023-11-10 11:45:00.000000 ixdat-0.2.9.dev3/src/ixdat/readers/autolab.py
+-rw-rw-rw-   0        0        0     5274 2023-11-10 11:45:00.000000 ixdat-0.2.9.dev3/src/ixdat/readers/avantage.py
+-rw-rw-rw-   0        0        0    23533 2024-02-05 20:16:49.000000 ixdat-0.2.9.dev3/src/ixdat/readers/biologic.py
+-rw-rw-rw-   0        0        0     1650 2023-11-10 11:45:00.000000 ixdat-0.2.9.dev3/src/ixdat/readers/chi.py
+-rw-rw-rw-   0        0        0     9995 2023-11-10 11:45:00.000000 ixdat-0.2.9.dev3/src/ixdat/readers/cinfdata.py
+-rw-rw-rw-   0        0        0    16430 2023-12-21 10:41:58.000000 ixdat-0.2.9.dev3/src/ixdat/readers/cinfdata_db.py
+-rw-rw-rw-   0        0        0     4156 2024-02-05 20:16:49.000000 ixdat-0.2.9.dev3/src/ixdat/readers/ec_ms_pkl.py
+-rw-rw-rw-   0        0        0     6150 2023-11-10 11:45:00.000000 ixdat-0.2.9.dev3/src/ixdat/readers/ivium.py
+-rw-rw-rw-   0        0        0    15897 2024-02-15 14:34:48.000000 ixdat-0.2.9.dev3/src/ixdat/readers/ixdat_csv.py
+-rw-rw-rw-   0        0        0    11629 2024-02-15 14:34:48.000000 ixdat-0.2.9.dev3/src/ixdat/readers/msrh_sec.py
+-rw-rw-rw-   0        0        0     2722 2024-02-18 22:05:33.000000 ixdat-0.2.9.dev3/src/ixdat/readers/nordic.py
+-rw-rw-rw-   0        0        0     3277 2023-11-10 11:45:00.000000 ixdat-0.2.9.dev3/src/ixdat/readers/pfeiffer.py
+-rw-rw-rw-   0        0        0     4065 2023-11-10 11:45:00.000000 ixdat-0.2.9.dev3/src/ixdat/readers/qexafs.py
+-rw-rw-rw-   0        0        0     7641 2024-02-05 20:16:49.000000 ixdat-0.2.9.dev3/src/ixdat/readers/reading_tools.py
+-rw-rw-rw-   0        0        0     1630 2023-11-10 11:45:00.000000 ixdat-0.2.9.dev3/src/ixdat/readers/rgasoft.py
+-rw-rw-rw-   0        0        0     2064 2023-12-21 10:41:58.000000 ixdat-0.2.9.dev3/src/ixdat/readers/xrdml.py
+-rw-rw-rw-   0        0        0    33178 2024-02-15 14:34:48.000000 ixdat-0.2.9.dev3/src/ixdat/readers/zilien.py
+-rw-rw-rw-   0        0        0    36587 2024-02-15 14:34:48.000000 ixdat-0.2.9.dev3/src/ixdat/spectra.py
+drwxrwxrwx   0        0        0        0 2024-02-18 22:07:21.711961 ixdat-0.2.9.dev3/src/ixdat/techniques/
+-rw-rw-rw-   0        0        0     1858 2024-02-05 21:15:22.000000 ixdat-0.2.9.dev3/src/ixdat/techniques/__init__.py
+-rw-rw-rw-   0        0        0     7629 2023-11-10 11:45:00.000000 ixdat-0.2.9.dev3/src/ixdat/techniques/analysis_tools.py
+-rw-rw-rw-   0        0        0    16335 2024-02-05 20:16:49.000000 ixdat-0.2.9.dev3/src/ixdat/techniques/cv.py
+-rw-rw-rw-   0        0        0     9511 2023-11-10 11:45:00.000000 ixdat-0.2.9.dev3/src/ixdat/techniques/deconvolution.py
+-rw-rw-rw-   0        0        0    15823 2024-02-05 21:14:22.000000 ixdat-0.2.9.dev3/src/ixdat/techniques/ec.py
+-rw-rw-rw-   0        0        0    19503 2024-02-05 21:38:18.000000 ixdat-0.2.9.dev3/src/ixdat/techniques/ec_ms.py
+-rw-rw-rw-   0        0        0    57810 2024-02-15 14:34:48.000000 ixdat-0.2.9.dev3/src/ixdat/techniques/ms.py
+-rw-rw-rw-   0        0        0    11749 2024-02-05 20:19:35.000000 ixdat-0.2.9.dev3/src/ixdat/techniques/reactor.py
+-rw-rw-rw-   0        0        0    13209 2024-01-31 08:13:08.000000 ixdat-0.2.9.dev3/src/ixdat/techniques/spectroelectrochemistry.py
+-rw-rw-rw-   0        0        0    11330 2024-02-05 20:16:49.000000 ixdat-0.2.9.dev3/src/ixdat/tools.py
+-rw-rw-rw-   0        0        0      480 2024-01-05 10:26:08.000000 ixdat-0.2.9.dev3/src/ixdat/units.py
+drwxrwxrwx   0        0        0        0 2024-02-18 22:07:21.661769 ixdat-0.2.9.dev3/src/ixdat.egg-info/
+-rw-rw-rw-   0        0        0     5917 2024-02-18 22:07:21.000000 ixdat-0.2.9.dev3/src/ixdat.egg-info/PKG-INFO
+-rw-rw-rw-   0        0        0     2105 2024-02-18 22:07:21.000000 ixdat-0.2.9.dev3/src/ixdat.egg-info/SOURCES.txt
+-rw-rw-rw-   0        0        0        1 2024-02-18 22:07:21.000000 ixdat-0.2.9.dev3/src/ixdat.egg-info/dependency_links.txt
+-rw-rw-rw-   0        0        0       59 2024-02-18 22:07:21.000000 ixdat-0.2.9.dev3/src/ixdat.egg-info/requires.txt
+-rw-rw-rw-   0        0        0        6 2024-02-18 22:07:21.000000 ixdat-0.2.9.dev3/src/ixdat.egg-info/top_level.txt
```

### Comparing `ixdat-0.2.9.dev2/PKG-INFO` & `ixdat-0.2.9.dev3/PKG-INFO`

 * *Files 20% similar despite different names*

```diff
@@ -1,154 +1,154 @@
-Metadata-Version: 2.1
-Name: ixdat
-Version: 0.2.9.dev2
-Summary: The in-situ experimental data tool
-Home-page: https://github.com/ixdat/ixdat
-Author: Soren B. Scott, Kenneth Nielsen, Anna Winiwarter, et al
-Author-email: sbs@chem.ku.dk
-License: MIT
-Classifier: Programming Language :: Python :: 3
-Classifier: License :: OSI Approved :: MIT License
-Classifier: Operating System :: OS Independent
-Requires-Python: >=3.6
-Description-Content-Type: text/x-rst
-License-File: LICENSE.txt
-
-.. figure:: docs/source/figures/logo.svg
-    :width: 200
-
-=============================================
-``ixdat``: The In-situ Experimental Data Tool
-=============================================
-
-With ``ixdat``, you can import, combine, and export complex experimental datasets
-as simply as::
-
-    ec = Measurement.read_set("awesome_EC_data", reader="biologic")
-    ec.plot_measurement()
-
-    ms = Measurement.read("2021-03-30 16_59_35 MS data.tsv", reader="zilien")
-    ms.plot_measurement()
-
-    ecms = ec + ms
-    ecms.plot_measurement()
-
-    ecms.export("my_combined_data.csv")
-
-Output:
-
-.. figure:: docs/source/figures/ixdat_example_figures.png
-    :width: 700
-
-    In-situ experimental data made easy
-
-Or rather than exporting, you can take advantage of ``ixdat``'s powerful analysis
-tools and database backends to be a one-stop tool from messy raw data to public
-repository accompanying your breakthrough publication and advancing our field.
-
-Version
--------
-This is the latest version.
-
-For changes up to this version, see `CHANGES.rst <https://github.com/ixdat/ixdat/blob/main/CHANGES.rst>`_
-
-For ixdat 0.1.9 see the `v0.1.x branch <https://github.com/ixdat/ixdat/tree/v0.1.x>`_.
-
-About
------
-
-``ixdat`` provides a powerful **object-oriented** interface to experimental data, especially in-situ experimental data for which it is of interest to combine data obtained simultaneously from multiple techniques.
-
-Documentation is at https://ixdat.readthedocs.io
-
-In addition to a **pluggable** parser interface for importing your data format, ``ixdat`` also includes
-pluggable exporters and plotters, as well as a database interface. A relational model of experimental data is
-designed into every level.
-
-.. list-table:: Techniques and Readers
-   :widths: 20 15 50
-   :header-rows: 1
-
-
-   * - Measurement technique
-     - Status
-     - Readers
-   * - Electrochemistry
-     - Released
-     - - biologic: .mpt files from Biologic's EC-Lab software
-       - autolab: ascii files from AutoLab's NOVA software
-       - ivium: .txt files from Ivium's IviumSoft software
-   * - Mass Spectrometry
-     - Released
-     - - pfeiffer: .dat files from Pfeiffer Vacuum's PVMassSpec software
-       - cinfdata: text export from DTU Physics' cinfdata system
-       - zilien: .tsv files from Spectro Inlets' Zilien software
-   * - Electrochemistry - Mass Spectrometry
-     - Released
-     - - zilien: .tsv files from Spectro Inlets' Zilien software
-       - EC_MS: .pkl files from the legacy EC_MS python package
-   * - Spectroelectrochemistry
-     - Released
-     - - msrh_sec: .csv file sets from Imperial College London's SEC system
-   * - X-ray photoelectron spectroscopy (XPS)
-     - Development
-     - - avantage: .avg files from Thermo Scientific's Avantage software
-   * - X-ray diffraction (XRD)
-     - Development
-     - - xrdml: .xrdml files from e.g. PanAnalytical's Empyereon
-   * - In-situ Electrochemistry - X-ray adsorption spectroscopy
-     - Development
-     - - qexafs: .dat files from Diamond's B18 beamline
-   * - Low-Energy Ion Scattering (LEIS)
-     - Future
-     -
-
-Tutorials are provided at https://ixdat.readthedocs.io/en/latest/tutorials/index.html
-
-Installation
-------------
-
-To use ``ixdat``, you need to have python installed. We recommend
-`Anaconda python <https://www.anaconda.com/products/individual>`_.
-
-To install ``ixdat``, just type in your terminal or Anaconda prompt::
-
-    $ pip install ixdat
-
-And hit enter.
-
-``ixdat`` is under development, and to make use of the newest features,
-you may need to upgrade to the latest version. This is also easy. Just type::
-
-    $ pip install --upgrade ixdat
-
-
-Article repositories
---------------------
-
-``ixdat`` is shown in practice in a growing number of open repositories of data and analysis
-for academic publications:
-
-- Soren B. Scott, et al.  **Tracking oxygen atoms in electrochemical CO oxidation –Part I: Oxygen exchange via CO2 hydration**. `Electrochimica Acta, 374, 137842 <https://doi.org/10.1016/j.electacta.2021.137842>`_, **2021**.
-
-  Repository: https://github.com/ScottSoren/pyCOox_public
-
-- Soren B. Scott, et al.  **Tracking oxygen atoms in electrochemical CO oxidation –Part II: Lattice oxygen reactivity in oxides of Pt and Ir**. `Electrochimica Acta, 374, 137844 <https://doi.org/10.1016/j.electacta.2021.137844>`_, **2021**.
-
-  Repository: https://github.com/ScottSoren/pyCOox_public
-
-- Kevin Krempl, et al. **Dynamic Interfacial Reaction Rates from Electrochemistry - Mass Spectrometry**. `Journal of Analytical Chemistry. 93, 7022-7028 <https://doi.org/10.1021/acs.analchem.1c00110>`_, **2021**
-
-  Repository: https://github.com/kkrempl/Dynamic-Interfacial-Reaction-Rates
-
-- Junheng Huang, et al. **Online Electrochemistry−Mass Spectrometry Evaluation of the Acidic Oxygen Evolution Reaction at Supported Catalysts**. `ACS Catal. 11, 12745-12753 <https://doi.org/10.1021/acscatal.1c03430>`_, **2021**
-
-  Repository: https://github.com/ScottSoren/Huang2021
-
-
-Join us
--------
-
-``ixdat`` is free and open source software and we welcome input and new collaborators. Please help us improve ``ixdat``!
-
-Contact us (https://github.com/ixdat/ixdat/discussions or sbscott@ic.ac.uk) or just
-`get started developing <https://ixdat.readthedocs.io/en/latest/developing/index.html>`_.
+Metadata-Version: 2.1
+Name: ixdat
+Version: 0.2.9.dev3
+Summary: The in-situ experimental data tool
+Home-page: https://github.com/ixdat/ixdat
+Author: Soren B. Scott, Kenneth Nielsen, Anna Winiwarter, et al
+Author-email: sbs@chem.ku.dk
+License: MIT
+Classifier: Programming Language :: Python :: 3
+Classifier: License :: OSI Approved :: MIT License
+Classifier: Operating System :: OS Independent
+Requires-Python: >=3.6
+Description-Content-Type: text/x-rst
+License-File: LICENSE.txt
+
+.. figure:: docs/source/figures/logo.svg
+    :width: 200
+
+=============================================
+``ixdat``: The In-situ Experimental Data Tool
+=============================================
+
+With ``ixdat``, you can import, combine, and export complex experimental datasets
+as simply as::
+
+    ec = Measurement.read_set("awesome_EC_data", reader="biologic")
+    ec.plot_measurement()
+
+    ms = Measurement.read("2021-03-30 16_59_35 MS data.tsv", reader="zilien")
+    ms.plot_measurement()
+
+    ecms = ec + ms
+    ecms.plot_measurement()
+
+    ecms.export("my_combined_data.csv")
+
+Output:
+
+.. figure:: docs/source/figures/ixdat_example_figures.png
+    :width: 700
+
+    In-situ experimental data made easy
+
+Or rather than exporting, you can take advantage of ``ixdat``'s powerful analysis
+tools and database backends to be a one-stop tool from messy raw data to public
+repository accompanying your breakthrough publication and advancing our field.
+
+Version
+-------
+This is the latest version.
+
+For changes up to this version, see `CHANGES.rst <https://github.com/ixdat/ixdat/blob/main/CHANGES.rst>`_
+
+For ixdat 0.1.9 see the `v0.1.x branch <https://github.com/ixdat/ixdat/tree/v0.1.x>`_.
+
+About
+-----
+
+``ixdat`` provides a powerful **object-oriented** interface to experimental data, especially in-situ experimental data for which it is of interest to combine data obtained simultaneously from multiple techniques.
+
+Documentation is at https://ixdat.readthedocs.io
+
+In addition to a **pluggable** parser interface for importing your data format, ``ixdat`` also includes
+pluggable exporters and plotters, as well as a database interface. A relational model of experimental data is
+designed into every level.
+
+.. list-table:: Techniques and Readers
+   :widths: 20 15 50
+   :header-rows: 1
+
+
+   * - Measurement technique
+     - Status
+     - Readers
+   * - Electrochemistry
+     - Released
+     - - biologic: .mpt files from Biologic's EC-Lab software
+       - autolab: ascii files from AutoLab's NOVA software
+       - ivium: .txt files from Ivium's IviumSoft software
+   * - Mass Spectrometry
+     - Released
+     - - pfeiffer: .dat files from Pfeiffer Vacuum's PVMassSpec software
+       - cinfdata: text export from DTU Physics' cinfdata system
+       - zilien: .tsv files from Spectro Inlets' Zilien software
+   * - Electrochemistry - Mass Spectrometry
+     - Released
+     - - zilien: .tsv files from Spectro Inlets' Zilien software
+       - EC_MS: .pkl files from the legacy EC_MS python package
+   * - Spectroelectrochemistry
+     - Released
+     - - msrh_sec: .csv file sets from Imperial College London's SEC system
+   * - X-ray photoelectron spectroscopy (XPS)
+     - Development
+     - - avantage: .avg files from Thermo Scientific's Avantage software
+   * - X-ray diffraction (XRD)
+     - Development
+     - - xrdml: .xrdml files from e.g. PanAnalytical's Empyereon
+   * - In-situ Electrochemistry - X-ray adsorption spectroscopy
+     - Development
+     - - qexafs: .dat files from Diamond's B18 beamline
+   * - Low-Energy Ion Scattering (LEIS)
+     - Future
+     -
+
+Tutorials are provided at https://ixdat.readthedocs.io/en/latest/tutorials/index.html
+
+Installation
+------------
+
+To use ``ixdat``, you need to have python installed. We recommend
+`Anaconda python <https://www.anaconda.com/products/individual>`_.
+
+To install ``ixdat``, just type in your terminal or Anaconda prompt::
+
+    $ pip install ixdat
+
+And hit enter.
+
+``ixdat`` is under development, and to make use of the newest features,
+you may need to upgrade to the latest version. This is also easy. Just type::
+
+    $ pip install --upgrade ixdat
+
+
+Article repositories
+--------------------
+
+``ixdat`` is shown in practice in a growing number of open repositories of data and analysis
+for academic publications:
+
+- Soren B. Scott, et al.  **Tracking oxygen atoms in electrochemical CO oxidation â€“Part I: Oxygen exchange via CO2 hydration**. `Electrochimica Acta, 374, 137842 <https://doi.org/10.1016/j.electacta.2021.137842>`_, **2021**.
+
+  Repository: https://github.com/ScottSoren/pyCOox_public
+
+- Soren B. Scott, et al.  **Tracking oxygen atoms in electrochemical CO oxidation â€“Part II: Lattice oxygen reactivity in oxides of Pt and Ir**. `Electrochimica Acta, 374, 137844 <https://doi.org/10.1016/j.electacta.2021.137844>`_, **2021**.
+
+  Repository: https://github.com/ScottSoren/pyCOox_public
+
+- Kevin Krempl, et al. **Dynamic Interfacial Reaction Rates from Electrochemistry - Mass Spectrometry**. `Journal of Analytical Chemistry. 93, 7022-7028 <https://doi.org/10.1021/acs.analchem.1c00110>`_, **2021**
+
+  Repository: https://github.com/kkrempl/Dynamic-Interfacial-Reaction-Rates
+
+- Junheng Huang, et al. **Online Electrochemistryâˆ’Mass Spectrometry Evaluation of the Acidic Oxygen Evolution Reaction at Supported Catalysts**. `ACS Catal. 11, 12745-12753 <https://doi.org/10.1021/acscatal.1c03430>`_, **2021**
+
+  Repository: https://github.com/ScottSoren/Huang2021
+
+
+Join us
+-------
+
+``ixdat`` is free and open source software and we welcome input and new collaborators. Please help us improve ``ixdat``!
+
+Contact us (https://github.com/ixdat/ixdat/discussions or sbscott@ic.ac.uk) or just
+`get started developing <https://ixdat.readthedocs.io/en/latest/developing/index.html>`_.
```

### Comparing `ixdat-0.2.9.dev2/README.rst` & `ixdat-0.2.9.dev3/src/ixdat.egg-info/PKG-INFO`

 * *Files 20% similar despite different names*

```diff
@@ -1,139 +1,154 @@
-.. figure:: docs/source/figures/logo.svg
-    :width: 200
-
-=============================================
-``ixdat``: The In-situ Experimental Data Tool
-=============================================
-
-With ``ixdat``, you can import, combine, and export complex experimental datasets
-as simply as::
-
-    ec = Measurement.read_set("awesome_EC_data", reader="biologic")
-    ec.plot_measurement()
-
-    ms = Measurement.read("2021-03-30 16_59_35 MS data.tsv", reader="zilien")
-    ms.plot_measurement()
-
-    ecms = ec + ms
-    ecms.plot_measurement()
-
-    ecms.export("my_combined_data.csv")
-
-Output:
-
-.. figure:: docs/source/figures/ixdat_example_figures.png
-    :width: 700
-
-    In-situ experimental data made easy
-
-Or rather than exporting, you can take advantage of ``ixdat``'s powerful analysis
-tools and database backends to be a one-stop tool from messy raw data to public
-repository accompanying your breakthrough publication and advancing our field.
-
-Version
--------
-This is the latest version.
-
-For changes up to this version, see `CHANGES.rst <https://github.com/ixdat/ixdat/blob/main/CHANGES.rst>`_
-
-For ixdat 0.1.9 see the `v0.1.x branch <https://github.com/ixdat/ixdat/tree/v0.1.x>`_.
-
-About
------
-
-``ixdat`` provides a powerful **object-oriented** interface to experimental data, especially in-situ experimental data for which it is of interest to combine data obtained simultaneously from multiple techniques.
-
-Documentation is at https://ixdat.readthedocs.io
-
-In addition to a **pluggable** parser interface for importing your data format, ``ixdat`` also includes
-pluggable exporters and plotters, as well as a database interface. A relational model of experimental data is
-designed into every level.
-
-.. list-table:: Techniques and Readers
-   :widths: 20 15 50
-   :header-rows: 1
-
-
-   * - Measurement technique
-     - Status
-     - Readers
-   * - Electrochemistry
-     - Released
-     - - biologic: .mpt files from Biologic's EC-Lab software
-       - autolab: ascii files from AutoLab's NOVA software
-       - ivium: .txt files from Ivium's IviumSoft software
-   * - Mass Spectrometry
-     - Released
-     - - pfeiffer: .dat files from Pfeiffer Vacuum's PVMassSpec software
-       - cinfdata: text export from DTU Physics' cinfdata system
-       - zilien: .tsv files from Spectro Inlets' Zilien software
-   * - Electrochemistry - Mass Spectrometry
-     - Released
-     - - zilien: .tsv files from Spectro Inlets' Zilien software
-       - EC_MS: .pkl files from the legacy EC_MS python package
-   * - Spectroelectrochemistry
-     - Released
-     - - msrh_sec: .csv file sets from Imperial College London's SEC system
-   * - X-ray photoelectron spectroscopy (XPS)
-     - Development
-     - - avantage: .avg files from Thermo Scientific's Avantage software
-   * - X-ray diffraction (XRD)
-     - Development
-     - - xrdml: .xrdml files from e.g. PanAnalytical's Empyereon
-   * - In-situ Electrochemistry - X-ray adsorption spectroscopy
-     - Development
-     - - qexafs: .dat files from Diamond's B18 beamline
-   * - Low-Energy Ion Scattering (LEIS)
-     - Future
-     -
-
-Tutorials are provided at https://ixdat.readthedocs.io/en/latest/tutorials/index.html
-
-Installation
-------------
-
-To use ``ixdat``, you need to have python installed. We recommend
-`Anaconda python <https://www.anaconda.com/products/individual>`_.
-
-To install ``ixdat``, just type in your terminal or Anaconda prompt::
-
-    $ pip install ixdat
-
-And hit enter.
-
-``ixdat`` is under development, and to make use of the newest features,
-you may need to upgrade to the latest version. This is also easy. Just type::
-
-    $ pip install --upgrade ixdat
-
-
-Article repositories
---------------------
-
-``ixdat`` is shown in practice in a growing number of open repositories of data and analysis
-for academic publications:
-
-- Soren B. Scott, et al.  **Tracking oxygen atoms in electrochemical CO oxidation –Part I: Oxygen exchange via CO2 hydration**. `Electrochimica Acta, 374, 137842 <https://doi.org/10.1016/j.electacta.2021.137842>`_, **2021**.
-
-  Repository: https://github.com/ScottSoren/pyCOox_public
-
-- Soren B. Scott, et al.  **Tracking oxygen atoms in electrochemical CO oxidation –Part II: Lattice oxygen reactivity in oxides of Pt and Ir**. `Electrochimica Acta, 374, 137844 <https://doi.org/10.1016/j.electacta.2021.137844>`_, **2021**.
-
-  Repository: https://github.com/ScottSoren/pyCOox_public
-
-- Kevin Krempl, et al. **Dynamic Interfacial Reaction Rates from Electrochemistry - Mass Spectrometry**. `Journal of Analytical Chemistry. 93, 7022-7028 <https://doi.org/10.1021/acs.analchem.1c00110>`_, **2021**
-
-  Repository: https://github.com/kkrempl/Dynamic-Interfacial-Reaction-Rates
-
-- Junheng Huang, et al. **Online Electrochemistry−Mass Spectrometry Evaluation of the Acidic Oxygen Evolution Reaction at Supported Catalysts**. `ACS Catal. 11, 12745-12753 <https://doi.org/10.1021/acscatal.1c03430>`_, **2021**
-
-  Repository: https://github.com/ScottSoren/Huang2021
-
-
-Join us
--------
-
-``ixdat`` is free and open source software and we welcome input and new collaborators. Please help us improve ``ixdat``!
-
-Contact us (https://github.com/ixdat/ixdat/discussions or sbscott@ic.ac.uk) or just
-`get started developing <https://ixdat.readthedocs.io/en/latest/developing/index.html>`_.
+Metadata-Version: 2.1
+Name: ixdat
+Version: 0.2.9.dev3
+Summary: The in-situ experimental data tool
+Home-page: https://github.com/ixdat/ixdat
+Author: Soren B. Scott, Kenneth Nielsen, Anna Winiwarter, et al
+Author-email: sbs@chem.ku.dk
+License: MIT
+Classifier: Programming Language :: Python :: 3
+Classifier: License :: OSI Approved :: MIT License
+Classifier: Operating System :: OS Independent
+Requires-Python: >=3.6
+Description-Content-Type: text/x-rst
+License-File: LICENSE.txt
+
+.. figure:: docs/source/figures/logo.svg
+    :width: 200
+
+=============================================
+``ixdat``: The In-situ Experimental Data Tool
+=============================================
+
+With ``ixdat``, you can import, combine, and export complex experimental datasets
+as simply as::
+
+    ec = Measurement.read_set("awesome_EC_data", reader="biologic")
+    ec.plot_measurement()
+
+    ms = Measurement.read("2021-03-30 16_59_35 MS data.tsv", reader="zilien")
+    ms.plot_measurement()
+
+    ecms = ec + ms
+    ecms.plot_measurement()
+
+    ecms.export("my_combined_data.csv")
+
+Output:
+
+.. figure:: docs/source/figures/ixdat_example_figures.png
+    :width: 700
+
+    In-situ experimental data made easy
+
+Or rather than exporting, you can take advantage of ``ixdat``'s powerful analysis
+tools and database backends to be a one-stop tool from messy raw data to public
+repository accompanying your breakthrough publication and advancing our field.
+
+Version
+-------
+This is the latest version.
+
+For changes up to this version, see `CHANGES.rst <https://github.com/ixdat/ixdat/blob/main/CHANGES.rst>`_
+
+For ixdat 0.1.9 see the `v0.1.x branch <https://github.com/ixdat/ixdat/tree/v0.1.x>`_.
+
+About
+-----
+
+``ixdat`` provides a powerful **object-oriented** interface to experimental data, especially in-situ experimental data for which it is of interest to combine data obtained simultaneously from multiple techniques.
+
+Documentation is at https://ixdat.readthedocs.io
+
+In addition to a **pluggable** parser interface for importing your data format, ``ixdat`` also includes
+pluggable exporters and plotters, as well as a database interface. A relational model of experimental data is
+designed into every level.
+
+.. list-table:: Techniques and Readers
+   :widths: 20 15 50
+   :header-rows: 1
+
+
+   * - Measurement technique
+     - Status
+     - Readers
+   * - Electrochemistry
+     - Released
+     - - biologic: .mpt files from Biologic's EC-Lab software
+       - autolab: ascii files from AutoLab's NOVA software
+       - ivium: .txt files from Ivium's IviumSoft software
+   * - Mass Spectrometry
+     - Released
+     - - pfeiffer: .dat files from Pfeiffer Vacuum's PVMassSpec software
+       - cinfdata: text export from DTU Physics' cinfdata system
+       - zilien: .tsv files from Spectro Inlets' Zilien software
+   * - Electrochemistry - Mass Spectrometry
+     - Released
+     - - zilien: .tsv files from Spectro Inlets' Zilien software
+       - EC_MS: .pkl files from the legacy EC_MS python package
+   * - Spectroelectrochemistry
+     - Released
+     - - msrh_sec: .csv file sets from Imperial College London's SEC system
+   * - X-ray photoelectron spectroscopy (XPS)
+     - Development
+     - - avantage: .avg files from Thermo Scientific's Avantage software
+   * - X-ray diffraction (XRD)
+     - Development
+     - - xrdml: .xrdml files from e.g. PanAnalytical's Empyereon
+   * - In-situ Electrochemistry - X-ray adsorption spectroscopy
+     - Development
+     - - qexafs: .dat files from Diamond's B18 beamline
+   * - Low-Energy Ion Scattering (LEIS)
+     - Future
+     -
+
+Tutorials are provided at https://ixdat.readthedocs.io/en/latest/tutorials/index.html
+
+Installation
+------------
+
+To use ``ixdat``, you need to have python installed. We recommend
+`Anaconda python <https://www.anaconda.com/products/individual>`_.
+
+To install ``ixdat``, just type in your terminal or Anaconda prompt::
+
+    $ pip install ixdat
+
+And hit enter.
+
+``ixdat`` is under development, and to make use of the newest features,
+you may need to upgrade to the latest version. This is also easy. Just type::
+
+    $ pip install --upgrade ixdat
+
+
+Article repositories
+--------------------
+
+``ixdat`` is shown in practice in a growing number of open repositories of data and analysis
+for academic publications:
+
+- Soren B. Scott, et al.  **Tracking oxygen atoms in electrochemical CO oxidation â€“Part I: Oxygen exchange via CO2 hydration**. `Electrochimica Acta, 374, 137842 <https://doi.org/10.1016/j.electacta.2021.137842>`_, **2021**.
+
+  Repository: https://github.com/ScottSoren/pyCOox_public
+
+- Soren B. Scott, et al.  **Tracking oxygen atoms in electrochemical CO oxidation â€“Part II: Lattice oxygen reactivity in oxides of Pt and Ir**. `Electrochimica Acta, 374, 137844 <https://doi.org/10.1016/j.electacta.2021.137844>`_, **2021**.
+
+  Repository: https://github.com/ScottSoren/pyCOox_public
+
+- Kevin Krempl, et al. **Dynamic Interfacial Reaction Rates from Electrochemistry - Mass Spectrometry**. `Journal of Analytical Chemistry. 93, 7022-7028 <https://doi.org/10.1021/acs.analchem.1c00110>`_, **2021**
+
+  Repository: https://github.com/kkrempl/Dynamic-Interfacial-Reaction-Rates
+
+- Junheng Huang, et al. **Online Electrochemistryâˆ’Mass Spectrometry Evaluation of the Acidic Oxygen Evolution Reaction at Supported Catalysts**. `ACS Catal. 11, 12745-12753 <https://doi.org/10.1021/acscatal.1c03430>`_, **2021**
+
+  Repository: https://github.com/ScottSoren/Huang2021
+
+
+Join us
+-------
+
+``ixdat`` is free and open source software and we welcome input and new collaborators. Please help us improve ``ixdat``!
+
+Contact us (https://github.com/ixdat/ixdat/discussions or sbscott@ic.ac.uk) or just
+`get started developing <https://ixdat.readthedocs.io/en/latest/developing/index.html>`_.
```

### Comparing `ixdat-0.2.9.dev2/setup.py` & `ixdat-0.2.9.dev3/setup.py`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,71 +1,71 @@
-"""This module is used for building the package for distribution"""
-
-import os
-import re
-import setuptools
-
-META_PATH = os.path.join("src", "ixdat", "__init__.py")
-HERE = os.path.abspath(os.path.dirname(__file__))
-PACKAGES = setuptools.find_packages(where="./src")
-
-
-# The `read` and `find_meta` functions are shamelessly stolen from
-# https://hynek.me/articles/sharing-your-labor-of-love-pypi-quick-and-dirty/
-# and their doc strings adapted
-
-
-def read(*parts):
-    """Build an absolute path from *parts* and return the contents of
-    the resulting file as a string.
-
-    Assume UTF-8 encoding.
-
-    """
-    path_to_file = os.path.join(HERE, *parts)
-    with open(path_to_file, "r") as f:
-        return f.read()
-
-
-META_FILE = read(META_PATH)
-
-
-def find_meta(meta):
-    """Extract a piece of double underscore defined metadata from the
-    global META_FILE (defined above).
-
-    If e.g. in META_FILE there is the code::
-
-     __author__ = "Guido"
-
-    then find_meta("author") will return "Guido"
-
-    """
-    meta_match = re.search(
-        r"^__{meta}__ = ['\"]([^'\"]*)['\"]".format(meta=meta), META_FILE, re.M
-    )
-    if meta_match:
-        print(f"found {meta}: '{meta_match.group(1)}'")  # debugging
-        return meta_match.group(1)
-    raise RuntimeError("Unable to find __{meta}__ string.".format(meta=meta))
-
-
-setuptools.setup(
-    name=find_meta("title"),
-    version=find_meta("version"),
-    license=find_meta("license"),
-    author=find_meta("author"),
-    author_email=find_meta("email"),
-    description=find_meta("description"),
-    long_description=read("README.rst"),
-    long_description_content_type="text/x-rst",
-    url="https://github.com/ixdat/ixdat",
-    packages=PACKAGES,
-    package_dir={"": "src"},
-    classifiers=[
-        "Programming Language :: Python :: 3",
-        "License :: OSI Approved :: MIT License",
-        "Operating System :: OS Independent",
-    ],
-    install_requires=read("requirements.txt").split("\n"),
-    python_requires=">=3.6",
-)
+"""This module is used for building the package for distribution"""
+
+import os
+import re
+import setuptools
+
+META_PATH = os.path.join("src", "ixdat", "__init__.py")
+HERE = os.path.abspath(os.path.dirname(__file__))
+PACKAGES = setuptools.find_packages(where="./src")
+
+
+# The `read` and `find_meta` functions are shamelessly stolen from
+# https://hynek.me/articles/sharing-your-labor-of-love-pypi-quick-and-dirty/
+# and their doc strings adapted
+
+
+def read(*parts):
+    """Build an absolute path from *parts* and return the contents of
+    the resulting file as a string.
+
+    Assume UTF-8 encoding.
+
+    """
+    path_to_file = os.path.join(HERE, *parts)
+    with open(path_to_file, "r") as f:
+        return f.read()
+
+
+META_FILE = read(META_PATH)
+
+
+def find_meta(meta):
+    """Extract a piece of double underscore defined metadata from the
+    global META_FILE (defined above).
+
+    If e.g. in META_FILE there is the code::
+
+     __author__ = "Guido"
+
+    then find_meta("author") will return "Guido"
+
+    """
+    meta_match = re.search(
+        r"^__{meta}__ = ['\"]([^'\"]*)['\"]".format(meta=meta), META_FILE, re.M
+    )
+    if meta_match:
+        print(f"found {meta}: '{meta_match.group(1)}'")  # debugging
+        return meta_match.group(1)
+    raise RuntimeError("Unable to find __{meta}__ string.".format(meta=meta))
+
+
+setuptools.setup(
+    name=find_meta("title"),
+    version=find_meta("version"),
+    license=find_meta("license"),
+    author=find_meta("author"),
+    author_email=find_meta("email"),
+    description=find_meta("description"),
+    long_description=read("README.rst"),
+    long_description_content_type="text/x-rst",
+    url="https://github.com/ixdat/ixdat",
+    packages=PACKAGES,
+    package_dir={"": "src"},
+    classifiers=[
+        "Programming Language :: Python :: 3",
+        "License :: OSI Approved :: MIT License",
+        "Operating System :: OS Independent",
+    ],
+    install_requires=read("requirements.txt").split("\n"),
+    python_requires=">=3.6",
+)
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat/__init__.py` & `ixdat-0.2.9.dev3/src/ixdat/__init__.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,21 +1,21 @@
-"""initialize ixdat, giving top-level access to a few of the important structures
-"""
-__version__ = "0.2.9.dev2"
-__title__ = "ixdat"
-__description__ = "The in-situ experimental data tool"
-__url__ = "https://ixdat.readthedocs.io"
-__author__ = "Soren B. Scott, Kenneth Nielsen, Anna Winiwarter, et al"
-__email__ = "sbs@chem.ku.dk"
-__license__ = "MIT"
-
-from .measurements import Measurement
-from .spectra import Spectrum
-from . import db
-from . import techniques
-from . import plotters
-from . import exporters
-from . import config
-from .config import plugins
-
-# I like this to be sure I'm importing from where I think I am:
-print(f"importing ixdat v{__version__} from {__file__}")
+"""initialize ixdat, giving top-level access to a few of the important structures
+"""
+__version__ = "0.2.9.dev3"
+__title__ = "ixdat"
+__description__ = "The in-situ experimental data tool"
+__url__ = "https://ixdat.readthedocs.io"
+__author__ = "Soren B. Scott, Kenneth Nielsen, Anna Winiwarter, et al"
+__email__ = "sbs@chem.ku.dk"
+__license__ = "MIT"
+
+from .measurements import Measurement
+from .spectra import Spectrum
+from . import db
+from . import techniques
+from . import plotters
+from . import exporters
+from . import config
+from .config import plugins
+
+# I like this to be sure I'm importing from where I think I am:
+print(f"importing ixdat v{__version__} from {__file__}")
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat/backends/__init__.py` & `ixdat-0.2.9.dev3/src/ixdat/backends/__init__.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,24 +1,24 @@
-"""Import techniques and build the technique_classes dictionary for direct import
-
-Constants:
-    DATABASE_BACKENDS (dict): Dictionary of {backend_name: BackendClass} where
-        backend_name is the name of the backend (like "directory") and BackendClass
-        is the backend class (inheriting from Backend) for saving and loading things.
-"""
-
-from .backend_base import BackendBase
-from .memory_backend import MemoryBackend
-from .directory_backend import DirBackend
-
-
-BACKEND_CLASSES = {
-    "none": BackendBase,
-    "memory": MemoryBackend,
-    "directory": DirBackend,
-}
-# FIXME: should automate that all initiated backends get added to database_backends?
-database_backends = {
-    "none": BackendBase(),  # Just assigns id's but doesn't keep track.
-    "memory": MemoryBackend(),  # Keeps track so child objects can be passed around
-    "directory": DirBackend(),  # Saves json files, stand-in for SQL, mainly for testing
-}
+"""Import techniques and build the technique_classes dictionary for direct import
+
+Constants:
+    DATABASE_BACKENDS (dict): Dictionary of {backend_name: BackendClass} where
+        backend_name is the name of the backend (like "directory") and BackendClass
+        is the backend class (inheriting from Backend) for saving and loading things.
+"""
+
+from .backend_base import BackendBase
+from .memory_backend import MemoryBackend
+from .directory_backend import DirBackend
+
+
+BACKEND_CLASSES = {
+    "none": BackendBase,
+    "memory": MemoryBackend,
+    "directory": DirBackend,
+}
+# FIXME: should automate that all initiated backends get added to database_backends?
+database_backends = {
+    "none": BackendBase(),  # Just assigns id's but doesn't keep track.
+    "memory": MemoryBackend(),  # Keeps track so child objects can be passed around
+    "directory": DirBackend(),  # Saves json files, stand-in for SQL, mainly for testing
+}
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat/backends/directory_backend.py` & `ixdat-0.2.9.dev3/src/ixdat/backends/directory_backend.py`

 * *Ordering differences only*

 * *Files 11% similar despite different names*

```diff
@@ -1,245 +1,245 @@
-"""This module implements a local json-file-based representation of a relational db
-
-FIXME: Saving and/or loading get quite slow when the number of rows in a table (usually
-  data_series) grows to hundreds. How to figure out why that happens?
-  # see https://github.com/ixdat/ixdat/pull/11#discussion_r663468719
-"""
-
-
-import json
-import numpy as np
-from .backend_base import BackendBase
-from ..config import config, prompt_for_permission
-
-
-char_substitutions = {  # substitutions needed to name .json file with data series name
-    "/": "_DIV_",  # slash (divided by)
-    "\\": "_BKSL_",  # backslash
-    ".": "_DOT_",  # decimal
-    "^": "_CFLX_",  # circumflex accent (raised-to-the)
-    "<": "_LTS_",  # less-than sign
-    ">": "_GTS_",  # greater-than sign
-}
-# TODO: consider implementing some kind of general solution with a tmp dir
-#   see: https://github.com/ixdat/ixdat/pull/5#discussion_r565075588
-
-
-def fix_name_for_saving(name):
-    """Replace problematic characters in name with the substitutions defined above"""
-    for bad_char, substitution in char_substitutions.items():
-        name = name.replace(bad_char, substitution)
-    return name
-
-
-def id_from_path(path):
-    """Return the id (int) of the row represented by given path to an ixdat file"""
-    try:
-        return int(path.stem.split("_")[0])
-    except ValueError:
-        print(f"couldn't find id in {path}")  # debugging
-        return None
-
-
-def name_from_path(path):
-    """Return the name (str) of the row represented by given path to an ixdat file"""
-    return path.stem.split("_", 1)[1]
-
-
-class DirBackend(BackendBase):
-    """A database backend that loads and saves .ix files from a directory
-
-    TODO: refactor i with the standard id_ inside methods
-        see github: https://github.com/ixdat/ixdat/pull/1#discussion_r546400226
-    """
-
-    backend_type = "directory"
-
-    def __init__(
-        self,
-        directory=config.standard_data_directory,
-        project_name=config.default_project_name,
-        metadata_suffix=config.standard_metadata_suffix,
-        data_suffix=config.standard_data_suffix,
-    ):
-        """Initialize a directory database backend with the directory as Path
-
-        Args:
-            directory (Path): the main ixdat directory
-            project_name (str): the name of the project (ixdat subdirectory)
-            metadata_suffix (str): The suffix to use for JSON-formatted metadata files
-            data_suffix (str): The suffix to use for numpy-formatted data files
-        """
-        self.project_directory = directory / project_name
-        self.project_directory.mkdir(parents=True, exist_ok=True)
-
-        self.metadata_suffix = metadata_suffix
-        self.data_suffix = data_suffix
-        super().__init__()
-
-    @property
-    def address(self):
-        """The directory containing the tables (folders) and rows (.ix files)"""
-        return str(self.project_directory)
-
-    def save(self, obj, force=False, no_updates=True):
-        """Save the Savable object as a file corresponding to a row in a table
-
-        Args:
-            obj (Savable): an object
-            force (bool): Whether to force updates if the object is already saved
-            no_updates (bool): Whether to allow updates if the object is already saved.
-                If both force and no_updates are False, the user will be prompted on
-                whether to save.
-        """
-        # First, we save any objects referenced by this object that need to survive a
-        # save-load cycle. These are listed in obj.child_attrs. They need to be saved
-        # first, so that they get their id's in this backend for the main object to
-        # correctly reference. This is done recursively.
-        if obj.child_attrs:
-            for child_list_name in obj.child_attrs:
-                # save any data objects first as this may change the references
-                child_list = getattr(obj, child_list_name) or []
-                for child_obj in child_list:
-                    self.save(child_obj, force=force, no_updates=True)
-        # Now we're ready to save the main object.
-        # The table_name is the table, the as_dict is the info for the row in the table.
-        table_name = obj.table_name
-        obj_as_dict = obj.as_dict()
-        # check if it's already saved and decide what to do if so:
-        if obj.backend is self and self.contains(table_name, obj.id):
-            okay_to_update = not no_updates
-            update_the_row = force or (
-                okay_to_update
-                and prompt_for_permission(
-                    f"Are you sure you would like to overwrite "
-                    f"{self} table={table_name} id={obj.id} with {obj}? "
-                    f"(You can use save() with force=True to suppress this.)"
-                )
-            )
-            if update_the_row:
-                self.update_row(table_name, obj.id, obj_as_dict)
-                return obj.id  # return the id of the updated row
-            else:
-                return  # return nothing since nothing was done
-        else:
-            i = self.add_row(table_name, obj_as_dict)
-            obj.set_id(i)
-            obj.set_backend(self)
-            return i
-
-    def save_data(self, data, table_name, i, fixed_name=None):
-        """Save the data item of a given row, by default as .ix.npy
-
-        Args:
-            data (Array): the numerical data
-            table_name (str): The name of the table to save in
-            i (int): The id of the row to save in
-            fixed_name (the name of the data, just used for the file name
-        """
-        folder = self.project_directory / table_name
-        data_file_name = f"{i}_{fixed_name}{self.data_suffix}"
-        np.save(folder / data_file_name, data)
-
-    def get(self, cls, i):
-        """Open a Saveable object represented as row i of table cls.table_name"""
-        table_name = cls.table_name
-        obj_as_dict = self.get_row_as_dict(table_name, i)
-        i = obj_as_dict.pop("id", i)
-        obj = cls.from_dict(obj_as_dict)
-        obj.set_backend(self)
-        obj.set_id(i)
-        return obj
-
-    def contains(self, table_name, i):
-        """Check if id `i` is already a principle key in the table named `table_name`"""
-        return i in self.get_id_list(table_name)
-
-    def load_obj_data(self, obj):
-        """Return the data for an object loaded from its .ixdata file"""
-        path_to_row = self.get_path_to_row(obj.table_name, obj.id)
-        try:
-            return np.load(path_to_row.with_suffix(self.data_suffix))
-        except FileNotFoundError:
-            # there's no data to be got.
-            print(f"could not find file {path_to_row}")
-            return
-
-    def add_row(self, table_name, obj_as_dict):
-        """Save object's serialization to the folder table_name (like adding a row)"""
-        folder = self.project_directory / table_name
-        if not folder.exists():
-            folder.mkdir(parents=True)
-        i = self.get_next_available_id(table_name)
-        obj_as_dict.update({"id": i})
-        fixed_name = fix_name_for_saving(obj_as_dict["name"])
-        if "data" in obj_as_dict:
-            self.save_data(obj_as_dict["data"], table_name, i, fixed_name)
-            obj_as_dict["data"] = None  # FIXME this could instead point to the data.
-        file_name = f"{i}_{fixed_name}{self.metadata_suffix}"
-        with open(folder / file_name, "w") as f:
-            json.dump(obj_as_dict, f, indent=4)
-        return i
-
-    def update_row(self, table_name, i, obj_as_dict):
-        """Update a file specified by `i` in the folder specified by `table_name`"""
-        folder = self.project_directory / table_name
-        if not folder.exists():
-            folder.mkdir()
-        obj_as_dict.update({"id": i})
-        fixed_name = fix_name_for_saving(obj_as_dict["name"])
-        if "data" in obj_as_dict:
-            self.save_data(obj_as_dict["data"], table_name, i, fixed_name)
-            obj_as_dict["data"] = None  # FIXME this could instead point to the data.
-        file_name = f"{i}_{fixed_name}{self.metadata_suffix}"
-        with open(folder / file_name, "w") as f:
-            json.dump(obj_as_dict, f, indent=4)
-
-    def get_row_as_dict(self, table_name, i):
-        """Return the serialization of the object represented in row i of table_name"""
-        path_to_row = self.get_path_to_row(table_name, i)
-        with open(path_to_row, "r") as file:
-            obj_as_dict = json.load(file)
-        return obj_as_dict
-
-    def get_path_to_row(self, table_name, i):
-        """Return the Path to the file representing row i of the table `table_name`"""
-        folder = self.project_directory / table_name
-        for p in folder.iterdir():
-            if id_from_path(p) == i and p.suffix == self.metadata_suffix:
-                return p
-        print(f"could not find row with id={i} in table '{table_name}'")
-        print(f"looking in folder: {folder}")  # debugging
-        return None  # if that row is not in the table.
-
-    def get_id_list(self, table_name):
-        """List the principle keys of the existing rows of a given table"""
-        folder = self.project_directory / table_name
-        id_list = []
-        for file in folder.iterdir():
-            if not file.is_dir():
-                try:
-                    i = int(file.name.split("_")[0])
-                    id_list.append(i)
-                except TypeError:
-                    pass
-        return id_list
-
-    def get_next_available_id(self, table_name, obj=None):
-        """Return the next available id for a given table"""
-        id_list = self.get_id_list(table_name)
-        if not id_list:
-            return 1
-        return max(self.get_id_list(table_name)) + 1
-
-    def __eq__(self, other):
-        """Two DirBackends are equivalent if they refer to the same directory"""
-        if other is self:
-            return True
-        if (
-            hasattr(other, "directory")
-            and other.project_directory.resolve() == self.project_directory.resolve()
-            and other.project_directory.lstat() == self.project_directory.lstat()
-            and other.__class__ is self.__class__
-        ):
-            return True
-        return False
+"""This module implements a local json-file-based representation of a relational db
+
+FIXME: Saving and/or loading get quite slow when the number of rows in a table (usually
+  data_series) grows to hundreds. How to figure out why that happens?
+  # see https://github.com/ixdat/ixdat/pull/11#discussion_r663468719
+"""
+
+
+import json
+import numpy as np
+from .backend_base import BackendBase
+from ..config import config, prompt_for_permission
+
+
+char_substitutions = {  # substitutions needed to name .json file with data series name
+    "/": "_DIV_",  # slash (divided by)
+    "\\": "_BKSL_",  # backslash
+    ".": "_DOT_",  # decimal
+    "^": "_CFLX_",  # circumflex accent (raised-to-the)
+    "<": "_LTS_",  # less-than sign
+    ">": "_GTS_",  # greater-than sign
+}
+# TODO: consider implementing some kind of general solution with a tmp dir
+#   see: https://github.com/ixdat/ixdat/pull/5#discussion_r565075588
+
+
+def fix_name_for_saving(name):
+    """Replace problematic characters in name with the substitutions defined above"""
+    for bad_char, substitution in char_substitutions.items():
+        name = name.replace(bad_char, substitution)
+    return name
+
+
+def id_from_path(path):
+    """Return the id (int) of the row represented by given path to an ixdat file"""
+    try:
+        return int(path.stem.split("_")[0])
+    except ValueError:
+        print(f"couldn't find id in {path}")  # debugging
+        return None
+
+
+def name_from_path(path):
+    """Return the name (str) of the row represented by given path to an ixdat file"""
+    return path.stem.split("_", 1)[1]
+
+
+class DirBackend(BackendBase):
+    """A database backend that loads and saves .ix files from a directory
+
+    TODO: refactor i with the standard id_ inside methods
+        see github: https://github.com/ixdat/ixdat/pull/1#discussion_r546400226
+    """
+
+    backend_type = "directory"
+
+    def __init__(
+        self,
+        directory=config.standard_data_directory,
+        project_name=config.default_project_name,
+        metadata_suffix=config.standard_metadata_suffix,
+        data_suffix=config.standard_data_suffix,
+    ):
+        """Initialize a directory database backend with the directory as Path
+
+        Args:
+            directory (Path): the main ixdat directory
+            project_name (str): the name of the project (ixdat subdirectory)
+            metadata_suffix (str): The suffix to use for JSON-formatted metadata files
+            data_suffix (str): The suffix to use for numpy-formatted data files
+        """
+        self.project_directory = directory / project_name
+        self.project_directory.mkdir(parents=True, exist_ok=True)
+
+        self.metadata_suffix = metadata_suffix
+        self.data_suffix = data_suffix
+        super().__init__()
+
+    @property
+    def address(self):
+        """The directory containing the tables (folders) and rows (.ix files)"""
+        return str(self.project_directory)
+
+    def save(self, obj, force=False, no_updates=True):
+        """Save the Savable object as a file corresponding to a row in a table
+
+        Args:
+            obj (Savable): an object
+            force (bool): Whether to force updates if the object is already saved
+            no_updates (bool): Whether to allow updates if the object is already saved.
+                If both force and no_updates are False, the user will be prompted on
+                whether to save.
+        """
+        # First, we save any objects referenced by this object that need to survive a
+        # save-load cycle. These are listed in obj.child_attrs. They need to be saved
+        # first, so that they get their id's in this backend for the main object to
+        # correctly reference. This is done recursively.
+        if obj.child_attrs:
+            for child_list_name in obj.child_attrs:
+                # save any data objects first as this may change the references
+                child_list = getattr(obj, child_list_name) or []
+                for child_obj in child_list:
+                    self.save(child_obj, force=force, no_updates=True)
+        # Now we're ready to save the main object.
+        # The table_name is the table, the as_dict is the info for the row in the table.
+        table_name = obj.table_name
+        obj_as_dict = obj.as_dict()
+        # check if it's already saved and decide what to do if so:
+        if obj.backend is self and self.contains(table_name, obj.id):
+            okay_to_update = not no_updates
+            update_the_row = force or (
+                okay_to_update
+                and prompt_for_permission(
+                    f"Are you sure you would like to overwrite "
+                    f"{self} table={table_name} id={obj.id} with {obj}? "
+                    f"(You can use save() with force=True to suppress this.)"
+                )
+            )
+            if update_the_row:
+                self.update_row(table_name, obj.id, obj_as_dict)
+                return obj.id  # return the id of the updated row
+            else:
+                return  # return nothing since nothing was done
+        else:
+            i = self.add_row(table_name, obj_as_dict)
+            obj.set_id(i)
+            obj.set_backend(self)
+            return i
+
+    def save_data(self, data, table_name, i, fixed_name=None):
+        """Save the data item of a given row, by default as .ix.npy
+
+        Args:
+            data (Array): the numerical data
+            table_name (str): The name of the table to save in
+            i (int): The id of the row to save in
+            fixed_name (the name of the data, just used for the file name
+        """
+        folder = self.project_directory / table_name
+        data_file_name = f"{i}_{fixed_name}{self.data_suffix}"
+        np.save(folder / data_file_name, data)
+
+    def get(self, cls, i):
+        """Open a Saveable object represented as row i of table cls.table_name"""
+        table_name = cls.table_name
+        obj_as_dict = self.get_row_as_dict(table_name, i)
+        i = obj_as_dict.pop("id", i)
+        obj = cls.from_dict(obj_as_dict)
+        obj.set_backend(self)
+        obj.set_id(i)
+        return obj
+
+    def contains(self, table_name, i):
+        """Check if id `i` is already a principle key in the table named `table_name`"""
+        return i in self.get_id_list(table_name)
+
+    def load_obj_data(self, obj):
+        """Return the data for an object loaded from its .ixdata file"""
+        path_to_row = self.get_path_to_row(obj.table_name, obj.id)
+        try:
+            return np.load(path_to_row.with_suffix(self.data_suffix))
+        except FileNotFoundError:
+            # there's no data to be got.
+            print(f"could not find file {path_to_row}")
+            return
+
+    def add_row(self, table_name, obj_as_dict):
+        """Save object's serialization to the folder table_name (like adding a row)"""
+        folder = self.project_directory / table_name
+        if not folder.exists():
+            folder.mkdir(parents=True)
+        i = self.get_next_available_id(table_name)
+        obj_as_dict.update({"id": i})
+        fixed_name = fix_name_for_saving(obj_as_dict["name"])
+        if "data" in obj_as_dict:
+            self.save_data(obj_as_dict["data"], table_name, i, fixed_name)
+            obj_as_dict["data"] = None  # FIXME this could instead point to the data.
+        file_name = f"{i}_{fixed_name}{self.metadata_suffix}"
+        with open(folder / file_name, "w") as f:
+            json.dump(obj_as_dict, f, indent=4)
+        return i
+
+    def update_row(self, table_name, i, obj_as_dict):
+        """Update a file specified by `i` in the folder specified by `table_name`"""
+        folder = self.project_directory / table_name
+        if not folder.exists():
+            folder.mkdir()
+        obj_as_dict.update({"id": i})
+        fixed_name = fix_name_for_saving(obj_as_dict["name"])
+        if "data" in obj_as_dict:
+            self.save_data(obj_as_dict["data"], table_name, i, fixed_name)
+            obj_as_dict["data"] = None  # FIXME this could instead point to the data.
+        file_name = f"{i}_{fixed_name}{self.metadata_suffix}"
+        with open(folder / file_name, "w") as f:
+            json.dump(obj_as_dict, f, indent=4)
+
+    def get_row_as_dict(self, table_name, i):
+        """Return the serialization of the object represented in row i of table_name"""
+        path_to_row = self.get_path_to_row(table_name, i)
+        with open(path_to_row, "r") as file:
+            obj_as_dict = json.load(file)
+        return obj_as_dict
+
+    def get_path_to_row(self, table_name, i):
+        """Return the Path to the file representing row i of the table `table_name`"""
+        folder = self.project_directory / table_name
+        for p in folder.iterdir():
+            if id_from_path(p) == i and p.suffix == self.metadata_suffix:
+                return p
+        print(f"could not find row with id={i} in table '{table_name}'")
+        print(f"looking in folder: {folder}")  # debugging
+        return None  # if that row is not in the table.
+
+    def get_id_list(self, table_name):
+        """List the principle keys of the existing rows of a given table"""
+        folder = self.project_directory / table_name
+        id_list = []
+        for file in folder.iterdir():
+            if not file.is_dir():
+                try:
+                    i = int(file.name.split("_")[0])
+                    id_list.append(i)
+                except TypeError:
+                    pass
+        return id_list
+
+    def get_next_available_id(self, table_name, obj=None):
+        """Return the next available id for a given table"""
+        id_list = self.get_id_list(table_name)
+        if not id_list:
+            return 1
+        return max(self.get_id_list(table_name)) + 1
+
+    def __eq__(self, other):
+        """Two DirBackends are equivalent if they refer to the same directory"""
+        if other is self:
+            return True
+        if (
+            hasattr(other, "directory")
+            and other.project_directory.resolve() == self.project_directory.resolve()
+            and other.project_directory.lstat() == self.project_directory.lstat()
+            and other.__class__ is self.__class__
+        ):
+            return True
+        return False
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat/backends/memory_backend.py` & `ixdat-0.2.9.dev3/src/ixdat/backends/memory_backend.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,42 +1,42 @@
-from .backend_base import BackendBase
-
-
-class MemoryBackend(BackendBase):
-    """A backend that assigns id's and keeps track of objects for later access.
-
-    This means that a Savable object can make a serializable representation of itself
-    that includes the short_identity in the memory backend of objects that it
-    references, and a new Savable object made from this representation can find the
-    original objects. See db.Savable.as_dict(), which ensures that "child objects"
-    are in memory.
-    """
-
-    backend_type = "memory"
-    address = "here"
-
-    def __init__(self):
-        """Initialize the backend with dict for {table_name (str): id_counter (int)}"""
-        super().__init__()
-        self.objects = {}
-
-    def get_next_available_id(self, table_name, obj=None):
-        """Return the id counter for table_name, starting with 1."""
-        i = super().get_next_available_id(table_name)
-        if table_name not in self.objects:
-            self.objects[table_name] = {}
-        return i
-
-    def get(self, cls, i):
-        """Return an object of a specified class and id by looking up in memory"""
-        return self.objects[cls.table_name][i]
-
-    def save(self, obj):
-        """Save the object into memory, and change its backend to this backend."""
-        if obj.backend is self:
-            return obj.id
-        table_name = obj.table_name
-        i = self.get_next_available_id(table_name, obj)
-        self.objects[obj.table_name][i] = obj
-        obj.set_id(i)
-        obj.set_backend(self)
-        return i
+from .backend_base import BackendBase
+
+
+class MemoryBackend(BackendBase):
+    """A backend that assigns id's and keeps track of objects for later access.
+
+    This means that a Savable object can make a serializable representation of itself
+    that includes the short_identity in the memory backend of objects that it
+    references, and a new Savable object made from this representation can find the
+    original objects. See db.Savable.as_dict(), which ensures that "child objects"
+    are in memory.
+    """
+
+    backend_type = "memory"
+    address = "here"
+
+    def __init__(self):
+        """Initialize the backend with dict for {table_name (str): id_counter (int)}"""
+        super().__init__()
+        self.objects = {}
+
+    def get_next_available_id(self, table_name, obj=None):
+        """Return the id counter for table_name, starting with 1."""
+        i = super().get_next_available_id(table_name)
+        if table_name not in self.objects:
+            self.objects[table_name] = {}
+        return i
+
+    def get(self, cls, i):
+        """Return an object of a specified class and id by looking up in memory"""
+        return self.objects[cls.table_name][i]
+
+    def save(self, obj):
+        """Save the object into memory, and change its backend to this backend."""
+        if obj.backend is self:
+            return obj.id
+        table_name = obj.table_name
+        i = self.get_next_available_id(table_name, obj)
+        self.objects[obj.table_name][i] = obj
+        obj.set_id(i)
+        obj.set_backend(self)
+        return i
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat/config.py` & `ixdat-0.2.9.dev3/src/ixdat/config.py`

 * *Ordering differences only*

 * *Files 11% similar despite different names*

```diff
@@ -1,252 +1,252 @@
-"""Configuration variables including standard data directory
-
-This module also defines user-specific options, such as whether to use plugins
-
-The object `plugins` created here gives direct access to these options.
-Example useage:
-
-```
-import ixdat
-
-ixdat.config.plugins.use_si_quant = True  # use the spectro_inlets_quantification package
-```
-
-See `help(ixdat.options.plugins)` for information.
-"""
-import datetime
-from pathlib import Path
-from .tools import deprecate
-
-
-class _Config:
-    """
-    Attributes:
-        standard_data_directory (Path): the directory in which to save by default with
-            the default database backend, which is to save files in a directory.
-            ixdat will make the directory if it does not exist.
-        standard_metadata_suffix (str): The file ext. for JSON format metadata files
-        standard_data_suffix (str): The file extension for numpy.save format data files
-        timestamp_string_format (str): A format string for datetime.datetime.strftime.
-            Defaults to ixdats custom datetime format: 22E18 14:34:55
-        timezone (datetime.timezone): The timezone timestamps should use when formatted
-            to string. Defaults to the current local timestamp.
-    """
-
-    def __init__(self):
-        self.standard_metadata_suffix = ".ix"
-        self.standard_data_suffix = ".ix.npy"
-        self.standard_ixdat_directory = Path.home() / ".ixdat"
-        self.standard_data_directory = self.standard_ixdat_directory / "projects"
-        self.default_project_name = "test"
-        self.timestamp_string_format = "native"
-        self.timezone = datetime.datetime.now(datetime.timezone.utc).astimezone().tzinfo
-
-    @property
-    def ixdat_temp_dir(self):
-        temp_dir = self.standard_data_directory / "temp"
-        if not temp_dir.exists():
-            temp_dir.mkdir(parents=True)
-        return temp_dir
-
-
-config = _Config()
-
-
-def prompt_for_permission(prompt):
-    yn = input(prompt + "\nEnter y for yes or anything else for no.")
-    return yn in ["y", "yes", "Y", "Yes"]
-
-
-class _PluginOptions:
-    """A class for activating plugins and giving access to items from plugins
-
-    This class has only one instance, initiated on import of ixdat. Use it by
-    `from ixdat.config import plugins`
-
-    These packages need to be separately installed.
-
-    Packages
-    --------
-
-    spectro_inlets_quantification
-    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-    - use_si_quant (bool): Read-only. If this is True, ixdat uses the
-        `spectro_inlets_quantification` package. This changes the behaviour of some
-        methods in `MSMeasurement` and inheriting classes. Defaults to False.
-    - activate_si_quant(): Sets use_si_quant to True and initializes the quant. package
-    - deactivate_si_quant(): Sets use_si_quant to False.
-    - si_quant (_QuantDeps). This is where the needed imports from the external
-        quantification package go. See the docstring of _QuantDeps
-    """
-
-    def __init__(self):
-        self._use_siq = False
-        self.siq = _SIQ()
-        self.cinfdata = _CinfData()
-
-    @property
-    def use_siq(self):
-        return self._use_siq
-
-    def activate_siq(self):
-        """Changes mass spec methods to use spectro_inlets_quantification"""
-        self._use_siq = True
-        self.siq.populate()
-
-    def deactivate_siq(self):
-        """Changes mass spec methods to use ixdat's native MS quantification"""
-        self._use_siq = False
-
-    def activate_cinfdata(self):
-        self.cinfdata.activate()
-
-    @deprecate(
-        last_supported_release="0.2.6",
-        update_message="`siq` is the universal abreviation "
-        "for `spectro_inlets_quantification`. "
-        "Thus, `ixdat.plugins.activate_si_quant` "
-        "is now `ixdat.plugins.activate_siq`",
-        hard_deprecation_release="0.3.0",
-        remove_release="1.0.0",
-    )
-    def activate_si_quant(self):
-        return self.activate_siq()
-
-    @property
-    @deprecate(
-        last_supported_release="0.2.6",
-        update_message="`siq` is the universal abreviation "
-        "for `spectro_inlets_quantification`. "
-        "Thus, `ixdat.plugins.use_si_quant` "
-        "is now `ixdat.plugins.use_siq`",
-        hard_deprecation_release="0.3.0",
-        remove_release="1.0.0",
-    )
-    def use_si_quant(self):
-        return self.use_siq
-
-    @property
-    @deprecate(
-        last_supported_release="0.2.6",
-        update_message="`siq` is the universal abreviation "
-        "for `spectro_inlets_quantification`. "
-        "Thus, `ixdat.plugins.si_quant` is now `ixdat.plugins.siq`",
-        hard_deprecation_release="0.3.0",
-        remove_release="1.0.0",
-    )
-    def si_quant(self):
-        return self.siq
-
-
-class _SIQ:
-    """Class storing items of `spectro_inlets_quantification`.
-
-    This class has one instance, which is an attribute of `plugins`. To print this
-    docstring, you would type:
-    ```
-    from ixdat.config import plugins
-
-    plugins.use_si_quant = True  # Activates plugins.quant.
-    help(plugins.si_quant)  #gives information on the si_quant package
-    ```
-
-    The attributes of this class are `None` until the property `plugins.use_si_quant` is
-    set to True, triggering their population (activating quant).
-
-    Once activated, the attributes of `plugins.quant` are:
-    - `Chip`: Class describing the MS inlet. More powerful than ixdat's `MSInlet`
-    - `Molecule`: Class with data about molecules relevant to (EC-)MS quantification
-    - `CalPoint`: Class with data about an MS calibration experiment
-    - `Calibration`: Class for storing, visualizing, and using multiple CalPoints.
-    - `Quantifier`: Class for using a Calibration to quantify MS data
-    - `quant_config`: The config object of the external quantification package
-
-    `plugins.quant` also has
-    - `QUANT_DIRECTORY`: A property for getting and setting the data directory used by
-      the external quantification package.
-    """
-
-    def __init__(self):
-        self.Chip = None
-        self.Molecule = None
-        self.Calibration = None
-        self.CalPoint = None
-        self.Quantifier = None
-        self.quant_config = None
-        self._QUANT_DIRECTORY = None
-
-    def populate(self):
-        from spectro_inlets_quantification.config import Config
-        from spectro_inlets_quantification.medium import Medium
-        from spectro_inlets_quantification.molecule import Molecule
-        from spectro_inlets_quantification.chip import Chip
-        from spectro_inlets_quantification.calibration import CalPoint, Calibration
-        from spectro_inlets_quantification.quantifier import Quantifier
-
-        self.quant_config = Config()
-        self.medium = Medium()
-        self.Molecule = Molecule
-        self.Chip = Chip
-        self.CalPoint = CalPoint
-        self.Calibration = Calibration
-        self.Quantifier = Quantifier
-
-    @property
-    def QUANT_DIRECTORY(self):
-        if not self._QUANT_DIRECTORY:
-            self._QUANT_DIRECTORY = (
-                config.standard_ixdat_directory / "plugin_data/ms_quant"
-            )
-            if not self._QUANT_DIRECTORY.exists():
-                self._QUANT_DIRECTORY.mkdir(parents=True)
-        return self._QUANT_DIRECTORY
-
-    @QUANT_DIRECTORY.setter
-    def QUANT_DIRECTORY(self, quant_directory):
-        self._QUANT_DIRECTORY = quant_directory
-        self.quant_config.aux_data_directory = self.QUANT_DIRECTORY
-
-
-class _CinfData:
-    """Class implement direct database read access using external module Cinfdata"""
-
-    def __init__(
-        self,
-    ):
-        self.cinfdata = None
-        self._managed_cinfdata_object = None
-        self._context_manager_kwargs = None
-
-    def activate(self):
-        from cinfdata import Cinfdata
-
-        self.DB = Cinfdata
-
-    def connect(self, setup_name=None, grouping_column=None):
-        """setup_name (str): The name of the table inside the database
-        grouping_column (str): Either the 'timestamp'/'comment' or 'Comment' column"""
-        return self.DB(setup_name=setup_name, grouping_column=grouping_column)
-
-    def __call__(self, **kwargs):
-        """**kwargs: setup_name (str) and grouping_column (str) (see connect())"""
-        self._context_manager_kwargs = kwargs
-        return self
-
-    def __enter__(self):
-        if self._managed_cinfdata_object:
-            self._context_manager_kwargs = None
-            raise RuntimeError(
-                "Using the cinfdata plugin as a context manager more "
-                "than once at the same time is not supported"
-            )
-
-        self._managed_cinfdata_object = self.connect(**self._context_manager_kwargs)
-        return self._managed_cinfdata_object
-
-    def __exit__(self, exc_type, exc_val, exc_tb):
-        self._managed_cinfdata_object.connection.close()
-        self._managed_cinfdata_object = None
-        self._context_manager_kwargs = None
-
-
-plugins = _PluginOptions()
+"""Configuration variables including standard data directory
+
+This module also defines user-specific options, such as whether to use plugins
+
+The object `plugins` created here gives direct access to these options.
+Example useage:
+
+```
+import ixdat
+
+ixdat.config.plugins.use_si_quant = True  # use the spectro_inlets_quantification package
+```
+
+See `help(ixdat.options.plugins)` for information.
+"""
+import datetime
+from pathlib import Path
+from .tools import deprecate
+
+
+class _Config:
+    """
+    Attributes:
+        standard_data_directory (Path): the directory in which to save by default with
+            the default database backend, which is to save files in a directory.
+            ixdat will make the directory if it does not exist.
+        standard_metadata_suffix (str): The file ext. for JSON format metadata files
+        standard_data_suffix (str): The file extension for numpy.save format data files
+        timestamp_string_format (str): A format string for datetime.datetime.strftime.
+            Defaults to ixdats custom datetime format: 22E18 14:34:55
+        timezone (datetime.timezone): The timezone timestamps should use when formatted
+            to string. Defaults to the current local timestamp.
+    """
+
+    def __init__(self):
+        self.standard_metadata_suffix = ".ix"
+        self.standard_data_suffix = ".ix.npy"
+        self.standard_ixdat_directory = Path.home() / ".ixdat"
+        self.standard_data_directory = self.standard_ixdat_directory / "projects"
+        self.default_project_name = "test"
+        self.timestamp_string_format = "native"
+        self.timezone = datetime.datetime.now(datetime.timezone.utc).astimezone().tzinfo
+
+    @property
+    def ixdat_temp_dir(self):
+        temp_dir = self.standard_data_directory / "temp"
+        if not temp_dir.exists():
+            temp_dir.mkdir(parents=True)
+        return temp_dir
+
+
+config = _Config()
+
+
+def prompt_for_permission(prompt):
+    yn = input(prompt + "\nEnter y for yes or anything else for no.")
+    return yn in ["y", "yes", "Y", "Yes"]
+
+
+class _PluginOptions:
+    """A class for activating plugins and giving access to items from plugins
+
+    This class has only one instance, initiated on import of ixdat. Use it by
+    `from ixdat.config import plugins`
+
+    These packages need to be separately installed.
+
+    Packages
+    --------
+
+    spectro_inlets_quantification
+    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+    - use_si_quant (bool): Read-only. If this is True, ixdat uses the
+        `spectro_inlets_quantification` package. This changes the behaviour of some
+        methods in `MSMeasurement` and inheriting classes. Defaults to False.
+    - activate_si_quant(): Sets use_si_quant to True and initializes the quant. package
+    - deactivate_si_quant(): Sets use_si_quant to False.
+    - si_quant (_QuantDeps). This is where the needed imports from the external
+        quantification package go. See the docstring of _QuantDeps
+    """
+
+    def __init__(self):
+        self._use_siq = False
+        self.siq = _SIQ()
+        self.cinfdata = _CinfData()
+
+    @property
+    def use_siq(self):
+        return self._use_siq
+
+    def activate_siq(self):
+        """Changes mass spec methods to use spectro_inlets_quantification"""
+        self._use_siq = True
+        self.siq.populate()
+
+    def deactivate_siq(self):
+        """Changes mass spec methods to use ixdat's native MS quantification"""
+        self._use_siq = False
+
+    def activate_cinfdata(self):
+        self.cinfdata.activate()
+
+    @deprecate(
+        last_supported_release="0.2.6",
+        update_message="`siq` is the universal abreviation "
+        "for `spectro_inlets_quantification`. "
+        "Thus, `ixdat.plugins.activate_si_quant` "
+        "is now `ixdat.plugins.activate_siq`",
+        hard_deprecation_release="0.3.0",
+        remove_release="1.0.0",
+    )
+    def activate_si_quant(self):
+        return self.activate_siq()
+
+    @property
+    @deprecate(
+        last_supported_release="0.2.6",
+        update_message="`siq` is the universal abreviation "
+        "for `spectro_inlets_quantification`. "
+        "Thus, `ixdat.plugins.use_si_quant` "
+        "is now `ixdat.plugins.use_siq`",
+        hard_deprecation_release="0.3.0",
+        remove_release="1.0.0",
+    )
+    def use_si_quant(self):
+        return self.use_siq
+
+    @property
+    @deprecate(
+        last_supported_release="0.2.6",
+        update_message="`siq` is the universal abreviation "
+        "for `spectro_inlets_quantification`. "
+        "Thus, `ixdat.plugins.si_quant` is now `ixdat.plugins.siq`",
+        hard_deprecation_release="0.3.0",
+        remove_release="1.0.0",
+    )
+    def si_quant(self):
+        return self.siq
+
+
+class _SIQ:
+    """Class storing items of `spectro_inlets_quantification`.
+
+    This class has one instance, which is an attribute of `plugins`. To print this
+    docstring, you would type:
+    ```
+    from ixdat.config import plugins
+
+    plugins.use_si_quant = True  # Activates plugins.quant.
+    help(plugins.si_quant)  #gives information on the si_quant package
+    ```
+
+    The attributes of this class are `None` until the property `plugins.use_si_quant` is
+    set to True, triggering their population (activating quant).
+
+    Once activated, the attributes of `plugins.quant` are:
+    - `Chip`: Class describing the MS inlet. More powerful than ixdat's `MSInlet`
+    - `Molecule`: Class with data about molecules relevant to (EC-)MS quantification
+    - `CalPoint`: Class with data about an MS calibration experiment
+    - `Calibration`: Class for storing, visualizing, and using multiple CalPoints.
+    - `Quantifier`: Class for using a Calibration to quantify MS data
+    - `quant_config`: The config object of the external quantification package
+
+    `plugins.quant` also has
+    - `QUANT_DIRECTORY`: A property for getting and setting the data directory used by
+      the external quantification package.
+    """
+
+    def __init__(self):
+        self.Chip = None
+        self.Molecule = None
+        self.Calibration = None
+        self.CalPoint = None
+        self.Quantifier = None
+        self.quant_config = None
+        self._QUANT_DIRECTORY = None
+
+    def populate(self):
+        from spectro_inlets_quantification.config import Config
+        from spectro_inlets_quantification.medium import Medium
+        from spectro_inlets_quantification.molecule import Molecule
+        from spectro_inlets_quantification.chip import Chip
+        from spectro_inlets_quantification.calibration import CalPoint, Calibration
+        from spectro_inlets_quantification.quantifier import Quantifier
+
+        self.quant_config = Config()
+        self.medium = Medium()
+        self.Molecule = Molecule
+        self.Chip = Chip
+        self.CalPoint = CalPoint
+        self.Calibration = Calibration
+        self.Quantifier = Quantifier
+
+    @property
+    def QUANT_DIRECTORY(self):
+        if not self._QUANT_DIRECTORY:
+            self._QUANT_DIRECTORY = (
+                config.standard_ixdat_directory / "plugin_data/ms_quant"
+            )
+            if not self._QUANT_DIRECTORY.exists():
+                self._QUANT_DIRECTORY.mkdir(parents=True)
+        return self._QUANT_DIRECTORY
+
+    @QUANT_DIRECTORY.setter
+    def QUANT_DIRECTORY(self, quant_directory):
+        self._QUANT_DIRECTORY = quant_directory
+        self.quant_config.aux_data_directory = self.QUANT_DIRECTORY
+
+
+class _CinfData:
+    """Class implement direct database read access using external module Cinfdata"""
+
+    def __init__(
+        self,
+    ):
+        self.cinfdata = None
+        self._managed_cinfdata_object = None
+        self._context_manager_kwargs = None
+
+    def activate(self):
+        from cinfdata import Cinfdata
+
+        self.DB = Cinfdata
+
+    def connect(self, setup_name=None, grouping_column=None):
+        """setup_name (str): The name of the table inside the database
+        grouping_column (str): Either the 'timestamp'/'comment' or 'Comment' column"""
+        return self.DB(setup_name=setup_name, grouping_column=grouping_column)
+
+    def __call__(self, **kwargs):
+        """**kwargs: setup_name (str) and grouping_column (str) (see connect())"""
+        self._context_manager_kwargs = kwargs
+        return self
+
+    def __enter__(self):
+        if self._managed_cinfdata_object:
+            self._context_manager_kwargs = None
+            raise RuntimeError(
+                "Using the cinfdata plugin as a context manager more "
+                "than once at the same time is not supported"
+            )
+
+        self._managed_cinfdata_object = self.connect(**self._context_manager_kwargs)
+        return self._managed_cinfdata_object
+
+    def __exit__(self, exc_type, exc_val, exc_tb):
+        self._managed_cinfdata_object.connection.close()
+        self._managed_cinfdata_object = None
+        self._context_manager_kwargs = None
+
+
+plugins = _PluginOptions()
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat/constants.py` & `ixdat-0.2.9.dev3/src/ixdat/constants.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,162 +1,162 @@
-import numpy as np
-from scipy import constants as scipy_constants
-
-# short-form aliases for a few scipy constants
-c = scipy_constants.c  # speed of light / (m/s)
-qe = scipy_constants.e  # fundamental charge / (C)
-h = scipy_constants.h  # planck's constant / (J*s)
-hbar = scipy_constants.hbar  # reduced planck's constant / (J*s)
-NA = scipy_constants.N_A  # Avogadro's number /(mol) or dimensionless
-me = scipy_constants.m_e  # mass of electron / (kg)
-kB = scipy_constants.k  # Boltzman constant / (J/K)
-u0 = scipy_constants.mu_0  # permeability of free space / (J*s^2/(m*C^2))
-e0 = scipy_constants.epsilon_0  # permittivity of free space / (C^2/(J*m))
-R = scipy_constants.R  # gas constant / (J/(mol*K))
-
-# a few extra derived constants
-amu = 1e-3 / NA  # atomic mass unit / (kg)    # amu=(1g/mol)/NA
-Far = NA * qe  # Faraday's constant, C/mol
-
-# long-form aliases
-FARADAY_CONSTANT = Far
-AVOGADRO_CONSTANT = NA
-BOLTZMANN_CONSTANT = kB
-
-# standard conditions
-STANDARD_TEMPERATURE = 298.15  # Standard temperature of 25 C in [K]
-STANDARD_PRESSURE = 1e5  # Standard pressure of 1 bar in [Pa]
-
-# molecule properties (should probably come from elsewhere).
-# TODO: Move these viscosities to external MS quantification package
-#    They are only used by MSInlet, which is deprecated.
-#    See https://github.com/ixdat/ixdat/issues/122
-
-DYNAMIC_VISCOSITIES = {  # Values is found on engineeringtoolbox.com [22C30]
-    "O2": np.array(
-        [
-            [200, 14.72e-06],  # Temperature [K], Dynamic Viscosit [Pa*s]
-            [220, 15.98e-06],
-            [240, 17.20e-06],
-            [260, 18.38e-06],
-            [280, 19.53e-06],
-            [300, 20.65e-06],
-            [320, 21.74e-06],
-            [340, 22.80e-06],
-            [360, 23.84e-06],
-            [400, 25.84e-06],
-            [500, 30.49e-06],
-            [600, 34.73e-06],
-            [700, 38.65e-06],
-            [800, 42.33e-06],
-        ]
-    ),
-    "N2": np.array(
-        [
-            [200, 12.91e-06],
-            [220, 13.97e-06],
-            [240, 15.00e-06],
-            [260, 15.99e-06],
-            [280, 16.96e-06],
-            [300, 17.89e-06],
-            [320, 18.80e-06],
-            [340, 19.68e-06],
-            [360, 20.55e-06],
-            [400, 22.21e-06],
-            [500, 26.06e-06],
-            [600, 29.58e-06],
-            [700, 32.83e-06],
-            [800, 35.89e-06],
-        ]
-    ),
-    "Ar": np.array(
-        [
-            [273.15, 2.10e-05],
-            [293.15, 2.23e-05],
-            [298.15, 2.27e-05],
-            [323.15, 2.42e-05],
-            [373.15, 2.73e-05],
-            [473.15, 3.28e-05],
-            [573.15, 3.77e-05],
-            [673.15, 4.22e-05],
-            [773.15, 4.64e-05],
-            [873.15, 5.04e-05],
-        ]
-    ),
-    "He": np.array(
-        [
-            [273.15, 1.87e-05],
-            [293.15, 1.96e-05],
-            [323.15, 2.10e-05],
-            [373.15, 2.32e-05],
-            [473.15, 2.73e-05],
-            [573.15, 3.12e-05],
-            [673.15, 3.48e-05],
-            [773.15, 3.84e-05],
-            [873.15, 4.18e-05],
-        ]
-    ),
-    "CO": np.array(
-        [
-            [273.15, 1.66e-05],
-            [293.15, 1.74e-05],
-            [323.15, 1.88e-05],
-            [373.15, 2.10e-05],
-            [473.15, 2.52e-05],
-            [573.15, 2.90e-05],
-            [673.15, 3.25e-05],
-            [773.15, 3.56e-05],
-            [873.15, 3.86e-05],
-        ]
-    ),
-    "CO2": np.array(
-        [
-            [220, 11.06e-06],
-            [240, 12.07e-06],
-            [250, 12.57e-06],
-            [260, 13.06e-06],
-            [280, 14.05e-06],
-            [300, 15.02e-06],
-            [320, 15.98e-06],
-            [340, 16.93e-06],
-            [360, 17.87e-06],
-            [400, 19.70e-06],
-            [450, 21.90e-06],
-            [500, 24.02e-06],
-            [600, 28.00e-06],
-            [650, 29.87e-06],
-            [850, 36.71e-06],
-            [1050, 42.69e-06],
-        ]
-    ),
-    "H2": np.array(
-        [
-            [273.15, 0.84e-05],
-            [293.15, 0.88e-05],
-            [323.15, 0.94e-05],
-            [373.15, 1.04e-05],
-            [473.15, 1.21e-05],
-            [573.15, 1.37e-05],
-            [673.15, 1.53e-05],
-            [773.15, 1.69e-05],
-            [873.15, 1.84e-05],
-        ]
-    ),
-}
-MOLECULAR_DIAMETERS = {
-    "O2": 3.55e-10,
-    "N2": 3.7e-10,
-    "Ar": 3.58e-10,
-    "He": 2.15e-10,
-    "CO": 3.76e-10,
-    "CO2": 3.34e-10,
-    "H2": 2.71e-10,
-}  # in [m]
-MOLAR_MASSES = {
-    "O2": 31.998,
-    "N2": 28.014,
-    "Ar": 39.948,
-    "He": 4.002,
-    "CO": 28.010,
-    "CO2": 44.010,
-    "H2": 2.016,
-}  # in [g/mol]
+import numpy as np
+from scipy import constants as scipy_constants
+
+# short-form aliases for a few scipy constants
+c = scipy_constants.c  # speed of light / (m/s)
+qe = scipy_constants.e  # fundamental charge / (C)
+h = scipy_constants.h  # planck's constant / (J*s)
+hbar = scipy_constants.hbar  # reduced planck's constant / (J*s)
+NA = scipy_constants.N_A  # Avogadro's number /(mol) or dimensionless
+me = scipy_constants.m_e  # mass of electron / (kg)
+kB = scipy_constants.k  # Boltzman constant / (J/K)
+u0 = scipy_constants.mu_0  # permeability of free space / (J*s^2/(m*C^2))
+e0 = scipy_constants.epsilon_0  # permittivity of free space / (C^2/(J*m))
+R = scipy_constants.R  # gas constant / (J/(mol*K))
+
+# a few extra derived constants
+amu = 1e-3 / NA  # atomic mass unit / (kg)    # amu=(1g/mol)/NA
+Far = NA * qe  # Faraday's constant, C/mol
+
+# long-form aliases
+FARADAY_CONSTANT = Far
+AVOGADRO_CONSTANT = NA
+BOLTZMANN_CONSTANT = kB
+
+# standard conditions
+STANDARD_TEMPERATURE = 298.15  # Standard temperature of 25 C in [K]
+STANDARD_PRESSURE = 1e5  # Standard pressure of 1 bar in [Pa]
+
+# molecule properties (should probably come from elsewhere).
+# TODO: Move these viscosities to external MS quantification package
+#    They are only used by MSInlet, which is deprecated.
+#    See https://github.com/ixdat/ixdat/issues/122
+
+DYNAMIC_VISCOSITIES = {  # Values is found on engineeringtoolbox.com [22C30]
+    "O2": np.array(
+        [
+            [200, 14.72e-06],  # Temperature [K], Dynamic Viscosit [Pa*s]
+            [220, 15.98e-06],
+            [240, 17.20e-06],
+            [260, 18.38e-06],
+            [280, 19.53e-06],
+            [300, 20.65e-06],
+            [320, 21.74e-06],
+            [340, 22.80e-06],
+            [360, 23.84e-06],
+            [400, 25.84e-06],
+            [500, 30.49e-06],
+            [600, 34.73e-06],
+            [700, 38.65e-06],
+            [800, 42.33e-06],
+        ]
+    ),
+    "N2": np.array(
+        [
+            [200, 12.91e-06],
+            [220, 13.97e-06],
+            [240, 15.00e-06],
+            [260, 15.99e-06],
+            [280, 16.96e-06],
+            [300, 17.89e-06],
+            [320, 18.80e-06],
+            [340, 19.68e-06],
+            [360, 20.55e-06],
+            [400, 22.21e-06],
+            [500, 26.06e-06],
+            [600, 29.58e-06],
+            [700, 32.83e-06],
+            [800, 35.89e-06],
+        ]
+    ),
+    "Ar": np.array(
+        [
+            [273.15, 2.10e-05],
+            [293.15, 2.23e-05],
+            [298.15, 2.27e-05],
+            [323.15, 2.42e-05],
+            [373.15, 2.73e-05],
+            [473.15, 3.28e-05],
+            [573.15, 3.77e-05],
+            [673.15, 4.22e-05],
+            [773.15, 4.64e-05],
+            [873.15, 5.04e-05],
+        ]
+    ),
+    "He": np.array(
+        [
+            [273.15, 1.87e-05],
+            [293.15, 1.96e-05],
+            [323.15, 2.10e-05],
+            [373.15, 2.32e-05],
+            [473.15, 2.73e-05],
+            [573.15, 3.12e-05],
+            [673.15, 3.48e-05],
+            [773.15, 3.84e-05],
+            [873.15, 4.18e-05],
+        ]
+    ),
+    "CO": np.array(
+        [
+            [273.15, 1.66e-05],
+            [293.15, 1.74e-05],
+            [323.15, 1.88e-05],
+            [373.15, 2.10e-05],
+            [473.15, 2.52e-05],
+            [573.15, 2.90e-05],
+            [673.15, 3.25e-05],
+            [773.15, 3.56e-05],
+            [873.15, 3.86e-05],
+        ]
+    ),
+    "CO2": np.array(
+        [
+            [220, 11.06e-06],
+            [240, 12.07e-06],
+            [250, 12.57e-06],
+            [260, 13.06e-06],
+            [280, 14.05e-06],
+            [300, 15.02e-06],
+            [320, 15.98e-06],
+            [340, 16.93e-06],
+            [360, 17.87e-06],
+            [400, 19.70e-06],
+            [450, 21.90e-06],
+            [500, 24.02e-06],
+            [600, 28.00e-06],
+            [650, 29.87e-06],
+            [850, 36.71e-06],
+            [1050, 42.69e-06],
+        ]
+    ),
+    "H2": np.array(
+        [
+            [273.15, 0.84e-05],
+            [293.15, 0.88e-05],
+            [323.15, 0.94e-05],
+            [373.15, 1.04e-05],
+            [473.15, 1.21e-05],
+            [573.15, 1.37e-05],
+            [673.15, 1.53e-05],
+            [773.15, 1.69e-05],
+            [873.15, 1.84e-05],
+        ]
+    ),
+}
+MOLECULAR_DIAMETERS = {
+    "O2": 3.55e-10,
+    "N2": 3.7e-10,
+    "Ar": 3.58e-10,
+    "He": 2.15e-10,
+    "CO": 3.76e-10,
+    "CO2": 3.34e-10,
+    "H2": 2.71e-10,
+}  # in [m]
+MOLAR_MASSES = {
+    "O2": 31.998,
+    "N2": 28.014,
+    "Ar": 39.948,
+    "He": 4.002,
+    "CO": 28.010,
+    "CO2": 44.010,
+    "H2": 2.016,
+}  # in [g/mol]
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat/data_series.py` & `ixdat-0.2.9.dev3/src/ixdat/data_series.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,428 +1,428 @@
-"""This module defines the DataSeries class, the elementary data structure of ixdat
-
-An ixdat DataSeries is a wrapper around a numpy array containing the metadata needed
-to combine it with other DataSeries. Typically this means a reference to the time
-variable corresponding to the rows of the array. The time variable itself is a special
-case, TimeSeries, which must know its absolute (unix) timestamp.
-"""
-
-import numpy as np
-from .db import Saveable
-from .tools import tstamp_to_string
-from .units import Unit
-from .exceptions import AxisError, BuildError
-
-
-class DataSeries(Saveable):
-    """The base class for all numerical data representation in ixdat.
-
-    These class's objects are saved and loaded as rows in the data_series table
-    """
-
-    table_name = "data_series"
-    column_attrs = {"name", "unit_name", "data", "series_type"}
-    series_type = "series"
-
-    def __init__(self, name, unit_name, data):
-        """initialize a data series with its name, unit, and data (id handled by parent)
-
-        Args:
-            name (str): The name of the data series
-            unit_name (str): The name of the unit in which the data is stored
-            data (np.array): The numerical data
-        """
-        super().__init__()
-        self.name = name
-        self.unit = Unit(unit_name)
-        self._data = data
-
-    @classmethod
-    def from_dict(cls, obj_as_dict):
-        """Return the right type of DataSeries based on the info in its serialization"""
-        series_type = obj_as_dict.pop("series_type")
-        series_class = SERIES_CLASSES[series_type]
-        return series_class(**obj_as_dict)
-
-    def __repr__(self):
-        return f"{self.__class__.__name__}(id={self.id}, name='{self.name}')"
-
-    @property
-    def data(self):
-        """The data as a np.array, loaded the first time it is needed."""
-        if self._data is None:
-            self._data = self.load_data()  # inherited from Savable.
-        return self._data
-
-    @property
-    def unit_name(self):
-        """The name of the data series' unit"""
-        return self.unit.name
-
-    @property
-    def shape(self):
-        return self.data.shape
-
-    @property
-    def size(self):
-        return self.data.size
-
-
-class TimeSeries(DataSeries):
-    """Class to store time data. These are characterized by having a tstamp"""
-
-    extra_column_attrs = {"tstamps": {"tstamp"}}
-    series_type = "tseries"
-
-    def __init__(self, name, unit_name, data, tstamp):
-        """Initiate a TimeSeries with name, unit_name, data, and a tstamp (float)
-
-        Args (in addition to those of parent, :class:`.DataSeries`):
-            tstamp (float): The unix timestamp of the time at which t=0 in the data
-        """
-        super().__init__(name, unit_name, data)
-        self.tstamp = tstamp
-
-    def __str__(self):
-        """Return TimeSeries string representation"""
-        # On the form: TimeSeries: 'NAME'. Min, max: 12, 4000 [s] @ 22E18 14:34:55
-        return (
-            f"{self.__class__.__name__}: '{self.name}'. "
-            f"Min, max: {min(self.data):.0f}, {max(self.data):.0f} [{self.unit.name}] "
-            f"@ {tstamp_to_string(self.tstamp)}"
-        )
-
-    @property
-    def t(self):
-        return self.data
-
-    @property
-    def tseries(self):
-        """Trivially, a TimeSeries is its own TimeSeries"""
-        return self
-
-
-class Field(DataSeries):
-    """Class for storing multi-dimensional data spanning 'axes'
-
-    Characterized by a list of references to these axes, which are themselves also
-    DataSeries. This is represented in the extra linkers.
-    """
-
-    extra_linkers = {"field_axes": ("data_series", "a_ids")}
-    child_attrs = ["axes_series"]
-    series_type = "field"
-
-    def __init__(self, name, unit_name, data, a_ids=None, axes_series=None):
-        """Initiate the Field and check that the supplied axes make sense.
-
-        Args (in addition to those of parent, :class:`.DataSeries`):
-            a_ids (list of int): The ids of the corresponding axes DataSeries, if not
-                the series are not given directly as `axes_series`
-            axes_series (list of DataSeries): The DataSeries describing the axes which
-                the field's data spans, if available
-        """
-        super().__init__(name, unit_name, data)
-        N = len(a_ids) if a_ids is not None else len(axes_series)
-        self.N_dimensions = N
-        self._a_ids = a_ids if a_ids is not None else ([None] * N)
-        # TODO: This could probably be handled more nicely with PlaceHolderObjects
-        #   see: Measurement and
-        #   https://github.com/ixdat/ixdat/pull/1#discussion_r551518461
-        self._axes_series = axes_series if axes_series is not None else ([None] * N)
-        self._check_axes()  # raises an AxisError if something's wrong
-
-    def get_axis_id(self, axis_number):
-        """Return the id of the `axis_number`'th axis of the data"""
-        if self._axes_series[axis_number]:
-            return self._axes_series[axis_number].id
-        return self._a_ids[axis_number]
-
-    def get_axis_series(self, axis_number):
-        """Return the DataSeries of the `axis_number`'th axis of the data"""
-        if not self._axes_series[axis_number]:
-            self._axes_series[axis_number] = DataSeries.get(i=self._a_ids[axis_number])
-            # And so as not have two id's for the axis_number'th axis:
-            self._a_ids[axis_number] = None
-        return self._axes_series[axis_number]
-
-    @property
-    def a_ids(self):
-        """List of the id's of the axes spanned by the field"""
-        return [self.get_axis_id(n) for n in range(self.N_dimensions)]
-
-    @property
-    def axes_series(self):
-        """List of the DataSeries defining the axes spanned by the field"""
-        return [self.get_axis_series(n) for n in range(self.N_dimensions)]
-
-    def _check_axes(self):
-        """Check that there are no contradictions in the Field's axes_series and id's"""
-        N = self.N_dimensions
-        if len(self._a_ids) != N:
-            raise AxisError(
-                f"{self!r} is {N}-D but initiated with {len(self._a_ids)} axis id's"
-            )
-        if len(self._axes_series) != N:
-            raise AxisError(
-                f"{self!r} is {N}-D but initiated with {len(self._axes_series)} axes"
-            )
-        for n, (a_id, axis_series) in enumerate(zip(self._a_ids, self._axes_series)):
-            if a_id is not None and axis_series is not None and a_id != axis_series.id:
-                raise AxisError(
-                    f"{self!r} initiated with contradicting id's for {n}'th axis"
-                )
-            elif a_id is None and axis_series is None:
-                raise AxisError(
-                    f"{self!r} has no axis id for series or id for its {n}'th axis"
-                )
-
-    @property
-    def data(self):
-        """When loading data, Field checks that its dimensions match its # of axes"""
-        if self._data is None:
-            self._data = self.load_data()
-            if len(self._data.shape) != self.N_dimensions:
-                raise AxisError(
-                    f"{self!r} has {self.N_dimensions} axes but its data is "
-                    f"{len(self._data.shape)}-dimensional."
-                )
-        return self._data.copy()  # TODO: make data series data immutable with numpy flag
-        # see: https://github.com/ixdat/ixdat/pull/101/files#r1126172936
-
-    @property
-    def tstamp(self):
-        """The unix time corresponding to t=0 for the time-resolved axis of the Field
-
-        The timestamp of a Field is the timestamp of its TimeSeries or ValueSeries
-        """
-        for s in self.axes_series:
-            if isinstance(s, (ValueSeries, TimeSeries)):
-                return s.tstamp
-
-
-class ValueSeries(Field):
-    """Class to store scalar values that are measured over time.
-
-    Characterized by a reference to the corresponding time series. This reference is
-    represented in relational databases as a row in an auxiliary linker table
-    """
-
-    series_type = "vseries"
-
-    def __init__(
-        self,
-        name,
-        unit_name,
-        data,
-        t_id=None,
-        tseries=None,
-        a_ids=None,
-        axes_series=None,
-    ):
-        """Initiate a ValueSeries with a TimeSeries or a reference thereto
-
-        Args (in addition to those of :class:`.Field`):
-            t_id (int): The id of the corresponding TimeSeries, if not given directly
-                (can also be supplied as `a_ids[0]`)
-            tseries (TimeSeries): The corresponding TimeSeries, if available
-                (can also be supplied as `axes_series[0]`)
-        """
-        a_ids = a_ids or [t_id]
-        axes_series = axes_series or [tseries]
-        super().__init__(name, unit_name, data, a_ids, axes_series)
-        # TODO: This could probably be handled more nicely with PlaceHolderObjects
-        #   see: Measurement and
-        #   https://github.com/ixdat/ixdat/pull/1#discussion_r551518461
-
-    def __str__(self):
-        """Return string representation"""
-        # Return a string representation on the form:
-        # ValueSeries: 'NAME'. Min, max: -1.23, 4.56 [V]
-        return (
-            f"{self.__class__.__name__}: '{self.name}'. "
-            f"Min, max: {min(self.data):.1e}, {max(self.data):.1e} [{self.unit.name}]"
-        )
-
-    @property
-    def tseries(self):
-        return self.axes_series[0]
-
-    @property
-    def t_id(self):
-        """int: the id of the TimeSeries"""
-        if self._axes_seriess:
-            return self.tseries.id
-        return self.a_ids[0]
-
-    @property
-    def v(self):
-        """The value as a 1-d np array"""
-        return self.data
-
-    @property
-    def t(self):
-        """The measurement times as a 1-d np array"""
-        return self.tseries.data
-
-    @property
-    def tstamp(self):
-        """The timestamp, from the TimeSeries of the ValueSeries"""
-        return self.tseries.tstamp
-
-    def __hash__(self):
-        return super().__hash__()
-
-
-class ConstantValue(ValueSeries):
-    """This is a stand-in for a VSeries for when we know the value is constant"""
-
-    series_type = "constantvalue"
-
-    def __init__(self, *args, **kwargs):
-        super().__init__(*args, **kwargs)
-        self._expanded_data = None
-
-    @property
-    def data(self):
-        if self._expanded_data is None:
-            if self._data is None:
-                self._data = self.load_data()  # inherited from Savable.
-            self._expanded_data = np.ones(self.t.shape) * self._data
-        return self._expanded_data
-
-
-SERIES_CLASSES = {
-    cls.series_type: cls
-    for cls in [DataSeries, TimeSeries, Field, ValueSeries, ConstantValue]
-}
-
-
-def append_series(series_list, sorted=True, name=None, tstamp=None):
-    """Return series appending series_list relative to series_list[0].tseries.tstamp
-
-    Args:
-        series_list (list of Series): The series to append (must all be of same type)
-        sorted (bool): Whether to sort the data so that time only goes forward
-        name (str): Name to give the appended series. Defaults to series_list[0].name
-        tstamp (unix tstamp): The t=0 of the returned series or its TimeSeries.
-    """
-    s0 = series_list[0]
-    if isinstance(s0, TimeSeries):
-        return append_tseries(series_list, sorted=sorted, name=name, tstamp=tstamp)
-    elif isinstance(s0, ValueSeries):
-        return append_vseries_by_time(
-            series_list, sorted=sorted, name=name, tstamp=tstamp
-        )
-    raise BuildError(
-        f"An algorithm of append_series for series like {s0!r} is not yet implemented"
-    )
-
-
-def append_vseries_by_time(series_list, sorted=True, name=None, tstamp=None):
-    """Return new ValueSeries with the data in series_list appended
-
-    Args:
-        series_list (list of ValueSeries): The value series to append
-        sorted (bool): Whether to sort the data so that time only goes forward
-        name (str): Name to give the appended series. Defaults to series_list[0].name
-        tstamp (unix tstamp): The t=0 of the returned ValueSeries' TimeSeries.
-    """
-    name = name or series_list[0].name
-    cls = series_list[0].__class__
-    unit = series_list[0].unit
-    data = np.array([])
-    tseries_list = [s.tseries for s in series_list]
-    if not all(isinstance(ts, TimeSeries) for ts in tseries_list):
-        raise BuildError(
-            f"can't append {series_list} w incompatible tseries list = {tseries_list}"
-        )
-    tseries, sort_indeces = append_tseries(
-        tseries_list, sorted=sorted, return_sort_indeces=True, tstamp=tstamp
-    )
-
-    for s in series_list:
-        data = np.append(data, s.data)
-    if sorted:
-        data = data[sort_indeces]
-
-    return cls(name=name, unit_name=unit.name, data=data, tseries=tseries)
-
-
-def append_tseries(
-    series_list, sorted=True, return_sort_indeces=False, name=None, tstamp=None
-):
-    """Return new TimeSeries with the data appended.
-
-    Args:
-        series_list (list of TimeSeries): The time series to append
-        sorted (bool): Whether to sort the data so that time only goes forward
-        return_sort_indeces (bool): Whether to return the indeces that sort the data
-        name (str): Name to give the appended series. Defaults to series_list[0].name
-        tstamp (unix tstamp): The t=0 of the returned TimeSeries.
-    """
-    name = name or series_list[0].name
-    cls = series_list[0].__class__
-    unit = series_list[0].unit
-    tstamp = tstamp or series_list[0].tstamp
-    data = np.array([])
-
-    for s in series_list:
-        if not (s.unit == unit and s.__class__ == cls):
-            raise BuildError(f"can't append {series_list}")
-        data = np.append(data, s.data + s.tstamp - tstamp)
-
-    if sorted:
-        sort_indices = np.argsort(data)
-        data = data[sort_indices]
-    else:
-        sort_indices = None
-
-    tseries = cls(name=name, unit_name=unit.name, data=data, tstamp=tstamp)
-    if return_sort_indeces:
-        return tseries, sort_indices
-    return tseries
-
-
-def time_shifted(series, tstamp=None):
-    """Return a series with the time shifted to be relative to tstamp"""
-    if tstamp is None:
-        return series
-    if tstamp == series.tstamp:
-        return series
-    cls = series.__class__
-    if isinstance(series, TimeSeries):
-        new_data = series.data + series.tstamp - tstamp  # shift the time.
-        return cls(
-            name=series.name,
-            unit_name=series.unit.name,
-            data=new_data,
-            tstamp=tstamp,
-        )
-    elif isinstance(series, ValueSeries):
-        series = cls(
-            name=series.name,
-            unit_name=series.unit.name,
-            data=series.data,
-            tseries=time_shifted(series.tseries, tstamp=tstamp),
-        )
-    return series
-
-
-def get_tspans_from_mask(t, mask):
-    """Return a list of tspans for time intervals remaining when mask is applied to t
-
-    FIXME: This is pure numpy manipulation and probably belongs somewhere else.
-    """
-    mask_prev = np.append(False, mask[:-1])  # the mask shifted right by one
-    mask_next = np.append(mask[1:], False)  # the mask shifted left by one
-    # An array that is True where intervals meeting the criteria start:
-    #   (This includes at [0] if mask[0] is True.)
-    interval_starts_here = np.logical_and(np.logical_not(mask_prev), mask)
-    # An array that is True where intervals meeting the criteria finish:
-    #   (This includes at [-1] if mask[-1] is True.)
-    interval_ends_here = np.logical_and(mask, np.logical_not(mask_next))
-
-    t_starts = list(t[interval_starts_here])  # the start times implied by the mask
-    t_ends = list(t[interval_ends_here])  # the finish times implied by the mask
-    tspans = zip(t_starts, t_ends)  # and, the timespans where the criteria is met!
-    return tspans
+"""This module defines the DataSeries class, the elementary data structure of ixdat
+
+An ixdat DataSeries is a wrapper around a numpy array containing the metadata needed
+to combine it with other DataSeries. Typically this means a reference to the time
+variable corresponding to the rows of the array. The time variable itself is a special
+case, TimeSeries, which must know its absolute (unix) timestamp.
+"""
+
+import numpy as np
+from .db import Saveable
+from .tools import tstamp_to_string
+from .units import Unit
+from .exceptions import AxisError, BuildError
+
+
+class DataSeries(Saveable):
+    """The base class for all numerical data representation in ixdat.
+
+    These class's objects are saved and loaded as rows in the data_series table
+    """
+
+    table_name = "data_series"
+    column_attrs = {"name", "unit_name", "data", "series_type"}
+    series_type = "series"
+
+    def __init__(self, name, unit_name, data):
+        """initialize a data series with its name, unit, and data (id handled by parent)
+
+        Args:
+            name (str): The name of the data series
+            unit_name (str): The name of the unit in which the data is stored
+            data (np.array): The numerical data
+        """
+        super().__init__()
+        self.name = name
+        self.unit = Unit(unit_name)
+        self._data = data
+
+    @classmethod
+    def from_dict(cls, obj_as_dict):
+        """Return the right type of DataSeries based on the info in its serialization"""
+        series_type = obj_as_dict.pop("series_type")
+        series_class = SERIES_CLASSES[series_type]
+        return series_class(**obj_as_dict)
+
+    def __repr__(self):
+        return f"{self.__class__.__name__}(id={self.id}, name='{self.name}')"
+
+    @property
+    def data(self):
+        """The data as a np.array, loaded the first time it is needed."""
+        if self._data is None:
+            self._data = self.load_data()  # inherited from Savable.
+        return self._data
+
+    @property
+    def unit_name(self):
+        """The name of the data series' unit"""
+        return self.unit.name
+
+    @property
+    def shape(self):
+        return self.data.shape
+
+    @property
+    def size(self):
+        return self.data.size
+
+
+class TimeSeries(DataSeries):
+    """Class to store time data. These are characterized by having a tstamp"""
+
+    extra_column_attrs = {"tstamps": {"tstamp"}}
+    series_type = "tseries"
+
+    def __init__(self, name, unit_name, data, tstamp):
+        """Initiate a TimeSeries with name, unit_name, data, and a tstamp (float)
+
+        Args (in addition to those of parent, :class:`.DataSeries`):
+            tstamp (float): The unix timestamp of the time at which t=0 in the data
+        """
+        super().__init__(name, unit_name, data)
+        self.tstamp = tstamp
+
+    def __str__(self):
+        """Return TimeSeries string representation"""
+        # On the form: TimeSeries: 'NAME'. Min, max: 12, 4000 [s] @ 22E18 14:34:55
+        return (
+            f"{self.__class__.__name__}: '{self.name}'. "
+            f"Min, max: {min(self.data):.0f}, {max(self.data):.0f} [{self.unit.name}] "
+            f"@ {tstamp_to_string(self.tstamp)}"
+        )
+
+    @property
+    def t(self):
+        return self.data
+
+    @property
+    def tseries(self):
+        """Trivially, a TimeSeries is its own TimeSeries"""
+        return self
+
+
+class Field(DataSeries):
+    """Class for storing multi-dimensional data spanning 'axes'
+
+    Characterized by a list of references to these axes, which are themselves also
+    DataSeries. This is represented in the extra linkers.
+    """
+
+    extra_linkers = {"field_axes": ("data_series", "a_ids")}
+    child_attrs = ["axes_series"]
+    series_type = "field"
+
+    def __init__(self, name, unit_name, data, a_ids=None, axes_series=None):
+        """Initiate the Field and check that the supplied axes make sense.
+
+        Args (in addition to those of parent, :class:`.DataSeries`):
+            a_ids (list of int): The ids of the corresponding axes DataSeries, if not
+                the series are not given directly as `axes_series`
+            axes_series (list of DataSeries): The DataSeries describing the axes which
+                the field's data spans, if available
+        """
+        super().__init__(name, unit_name, data)
+        N = len(a_ids) if a_ids is not None else len(axes_series)
+        self.N_dimensions = N
+        self._a_ids = a_ids if a_ids is not None else ([None] * N)
+        # TODO: This could probably be handled more nicely with PlaceHolderObjects
+        #   see: Measurement and
+        #   https://github.com/ixdat/ixdat/pull/1#discussion_r551518461
+        self._axes_series = axes_series if axes_series is not None else ([None] * N)
+        self._check_axes()  # raises an AxisError if something's wrong
+
+    def get_axis_id(self, axis_number):
+        """Return the id of the `axis_number`'th axis of the data"""
+        if self._axes_series[axis_number]:
+            return self._axes_series[axis_number].id
+        return self._a_ids[axis_number]
+
+    def get_axis_series(self, axis_number):
+        """Return the DataSeries of the `axis_number`'th axis of the data"""
+        if not self._axes_series[axis_number]:
+            self._axes_series[axis_number] = DataSeries.get(i=self._a_ids[axis_number])
+            # And so as not have two id's for the axis_number'th axis:
+            self._a_ids[axis_number] = None
+        return self._axes_series[axis_number]
+
+    @property
+    def a_ids(self):
+        """List of the id's of the axes spanned by the field"""
+        return [self.get_axis_id(n) for n in range(self.N_dimensions)]
+
+    @property
+    def axes_series(self):
+        """List of the DataSeries defining the axes spanned by the field"""
+        return [self.get_axis_series(n) for n in range(self.N_dimensions)]
+
+    def _check_axes(self):
+        """Check that there are no contradictions in the Field's axes_series and id's"""
+        N = self.N_dimensions
+        if len(self._a_ids) != N:
+            raise AxisError(
+                f"{self!r} is {N}-D but initiated with {len(self._a_ids)} axis id's"
+            )
+        if len(self._axes_series) != N:
+            raise AxisError(
+                f"{self!r} is {N}-D but initiated with {len(self._axes_series)} axes"
+            )
+        for n, (a_id, axis_series) in enumerate(zip(self._a_ids, self._axes_series)):
+            if a_id is not None and axis_series is not None and a_id != axis_series.id:
+                raise AxisError(
+                    f"{self!r} initiated with contradicting id's for {n}'th axis"
+                )
+            elif a_id is None and axis_series is None:
+                raise AxisError(
+                    f"{self!r} has no axis id for series or id for its {n}'th axis"
+                )
+
+    @property
+    def data(self):
+        """When loading data, Field checks that its dimensions match its # of axes"""
+        if self._data is None:
+            self._data = self.load_data()
+            if len(self._data.shape) != self.N_dimensions:
+                raise AxisError(
+                    f"{self!r} has {self.N_dimensions} axes but its data is "
+                    f"{len(self._data.shape)}-dimensional."
+                )
+        return self._data.copy()  # TODO: make data series data immutable with numpy flag
+        # see: https://github.com/ixdat/ixdat/pull/101/files#r1126172936
+
+    @property
+    def tstamp(self):
+        """The unix time corresponding to t=0 for the time-resolved axis of the Field
+
+        The timestamp of a Field is the timestamp of its TimeSeries or ValueSeries
+        """
+        for s in self.axes_series:
+            if isinstance(s, (ValueSeries, TimeSeries)):
+                return s.tstamp
+
+
+class ValueSeries(Field):
+    """Class to store scalar values that are measured over time.
+
+    Characterized by a reference to the corresponding time series. This reference is
+    represented in relational databases as a row in an auxiliary linker table
+    """
+
+    series_type = "vseries"
+
+    def __init__(
+        self,
+        name,
+        unit_name,
+        data,
+        t_id=None,
+        tseries=None,
+        a_ids=None,
+        axes_series=None,
+    ):
+        """Initiate a ValueSeries with a TimeSeries or a reference thereto
+
+        Args (in addition to those of :class:`.Field`):
+            t_id (int): The id of the corresponding TimeSeries, if not given directly
+                (can also be supplied as `a_ids[0]`)
+            tseries (TimeSeries): The corresponding TimeSeries, if available
+                (can also be supplied as `axes_series[0]`)
+        """
+        a_ids = a_ids or [t_id]
+        axes_series = axes_series or [tseries]
+        super().__init__(name, unit_name, data, a_ids, axes_series)
+        # TODO: This could probably be handled more nicely with PlaceHolderObjects
+        #   see: Measurement and
+        #   https://github.com/ixdat/ixdat/pull/1#discussion_r551518461
+
+    def __str__(self):
+        """Return string representation"""
+        # Return a string representation on the form:
+        # ValueSeries: 'NAME'. Min, max: -1.23, 4.56 [V]
+        return (
+            f"{self.__class__.__name__}: '{self.name}'. "
+            f"Min, max: {min(self.data):.1e}, {max(self.data):.1e} [{self.unit.name}]"
+        )
+
+    @property
+    def tseries(self):
+        return self.axes_series[0]
+
+    @property
+    def t_id(self):
+        """int: the id of the TimeSeries"""
+        if self._axes_seriess:
+            return self.tseries.id
+        return self.a_ids[0]
+
+    @property
+    def v(self):
+        """The value as a 1-d np array"""
+        return self.data
+
+    @property
+    def t(self):
+        """The measurement times as a 1-d np array"""
+        return self.tseries.data
+
+    @property
+    def tstamp(self):
+        """The timestamp, from the TimeSeries of the ValueSeries"""
+        return self.tseries.tstamp
+
+    def __hash__(self):
+        return super().__hash__()
+
+
+class ConstantValue(ValueSeries):
+    """This is a stand-in for a VSeries for when we know the value is constant"""
+
+    series_type = "constantvalue"
+
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        self._expanded_data = None
+
+    @property
+    def data(self):
+        if self._expanded_data is None:
+            if self._data is None:
+                self._data = self.load_data()  # inherited from Savable.
+            self._expanded_data = np.ones(self.t.shape) * self._data
+        return self._expanded_data
+
+
+SERIES_CLASSES = {
+    cls.series_type: cls
+    for cls in [DataSeries, TimeSeries, Field, ValueSeries, ConstantValue]
+}
+
+
+def append_series(series_list, sorted=True, name=None, tstamp=None):
+    """Return series appending series_list relative to series_list[0].tseries.tstamp
+
+    Args:
+        series_list (list of Series): The series to append (must all be of same type)
+        sorted (bool): Whether to sort the data so that time only goes forward
+        name (str): Name to give the appended series. Defaults to series_list[0].name
+        tstamp (unix tstamp): The t=0 of the returned series or its TimeSeries.
+    """
+    s0 = series_list[0]
+    if isinstance(s0, TimeSeries):
+        return append_tseries(series_list, sorted=sorted, name=name, tstamp=tstamp)
+    elif isinstance(s0, ValueSeries):
+        return append_vseries_by_time(
+            series_list, sorted=sorted, name=name, tstamp=tstamp
+        )
+    raise BuildError(
+        f"An algorithm of append_series for series like {s0!r} is not yet implemented"
+    )
+
+
+def append_vseries_by_time(series_list, sorted=True, name=None, tstamp=None):
+    """Return new ValueSeries with the data in series_list appended
+
+    Args:
+        series_list (list of ValueSeries): The value series to append
+        sorted (bool): Whether to sort the data so that time only goes forward
+        name (str): Name to give the appended series. Defaults to series_list[0].name
+        tstamp (unix tstamp): The t=0 of the returned ValueSeries' TimeSeries.
+    """
+    name = name or series_list[0].name
+    cls = series_list[0].__class__
+    unit = series_list[0].unit
+    data = np.array([])
+    tseries_list = [s.tseries for s in series_list]
+    if not all(isinstance(ts, TimeSeries) for ts in tseries_list):
+        raise BuildError(
+            f"can't append {series_list} w incompatible tseries list = {tseries_list}"
+        )
+    tseries, sort_indeces = append_tseries(
+        tseries_list, sorted=sorted, return_sort_indeces=True, tstamp=tstamp
+    )
+
+    for s in series_list:
+        data = np.append(data, s.data)
+    if sorted:
+        data = data[sort_indeces]
+
+    return cls(name=name, unit_name=unit.name, data=data, tseries=tseries)
+
+
+def append_tseries(
+    series_list, sorted=True, return_sort_indeces=False, name=None, tstamp=None
+):
+    """Return new TimeSeries with the data appended.
+
+    Args:
+        series_list (list of TimeSeries): The time series to append
+        sorted (bool): Whether to sort the data so that time only goes forward
+        return_sort_indeces (bool): Whether to return the indeces that sort the data
+        name (str): Name to give the appended series. Defaults to series_list[0].name
+        tstamp (unix tstamp): The t=0 of the returned TimeSeries.
+    """
+    name = name or series_list[0].name
+    cls = series_list[0].__class__
+    unit = series_list[0].unit
+    tstamp = tstamp or series_list[0].tstamp
+    data = np.array([])
+
+    for s in series_list:
+        if not (s.unit == unit and s.__class__ == cls):
+            raise BuildError(f"can't append {series_list}")
+        data = np.append(data, s.data + s.tstamp - tstamp)
+
+    if sorted:
+        sort_indices = np.argsort(data)
+        data = data[sort_indices]
+    else:
+        sort_indices = None
+
+    tseries = cls(name=name, unit_name=unit.name, data=data, tstamp=tstamp)
+    if return_sort_indeces:
+        return tseries, sort_indices
+    return tseries
+
+
+def time_shifted(series, tstamp=None):
+    """Return a series with the time shifted to be relative to tstamp"""
+    if tstamp is None:
+        return series
+    if tstamp == series.tstamp:
+        return series
+    cls = series.__class__
+    if isinstance(series, TimeSeries):
+        new_data = series.data + series.tstamp - tstamp  # shift the time.
+        return cls(
+            name=series.name,
+            unit_name=series.unit.name,
+            data=new_data,
+            tstamp=tstamp,
+        )
+    elif isinstance(series, ValueSeries):
+        series = cls(
+            name=series.name,
+            unit_name=series.unit.name,
+            data=series.data,
+            tseries=time_shifted(series.tseries, tstamp=tstamp),
+        )
+    return series
+
+
+def get_tspans_from_mask(t, mask):
+    """Return a list of tspans for time intervals remaining when mask is applied to t
+
+    FIXME: This is pure numpy manipulation and probably belongs somewhere else.
+    """
+    mask_prev = np.append(False, mask[:-1])  # the mask shifted right by one
+    mask_next = np.append(mask[1:], False)  # the mask shifted left by one
+    # An array that is True where intervals meeting the criteria start:
+    #   (This includes at [0] if mask[0] is True.)
+    interval_starts_here = np.logical_and(np.logical_not(mask_prev), mask)
+    # An array that is True where intervals meeting the criteria finish:
+    #   (This includes at [-1] if mask[-1] is True.)
+    interval_ends_here = np.logical_and(mask, np.logical_not(mask_next))
+
+    t_starts = list(t[interval_starts_here])  # the start times implied by the mask
+    t_ends = list(t[interval_ends_here])  # the finish times implied by the mask
+    tspans = zip(t_starts, t_ends)  # and, the timespans where the criteria is met!
+    return tspans
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat/db.py` & `ixdat-0.2.9.dev3/src/ixdat/db.py`

 * *Ordering differences only*

 * *Files 11% similar despite different names*

```diff
@@ -1,490 +1,490 @@
-"""This module contains the classes which pass on database functionality
-
-Note on terminology:
-    In ixdat, we seek to use the following naming conventions:
-        `load` grabs *an object* from a database backend given its class or table name
-            and the name of the specific object desired (see DataBase.load).
-        `load_xxx` grabs `xxx` from a database backend given the object for which
-            xxx is desired (see DataBase.load_object_data).
-        `get` grabs *an object* from a database backend given its class or table name and
-            the princple key (the id) of the row in the corresponding table
-        `get_xxx` grabs `xxx` from a database backend given the principle key (the id) of
-            the row in the table corresponding to xxx (see `Database.get`)
-
-        So `load` works by `name` or existing object, while `get` works by `id`.
-        `get_xx` is also used as the counterpart to `set_xx` to grab `xx`, typically a
-        managed attribute, from an object in memory.
-
-        `load` and `get` convention holds vertically - i.e. the Backend, the DataBase,
-            up through the Saveable parent class for all ixdat classes corresponding to
-            database tables have `load` and `get` methods which call downwards. TODO.
-    see: https://github.com/ixdat/ixdat/pull/1#discussion_r546400793
-"""
-
-from .exceptions import DataBaseError
-from .backends import BACKEND_CLASSES, database_backends
-from .tools import thing_is_close
-
-
-class DataBase:
-    """This class is a kind of middle-man between a Backend and a Saveable class
-
-    The reason for a middle man here is that it enables different databases (backends)
-    to be switched between and kept track of in a single ixdat session.
-
-    The DataBase should be initialized with a backend, and by default uses DirBackend,
-    which saves to a folder.
-    """
-
-    def __init__(self, backend=None):
-        """Initialize the database with its backend"""
-        self.backend = backend or database_backends["directory"]
-        self.new_object_backend = "none"
-
-    def save(self, obj):
-        """Save a Saveable object with the backend"""
-        return self.backend.save(obj)
-
-    def get(self, cls, i, backend=None):
-        """Select and return object of Saveable class cls with id=i from the backend"""
-        backend = backend or self.backend
-        obj = backend.get(cls, i)
-        # obj will already have obj.id = i and obj.backend = self.backend from backend
-        return obj
-
-    def load(self, cls, name):
-        """Select and return object of Saveable class cls with name=name from backend"""
-
-    def load_obj_data(self, obj):
-        """Load and return the numerical data (obj.data) for a Saveable object"""
-        return self.backend.load_obj_data(obj)
-
-    def set_backend(self, backend_name, **db_kwargs):
-        """Change backend to the class given by backend_name initiated with db_kwargs"""
-        if not isinstance(backend_name, str):
-            # Then we assume that it is the backend itself, not the backend name
-            self.backend = backend_name
-        elif backend_name in BACKEND_CLASSES:
-            BackendClass = BACKEND_CLASSES[backend_name]
-            self.backend = BackendClass(**db_kwargs)
-        else:
-            raise NotImplementedError(
-                f"ixdat doesn't recognize db_name = '{backend_name}'. If this is a new"
-                "database backend, make sure it is added to the DATABASE_BACKENDS "
-                "constant in ixdat.backends."
-                "Or manually set it directly with DB.backend = <my_backend>"
-            )
-        return self.backend
-
-
-DB = DataBase()  # initate the database. It functions as a global "constant"
-
-
-# THIS is proposed as the main mechanism for changing backend, to make
-# the shared global nature of it explicit. And in any case, the user
-# will never have to deal with the db, except when changing it away
-# from the default. This function should probably be exposed in the
-# top name space.
-
-
-def change_database(db_name, **db_kwargs):
-    """Change the backend specifying which database objects are saved to/loaded from"""
-    DB.set_backend(db_name, **db_kwargs)
-
-
-def get_database_name():
-    """Return the name of the class of which the database backend is an instance"""
-    return DB.backend.__class__.__name__
-
-
-class Saveable:
-    """Base class for table-representing classes implementing database functionality.
-
-    This enables seamless interoperability between database tables and ixdat classes.
-    Classes inheriting from this need to provide just a bit of info to define the
-    corresponding table, and then saving and loading should just work.
-
-    At a minimum, the `table_name` and `column_attrs` class attributes need to be
-    overwritten in inheriting classes to define the name and columns of the main
-    corresponding table. If an auxiliary table is needed to store lists of references
-    as rows, this should be represented in `linkers`. Sub-sub classes can use
-    `extra_column_attrs` to add extra columns via an auxiliary table without changing
-    the main table name.
-
-    ixdat is lazy, only loading things when needed. Correspondingly, all of the columns
-    of table mentioned above should refer to (lists of) id's and not actual objects of
-    other ixdat classes.
-
-    The class attributes (defined before __init__) and object attributes (defined in
-    __init__) are described here. See the other methods and the relevant inheriting
-    classes for more info.
-
-    Class attributes:
-        db (DataBase): the database, DB, which has the save, get, and load_data methods
-        table_name (str): The name of the table or folder in which objects are saved
-        column_attrs (set of str): {attr} where attr is the name of the column in the
-            table and also the name of the attribute of the class.
-        extra_column_attrs (dict): {table_name: {attr}} for auxiliary tables
-            to represent the "extra" attributes, for double-inheriting classes.
-        linkers (dict): {table_name: (reference_table, attr)} for defining
-            the connections between objects.
-
-    Object attributes:
-        backend (Backend): the backend where the object is saved. For a
-            new, un-saved, object, this is "memory".
-        _id (int): the principle key of the object, also accessible as `id`. This should
-            be set explicitly in the backend when loading an object. For objects
-            initiated directly in the session, it will become the id provided by the
-            memory backend, which just counts objects of each table starting with 1.
-            TODO: consider renaming.
-                See: https://github.com/ixdat/ixdat/pull/1#discussion_r546434676
-        name (str): The name of the object. `name` should be a column in ixdat tables.
-    """
-
-    db = DB
-    table_name = None  # THIS MUST BE OVERWRITTEN IN INHERITING CLASSES
-    # TODO: restructure column_attrs, extra_column_attrs, extra_linkers so that they are
-    #  sufficient to fully define the tables in an SQL backend
-    column_attrs = None  # THIS SHOULD BE OVERWRITTEN IN INHERITING CLASSES
-    extra_column_attrs = None  # THIS CAN BE OVERWRITTEN IN INHERITING CLASSES
-    extra_linkers = None  # THIS CAN BE OVERWRITTEN IN INHERITING CLASSES
-    # TODO: derive child_attrs somehow from the above class attributes, and have it in
-    #   a way where it's easy to tell which id goes with which attribute, i.e. s_ids
-    #   goes with series_list
-    child_attrs = None  # THIS SHOULD BE OVERWRITTEN IN CLASSES WITH DATA REFERENCES
-
-    def __init__(self, backend=None, **self_as_dict):
-        """Initialize a Saveable object from its dictionary serialization
-
-        This is the default behavior, and should be overwritten using an argument-free
-        call to super().__init__() in inheriting classes.
-
-        Args:
-            self_as_dict: all key-word arguments are by default set to object attributes
-        """
-        for attr, value in self_as_dict:
-            setattr(self, attr, value)
-        if self_as_dict and not self.column_attrs:
-            self.column_attrs = {attr: attr for attr in self_as_dict.keys()}
-        self._backend = None
-        self.backend = backend  # backend's setter will look up the backend name
-        self._id = None  # SHOULD BE SET AFTER THE __INIT__ FOR LOADED OBJECTS
-        self.name = None  # MUST BE SET IN THE __INIT__ OF INHERITING CLASSES
-
-    def __repr__(self):
-        return f"{self.__class__.__name__}(id={self.id}, name='{self.name}')"
-
-    @property
-    def id(self):
-        """The principle-key identifier. Set by backend or counted in memory."""
-        if not self._id:
-            if self.backend_type in ("none", "memory"):
-                self._id = self.backend.get_next_available_id(self.table_name, obj=self)
-                # TODO: Wouldn't it be better if the backend was always asked for the
-                #   ID by Saveable.__init__ ?
-            else:
-                raise DataBaseError(
-                    f"{self!r} comes from {self.backend_name} "
-                    "but did not get an id from its backend."
-                )
-        return self._id
-
-    @property
-    def short_identity(self):
-        """short_identity is the backend if different from the active backend and the id
-
-        FIXME: The overloaded return here is annoying and dangerous, but necessary for
-          `Measurement.from_dict(m.as_dict())` to work as a copy, since the call to
-          `fill_object_list` has to specify where the objects represented by
-          PlaceHolderObjects live. Note that calling save() on a Saveable object will
-          turn the backends into DB.backend, so this will only give id's when saving.
-        This is (usually) sufficient to tell if two objects refer to the same thing,
-        when used together with the class attribute table_name
-        """
-        if self.backend is DB.backend:
-            return self.id
-        return self.backend, self.id
-
-    @property
-    def full_identity(self):
-        """The full immutable object short_identity as (str, str, str, int)
-
-        Specifically: (backend_type, backend.address, table_name, id)
-        """
-        return self.backend_type, self.backend.address, self.table_name, self.id
-
-    @property
-    def backend(self):
-        """The backend the Saveable object was loaded from or last saved to."""
-        if not self._backend:
-            self._backend = database_backends["none"]
-        return self._backend
-
-    @backend.setter
-    def backend(self, new_backend):
-        """"""
-        new_backend = new_backend or DB.new_object_backend
-        if isinstance(new_backend, str):
-            if new_backend in database_backends:
-                new_backend = database_backends[new_backend]
-            elif new_backend in BACKEND_CLASSES:
-                new_backend = BACKEND_CLASSES[new_backend]()
-            else:
-                print(f"WARNING! {self} given unrecognized backend = {new_backend}")
-        self._backend = new_backend
-
-    @property
-    def backend_name(self):
-        """The name of the backend in which self has been saved to / loaded from"""
-        return self.backend.name
-
-    @property
-    def backend_type(self):
-        return self.backend.backend_type
-
-    def set_id(self, i):
-        """Backends set obj.id here after loading/saving a Saveable obj"""
-        self._id = i
-
-    def set_backend(self, backend):
-        """Backends set obj.backend here after loading/saving a Saveable obj"""
-        self.backend = backend
-
-    def get_main_dict(self, exclude=None):
-        """Return dict: serializition only of the row of the object's main table
-
-        Args:
-            exclude (list): List of attribute names to leave out of the dict
-        """
-        exclude = exclude or []
-        if self.column_attrs is None:
-            raise DataBaseError(
-                f"{self!r} can't be serialized because the class "
-                f"{self.__class__.__name__} hasn't defined column_attrs"
-            )
-        self_as_dict = {  # FIXME: probably better as loop, fix with table definitions.
-            attr: getattr(self, attr)
-            for attr in self.column_attrs
-            if attr not in exclude
-        }
-        return self_as_dict
-
-    def as_dict(self, exclude=None):
-        """Return dict: serialization of the object main and auxiliary tables"""
-
-        # So that a new object initiated using the dictionary returned here
-        #   can find objects referenced by identities (i.e. s_ids for series_list),
-        #   we need to make sure they are saved in a real backend. Before adding the
-        #   id's to a list.
-        # FIXME: There would be a more precise and elegant way to do this if the id's
-        #   and corresponding attributes could be connected through class attributes.
-        #   To do with table definitions.
-        if self.child_attrs:
-            for child_object_list_name in self.child_attrs:
-                for child_obj in getattr(self, child_object_list_name):
-                    if child_obj.backend is database_backends["none"]:
-                        database_backends["memory"].save(child_obj)
-
-        exclude = exclude or []
-        self_as_dict = self.get_main_dict(exclude=exclude)
-        if self.extra_column_attrs:
-            # FIXME: comprehension best as loop. Will be redone with proper table defs.
-            aux_tables_dict = {
-                table_name: {
-                    attr: getattr(self, attr) for attr in extras if attr not in exclude
-                }
-                for table_name, extras in self.extra_column_attrs.items()
-            }
-            for aux_dict in aux_tables_dict.values():
-                self_as_dict.update(**aux_dict)
-        if self.extra_linkers:
-            # FIXME: comprehension best as loop. Will be redone with proper table defs.
-            linker_tables_dict = {
-                (table_name, linked_table_name): {attr: getattr(self, attr)}
-                for table_name, (linked_table_name, attr) in self.extra_linkers.items()
-                if attr not in exclude
-            }
-            for linked_attrs in linker_tables_dict.values():
-                self_as_dict.update(**linked_attrs)
-
-        return self_as_dict
-
-    def __eq__(self, other):
-        """Return whether self is functionally equivalent to other
-
-        This means that everything in the dictionary representation is either equal
-        or close enough, and that every owned Saveable object in self and other are
-        equal by the same condition.
-
-        FIXME: as_dict() should perhaps be an ordered dict. That way we could ensure
-            that the order of the checks, in general, and in particular of the
-            property names further down, is intentional to keep cheap result determining
-            comparisons first and expensive ones last, for performance reasons
-        """
-        if self is other:
-            # If they're actually the same object of course they're equal.
-            return True
-        if self.__class__ is not other.__class__:
-            # If they're not the same class, they are not equal
-            return False
-        # Otherwise we compare their dictionary representations.
-        self_as_dict = self.as_dict()
-        other_as_dict = other.as_dict()
-        if not len(self_as_dict) == len(other_as_dict):
-            # If they don't have the same number of items, they are not equal:
-            return False
-        if self.extra_linkers:
-            linker_id_names = [
-                id_name
-                for (
-                    linker_table_name,
-                    (linked_table_name, id_name),
-                ) in self.extra_linkers.items()
-            ]  # FIXME: This will be made much simpler with coming metaprogramming
-        else:
-            linker_id_names = []
-        for key in self_as_dict:
-            # Here we go through the values
-            if key not in other_as_dict:
-                # other.as_dict() must have all the keys of self.as_dict() to be equal
-                return False
-            if key in linker_id_names:
-                # So, we don't want the linker names.
-                # FIXME: Right now there's no way to figure out what the property (named
-                #   in child_attrs) is called from the id_name :( ... so we have to
-                #   pass here and do a new loop with the child_attrs.
-                pass
-
-            if not thing_is_close(self_as_dict[key], other_as_dict[key]):
-                # Then the values aren't close (for floats and np arrays) or aren't
-                # equal (for all else)
-                return False
-
-        # Now we have to go through the owned Saveable objects:
-        if self.child_attrs:
-            for object_list_name in self.child_attrs:
-                object_list = getattr(self, object_list_name)
-                other_object_list = getattr(other, object_list_name)
-                # These two object lists need to have every corresponding element equal:
-                for object, other_object in zip(object_list, other_object_list):
-                    if object != other_object:
-                        return False
-
-        # If False hasn't been returned yet, then self and other are functionally equal.
-        return True
-
-    # This is necessary, because overriding __eq__ means that __hash__ is set to None
-    # https://docs.python.org/3/reference/datamodel.html#object.__hash__
-    # On the other hand, many Saveable objects are mutable, so maybe shouldn't have hash
-    __hash__ = object.__hash__
-
-    def save(self, db=None):
-        """Save self and return the id. This sets self.backend_name and self.id"""
-        db = db or self.db
-        return db.save(self)
-
-    @classmethod
-    def get_all_column_attrs(cls):
-        """List all attributes of objects of cls that correspond to table columns"""
-        all_attrs = cls.column_attrs
-        if cls.extra_column_attrs:
-            for table, attrs in cls.extra_column_attrs.items():
-                all_attrs = all_attrs.union(attrs)
-        if cls.extra_linkers:
-            for table, (ref_table, attr) in cls.extra_linkers.items():
-                all_attrs.add(attr)
-        return all_attrs
-
-    @classmethod
-    def from_dict(cls, obj_as_dict):
-        """Return an object built from its serialization."""
-        return cls(**obj_as_dict)
-
-    @classmethod
-    def get(cls, i, backend=None):
-        """Open an object of cls given its id (the table is cls.table_name)"""
-        old_backend = DB.backend
-        DB.set_backend(backend or old_backend)
-        obj = DB.get(cls, i)  # gets it from the requested backend.
-        DB.set_backend(old_backend)
-        return obj
-
-    def load_data(self, db=None):
-        """Load the data of the object, if ixdat in its laziness hasn't done so yet"""
-        db = db or self.db
-        return db.load_obj_data(self)
-
-
-class PlaceHolderObject:
-    """A tool for ixdat's laziness, instances sit in for Saveable objects."""
-
-    def __init__(self, identity, cls, backend=None):
-        """Initiate a PlaceHolderObject with info for loading the real obj when needed
-
-        Args:
-            identity (int or tuple): The id (principle key) of the object represented OR
-                the short identity, i.e. a tuple of the id and the backend. In the later
-                case, identity[1] overrides a backend if given
-            cls (class): Class inheriting from Saveable and thus specifiying the table
-            backend (Backend, optional): by default, placeholders objects must live in
-                the active backend. This is the case if loaded with get().
-        """
-        if isinstance(identity, int):
-            i = identity
-        else:
-            backend, i = identity
-        self.id = i
-        self.cls = cls
-        if not backend:  #
-            backend = DB.backend
-        if not backend or backend == "none" or backend is database_backends["none"]:
-            raise DataBaseError(f"Can't make a PlaceHolderObject with backend={backend}")
-        self.backend = backend
-
-    def get_object(self):
-        """Return the loaded real object represented by the PlaceHolderObject"""
-        return self.cls.get(self.id, backend=self.backend)
-
-    @property
-    def short_identity(self):
-        """Placeholder also has a short_identity to check equivalence without loading"""
-        if self.backend is DB.backend:
-            return self.id
-        return self.backend, self.id
-
-
-def fill_object_list(object_list, obj_ids, cls=None):
-    """Add PlaceHolderObjects to object_list for any unrepresented obj_ids.
-
-    Args:
-        object_list (list of objects or None): The objects already known,
-            in a list. This is the list to be appended to. If None, an empty
-            list will be appended to.
-        obj_ids (list of ints or None): The id's of objects to ensure are in
-            the list. Any id in obj_ids not already represented in object_list
-            is added to the list as a PlaceHolderObject
-        cls (Saveable class): the class remembered by any PlaceHolderObjects
-            added to the object_list, so that eventually the right object will
-            be loaded. Must be specified if object_list is empty.
-    """
-    cls = cls or object_list[0].__class__
-    object_list = object_list or []
-    provided_series_ids = [s.id for s in object_list]
-    if not obj_ids:
-        return object_list
-    for identity in obj_ids:
-        if identity not in provided_series_ids:
-            object_list.append(PlaceHolderObject(identity=identity, cls=cls))
-    return object_list
-
-
-def with_memory(function):
-    """Decorator for saving all new Saveable objects initiated in the memory backend"""
-
-    def function_with_memory(*args, **kwargs):
-        DB.new_object_backend = "memory"
-        to_return = function(*args, **kwargs)
-        DB.new_object_backend = "none"
-        return to_return
-
-    return function_with_memory
+"""This module contains the classes which pass on database functionality
+
+Note on terminology:
+    In ixdat, we seek to use the following naming conventions:
+        `load` grabs *an object* from a database backend given its class or table name
+            and the name of the specific object desired (see DataBase.load).
+        `load_xxx` grabs `xxx` from a database backend given the object for which
+            xxx is desired (see DataBase.load_object_data).
+        `get` grabs *an object* from a database backend given its class or table name and
+            the princple key (the id) of the row in the corresponding table
+        `get_xxx` grabs `xxx` from a database backend given the principle key (the id) of
+            the row in the table corresponding to xxx (see `Database.get`)
+
+        So `load` works by `name` or existing object, while `get` works by `id`.
+        `get_xx` is also used as the counterpart to `set_xx` to grab `xx`, typically a
+        managed attribute, from an object in memory.
+
+        `load` and `get` convention holds vertically - i.e. the Backend, the DataBase,
+            up through the Saveable parent class for all ixdat classes corresponding to
+            database tables have `load` and `get` methods which call downwards. TODO.
+    see: https://github.com/ixdat/ixdat/pull/1#discussion_r546400793
+"""
+
+from .exceptions import DataBaseError
+from .backends import BACKEND_CLASSES, database_backends
+from .tools import thing_is_close
+
+
+class DataBase:
+    """This class is a kind of middle-man between a Backend and a Saveable class
+
+    The reason for a middle man here is that it enables different databases (backends)
+    to be switched between and kept track of in a single ixdat session.
+
+    The DataBase should be initialized with a backend, and by default uses DirBackend,
+    which saves to a folder.
+    """
+
+    def __init__(self, backend=None):
+        """Initialize the database with its backend"""
+        self.backend = backend or database_backends["directory"]
+        self.new_object_backend = "none"
+
+    def save(self, obj):
+        """Save a Saveable object with the backend"""
+        return self.backend.save(obj)
+
+    def get(self, cls, i, backend=None):
+        """Select and return object of Saveable class cls with id=i from the backend"""
+        backend = backend or self.backend
+        obj = backend.get(cls, i)
+        # obj will already have obj.id = i and obj.backend = self.backend from backend
+        return obj
+
+    def load(self, cls, name):
+        """Select and return object of Saveable class cls with name=name from backend"""
+
+    def load_obj_data(self, obj):
+        """Load and return the numerical data (obj.data) for a Saveable object"""
+        return self.backend.load_obj_data(obj)
+
+    def set_backend(self, backend_name, **db_kwargs):
+        """Change backend to the class given by backend_name initiated with db_kwargs"""
+        if not isinstance(backend_name, str):
+            # Then we assume that it is the backend itself, not the backend name
+            self.backend = backend_name
+        elif backend_name in BACKEND_CLASSES:
+            BackendClass = BACKEND_CLASSES[backend_name]
+            self.backend = BackendClass(**db_kwargs)
+        else:
+            raise NotImplementedError(
+                f"ixdat doesn't recognize db_name = '{backend_name}'. If this is a new"
+                "database backend, make sure it is added to the DATABASE_BACKENDS "
+                "constant in ixdat.backends."
+                "Or manually set it directly with DB.backend = <my_backend>"
+            )
+        return self.backend
+
+
+DB = DataBase()  # initate the database. It functions as a global "constant"
+
+
+# THIS is proposed as the main mechanism for changing backend, to make
+# the shared global nature of it explicit. And in any case, the user
+# will never have to deal with the db, except when changing it away
+# from the default. This function should probably be exposed in the
+# top name space.
+
+
+def change_database(db_name, **db_kwargs):
+    """Change the backend specifying which database objects are saved to/loaded from"""
+    DB.set_backend(db_name, **db_kwargs)
+
+
+def get_database_name():
+    """Return the name of the class of which the database backend is an instance"""
+    return DB.backend.__class__.__name__
+
+
+class Saveable:
+    """Base class for table-representing classes implementing database functionality.
+
+    This enables seamless interoperability between database tables and ixdat classes.
+    Classes inheriting from this need to provide just a bit of info to define the
+    corresponding table, and then saving and loading should just work.
+
+    At a minimum, the `table_name` and `column_attrs` class attributes need to be
+    overwritten in inheriting classes to define the name and columns of the main
+    corresponding table. If an auxiliary table is needed to store lists of references
+    as rows, this should be represented in `linkers`. Sub-sub classes can use
+    `extra_column_attrs` to add extra columns via an auxiliary table without changing
+    the main table name.
+
+    ixdat is lazy, only loading things when needed. Correspondingly, all of the columns
+    of table mentioned above should refer to (lists of) id's and not actual objects of
+    other ixdat classes.
+
+    The class attributes (defined before __init__) and object attributes (defined in
+    __init__) are described here. See the other methods and the relevant inheriting
+    classes for more info.
+
+    Class attributes:
+        db (DataBase): the database, DB, which has the save, get, and load_data methods
+        table_name (str): The name of the table or folder in which objects are saved
+        column_attrs (set of str): {attr} where attr is the name of the column in the
+            table and also the name of the attribute of the class.
+        extra_column_attrs (dict): {table_name: {attr}} for auxiliary tables
+            to represent the "extra" attributes, for double-inheriting classes.
+        linkers (dict): {table_name: (reference_table, attr)} for defining
+            the connections between objects.
+
+    Object attributes:
+        backend (Backend): the backend where the object is saved. For a
+            new, un-saved, object, this is "memory".
+        _id (int): the principle key of the object, also accessible as `id`. This should
+            be set explicitly in the backend when loading an object. For objects
+            initiated directly in the session, it will become the id provided by the
+            memory backend, which just counts objects of each table starting with 1.
+            TODO: consider renaming.
+                See: https://github.com/ixdat/ixdat/pull/1#discussion_r546434676
+        name (str): The name of the object. `name` should be a column in ixdat tables.
+    """
+
+    db = DB
+    table_name = None  # THIS MUST BE OVERWRITTEN IN INHERITING CLASSES
+    # TODO: restructure column_attrs, extra_column_attrs, extra_linkers so that they are
+    #  sufficient to fully define the tables in an SQL backend
+    column_attrs = None  # THIS SHOULD BE OVERWRITTEN IN INHERITING CLASSES
+    extra_column_attrs = None  # THIS CAN BE OVERWRITTEN IN INHERITING CLASSES
+    extra_linkers = None  # THIS CAN BE OVERWRITTEN IN INHERITING CLASSES
+    # TODO: derive child_attrs somehow from the above class attributes, and have it in
+    #   a way where it's easy to tell which id goes with which attribute, i.e. s_ids
+    #   goes with series_list
+    child_attrs = None  # THIS SHOULD BE OVERWRITTEN IN CLASSES WITH DATA REFERENCES
+
+    def __init__(self, backend=None, **self_as_dict):
+        """Initialize a Saveable object from its dictionary serialization
+
+        This is the default behavior, and should be overwritten using an argument-free
+        call to super().__init__() in inheriting classes.
+
+        Args:
+            self_as_dict: all key-word arguments are by default set to object attributes
+        """
+        for attr, value in self_as_dict:
+            setattr(self, attr, value)
+        if self_as_dict and not self.column_attrs:
+            self.column_attrs = {attr: attr for attr in self_as_dict.keys()}
+        self._backend = None
+        self.backend = backend  # backend's setter will look up the backend name
+        self._id = None  # SHOULD BE SET AFTER THE __INIT__ FOR LOADED OBJECTS
+        self.name = None  # MUST BE SET IN THE __INIT__ OF INHERITING CLASSES
+
+    def __repr__(self):
+        return f"{self.__class__.__name__}(id={self.id}, name='{self.name}')"
+
+    @property
+    def id(self):
+        """The principle-key identifier. Set by backend or counted in memory."""
+        if not self._id:
+            if self.backend_type in ("none", "memory"):
+                self._id = self.backend.get_next_available_id(self.table_name, obj=self)
+                # TODO: Wouldn't it be better if the backend was always asked for the
+                #   ID by Saveable.__init__ ?
+            else:
+                raise DataBaseError(
+                    f"{self!r} comes from {self.backend_name} "
+                    "but did not get an id from its backend."
+                )
+        return self._id
+
+    @property
+    def short_identity(self):
+        """short_identity is the backend if different from the active backend and the id
+
+        FIXME: The overloaded return here is annoying and dangerous, but necessary for
+          `Measurement.from_dict(m.as_dict())` to work as a copy, since the call to
+          `fill_object_list` has to specify where the objects represented by
+          PlaceHolderObjects live. Note that calling save() on a Saveable object will
+          turn the backends into DB.backend, so this will only give id's when saving.
+        This is (usually) sufficient to tell if two objects refer to the same thing,
+        when used together with the class attribute table_name
+        """
+        if self.backend is DB.backend:
+            return self.id
+        return self.backend, self.id
+
+    @property
+    def full_identity(self):
+        """The full immutable object short_identity as (str, str, str, int)
+
+        Specifically: (backend_type, backend.address, table_name, id)
+        """
+        return self.backend_type, self.backend.address, self.table_name, self.id
+
+    @property
+    def backend(self):
+        """The backend the Saveable object was loaded from or last saved to."""
+        if not self._backend:
+            self._backend = database_backends["none"]
+        return self._backend
+
+    @backend.setter
+    def backend(self, new_backend):
+        """"""
+        new_backend = new_backend or DB.new_object_backend
+        if isinstance(new_backend, str):
+            if new_backend in database_backends:
+                new_backend = database_backends[new_backend]
+            elif new_backend in BACKEND_CLASSES:
+                new_backend = BACKEND_CLASSES[new_backend]()
+            else:
+                print(f"WARNING! {self} given unrecognized backend = {new_backend}")
+        self._backend = new_backend
+
+    @property
+    def backend_name(self):
+        """The name of the backend in which self has been saved to / loaded from"""
+        return self.backend.name
+
+    @property
+    def backend_type(self):
+        return self.backend.backend_type
+
+    def set_id(self, i):
+        """Backends set obj.id here after loading/saving a Saveable obj"""
+        self._id = i
+
+    def set_backend(self, backend):
+        """Backends set obj.backend here after loading/saving a Saveable obj"""
+        self.backend = backend
+
+    def get_main_dict(self, exclude=None):
+        """Return dict: serializition only of the row of the object's main table
+
+        Args:
+            exclude (list): List of attribute names to leave out of the dict
+        """
+        exclude = exclude or []
+        if self.column_attrs is None:
+            raise DataBaseError(
+                f"{self!r} can't be serialized because the class "
+                f"{self.__class__.__name__} hasn't defined column_attrs"
+            )
+        self_as_dict = {  # FIXME: probably better as loop, fix with table definitions.
+            attr: getattr(self, attr)
+            for attr in self.column_attrs
+            if attr not in exclude
+        }
+        return self_as_dict
+
+    def as_dict(self, exclude=None):
+        """Return dict: serialization of the object main and auxiliary tables"""
+
+        # So that a new object initiated using the dictionary returned here
+        #   can find objects referenced by identities (i.e. s_ids for series_list),
+        #   we need to make sure they are saved in a real backend. Before adding the
+        #   id's to a list.
+        # FIXME: There would be a more precise and elegant way to do this if the id's
+        #   and corresponding attributes could be connected through class attributes.
+        #   To do with table definitions.
+        if self.child_attrs:
+            for child_object_list_name in self.child_attrs:
+                for child_obj in getattr(self, child_object_list_name):
+                    if child_obj.backend is database_backends["none"]:
+                        database_backends["memory"].save(child_obj)
+
+        exclude = exclude or []
+        self_as_dict = self.get_main_dict(exclude=exclude)
+        if self.extra_column_attrs:
+            # FIXME: comprehension best as loop. Will be redone with proper table defs.
+            aux_tables_dict = {
+                table_name: {
+                    attr: getattr(self, attr) for attr in extras if attr not in exclude
+                }
+                for table_name, extras in self.extra_column_attrs.items()
+            }
+            for aux_dict in aux_tables_dict.values():
+                self_as_dict.update(**aux_dict)
+        if self.extra_linkers:
+            # FIXME: comprehension best as loop. Will be redone with proper table defs.
+            linker_tables_dict = {
+                (table_name, linked_table_name): {attr: getattr(self, attr)}
+                for table_name, (linked_table_name, attr) in self.extra_linkers.items()
+                if attr not in exclude
+            }
+            for linked_attrs in linker_tables_dict.values():
+                self_as_dict.update(**linked_attrs)
+
+        return self_as_dict
+
+    def __eq__(self, other):
+        """Return whether self is functionally equivalent to other
+
+        This means that everything in the dictionary representation is either equal
+        or close enough, and that every owned Saveable object in self and other are
+        equal by the same condition.
+
+        FIXME: as_dict() should perhaps be an ordered dict. That way we could ensure
+            that the order of the checks, in general, and in particular of the
+            property names further down, is intentional to keep cheap result determining
+            comparisons first and expensive ones last, for performance reasons
+        """
+        if self is other:
+            # If they're actually the same object of course they're equal.
+            return True
+        if self.__class__ is not other.__class__:
+            # If they're not the same class, they are not equal
+            return False
+        # Otherwise we compare their dictionary representations.
+        self_as_dict = self.as_dict()
+        other_as_dict = other.as_dict()
+        if not len(self_as_dict) == len(other_as_dict):
+            # If they don't have the same number of items, they are not equal:
+            return False
+        if self.extra_linkers:
+            linker_id_names = [
+                id_name
+                for (
+                    linker_table_name,
+                    (linked_table_name, id_name),
+                ) in self.extra_linkers.items()
+            ]  # FIXME: This will be made much simpler with coming metaprogramming
+        else:
+            linker_id_names = []
+        for key in self_as_dict:
+            # Here we go through the values
+            if key not in other_as_dict:
+                # other.as_dict() must have all the keys of self.as_dict() to be equal
+                return False
+            if key in linker_id_names:
+                # So, we don't want the linker names.
+                # FIXME: Right now there's no way to figure out what the property (named
+                #   in child_attrs) is called from the id_name :( ... so we have to
+                #   pass here and do a new loop with the child_attrs.
+                pass
+
+            if not thing_is_close(self_as_dict[key], other_as_dict[key]):
+                # Then the values aren't close (for floats and np arrays) or aren't
+                # equal (for all else)
+                return False
+
+        # Now we have to go through the owned Saveable objects:
+        if self.child_attrs:
+            for object_list_name in self.child_attrs:
+                object_list = getattr(self, object_list_name)
+                other_object_list = getattr(other, object_list_name)
+                # These two object lists need to have every corresponding element equal:
+                for object, other_object in zip(object_list, other_object_list):
+                    if object != other_object:
+                        return False
+
+        # If False hasn't been returned yet, then self and other are functionally equal.
+        return True
+
+    # This is necessary, because overriding __eq__ means that __hash__ is set to None
+    # https://docs.python.org/3/reference/datamodel.html#object.__hash__
+    # On the other hand, many Saveable objects are mutable, so maybe shouldn't have hash
+    __hash__ = object.__hash__
+
+    def save(self, db=None):
+        """Save self and return the id. This sets self.backend_name and self.id"""
+        db = db or self.db
+        return db.save(self)
+
+    @classmethod
+    def get_all_column_attrs(cls):
+        """List all attributes of objects of cls that correspond to table columns"""
+        all_attrs = cls.column_attrs
+        if cls.extra_column_attrs:
+            for table, attrs in cls.extra_column_attrs.items():
+                all_attrs = all_attrs.union(attrs)
+        if cls.extra_linkers:
+            for table, (ref_table, attr) in cls.extra_linkers.items():
+                all_attrs.add(attr)
+        return all_attrs
+
+    @classmethod
+    def from_dict(cls, obj_as_dict):
+        """Return an object built from its serialization."""
+        return cls(**obj_as_dict)
+
+    @classmethod
+    def get(cls, i, backend=None):
+        """Open an object of cls given its id (the table is cls.table_name)"""
+        old_backend = DB.backend
+        DB.set_backend(backend or old_backend)
+        obj = DB.get(cls, i)  # gets it from the requested backend.
+        DB.set_backend(old_backend)
+        return obj
+
+    def load_data(self, db=None):
+        """Load the data of the object, if ixdat in its laziness hasn't done so yet"""
+        db = db or self.db
+        return db.load_obj_data(self)
+
+
+class PlaceHolderObject:
+    """A tool for ixdat's laziness, instances sit in for Saveable objects."""
+
+    def __init__(self, identity, cls, backend=None):
+        """Initiate a PlaceHolderObject with info for loading the real obj when needed
+
+        Args:
+            identity (int or tuple): The id (principle key) of the object represented OR
+                the short identity, i.e. a tuple of the id and the backend. In the later
+                case, identity[1] overrides a backend if given
+            cls (class): Class inheriting from Saveable and thus specifiying the table
+            backend (Backend, optional): by default, placeholders objects must live in
+                the active backend. This is the case if loaded with get().
+        """
+        if isinstance(identity, int):
+            i = identity
+        else:
+            backend, i = identity
+        self.id = i
+        self.cls = cls
+        if not backend:  #
+            backend = DB.backend
+        if not backend or backend == "none" or backend is database_backends["none"]:
+            raise DataBaseError(f"Can't make a PlaceHolderObject with backend={backend}")
+        self.backend = backend
+
+    def get_object(self):
+        """Return the loaded real object represented by the PlaceHolderObject"""
+        return self.cls.get(self.id, backend=self.backend)
+
+    @property
+    def short_identity(self):
+        """Placeholder also has a short_identity to check equivalence without loading"""
+        if self.backend is DB.backend:
+            return self.id
+        return self.backend, self.id
+
+
+def fill_object_list(object_list, obj_ids, cls=None):
+    """Add PlaceHolderObjects to object_list for any unrepresented obj_ids.
+
+    Args:
+        object_list (list of objects or None): The objects already known,
+            in a list. This is the list to be appended to. If None, an empty
+            list will be appended to.
+        obj_ids (list of ints or None): The id's of objects to ensure are in
+            the list. Any id in obj_ids not already represented in object_list
+            is added to the list as a PlaceHolderObject
+        cls (Saveable class): the class remembered by any PlaceHolderObjects
+            added to the object_list, so that eventually the right object will
+            be loaded. Must be specified if object_list is empty.
+    """
+    cls = cls or object_list[0].__class__
+    object_list = object_list or []
+    provided_series_ids = [s.id for s in object_list]
+    if not obj_ids:
+        return object_list
+    for identity in obj_ids:
+        if identity not in provided_series_ids:
+            object_list.append(PlaceHolderObject(identity=identity, cls=cls))
+    return object_list
+
+
+def with_memory(function):
+    """Decorator for saving all new Saveable objects initiated in the memory backend"""
+
+    def function_with_memory(*args, **kwargs):
+        DB.new_object_backend = "memory"
+        to_return = function(*args, **kwargs)
+        DB.new_object_backend = "none"
+        return to_return
+
+    return function_with_memory
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat/exporters/csv_exporter.py` & `ixdat-0.2.9.dev3/src/ixdat/exporters/csv_exporter.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,198 +1,198 @@
-"""Classes for exporting measurement data"""
-from pathlib import Path
-import json
-import numpy as np
-from .. import __version__
-from ..data_series import TimeSeries
-from ..exceptions import SeriesNotFoundError
-
-UNIFORM_TIME_COLUMN_NAME = "projected time / [s]"
-
-
-class CSVExporter:
-    """The default exporter, which writes delimited measurement data row-wise to file"""
-
-    default_export_columns = None  # Typically overwritten by inheriting Exporters
-    """The names of the value series to export by default."""
-
-    def __init__(self, measurement=None, delim=","):
-        """Initiate the exported with a measurement (Measurement) and delimiter (str)"""
-        self.measurement = measurement
-        self.delim = delim
-        self.header_lines = None
-        self.time_step = None
-        self.s_list = None
-        self.columns = []
-        self.columns_data = None
-        self.path_to_file = None
-
-    def export(
-        self,
-        path_to_file=None,
-        measurement=None,
-        columns=None,
-        tspan=None,
-        time_step=None,
-        delim=None,
-    ):
-        """Export a given measurement to a specified file.
-
-        To improve flexibility with inheritance, this method delegates its work to:
-        - CSVExporter.prepare_header_and_data()
-        - CSVExporter.write_header()
-        - CSVExporter.write_data()
-
-        Args:
-            measurement (Measurement): The measurement to export.
-                Defaults to self.measurement.
-                TODO: remove this kwarg. See conversation here:
-                   https://github.com/ixdat/ixdat/pull/30/files#r810926968
-            path_to_file (Path): The path to the file to write. If it has no suffix,
-                a .csv suffix is appended. Defaults to f"{measurement.name}.csv"
-            columns (list of str): The names of the data series to include. Defaults in
-                CSVExporter to all VSeries and TSeries in the measurement. This default
-                may be overwritten in inheriting exporters.
-            tspan (timespan): The timespan to include in the file, defaults to all of it
-            time_step (float): Optional. The time spacing between data points. Can be
-                used to reduce file size.
-            delim (str): Delimiter. Defaults to self.delim (which is ",")
-        """
-        measurement = measurement or self.measurement
-        if not path_to_file:
-            path_to_file = f"{measurement.name}.csv"
-        if isinstance(path_to_file, str):
-            path_to_file = Path(path_to_file)
-        if not path_to_file.suffix:
-            path_to_file = path_to_file.with_suffix(".csv")
-        self.delim = delim or self.delim
-        self.time_step = time_step
-        self.path_to_file = path_to_file
-        self.prepare_header_and_data(measurement, columns, tspan, time_step)
-        self.prepare_column_header()
-        self.write_header()
-        self.write_data()
-
-    @property
-    def aliases(self):
-        """The aliases, needed for techniques with essential series that get renamed."""
-        aliases = self.measurement.aliases.copy()
-        if self.time_step and hasattr(self.measurement, "t_name"):
-            aliases[self.measurement.t_name] = (UNIFORM_TIME_COLUMN_NAME,)
-        return aliases
-
-    def prepare_header_and_data(self, measurement, columns, tspan=None, time_step=None):
-        """Prepare self.header_lines to include metadata and value-time pairs
-
-        Args:
-            measurement (Measurement): The measurement being exported
-            columns (list of str): The names of the ValueSeries to include
-            tspan (timespan): The timespan of the data to include in the export
-            time_step (float): Optional. The time spacing between data points. Can be
-                used to reduce file size.
-
-        Keyword arguments ``tspan`` and ``time_step`` as in :meth:`export`.
-        """
-        columns_data = {}
-        # list of the value names to export:
-        self.columns = (
-            columns or self.default_export_columns or list(measurement.value_names)
-        )
-        s_list = []  # list of the series names to export.
-        # s_list will also include names of TimeSeries.
-
-        if time_step:
-            tspan = tspan or measurement.tspan
-            t = np.arange(start=tspan[0], stop=tspan[-1], step=time_step)
-            uniform_tseries = TimeSeries(
-                name=UNIFORM_TIME_COLUMN_NAME,
-                unit_name="s",
-                data=t,
-                tstamp=measurement.tstamp,
-            )
-        else:
-            uniform_tseries = None
-
-        timecols = {}  # Will be {time_name: value_names}, for the header.
-        for v_name in self.columns:
-            if time_step:
-                t_name = uniform_tseries.name
-                t = uniform_tseries.data
-                v = measurement.grab_for_t(v_name, t=t)
-            else:
-                # Collect data and names for each ValueSeries and TimeSeries
-                try:
-                    tseries = measurement[v_name].tseries
-                    t_name = tseries.name
-                except SeriesNotFoundError:
-                    # can still be okay. Sometimes grab works even if getitem doesn't
-                    t_name = "t"
-                t, v = measurement.grab(v_name, tspan=tspan)
-            if t_name in timecols:
-                # We've already collected the data for this time column
-                timecols[t_name].append(v_name)
-            else:
-                # New time column. Collect its data and add it to the timecols.
-                columns_data[t_name] = t
-                s_list.append(t_name)
-                timecols[t_name] = [v_name]
-            columns_data[v_name] = v
-            s_list.append(v_name)
-
-        header_lines = []
-        ixdat_version_line = f"ixdat version = {__version__}\n"
-        header_lines.append(ixdat_version_line)
-        for attr in ["name", "technique", "tstamp", "backend_name", "id"]:
-            line = f"{attr} = {getattr(measurement, attr)}\n"
-            header_lines.append(line)
-            # TODO: This should be more automated... the exporter should put all
-            #    the appropriate metadata attributes of the object, read from its
-            #    table definition, in the header.
-        for t_name, v_names in timecols.items():
-            # Header includes a line for each time column stating which values use it:
-            line = (
-                f"timecol '{t_name}' for: "
-                + " and ".join([f"'{v_name}'" for v_name in v_names])
-                + "\n"
-            )
-            header_lines.append(line)
-        if self.aliases:
-            # For now, aliases is nice after the timecol lines. But see the to-do above.
-            aliases_line = f"aliases = {json.dumps(self.aliases)}\n"
-            header_lines.append(aliases_line)
-        if self.time_step:
-            aliases_line = f"time_step = {self.time_step}\n"
-            header_lines.append(aliases_line)
-
-        self.header_lines = header_lines
-        self.s_list = s_list
-        self.columns_data = columns_data
-
-    def prepare_column_header(self):
-        """Prepare the column header line and finish the header_lines"""
-        N_header_lines = len(self.header_lines) + 3
-        self.header_lines.append(f"N_header_lines = {N_header_lines}\n")
-        self.header_lines.append("\n")
-
-        col_header_line = self.delim.join(self.s_list) + "\n"
-        self.header_lines.append(col_header_line)
-
-    def write_header(self):
-        """Create the file and write the header lines."""
-        with open(self.path_to_file, "w") as f:
-            f.writelines(self.header_lines)
-
-    def write_data(self):
-        """Write data to the file one line at a time."""
-        max_length = max([len(data) for data in self.columns_data.values()])
-        for n in range(max_length):
-            data_strings = []
-            for s_name in self.s_list:
-                if len(self.columns_data[s_name]) > n:
-                    # Then there's more data to write for this series
-                    data_strings.append(str(self.columns_data[s_name][n]))
-                else:
-                    # Then all this series is written. Just leave space.
-                    data_strings.append("")
-            line = self.delim.join(data_strings) + "\n"
-            with open(self.path_to_file, "a") as f:
-                f.write(line)
+"""Classes for exporting measurement data"""
+from pathlib import Path
+import json
+import numpy as np
+from .. import __version__
+from ..data_series import TimeSeries
+from ..exceptions import SeriesNotFoundError
+
+UNIFORM_TIME_COLUMN_NAME = "projected time / [s]"
+
+
+class CSVExporter:
+    """The default exporter, which writes delimited measurement data row-wise to file"""
+
+    default_export_columns = None  # Typically overwritten by inheriting Exporters
+    """The names of the value series to export by default."""
+
+    def __init__(self, measurement=None, delim=","):
+        """Initiate the exported with a measurement (Measurement) and delimiter (str)"""
+        self.measurement = measurement
+        self.delim = delim
+        self.header_lines = None
+        self.time_step = None
+        self.s_list = None
+        self.columns = []
+        self.columns_data = None
+        self.path_to_file = None
+
+    def export(
+        self,
+        path_to_file=None,
+        measurement=None,
+        columns=None,
+        tspan=None,
+        time_step=None,
+        delim=None,
+    ):
+        """Export a given measurement to a specified file.
+
+        To improve flexibility with inheritance, this method delegates its work to:
+        - CSVExporter.prepare_header_and_data()
+        - CSVExporter.write_header()
+        - CSVExporter.write_data()
+
+        Args:
+            measurement (Measurement): The measurement to export.
+                Defaults to self.measurement.
+                TODO: remove this kwarg. See conversation here:
+                   https://github.com/ixdat/ixdat/pull/30/files#r810926968
+            path_to_file (Path): The path to the file to write. If it has no suffix,
+                a .csv suffix is appended. Defaults to f"{measurement.name}.csv"
+            columns (list of str): The names of the data series to include. Defaults in
+                CSVExporter to all VSeries and TSeries in the measurement. This default
+                may be overwritten in inheriting exporters.
+            tspan (timespan): The timespan to include in the file, defaults to all of it
+            time_step (float): Optional. The time spacing between data points. Can be
+                used to reduce file size.
+            delim (str): Delimiter. Defaults to self.delim (which is ",")
+        """
+        measurement = measurement or self.measurement
+        if not path_to_file:
+            path_to_file = f"{measurement.name}.csv"
+        if isinstance(path_to_file, str):
+            path_to_file = Path(path_to_file)
+        if not path_to_file.suffix:
+            path_to_file = path_to_file.with_suffix(".csv")
+        self.delim = delim or self.delim
+        self.time_step = time_step
+        self.path_to_file = path_to_file
+        self.prepare_header_and_data(measurement, columns, tspan, time_step)
+        self.prepare_column_header()
+        self.write_header()
+        self.write_data()
+
+    @property
+    def aliases(self):
+        """The aliases, needed for techniques with essential series that get renamed."""
+        aliases = self.measurement.aliases.copy()
+        if self.time_step and hasattr(self.measurement, "t_name"):
+            aliases[self.measurement.t_name] = (UNIFORM_TIME_COLUMN_NAME,)
+        return aliases
+
+    def prepare_header_and_data(self, measurement, columns, tspan=None, time_step=None):
+        """Prepare self.header_lines to include metadata and value-time pairs
+
+        Args:
+            measurement (Measurement): The measurement being exported
+            columns (list of str): The names of the ValueSeries to include
+            tspan (timespan): The timespan of the data to include in the export
+            time_step (float): Optional. The time spacing between data points. Can be
+                used to reduce file size.
+
+        Keyword arguments ``tspan`` and ``time_step`` as in :meth:`export`.
+        """
+        columns_data = {}
+        # list of the value names to export:
+        self.columns = (
+            columns or self.default_export_columns or list(measurement.value_names)
+        )
+        s_list = []  # list of the series names to export.
+        # s_list will also include names of TimeSeries.
+
+        if time_step:
+            tspan = tspan or measurement.tspan
+            t = np.arange(start=tspan[0], stop=tspan[-1], step=time_step)
+            uniform_tseries = TimeSeries(
+                name=UNIFORM_TIME_COLUMN_NAME,
+                unit_name="s",
+                data=t,
+                tstamp=measurement.tstamp,
+            )
+        else:
+            uniform_tseries = None
+
+        timecols = {}  # Will be {time_name: value_names}, for the header.
+        for v_name in self.columns:
+            if time_step:
+                t_name = uniform_tseries.name
+                t = uniform_tseries.data
+                v = measurement.grab_for_t(v_name, t=t)
+            else:
+                # Collect data and names for each ValueSeries and TimeSeries
+                try:
+                    tseries = measurement[v_name].tseries
+                    t_name = tseries.name
+                except SeriesNotFoundError:
+                    # can still be okay. Sometimes grab works even if getitem doesn't
+                    t_name = "t"
+                t, v = measurement.grab(v_name, tspan=tspan)
+            if t_name in timecols:
+                # We've already collected the data for this time column
+                timecols[t_name].append(v_name)
+            else:
+                # New time column. Collect its data and add it to the timecols.
+                columns_data[t_name] = t
+                s_list.append(t_name)
+                timecols[t_name] = [v_name]
+            columns_data[v_name] = v
+            s_list.append(v_name)
+
+        header_lines = []
+        ixdat_version_line = f"ixdat version = {__version__}\n"
+        header_lines.append(ixdat_version_line)
+        for attr in ["name", "technique", "tstamp", "backend_name", "id"]:
+            line = f"{attr} = {getattr(measurement, attr)}\n"
+            header_lines.append(line)
+            # TODO: This should be more automated... the exporter should put all
+            #    the appropriate metadata attributes of the object, read from its
+            #    table definition, in the header.
+        for t_name, v_names in timecols.items():
+            # Header includes a line for each time column stating which values use it:
+            line = (
+                f"timecol '{t_name}' for: "
+                + " and ".join([f"'{v_name}'" for v_name in v_names])
+                + "\n"
+            )
+            header_lines.append(line)
+        if self.aliases:
+            # For now, aliases is nice after the timecol lines. But see the to-do above.
+            aliases_line = f"aliases = {json.dumps(self.aliases)}\n"
+            header_lines.append(aliases_line)
+        if self.time_step:
+            aliases_line = f"time_step = {self.time_step}\n"
+            header_lines.append(aliases_line)
+
+        self.header_lines = header_lines
+        self.s_list = s_list
+        self.columns_data = columns_data
+
+    def prepare_column_header(self):
+        """Prepare the column header line and finish the header_lines"""
+        N_header_lines = len(self.header_lines) + 3
+        self.header_lines.append(f"N_header_lines = {N_header_lines}\n")
+        self.header_lines.append("\n")
+
+        col_header_line = self.delim.join(self.s_list) + "\n"
+        self.header_lines.append(col_header_line)
+
+    def write_header(self):
+        """Create the file and write the header lines."""
+        with open(self.path_to_file, "w") as f:
+            f.writelines(self.header_lines)
+
+    def write_data(self):
+        """Write data to the file one line at a time."""
+        max_length = max([len(data) for data in self.columns_data.values()])
+        for n in range(max_length):
+            data_strings = []
+            for s_name in self.s_list:
+                if len(self.columns_data[s_name]) > n:
+                    # Then there's more data to write for this series
+                    data_strings.append(str(self.columns_data[s_name][n]))
+                else:
+                    # Then all this series is written. Just leave space.
+                    data_strings.append("")
+            line = self.delim.join(data_strings) + "\n"
+            with open(self.path_to_file, "a") as f:
+                f.write(line)
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat/exporters/ec_exporter.py` & `ixdat-0.2.9.dev3/src/ixdat/exporters/ec_exporter.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,43 +1,43 @@
-from .csv_exporter import CSVExporter
-
-
-class ECExporter(CSVExporter):
-    """A CSVExporter that by default exports potential, current, and selector"""
-
-    @property
-    def default_export_columns(self):
-        """The default v_list for ECExporter is V_str, J_str, and sel_str"""
-        return [
-            # self.measurement.t_name,  # gets included automatically.
-            self.measurement.U_name,
-            self.measurement.J_name,
-            self.measurement.selector_name,
-        ]
-
-    @property
-    def aliases(self):  # TODO: Figure out if this can be deleted.
-        """Ensure that the essential series are accessible as aliases."""
-        aliases = super().aliases.copy()
-        for name, prop_names in [
-            ("t", ("t_name",)),
-            ("raw_potential", ("E_name", "U_name")),
-            ("raw_current", ("I_name", "J_name")),
-            ("selector", ("selector_name",)),
-        ]:
-            # This is bit complex because the essential series for an ECMeasureemnt
-            #   are t, raw_current, and raw_potential, but by default the calibrated
-            #   potential and normalized current are exported if available. So
-            #   we need to go through and make sure the reader of the exported .csv can
-            #   find a "raw_current" and "raw_potential" using aliases, even if it
-            #   isn't actually "raw". And at the same time we have to avoid circular
-            #   lookups in aliases. Here goes:
-            for prop_name in prop_names:
-                name_in_measurement = getattr(self.measurement, prop_name)
-                if (
-                    name_in_measurement in self.columns
-                    and (name not in aliases or name_in_measurement not in aliases[name])
-                    and name_in_measurement != name
-                ):
-                    aliases[name] = (name_in_measurement,)
-                    break
-        return aliases
+from .csv_exporter import CSVExporter
+
+
+class ECExporter(CSVExporter):
+    """A CSVExporter that by default exports potential, current, and selector"""
+
+    @property
+    def default_export_columns(self):
+        """The default v_list for ECExporter is V_str, J_str, and sel_str"""
+        return [
+            # self.measurement.t_name,  # gets included automatically.
+            self.measurement.U_name,
+            self.measurement.J_name,
+            self.measurement.selector_name,
+        ]
+
+    @property
+    def aliases(self):  # TODO: Figure out if this can be deleted.
+        """Ensure that the essential series are accessible as aliases."""
+        aliases = super().aliases.copy()
+        for name, prop_names in [
+            ("t", ("t_name",)),
+            ("raw_potential", ("E_name", "U_name")),
+            ("raw_current", ("I_name", "J_name")),
+            ("selector", ("selector_name",)),
+        ]:
+            # This is bit complex because the essential series for an ECMeasureemnt
+            #   are t, raw_current, and raw_potential, but by default the calibrated
+            #   potential and normalized current are exported if available. So
+            #   we need to go through and make sure the reader of the exported .csv can
+            #   find a "raw_current" and "raw_potential" using aliases, even if it
+            #   isn't actually "raw". And at the same time we have to avoid circular
+            #   lookups in aliases. Here goes:
+            for prop_name in prop_names:
+                name_in_measurement = getattr(self.measurement, prop_name)
+                if (
+                    name_in_measurement in self.columns
+                    and (name not in aliases or name_in_measurement not in aliases[name])
+                    and name_in_measurement != name
+                ):
+                    aliases[name] = (name_in_measurement,)
+                    break
+        return aliases
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat/exporters/ecms_exporter.py` & `ixdat-0.2.9.dev3/src/ixdat/exporters/ecms_exporter.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,66 +1,66 @@
-from .ec_exporter import ECExporter
-from ..tools import deprecate
-
-
-class ECMSExporter(ECExporter):
-    """A CSVExporter that by default exports potential, current, selector, and all MID"""
-
-    @property
-    def default_export_columns(self):
-        """The default EC columns plus all MID signals"""
-        v_list = (
-            ECExporter(measurement=self.measurement).default_export_columns
-            + self.measurement.mass_list
-        )
-
-        return v_list
-
-    @deprecate("0.2.0", "use `columns` instead", "0.3.0", kwarg_name="v_list")
-    def export(
-        self,
-        path_to_file=None,
-        measurement=None,
-        columns=None,
-        v_list=None,
-        mass_list=None,
-        mol_list=None,
-        tspan=None,
-        time_step=None,
-    ):
-        """Export a given measurement to a specified file.
-
-        This method delegates the majority of the export work, via inheritance, to:
-        - CSVExporter.prepare_header_and_data()
-        - CSVExporter.write_header()
-        - CSVExporter.write_data()
-
-        Args:
-            path_to_file (Path): The path to the file to write. If it has no suffix,
-                a .csv suffix is appended. Defaults to f"{measurement.name}.csv"
-            measurement (Measurement): The measurement to export.
-                Defaults to self.measurement.
-                TODO: remove this kwarg. See conversation here:
-                   https://github.com/ixdat/ixdat/pull/30/files#r810926968
-            columns (list of str): The names of the data series to include. Defaults to
-                potential, current, and all MID signals.
-            v_list: DEPRECATED. Use `columns` instead.
-            mass_list (list of str): Names of masses to export. Defaults to all.
-            mol_list (list of str or `ECMSCalResult`): Names of mols for which to export
-                quantified data.
-            tspan (timespan): The timespan to include in the file, defaults to all of it
-            time_step (float): Optional. The time spacing between data points. Can be
-                used to reduce file size. Requires `tspan`.
-        """
-        columns = columns or v_list  # deal with deprecated argument
-        if not columns:
-            if mass_list:
-                columns = ECExporter(measurement=self.measurement).default_export_columns
-            else:
-                columns = self.default_export_columns
-        if mass_list:
-            columns += mass_list
-        if mol_list:
-            columns += [f"n_dot_{mol}" for mol in mol_list]
-        return super().export(
-            path_to_file, measurement, columns, tspan, time_step=time_step
-        )
+from .ec_exporter import ECExporter
+from ..tools import deprecate
+
+
+class ECMSExporter(ECExporter):
+    """A CSVExporter that by default exports potential, current, selector, and all MID"""
+
+    @property
+    def default_export_columns(self):
+        """The default EC columns plus all MID signals"""
+        v_list = (
+            ECExporter(measurement=self.measurement).default_export_columns
+            + self.measurement.mass_list
+        )
+
+        return v_list
+
+    @deprecate("0.2.0", "use `columns` instead", "0.3.0", kwarg_name="v_list")
+    def export(
+        self,
+        path_to_file=None,
+        measurement=None,
+        columns=None,
+        v_list=None,
+        mass_list=None,
+        mol_list=None,
+        tspan=None,
+        time_step=None,
+    ):
+        """Export a given measurement to a specified file.
+
+        This method delegates the majority of the export work, via inheritance, to:
+        - CSVExporter.prepare_header_and_data()
+        - CSVExporter.write_header()
+        - CSVExporter.write_data()
+
+        Args:
+            path_to_file (Path): The path to the file to write. If it has no suffix,
+                a .csv suffix is appended. Defaults to f"{measurement.name}.csv"
+            measurement (Measurement): The measurement to export.
+                Defaults to self.measurement.
+                TODO: remove this kwarg. See conversation here:
+                   https://github.com/ixdat/ixdat/pull/30/files#r810926968
+            columns (list of str): The names of the data series to include. Defaults to
+                potential, current, and all MID signals.
+            v_list: DEPRECATED. Use `columns` instead.
+            mass_list (list of str): Names of masses to export. Defaults to all.
+            mol_list (list of str or `ECMSCalResult`): Names of mols for which to export
+                quantified data.
+            tspan (timespan): The timespan to include in the file, defaults to all of it
+            time_step (float): Optional. The time spacing between data points. Can be
+                used to reduce file size. Requires `tspan`.
+        """
+        columns = columns or v_list  # deal with deprecated argument
+        if not columns:
+            if mass_list:
+                columns = ECExporter(measurement=self.measurement).default_export_columns
+            else:
+                columns = self.default_export_columns
+        if mass_list:
+            columns += mass_list
+        if mol_list:
+            columns += [f"n_dot_{mol}" for mol in mol_list]
+        return super().export(
+            path_to_file, measurement, columns, tspan, time_step=time_step
+        )
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat/exporters/ms_exporter.py` & `ixdat-0.2.9.dev3/src/ixdat/exporters/ms_exporter.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,44 +1,44 @@
-from . import CSVExporter, SpectrumSeriesExporter
-
-
-class MSExporter(CSVExporter):
-    """By default, everything from an MS Measurement is exported."""
-
-    pass
-
-
-class MSSpectroExporter(MSExporter):
-    def __init__(self, measurement, delim=","):
-        super().__init__(measurement, delim=delim)
-        # self.spectra_exporter = SpectrumSeriesExporter(measurement.spectrum_series)
-        # FIXME: Have to do a property because this __int__ gets called before the
-        #    measurement's __init__ is finished...
-        self._spectra_exporter = None
-
-    @property
-    def spectra_exporter(self):
-        if not self._spectra_exporter:
-            self._spectra_exporter = SpectrumSeriesExporter(
-                self.measurement.spectrum_series
-            )
-        return self._spectra_exporter
-
-    def prepare_header_and_data(self, measurement, columns, tspan=None, time_step=None):
-        """Do the standard ixdat csv export header preparation, plus SEC stuff.
-
-        The MS Spectra stuff is:
-            - export the MSSpectrumSeries
-            - add a line to the main file header pointing to the spectra file
-
-        Args and Kwargs: see :meth:`ECExporter.prepare_header_and_data`
-        """
-        super().prepare_header_and_data(measurement, columns, tspan, time_step=time_step)
-        path_to_spectra_file = self.path_to_file.parent / (
-            self.path_to_file.stem + "_spectra.csv"
-        )
-        self.header_lines.append(
-            f"'spectrum_series' in file: '{path_to_spectra_file.name}'\n"
-        )
-        self.spectra_exporter.export(path_to_spectra_file)
-
-        print(f"writing {self.path_to_file}!")
+from . import CSVExporter, SpectrumSeriesExporter
+
+
+class MSExporter(CSVExporter):
+    """By default, everything from an MS Measurement is exported."""
+
+    pass
+
+
+class MSSpectroExporter(MSExporter):
+    def __init__(self, measurement, delim=","):
+        super().__init__(measurement, delim=delim)
+        # self.spectra_exporter = SpectrumSeriesExporter(measurement.spectrum_series)
+        # FIXME: Have to do a property because this __int__ gets called before the
+        #    measurement's __init__ is finished...
+        self._spectra_exporter = None
+
+    @property
+    def spectra_exporter(self):
+        if not self._spectra_exporter:
+            self._spectra_exporter = SpectrumSeriesExporter(
+                self.measurement.spectrum_series
+            )
+        return self._spectra_exporter
+
+    def prepare_header_and_data(self, measurement, columns, tspan=None, time_step=None):
+        """Do the standard ixdat csv export header preparation, plus SEC stuff.
+
+        The MS Spectra stuff is:
+            - export the MSSpectrumSeries
+            - add a line to the main file header pointing to the spectra file
+
+        Args and Kwargs: see :meth:`ECExporter.prepare_header_and_data`
+        """
+        super().prepare_header_and_data(measurement, columns, tspan, time_step=time_step)
+        path_to_spectra_file = self.path_to_file.parent / (
+            self.path_to_file.stem + "_spectra.csv"
+        )
+        self.header_lines.append(
+            f"'spectrum_series' in file: '{path_to_spectra_file.name}'\n"
+        )
+        self.spectra_exporter.export(path_to_spectra_file)
+
+        print(f"writing {self.path_to_file}!")
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat/exporters/sec_exporter.py` & `ixdat-0.2.9.dev3/src/ixdat/exporters/sec_exporter.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,70 +1,70 @@
-from .ec_exporter import ECExporter
-from .spectrum_exporter import SpectrumExporter, SpectrumSeriesExporter
-
-
-class SECExporter(ECExporter):
-    """Adds to CSVExporter the export of the Field with the SEC spectra"""
-
-    def __init__(self, measurement, delim=","):
-        super().__init__(measurement, delim=delim)
-        # FIXME: The lines below don't work because this __init__ gets called before
-        #   the measurement's __init__ is finished.
-        # self.reference_exporter = SpectrumExporter(measurement.reference_spectrum)
-        # self.spectra_exporter = SpectrumSeriesExporter(measurement.spectrum_series)
-        self._reference_exporter = None
-        self._spectra_exporter = None
-
-    @property
-    def reference_exporter(self):
-        if not self._reference_exporter:
-            self._reference_exporter = SpectrumExporter(
-                self.measurement.reference_spectrum
-            )
-        return self._reference_exporter
-
-    @property
-    def spectra_exporter(self):
-        if not self._spectra_exporter:
-            self._spectra_exporter = SpectrumSeriesExporter(
-                self.measurement.spectrum_series
-            )
-        return self._spectra_exporter
-
-    @property
-    def default_export_columns(self):
-        """The default v_list for SECExporter is that from EC and tracked wavelengths"""
-        columns = (
-            ECExporter(measurement=self.measurement).default_export_columns
-            + self.measurement.tracked_wavelengths
-        )
-        return columns
-
-    def prepare_header_and_data(self, measurement, columns, tspan=None, time_step=None):
-        """Do the standard ixdat csv export header preparation, plus SEC stuff.
-
-        The SEC stuff is:
-            - export the spectroelectrochemistry spectra
-            - export the actual reference spectrum
-            - add lines to the main file header pointing to the files with the
-                above two exports.
-
-        Args and Kwargs: see :meth:`ECExporter.prepare_header_and_data`
-        """
-        super().prepare_header_and_data(measurement, columns, tspan, time_step=time_step)
-        path_to_spectra_file = self.path_to_file.parent / (
-            self.path_to_file.stem + "_spectra.csv"
-        )
-        measurement = measurement or self.measurement
-        self.header_lines.append(
-            f"'spectrum_series' in file: '{path_to_spectra_file.name}'\n"
-        )
-        self.spectra_exporter.export(path_to_spectra_file)
-        path_to_reference_spectrum_file = self.path_to_file.parent / (
-            self.path_to_file.stem + "_reference.csv"
-        )
-        self.header_lines.append(
-            f"'reference_spectrum' in file: '{path_to_reference_spectrum_file.name}'\n"
-        )
-        self.reference_exporter.export(path_to_reference_spectrum_file)
-
-        print(f"writing {self.path_to_file}!")
+from .ec_exporter import ECExporter
+from .spectrum_exporter import SpectrumExporter, SpectrumSeriesExporter
+
+
+class SECExporter(ECExporter):
+    """Adds to CSVExporter the export of the Field with the SEC spectra"""
+
+    def __init__(self, measurement, delim=","):
+        super().__init__(measurement, delim=delim)
+        # FIXME: The lines below don't work because this __init__ gets called before
+        #   the measurement's __init__ is finished.
+        # self.reference_exporter = SpectrumExporter(measurement.reference_spectrum)
+        # self.spectra_exporter = SpectrumSeriesExporter(measurement.spectrum_series)
+        self._reference_exporter = None
+        self._spectra_exporter = None
+
+    @property
+    def reference_exporter(self):
+        if not self._reference_exporter:
+            self._reference_exporter = SpectrumExporter(
+                self.measurement.reference_spectrum
+            )
+        return self._reference_exporter
+
+    @property
+    def spectra_exporter(self):
+        if not self._spectra_exporter:
+            self._spectra_exporter = SpectrumSeriesExporter(
+                self.measurement.spectrum_series
+            )
+        return self._spectra_exporter
+
+    @property
+    def default_export_columns(self):
+        """The default v_list for SECExporter is that from EC and tracked wavelengths"""
+        columns = (
+            ECExporter(measurement=self.measurement).default_export_columns
+            + self.measurement.tracked_wavelengths
+        )
+        return columns
+
+    def prepare_header_and_data(self, measurement, columns, tspan=None, time_step=None):
+        """Do the standard ixdat csv export header preparation, plus SEC stuff.
+
+        The SEC stuff is:
+            - export the spectroelectrochemistry spectra
+            - export the actual reference spectrum
+            - add lines to the main file header pointing to the files with the
+                above two exports.
+
+        Args and Kwargs: see :meth:`ECExporter.prepare_header_and_data`
+        """
+        super().prepare_header_and_data(measurement, columns, tspan, time_step=time_step)
+        path_to_spectra_file = self.path_to_file.parent / (
+            self.path_to_file.stem + "_spectra.csv"
+        )
+        measurement = measurement or self.measurement
+        self.header_lines.append(
+            f"'spectrum_series' in file: '{path_to_spectra_file.name}'\n"
+        )
+        self.spectra_exporter.export(path_to_spectra_file)
+        path_to_reference_spectrum_file = self.path_to_file.parent / (
+            self.path_to_file.stem + "_reference.csv"
+        )
+        self.header_lines.append(
+            f"'reference_spectrum' in file: '{path_to_reference_spectrum_file.name}'\n"
+        )
+        self.reference_exporter.export(path_to_reference_spectrum_file)
+
+        print(f"writing {self.path_to_file}!")
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat/exporters/spectrum_exporter.py` & `ixdat-0.2.9.dev3/src/ixdat/exporters/spectrum_exporter.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,139 +1,139 @@
-import pandas as pd
-from collections import OrderedDict
-
-
-class SpectrumExporter:
-    """An ixdat CSV exporter for spectra. Uses pandas."""
-
-    def __init__(self, spectrum, delim=","):
-        """Initiate the SpectrumExporter.
-
-        Args:
-            spectrum (Spectrum): The spectrum to export by default
-            delim (char): The separator for the .csv file. Note that this cannot be
-                the ",\t" used by ixdat's main exporter since pandas only accepts single
-                character delimiters.
-        """
-        self.spectrum = spectrum
-        self.delim = delim
-
-    def export(self, path_to_file, spectrum=None):
-        """Export spectrum to path_to_file.
-
-        Args:
-            path_to_file (str or Path): The path of the file to export to. Note that if a
-                file already exists with this path, it will be overwritten.
-            spectrum (Spectrum): The spectrum to export if different from self.spectrum
-                TODO: remove this kwarg. See conversation here:
-                   https://github.com/ixdat/ixdat/pull/30/files#r810926968
-        """
-        spectrum = spectrum or self.spectrum
-        df = pd.DataFrame({spectrum.x_name: spectrum.x, spectrum.y_name: spectrum.y})
-
-        header_lines = []
-        for attr in ["name", "technique", "tstamp", "backend_name", "id"]:
-            line = f"{attr} = {getattr(spectrum, attr)}\n"
-            header_lines.append(line)
-
-        header_lines.append("\n")
-
-        # Insert a line, after the first two lines, saying how long the header is.
-        N_header_lines = len(header_lines) + 2
-        # The `+ 2` is for the header length line and the column header line.
-        header_length_line = f"N_header_lines = {N_header_lines}\n"
-        header_lines = header_lines[:2] + [header_length_line] + header_lines[2:]
-
-        with open(path_to_file, "w") as f:
-            f.writelines(header_lines)
-        with open(path_to_file, "a") as f:
-            df.to_csv(f, index=False, sep=self.delim, lineterminator="\n")
-
-        print(f"wrote {path_to_file}!")
-
-
-class SpectrumSeriesExporter:
-    """An exporter for ixdat spectrum series."""
-
-    def __init__(self, spectrum_series, delim=","):
-        """Initiate the SpectrumSeriesExporter.
-
-        Args:
-            spectrum_series (SpectrumSeries): The spectrum to export by default
-            delim (char): The separator for the .csv file. Note that this cannot be
-                the ",\t" used by ixdat's main exporter since pandas only accepts single
-                character delimiters.
-        """
-        self.spectrum_series = spectrum_series
-        self.delim = delim
-
-    def export(self, path_to_file=None, spectrum_series=None, spectra_as_rows=True):
-        """Export spectrum series to path_to_file.
-
-        Args:
-            spectrum_series (Spectrum): The spectrum_series to export if different from
-                self.spectrum_series
-                TODO: remove this kwarg. See conversation here:
-                   https://github.com/ixdat/ixdat/pull/30/files#r810926968
-            path_to_file (str or Path): The path of the file to export to. Note that if a
-                file already exists with this path, it will be overwritten.
-            spectra_as_rows (bool): This specifies the orientation of the data exported.
-                If True, the scanning variabe (e.g. wavelength) increases to the right
-                and the time variable increases downward. If False, the scanning
-                variable increases downwards and the time variable increases to the
-                right. Either way it is clarified in the file header. Defaults to True.
-        """
-
-        spectrum_series = spectrum_series or self.spectrum_series
-
-        field = spectrum_series.field
-        data = field.data
-        tseries, xseries = spectrum_series.field.axes_series
-        t = tseries.t + tseries.tstamp - spectrum_series.tstamp
-        x = xseries.data
-
-        header_lines = []
-        for attr in ["name", "technique", "tstamp", "backend_name", "id"]:
-            line = f"{attr} = {getattr(spectrum_series, attr)}\n"
-            header_lines.append(line)
-
-        header_lines.append(
-            f"values are y='{field.name}' with units [{field.unit_name}]\n"
-        )
-
-        if spectra_as_rows:  # columns are ValueSeries
-            data_as_list_of_tuples = [(spectrum_series.t_name, t)] + [
-                (x_i, data[:, i]) for i, x_i in enumerate(x)
-            ]
-            df = pd.DataFrame(OrderedDict(data_as_list_of_tuples))
-            header_lines.append(
-                f"first row is x='{xseries.name}' with units [{xseries.unit_name}]\n"
-            )
-            header_lines.append(
-                f"first column is t='{tseries.name}' with units [{tseries.unit_name}]\n"
-            )
-        else:  # spectra as columns. rows are ValueSeries
-            data_as_list_of_tuples = [(spectrum_series.x_name, x)] + [
-                (t_i, data[i, :]) for i, t_i in enumerate(t)
-            ]
-            df = pd.DataFrame(OrderedDict(data_as_list_of_tuples))
-            header_lines.append(
-                f"first row is t='{tseries.name}' with units [{tseries.unit_name}]\n"
-            )
-            header_lines.append(
-                f"first column is x='{xseries.name}' with units [{xseries.unit_name}]\n"
-            )
-
-        header_lines.append("\n")
-
-        # Insert a line, after the first two lines, saying how long the header is.
-        N_header_lines = len(header_lines) + 2
-        # The `+ 2` is for the header length line and the column header line.
-        header_length_line = f"N_header_lines = {N_header_lines}\n"
-        header_lines = header_lines[:2] + [header_length_line] + header_lines[2:]
-
-        with open(path_to_file, "w") as f:
-            f.writelines(header_lines)
-        with open(path_to_file, "a") as f:
-            df.to_csv(f, index=False, sep=self.delim, lineterminator="\n")
-
-        print(f"wrote {path_to_file}!")
+import pandas as pd
+from collections import OrderedDict
+
+
+class SpectrumExporter:
+    """An ixdat CSV exporter for spectra. Uses pandas."""
+
+    def __init__(self, spectrum, delim=","):
+        """Initiate the SpectrumExporter.
+
+        Args:
+            spectrum (Spectrum): The spectrum to export by default
+            delim (char): The separator for the .csv file. Note that this cannot be
+                the ",\t" used by ixdat's main exporter since pandas only accepts single
+                character delimiters.
+        """
+        self.spectrum = spectrum
+        self.delim = delim
+
+    def export(self, path_to_file, spectrum=None):
+        """Export spectrum to path_to_file.
+
+        Args:
+            path_to_file (str or Path): The path of the file to export to. Note that if a
+                file already exists with this path, it will be overwritten.
+            spectrum (Spectrum): The spectrum to export if different from self.spectrum
+                TODO: remove this kwarg. See conversation here:
+                   https://github.com/ixdat/ixdat/pull/30/files#r810926968
+        """
+        spectrum = spectrum or self.spectrum
+        df = pd.DataFrame({spectrum.x_name: spectrum.x, spectrum.y_name: spectrum.y})
+
+        header_lines = []
+        for attr in ["name", "technique", "tstamp", "backend_name", "id"]:
+            line = f"{attr} = {getattr(spectrum, attr)}\n"
+            header_lines.append(line)
+
+        header_lines.append("\n")
+
+        # Insert a line, after the first two lines, saying how long the header is.
+        N_header_lines = len(header_lines) + 2
+        # The `+ 2` is for the header length line and the column header line.
+        header_length_line = f"N_header_lines = {N_header_lines}\n"
+        header_lines = header_lines[:2] + [header_length_line] + header_lines[2:]
+
+        with open(path_to_file, "w") as f:
+            f.writelines(header_lines)
+        with open(path_to_file, "a") as f:
+            df.to_csv(f, index=False, sep=self.delim, lineterminator="\n")
+
+        print(f"wrote {path_to_file}!")
+
+
+class SpectrumSeriesExporter:
+    """An exporter for ixdat spectrum series."""
+
+    def __init__(self, spectrum_series, delim=","):
+        """Initiate the SpectrumSeriesExporter.
+
+        Args:
+            spectrum_series (SpectrumSeries): The spectrum to export by default
+            delim (char): The separator for the .csv file. Note that this cannot be
+                the ",\t" used by ixdat's main exporter since pandas only accepts single
+                character delimiters.
+        """
+        self.spectrum_series = spectrum_series
+        self.delim = delim
+
+    def export(self, path_to_file=None, spectrum_series=None, spectra_as_rows=True):
+        """Export spectrum series to path_to_file.
+
+        Args:
+            spectrum_series (Spectrum): The spectrum_series to export if different from
+                self.spectrum_series
+                TODO: remove this kwarg. See conversation here:
+                   https://github.com/ixdat/ixdat/pull/30/files#r810926968
+            path_to_file (str or Path): The path of the file to export to. Note that if a
+                file already exists with this path, it will be overwritten.
+            spectra_as_rows (bool): This specifies the orientation of the data exported.
+                If True, the scanning variabe (e.g. wavelength) increases to the right
+                and the time variable increases downward. If False, the scanning
+                variable increases downwards and the time variable increases to the
+                right. Either way it is clarified in the file header. Defaults to True.
+        """
+
+        spectrum_series = spectrum_series or self.spectrum_series
+
+        field = spectrum_series.field
+        data = field.data
+        tseries, xseries = spectrum_series.field.axes_series
+        t = tseries.t + tseries.tstamp - spectrum_series.tstamp
+        x = xseries.data
+
+        header_lines = []
+        for attr in ["name", "technique", "tstamp", "backend_name", "id"]:
+            line = f"{attr} = {getattr(spectrum_series, attr)}\n"
+            header_lines.append(line)
+
+        header_lines.append(
+            f"values are y='{field.name}' with units [{field.unit_name}]\n"
+        )
+
+        if spectra_as_rows:  # columns are ValueSeries
+            data_as_list_of_tuples = [(spectrum_series.t_name, t)] + [
+                (x_i, data[:, i]) for i, x_i in enumerate(x)
+            ]
+            df = pd.DataFrame(OrderedDict(data_as_list_of_tuples))
+            header_lines.append(
+                f"first row is x='{xseries.name}' with units [{xseries.unit_name}]\n"
+            )
+            header_lines.append(
+                f"first column is t='{tseries.name}' with units [{tseries.unit_name}]\n"
+            )
+        else:  # spectra as columns. rows are ValueSeries
+            data_as_list_of_tuples = [(spectrum_series.x_name, x)] + [
+                (t_i, data[i, :]) for i, t_i in enumerate(t)
+            ]
+            df = pd.DataFrame(OrderedDict(data_as_list_of_tuples))
+            header_lines.append(
+                f"first row is t='{tseries.name}' with units [{tseries.unit_name}]\n"
+            )
+            header_lines.append(
+                f"first column is x='{xseries.name}' with units [{xseries.unit_name}]\n"
+            )
+
+        header_lines.append("\n")
+
+        # Insert a line, after the first two lines, saying how long the header is.
+        N_header_lines = len(header_lines) + 2
+        # The `+ 2` is for the header length line and the column header line.
+        header_length_line = f"N_header_lines = {N_header_lines}\n"
+        header_lines = header_lines[:2] + [header_length_line] + header_lines[2:]
+
+        with open(path_to_file, "w") as f:
+            f.writelines(header_lines)
+        with open(path_to_file, "a") as f:
+            df.to_csv(f, index=False, sep=self.delim, lineterminator="\n")
+
+        print(f"wrote {path_to_file}!")
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat/measurements.py` & `ixdat-0.2.9.dev3/src/ixdat/measurements.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,1489 +1,1489 @@
-"""This module defines the Measurement class, the central data structure of ixdat
-
-An ixdat Measurement is a collection of references to DataSeries and the metadata needed
-to combine them, i.e. "build" the combined dataset. It has a number of general methods
-to visualize and analyze the combined dataset. Measurement is also the base class for a
-number of technique-specific Measurement-derived classes.
-
-A Measurement will typically be accompanied by one or more Calibration. This module
-also defines the base class for Calibration, while technique-specific Calibration
-classes will be defined in the corresponding module in ./techniques/
-"""
-import json
-import numpy as np
-from .db import Saveable, PlaceHolderObject, fill_object_list
-from .data_series import (
-    DataSeries,
-    TimeSeries,
-    ValueSeries,
-    ConstantValue,
-    append_series,
-    time_shifted,
-    get_tspans_from_mask,
-)
-from .projects.samples import Sample
-from .projects.lablogs import LabLog
-from .exporters.csv_exporter import CSVExporter
-from .plotters.value_plotter import ValuePlotter
-from .exceptions import BuildError, SeriesNotFoundError, TechniqueError, ReadError
-from .tools import deprecate, tstamp_to_string
-
-
-class Measurement(Saveable):
-    """The Measurement class"""
-
-    # ------ table description class attributes --------
-    table_name = "measurement"
-    column_attrs = {
-        "name",
-        "technique",
-        "metadata",
-        "aliases",
-        "sample_name",
-        "tstamp",
-    }
-    extra_linkers = {
-        "component_measurements": ("measurements", "m_ids"),
-        "measurement_calibrations": ("calibrations", "c_ids"),
-        "measurement_series": ("data_series", "s_ids"),
-    }
-    child_attrs = ["component_measurements", "calibration_list", "series_list"]
-    # TODO: child_attrs should be derivable from extra_linkers?
-
-    # ---- measurement class attributes, can be overwritten in inheriting classes ---- #
-    control_technique_name = None
-    """Name of the control technique primarily used to control the experiment"""
-    control_series_name = None
-    """Name (or alias) for main time variable or main time-dependent value variable,
-    typically of the control technique"""
-    selector_name = "selector"
-    """Name of the default selector"""
-    selection_series_names = ("file_number",)
-    """Name of the default things to use to construct the selector"""
-    series_constructors = {
-        "file_number": "_build_file_number_series",
-        "selector": "_build_selector_series",
-    }
-    """Series which should be constructed from other series by the specified method
-    and cached the first time they are looked up"""
-    essential_series_names = None
-    """Series which should always be present"""
-    default_plotter = ValuePlotter
-    default_exporter = CSVExporter
-
-    def __init__(
-        self,
-        name,
-        technique=None,
-        metadata=None,
-        s_ids=None,
-        series_list=None,
-        c_ids=None,
-        calibration_list=None,
-        m_ids=None,
-        component_measurements=None,
-        aliases=None,
-        reader=None,
-        plotter=None,
-        exporter=None,
-        sample=None,
-        lablog=None,
-        tstamp=None,
-    ):
-        """initialize a measurement
-
-        Args:
-            name (str): The name of the measurement
-            metadata (dict): Free-form measurement metadata. Must be json-compatible.
-            technique (str): The measurement technique
-            s_ids (list of int): The id's of the measurement's DataSeries, if
-                to be loaded (instead of given directly in series_list)
-            series_list (list of DataSeries): The measurement's DataSeries
-            c_ids (list of int): The id's of the measurement's Calibrations, if
-                to be loaded (instead of given directly in calibration_list)
-            calibration_list: The measurement's Calibrations
-            m_ids (list of int): The id's of the component measurements, if to be
-                loaded. None unless this is a combined measurement (typically
-                corresponding to more than one file).
-            component_measurements (list of Measurements): The measurements of which
-                this measurement is a combination
-            aliases (dict): Alternative names for DataSeries for versatile access
-            reader (Reader): The file reader (None unless read from a file)
-            plotter (Plotter): The visualization tool for the measurement
-            exporter (Exporter): The exporting tool for the measurement
-            sample (Sample or str): The sample being measured
-            lablog (LabLog): The log entry with e.g. notes taken during the measurement
-            tstamp (float): The nominal starting time of the measurement, used for
-                data selection, visualization, and exporting.
-        """
-        super().__init__()
-        self.name = name
-        self.technique = technique
-        self.metadata = metadata or {}
-        self.reader = reader
-        if isinstance(sample, str):
-            sample = Sample.load_or_make(sample)
-        self.sample = sample
-        if isinstance(lablog, str):
-            lablog = LabLog.load_or_make(lablog)
-        self.lablog = lablog
-        self._series_list = fill_object_list(series_list, s_ids, cls=DataSeries)
-        self._component_measurements = fill_object_list(
-            component_measurements, m_ids, cls=Measurement
-        )
-        self._calibration_list = fill_object_list(
-            calibration_list, c_ids, cls=Calibration
-        )
-        self._tstamp = tstamp
-
-        self._cached_series = {}
-        self._aliases = aliases or {}
-
-        self.plotter = plotter or self.__class__.default_plotter(measurement=self)
-        self.exporter = exporter or self.__class__.default_exporter(measurement=self)
-        # defining these methods here gets them the right docstrings :D
-        self.plot_measurement = self.plotter.plot_measurement
-        self.plot = self.plotter.plot_measurement
-        self.export = self.exporter.export
-        # TODO: ... but we need to think a bit more about how to most elegantly and
-        #    dynamically choose plotters (Nice idea from Anna:
-        #    https://github.com/ixdat/ixdat/issues/32)
-
-    def __str__(self):
-        """Return string representation"""
-        tseries_to_valueseries = {}
-        for series in self.series_list:
-            if isinstance(series, TimeSeries):
-                if series not in tseries_to_valueseries:
-                    tseries_to_valueseries[series] = []
-            else:
-                if series.tseries in tseries_to_valueseries:
-                    tseries_to_valueseries[series.tseries].append(series)
-                else:
-                    tseries_to_valueseries[series.tseries] = [series]
-
-        out = []
-        for tseries, value_serieses in tseries_to_valueseries.items():
-            out.append("┏ " + str(tseries))
-            for n, value_series in enumerate(value_serieses):
-                if n == len(value_serieses) - 1:
-                    out.append("┗━ " + str(value_series))
-                else:
-                    out.append("┣━ " + str(value_series))
-
-        return (
-            f"{self.__class__.__name__} '{self.name}' with {len(self.series_list)} "
-            "series\n\n"
-            "Series list:\n" + "\n".join(out)
-        )
-
-    @classmethod
-    def from_dict(cls, obj_as_dict):
-        """Return an object of the measurement class of the right technique
-
-        Args:
-              obj_as_dict (dict): The full serializaiton (rows from table and aux
-                tables) of the measurement. obj_as_dict["technique"] specifies the
-                technique class to use, from TECHNIQUE_CLASSES
-        """
-        # TODO: see if there isn't a way to put the import at the top of the module.
-        #    see: https://github.com/ixdat/ixdat/pull/1#discussion_r546437410
-        from .techniques import TECHNIQUE_CLASSES
-
-        # certain objects stored in the Measurement, but only saved as their names.
-        #   __init__() will get the object from the name, but the argument is
-        #   called like the object either way. For example __init__() takes an argument
-        #   called `sample` which can be an ixdat.Sample or a string interpreted as the
-        #   name of the sample to load. Subsequently, the sample name is accessible as
-        #   the property `sample_name`. But in the database is only saved the sample's
-        #   name as a string with the key/column "sample_name". So
-        #   obj_as_dict["sample_name"] needs to be renamed obj_as_dict["sample"] before
-        #   obj_as_dict can be passed to __init__.
-        #   TODO: This is a rather general problem (see, e.g. DataSeries.unit vs
-        #       DataSeries.unit_name) and as such should be moved to db.Saveable
-        #       see: https://github.com/ixdat/ixdat/pull/5#discussion_r565090372.
-        #       Will be fixed with the table definition PR.
-        objects_saved_as_their_name = ["sample"]
-        for object_type_str in objects_saved_as_their_name:
-            object_name_str = object_type_str + "_name"
-            if object_name_str in obj_as_dict:
-                obj_as_dict[object_type_str] = obj_as_dict[object_name_str]
-                del obj_as_dict[object_name_str]
-
-        if obj_as_dict["technique"] in TECHNIQUE_CLASSES:
-            # This makes it so that from_dict() can be used to initiate for any more
-            # derived technique, so long as obj_as_dict specifies the technique name!
-            technique_class = TECHNIQUE_CLASSES[obj_as_dict["technique"]]
-            if not issubclass(technique_class, cls):
-                # But we never want obj_as_dict["technique"] to take us to a *less*
-                # specific technique, if the user has been intentional about which
-                # class they call `as_dict` from (e.g. via a Reader)!
-                technique_class = cls
-        else:
-            technique_class = cls
-        try:
-            measurement = technique_class(**obj_as_dict)
-        except TypeError as e:
-            raise TechniqueError(
-                "ixdat ran into an error while trying to set up an object of type\n"
-                f"  {technique_class}. This usually happens when ixdat wasn't able\n"
-                f"  to correctly determine the measurement technique. Error: \n{e}\n\n"
-                "Consider passing the `technique` argument into the read() function.\n"
-                "For a list of available techniques use: \n "
-                ">>> from ixdat.techniques import TECHNIQUE_CLASSES\n"
-                ">>> print(TECHNIQUE_CLASSES.keys())\n"
-            )
-        return measurement
-
-    @classmethod
-    def read(cls, path_to_file, reader=None, **kwargs):
-        """Return a Measurement object from parsing a file with the specified reader
-
-        Args:
-            path_to_file (Path or str): The path to the file to read
-            reader (str or Reader class): The (name of the) reader to read the file
-                with. If not specified, ixdat will try to determine the reader from the
-                file suffix.
-            kwargs: key-word arguments are passed on to the reader's read() method.
-        """
-        if not reader:
-            # Check if there is a default reader based on the file's suffix
-            from .readers.reading_tools import get_default_reader_name
-
-            reader = get_default_reader_name(path_to_file)
-            if not reader:
-                raise ValueError(
-                    f"There is no default reader for files of the type {path_to_file}. "
-                    "Please specify a reader to read this file."
-                )
-
-        if isinstance(reader, str):
-            # TODO: see if there isn't a way to put the import at the top of the module.
-            #    see: https://github.com/ixdat/ixdat/pull/1#discussion_r546437471
-            from .readers import READER_CLASSES
-
-            reader = READER_CLASSES[reader]()
-        obj = reader.read(path_to_file, cls=cls, **kwargs)
-
-        if getattr(obj.__class__, "essential_series_names", None):
-            for series_name in obj.__class__.essential_series_names:
-                try:
-                    _ = obj[series_name]  # this also caches it.
-                except SeriesNotFoundError:
-                    raise SeriesNotFoundError(
-                        f"{reader} loaded without {obj.__class__.__name__} "
-                        f"essential series '{series_name}'"
-                    )
-        return obj
-
-    @classmethod
-    def read_url(cls, url, reader=None, **kwargs):
-        """Read a url (via a temporary file) using the specified reader"""
-        from .readers.reading_tools import url_to_file
-
-        path_to_temp_file = url_to_file(url)
-        measurement = cls.read(path_to_temp_file, reader=reader, **kwargs)
-        path_to_temp_file.unlink()
-        return measurement
-
-    @classmethod
-    def read_set(
-        cls,
-        path_to_file_start=True,
-        part=None,
-        suffix=None,
-        file_list=None,
-        reader=None,
-        **kwargs,
-    ):
-        """Read and append a set of files.
-
-        Args:
-            path_to_file_start (Path or str): The path to the files to read including
-                the shared start of the file name: `Path(path_to_file).parent` is
-                interpreted as the folder where the file are.
-                `Path(path_to_file).name` is interpreted as the shared start of the files
-                to be appended.
-                Alternatively, path_to_file_start can be a folder, in which case all
-                files in that folder (with the specified suffix) are included.
-            part (Path or str): A path where the folder is the folder containing data
-                and the name is a part of the name of each of the files to be read and
-                combined.
-            suffix (str): If a suffix is given, only files with the specified ending are
-                added to the file list
-            file_list (list of Path): As an alternative to path_to_file_start, the
-                exact files to append can be specified in a list
-            reader (str or Reader class): The (name of the) reader to read the files with
-            kwargs: Key-word arguments are passed via cls.read() to the reader's read()
-                method, AND to cls.from_component_measurements()
-        """
-        from .readers.reading_tools import get_file_list
-
-        file_list = file_list or get_file_list(path_to_file_start, part, suffix)
-        if not file_list:
-            raise ReadError(
-                "No files found! Please check that there are files satisfying:\n"
-                f"path_to_file_start={path_to_file_start}, part={part}, suffix={suffix}"
-            )
-        component_measurements = [
-            cls.read(f, reader=reader, **kwargs) for f in file_list
-        ]
-        measurement = None
-        for meas in component_measurements:
-            measurement = measurement + meas if measurement else meas
-        return measurement
-
-    @classmethod
-    def from_component_measurements(
-        cls, component_measurements, keep_originals=True, sorted=True, **kwargs
-    ):
-        """Return a measurement with the data contained in the component measurements
-
-        TODO: This function "builds" the resulting measurement, i.e. it appends series
-            of the same name rather than keeping all the original copies. This should be
-            made more explicit, and a `build()` method should take over some of the work.
-
-        Args:
-            component_measurements (list of Measurement)
-            keep_originals: Whether to keep a list of component_measurements referenced.
-                This may result in redundant numpy arrays in RAM.
-            sorted (bool): Whether to sort the series according to time
-            kwargs: key-word arguments are added to the dictionary for cls.from_dict()
-
-        Returns cls: the combined measurement.
-        """
-
-        # First prepare everything but the series_list in the object dictionary
-        obj_as_dict = component_measurements[0].as_dict()
-        obj_as_dict.update(kwargs)
-        del obj_as_dict["m_ids"], obj_as_dict["s_ids"]
-        if keep_originals:
-            obj_as_dict["component_measurements"] = component_measurements
-
-        # Now, prepare the built series. First, we loop through the component
-        # measurements and get all the data and metadata organized in a dictionary:
-        series_as_dicts = {}
-        tstamp = component_measurements[0].tstamp
-        for meas in component_measurements:
-            tstamp_i = meas.tstamp  # save this for later.
-            meas.tstamp = tstamp  # so that the time vectors share a t=0
-            for s_name in meas.series_names:
-                series = meas[s_name]
-                if s_name in series_as_dicts:
-                    series_as_dicts[s_name]["data"] = np.append(
-                        series_as_dicts[s_name]["data"], series.data
-                    )
-                else:
-                    series_as_dicts[s_name] = series.as_dict()
-                    series_as_dicts[s_name]["data"] = series.data
-                    if isinstance(series, ValueSeries):
-                        # This will serve to match it to a TimeSeries later:
-                        series_as_dicts[s_name]["t_name"] = series.tseries.name
-            meas.tstamp = tstamp_i  # so it's not changed in the outer scope
-
-        # Now we make DataSeries, starting with all the TimeSeries
-        tseries_dict = {}
-        sort_indeces = {}
-        for name, s_as_dict in series_as_dicts.items():
-            if "tstamp" in s_as_dict:
-                if sorted:
-                    sort_indeces[name] = np.argsort(s_as_dict["data"])
-                    s_as_dict["data"] = s_as_dict["data"][sort_indeces[name]]
-                tseries_dict[name] = TimeSeries.from_dict(s_as_dict)
-        # And then ValueSeries, and put both in with the TimeSeries
-        series_list = []
-        for name, s_as_dict in series_as_dicts.items():
-            if name in tseries_dict:
-                series_list.append(tseries_dict[name])
-            elif "t_name" in s_as_dict:
-                tseries = tseries_dict[s_as_dict["t_name"]]
-                if s_as_dict["data"].shape == tseries.shape:
-                    # Then we assume that the time and value data have lined up
-                    # successfully! :D
-                    if sorted:
-                        s_as_dict["data"] = s_as_dict["data"][sort_indeces[tseries.name]]
-                    vseries = ValueSeries(
-                        name=name,
-                        data=s_as_dict["data"],
-                        unit_name=s_as_dict["unit_name"],
-                        tseries=tseries,
-                    )
-                else:
-                    # this will be the case if vseries sharing the same tseries
-                    # are not present in the same subset of component_measurements.
-                    # In that case just append the vseries even though some tdata gets
-                    # duplicated.
-                    vseries = append_series(
-                        [
-                            s
-                            for m in component_measurements
-                            for s in m.series_list
-                            if s.name == name
-                        ],
-                        sorted=sorted,
-                    )
-                series_list.append(vseries)
-
-        # Finally, add the series to the dictionary representation and return the object
-        obj_as_dict["series_list"] = series_list
-        return cls.from_dict(obj_as_dict)
-
-    @property
-    def tstamp(self):
-        """Float: The unix epoch time used by the measurement as t=0"""
-        return self._tstamp
-
-    @tstamp.setter
-    def tstamp(self, tstamp):
-        # Resetting the tstamp needs to clear the cache, so series are returned wrt the
-        # new timestamp.
-        self.clear_cache()
-        self._tstamp = tstamp
-
-    @property
-    def yyMdd(self):
-        return tstamp_to_string(self.tstamp, string_format="native_date")
-
-    @property
-    def metadata_json_string(self):
-        """Measurement metadata as a JSON-formatted string"""
-        return json.dumps(self.metadata, indent=4)
-
-    @property
-    def sample_name(self):
-        """Name of the sample on which the measurement was conducted"""
-        if self.sample:
-            return self.sample.name
-
-    @property
-    def component_measurements(self):
-        """List of the component measurements of which this measurement is a combination
-
-        For a pure measurement (not a measurement set), this is itself in a list.
-        """
-        for i, m in enumerate(self._component_measurements):
-            if isinstance(m, PlaceHolderObject):
-                # This is where we find objects from a Backend including MemoryBackend:
-                self._component_measurements[i] = m.get_object()
-        return self._component_measurements
-
-    @property
-    def m_ids(self):
-        """List of the id's of a combined measurement's component measurements
-        FIXME: m.id can be (backend, id) if it's not on the active backend.
-            This is as of now necessary to find it if you're only given self.as_dict()
-            see https://github.com/ixdat/ixdat/pull/11#discussion_r746632897
-        """
-        if not self._component_measurements:
-            return None
-        return [m.short_identity for m in self.component_measurements]
-
-    @property
-    def calibration_list(self):
-        """List of calibrations (with placeholders filled)"""
-        for i, c in enumerate(self._calibration_list):
-            if isinstance(c, PlaceHolderObject):
-                # This is where we find objects from a Backend including MemoryBackend:
-                self._calibration_list[i] = c.get_object()
-        return self._calibration_list
-
-    @property
-    def calibrations(self):
-        """For overriding: List of calibrations with any needed manipulation done."""
-        return self.calibration_list
-
-    @property
-    def c_ids(self):
-        """List of the id's of the measurement's Calibrations
-        FIXME: c.id can be (backend, id) if it's not on the active backend.
-            This is as of now necessary to find it if you're only given self.as_dict()
-             see https://github.com/ixdat/ixdat/pull/11#discussion_r746632897
-        """
-        return [c.short_identity for c in self.calibration_list]
-
-    def add_calibration(self, calibration):
-        self._calibration_list = [calibration] + self._calibration_list
-        self.clear_cache()
-
-    def calibrate(self, *args, **kwargs):
-        """Add a calibration of the Measurement's default calibration type
-
-        The calibration class is determined by the measurement's `technique`.
-        *args and **kwargs are passed to the calibration class's `__init__`.
-
-        Raises:
-            TechniqueError if no calibration class for the measurement's technique
-        """
-
-        from .techniques import CALIBRATION_CLASSES
-
-        if self.technique in CALIBRATION_CLASSES:
-            calibration_class = CALIBRATION_CLASSES[self.technique]
-        else:
-            raise TechniqueError(
-                f"{self!r} is of technique '{self.technique}, for which there is not an "
-                "available default calibration. Instead, import one of the following "
-                "classes to initiate a calibration, and then use `add_calibration`. "
-                f"\nOptions: \n{CALIBRATION_CLASSES}"
-            )
-
-        self.add_calibration(calibration_class(*args, **kwargs))
-        self.clear_cache()
-
-    @property
-    @deprecate(
-        last_supported_release="0.1",
-        update_message=(
-            "At present, ixdat measurements have a `calibration_list` but no compound "
-            "`calibration`, and this property just returns the first from the list."
-        ),
-        hard_deprecation_release=None,
-    )
-    def calibration(self):
-        return self.calibration_list[0]
-
-    @calibration.setter
-    @deprecate(
-        last_supported_release="0.1",
-        update_message=(
-            "Setting `calibration` is deprecated. For now it clears `calibration_list` "
-            "and replaces it with a single calibration. "
-            "Use `add_calibration()` instead."
-        ),
-        hard_deprecation_release="0.3",
-    )
-    def calibration(self, calibration):
-        self._calibration_list = [calibration]
-
-    @property
-    def series_list(self):
-        """List of the DataSeries containing the measurement's data"""
-        for i, s in enumerate(self._series_list):
-            if isinstance(s, PlaceHolderObject):
-                # This is where we find objects from a Backend including MemoryBackend:
-                self._series_list[i] = s.get_object()
-        return self._series_list
-
-    @property
-    def s_ids(self):
-        """List of the id's of the measurement's DataSeries
-        FIXME: m.id can be (backend, id) if it's not on the active backend.
-            This is as of now necessary to find it if you're only given self.as_dict()
-            see https://github.com/ixdat/ixdat/pull/11#discussion_r746632897
-        """
-        return [series.short_identity for series in self._series_list]
-
-    @property
-    def series_names(self):
-        """Set of the names of the series in the measurement"""
-        return set([series.name for series in self.series_list])
-
-    @property
-    def value_names(self):
-        """Set of the names of the VSeries in the measurement's DataSeries"""
-        return set([vseries.name for vseries in self.value_series])
-
-    @property
-    def time_names(self):
-        """Set of the names of the VSeries in the measurement's DataSeries"""
-        return set([tseries.name for tseries in self.time_series])
-
-    @property
-    def value_series(self):
-        """List of the VSeries in the measurement's DataSeries"""
-        return [series for series in self.series_list if isinstance(series, ValueSeries)]
-
-    @property
-    def time_series(self):
-        """List of the TSeries in the measurement's DataSeries. NOT timeshifted!"""
-        return [series for series in self.series_list if isinstance(series, TimeSeries)]
-
-    @property
-    def aliases(self):
-        """Dictionary of {key: series_names} pointing to where desired raw data is
-
-        TODO: get the possible aliases based on calibrations, etc, in here.
-        """
-        return self._aliases.copy()
-
-    @property
-    def reverse_aliases(self):
-        """{series_name: standard_names} indicating how raw data can be accessed"""
-        rev_aliases = {}
-        for name, other_names in self.aliases.items():
-            for other_name in other_names:
-                if other_name in rev_aliases:
-                    rev_aliases[other_name].append(name)
-                else:
-                    rev_aliases[other_name] = [name]
-        return rev_aliases
-
-    def get_series_names(self, key):
-        """Return list: series names for key found by (recursive) lookup in aliases"""
-        keys = [key] if key in self.series_names else []
-        for k in self.aliases.get(key, []):
-            keys += self.get_series_names(k)
-        return keys
-
-    def __getitem__(self, key):
-        """Return the built measurement DataSeries with its name specified by key
-
-        This method does the following:
-        1. check if `key` is in in the cache. If so return the cached data series
-        2. find or build the desired data series by the first possible of:
-            A. Check if `key` corresponds to a method in `series_constructors`. If
-                so, build the data series with that method.
-            B. Check if the `calibration`'s `calibrate_series` returns a data series
-                for `key` given the data in this measurement. (Note that the
-                `calibration` will typically start with raw data looked C, below.)
-            C. Generate a list of data series and append them:
-                i. Check if `key` is in `aliases`. If so, append all the data series
-                    returned for each key in `aliases[key]`.
-                ii. Otherwise, check if there are data series in `series_list` that
-                    have `key` as their `name`. If so, append them.
-            D. Finally, check if the user is using a suffix.
-                i. If `key` ends with "-y" or "-v", look it up with the suffix removed.
-                ii. If `key` ends with "-x" or "-t", look up `key` with the suffix
-                    removed and use instead the corresponding `tseries`.
-        3. Cache and return the data series found or built in (2).
-
-        Step (2) above, the searching step, is outsourced to the method
-        `get_series(key)`.
-        Notice that some calls of `__getitem__` can be recursive. For example, we
-        suppose that a new `ECMeasurement` is read from a source that calls raw
-        potential `Ewe/V`, and that this measurement is then calibrated:
-
-        >>> ec_meas = Measurement.read(...)
-        >>> ec_meas.aliases
-        {..., 'raw_potential': ['Ewe/V'], ...}
-        >>> ec_meas["raw_potential"]  # first lookup, explained below
-        ValueSeries("Ewe/V", ...)
-        >>> ec_meas.calibrate_RE(RE_vs_RHE=0.7)
-        >>> ec_meas["potential"]      # second lookup, explained below
-        ValueSeries("U_{RHE} / [V]", ...)
-
-        - The first lookup, with `key="raw_potential"`, (1) checks for
-        "raw_potential" in the cache, doesn't find it; then (2A) checks in
-        `series_constructors`, doesn't find it; (2B) asks the calibration for
-        "raw_potential" and doesn't get anything back; and finally (2Ci) checks
-        `aliases` for raw potential where it finds that "raw_potential" is called
-        "Ewe/V". Then it looks up again, this time with `key="Ewe/V"`, which it doesn't
-        find in (1) the cache, (2A) `series_consturctors`, (2B) the calibration, or
-        (2Ci) `aliases`, but does find in (2Cii) `series_list`. There is only one
-        data series named "Ewe/V" so no appending is necessary, but it does ensure that
-        the series has the measurement's `tstamp` before cache'ing and returning it.
-        Now we're back in the original lookup, from which __getitem__ (3) caches
-        the data series (which still has the name "Ewe/V") as "raw_potential" and
-        returns it.
-        - The second lookup, with `key="potential"`, (1) checks for "potential" in
-        the cache, doesn't find it; then (2A) checks in `series_constructors`,
-        doesn't find it; and then (2B) asks the calibration for "potential". The
-        calibration knows that when asked for "potential" it should look for
-        "raw_potential" and add `RE_vs_RHE`. So it does a lookup with
-        `key="raw_potential"` and (1) finds it in the cache. The calibration does
-        the math and returns a new data series for the calibrated potential, bringing
-        us back to the original lookup. The data series returned by the
-        calibration is then (3) cached and returned to the user.
-
-        Note that, if the user had not looked up "raw_potential" before looking up
-        "potential", "raw_potential" would not have been in the cache and the first
-        lookup above would have been nested in the second.
-
-        Args:
-            key (str): The name of a DataSeries (see above)
-        Raises:
-            SeriesNotFoundError if none of the above lookups find the key.
-        Side-effects:
-            if key is not already in the cache, it gets added
-        Returns:
-            The (calibrated) (appended) dataseries for key with the right t=0.
-        """
-        # step 1
-        if key in self._cached_series:
-            return self._cached_series[key]
-        # step 2
-        series = self.get_series(key)
-        # Finally, wherever we found the series, cache it and return it.
-        # step 3.
-        self._cache_series(key, series)
-        return series
-
-    def _cache_series(self, key, series):
-        """Cache `series` such that it can be looked up with its name or with `key`."""
-        self._cached_series[key] = series  # now it can be looked up with by `key`
-        # If the name of the series is not `key`, we can get in a situation where
-        # looking up the series name raises a SeriesNotFoundError. To avoid this
-        # problematic situation, we check if it can be looked up, and if not,
-        # add it a second time to the cached_series, now under `series.name`
-        try:
-            _ = self[series.name]
-        except SeriesNotFoundError:
-            self._cached_series[series.name] = series
-
-    def get_series(self, key):
-        """Find or build the data series corresponding to key without direct cache'ing
-
-        See more detailed documentation under `__getitem__`, for which this is a
-        helper method. This method (A) looks for a method for `key` in the measurement's
-        `series_constructors`; (B) requests its `calibration` for `key`; and if those
-        fail appends the data series that either (Ci) are returned by looking up the
-        key's `aliases` or (Cii) have `key` as their name; and finally (D) check if the
-        user was using a key with a suffix.
-
-        Args:
-            key (str): The key to look up
-
-        Returns DataSeries: the data series corresponding to key
-        Raises SeriesNotFoundError if no series found for key
-        """
-        # A
-        if key in self.series_constructors:
-            return getattr(self, self.series_constructors[key])()
-        # B
-        for calibration in self.calibrations:
-            series = calibration.calibrate_series(key, measurement=self)
-            # ^ the calibration will call __getitem__ with the name of the
-            #   corresponding raw data and return a new series with calibrated data
-            #   if possible. Otherwise it will return None.
-            if series:
-                return series
-        # C
-        series_to_append = []
-        if key in self.series_names:  # ii
-            # Then we'll append any series matching the desired name
-            series_to_append += [s for s in self.series_list if s.name == key]
-        if key in self.aliases:  # i
-            # Then we'll look up the aliases instead and append them
-            for k in self.aliases[key]:
-                if k == key:  # this would result in infinite recursion.
-                    print(  # TODO: Real warnings.
-                        "WARNING!!!\n"
-                        f"\t{self!r} has {key} in its aliases for {key}:\n"
-                        f"\tself.aliases['{key}'] = {self.aliases[key]}"
-                    )
-                    continue
-                try:
-                    series_to_append.append(self[k])
-                except SeriesNotFoundError:
-                    continue
-        # If the key is something in the data, by now we have series to append.
-        if series_to_append:
-            # the following if's are to do as little extra manipulation as possible:
-            if len(series_to_append) == 1:  # no appending needed
-                if series_to_append[0].tstamp == self.tstamp:  # no time-shifting needed
-                    return series_to_append[0]
-                return time_shifted(series_to_append[0], tstamp=self.tstamp)
-            return append_series(series_to_append, name=key, tstamp=self.tstamp)
-        # D
-        if key.endswith("-t") or key.endswith("-x"):
-            return self[key[:-2]].tseries
-        if key.endswith("-v") or key.endswith("-y"):
-            return self[key[:-2]]
-
-        raise SeriesNotFoundError(f"{self!r} does not contain '{key}'")
-
-    def replace_series(self, series_name, new_series=None):
-        """Remove an existing series, add a series to the measurement, or both.
-
-        FIXME: This will not appear to change the series for the user if the
-            measurement's calibration returns something for ´series_name´, since
-            __getitem__ asks the calibration before looking in series_list.
-
-        Args:
-            series_name (str): The name of a series. If the measurement has (raw) data
-                series with this name, cached series with this name, and/or aliases for
-                this name, they will be removed.
-            new_series (DataSeries): Optional new series to append to the measurement's
-                series_list. To sanity check, it must have ´series_name´ as its ´name´.
-        """
-        if new_series and not series_name == new_series.name:
-            raise TypeError(
-                f"Cannot replace {series_name} in {self!r} with {new_series}. "
-                f"Names must agree."
-            )
-        if series_name in self._cached_series:
-            del self._cached_series[series_name]
-        if series_name in self._aliases:
-            del self._aliases[series_name]
-        new_series_list = [s for s in self.series_list if not s.name == series_name]
-        if new_series:
-            new_series_list.append(new_series)
-        self._series_list = new_series_list
-
-    def clear_cache(self):
-        """Clear the cache so derived series are constructed again with updated info"""
-        self._cached_series = {}
-
-    def correct_data(self, value_name, new_data):
-        """Replace the old data for ´value_name´ (str) with ´new_data` (np array)"""
-        old_vseries = self[value_name]
-        new_vseries = ValueSeries(
-            name=value_name,
-            unit_name=old_vseries.unit_name,
-            data=new_data,
-            tseries=old_vseries.tseries,
-        )
-        self.replace_series(value_name, new_vseries)
-
-    def grab(self, item, tspan=None, include_endpoints=False, tspan_bg=None):
-        """Return a value vector with the corresponding time vector
-
-        Grab is the *canonical* way to retrieve numerical time-dependent data from a
-        measurement in ixdat. The first argument is always the name of the value to get
-        time-resolved data for (the name of a ValueSeries). The second, optional,
-        argument is a timespan to select the data for.
-        Two vectors are returned: first time (t), then value (v). They are of the same
-        length so that `v` can be plotted against `t`, integrated over `t`, interpolated
-        via `t`, etc. `t` and `v` are returned in the units of their DataSeries.
-        TODO: option to specifiy desired units
-
-        Typical usage::
-            t, v = measurement.grab("potential", tspan=[0, 100])
-
-        Args:
-            item (str): The name of the DataSeries to grab data for
-                TODO: Should this be called "name" or "key" instead? And/or, should
-                   the argument to __getitem__ be called "item" instead of "key"?
-            tspan (iter of float): Defines the timespan with its first and last values.
-                Optional. By default the entire time of the measurement is included.
-            include_endpoints (bool): Whether to add a points at t = tspan[0] and
-                t = tspan[-1] to the data returned. This makes trapezoidal integration
-                less dependent on the time resolution. Default is False.
-            tspan_bg (iterable): Optional. A timespan defining when `item` is at its
-                baseline level. The average value of `item` in this interval will be
-                subtracted from the values returned.
-        """
-        vseries = self[item]
-        tseries = vseries.tseries
-        v = vseries.data
-        t = tseries.data + tseries.tstamp - self.tstamp
-        if tspan is not None:  # np arrays don't boolean well :(
-            if include_endpoints:
-                if t[0] < tspan[0]:  # then add a point to include tspan[0]
-                    v_0 = np.interp(tspan[0], t, v)
-                    t = np.append(tspan[0], t)
-                    v = np.append(v_0, v)
-                if tspan[-1] < t[-1]:  # then add a point to include tspan[-1]
-                    v_end = np.interp(tspan[-1], t, v)
-                    t = np.append(t, tspan[-1])
-                    v = np.append(v, v_end)
-            mask = np.logical_and(tspan[0] <= t, t <= tspan[-1])
-            t, v = t[mask], v[mask]
-        if tspan_bg:
-            t_bg, v_bg = self.grab(item, tspan=tspan_bg)
-            v = v - np.mean(v_bg)
-        return t, v
-
-    def grab_for_t(self, item, t, tspan_bg=None):
-        """Return a numpy array with the value of item interpolated to time t
-
-        Args:
-            item (str): The name of the value to grab
-            t (np array): The time vector to grab the value for
-            tspan_bg (iterable): Optional. A timespan defining when `item` is at its
-                baseline level. The average value of `item` in this interval will be
-                subtracted from what is returned.
-        """
-        vseries = self[item]
-        tseries = vseries.tseries
-        v_0 = vseries.data
-        t_0 = tseries.data + tseries.tstamp - self.tstamp
-        v = np.interp(t, t_0, v_0)
-        if tspan_bg:
-            t_bg, v_bg = self.grab(item, tspan=tspan_bg)
-            v = v - np.mean(v_bg)
-        return v
-
-    def integrate(self, item, tspan=None, ax=None):
-        """Return the time integral of item in the specified timespan"""
-        t, v = self.grab(item, tspan, include_endpoints=True)
-        if ax:
-            if ax == "new":
-                ax = self.plotter.new_ax(ylabel=item)
-                # FIXME: xlabel=self[item].tseries.name gives a problem :(
-            ax.plot(t, v, color="k", label=item)
-            ax.fill_between(t, v, np.zeros(t.shape), where=v > 0, color="g", alpha=0.3)
-            ax.fill_between(
-                t, v, np.zeros(t.shape), where=v < 0, color="g", alpha=0.1, hatch="//"
-            )
-
-        return np.trapz(v, t)
-
-    @property
-    def t(self):
-        return self[self.control_series_name].t
-
-    @property
-    def t_name(self):
-        return self[self.control_series_name].tseries.name
-
-    def _build_file_number_series(self):
-        """Build a `file_number` series based on component measurements times."""
-        series_to_append = []
-        for i, m in enumerate(self.component_measurements or [self]):
-            if (
-                self.control_technique_name
-                and not m.technique == self.control_technique_name
-            ):
-                continue
-            if not self.control_series_name:
-                tseries = m.time_series[0]
-            else:
-                try:
-                    tseries = m[self.control_series_name].tseries
-                except SeriesNotFoundError:
-                    continue
-            series_to_append.append(
-                ConstantValue(name="file_number", unit_name="", data=i, tseries=tseries)
-            )
-        return append_series(series_to_append, name="file_number", tstamp=self.tstamp)
-
-    def _build_selector_series(
-        self, selector_string=None, columns=None, extra_columns=None
-    ):
-        """Build a `selector` series which demarcates the data.
-
-        The `selector` is a series which can be used to conveniently and powerfully
-        grab sections of the data. It is built up from less powerful demarcation series
-        in the raw data (like `cycle_number`, `step_number`, `loop_number`, etc) and
-        `file_number` by counting the cumulative changes in those series.
-        See slide 3 of:
-        https://www.dropbox.com/s/sjxzr52fw8yml5k/21E18_DWS3_cont.pptx?dl=0
-
-        Args:
-            selector_string (str): The name to use for the selector series
-            columns (list): The list of demarcation series. The demarcation series have
-                to have equal-length tseries, which should be the one pointed to by the
-                meausrement's `control_series_name`.
-            extra_columns (list): Extra demarcation series to include if needed.
-        """
-        # the name of the selector series:
-        selector_string = selector_string or self.selector_name
-        # a vector that will be True at the points where a series changes:
-        changes = np.tile(False, self.t.shape)
-        # the names of the series which help demarcate the data
-        columns = columns or self.selection_series_names
-        if extra_columns:
-            columns += extra_columns
-        for col in columns:
-            try:
-                vseries = self[col]
-            except SeriesNotFoundError:
-                continue
-            values = vseries.data
-            if len(values) == 0:
-                print("WARNING: " + col + " is empty")
-                continue
-            elif not len(values) == len(changes):
-                print("WARNING: " + col + " has an unexpected length")
-                continue
-            # a vector which is shifted one.
-            last_value = np.append(values[0], values[:-1])
-            # comparing value and last_value shows where in the vector changes occur:
-            changes = np.logical_or(changes, last_value != values)
-        # taking the cumsum makes a vector that increases 1 each time one of the
-        #   original demarcation vector changes
-        selector_data = np.cumsum(changes)
-        selector_series = ValueSeries(
-            name=selector_string,
-            unit_name="",
-            data=selector_data,
-            tseries=self[self.control_series_name].tseries,
-        )
-        return selector_series
-
-    def rebuild_selector(self, selector_string=None, columns=None, extra_columns=None):
-        """Build a new selector series for the measurement and cache it.
-
-        This can be useful if a user wants to change how their measurement counts
-        sections (for example, only count sections when technique or file number changes)
-
-        Args:
-            selector_string (str): The name to use for the selector series
-            columns (list): The list of demarcation series. The demarcation series have
-                to have the same tseries, which should be the one pointed to by the
-                meausrement's `control_series_name`.
-            extra_columns (list): Extra demarcation series to include if needed.
-        """
-        selector_string = selector_string or self.selector_name
-        selector_series = self._build_selector_series(
-            selector_string=selector_string,
-            columns=columns,
-            extra_columns=extra_columns,
-        )
-        self._cache_series(selector_string, selector_series)
-        return selector_series
-
-    @property
-    def selector(self):
-        return self[self.selector_name]
-
-    @property
-    def data_cols(self):
-        """Return a set of the names of all of the measurement's VSeries and TSeries"""
-        return set([s.name for s in (self.value_series + self.time_series)])
-
-    def get_original_m_ids_of_series(self, series):
-        """Return a list of id's of component measurements to which `series` belongs."""
-        m_id_list = []
-        for m in self.component_measurements:
-            if series.short_identity in m.s_ids:
-                # FIXME: the whole id vs short_identity issue
-                #   see https://github.com/ixdat/ixdat/pull/11#discussion_r746632897
-                m_id_list.append(m.id)
-        return m_id_list
-
-    @property
-    def tspan(self):
-        """The minimum timespan (with respect to self.tstamp) containing all the data"""
-        t_start = None
-        t_finish = None
-        if not self.time_names:  # No TimeSeries in the measurement means no tspan.
-            return None
-        for t_name in self.time_names:
-            t = self[t_name].data
-            if len(t) == 0:
-                return None
-            t_start = t[0] if t_start is None else min(t_start, t[0])
-            t_finish = t[-1] if t_finish is None else max(t_finish, t[-1])
-        return [t_start, t_finish]
-
-    def cut(self, tspan, t_zero=None):
-        """Return a new measurement with the data in the given time interval
-
-        Args:
-            tspan (iter of float): The time interval to use, relative to self.tstamp
-                tspan[0] is the start time of the interval, and tspan[-1] is the end
-                time of the interval. Using tspan[-1] means you can directly use a
-                long time vector that you have at hand to describe the time interval
-                you're looking for.
-            t_zero (float or str): The time in the measurement to set to t=0. If a
-                float, it is interpreted as wrt the original tstamp. String options
-                include "start", which puts t=0 at the start of the cut interval.
-        """
-        # Start with self's dictionary representation, but
-        # we don't want original series (s_ids) or component_measurements (m_ids):
-        obj_as_dict = self.as_dict(exclude=["s_ids", "m_ids"])
-
-        # first, cut the series list:
-        new_series_list = []
-        time_cutting_stuff = {}  # {tseries_id: (mask, new_tseries)}
-        for series in self.series_list:
-            try:
-                tseries = series.tseries
-                if tseries is None:
-                    raise AttributeError
-            except AttributeError:  # series independent of time are uneffected by cut
-                new_series_list.append(series)
-            else:
-                t_identity = tseries.full_identity
-
-                if t_identity in time_cutting_stuff:
-                    mask, new_tseries = time_cutting_stuff[t_identity]
-                else:
-                    t = tseries.t + tseries.tstamp - self.tstamp
-                    mask = np.logical_and(tspan[0] <= t, t <= tspan[-1])
-                    new_tseries = TimeSeries(
-                        name=tseries.name,
-                        unit_name=tseries.unit_name,
-                        tstamp=tseries.tstamp,
-                        data=tseries.data[mask],
-                    )
-                    time_cutting_stuff[t_identity] = (mask, new_tseries)
-                if True not in mask:
-                    continue
-                if False not in mask:
-                    new_series_list.append(series)
-                elif series.full_identity == t_identity:
-                    new_series_list.append(new_tseries)
-                else:
-                    new_series = series.__class__(
-                        name=series.name,
-                        unit_name=series.unit_name,
-                        data=series.data[mask],
-                        tseries=new_tseries,
-                    )
-                    new_series_list.append(new_series)
-        obj_as_dict["series_list"] = new_series_list
-
-        # then cut the component measurements.
-        new_component_measurements = []
-        for m in self._component_measurements:
-            # FIXME: This is perhaps overkill, to make new cut component measurements,
-            #    as it duplicates data (a big no)... especially bad because
-            #    new_measurement.save() saves them.
-            #    The step is here in order for file_number to get built correctly.
-            if not m.tspan:
-                # if it has no TimeSeries it must be a "constant". Best to include:
-                new_component_measurements.append(m)
-                continue
-            # Otherwise we have to cut it according to the present tspan.
-            dt = m.tstamp - self.tstamp
-            try:
-                tspan_m = [tspan[0] - dt, tspan[1] - dt]
-            except IndexError:  # Apparently this can happen for empty files. See:
-                continue  # https://github.com/ixdat/ixdat/issues/93
-            if m.tspan[-1] < tspan_m[0] or tspan_m[-1] < m.tspan[0]:
-                continue
-            new_component_measurements.append(m.cut(tspan_m))
-        obj_as_dict["component_measurements"] = new_component_measurements
-
-        new_measurement = self.__class__.from_dict(obj_as_dict)
-        if t_zero:
-            if t_zero == "start":
-                new_measurement.tstamp += tspan[0]
-            else:
-                new_measurement.tstamp += t_zero
-        return new_measurement
-
-    def multicut(self, tspans):
-        """Return a selection of the measurement including each of the given tspans"""
-        # go through the tspans, cuting the measurement and appending the results
-        new_measurement = None
-        for tspan in tspans:
-            if new_measurement:
-                new_measurement = new_measurement + self.cut(tspan)
-            else:
-                new_measurement = self.cut(tspan)
-        return new_measurement
-
-    def select_value(self, *args, **kwargs):
-        """Return a selection of the measurement where a criterion is matched.
-
-        Specifically, this method returns a new Measurement where the time(s) returned
-        are those where the values match the provided criteria, i.e. the part of the
-        measurement where `self[series_name] == value`
-
-        Can only take one arg or kwarg!
-        The `series_name` is `self.selector_name` if given an argument without keyword.
-        If given a keyword argument, the kyword is the name of the series to select on.
-        Either way the argument is the `value` to be selected for.
-
-        The method finds all time intervals for which `self[series_name] == value`
-        It then cuts the measurement according to each time interval and adds these
-        segments together.
-        TODO: This can maybe be done better, i.e. without chopping series.
-        TODO: Some way of less than and greater than kwargs.
-            Ideally you should be able to say e.g., `select(cycle=1, 0.5<potential<1)`
-            But this is hard,
-            see: https://github.com/ixdat/ixdat/pull/11#discussion_r677272239
-        """
-        if len(args) + len(kwargs) != 1:
-            raise BuildError("Need exactly 1 arg. Use `select_values` for more.")
-        if args:
-            if not self.selector_name:
-                raise BuildError(
-                    f"{self!r} does not have a default selection string "
-                    f"(Measurement.sel_str), and so selection only works with kwargs."
-                )
-            kwargs[self.selector_name] = args[0]
-
-        ((series_name, value),) = kwargs.items()
-
-        # The time and values of the series to be selected on:
-        t, v = self.grab(series_name)
-        # This mask is true everywhere on `t` that the condition is met:
-        mask = v == value  # linter doesn't realize this is a np array
-
-        # Now we have to convert that to timespans on which `t` is met. This means
-        #  finding the start and finish times of the intervals on which mask is True.
-        #  this is done with a helper function:
-        tspans = get_tspans_from_mask(t, mask)
-
-        # now we go through the tspans, cuting the measurement and appending the results:
-        return self.multicut(tspans)
-
-    def select_values(self, *args, selector_name=None, **kwargs):
-        """Return a selection of the measurement based on one or several criteria
-
-        Specifically, this method returns a new Measurement where the time(s) returned
-        are those where the values match the provided criteria, i.e. the part of the
-        measurement where `self[series_name] == value`
-
-        Any series can be selected for using the series name as a key-word. Arguments
-        can be single acceptable values or lists of acceptable values.
-        You can select for one or more series without valid python variable names by
-        providing the kwargs using ** notation (see last example below).
-
-        Arguments without key-word are considered valid values of the default
-        selector, which is normally `self.selector_name` but can also be specified
-        here using the key-word argument `selector_name`. Multiple criteria are
-        applied sequentially, i.e. you get the intersection of satisfying parts.
-
-        Examples of valid calls given a measurement `meas`:
-        ```
-        # to select where the default selector is 3, use:
-        meas.select_values(3)
-        # to select for where the default selector is 4 or 5:
-        meas.select_values(4, 5)
-        # to select for where "cycle" (i.e. the value of meas["cycle"].data) is 4:
-        meas.select_values(cycle=4)
-        # to select for where "loop_number" is 1 AND "cycle" is 3, 4, or 5:
-        meas.select_values(loop_number=1, cycle=[3, 4, 5])
-        # to select for where "cycle number" (notice the space) is 2 or 3:
-        meas.select_values([2, 3], selector_name="cycle number")
-        # which is equivalent to:
-        meas.select_values(**{"cycle number": [2, 3]})
-
-        Args:
-            args (tuple): Argument(s) given without keyword are understood as acceptable
-                value(s) for the selector (that named by selector_name or
-                self.selector_name).
-            selector_name: The name of the selector to which the args specify
-            kwargs (dict): Each key-word arguments is understood as the name
-                of a series and its acceptable value(s).
-        """
-        if args:
-            # Then we must interpret the arguments as allowed values of a selector,
-            #  either specified in the kwargs or the Measurement's default selector:
-            selector_name = selector_name or self.selector_name
-            if not selector_name:
-                raise BuildError(
-                    f"{self:r} does not have a default selector_name "
-                    f"(Measurement.selector_name), and so selection only works "
-                    f"with a selector_name specified "
-                    f"(see `help(Measurement.select_values)`)"
-                )
-            # Get the args into a simple list:
-            flat_args = []
-            for arg in args:
-                if hasattr(arg, "__iter__"):
-                    flat_args += list(arg)
-                else:
-                    flat_args.append(arg)
-            if selector_name in kwargs:
-                raise ValueError(
-                    "Don't call select_values with both arguments and "
-                    "'{self.selector_name}' as a key-word argument"
-                )
-            kwargs[self.selector_name] = flat_args
-
-        t = self.t
-        mask = np.tile(np.array([True]), t.shape)
-        for series_name, allowed_values in kwargs.items():
-            if not hasattr(allowed_values, "__iter__"):
-                allowed_values = [allowed_values]
-            v = self.grab_for_t(series_name, t)
-            submask = np.tile(np.array([False]), t.shape)
-            for allowed_value in allowed_values:
-                submask = np.logical_or(submask, v == allowed_value)
-            mask = np.logical_and(mask, submask)
-
-        tspans = get_tspans_from_mask(t, mask)
-
-        return self.multicut(tspans)
-
-    def select(self, *args, tspan=None, **kwargs):
-        """`cut` (with tspan) and `select_values` (with *args and/or **kwargs).
-
-        These all work for measurements that have a default selector and/or the
-        indicated columns:
-        - `meas.select(1, 2)`
-        - `meas.select(tspan=[200, 300])`
-        - `meas.select(range(10))`
-        - `meas.select(cycle=4)`
-        - `meas.select(**{"cycle number": [20, 21]})
-        - `meas.select(loop_number=1, tspan=[1000, 2000])
-        - `meas.select(1, range(5, 20), file_number=1, tspan=[1000, 2000])`
-        """
-        new_measurement = self
-        if tspan:
-            new_measurement = new_measurement.cut(tspan=tspan)
-        if args or kwargs:
-            new_measurement = new_measurement.select_values(*args, **kwargs)
-        return new_measurement
-
-    def copy(self):
-        """Make a copy of the Measurement via its dictionary representation"""
-        return self.__class__.from_dict(self.as_dict())
-
-    def __add__(self, other):
-        """Addition of measurements appends the series and component measurements lists.
-
-        Adding results in a new Measurement. If the combination of the two measurements'
-        techniques is a recognized hyphenated technique, it returns an object of that
-        technique's measurement class. Otherwise it returns an object of Measurement.
-        metadata, sample, and logentry come from the first measurement.
-
-        An important point about addition is that it is almost but not quite associative
-        and commutative i.e.
-        A + (B + C) == (A + B) + C == C + B + A   is not quite true
-        Each one results in the same series and component measurements. They will even
-        appear in the same order in A + (B + C) and (A + B) + C. However, the technique
-        might be different, as a new technique might be determined each time.
-
-        Note also that there is no difference between hyphenating (simultaneous EC and
-        MS datasets, for example) and appending (sequential EC datasets). Either way,
-        all the raw series (or their placeholders) are just stored in the lists.
-        """
-        from .spectra import SpectrumSeries, add_spectrum_series_to_measurement
-
-        if isinstance(other, SpectrumSeries):
-            return add_spectrum_series_to_measurement(self, other)
-
-        new_name = self.name + " AND " + other.name
-        new_technique = get_combined_technique(self.technique, other.technique)
-
-        # TODO: see if there isn't a way to put the import at the top of the module.
-        #    see: https://github.com/ixdat/ixdat/pull/1#discussion_r546437410
-        from .techniques import TECHNIQUE_CLASSES
-
-        if new_technique in TECHNIQUE_CLASSES:
-            cls = TECHNIQUE_CLASSES[new_technique]
-        elif self.__class__ is other.__class__:
-            cls = self.__class__
-        else:
-            cls = Measurement
-
-        new_series_list = list(set(self.series_list + other.series_list))
-        new_component_measurements = list(
-            set(
-                (self.component_measurements or [self])
-                + (other.component_measurements or [other])
-            )
-        )
-        new_calibration_list = list(
-            set(self._calibration_list + other._calibration_list)
-        )
-        new_aliases = self.aliases.copy()
-        for key, names in other.aliases.items():
-            if key in new_aliases:
-                new_aliases[key] = list(set(new_aliases[key] + other.aliases[key]))
-            else:
-                new_aliases[key] = other.aliases[key]
-        obj_as_dict = self.as_dict()
-        other_as_dict = other.as_dict()
-        for k, v in other_as_dict.items():
-            # Looking forward to the "|" operator!
-            if k not in obj_as_dict:
-                obj_as_dict[k] = v
-        obj_as_dict.update(
-            name=new_name,
-            technique=new_technique,
-            series_list=new_series_list,
-            component_measurements=new_component_measurements,
-            calibration_list=new_calibration_list,
-            aliases=new_aliases,
-        )
-        # don't want the original calibrations, component measurements, or series:
-        del obj_as_dict["c_ids"]
-        del obj_as_dict["m_ids"]
-        del obj_as_dict["s_ids"]
-        return cls.from_dict(obj_as_dict)
-
-    def join(self, other, join_on=None):
-        """Join two measurements based on a shared data series
-
-        This involves projecting all timeseries from other's data series so that the
-        variable named by `join_on` is shared between all data series.
-        This is analogous to an explicit inner join.
-
-        Args:
-            other (Measurement): a second measurement to join to self
-            join_on (str or tuple): Either a string, if the value to join on is called
-                the same thing in both measurements, or a tuple of two strings where
-                the first is the name of the variable in self and the second in other.
-                The variable described by join_on must be monotonically increasing in
-                both measurements.
-        """
-        raise NotImplementedError
-
-
-class Calibration(Saveable):
-    """Base class for calibrations."""
-
-    table_name = "calibration"
-    column_attrs = {
-        "name",
-        "technique",
-        "tstamp",
-    }
-
-    def __init__(self, *, name=None, technique=None, tstamp=None, measurement=None):
-        """Initiate a Calibration
-
-        Args:
-            name (str): The name of the calibration
-            technique (str): The technique of the calibration
-            tstamp (float): The time at which the calibration took place or is valid
-            measurement (Measurement): Optional. A measurement to calibrate by default.
-        """
-        super().__init__()
-        # NOTE: The :r syntax in f-strings doesn't work on None
-        self.name = name or f"{self.__class__.__name__}({repr(measurement)})"
-        self.technique = technique
-        self.tstamp = tstamp or (measurement.tstamp if measurement else None)
-        self.measurement = measurement
-
-    @classmethod
-    def from_dict(cls, obj_as_dict):
-        """Return an object of the Calibration class of the right technique
-
-        Args:
-              obj_as_dict (dict): The full serializaiton (rows from table and aux
-                tables) of the measurement. obj_as_dict["technique"] specifies the
-                technique class to use, from TECHNIQUE_CLASSES
-        """
-        # TODO: see if there isn't a way to put the import at the top of the module.
-        #    see: https://github.com/ixdat/ixdat/pull/1#discussion_r546437410
-        from .techniques import CALIBRATION_CLASSES
-
-        if obj_as_dict["technique"] in CALIBRATION_CLASSES:
-            calibration_class = CALIBRATION_CLASSES[obj_as_dict["technique"]]
-        else:
-            calibration_class = cls
-        try:
-            calibration = calibration_class(**obj_as_dict)
-        except Exception:
-            raise
-        return calibration
-
-    def export(self, path_to_file=None):
-        """Export an ECMSCalibration as a json-formatted text file"""
-        path_to_file = path_to_file or (self.name + ".ix")
-        self_as_dict = self.as_dict()
-        with open(path_to_file, "w") as f:
-            json.dump(self_as_dict, f, indent=4)
-
-    @classmethod
-    def read(cls, path_to_file):
-        """Read a Calibration from a json-formatted text file"""
-        with open(path_to_file) as f:
-            obj_as_dict = json.load(f)
-        return cls.from_dict(obj_as_dict)
-
-    def calibrate_series(self, key, measurement=None):
-        """This should be overwritten in real calibration classes.
-
-        FIXME: Add more documentation about how to write this in inheriting classes.
-        """
-        raise NotImplementedError
-
-
-def get_combined_technique(technique_1, technique_2):
-    """Return the name of the technique resulting from adding two techniques"""
-    # TODO: see if there isn't a way to put the import at the top of the module.
-    #    see: https://github.com/ixdat/ixdat/pull/1#discussion_r546437410
-    if technique_1 == technique_2:
-        return technique_1
-
-    # if we're a component technique of a hyphenated technique to that hyphenated
-    # technique, the result is still the hyphenated technique. e.g. EC-MS + MS = EC-MS
-    if "-" in technique_1 and technique_2 in technique_1.split("-"):
-        return technique_1
-    elif "-" in technique_2 and technique_1 in technique_2.split("-"):
-        return technique_2
-
-    # if we're adding two independent technique which are components of a hyphenated
-    # technique, then we want that hyphenated technique. e.g. EC + MS = EC-MS
-    from .techniques import TECHNIQUE_CLASSES
-
-    for hyphenated in [
-        technique_1 + "-" + technique_2,
-        technique_2 + "-" + technique_1,
-    ]:
-        if hyphenated in TECHNIQUE_CLASSES:
-            return hyphenated
-
-    # if all else fails, we just join them with " and ". e.g. MS + XRD = MS and XRD
-    return technique_1 + " and " + technique_2
+"""This module defines the Measurement class, the central data structure of ixdat
+
+An ixdat Measurement is a collection of references to DataSeries and the metadata needed
+to combine them, i.e. "build" the combined dataset. It has a number of general methods
+to visualize and analyze the combined dataset. Measurement is also the base class for a
+number of technique-specific Measurement-derived classes.
+
+A Measurement will typically be accompanied by one or more Calibration. This module
+also defines the base class for Calibration, while technique-specific Calibration
+classes will be defined in the corresponding module in ./techniques/
+"""
+import json
+import numpy as np
+from .db import Saveable, PlaceHolderObject, fill_object_list
+from .data_series import (
+    DataSeries,
+    TimeSeries,
+    ValueSeries,
+    ConstantValue,
+    append_series,
+    time_shifted,
+    get_tspans_from_mask,
+)
+from .projects.samples import Sample
+from .projects.lablogs import LabLog
+from .exporters.csv_exporter import CSVExporter
+from .plotters.value_plotter import ValuePlotter
+from .exceptions import BuildError, SeriesNotFoundError, TechniqueError, ReadError
+from .tools import deprecate, tstamp_to_string
+
+
+class Measurement(Saveable):
+    """The Measurement class"""
+
+    # ------ table description class attributes --------
+    table_name = "measurement"
+    column_attrs = {
+        "name",
+        "technique",
+        "metadata",
+        "aliases",
+        "sample_name",
+        "tstamp",
+    }
+    extra_linkers = {
+        "component_measurements": ("measurements", "m_ids"),
+        "measurement_calibrations": ("calibrations", "c_ids"),
+        "measurement_series": ("data_series", "s_ids"),
+    }
+    child_attrs = ["component_measurements", "calibration_list", "series_list"]
+    # TODO: child_attrs should be derivable from extra_linkers?
+
+    # ---- measurement class attributes, can be overwritten in inheriting classes ---- #
+    control_technique_name = None
+    """Name of the control technique primarily used to control the experiment"""
+    control_series_name = None
+    """Name (or alias) for main time variable or main time-dependent value variable,
+    typically of the control technique"""
+    selector_name = "selector"
+    """Name of the default selector"""
+    selection_series_names = ("file_number",)
+    """Name of the default things to use to construct the selector"""
+    series_constructors = {
+        "file_number": "_build_file_number_series",
+        "selector": "_build_selector_series",
+    }
+    """Series which should be constructed from other series by the specified method
+    and cached the first time they are looked up"""
+    essential_series_names = None
+    """Series which should always be present"""
+    default_plotter = ValuePlotter
+    default_exporter = CSVExporter
+
+    def __init__(
+        self,
+        name,
+        technique=None,
+        metadata=None,
+        s_ids=None,
+        series_list=None,
+        c_ids=None,
+        calibration_list=None,
+        m_ids=None,
+        component_measurements=None,
+        aliases=None,
+        reader=None,
+        plotter=None,
+        exporter=None,
+        sample=None,
+        lablog=None,
+        tstamp=None,
+    ):
+        """initialize a measurement
+
+        Args:
+            name (str): The name of the measurement
+            metadata (dict): Free-form measurement metadata. Must be json-compatible.
+            technique (str): The measurement technique
+            s_ids (list of int): The id's of the measurement's DataSeries, if
+                to be loaded (instead of given directly in series_list)
+            series_list (list of DataSeries): The measurement's DataSeries
+            c_ids (list of int): The id's of the measurement's Calibrations, if
+                to be loaded (instead of given directly in calibration_list)
+            calibration_list: The measurement's Calibrations
+            m_ids (list of int): The id's of the component measurements, if to be
+                loaded. None unless this is a combined measurement (typically
+                corresponding to more than one file).
+            component_measurements (list of Measurements): The measurements of which
+                this measurement is a combination
+            aliases (dict): Alternative names for DataSeries for versatile access
+            reader (Reader): The file reader (None unless read from a file)
+            plotter (Plotter): The visualization tool for the measurement
+            exporter (Exporter): The exporting tool for the measurement
+            sample (Sample or str): The sample being measured
+            lablog (LabLog): The log entry with e.g. notes taken during the measurement
+            tstamp (float): The nominal starting time of the measurement, used for
+                data selection, visualization, and exporting.
+        """
+        super().__init__()
+        self.name = name
+        self.technique = technique
+        self.metadata = metadata or {}
+        self.reader = reader
+        if isinstance(sample, str):
+            sample = Sample.load_or_make(sample)
+        self.sample = sample
+        if isinstance(lablog, str):
+            lablog = LabLog.load_or_make(lablog)
+        self.lablog = lablog
+        self._series_list = fill_object_list(series_list, s_ids, cls=DataSeries)
+        self._component_measurements = fill_object_list(
+            component_measurements, m_ids, cls=Measurement
+        )
+        self._calibration_list = fill_object_list(
+            calibration_list, c_ids, cls=Calibration
+        )
+        self._tstamp = tstamp
+
+        self._cached_series = {}
+        self._aliases = aliases or {}
+
+        self.plotter = plotter or self.__class__.default_plotter(measurement=self)
+        self.exporter = exporter or self.__class__.default_exporter(measurement=self)
+        # defining these methods here gets them the right docstrings :D
+        self.plot_measurement = self.plotter.plot_measurement
+        self.plot = self.plotter.plot_measurement
+        self.export = self.exporter.export
+        # TODO: ... but we need to think a bit more about how to most elegantly and
+        #    dynamically choose plotters (Nice idea from Anna:
+        #    https://github.com/ixdat/ixdat/issues/32)
+
+    def __str__(self):
+        """Return string representation"""
+        tseries_to_valueseries = {}
+        for series in self.series_list:
+            if isinstance(series, TimeSeries):
+                if series not in tseries_to_valueseries:
+                    tseries_to_valueseries[series] = []
+            else:
+                if series.tseries in tseries_to_valueseries:
+                    tseries_to_valueseries[series.tseries].append(series)
+                else:
+                    tseries_to_valueseries[series.tseries] = [series]
+
+        out = []
+        for tseries, value_serieses in tseries_to_valueseries.items():
+            out.append("┏ " + str(tseries))
+            for n, value_series in enumerate(value_serieses):
+                if n == len(value_serieses) - 1:
+                    out.append("┗━ " + str(value_series))
+                else:
+                    out.append("┣━ " + str(value_series))
+
+        return (
+            f"{self.__class__.__name__} '{self.name}' with {len(self.series_list)} "
+            "series\n\n"
+            "Series list:\n" + "\n".join(out)
+        )
+
+    @classmethod
+    def from_dict(cls, obj_as_dict):
+        """Return an object of the measurement class of the right technique
+
+        Args:
+              obj_as_dict (dict): The full serializaiton (rows from table and aux
+                tables) of the measurement. obj_as_dict["technique"] specifies the
+                technique class to use, from TECHNIQUE_CLASSES
+        """
+        # TODO: see if there isn't a way to put the import at the top of the module.
+        #    see: https://github.com/ixdat/ixdat/pull/1#discussion_r546437410
+        from .techniques import TECHNIQUE_CLASSES
+
+        # certain objects stored in the Measurement, but only saved as their names.
+        #   __init__() will get the object from the name, but the argument is
+        #   called like the object either way. For example __init__() takes an argument
+        #   called `sample` which can be an ixdat.Sample or a string interpreted as the
+        #   name of the sample to load. Subsequently, the sample name is accessible as
+        #   the property `sample_name`. But in the database is only saved the sample's
+        #   name as a string with the key/column "sample_name". So
+        #   obj_as_dict["sample_name"] needs to be renamed obj_as_dict["sample"] before
+        #   obj_as_dict can be passed to __init__.
+        #   TODO: This is a rather general problem (see, e.g. DataSeries.unit vs
+        #       DataSeries.unit_name) and as such should be moved to db.Saveable
+        #       see: https://github.com/ixdat/ixdat/pull/5#discussion_r565090372.
+        #       Will be fixed with the table definition PR.
+        objects_saved_as_their_name = ["sample"]
+        for object_type_str in objects_saved_as_their_name:
+            object_name_str = object_type_str + "_name"
+            if object_name_str in obj_as_dict:
+                obj_as_dict[object_type_str] = obj_as_dict[object_name_str]
+                del obj_as_dict[object_name_str]
+
+        if obj_as_dict["technique"] in TECHNIQUE_CLASSES:
+            # This makes it so that from_dict() can be used to initiate for any more
+            # derived technique, so long as obj_as_dict specifies the technique name!
+            technique_class = TECHNIQUE_CLASSES[obj_as_dict["technique"]]
+            if not issubclass(technique_class, cls):
+                # But we never want obj_as_dict["technique"] to take us to a *less*
+                # specific technique, if the user has been intentional about which
+                # class they call `as_dict` from (e.g. via a Reader)!
+                technique_class = cls
+        else:
+            technique_class = cls
+        try:
+            measurement = technique_class(**obj_as_dict)
+        except TypeError as e:
+            raise TechniqueError(
+                "ixdat ran into an error while trying to set up an object of type\n"
+                f"  {technique_class}. This usually happens when ixdat wasn't able\n"
+                f"  to correctly determine the measurement technique. Error: \n{e}\n\n"
+                "Consider passing the `technique` argument into the read() function.\n"
+                "For a list of available techniques use: \n "
+                ">>> from ixdat.techniques import TECHNIQUE_CLASSES\n"
+                ">>> print(TECHNIQUE_CLASSES.keys())\n"
+            )
+        return measurement
+
+    @classmethod
+    def read(cls, path_to_file, reader=None, **kwargs):
+        """Return a Measurement object from parsing a file with the specified reader
+
+        Args:
+            path_to_file (Path or str): The path to the file to read
+            reader (str or Reader class): The (name of the) reader to read the file
+                with. If not specified, ixdat will try to determine the reader from the
+                file suffix.
+            kwargs: key-word arguments are passed on to the reader's read() method.
+        """
+        if not reader:
+            # Check if there is a default reader based on the file's suffix
+            from .readers.reading_tools import get_default_reader_name
+
+            reader = get_default_reader_name(path_to_file)
+            if not reader:
+                raise ValueError(
+                    f"There is no default reader for files of the type {path_to_file}. "
+                    "Please specify a reader to read this file."
+                )
+
+        if isinstance(reader, str):
+            # TODO: see if there isn't a way to put the import at the top of the module.
+            #    see: https://github.com/ixdat/ixdat/pull/1#discussion_r546437471
+            from .readers import READER_CLASSES
+
+            reader = READER_CLASSES[reader]()
+        obj = reader.read(path_to_file, cls=cls, **kwargs)
+
+        if getattr(obj.__class__, "essential_series_names", None):
+            for series_name in obj.__class__.essential_series_names:
+                try:
+                    _ = obj[series_name]  # this also caches it.
+                except SeriesNotFoundError:
+                    raise SeriesNotFoundError(
+                        f"{reader} loaded without {obj.__class__.__name__} "
+                        f"essential series '{series_name}'"
+                    )
+        return obj
+
+    @classmethod
+    def read_url(cls, url, reader=None, **kwargs):
+        """Read a url (via a temporary file) using the specified reader"""
+        from .readers.reading_tools import url_to_file
+
+        path_to_temp_file = url_to_file(url)
+        measurement = cls.read(path_to_temp_file, reader=reader, **kwargs)
+        path_to_temp_file.unlink()
+        return measurement
+
+    @classmethod
+    def read_set(
+        cls,
+        path_to_file_start=True,
+        part=None,
+        suffix=None,
+        file_list=None,
+        reader=None,
+        **kwargs,
+    ):
+        """Read and append a set of files.
+
+        Args:
+            path_to_file_start (Path or str): The path to the files to read including
+                the shared start of the file name: `Path(path_to_file).parent` is
+                interpreted as the folder where the file are.
+                `Path(path_to_file).name` is interpreted as the shared start of the files
+                to be appended.
+                Alternatively, path_to_file_start can be a folder, in which case all
+                files in that folder (with the specified suffix) are included.
+            part (Path or str): A path where the folder is the folder containing data
+                and the name is a part of the name of each of the files to be read and
+                combined.
+            suffix (str): If a suffix is given, only files with the specified ending are
+                added to the file list
+            file_list (list of Path): As an alternative to path_to_file_start, the
+                exact files to append can be specified in a list
+            reader (str or Reader class): The (name of the) reader to read the files with
+            kwargs: Key-word arguments are passed via cls.read() to the reader's read()
+                method, AND to cls.from_component_measurements()
+        """
+        from .readers.reading_tools import get_file_list
+
+        file_list = file_list or get_file_list(path_to_file_start, part, suffix)
+        if not file_list:
+            raise ReadError(
+                "No files found! Please check that there are files satisfying:\n"
+                f"path_to_file_start={path_to_file_start}, part={part}, suffix={suffix}"
+            )
+        component_measurements = [
+            cls.read(f, reader=reader, **kwargs) for f in file_list
+        ]
+        measurement = None
+        for meas in component_measurements:
+            measurement = measurement + meas if measurement else meas
+        return measurement
+
+    @classmethod
+    def from_component_measurements(
+        cls, component_measurements, keep_originals=True, sorted=True, **kwargs
+    ):
+        """Return a measurement with the data contained in the component measurements
+
+        TODO: This function "builds" the resulting measurement, i.e. it appends series
+            of the same name rather than keeping all the original copies. This should be
+            made more explicit, and a `build()` method should take over some of the work.
+
+        Args:
+            component_measurements (list of Measurement)
+            keep_originals: Whether to keep a list of component_measurements referenced.
+                This may result in redundant numpy arrays in RAM.
+            sorted (bool): Whether to sort the series according to time
+            kwargs: key-word arguments are added to the dictionary for cls.from_dict()
+
+        Returns cls: the combined measurement.
+        """
+
+        # First prepare everything but the series_list in the object dictionary
+        obj_as_dict = component_measurements[0].as_dict()
+        obj_as_dict.update(kwargs)
+        del obj_as_dict["m_ids"], obj_as_dict["s_ids"]
+        if keep_originals:
+            obj_as_dict["component_measurements"] = component_measurements
+
+        # Now, prepare the built series. First, we loop through the component
+        # measurements and get all the data and metadata organized in a dictionary:
+        series_as_dicts = {}
+        tstamp = component_measurements[0].tstamp
+        for meas in component_measurements:
+            tstamp_i = meas.tstamp  # save this for later.
+            meas.tstamp = tstamp  # so that the time vectors share a t=0
+            for s_name in meas.series_names:
+                series = meas[s_name]
+                if s_name in series_as_dicts:
+                    series_as_dicts[s_name]["data"] = np.append(
+                        series_as_dicts[s_name]["data"], series.data
+                    )
+                else:
+                    series_as_dicts[s_name] = series.as_dict()
+                    series_as_dicts[s_name]["data"] = series.data
+                    if isinstance(series, ValueSeries):
+                        # This will serve to match it to a TimeSeries later:
+                        series_as_dicts[s_name]["t_name"] = series.tseries.name
+            meas.tstamp = tstamp_i  # so it's not changed in the outer scope
+
+        # Now we make DataSeries, starting with all the TimeSeries
+        tseries_dict = {}
+        sort_indeces = {}
+        for name, s_as_dict in series_as_dicts.items():
+            if "tstamp" in s_as_dict:
+                if sorted:
+                    sort_indeces[name] = np.argsort(s_as_dict["data"])
+                    s_as_dict["data"] = s_as_dict["data"][sort_indeces[name]]
+                tseries_dict[name] = TimeSeries.from_dict(s_as_dict)
+        # And then ValueSeries, and put both in with the TimeSeries
+        series_list = []
+        for name, s_as_dict in series_as_dicts.items():
+            if name in tseries_dict:
+                series_list.append(tseries_dict[name])
+            elif "t_name" in s_as_dict:
+                tseries = tseries_dict[s_as_dict["t_name"]]
+                if s_as_dict["data"].shape == tseries.shape:
+                    # Then we assume that the time and value data have lined up
+                    # successfully! :D
+                    if sorted:
+                        s_as_dict["data"] = s_as_dict["data"][sort_indeces[tseries.name]]
+                    vseries = ValueSeries(
+                        name=name,
+                        data=s_as_dict["data"],
+                        unit_name=s_as_dict["unit_name"],
+                        tseries=tseries,
+                    )
+                else:
+                    # this will be the case if vseries sharing the same tseries
+                    # are not present in the same subset of component_measurements.
+                    # In that case just append the vseries even though some tdata gets
+                    # duplicated.
+                    vseries = append_series(
+                        [
+                            s
+                            for m in component_measurements
+                            for s in m.series_list
+                            if s.name == name
+                        ],
+                        sorted=sorted,
+                    )
+                series_list.append(vseries)
+
+        # Finally, add the series to the dictionary representation and return the object
+        obj_as_dict["series_list"] = series_list
+        return cls.from_dict(obj_as_dict)
+
+    @property
+    def tstamp(self):
+        """Float: The unix epoch time used by the measurement as t=0"""
+        return self._tstamp
+
+    @tstamp.setter
+    def tstamp(self, tstamp):
+        # Resetting the tstamp needs to clear the cache, so series are returned wrt the
+        # new timestamp.
+        self.clear_cache()
+        self._tstamp = tstamp
+
+    @property
+    def yyMdd(self):
+        return tstamp_to_string(self.tstamp, string_format="native_date")
+
+    @property
+    def metadata_json_string(self):
+        """Measurement metadata as a JSON-formatted string"""
+        return json.dumps(self.metadata, indent=4)
+
+    @property
+    def sample_name(self):
+        """Name of the sample on which the measurement was conducted"""
+        if self.sample:
+            return self.sample.name
+
+    @property
+    def component_measurements(self):
+        """List of the component measurements of which this measurement is a combination
+
+        For a pure measurement (not a measurement set), this is itself in a list.
+        """
+        for i, m in enumerate(self._component_measurements):
+            if isinstance(m, PlaceHolderObject):
+                # This is where we find objects from a Backend including MemoryBackend:
+                self._component_measurements[i] = m.get_object()
+        return self._component_measurements
+
+    @property
+    def m_ids(self):
+        """List of the id's of a combined measurement's component measurements
+        FIXME: m.id can be (backend, id) if it's not on the active backend.
+            This is as of now necessary to find it if you're only given self.as_dict()
+            see https://github.com/ixdat/ixdat/pull/11#discussion_r746632897
+        """
+        if not self._component_measurements:
+            return None
+        return [m.short_identity for m in self.component_measurements]
+
+    @property
+    def calibration_list(self):
+        """List of calibrations (with placeholders filled)"""
+        for i, c in enumerate(self._calibration_list):
+            if isinstance(c, PlaceHolderObject):
+                # This is where we find objects from a Backend including MemoryBackend:
+                self._calibration_list[i] = c.get_object()
+        return self._calibration_list
+
+    @property
+    def calibrations(self):
+        """For overriding: List of calibrations with any needed manipulation done."""
+        return self.calibration_list
+
+    @property
+    def c_ids(self):
+        """List of the id's of the measurement's Calibrations
+        FIXME: c.id can be (backend, id) if it's not on the active backend.
+            This is as of now necessary to find it if you're only given self.as_dict()
+             see https://github.com/ixdat/ixdat/pull/11#discussion_r746632897
+        """
+        return [c.short_identity for c in self.calibration_list]
+
+    def add_calibration(self, calibration):
+        self._calibration_list = [calibration] + self._calibration_list
+        self.clear_cache()
+
+    def calibrate(self, *args, **kwargs):
+        """Add a calibration of the Measurement's default calibration type
+
+        The calibration class is determined by the measurement's `technique`.
+        *args and **kwargs are passed to the calibration class's `__init__`.
+
+        Raises:
+            TechniqueError if no calibration class for the measurement's technique
+        """
+
+        from .techniques import CALIBRATION_CLASSES
+
+        if self.technique in CALIBRATION_CLASSES:
+            calibration_class = CALIBRATION_CLASSES[self.technique]
+        else:
+            raise TechniqueError(
+                f"{self!r} is of technique '{self.technique}, for which there is not an "
+                "available default calibration. Instead, import one of the following "
+                "classes to initiate a calibration, and then use `add_calibration`. "
+                f"\nOptions: \n{CALIBRATION_CLASSES}"
+            )
+
+        self.add_calibration(calibration_class(*args, **kwargs))
+        self.clear_cache()
+
+    @property
+    @deprecate(
+        last_supported_release="0.1",
+        update_message=(
+            "At present, ixdat measurements have a `calibration_list` but no compound "
+            "`calibration`, and this property just returns the first from the list."
+        ),
+        hard_deprecation_release=None,
+    )
+    def calibration(self):
+        return self.calibration_list[0]
+
+    @calibration.setter
+    @deprecate(
+        last_supported_release="0.1",
+        update_message=(
+            "Setting `calibration` is deprecated. For now it clears `calibration_list` "
+            "and replaces it with a single calibration. "
+            "Use `add_calibration()` instead."
+        ),
+        hard_deprecation_release="0.3",
+    )
+    def calibration(self, calibration):
+        self._calibration_list = [calibration]
+
+    @property
+    def series_list(self):
+        """List of the DataSeries containing the measurement's data"""
+        for i, s in enumerate(self._series_list):
+            if isinstance(s, PlaceHolderObject):
+                # This is where we find objects from a Backend including MemoryBackend:
+                self._series_list[i] = s.get_object()
+        return self._series_list
+
+    @property
+    def s_ids(self):
+        """List of the id's of the measurement's DataSeries
+        FIXME: m.id can be (backend, id) if it's not on the active backend.
+            This is as of now necessary to find it if you're only given self.as_dict()
+            see https://github.com/ixdat/ixdat/pull/11#discussion_r746632897
+        """
+        return [series.short_identity for series in self._series_list]
+
+    @property
+    def series_names(self):
+        """Set of the names of the series in the measurement"""
+        return set([series.name for series in self.series_list])
+
+    @property
+    def value_names(self):
+        """Set of the names of the VSeries in the measurement's DataSeries"""
+        return set([vseries.name for vseries in self.value_series])
+
+    @property
+    def time_names(self):
+        """Set of the names of the VSeries in the measurement's DataSeries"""
+        return set([tseries.name for tseries in self.time_series])
+
+    @property
+    def value_series(self):
+        """List of the VSeries in the measurement's DataSeries"""
+        return [series for series in self.series_list if isinstance(series, ValueSeries)]
+
+    @property
+    def time_series(self):
+        """List of the TSeries in the measurement's DataSeries. NOT timeshifted!"""
+        return [series for series in self.series_list if isinstance(series, TimeSeries)]
+
+    @property
+    def aliases(self):
+        """Dictionary of {key: series_names} pointing to where desired raw data is
+
+        TODO: get the possible aliases based on calibrations, etc, in here.
+        """
+        return self._aliases.copy()
+
+    @property
+    def reverse_aliases(self):
+        """{series_name: standard_names} indicating how raw data can be accessed"""
+        rev_aliases = {}
+        for name, other_names in self.aliases.items():
+            for other_name in other_names:
+                if other_name in rev_aliases:
+                    rev_aliases[other_name].append(name)
+                else:
+                    rev_aliases[other_name] = [name]
+        return rev_aliases
+
+    def get_series_names(self, key):
+        """Return list: series names for key found by (recursive) lookup in aliases"""
+        keys = [key] if key in self.series_names else []
+        for k in self.aliases.get(key, []):
+            keys += self.get_series_names(k)
+        return keys
+
+    def __getitem__(self, key):
+        """Return the built measurement DataSeries with its name specified by key
+
+        This method does the following:
+        1. check if `key` is in in the cache. If so return the cached data series
+        2. find or build the desired data series by the first possible of:
+            A. Check if `key` corresponds to a method in `series_constructors`. If
+                so, build the data series with that method.
+            B. Check if the `calibration`'s `calibrate_series` returns a data series
+                for `key` given the data in this measurement. (Note that the
+                `calibration` will typically start with raw data looked C, below.)
+            C. Generate a list of data series and append them:
+                i. Check if `key` is in `aliases`. If so, append all the data series
+                    returned for each key in `aliases[key]`.
+                ii. Otherwise, check if there are data series in `series_list` that
+                    have `key` as their `name`. If so, append them.
+            D. Finally, check if the user is using a suffix.
+                i. If `key` ends with "-y" or "-v", look it up with the suffix removed.
+                ii. If `key` ends with "-x" or "-t", look up `key` with the suffix
+                    removed and use instead the corresponding `tseries`.
+        3. Cache and return the data series found or built in (2).
+
+        Step (2) above, the searching step, is outsourced to the method
+        `get_series(key)`.
+        Notice that some calls of `__getitem__` can be recursive. For example, we
+        suppose that a new `ECMeasurement` is read from a source that calls raw
+        potential `Ewe/V`, and that this measurement is then calibrated:
+
+        >>> ec_meas = Measurement.read(...)
+        >>> ec_meas.aliases
+        {..., 'raw_potential': ['Ewe/V'], ...}
+        >>> ec_meas["raw_potential"]  # first lookup, explained below
+        ValueSeries("Ewe/V", ...)
+        >>> ec_meas.calibrate_RE(RE_vs_RHE=0.7)
+        >>> ec_meas["potential"]      # second lookup, explained below
+        ValueSeries("U_{RHE} / [V]", ...)
+
+        - The first lookup, with `key="raw_potential"`, (1) checks for
+        "raw_potential" in the cache, doesn't find it; then (2A) checks in
+        `series_constructors`, doesn't find it; (2B) asks the calibration for
+        "raw_potential" and doesn't get anything back; and finally (2Ci) checks
+        `aliases` for raw potential where it finds that "raw_potential" is called
+        "Ewe/V". Then it looks up again, this time with `key="Ewe/V"`, which it doesn't
+        find in (1) the cache, (2A) `series_consturctors`, (2B) the calibration, or
+        (2Ci) `aliases`, but does find in (2Cii) `series_list`. There is only one
+        data series named "Ewe/V" so no appending is necessary, but it does ensure that
+        the series has the measurement's `tstamp` before cache'ing and returning it.
+        Now we're back in the original lookup, from which __getitem__ (3) caches
+        the data series (which still has the name "Ewe/V") as "raw_potential" and
+        returns it.
+        - The second lookup, with `key="potential"`, (1) checks for "potential" in
+        the cache, doesn't find it; then (2A) checks in `series_constructors`,
+        doesn't find it; and then (2B) asks the calibration for "potential". The
+        calibration knows that when asked for "potential" it should look for
+        "raw_potential" and add `RE_vs_RHE`. So it does a lookup with
+        `key="raw_potential"` and (1) finds it in the cache. The calibration does
+        the math and returns a new data series for the calibrated potential, bringing
+        us back to the original lookup. The data series returned by the
+        calibration is then (3) cached and returned to the user.
+
+        Note that, if the user had not looked up "raw_potential" before looking up
+        "potential", "raw_potential" would not have been in the cache and the first
+        lookup above would have been nested in the second.
+
+        Args:
+            key (str): The name of a DataSeries (see above)
+        Raises:
+            SeriesNotFoundError if none of the above lookups find the key.
+        Side-effects:
+            if key is not already in the cache, it gets added
+        Returns:
+            The (calibrated) (appended) dataseries for key with the right t=0.
+        """
+        # step 1
+        if key in self._cached_series:
+            return self._cached_series[key]
+        # step 2
+        series = self.get_series(key)
+        # Finally, wherever we found the series, cache it and return it.
+        # step 3.
+        self._cache_series(key, series)
+        return series
+
+    def _cache_series(self, key, series):
+        """Cache `series` such that it can be looked up with its name or with `key`."""
+        self._cached_series[key] = series  # now it can be looked up with by `key`
+        # If the name of the series is not `key`, we can get in a situation where
+        # looking up the series name raises a SeriesNotFoundError. To avoid this
+        # problematic situation, we check if it can be looked up, and if not,
+        # add it a second time to the cached_series, now under `series.name`
+        try:
+            _ = self[series.name]
+        except SeriesNotFoundError:
+            self._cached_series[series.name] = series
+
+    def get_series(self, key):
+        """Find or build the data series corresponding to key without direct cache'ing
+
+        See more detailed documentation under `__getitem__`, for which this is a
+        helper method. This method (A) looks for a method for `key` in the measurement's
+        `series_constructors`; (B) requests its `calibration` for `key`; and if those
+        fail appends the data series that either (Ci) are returned by looking up the
+        key's `aliases` or (Cii) have `key` as their name; and finally (D) check if the
+        user was using a key with a suffix.
+
+        Args:
+            key (str): The key to look up
+
+        Returns DataSeries: the data series corresponding to key
+        Raises SeriesNotFoundError if no series found for key
+        """
+        # A
+        if key in self.series_constructors:
+            return getattr(self, self.series_constructors[key])()
+        # B
+        for calibration in self.calibrations:
+            series = calibration.calibrate_series(key, measurement=self)
+            # ^ the calibration will call __getitem__ with the name of the
+            #   corresponding raw data and return a new series with calibrated data
+            #   if possible. Otherwise it will return None.
+            if series:
+                return series
+        # C
+        series_to_append = []
+        if key in self.series_names:  # ii
+            # Then we'll append any series matching the desired name
+            series_to_append += [s for s in self.series_list if s.name == key]
+        if key in self.aliases:  # i
+            # Then we'll look up the aliases instead and append them
+            for k in self.aliases[key]:
+                if k == key:  # this would result in infinite recursion.
+                    print(  # TODO: Real warnings.
+                        "WARNING!!!\n"
+                        f"\t{self!r} has {key} in its aliases for {key}:\n"
+                        f"\tself.aliases['{key}'] = {self.aliases[key]}"
+                    )
+                    continue
+                try:
+                    series_to_append.append(self[k])
+                except SeriesNotFoundError:
+                    continue
+        # If the key is something in the data, by now we have series to append.
+        if series_to_append:
+            # the following if's are to do as little extra manipulation as possible:
+            if len(series_to_append) == 1:  # no appending needed
+                if series_to_append[0].tstamp == self.tstamp:  # no time-shifting needed
+                    return series_to_append[0]
+                return time_shifted(series_to_append[0], tstamp=self.tstamp)
+            return append_series(series_to_append, name=key, tstamp=self.tstamp)
+        # D
+        if key.endswith("-t") or key.endswith("-x"):
+            return self[key[:-2]].tseries
+        if key.endswith("-v") or key.endswith("-y"):
+            return self[key[:-2]]
+
+        raise SeriesNotFoundError(f"{self!r} does not contain '{key}'")
+
+    def replace_series(self, series_name, new_series=None):
+        """Remove an existing series, add a series to the measurement, or both.
+
+        FIXME: This will not appear to change the series for the user if the
+            measurement's calibration returns something for ´series_name´, since
+            __getitem__ asks the calibration before looking in series_list.
+
+        Args:
+            series_name (str): The name of a series. If the measurement has (raw) data
+                series with this name, cached series with this name, and/or aliases for
+                this name, they will be removed.
+            new_series (DataSeries): Optional new series to append to the measurement's
+                series_list. To sanity check, it must have ´series_name´ as its ´name´.
+        """
+        if new_series and not series_name == new_series.name:
+            raise TypeError(
+                f"Cannot replace {series_name} in {self!r} with {new_series}. "
+                f"Names must agree."
+            )
+        if series_name in self._cached_series:
+            del self._cached_series[series_name]
+        if series_name in self._aliases:
+            del self._aliases[series_name]
+        new_series_list = [s for s in self.series_list if not s.name == series_name]
+        if new_series:
+            new_series_list.append(new_series)
+        self._series_list = new_series_list
+
+    def clear_cache(self):
+        """Clear the cache so derived series are constructed again with updated info"""
+        self._cached_series = {}
+
+    def correct_data(self, value_name, new_data):
+        """Replace the old data for ´value_name´ (str) with ´new_data` (np array)"""
+        old_vseries = self[value_name]
+        new_vseries = ValueSeries(
+            name=value_name,
+            unit_name=old_vseries.unit_name,
+            data=new_data,
+            tseries=old_vseries.tseries,
+        )
+        self.replace_series(value_name, new_vseries)
+
+    def grab(self, item, tspan=None, include_endpoints=False, tspan_bg=None):
+        """Return a value vector with the corresponding time vector
+
+        Grab is the *canonical* way to retrieve numerical time-dependent data from a
+        measurement in ixdat. The first argument is always the name of the value to get
+        time-resolved data for (the name of a ValueSeries). The second, optional,
+        argument is a timespan to select the data for.
+        Two vectors are returned: first time (t), then value (v). They are of the same
+        length so that `v` can be plotted against `t`, integrated over `t`, interpolated
+        via `t`, etc. `t` and `v` are returned in the units of their DataSeries.
+        TODO: option to specifiy desired units
+
+        Typical usage::
+            t, v = measurement.grab("potential", tspan=[0, 100])
+
+        Args:
+            item (str): The name of the DataSeries to grab data for
+                TODO: Should this be called "name" or "key" instead? And/or, should
+                   the argument to __getitem__ be called "item" instead of "key"?
+            tspan (iter of float): Defines the timespan with its first and last values.
+                Optional. By default the entire time of the measurement is included.
+            include_endpoints (bool): Whether to add a points at t = tspan[0] and
+                t = tspan[-1] to the data returned. This makes trapezoidal integration
+                less dependent on the time resolution. Default is False.
+            tspan_bg (iterable): Optional. A timespan defining when `item` is at its
+                baseline level. The average value of `item` in this interval will be
+                subtracted from the values returned.
+        """
+        vseries = self[item]
+        tseries = vseries.tseries
+        v = vseries.data
+        t = tseries.data + tseries.tstamp - self.tstamp
+        if tspan is not None:  # np arrays don't boolean well :(
+            if include_endpoints:
+                if t[0] < tspan[0]:  # then add a point to include tspan[0]
+                    v_0 = np.interp(tspan[0], t, v)
+                    t = np.append(tspan[0], t)
+                    v = np.append(v_0, v)
+                if tspan[-1] < t[-1]:  # then add a point to include tspan[-1]
+                    v_end = np.interp(tspan[-1], t, v)
+                    t = np.append(t, tspan[-1])
+                    v = np.append(v, v_end)
+            mask = np.logical_and(tspan[0] <= t, t <= tspan[-1])
+            t, v = t[mask], v[mask]
+        if tspan_bg:
+            t_bg, v_bg = self.grab(item, tspan=tspan_bg)
+            v = v - np.mean(v_bg)
+        return t, v
+
+    def grab_for_t(self, item, t, tspan_bg=None):
+        """Return a numpy array with the value of item interpolated to time t
+
+        Args:
+            item (str): The name of the value to grab
+            t (np array): The time vector to grab the value for
+            tspan_bg (iterable): Optional. A timespan defining when `item` is at its
+                baseline level. The average value of `item` in this interval will be
+                subtracted from what is returned.
+        """
+        vseries = self[item]
+        tseries = vseries.tseries
+        v_0 = vseries.data
+        t_0 = tseries.data + tseries.tstamp - self.tstamp
+        v = np.interp(t, t_0, v_0)
+        if tspan_bg:
+            t_bg, v_bg = self.grab(item, tspan=tspan_bg)
+            v = v - np.mean(v_bg)
+        return v
+
+    def integrate(self, item, tspan=None, ax=None):
+        """Return the time integral of item in the specified timespan"""
+        t, v = self.grab(item, tspan, include_endpoints=True)
+        if ax:
+            if ax == "new":
+                ax = self.plotter.new_ax(ylabel=item)
+                # FIXME: xlabel=self[item].tseries.name gives a problem :(
+            ax.plot(t, v, color="k", label=item)
+            ax.fill_between(t, v, np.zeros(t.shape), where=v > 0, color="g", alpha=0.3)
+            ax.fill_between(
+                t, v, np.zeros(t.shape), where=v < 0, color="g", alpha=0.1, hatch="//"
+            )
+
+        return np.trapz(v, t)
+
+    @property
+    def t(self):
+        return self[self.control_series_name].t
+
+    @property
+    def t_name(self):
+        return self[self.control_series_name].tseries.name
+
+    def _build_file_number_series(self):
+        """Build a `file_number` series based on component measurements times."""
+        series_to_append = []
+        for i, m in enumerate(self.component_measurements or [self]):
+            if (
+                self.control_technique_name
+                and not m.technique == self.control_technique_name
+            ):
+                continue
+            if not self.control_series_name:
+                tseries = m.time_series[0]
+            else:
+                try:
+                    tseries = m[self.control_series_name].tseries
+                except SeriesNotFoundError:
+                    continue
+            series_to_append.append(
+                ConstantValue(name="file_number", unit_name="", data=i, tseries=tseries)
+            )
+        return append_series(series_to_append, name="file_number", tstamp=self.tstamp)
+
+    def _build_selector_series(
+        self, selector_string=None, columns=None, extra_columns=None
+    ):
+        """Build a `selector` series which demarcates the data.
+
+        The `selector` is a series which can be used to conveniently and powerfully
+        grab sections of the data. It is built up from less powerful demarcation series
+        in the raw data (like `cycle_number`, `step_number`, `loop_number`, etc) and
+        `file_number` by counting the cumulative changes in those series.
+        See slide 3 of:
+        https://www.dropbox.com/s/sjxzr52fw8yml5k/21E18_DWS3_cont.pptx?dl=0
+
+        Args:
+            selector_string (str): The name to use for the selector series
+            columns (list): The list of demarcation series. The demarcation series have
+                to have equal-length tseries, which should be the one pointed to by the
+                meausrement's `control_series_name`.
+            extra_columns (list): Extra demarcation series to include if needed.
+        """
+        # the name of the selector series:
+        selector_string = selector_string or self.selector_name
+        # a vector that will be True at the points where a series changes:
+        changes = np.tile(False, self.t.shape)
+        # the names of the series which help demarcate the data
+        columns = columns or self.selection_series_names
+        if extra_columns:
+            columns += extra_columns
+        for col in columns:
+            try:
+                vseries = self[col]
+            except SeriesNotFoundError:
+                continue
+            values = vseries.data
+            if len(values) == 0:
+                print("WARNING: " + col + " is empty")
+                continue
+            elif not len(values) == len(changes):
+                print("WARNING: " + col + " has an unexpected length")
+                continue
+            # a vector which is shifted one.
+            last_value = np.append(values[0], values[:-1])
+            # comparing value and last_value shows where in the vector changes occur:
+            changes = np.logical_or(changes, last_value != values)
+        # taking the cumsum makes a vector that increases 1 each time one of the
+        #   original demarcation vector changes
+        selector_data = np.cumsum(changes)
+        selector_series = ValueSeries(
+            name=selector_string,
+            unit_name="",
+            data=selector_data,
+            tseries=self[self.control_series_name].tseries,
+        )
+        return selector_series
+
+    def rebuild_selector(self, selector_string=None, columns=None, extra_columns=None):
+        """Build a new selector series for the measurement and cache it.
+
+        This can be useful if a user wants to change how their measurement counts
+        sections (for example, only count sections when technique or file number changes)
+
+        Args:
+            selector_string (str): The name to use for the selector series
+            columns (list): The list of demarcation series. The demarcation series have
+                to have the same tseries, which should be the one pointed to by the
+                meausrement's `control_series_name`.
+            extra_columns (list): Extra demarcation series to include if needed.
+        """
+        selector_string = selector_string or self.selector_name
+        selector_series = self._build_selector_series(
+            selector_string=selector_string,
+            columns=columns,
+            extra_columns=extra_columns,
+        )
+        self._cache_series(selector_string, selector_series)
+        return selector_series
+
+    @property
+    def selector(self):
+        return self[self.selector_name]
+
+    @property
+    def data_cols(self):
+        """Return a set of the names of all of the measurement's VSeries and TSeries"""
+        return set([s.name for s in (self.value_series + self.time_series)])
+
+    def get_original_m_ids_of_series(self, series):
+        """Return a list of id's of component measurements to which `series` belongs."""
+        m_id_list = []
+        for m in self.component_measurements:
+            if series.short_identity in m.s_ids:
+                # FIXME: the whole id vs short_identity issue
+                #   see https://github.com/ixdat/ixdat/pull/11#discussion_r746632897
+                m_id_list.append(m.id)
+        return m_id_list
+
+    @property
+    def tspan(self):
+        """The minimum timespan (with respect to self.tstamp) containing all the data"""
+        t_start = None
+        t_finish = None
+        if not self.time_names:  # No TimeSeries in the measurement means no tspan.
+            return None
+        for t_name in self.time_names:
+            t = self[t_name].data
+            if len(t) == 0:
+                return None
+            t_start = t[0] if t_start is None else min(t_start, t[0])
+            t_finish = t[-1] if t_finish is None else max(t_finish, t[-1])
+        return [t_start, t_finish]
+
+    def cut(self, tspan, t_zero=None):
+        """Return a new measurement with the data in the given time interval
+
+        Args:
+            tspan (iter of float): The time interval to use, relative to self.tstamp
+                tspan[0] is the start time of the interval, and tspan[-1] is the end
+                time of the interval. Using tspan[-1] means you can directly use a
+                long time vector that you have at hand to describe the time interval
+                you're looking for.
+            t_zero (float or str): The time in the measurement to set to t=0. If a
+                float, it is interpreted as wrt the original tstamp. String options
+                include "start", which puts t=0 at the start of the cut interval.
+        """
+        # Start with self's dictionary representation, but
+        # we don't want original series (s_ids) or component_measurements (m_ids):
+        obj_as_dict = self.as_dict(exclude=["s_ids", "m_ids"])
+
+        # first, cut the series list:
+        new_series_list = []
+        time_cutting_stuff = {}  # {tseries_id: (mask, new_tseries)}
+        for series in self.series_list:
+            try:
+                tseries = series.tseries
+                if tseries is None:
+                    raise AttributeError
+            except AttributeError:  # series independent of time are uneffected by cut
+                new_series_list.append(series)
+            else:
+                t_identity = tseries.full_identity
+
+                if t_identity in time_cutting_stuff:
+                    mask, new_tseries = time_cutting_stuff[t_identity]
+                else:
+                    t = tseries.t + tseries.tstamp - self.tstamp
+                    mask = np.logical_and(tspan[0] <= t, t <= tspan[-1])
+                    new_tseries = TimeSeries(
+                        name=tseries.name,
+                        unit_name=tseries.unit_name,
+                        tstamp=tseries.tstamp,
+                        data=tseries.data[mask],
+                    )
+                    time_cutting_stuff[t_identity] = (mask, new_tseries)
+                if True not in mask:
+                    continue
+                if False not in mask:
+                    new_series_list.append(series)
+                elif series.full_identity == t_identity:
+                    new_series_list.append(new_tseries)
+                else:
+                    new_series = series.__class__(
+                        name=series.name,
+                        unit_name=series.unit_name,
+                        data=series.data[mask],
+                        tseries=new_tseries,
+                    )
+                    new_series_list.append(new_series)
+        obj_as_dict["series_list"] = new_series_list
+
+        # then cut the component measurements.
+        new_component_measurements = []
+        for m in self._component_measurements:
+            # FIXME: This is perhaps overkill, to make new cut component measurements,
+            #    as it duplicates data (a big no)... especially bad because
+            #    new_measurement.save() saves them.
+            #    The step is here in order for file_number to get built correctly.
+            if not m.tspan:
+                # if it has no TimeSeries it must be a "constant". Best to include:
+                new_component_measurements.append(m)
+                continue
+            # Otherwise we have to cut it according to the present tspan.
+            dt = m.tstamp - self.tstamp
+            try:
+                tspan_m = [tspan[0] - dt, tspan[1] - dt]
+            except IndexError:  # Apparently this can happen for empty files. See:
+                continue  # https://github.com/ixdat/ixdat/issues/93
+            if m.tspan[-1] < tspan_m[0] or tspan_m[-1] < m.tspan[0]:
+                continue
+            new_component_measurements.append(m.cut(tspan_m))
+        obj_as_dict["component_measurements"] = new_component_measurements
+
+        new_measurement = self.__class__.from_dict(obj_as_dict)
+        if t_zero:
+            if t_zero == "start":
+                new_measurement.tstamp += tspan[0]
+            else:
+                new_measurement.tstamp += t_zero
+        return new_measurement
+
+    def multicut(self, tspans):
+        """Return a selection of the measurement including each of the given tspans"""
+        # go through the tspans, cuting the measurement and appending the results
+        new_measurement = None
+        for tspan in tspans:
+            if new_measurement:
+                new_measurement = new_measurement + self.cut(tspan)
+            else:
+                new_measurement = self.cut(tspan)
+        return new_measurement
+
+    def select_value(self, *args, **kwargs):
+        """Return a selection of the measurement where a criterion is matched.
+
+        Specifically, this method returns a new Measurement where the time(s) returned
+        are those where the values match the provided criteria, i.e. the part of the
+        measurement where `self[series_name] == value`
+
+        Can only take one arg or kwarg!
+        The `series_name` is `self.selector_name` if given an argument without keyword.
+        If given a keyword argument, the kyword is the name of the series to select on.
+        Either way the argument is the `value` to be selected for.
+
+        The method finds all time intervals for which `self[series_name] == value`
+        It then cuts the measurement according to each time interval and adds these
+        segments together.
+        TODO: This can maybe be done better, i.e. without chopping series.
+        TODO: Some way of less than and greater than kwargs.
+            Ideally you should be able to say e.g., `select(cycle=1, 0.5<potential<1)`
+            But this is hard,
+            see: https://github.com/ixdat/ixdat/pull/11#discussion_r677272239
+        """
+        if len(args) + len(kwargs) != 1:
+            raise BuildError("Need exactly 1 arg. Use `select_values` for more.")
+        if args:
+            if not self.selector_name:
+                raise BuildError(
+                    f"{self!r} does not have a default selection string "
+                    f"(Measurement.sel_str), and so selection only works with kwargs."
+                )
+            kwargs[self.selector_name] = args[0]
+
+        ((series_name, value),) = kwargs.items()
+
+        # The time and values of the series to be selected on:
+        t, v = self.grab(series_name)
+        # This mask is true everywhere on `t` that the condition is met:
+        mask = v == value  # linter doesn't realize this is a np array
+
+        # Now we have to convert that to timespans on which `t` is met. This means
+        #  finding the start and finish times of the intervals on which mask is True.
+        #  this is done with a helper function:
+        tspans = get_tspans_from_mask(t, mask)
+
+        # now we go through the tspans, cuting the measurement and appending the results:
+        return self.multicut(tspans)
+
+    def select_values(self, *args, selector_name=None, **kwargs):
+        """Return a selection of the measurement based on one or several criteria
+
+        Specifically, this method returns a new Measurement where the time(s) returned
+        are those where the values match the provided criteria, i.e. the part of the
+        measurement where `self[series_name] == value`
+
+        Any series can be selected for using the series name as a key-word. Arguments
+        can be single acceptable values or lists of acceptable values.
+        You can select for one or more series without valid python variable names by
+        providing the kwargs using ** notation (see last example below).
+
+        Arguments without key-word are considered valid values of the default
+        selector, which is normally `self.selector_name` but can also be specified
+        here using the key-word argument `selector_name`. Multiple criteria are
+        applied sequentially, i.e. you get the intersection of satisfying parts.
+
+        Examples of valid calls given a measurement `meas`:
+        ```
+        # to select where the default selector is 3, use:
+        meas.select_values(3)
+        # to select for where the default selector is 4 or 5:
+        meas.select_values(4, 5)
+        # to select for where "cycle" (i.e. the value of meas["cycle"].data) is 4:
+        meas.select_values(cycle=4)
+        # to select for where "loop_number" is 1 AND "cycle" is 3, 4, or 5:
+        meas.select_values(loop_number=1, cycle=[3, 4, 5])
+        # to select for where "cycle number" (notice the space) is 2 or 3:
+        meas.select_values([2, 3], selector_name="cycle number")
+        # which is equivalent to:
+        meas.select_values(**{"cycle number": [2, 3]})
+
+        Args:
+            args (tuple): Argument(s) given without keyword are understood as acceptable
+                value(s) for the selector (that named by selector_name or
+                self.selector_name).
+            selector_name: The name of the selector to which the args specify
+            kwargs (dict): Each key-word arguments is understood as the name
+                of a series and its acceptable value(s).
+        """
+        if args:
+            # Then we must interpret the arguments as allowed values of a selector,
+            #  either specified in the kwargs or the Measurement's default selector:
+            selector_name = selector_name or self.selector_name
+            if not selector_name:
+                raise BuildError(
+                    f"{self:r} does not have a default selector_name "
+                    f"(Measurement.selector_name), and so selection only works "
+                    f"with a selector_name specified "
+                    f"(see `help(Measurement.select_values)`)"
+                )
+            # Get the args into a simple list:
+            flat_args = []
+            for arg in args:
+                if hasattr(arg, "__iter__"):
+                    flat_args += list(arg)
+                else:
+                    flat_args.append(arg)
+            if selector_name in kwargs:
+                raise ValueError(
+                    "Don't call select_values with both arguments and "
+                    "'{self.selector_name}' as a key-word argument"
+                )
+            kwargs[self.selector_name] = flat_args
+
+        t = self.t
+        mask = np.tile(np.array([True]), t.shape)
+        for series_name, allowed_values in kwargs.items():
+            if not hasattr(allowed_values, "__iter__"):
+                allowed_values = [allowed_values]
+            v = self.grab_for_t(series_name, t)
+            submask = np.tile(np.array([False]), t.shape)
+            for allowed_value in allowed_values:
+                submask = np.logical_or(submask, v == allowed_value)
+            mask = np.logical_and(mask, submask)
+
+        tspans = get_tspans_from_mask(t, mask)
+
+        return self.multicut(tspans)
+
+    def select(self, *args, tspan=None, **kwargs):
+        """`cut` (with tspan) and `select_values` (with *args and/or **kwargs).
+
+        These all work for measurements that have a default selector and/or the
+        indicated columns:
+        - `meas.select(1, 2)`
+        - `meas.select(tspan=[200, 300])`
+        - `meas.select(range(10))`
+        - `meas.select(cycle=4)`
+        - `meas.select(**{"cycle number": [20, 21]})
+        - `meas.select(loop_number=1, tspan=[1000, 2000])
+        - `meas.select(1, range(5, 20), file_number=1, tspan=[1000, 2000])`
+        """
+        new_measurement = self
+        if tspan:
+            new_measurement = new_measurement.cut(tspan=tspan)
+        if args or kwargs:
+            new_measurement = new_measurement.select_values(*args, **kwargs)
+        return new_measurement
+
+    def copy(self):
+        """Make a copy of the Measurement via its dictionary representation"""
+        return self.__class__.from_dict(self.as_dict())
+
+    def __add__(self, other):
+        """Addition of measurements appends the series and component measurements lists.
+
+        Adding results in a new Measurement. If the combination of the two measurements'
+        techniques is a recognized hyphenated technique, it returns an object of that
+        technique's measurement class. Otherwise it returns an object of Measurement.
+        metadata, sample, and logentry come from the first measurement.
+
+        An important point about addition is that it is almost but not quite associative
+        and commutative i.e.
+        A + (B + C) == (A + B) + C == C + B + A   is not quite true
+        Each one results in the same series and component measurements. They will even
+        appear in the same order in A + (B + C) and (A + B) + C. However, the technique
+        might be different, as a new technique might be determined each time.
+
+        Note also that there is no difference between hyphenating (simultaneous EC and
+        MS datasets, for example) and appending (sequential EC datasets). Either way,
+        all the raw series (or their placeholders) are just stored in the lists.
+        """
+        from .spectra import SpectrumSeries, add_spectrum_series_to_measurement
+
+        if isinstance(other, SpectrumSeries):
+            return add_spectrum_series_to_measurement(self, other)
+
+        new_name = self.name + " AND " + other.name
+        new_technique = get_combined_technique(self.technique, other.technique)
+
+        # TODO: see if there isn't a way to put the import at the top of the module.
+        #    see: https://github.com/ixdat/ixdat/pull/1#discussion_r546437410
+        from .techniques import TECHNIQUE_CLASSES
+
+        if new_technique in TECHNIQUE_CLASSES:
+            cls = TECHNIQUE_CLASSES[new_technique]
+        elif self.__class__ is other.__class__:
+            cls = self.__class__
+        else:
+            cls = Measurement
+
+        new_series_list = list(set(self.series_list + other.series_list))
+        new_component_measurements = list(
+            set(
+                (self.component_measurements or [self])
+                + (other.component_measurements or [other])
+            )
+        )
+        new_calibration_list = list(
+            set(self._calibration_list + other._calibration_list)
+        )
+        new_aliases = self.aliases.copy()
+        for key, names in other.aliases.items():
+            if key in new_aliases:
+                new_aliases[key] = list(set(new_aliases[key] + other.aliases[key]))
+            else:
+                new_aliases[key] = other.aliases[key]
+        obj_as_dict = self.as_dict()
+        other_as_dict = other.as_dict()
+        for k, v in other_as_dict.items():
+            # Looking forward to the "|" operator!
+            if k not in obj_as_dict:
+                obj_as_dict[k] = v
+        obj_as_dict.update(
+            name=new_name,
+            technique=new_technique,
+            series_list=new_series_list,
+            component_measurements=new_component_measurements,
+            calibration_list=new_calibration_list,
+            aliases=new_aliases,
+        )
+        # don't want the original calibrations, component measurements, or series:
+        del obj_as_dict["c_ids"]
+        del obj_as_dict["m_ids"]
+        del obj_as_dict["s_ids"]
+        return cls.from_dict(obj_as_dict)
+
+    def join(self, other, join_on=None):
+        """Join two measurements based on a shared data series
+
+        This involves projecting all timeseries from other's data series so that the
+        variable named by `join_on` is shared between all data series.
+        This is analogous to an explicit inner join.
+
+        Args:
+            other (Measurement): a second measurement to join to self
+            join_on (str or tuple): Either a string, if the value to join on is called
+                the same thing in both measurements, or a tuple of two strings where
+                the first is the name of the variable in self and the second in other.
+                The variable described by join_on must be monotonically increasing in
+                both measurements.
+        """
+        raise NotImplementedError
+
+
+class Calibration(Saveable):
+    """Base class for calibrations."""
+
+    table_name = "calibration"
+    column_attrs = {
+        "name",
+        "technique",
+        "tstamp",
+    }
+
+    def __init__(self, *, name=None, technique=None, tstamp=None, measurement=None):
+        """Initiate a Calibration
+
+        Args:
+            name (str): The name of the calibration
+            technique (str): The technique of the calibration
+            tstamp (float): The time at which the calibration took place or is valid
+            measurement (Measurement): Optional. A measurement to calibrate by default.
+        """
+        super().__init__()
+        # NOTE: The :r syntax in f-strings doesn't work on None
+        self.name = name or f"{self.__class__.__name__}({repr(measurement)})"
+        self.technique = technique
+        self.tstamp = tstamp or (measurement.tstamp if measurement else None)
+        self.measurement = measurement
+
+    @classmethod
+    def from_dict(cls, obj_as_dict):
+        """Return an object of the Calibration class of the right technique
+
+        Args:
+              obj_as_dict (dict): The full serializaiton (rows from table and aux
+                tables) of the measurement. obj_as_dict["technique"] specifies the
+                technique class to use, from TECHNIQUE_CLASSES
+        """
+        # TODO: see if there isn't a way to put the import at the top of the module.
+        #    see: https://github.com/ixdat/ixdat/pull/1#discussion_r546437410
+        from .techniques import CALIBRATION_CLASSES
+
+        if obj_as_dict["technique"] in CALIBRATION_CLASSES:
+            calibration_class = CALIBRATION_CLASSES[obj_as_dict["technique"]]
+        else:
+            calibration_class = cls
+        try:
+            calibration = calibration_class(**obj_as_dict)
+        except Exception:
+            raise
+        return calibration
+
+    def export(self, path_to_file=None):
+        """Export an ECMSCalibration as a json-formatted text file"""
+        path_to_file = path_to_file or (self.name + ".ix")
+        self_as_dict = self.as_dict()
+        with open(path_to_file, "w") as f:
+            json.dump(self_as_dict, f, indent=4)
+
+    @classmethod
+    def read(cls, path_to_file):
+        """Read a Calibration from a json-formatted text file"""
+        with open(path_to_file) as f:
+            obj_as_dict = json.load(f)
+        return cls.from_dict(obj_as_dict)
+
+    def calibrate_series(self, key, measurement=None):
+        """This should be overwritten in real calibration classes.
+
+        FIXME: Add more documentation about how to write this in inheriting classes.
+        """
+        raise NotImplementedError
+
+
+def get_combined_technique(technique_1, technique_2):
+    """Return the name of the technique resulting from adding two techniques"""
+    # TODO: see if there isn't a way to put the import at the top of the module.
+    #    see: https://github.com/ixdat/ixdat/pull/1#discussion_r546437410
+    if technique_1 == technique_2:
+        return technique_1
+
+    # if we're a component technique of a hyphenated technique to that hyphenated
+    # technique, the result is still the hyphenated technique. e.g. EC-MS + MS = EC-MS
+    if "-" in technique_1 and technique_2 in technique_1.split("-"):
+        return technique_1
+    elif "-" in technique_2 and technique_1 in technique_2.split("-"):
+        return technique_2
+
+    # if we're adding two independent technique which are components of a hyphenated
+    # technique, then we want that hyphenated technique. e.g. EC + MS = EC-MS
+    from .techniques import TECHNIQUE_CLASSES
+
+    for hyphenated in [
+        technique_1 + "-" + technique_2,
+        technique_2 + "-" + technique_1,
+    ]:
+        if hyphenated in TECHNIQUE_CLASSES:
+            return hyphenated
+
+    # if all else fails, we just join them with " and ". e.g. MS + XRD = MS and XRD
+    return technique_1 + " and " + technique_2
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat/plotters/base_mpl_plotter.py` & `ixdat-0.2.9.dev3/src/ixdat/plotters/base_mpl_plotter.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,193 +1,193 @@
-"""Base class for plotters using matplotlib"""
-
-from collections import defaultdict
-
-from matplotlib import pyplot as plt
-from matplotlib import gridspec
-
-
-class MPLPlotter:
-    """Base class for plotters based on matplotlib. Has methods for making mpl axes."""
-
-    def __init__(self):
-        # Instantiate data holders for dynamic range selection
-        self._dynamically_added_objects = defaultdict(list)
-        self._axis_for_range_selection = set()
-        self._selected_range = {"left": None, "right": None}
-
-    def new_ax(self, xlabel=None, ylabel=None, interactive=True):
-        """Return a new matplotlib axis optionally with the given x and y labels
-
-        Args:
-            xlabel (str): The label to apply to the x-axis
-            ylabel (str): The label to apply to the y-axis
-            interactive (bool): Whether to activate interactive range selection (default
-                True)
-
-        """
-
-        fig, ax = plt.subplots()
-        if xlabel:
-            ax.set_xlabel(xlabel)
-        if ylabel:
-            ax.set_ylabel(ylabel)
-
-        # Add the axis to those we perform range selection on and connect mouse events
-        if interactive:
-            self._axis_for_range_selection.add(ax)
-            fig.canvas.mpl_connect("button_press_event", self.onclick)
-
-        return ax
-
-    def new_two_panel_axes(self, n_bottom=1, n_top=1, emphasis="top", interactive=True):
-        """Return the axes handles for a bottom and top panel.
-
-        Args:
-            n_top (int): 1 for a single y-axis, 2 for left and right y-axes on top panel
-            n_bottom (int): 1 for a single y-axis, 2 for left and right y-axes on bottom
-            emphasis (str or None): "top" for bigger top panel, "bottom" for bigger
-                bottom panel, None for equal-sized panels
-            interactive (bool): Whether to activate interactive range selection (default
-                True)
-
-        Returns list of axes: top left, bottom left(, top right, bottom right)
-        """
-        # Necessary to avoid deleting an open figure:
-        fig = plt.figure()
-
-        if emphasis == "top":
-            gs = gridspec.GridSpec(5, 1, figure=fig)
-            # gs.update(hspace=0.025)
-            axes = [plt.subplot(gs[0:3, 0])]
-            axes += [plt.subplot(gs[3:5, 0])]
-        elif emphasis == "bottom":
-            gs = gridspec.GridSpec(5, 1, figure=fig)
-            # gs.update(hspace=0.025)
-            axes = [plt.subplot(gs[0:2, 0])]
-            axes += [plt.subplot(gs[2:5, 0])]
-        else:
-            gs = gridspec.GridSpec(6, 1, figure=fig)
-            # gs.update(hspace=0.025)
-            axes = [plt.subplot(gs[0:3, 0])]
-            axes += [plt.subplot(gs[3:6, 0])]
-
-        if interactive:
-            self._axis_for_range_selection = set(axes)
-
-        axes[0].xaxis.set_label_position("top")
-        axes[0].tick_params(
-            axis="x", top=True, bottom=False, labeltop=True, labelbottom=False
-        )
-
-        if n_bottom == 2 or n_top == 2:
-            axes += [None, None]
-        if n_top == 2:
-            axes[2] = axes[0].twinx()
-        if n_bottom == 2:
-            axes[3] = axes[1].twinx()
-
-        return axes
-
-    def new_three_panel_axes(self, n_bottom=1, n_middle=1, n_top=1, interactive=True):
-        """Return the axes handles for a bottom, middle, and top panel.
-
-        Args:
-            n_top (int): 1 for a single y-axis, 2 for left and right y-axes on top panel
-            n_middle (int): 1 for a single y-axis, 2 for left and right y-axes on middle
-            n_bottom (int): 1 for a single y-axis, 2 for left and right y-axes on bottom
-            interactive (bool): Whether to activate interactive range selection (default
-                True)
-
-        Returns list of axes: top left, middle left, bottom left(, top right, middle
-            right, bottom right)
-        """
-        # Necessary to avoid deleting an open figure, I don't know why
-        self.new_ax(interactive=interactive)
-
-        gs = gridspec.GridSpec(12, 1)
-        # gs.update(hspace=0.025)
-        axes = [plt.subplot(gs[0:4, 0])]
-        axes += [plt.subplot(gs[4:8, 0])]
-        axes += [plt.subplot(gs[8:12, 0])]
-
-        if interactive:
-            self._axis_for_range_selection = set(axes)
-
-        axes[0].xaxis.set_label_position("top")
-        axes[0].tick_params(
-            axis="x", top=True, bottom=False, labeltop=True, labelbottom=False
-        )
-        axes[1].tick_params(
-            axis="x", top=True, bottom=True, labeltop=False, labelbottom=False
-        )
-
-        if n_bottom == 2 or n_middle == 2 or n_top == 2:
-            axes += [None, None, None]
-        if n_top == 2:
-            axes[3] = axes[0].twinx()
-        if n_middle == 2:
-            axes[4] = axes[1].twinx()
-        if n_bottom == 2:
-            axes[5] = axes[2].twinx()
-
-        return axes
-
-    def onclick(self, event):
-        """Place range markers in plot"""
-        # Don't place markers if outside the plotted area
-        if event.xdata is None or event.ydata is None:
-            return
-
-        # Clear the previous marker line of this type (left/right)
-        for line in self._dynamically_added_objects.pop(event.button, []):
-            line.remove()
-
-        # Just remove the marker on double-clicks
-        if event.dblclick:
-            self._selected_range[event.button.name.lower()] = None
-            plt.draw()
-            return
-
-        # Add the new marker line
-        for ax in self._axis_for_range_selection:
-            ylim = ax.get_ylim()
-            self._dynamically_added_objects[event.button] += ax.plot(
-                [event.xdata] * 2,
-                ylim,
-                color="black",
-                linewidth=0.2,
-            )
-            ax.set_ylim(ylim)
-
-        # Add to recorded limits and print
-        self._selected_range[event.button.name.lower()] = event.xdata
-        if (
-            self._selected_range["left"] is not None
-            and self._selected_range["right"] is not None
-        ):
-            # When we have both left and right selection, extract the axis type and form
-            # a nice range name
-            extracted_xlabel = ""
-            for ax in self._axis_for_range_selection:
-                extracted_xlabel = ax.get_xlabel()
-                if extracted_xlabel != "":
-                    break
-
-            if "time" in extracted_xlabel:
-                range_name = "tspan"
-            else:
-                range_name = "xspan"
-
-            # Print span and span size
-            span_size = abs(self._selected_range["right"] - self._selected_range["left"])
-            print(
-                f"{range_name}={list(sorted(self._selected_range.values()))}"
-                f"   span_size={span_size}"
-            )
-        else:
-            # Print the one added selector
-            for side, value in self._selected_range.items():
-                if value is not None:
-                    print(f"{side}={value}")
-
-        plt.draw()
+"""Base class for plotters using matplotlib"""
+
+from collections import defaultdict
+
+from matplotlib import pyplot as plt
+from matplotlib import gridspec
+
+
+class MPLPlotter:
+    """Base class for plotters based on matplotlib. Has methods for making mpl axes."""
+
+    def __init__(self):
+        # Instantiate data holders for dynamic range selection
+        self._dynamically_added_objects = defaultdict(list)
+        self._axis_for_range_selection = set()
+        self._selected_range = {"left": None, "right": None}
+
+    def new_ax(self, xlabel=None, ylabel=None, interactive=True):
+        """Return a new matplotlib axis optionally with the given x and y labels
+
+        Args:
+            xlabel (str): The label to apply to the x-axis
+            ylabel (str): The label to apply to the y-axis
+            interactive (bool): Whether to activate interactive range selection (default
+                True)
+
+        """
+
+        fig, ax = plt.subplots()
+        if xlabel:
+            ax.set_xlabel(xlabel)
+        if ylabel:
+            ax.set_ylabel(ylabel)
+
+        # Add the axis to those we perform range selection on and connect mouse events
+        if interactive:
+            self._axis_for_range_selection.add(ax)
+            fig.canvas.mpl_connect("button_press_event", self.onclick)
+
+        return ax
+
+    def new_two_panel_axes(self, n_bottom=1, n_top=1, emphasis="top", interactive=True):
+        """Return the axes handles for a bottom and top panel.
+
+        Args:
+            n_top (int): 1 for a single y-axis, 2 for left and right y-axes on top panel
+            n_bottom (int): 1 for a single y-axis, 2 for left and right y-axes on bottom
+            emphasis (str or None): "top" for bigger top panel, "bottom" for bigger
+                bottom panel, None for equal-sized panels
+            interactive (bool): Whether to activate interactive range selection (default
+                True)
+
+        Returns list of axes: top left, bottom left(, top right, bottom right)
+        """
+        # Necessary to avoid deleting an open figure:
+        fig = plt.figure()
+
+        if emphasis == "top":
+            gs = gridspec.GridSpec(5, 1, figure=fig)
+            # gs.update(hspace=0.025)
+            axes = [plt.subplot(gs[0:3, 0])]
+            axes += [plt.subplot(gs[3:5, 0])]
+        elif emphasis == "bottom":
+            gs = gridspec.GridSpec(5, 1, figure=fig)
+            # gs.update(hspace=0.025)
+            axes = [plt.subplot(gs[0:2, 0])]
+            axes += [plt.subplot(gs[2:5, 0])]
+        else:
+            gs = gridspec.GridSpec(6, 1, figure=fig)
+            # gs.update(hspace=0.025)
+            axes = [plt.subplot(gs[0:3, 0])]
+            axes += [plt.subplot(gs[3:6, 0])]
+
+        if interactive:
+            self._axis_for_range_selection = set(axes)
+
+        axes[0].xaxis.set_label_position("top")
+        axes[0].tick_params(
+            axis="x", top=True, bottom=False, labeltop=True, labelbottom=False
+        )
+
+        if n_bottom == 2 or n_top == 2:
+            axes += [None, None]
+        if n_top == 2:
+            axes[2] = axes[0].twinx()
+        if n_bottom == 2:
+            axes[3] = axes[1].twinx()
+
+        return axes
+
+    def new_three_panel_axes(self, n_bottom=1, n_middle=1, n_top=1, interactive=True):
+        """Return the axes handles for a bottom, middle, and top panel.
+
+        Args:
+            n_top (int): 1 for a single y-axis, 2 for left and right y-axes on top panel
+            n_middle (int): 1 for a single y-axis, 2 for left and right y-axes on middle
+            n_bottom (int): 1 for a single y-axis, 2 for left and right y-axes on bottom
+            interactive (bool): Whether to activate interactive range selection (default
+                True)
+
+        Returns list of axes: top left, middle left, bottom left(, top right, middle
+            right, bottom right)
+        """
+        # Necessary to avoid deleting an open figure, I don't know why
+        self.new_ax(interactive=interactive)
+
+        gs = gridspec.GridSpec(12, 1)
+        # gs.update(hspace=0.025)
+        axes = [plt.subplot(gs[0:4, 0])]
+        axes += [plt.subplot(gs[4:8, 0])]
+        axes += [plt.subplot(gs[8:12, 0])]
+
+        if interactive:
+            self._axis_for_range_selection = set(axes)
+
+        axes[0].xaxis.set_label_position("top")
+        axes[0].tick_params(
+            axis="x", top=True, bottom=False, labeltop=True, labelbottom=False
+        )
+        axes[1].tick_params(
+            axis="x", top=True, bottom=True, labeltop=False, labelbottom=False
+        )
+
+        if n_bottom == 2 or n_middle == 2 or n_top == 2:
+            axes += [None, None, None]
+        if n_top == 2:
+            axes[3] = axes[0].twinx()
+        if n_middle == 2:
+            axes[4] = axes[1].twinx()
+        if n_bottom == 2:
+            axes[5] = axes[2].twinx()
+
+        return axes
+
+    def onclick(self, event):
+        """Place range markers in plot"""
+        # Don't place markers if outside the plotted area
+        if event.xdata is None or event.ydata is None:
+            return
+
+        # Clear the previous marker line of this type (left/right)
+        for line in self._dynamically_added_objects.pop(event.button, []):
+            line.remove()
+
+        # Just remove the marker on double-clicks
+        if event.dblclick:
+            self._selected_range[event.button.name.lower()] = None
+            plt.draw()
+            return
+
+        # Add the new marker line
+        for ax in self._axis_for_range_selection:
+            ylim = ax.get_ylim()
+            self._dynamically_added_objects[event.button] += ax.plot(
+                [event.xdata] * 2,
+                ylim,
+                color="black",
+                linewidth=0.2,
+            )
+            ax.set_ylim(ylim)
+
+        # Add to recorded limits and print
+        self._selected_range[event.button.name.lower()] = event.xdata
+        if (
+            self._selected_range["left"] is not None
+            and self._selected_range["right"] is not None
+        ):
+            # When we have both left and right selection, extract the axis type and form
+            # a nice range name
+            extracted_xlabel = ""
+            for ax in self._axis_for_range_selection:
+                extracted_xlabel = ax.get_xlabel()
+                if extracted_xlabel != "":
+                    break
+
+            if "time" in extracted_xlabel:
+                range_name = "tspan"
+            else:
+                range_name = "xspan"
+
+            # Print span and span size
+            span_size = abs(self._selected_range["right"] - self._selected_range["left"])
+            print(
+                f"{range_name}={list(sorted(self._selected_range.values()))}"
+                f"   span_size={span_size}"
+            )
+        else:
+            # Print the one added selector
+            for side, value in self._selected_range.items():
+                if value is not None:
+                    print(f"{side}={value}")
+
+        plt.draw()
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat/plotters/ec_plotter.py` & `ixdat-0.2.9.dev3/src/ixdat/plotters/ec_plotter.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,246 +1,246 @@
-"""Plotter for Electrochemistry"""
-
-import warnings
-import numpy as np
-from .base_mpl_plotter import MPLPlotter
-from .plotting_tools import color_axis
-from ..tools import deprecate
-from ..exceptions import SeriesNotFoundError
-
-
-class ECPlotter(MPLPlotter):
-    """A matplotlib plotter specialized in electrochemistry measurements."""
-
-    def __init__(self, measurement=None):
-        """Initiate the ECPlotter with its default Meausurement to plot"""
-        super().__init__()
-        self.measurement = measurement
-
-    @deprecate("0.1", "Use `U_name` instead.", "0.3", kwarg_name="V_str")
-    @deprecate("0.1", "Use `J_name` instead.", "0.3", kwarg_name="J_str")
-    @deprecate("0.1", "Use `U_color` instead.", "0.3", kwarg_name="V_color")
-    def plot_measurement(
-        self,
-        *,
-        measurement=None,
-        tspan=None,
-        U_name=None,
-        J_name=None,
-        U_color=None,
-        J_color=None,
-        V_str=None,
-        J_str=None,
-        V_color=None,
-        axes=None,
-        **plot_kwargs,
-    ):
-        """Plot two variables on two y-axes vs time
-
-        All arguments are optional. By default it plots potential in black on the left
-        y-axis and current in red on the right y-axis, using data from its entire
-        measurement. The axes are colored to match the traces and labeled with the
-        respective series names.
-
-        Args:
-            measurement (Measurement): The measurement to plot, if not the one the
-                plotter was initiated with.
-            tspan (iter of float): The timespan (wrt to measurement.tstamp) to plot.
-            axes (list of matplotlib.Axis): Two axes to plot on, if not the default
-                new twinx()'d axes. axes[0] is for `U_name` and axes[1] for `J_name`.
-            U_name (string): The name of the ValueSeries to plot on the left y-axis.
-                Defaults to measurement.V_str, which for an ECMeasurement is the name
-                of its most calibrated/correct potential.
-            J_name (string): The name of the ValueSeries to plot on the right y-axis.
-                Defaults to measurement.J_str, which for an ECMeasurement is the name
-                of its most normalized/correct current.
-            U_color (str): The color to plot U_name. Defaults to black.
-            J_color (str): The color to plot J_name. Defaults to red.
-            V_str (str): DEPRECATED. Use `U_name`.
-            J_str (str): DEPRECATED. Use `J_name`.
-            V_color (str): DEPRECATED. Use `U_color`.
-            **plot_kwargs (dict): Additional key-word arguments are passed to
-                matplotlib's plot() function. See below for a few examples
-
-        Keyword Arguments:
-            linestle (str): Type of line, e.g. "-" for solid or "--" for dotted
-
-        Returns list of matplotlib.pyplot.Axis: The axes plotted on.
-        """
-        measurement = measurement or self.measurement
-
-        # apply deprecated arguments (the user will get a warning):
-        U_name = U_name or V_str
-        J_name = J_name or J_str
-        U_color = U_color or V_color
-
-        # apply defaults.
-        U_name = U_name or measurement.U_name
-        J_name = J_name or measurement.J_name
-        U_color = U_color or "k"
-        J_color = J_color or "r"
-
-        if axes:
-            ax1, ax2 = axes
-        else:
-            ax1 = self.new_ax()
-            ax2 = ax1.twinx()
-            axes = [ax1, ax2]
-        ax1.set_xlabel("time / [s]")
-        ax1.set_ylabel(U_name)
-        ax2.set_ylabel(J_name)
-        color_axis(ax1, U_color, lr="left")
-        color_axis(ax2, J_color, lr="right")
-
-        try:
-            t_v, v = measurement.grab(U_name, tspan=tspan)
-        except SeriesNotFoundError:
-            warnings.warn(f"No '{U_name}' found in {measurement}")
-        else:
-            ax1.plot(t_v, v, "-", color=U_color, label=U_name, **plot_kwargs)
-
-        try:
-            t_j, j = measurement.grab(J_name, tspan=tspan)
-        except SeriesNotFoundError:
-            warnings.warn(f"No '{J_name}' found in {measurement}")
-        else:
-            ax2.plot(t_j, j, "-", color=J_color, label=J_name, **plot_kwargs)
-
-        return axes
-
-    def plot_vs_potential(
-        self,
-        measurement=None,
-        tspan=None,
-        U_name=None,
-        J_name=None,
-        ax=None,
-        **plot_kwargs,
-    ):
-        """Plot an ECMeasurement with electrode potential on the x-axis.
-
-        This can actually plot with anything on the x-axis, by specifying what you want
-        on the x-axis using V_str. The y-axis variable, which can be specified by J_str,
-        is interpolated onto the time corresponding to the x-axis variable.
-        .. TODO::
-            This is a special case of the not-yet-implemented generalized
-            `plot_vs`. Consider an inheritance structure to reduce redundancy in
-            future plotters.
-            sub-TODO: hide or fix TODO's using sphix boxes.
-        All arguments are optional. By default it will plot current vs potential in
-        black on a single axis for the whole experiment.
-            TODO: color gradient (cmap=inferno) from first to last cycle.
-
-        Args:
-            measurement (Measurement): What to plot. Defaults to the measurement the
-                plotter was initialized with
-            tspan (iter of float): The timespan, relative to vs measurement.tstamp, on
-                which to plot.
-            U_name (str): Name of the x-axis variable. Defaults to calibrated potential
-            J_name (str): Name of the y-axis variable. Defaults to normalized current.
-            ax (matplotlib.pyplot.Axis): The axis to plot on, if not a new one.
-            **plot_kwargs (dict): Additional key-word arguments are passed to
-                matplotlib's plot() function. See below for a few examples
-
-        Keyword Arguments:
-            color (color): Color of the trace, e.g. "r", "blue", or RGB like [0, 0, 1]
-            linestle (str): Type of line, e.g. "-" for solid or "--" for dotted
-
-        Returns matplotlib.pyplot.axis: The axis plotted on.
-        """
-
-        measurement = measurement or self.measurement
-        U_name = U_name or measurement.U_name
-        J_name = J_name or measurement.J_name
-        t_v, v = measurement.grab(U_name, tspan=tspan)
-        t_j, j = measurement.grab(J_name, tspan=tspan)
-
-        j_v = np.interp(t_v, t_j, j)
-        if not ax:
-            ax = self.new_ax()
-
-        if "color" not in plot_kwargs:
-            plot_kwargs["color"] = "k"
-        ax.plot(v, j_v, **plot_kwargs)
-        ax.set_xlabel(U_name)
-        ax.set_ylabel(J_name)
-        return ax
-
-
-class CVDiffPlotter(MPLPlotter):
-    """A matplotlib plotter for highlighting the difference between two cv's."""
-
-    def __init__(self, measurement=None):
-        """Initiate the ECPlotter with its default CyclicVoltammagramDiff to plot"""
-        super().__init__()
-        self.measurement = measurement
-
-    def plot(self, measurement=None, ax=None):
-        """Plot the two cycles of the CVDiff measurement and fill in the areas between
-
-        example: https://ixdat.readthedocs.io/en/latest/_images/cv_diff.svg
-        """
-        measurement = measurement or self.measurement
-        # FIXME: This is probably the wrong use of plotter functions.
-        #    see https://github.com/ixdat/ixdat/pull/30/files#r810926968
-        ax = ECPlotter.plot_vs_potential(
-            self, measurement=measurement.cv_compare_1, ax=ax, color="g"
-        )
-        ax = ECPlotter.plot_vs_potential(
-            self, measurement=measurement.cv_compare_2, ax=ax, color="k", linestyle="--"
-        )
-        t1, U1 = measurement.cv_compare_1.grab("potential")
-        J1 = measurement.cv_compare_1.grab_for_t("current", t=t1)
-        J_diff = measurement.grab_for_t("current", t=t1)
-        # a mask which is true when cv_1 had bigger current than cv_2:
-        v_scan = measurement.grab_for_t("scan_rate", t=t1)
-        mask = np.logical_xor(0 < J_diff, v_scan < 0)
-
-        ax.fill_between(U1, J1 - J_diff, J1, where=mask, alpha=0.2, color="g")
-        ax.fill_between(
-            U1,
-            J1 - J_diff,
-            J1,
-            where=np.logical_not(mask),
-            alpha=0.1,
-            hatch="//",
-            color="g",
-        )
-
-        return ax
-
-    def plot_measurement(self, measurement=None, axes=None, **kwargs):
-        """Plot the difference between the two cv's vs time"""
-        measurement = measurement or self.measurement
-        # FIXME: not correct useage of
-        return ECPlotter.plot_measurement(
-            self, measurement=measurement, axes=axes, **kwargs
-        )
-
-    def plot_diff(self, measurement=None, tspan=None, ax=None):
-        """Plot the difference between the two cv's vs potential.
-
-        The trace is solid where the current in cv_2 is greater than cv_1 in the anodic
-        scan or the current cv_2 is more negative than cv_1 in the cathodic scan.
-        """
-        measurement = measurement or self.measurement
-        t, U = measurement.grab("potential", tspan=tspan, include_endpoints=False)
-        j_diff = measurement.grab_for_t("current", t)
-        v_scan = measurement.grab_for_t("scan_rate", t)
-        # a mask which is true when cv_1 had bigger current than cv_2:
-        mask = np.logical_xor(0 < j_diff, v_scan < 0)
-
-        if not ax:
-            ax = self.new_ax()
-
-        ax.plot(U[mask], j_diff[mask], "k-", label="cv1 > cv2")
-        ax.plot(
-            U[np.logical_not(mask)],
-            j_diff[np.logical_not(mask)],
-            "k--",
-            label="cv1 < cv2",
-        )
-        return ax
-
-    def plot_vs_potential(self):
-        """FIXME: This is needed to satisfy ECMeasurement.__init__"""
-        pass
+"""Plotter for Electrochemistry"""
+
+import warnings
+import numpy as np
+from .base_mpl_plotter import MPLPlotter
+from .plotting_tools import color_axis
+from ..tools import deprecate
+from ..exceptions import SeriesNotFoundError
+
+
+class ECPlotter(MPLPlotter):
+    """A matplotlib plotter specialized in electrochemistry measurements."""
+
+    def __init__(self, measurement=None):
+        """Initiate the ECPlotter with its default Meausurement to plot"""
+        super().__init__()
+        self.measurement = measurement
+
+    @deprecate("0.1", "Use `U_name` instead.", "0.3", kwarg_name="V_str")
+    @deprecate("0.1", "Use `J_name` instead.", "0.3", kwarg_name="J_str")
+    @deprecate("0.1", "Use `U_color` instead.", "0.3", kwarg_name="V_color")
+    def plot_measurement(
+        self,
+        *,
+        measurement=None,
+        tspan=None,
+        U_name=None,
+        J_name=None,
+        U_color=None,
+        J_color=None,
+        V_str=None,
+        J_str=None,
+        V_color=None,
+        axes=None,
+        **plot_kwargs,
+    ):
+        """Plot two variables on two y-axes vs time
+
+        All arguments are optional. By default it plots potential in black on the left
+        y-axis and current in red on the right y-axis, using data from its entire
+        measurement. The axes are colored to match the traces and labeled with the
+        respective series names.
+
+        Args:
+            measurement (Measurement): The measurement to plot, if not the one the
+                plotter was initiated with.
+            tspan (iter of float): The timespan (wrt to measurement.tstamp) to plot.
+            axes (list of matplotlib.Axis): Two axes to plot on, if not the default
+                new twinx()'d axes. axes[0] is for `U_name` and axes[1] for `J_name`.
+            U_name (string): The name of the ValueSeries to plot on the left y-axis.
+                Defaults to measurement.V_str, which for an ECMeasurement is the name
+                of its most calibrated/correct potential.
+            J_name (string): The name of the ValueSeries to plot on the right y-axis.
+                Defaults to measurement.J_str, which for an ECMeasurement is the name
+                of its most normalized/correct current.
+            U_color (str): The color to plot U_name. Defaults to black.
+            J_color (str): The color to plot J_name. Defaults to red.
+            V_str (str): DEPRECATED. Use `U_name`.
+            J_str (str): DEPRECATED. Use `J_name`.
+            V_color (str): DEPRECATED. Use `U_color`.
+            **plot_kwargs (dict): Additional key-word arguments are passed to
+                matplotlib's plot() function. See below for a few examples
+
+        Keyword Arguments:
+            linestle (str): Type of line, e.g. "-" for solid or "--" for dotted
+
+        Returns list of matplotlib.pyplot.Axis: The axes plotted on.
+        """
+        measurement = measurement or self.measurement
+
+        # apply deprecated arguments (the user will get a warning):
+        U_name = U_name or V_str
+        J_name = J_name or J_str
+        U_color = U_color or V_color
+
+        # apply defaults.
+        U_name = U_name or measurement.U_name
+        J_name = J_name or measurement.J_name
+        U_color = U_color or "k"
+        J_color = J_color or "r"
+
+        if axes:
+            ax1, ax2 = axes
+        else:
+            ax1 = self.new_ax()
+            ax2 = ax1.twinx()
+            axes = [ax1, ax2]
+        ax1.set_xlabel("time / [s]")
+        ax1.set_ylabel(U_name)
+        ax2.set_ylabel(J_name)
+        color_axis(ax1, U_color, lr="left")
+        color_axis(ax2, J_color, lr="right")
+
+        try:
+            t_v, v = measurement.grab(U_name, tspan=tspan)
+        except SeriesNotFoundError:
+            warnings.warn(f"No '{U_name}' found in {measurement}")
+        else:
+            ax1.plot(t_v, v, "-", color=U_color, label=U_name, **plot_kwargs)
+
+        try:
+            t_j, j = measurement.grab(J_name, tspan=tspan)
+        except SeriesNotFoundError:
+            warnings.warn(f"No '{J_name}' found in {measurement}")
+        else:
+            ax2.plot(t_j, j, "-", color=J_color, label=J_name, **plot_kwargs)
+
+        return axes
+
+    def plot_vs_potential(
+        self,
+        measurement=None,
+        tspan=None,
+        U_name=None,
+        J_name=None,
+        ax=None,
+        **plot_kwargs,
+    ):
+        """Plot an ECMeasurement with electrode potential on the x-axis.
+
+        This can actually plot with anything on the x-axis, by specifying what you want
+        on the x-axis using V_str. The y-axis variable, which can be specified by J_str,
+        is interpolated onto the time corresponding to the x-axis variable.
+        .. TODO::
+            This is a special case of the not-yet-implemented generalized
+            `plot_vs`. Consider an inheritance structure to reduce redundancy in
+            future plotters.
+            sub-TODO: hide or fix TODO's using sphix boxes.
+        All arguments are optional. By default it will plot current vs potential in
+        black on a single axis for the whole experiment.
+            TODO: color gradient (cmap=inferno) from first to last cycle.
+
+        Args:
+            measurement (Measurement): What to plot. Defaults to the measurement the
+                plotter was initialized with
+            tspan (iter of float): The timespan, relative to vs measurement.tstamp, on
+                which to plot.
+            U_name (str): Name of the x-axis variable. Defaults to calibrated potential
+            J_name (str): Name of the y-axis variable. Defaults to normalized current.
+            ax (matplotlib.pyplot.Axis): The axis to plot on, if not a new one.
+            **plot_kwargs (dict): Additional key-word arguments are passed to
+                matplotlib's plot() function. See below for a few examples
+
+        Keyword Arguments:
+            color (color): Color of the trace, e.g. "r", "blue", or RGB like [0, 0, 1]
+            linestle (str): Type of line, e.g. "-" for solid or "--" for dotted
+
+        Returns matplotlib.pyplot.axis: The axis plotted on.
+        """
+
+        measurement = measurement or self.measurement
+        U_name = U_name or measurement.U_name
+        J_name = J_name or measurement.J_name
+        t_v, v = measurement.grab(U_name, tspan=tspan)
+        t_j, j = measurement.grab(J_name, tspan=tspan)
+
+        j_v = np.interp(t_v, t_j, j)
+        if not ax:
+            ax = self.new_ax()
+
+        if "color" not in plot_kwargs:
+            plot_kwargs["color"] = "k"
+        ax.plot(v, j_v, **plot_kwargs)
+        ax.set_xlabel(U_name)
+        ax.set_ylabel(J_name)
+        return ax
+
+
+class CVDiffPlotter(MPLPlotter):
+    """A matplotlib plotter for highlighting the difference between two cv's."""
+
+    def __init__(self, measurement=None):
+        """Initiate the ECPlotter with its default CyclicVoltammagramDiff to plot"""
+        super().__init__()
+        self.measurement = measurement
+
+    def plot(self, measurement=None, ax=None):
+        """Plot the two cycles of the CVDiff measurement and fill in the areas between
+
+        example: https://ixdat.readthedocs.io/en/latest/_images/cv_diff.svg
+        """
+        measurement = measurement or self.measurement
+        # FIXME: This is probably the wrong use of plotter functions.
+        #    see https://github.com/ixdat/ixdat/pull/30/files#r810926968
+        ax = ECPlotter.plot_vs_potential(
+            self, measurement=measurement.cv_compare_1, ax=ax, color="g"
+        )
+        ax = ECPlotter.plot_vs_potential(
+            self, measurement=measurement.cv_compare_2, ax=ax, color="k", linestyle="--"
+        )
+        t1, U1 = measurement.cv_compare_1.grab("potential")
+        J1 = measurement.cv_compare_1.grab_for_t("current", t=t1)
+        J_diff = measurement.grab_for_t("current", t=t1)
+        # a mask which is true when cv_1 had bigger current than cv_2:
+        v_scan = measurement.grab_for_t("scan_rate", t=t1)
+        mask = np.logical_xor(0 < J_diff, v_scan < 0)
+
+        ax.fill_between(U1, J1 - J_diff, J1, where=mask, alpha=0.2, color="g")
+        ax.fill_between(
+            U1,
+            J1 - J_diff,
+            J1,
+            where=np.logical_not(mask),
+            alpha=0.1,
+            hatch="//",
+            color="g",
+        )
+
+        return ax
+
+    def plot_measurement(self, measurement=None, axes=None, **kwargs):
+        """Plot the difference between the two cv's vs time"""
+        measurement = measurement or self.measurement
+        # FIXME: not correct useage of
+        return ECPlotter.plot_measurement(
+            self, measurement=measurement, axes=axes, **kwargs
+        )
+
+    def plot_diff(self, measurement=None, tspan=None, ax=None):
+        """Plot the difference between the two cv's vs potential.
+
+        The trace is solid where the current in cv_2 is greater than cv_1 in the anodic
+        scan or the current cv_2 is more negative than cv_1 in the cathodic scan.
+        """
+        measurement = measurement or self.measurement
+        t, U = measurement.grab("potential", tspan=tspan, include_endpoints=False)
+        j_diff = measurement.grab_for_t("current", t)
+        v_scan = measurement.grab_for_t("scan_rate", t)
+        # a mask which is true when cv_1 had bigger current than cv_2:
+        mask = np.logical_xor(0 < j_diff, v_scan < 0)
+
+        if not ax:
+            ax = self.new_ax()
+
+        ax.plot(U[mask], j_diff[mask], "k-", label="cv1 > cv2")
+        ax.plot(
+            U[np.logical_not(mask)],
+            j_diff[np.logical_not(mask)],
+            "k--",
+            label="cv1 < cv2",
+        )
+        return ax
+
+    def plot_vs_potential(self):
+        """FIXME: This is needed to satisfy ECMeasurement.__init__"""
+        pass
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat/plotters/ms_plotter.py` & `ixdat-0.2.9.dev3/src/ixdat/plotters/ms_plotter.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,1006 +1,1006 @@
-"""Plotter for Mass Spectrometry"""
-import warnings
-from ..data_series import Field
-import numpy as np
-from .base_mpl_plotter import MPLPlotter
-
-
-class MSPlotter(MPLPlotter):
-    """A matplotlib plotter specialized in mass spectrometry MID measurements."""
-
-    def __init__(self, measurement=None):
-        """Initiate the ECMSPlotter with its default Meausurement to plot"""
-        super().__init__()
-        self.measurement = measurement
-
-    def plot_measurement(
-        self,
-        *,
-        measurement=None,
-        ax=None,
-        axes=None,
-        mass_list=None,
-        mass_lists=None,
-        mol_list=None,
-        mol_lists=None,
-        tspan=None,
-        tspan_bg=None,
-        remove_background=None,
-        unit=None,
-        x_unit=None,
-        logplot=True,
-        logdata=False,
-        legend=True,
-        **kwargs,
-    ):
-        """Plot m/z signal vs time (MID) data and return the axis.
-
-        There are four ways to specify what to plot. Only specify one of these::
-            mass_list: Uncalibrated signals in [(u/n/p)A] on on axis
-            mass_lists: Uncalibrated signals in [(u/n/p)A] on two axes
-            mol_list: Calibrated signals in [(u/n/p)mol/s] on on axis
-            mol_lists: Calibrated signals in [(u/n/p)mol/s] on two axes
-
-        Two axes refers to separate left and right y-axes. Default is to use all
-        available masses as mass_list.
-
-        Args:
-            measurement (MSMeasurement): Defaults to the one that initiated the plotter
-            ax (matplotlib axis): Defaults to a new axis
-            axes (list of matplotlib axis): Left and right y-axes if mass_lists are given
-            mass_list (list of str): The names of the m/z values, eg. ["M2", ...] to
-                plot. Defaults to all of them (measurement.mass_list)
-            mass_lists (list of list of str): Alternately, two lists can be given for
-                masses in which case one list is plotted on the left y-axis and the other
-                on the right y-axis of the top panel.
-            mol_list (list of str): The names of the molecules, eg. ["H2", ...] to
-                plot. Defaults to all of them if quantified (measurement.mass_list)
-            mol_lists (list of list of str): Alternately, two lists can be given for
-                molecules in which case one list is plotted on the left y-axis and the
-                other on the right y-axis of the top panel.
-            tspan (iter of float): The time interval to plot, wrt measurement.tstamp
-            tspan_bg (timespan): A timespan for which to assume the signal is at its
-                background. The average signals during this timespan are subtracted.
-                If `mass_lists` are given rather than a single `mass_list`, `tspan_bg`
-                must also be two timespans - one for each axis. Default is `None` for no
-                background subtraction.
-            remove_background (bool): Whether otherwise to subtract pre-determined
-                background signals if available. Defaults to (not logplot)
-            unit (str): unit of the y axis. defaults to "A" or "mol/s"
-            x_unit (str): unit of the x axis variable (usually time). defaults to "s"
-            logplot (bool): Whether to plot the MS data on a log scale (default True)
-            logdata (bool): Whether to plot the natural logarithm of MS data on a
-                linear scale (default False)
-            legend (bool): Whether to use a legend for the MS data (default True)
-            kwargs: extra key-word args are passed on to matplotlib's plot()
-        """
-        measurement = measurement or self.measurement
-        if remove_background is None:
-            remove_background = not logplot
-
-        # Figure out, based on the inputs, whether or not to plot calibrated results
-        # (`quantified`), specifications for the axis to plot on now (`specs_this_axis`)
-        # and specifications for the next axis to plot on, if any (`specs_next_axis`):
-        quantified, specs_this_axis, specs_next_axis = self._parse_overloaded_inputs(
-            mass_list,
-            mass_lists,
-            mol_list,
-            mol_lists,
-            unit,
-            tspan_bg,
-            ax,
-            axes,
-            measurement,
-        )
-        ax = specs_this_axis["ax"]
-        v_list = specs_this_axis["v_list"]
-        tspan_bg = specs_this_axis["tspan_bg"]
-        unit = specs_this_axis["unit"]
-        unit_factor = specs_this_axis["unit_factor"]
-        for v_or_v_name in v_list:
-            if isinstance(v_or_v_name, str):
-                v_name = v_or_v_name
-                color = STANDARD_COLORS.get(v_name, "k")
-            else:
-                v_name = v_or_v_name.name
-                color = v_or_v_name.color
-            if quantified:
-                t, v = measurement.grab_flux(
-                    v_or_v_name,
-                    tspan=tspan,
-                    tspan_bg=tspan_bg,
-                    remove_background=remove_background,
-                    include_endpoints=False,
-                )
-            else:
-                t, v = measurement.grab_signal(
-                    v_or_v_name,
-                    tspan=tspan,
-                    tspan_bg=tspan_bg,
-                    remove_background=remove_background,
-                    include_endpoints=False,
-                )
-            if logplot:
-                v[v < MIN_SIGNAL] = MIN_SIGNAL
-            if logdata:
-                logplot = False
-                # to correctly plot data with corrosponding unit and unit_factor
-                v = np.log(v * unit_factor) * 1 / unit_factor
-                unit = f"ln({unit})"
-            # expect always to plot against time
-            x_unit_factor, x_unit = self._get_x_unit_factor(x_unit, "s")
-            ax.plot(
-                t * x_unit_factor,
-                v * unit_factor,
-                color=color,
-                label=v_name,
-                **kwargs,
-            )
-        ax.set_ylabel(f"signal / [{unit}]")
-        ax.set_xlabel(f"time / [{x_unit}]")
-        if specs_next_axis:
-            self.plot_measurement(
-                measurement=measurement,
-                ax=specs_next_axis["ax"],
-                mass_list=specs_next_axis["mass_list"],
-                mol_list=specs_next_axis["mol_list"],
-                unit=specs_next_axis["unit"],
-                x_unit=x_unit,
-                tspan=tspan,
-                tspan_bg=specs_next_axis["tspan_bg"],
-                logplot=logplot,
-                logdata=logdata,
-                legend=legend,
-                **kwargs,
-            )
-            axes = [ax, specs_next_axis["ax"]]
-        else:
-            axes = None
-
-        if logplot:
-            ax.set_yscale("log")
-        if legend:
-            ax.legend()
-
-        return axes if axes else ax
-
-    def plot_vs(
-        self,
-        *,
-        x_name,
-        measurement=None,
-        ax=None,
-        axes=None,
-        mass_list=None,
-        mass_lists=None,
-        mol_list=None,
-        mol_lists=None,
-        tspan=None,
-        tspan_bg=None,
-        remove_background=None,
-        unit=None,
-        x_unit=None,
-        logplot=True,
-        logdata=False,
-        legend=True,
-        **plot_kwargs,
-    ):
-        """Plot m/z signal (MID) data against a specified variable and return the axis.
-
-        There are four ways to specify what to plot. Only specify one of these::
-            mass_list: Uncalibrated signals in [(u/n/p)A] on on axis
-            mass_lists: Uncalibrated signals in [(u/n/p)A] on two axes
-            mol_list: Calibrated signals in [(u/n/p)mol/s] on on axis
-            mol_lists: Calibrated signals in [(u/n/p)mol/s] on two axes
-
-        Two axes refers to seperate left and right y-axes. Default is to use all
-        available masses as mass_list.
-
-        Args:
-            x_name (str): Name of the variable to plot on the x-axis
-            measurement (MSMeasurement): Defaults to the one that initiated the plotter
-            ax (matplotlib axis): Defaults to a new axis
-            axes (list of matplotlib axis): Left and right y-axes if mass_lists are given
-            mass_list (list of str): The names of the m/z values, eg. ["M2", ...] to
-                plot. Defaults to all of them (measurement.mass_list)
-            mass_lists (list of list of str): Alternately, two lists can be given for
-                masses in which case one list is plotted on the left y-axis and the other
-                on the right y-axis of the top panel.
-            mol_list (list of str): The names of the molecules, eg. ["H2", ...] to
-                plot. Defaults to all of them (measurement.mass_list)
-            mol_lists (list of list of str): Alternately, two lists can be given for
-                molecules in which case one list is plotted on the left y-axis and the
-                other on the right y-axis of the top panel.
-            tspan (iter of float): The time interval to plot, wrt measurement.tstamp
-            tspan_bg (timespan): A timespan for which to assume the signal is at its
-                background. The average signals during this timespan are subtracted.
-                If `mass_lists` are given rather than a single `mass_list`, `tspan_bg`
-                must also be two timespans - one for each axis. Default is `None` for no
-                background subtraction.
-            remove_background (bool): Whether otherwise to subtract pre-determined
-                background signals if available
-            unit (str): defaults to "A" or "mol/s"
-            x_unit (str): defaults to x_name.unit.name
-            logplot (bool): Whether to plot the MS data on a log scale (default True)
-            logdata (bool): Whether to plot the natural logarithm of MS data on a
-                linear scale (default False)
-            legend (bool): Whether to use a legend for the MS data (default True)
-            plot_kwargs: additional key-word args are passed on to matplotlib's plot()
-        """
-        measurement = measurement or self.measurement
-        if remove_background is None:
-            remove_background = not logplot
-
-        # The overloaded inputs are a pain in the ass. This function helps:
-        quantified, specs_this_axis, specs_next_axis = self._parse_overloaded_inputs(
-            mass_list,
-            mass_lists,
-            mol_list,
-            mol_lists,
-            unit,
-            tspan_bg,
-            ax,
-            axes,
-            measurement,
-        )
-        ax = specs_this_axis["ax"]
-        v_list = specs_this_axis["v_list"]
-        tspan_bg = specs_this_axis["tspan_bg"]
-        unit = specs_this_axis["unit"]
-        unit_factor = specs_this_axis["unit_factor"]
-
-        t, x = measurement.grab(x_name, tspan=tspan, include_endpoints=True)
-        for i, v_name in enumerate(v_list):
-            if quantified:
-                t_v, v = measurement.grab_flux(
-                    v_name,
-                    tspan=tspan,
-                    tspan_bg=tspan_bg,
-                    remove_background=remove_background,
-                    include_endpoints=False,
-                )
-            else:
-                t_v, v = measurement.grab_signal(
-                    v_name,
-                    tspan=tspan,
-                    tspan_bg=tspan_bg,
-                    remove_background=remove_background,
-                    include_endpoints=False,
-                )
-            if logplot:
-                v[v < MIN_SIGNAL] = MIN_SIGNAL
-            x_mass = np.interp(t_v, t, x)
-            plot_kwargs_this_mass = plot_kwargs.copy()
-            if "color" not in plot_kwargs:
-                plot_kwargs_this_mass["color"] = STANDARD_COLORS.get(v_name, "k")
-            if "label" not in plot_kwargs:
-                plot_kwargs_this_mass["label"] = v_name
-
-            x_unit_factor, x_unit = self._get_x_unit_factor(
-                x_unit, measurement[x_name].unit.name
-            )
-            # Used to plot ln(mol) on a linear scale
-            if logdata:
-                logplot = False
-                v = np.log(v * unit_factor) * 1 / unit_factor
-                if not i:  # To avoid looping ln()'s around unit for each mass plotted.
-                    unit = f"ln({unit})"
-
-            ax.plot(
-                x_mass * x_unit_factor,
-                v * unit_factor,
-                **plot_kwargs_this_mass,
-            )
-
-        ax.set_ylabel(f"signal / [{unit}]")
-
-        ax.set_xlabel(f"{x_name} / [{x_unit}]")
-
-        if specs_next_axis:
-            self.plot_vs(
-                x_name=x_name,
-                measurement=measurement,
-                ax=specs_next_axis["ax"],
-                mass_list=specs_next_axis["mass_list"],
-                mol_list=specs_next_axis["mol_list"],
-                unit=specs_next_axis["unit"],
-                x_unit=x_unit,
-                tspan=tspan,
-                tspan_bg=specs_next_axis["tspan_bg"],
-                logplot=logplot,
-                legend=legend,
-                logdata=logdata,
-                **plot_kwargs,
-            )
-            axes = [ax, specs_next_axis["ax"]]
-        else:
-            axes = None
-
-        if logplot:
-            ax.set_yscale("log")
-        if legend:
-            ax.legend()
-
-        return axes if axes else ax
-
-    def _get_x_unit_factor(
-        self,
-        new_x_unit,
-        original_unit_name,
-    ):
-        # FIXME: This function will disappear with pint units (#146)
-        if not new_x_unit:
-            return 1, original_unit_name
-        if (original_unit_name or new_x_unit) in ["kelvin", "K", "celsius", "C"]:
-            warnings.warn(
-                f"Converting '{original_unit_name}' to '{new_x_unit}' should be done in "
-                f"({self.measurement.__class__.__name__}) prior to plotting using "
-                f"({self.__class__.__name__}). "
-                f"Plotting using '{original_unit_name}'.",
-                stacklevel=2,
-            )
-            return 1, original_unit_name
-        try:
-            x_unit_factor = {
-                # Time conversion
-                "s": 1,
-                "min": 1 / 60,
-                "minutes": 1 / 60,
-                "h": 1 / 3600,
-                "hr": 1 / 3600,
-                "hour": 1 / 3600,
-                "hours": 1 / 3600,
-                "d": 1 / (3600 * 24),
-                "days": 1 / (3600 * 24),
-                # Pressure conversion
-                "mbar": 1,
-                "bar": 1000,
-                "hPa": 1,
-                "kPa": 0.1,
-                # Temperature conversion
-                # "K": 273.15,
-                # "kelvin": 273.15,
-                # "C": -273.15,
-                # "celsius": -273.15,
-            }[new_x_unit]
-        except KeyError:
-            warnings.warn(
-                f"Can't convert original unit '{original_unit_name}' to new unit "
-                f"'{new_x_unit}'. Plotting using original unit '{new_x_unit}' with "
-                "unit_factor=1 (one).",
-                stacklevel=2,
-            )
-            x_unit_factor = 1
-            new_x_unit = original_unit_name
-        return x_unit_factor, new_x_unit
-
-    def _parse_overloaded_inputs(
-        self,
-        mass_list,
-        mass_lists,
-        mol_list,
-        mol_lists,
-        unit,
-        tspan_bg,
-        ax,
-        axes,
-        measurement,
-    ):
-        """From the overloaded function inputs, figure out what the user wants to do.
-
-        This includes:
-        1. determine if we're doing quantifed results (mols) or raw (masses)
-        2. figure out if there's one or two axes (remaining) and what goes on them.
-        3. figure out what to multiply numbers by when plotting to match the unit.
-        """
-        # TODO: Maybe there's a way to do this function as a decorator?
-        # So this function is overloaded in the sense that the user can give
-        #   exactly one of mol_list, mol_lists, mass_list, mass_lists.
-        # To manage that complexity, first we reduce it to two options, that down to
-        #   either v_list or v_lists and a boolean "quantified":
-        quantified = False  # default, if they give nothing
-        v_lists = None  # default, if they give nothing
-        v_list = measurement.mass_list  # default, if they give nothing
-        if mol_list:
-            quantified = True
-            v_list = mol_list
-        elif mol_lists:
-            quantified = True
-            v_lists = mol_lists
-        elif mass_list:
-            quantified = False
-            v_list = mass_list
-        elif mass_lists:
-            quantified = False
-            v_lists = mass_lists
-
-        if not ax:
-            ax = (
-                axes[0]
-                if axes
-                else self.new_ax(ylabel=f"signal / [{unit}]", xlabel="time / [s]")
-            )
-        # as the next simplification, if they give two things (v_lists), we pretend we
-        #   got one (v_list) but prepare an axis for a recursive call of this function.
-        if v_lists:
-            axes = axes or [ax, ax.twinx()]  # prepare an axis unless we were given two.
-            ax_right = axes[-1]
-            ax = axes[0]
-            v_list = v_lists[0]
-            v_list_right = v_lists[1]
-            # ah, and to enable different background tspans for the two axes:
-            try:
-                tspan_bg_right = tspan_bg[1]
-                if isinstance(tspan_bg_right, (float, int)):
-                    raise TypeError
-            except (KeyError, TypeError):
-                tspan_bg_right = None
-            else:
-                tspan_bg = tspan_bg[0]
-            if isinstance(unit, str) or not unit:
-                unit_right = unit
-            else:
-                unit_right = unit[1]
-                unit = unit[0]
-            specs_next_axis = {
-                "ax": ax_right,
-                "unit": unit_right,
-                "mass_list": None if quantified else v_list_right,
-                "mol_list": v_list_right if quantified else None,
-                "tspan_bg": tspan_bg_right,
-            }
-        else:
-            specs_next_axis = None
-
-        if quantified:
-            unit = unit or "mol/s"
-            unit_factor = {
-                "pmol/s": 1e12,
-                "nmol/s": 1e9,
-                "umol/s": 1e6,
-                "mmol/s": 1e3,
-                "mol/s": 1,  # noqa
-                "pmol/s/cm^2": 1e12,
-                "nmol/s/cm^2": 1e9,
-                "umol/s/cm^2": 1e6,
-                "mmol/s/cm^2": 1e3,
-                "mol/s/cm^2": 1,  # noqa
-            }[unit]
-            if "/cm^2" in unit:
-                unit_factor = unit_factor / measurement.A_el
-        else:
-            unit = unit or "A"
-            unit_factor = {"pA": 1e12, "nA": 1e9, "uA": 1e6, "mA": 1e3, "A": 1}[unit]
-        # TODO: Real units with a unit module! This should even be able to figure out the
-        #  unit prefix to put stuff in a nice 1-to-1e3 range
-
-        specs_this_axis = {
-            "ax": ax,
-            "v_list": v_list,
-            "unit": unit,
-            "unit_factor": unit_factor,
-            "tspan_bg": tspan_bg,
-        }
-
-        return quantified, specs_this_axis, specs_next_axis
-
-
-class MSSpectroPlotter(MPLPlotter):
-    """A matplotlib plotter specialized in mass spectrometry MID measurements."""
-
-    def __init__(self, measurement=None):
-        """Initiate the SpectroMSPlotter with its default Meausurement to plot"""
-        super().__init__()
-        self.measurement = measurement
-        self.ms_plotter = MSPlotter(measurement=measurement)
-
-    def plot_measurement(
-        self,
-        *,
-        measurement=None,
-        axes=None,
-        mass_list=None,
-        mass_lists=None,
-        mol_list=None,
-        mol_lists=None,
-        tspan=None,
-        tspan_bg=None,
-        remove_background=None,
-        unit=None,
-        x_unit=None,
-        logplot=True,
-        logdata=False,
-        legend=True,
-        xspan=None,
-        cmap_name="inferno",
-        make_colorbar=False,
-        emphasis="top",
-        ms_data="top",
-        max_threshold=None,
-        min_threshold=None,
-        scanning_mask=None,
-        vmin=None,
-        vmax=None,
-        **kwargs,
-    ):
-        """Plot m/z signal, mass spectra vs time (MID) data and return the axes of a two
-        panel figure.
-
-        There are four ways to specify what to plot. Only specify one of these::
-            mass_list: Uncalibrated signals in [(u/n/p)A] on on axis
-            mass_lists: Uncalibrated signals in [(u/n/p)A] on two axes
-            mol_list: Calibrated signals in [(u/n/p)mol/s] on on axis
-            mol_lists: Calibrated signals in [(u/n/p)mol/s] on two axes
-
-        Two axes refers to separate left and right y-axes. Default is to use all
-        available masses as mass_list.
-
-        Args:
-            measurement (SpectroMSMeasurement): Defaults to the one that initiated the
-                plotter
-            axes (list of matplotlib axis): Left and right y-axes if mass_lists are given
-                default to axes[0] as left and axes[2] as right axis for plotting masses
-                and axes[1] for plotting MSSpectra
-            mass_list (list of str): The names of the m/z values, eg. ["M2", ...] to
-                plot. Defaults to all of them (measurement.mass_list)
-            mass_lists (list of list of str): Alternately, two lists can be given for
-                masses in which case one list is plotted on the left y-axis and the other
-                on the right y-axis of the top panel.
-            mol_list (list of str): The names of the molecules, eg. ["H2", ...] to
-                plot. Defaults to all of them (measurement.mass_list)
-            mol_lists (list of list of str): Alternately, two lists can be given for
-                molecules in which case one list is plotted on the left y-axis and the
-                other on the right y-axis of the top panel.
-            tspan (iter of float): The time interval to plot, wrt measurement.tstamp
-            tspan_bg (timespan): A timespan for which to assume the signal is at its
-                background. The average signals during this timespan are subtracted.
-                If `mass_lists` are given rather than a single `mass_list`, `tspan_bg`
-                must also be two timespans - one for each axis. Default is `None` for no
-                background subtraction.
-            remove_background (bool): Whether otherwise to subtract pre-determined
-                background signals if available. Defaults to (not logplot)
-            unit (str): defaults to "A" or "mol/s"
-            x_unit (str): defaults to "s"
-            logplot (bool): Whether to plot the MS data on a log scale (default True)
-            logdata (bool): Whether to plot the natural logarithm of MS data on a
-                linear scale (default False)
-            legend (bool): Whether to use a legend for the MS data (default True)
-            xspan (iter of float): The physical span for spectra to plot
-            cmap_name (str): Colour map to pass to heat_plot method
-            make_colorbar (bool): Include a colour bar. Misalignes time axis with other
-                panels in same figure
-            emphasis (str): Whether to emphasise top or bottom panel 3/5 fig size or eq.
-            ms_data (str): Whether to plot ms_data on "top" or "bottom" panel
-            max_threshold (int or float): Only applies to spectra plotted with heat_plot.
-                All values above max threshold is set to 0 (zero).
-            min_threshold (int or float): Only applies to spectra plotted with heat_plot.
-                All values below threshold is set to 0 (zero).
-            scanning_mask (boolean list): Only applies to spectra plotted with heat_plot.
-                List of booleans of same shape as SpectrumSeries.data to exclude specific
-                data prior to plotting
-            vmin (int or float): Shift minimum value in color bar. Default lowest value
-                in measurement.spectrum_series.
-            vmax (int or float): Shift maximum value in color bar. Default highest value
-                in measurement.spectrum_series.
-            kwargs: extra key-word args are passed on to matplotlib's plot()
-        """
-
-        if logplot is None:
-            logplot = not mol_lists and not mass_lists
-
-        if not axes:
-            if ms_data == "top":
-                n_bottom = 1
-                n_top = 2 if (mass_lists or mol_lists) else 1
-                ms_axes = 0
-                ms_spec_axes = 1
-            else:
-                n_top = 1
-                n_bottom = 2 if (mass_lists or mol_lists) else 1
-                ms_axes = 1
-                ms_spec_axes = 0
-            axes = self.new_two_panel_axes(
-                n_bottom=n_bottom,
-                n_top=n_top,
-                emphasis=emphasis,
-            )
-
-        measurement = measurement or self.measurement
-
-        if (
-            mass_list
-            or mass_lists
-            or mol_list
-            or mol_lists
-            or hasattr(measurement, "mass_list")
-        ):
-            # then we have MS data!
-            self.ms_plotter.plot_measurement(
-                measurement=measurement,
-                axes=[axes[ms_axes], axes[2]]
-                if (mass_lists or mol_lists)
-                else [axes[ms_axes]],
-                tspan=tspan,
-                tspan_bg=tspan_bg,
-                remove_background=remove_background,
-                mass_list=mass_list,
-                mass_lists=mass_lists,
-                mol_list=mol_list,
-                mol_lists=mol_lists,
-                unit=unit,
-                x_unit=x_unit,
-                logplot=logplot,
-                logdata=logdata,
-                legend=legend,
-                **kwargs,
-            )
-
-        # then we have the spectrum series to plot
-        ax_heat_plot = measurement.spectrum_series.heat_plot(
-            ax=axes[ms_spec_axes],
-            tspan=tspan,
-            xspan=xspan,
-            cmap_name=cmap_name,
-            make_colorbar=make_colorbar,
-            max_threshold=max_threshold,
-            min_threshold=min_threshold,
-            scanning_mask=scanning_mask,
-            vmin=vmin,
-            vmax=vmax,
-        )
-
-        # Get the time variables aligned!
-        ax_heat_plot.set_xlim(axes[ms_axes].get_xlim())
-
-        return axes
-
-    def plot_measurement_vs(
-        self,
-        *,
-        vs_name,
-        measurement=None,
-        axes=None,
-        mass_list=None,
-        mass_lists=None,
-        mol_list=None,
-        mol_lists=None,
-        vspan=None,
-        tspan=None,
-        tspan_bg=None,
-        remove_background=None,
-        unit=None,
-        vs_unit=None,
-        logplot=True,
-        logdata=False,
-        legend=True,
-        xspan=None,
-        cmap_name="inferno",
-        make_colorbar=False,
-        vmin=None,
-        vmax=None,
-        emphasis="top",
-        ms_data="top",
-        max_threshold=None,
-        min_threshold=None,
-        scanning_mask=None,
-        sort_spectra="linear",
-        **kwargs,
-    ):
-        """Plot m/z signal and MSSpectra data in a two panel subfigure vs a specified
-        variable and return the axes.
-
-        There are four ways to specify which (MID) signals to plot in panel.
-        Only specify one of these:
-            mass_list: Uncalibrated signals in [(u/n/p)A] on on axis
-            mass_lists: Uncalibrated signals in [(u/n/p)A] on two axes
-            mol_list: Calibrated signals in [(u/n/p)mol/s] on on axis
-            mol_lists: Calibrated signals in [(u/n/p)mol/s] on two axes
-
-        Two axes refers to separate left and right y-axes. Default is to use all
-        available masses as mass_list.
-
-
-        Args:
-            measurement (SpectroMSMeasurement): Defaults to the one that initiated the
-                plotter
-            vs_name (str): Name of the series to plot versus.
-            axes (list of matplotlib axis): Defaults to axes[0], axes[2] for left and
-                right axis for plotting masses and axes[1] for MSSpectra data.
-            mass_list (list of str): The names of the m/z values, eg. ["M2", ...] to
-                plot. Defaults to all of them (measurement.mass_list)
-            mass_lists (list of list of str): Alternately, two lists can be given for
-                masses in which case one list is plotted on the left y-axis and the other
-                on the right y-axis of the top panel.
-            mol_list (list of str): The names of the molecules, eg. ["H2", ...] to
-                plot. Defaults to all of them (measurement.mass_list)
-            mol_lists (list of list of str): Alternately, two lists can be given for
-                molecules in which case one list is plotted on the left y-axis and the
-                other on the right y-axis of the top panel.
-            vspan (iter of float): The value interval to plot on x_axis
-            tspan (iter of float): The time interval to plot, wrt measurement.tstamp
-            tspan_bg (timespan): A timespan for which to assume the signal is at its
-                background. The average signals during this timespan are subtracted.
-                If `mass_lists` are given rather than a single `mass_list`, `tspan_bg`
-                must also be two timespans - one for each axis. Default is `None` for no
-                background subtraction.
-            remove_background (bool): Whether otherwise to subtract pre-determined
-                background signals if available. Defaults to (not logplot)
-            unit (str): defaults to "A" or "mol/s"
-            vs_unit (str): defaults to v_name.unit.name
-            logplot (bool): Whether to plot the MS data on a log scale (default True)
-            logdata (bool): Whether to plot the natural logarithm of MS data on a
-                linear scale (default False)
-            legend (bool): Whether to use a legend for the MS data (default True)
-            xspan (iter of float): The physical span for spectra to plot
-            cmap_name (str): Colour map to pass to heat_plot method
-            make_colorbar (bool): Include a colour bar. Misalignes time axis with other
-                panels in same figure
-            vmin (int or float): Shift minimum value in color bar. Default lowest value
-                in measurement.spectrum_series.
-            vmax (int or float): Shift maximum value in color bar. Default highest value
-                in measurement.spectrum_series.
-            emphasis (str): Whether to emphasise top or bottom panel 3/5 fig size or eq.
-                Default 'top'
-            ms_data (str): Whether to plot ms_data on "top" or "bottom" panel.
-                Default 'top'
-            max_threshold (int or float): Only applies to spectra plotted with heat_plot.
-                All values above max threshold is set to 0 (zero).
-            min_threshold (int or float): Only applies to spectra plotted with heat_plot.
-                All values below threshold is set to 0 (zero).
-            scanning_mask (boolean list): Only applies to spectra plotted with heat_plot.
-                List of booleans of same shape as SpectrumSeries.data to exclude specific
-                data prior to plotting
-            sort_spectra (list or str): Whether or not to sort the spectra data prior to
-                plotting.
-                There is three specifers:
-                    'none':
-                        This gives no new sorting. Effectively the spectras are sorted
-                        by time. (tstamp for each of the spectrum).
-                    'linear' (default):
-                        the spectras are sorted linear to v_name from low to high.
-                    a list of same shape as field.data to be sorted:
-                        This list is passed directly as the indices to sort the spectras.
-                        Defaults to sort lowest to highest value. Example
-                Note: If tspan spans a time span of a measurement with up and down
-                cycles in v_name, this might yield funny looking heat_plots.
-
-                    Example:
-                    Scanning up and down in temperature from T_low to T_high the spectras
-                    obtained wil be plotted from [T_low_start ..  T_high .. T_low_end].
-
-                    - If 'none' sorting is specified leads to heat_plot xaxis linearly
-                    from T_low_start to T_low_end missing representation of the high
-                    values in the middle of the axis.
-
-                    - If 'linear' sorting is specified all spectras obtained
-                        are sorted linearly from lowest v_name_value to highest v_name.
-                        When data is assymetric from scanning up or down in v_name this
-                        leads to abrupt looking figures since two non similair spectras
-                        are obtained at similair v_name value and hence plotted next to
-                        eachother.
-
-            kwargs: extra key-word args are passed on to matplotlib's plot()
-        """
-
-        if logplot is None:
-            logplot = not mol_lists and not mass_lists
-
-        if not axes:
-            if ms_data == "top":
-                n_bottom = 1
-                n_top = 2 if (mass_lists or mol_lists) else 1
-                ms_axes = 0
-                ms_spec_axes = 1
-            else:
-                n_top = 1
-                n_bottom = 2 if (mass_lists or mol_lists) else 1
-                ms_axes = 1
-                ms_spec_axes = 0
-
-            axes = self.new_two_panel_axes(
-                n_bottom=n_bottom,
-                n_top=n_top,
-                emphasis=emphasis,
-            )
-
-        measurement = measurement or self.measurement
-
-        if (
-            mass_list
-            or mass_lists
-            or mol_list
-            or mol_lists
-            or hasattr(measurement, "mass_list")
-        ):
-            # define where one or two axes is plotted for MS data!
-            ms_plot_axes = (
-                [axes[ms_axes], axes[2]]
-                if (mass_lists or mol_lists)
-                else [axes[ms_axes]]
-            )
-
-            # then we have MS data!
-            self.ms_plotter.plot_vs(
-                x_name=vs_name,
-                measurement=measurement,
-                axes=ms_plot_axes,
-                tspan=tspan,
-                tspan_bg=tspan_bg,
-                remove_background=remove_background,
-                mass_list=mass_list,
-                mass_lists=mass_lists,
-                mol_list=mol_list,
-                mol_lists=mol_lists,
-                unit=unit,
-                x_unit=vs_unit,
-                logplot=logplot,
-                logdata=logdata,
-                legend=legend,
-                **kwargs,
-            )
-
-        # To plot heat plot.
-        # First get all values for v_name at all spectrum times
-        field = measurement.spectrum_series.field
-        _data = field.data.copy()
-
-        _t = field.axes_series[0].t
-        _v = measurement.grab_for_t(item=vs_name, t=_t)
-
-        if tspan:
-            # create t_mask from tspan
-            t_mask = np.logical_and(tspan[0] < _t, _t < tspan[-1])
-            # apply t_mask to field.data and vs_name.data
-            _data = _data[t_mask]
-            _v = _v[t_mask]
-
-        if isinstance(sort_spectra, str):
-            if sort_spectra == "linear":
-                sorted_indicies = np.argsort(_v)
-
-            elif sort_spectra == "none":
-                sorted_indicies = np.array([])
-
-            else:
-                warnings.warn(
-                    f"Recieved {sort_spectra} for sort_spectra."
-                    "sort_spectra has to be 'linear',  'none' or "
-                    "of type list with same length as spectrum_series.",
-                    stacklevel=2,
-                )
-
-        elif isinstance(sort_spectra, list):
-            if len(_t) == len(sort_spectra):
-                sorted_indicies = sort_spectra
-            else:
-                warnings.warn(
-                    f"length [{len(sort_spectra)}] of 'sort_spectra' has to be equal"
-                    f"to [{len(_t)}]."
-                    "sort_spectra can be 'linear', 'none' or "
-                    "of type list with same length as spectrum_series.",
-                    stacklevel=2,
-                )
-        else:
-            warnings.warn(
-                "sort_spectra has to be 'linear', 'none' or "
-                "of type list with same length as spectrum_series."
-                "Recived {sort_spectra}.",
-                stacklevel=2,
-            )
-        # sort field data and x_axis equal
-        new_field_data = _data[sorted_indicies, :] if len(sorted_indicies) > 0 else _data
-        new_x_axis = _v[sorted_indicies] if len(sorted_indicies) > 0 else _v
-
-        new_field = Field(
-            name=field.name + f"_sorted_vs_{vs_name}_for_{tspan}",
-            unit_name=field.unit_name,
-            axes_series=field.axes_series,
-            data=new_field_data,
-        )
-        # Now we can plot the heat plot
-        measurement.spectrum_series.heat_plot(
-            ax=axes[ms_spec_axes],
-            t=new_x_axis,
-            field=new_field,
-            t_name=vs_name,
-            tspan=vspan,
-            xspan=xspan,
-            cmap_name=cmap_name,
-            make_colorbar=make_colorbar,
-            vmin=vmin,
-            vmax=vmax,
-            max_threshold=max_threshold,
-            min_threshold=min_threshold,
-            scanning_mask=scanning_mask,
-        )
-
-        axes[ms_spec_axes].set_xlim(axes[ms_axes].get_xlim())
-
-        if vspan:
-
-            axes[ms_axes].set_xlim([vspan[0], vspan[-1]])
-            axes[ms_spec_axes].set_xlim([vspan[0], vspan[-1]])
-
-        return axes
-
-
-#  ----- These are the standard colors for EC-MS plots! ------- #
-
-MIN_SIGNAL = 1e-14  # So that the bottom half of the plot isn't wasted on log(noise)
-# TODO: This should probably be customizeable from a settings file.
-
-STANDARD_COLORS = {
-    "M2": "b",
-    "M4": "m",
-    "M18": "y",
-    "M28": "0.5",
-    "M32": "k",
-    "M40": "c",
-    "M44": "brown",
-    "M15": "r",
-    "M26": "g",
-    "M27": "limegreen",
-    "M30": "darkorange",
-    "M31": "yellowgreen",
-    "M43": "tan",
-    "M45": "darkgreen",
-    "M34": "r",
-    "M36": "g",
-    "M46": "purple",
-    "M48": "darkslategray",
-    "M20": "slateblue",
-    "M16": "steelblue",
-    "M19": "teal",
-    "M17": "chocolate",
-    "M41": "#FF2E2E",
-    "M42": "olive",
-    "M29": "#001146",
-    "M70": "purple",
-    "M3": "orange",
-    "M73": "crimson",
-    "M74": "r",
-    "M60": "g",
-    "M58": "darkcyan",
-    "M88": "darkred",
-    "M89": "darkmagenta",
-    "M130": "purple",
-    "M132": "purple",
-    # and now, molecules:
-    "H2": "b",
-    "He": "m",
-    "H2O": "y",
-    "CO": "0.5",
-    "N2": "#8f8fffff",  # light blue-ish-purple
-    "O2": "k",
-    "Ar": "c",
-    "CO2": "brown",
-    "CH4": "r",
-    "C2H4": "g",
-    "NH3": "steelblue",
-    "O2@M32": "k",
-    "O2@M34": "r",
-    "O2@M36": "g",
-    "CO2@M44": "brown",
-    "CO2@M46": "purple",
-    "CO2@M48": "darkslategray",
-    # FIXME: Upgrade to include user defined colours from file or other module
-    # https://github.com/ixdat/ixdat/pull/101/files#r1088739480
-    # Inset of meta channels #
-    "TC temperature": "#808000",
-    "RTD temperature": "#808000",
-    "Reactor pressure": "#808000",
-    "Baratron pressure": "#808000",
-    "Containment pressure": "#808000",
-    "Flow1": "k",
-    "Flow2": "brown",
-    "Flow3": "c",
-    "Flow4": "b",
-    "Flow5": "r",
-    "Flow6": "0.5",
-    # Inset for anodic bonding #
-    "TC anodic bonding (top)": "#000075",  # "#808000",
-    "TC anodic bonding (bottom)": "#4363d8",  # "#9A6324",
-    "Total power": "#800000",
-    "Heater voltage 1": "#fabed4",
-    "Heater current 1": "#ffd8b1",
-}
+"""Plotter for Mass Spectrometry"""
+import warnings
+from ..data_series import Field
+import numpy as np
+from .base_mpl_plotter import MPLPlotter
+
+
+class MSPlotter(MPLPlotter):
+    """A matplotlib plotter specialized in mass spectrometry MID measurements."""
+
+    def __init__(self, measurement=None):
+        """Initiate the ECMSPlotter with its default Meausurement to plot"""
+        super().__init__()
+        self.measurement = measurement
+
+    def plot_measurement(
+        self,
+        *,
+        measurement=None,
+        ax=None,
+        axes=None,
+        mass_list=None,
+        mass_lists=None,
+        mol_list=None,
+        mol_lists=None,
+        tspan=None,
+        tspan_bg=None,
+        remove_background=None,
+        unit=None,
+        x_unit=None,
+        logplot=True,
+        logdata=False,
+        legend=True,
+        **kwargs,
+    ):
+        """Plot m/z signal vs time (MID) data and return the axis.
+
+        There are four ways to specify what to plot. Only specify one of these::
+            mass_list: Uncalibrated signals in [(u/n/p)A] on on axis
+            mass_lists: Uncalibrated signals in [(u/n/p)A] on two axes
+            mol_list: Calibrated signals in [(u/n/p)mol/s] on on axis
+            mol_lists: Calibrated signals in [(u/n/p)mol/s] on two axes
+
+        Two axes refers to separate left and right y-axes. Default is to use all
+        available masses as mass_list.
+
+        Args:
+            measurement (MSMeasurement): Defaults to the one that initiated the plotter
+            ax (matplotlib axis): Defaults to a new axis
+            axes (list of matplotlib axis): Left and right y-axes if mass_lists are given
+            mass_list (list of str): The names of the m/z values, eg. ["M2", ...] to
+                plot. Defaults to all of them (measurement.mass_list)
+            mass_lists (list of list of str): Alternately, two lists can be given for
+                masses in which case one list is plotted on the left y-axis and the other
+                on the right y-axis of the top panel.
+            mol_list (list of str): The names of the molecules, eg. ["H2", ...] to
+                plot. Defaults to all of them if quantified (measurement.mass_list)
+            mol_lists (list of list of str): Alternately, two lists can be given for
+                molecules in which case one list is plotted on the left y-axis and the
+                other on the right y-axis of the top panel.
+            tspan (iter of float): The time interval to plot, wrt measurement.tstamp
+            tspan_bg (timespan): A timespan for which to assume the signal is at its
+                background. The average signals during this timespan are subtracted.
+                If `mass_lists` are given rather than a single `mass_list`, `tspan_bg`
+                must also be two timespans - one for each axis. Default is `None` for no
+                background subtraction.
+            remove_background (bool): Whether otherwise to subtract pre-determined
+                background signals if available. Defaults to (not logplot)
+            unit (str): unit of the y axis. defaults to "A" or "mol/s"
+            x_unit (str): unit of the x axis variable (usually time). defaults to "s"
+            logplot (bool): Whether to plot the MS data on a log scale (default True)
+            logdata (bool): Whether to plot the natural logarithm of MS data on a
+                linear scale (default False)
+            legend (bool): Whether to use a legend for the MS data (default True)
+            kwargs: extra key-word args are passed on to matplotlib's plot()
+        """
+        measurement = measurement or self.measurement
+        if remove_background is None:
+            remove_background = not logplot
+
+        # Figure out, based on the inputs, whether or not to plot calibrated results
+        # (`quantified`), specifications for the axis to plot on now (`specs_this_axis`)
+        # and specifications for the next axis to plot on, if any (`specs_next_axis`):
+        quantified, specs_this_axis, specs_next_axis = self._parse_overloaded_inputs(
+            mass_list,
+            mass_lists,
+            mol_list,
+            mol_lists,
+            unit,
+            tspan_bg,
+            ax,
+            axes,
+            measurement,
+        )
+        ax = specs_this_axis["ax"]
+        v_list = specs_this_axis["v_list"]
+        tspan_bg = specs_this_axis["tspan_bg"]
+        unit = specs_this_axis["unit"]
+        unit_factor = specs_this_axis["unit_factor"]
+        for v_or_v_name in v_list:
+            if isinstance(v_or_v_name, str):
+                v_name = v_or_v_name
+                color = STANDARD_COLORS.get(v_name, "k")
+            else:
+                v_name = v_or_v_name.name
+                color = v_or_v_name.color
+            if quantified:
+                t, v = measurement.grab_flux(
+                    v_or_v_name,
+                    tspan=tspan,
+                    tspan_bg=tspan_bg,
+                    remove_background=remove_background,
+                    include_endpoints=False,
+                )
+            else:
+                t, v = measurement.grab_signal(
+                    v_or_v_name,
+                    tspan=tspan,
+                    tspan_bg=tspan_bg,
+                    remove_background=remove_background,
+                    include_endpoints=False,
+                )
+            if logplot:
+                v[v < MIN_SIGNAL] = MIN_SIGNAL
+            if logdata:
+                logplot = False
+                # to correctly plot data with corrosponding unit and unit_factor
+                v = np.log(v * unit_factor) * 1 / unit_factor
+                unit = f"ln({unit})"
+            # expect always to plot against time
+            x_unit_factor, x_unit = self._get_x_unit_factor(x_unit, "s")
+            ax.plot(
+                t * x_unit_factor,
+                v * unit_factor,
+                color=color,
+                label=v_name,
+                **kwargs,
+            )
+        ax.set_ylabel(f"signal / [{unit}]")
+        ax.set_xlabel(f"time / [{x_unit}]")
+        if specs_next_axis:
+            self.plot_measurement(
+                measurement=measurement,
+                ax=specs_next_axis["ax"],
+                mass_list=specs_next_axis["mass_list"],
+                mol_list=specs_next_axis["mol_list"],
+                unit=specs_next_axis["unit"],
+                x_unit=x_unit,
+                tspan=tspan,
+                tspan_bg=specs_next_axis["tspan_bg"],
+                logplot=logplot,
+                logdata=logdata,
+                legend=legend,
+                **kwargs,
+            )
+            axes = [ax, specs_next_axis["ax"]]
+        else:
+            axes = None
+
+        if logplot:
+            ax.set_yscale("log")
+        if legend:
+            ax.legend()
+
+        return axes if axes else ax
+
+    def plot_vs(
+        self,
+        *,
+        x_name,
+        measurement=None,
+        ax=None,
+        axes=None,
+        mass_list=None,
+        mass_lists=None,
+        mol_list=None,
+        mol_lists=None,
+        tspan=None,
+        tspan_bg=None,
+        remove_background=None,
+        unit=None,
+        x_unit=None,
+        logplot=True,
+        logdata=False,
+        legend=True,
+        **plot_kwargs,
+    ):
+        """Plot m/z signal (MID) data against a specified variable and return the axis.
+
+        There are four ways to specify what to plot. Only specify one of these::
+            mass_list: Uncalibrated signals in [(u/n/p)A] on on axis
+            mass_lists: Uncalibrated signals in [(u/n/p)A] on two axes
+            mol_list: Calibrated signals in [(u/n/p)mol/s] on on axis
+            mol_lists: Calibrated signals in [(u/n/p)mol/s] on two axes
+
+        Two axes refers to seperate left and right y-axes. Default is to use all
+        available masses as mass_list.
+
+        Args:
+            x_name (str): Name of the variable to plot on the x-axis
+            measurement (MSMeasurement): Defaults to the one that initiated the plotter
+            ax (matplotlib axis): Defaults to a new axis
+            axes (list of matplotlib axis): Left and right y-axes if mass_lists are given
+            mass_list (list of str): The names of the m/z values, eg. ["M2", ...] to
+                plot. Defaults to all of them (measurement.mass_list)
+            mass_lists (list of list of str): Alternately, two lists can be given for
+                masses in which case one list is plotted on the left y-axis and the other
+                on the right y-axis of the top panel.
+            mol_list (list of str): The names of the molecules, eg. ["H2", ...] to
+                plot. Defaults to all of them (measurement.mass_list)
+            mol_lists (list of list of str): Alternately, two lists can be given for
+                molecules in which case one list is plotted on the left y-axis and the
+                other on the right y-axis of the top panel.
+            tspan (iter of float): The time interval to plot, wrt measurement.tstamp
+            tspan_bg (timespan): A timespan for which to assume the signal is at its
+                background. The average signals during this timespan are subtracted.
+                If `mass_lists` are given rather than a single `mass_list`, `tspan_bg`
+                must also be two timespans - one for each axis. Default is `None` for no
+                background subtraction.
+            remove_background (bool): Whether otherwise to subtract pre-determined
+                background signals if available
+            unit (str): defaults to "A" or "mol/s"
+            x_unit (str): defaults to x_name.unit.name
+            logplot (bool): Whether to plot the MS data on a log scale (default True)
+            logdata (bool): Whether to plot the natural logarithm of MS data on a
+                linear scale (default False)
+            legend (bool): Whether to use a legend for the MS data (default True)
+            plot_kwargs: additional key-word args are passed on to matplotlib's plot()
+        """
+        measurement = measurement or self.measurement
+        if remove_background is None:
+            remove_background = not logplot
+
+        # The overloaded inputs are a pain in the ass. This function helps:
+        quantified, specs_this_axis, specs_next_axis = self._parse_overloaded_inputs(
+            mass_list,
+            mass_lists,
+            mol_list,
+            mol_lists,
+            unit,
+            tspan_bg,
+            ax,
+            axes,
+            measurement,
+        )
+        ax = specs_this_axis["ax"]
+        v_list = specs_this_axis["v_list"]
+        tspan_bg = specs_this_axis["tspan_bg"]
+        unit = specs_this_axis["unit"]
+        unit_factor = specs_this_axis["unit_factor"]
+
+        t, x = measurement.grab(x_name, tspan=tspan, include_endpoints=True)
+        for i, v_name in enumerate(v_list):
+            if quantified:
+                t_v, v = measurement.grab_flux(
+                    v_name,
+                    tspan=tspan,
+                    tspan_bg=tspan_bg,
+                    remove_background=remove_background,
+                    include_endpoints=False,
+                )
+            else:
+                t_v, v = measurement.grab_signal(
+                    v_name,
+                    tspan=tspan,
+                    tspan_bg=tspan_bg,
+                    remove_background=remove_background,
+                    include_endpoints=False,
+                )
+            if logplot:
+                v[v < MIN_SIGNAL] = MIN_SIGNAL
+            x_mass = np.interp(t_v, t, x)
+            plot_kwargs_this_mass = plot_kwargs.copy()
+            if "color" not in plot_kwargs:
+                plot_kwargs_this_mass["color"] = STANDARD_COLORS.get(v_name, "k")
+            if "label" not in plot_kwargs:
+                plot_kwargs_this_mass["label"] = v_name
+
+            x_unit_factor, x_unit = self._get_x_unit_factor(
+                x_unit, measurement[x_name].unit.name
+            )
+            # Used to plot ln(mol) on a linear scale
+            if logdata:
+                logplot = False
+                v = np.log(v * unit_factor) * 1 / unit_factor
+                if not i:  # To avoid looping ln()'s around unit for each mass plotted.
+                    unit = f"ln({unit})"
+
+            ax.plot(
+                x_mass * x_unit_factor,
+                v * unit_factor,
+                **plot_kwargs_this_mass,
+            )
+
+        ax.set_ylabel(f"signal / [{unit}]")
+
+        ax.set_xlabel(f"{x_name} / [{x_unit}]")
+
+        if specs_next_axis:
+            self.plot_vs(
+                x_name=x_name,
+                measurement=measurement,
+                ax=specs_next_axis["ax"],
+                mass_list=specs_next_axis["mass_list"],
+                mol_list=specs_next_axis["mol_list"],
+                unit=specs_next_axis["unit"],
+                x_unit=x_unit,
+                tspan=tspan,
+                tspan_bg=specs_next_axis["tspan_bg"],
+                logplot=logplot,
+                legend=legend,
+                logdata=logdata,
+                **plot_kwargs,
+            )
+            axes = [ax, specs_next_axis["ax"]]
+        else:
+            axes = None
+
+        if logplot:
+            ax.set_yscale("log")
+        if legend:
+            ax.legend()
+
+        return axes if axes else ax
+
+    def _get_x_unit_factor(
+        self,
+        new_x_unit,
+        original_unit_name,
+    ):
+        # FIXME: This function will disappear with pint units (#146)
+        if not new_x_unit:
+            return 1, original_unit_name
+        if (original_unit_name or new_x_unit) in ["kelvin", "K", "celsius", "C"]:
+            warnings.warn(
+                f"Converting '{original_unit_name}' to '{new_x_unit}' should be done in "
+                f"({self.measurement.__class__.__name__}) prior to plotting using "
+                f"({self.__class__.__name__}). "
+                f"Plotting using '{original_unit_name}'.",
+                stacklevel=2,
+            )
+            return 1, original_unit_name
+        try:
+            x_unit_factor = {
+                # Time conversion
+                "s": 1,
+                "min": 1 / 60,
+                "minutes": 1 / 60,
+                "h": 1 / 3600,
+                "hr": 1 / 3600,
+                "hour": 1 / 3600,
+                "hours": 1 / 3600,
+                "d": 1 / (3600 * 24),
+                "days": 1 / (3600 * 24),
+                # Pressure conversion
+                "mbar": 1,
+                "bar": 1000,
+                "hPa": 1,
+                "kPa": 0.1,
+                # Temperature conversion
+                # "K": 273.15,
+                # "kelvin": 273.15,
+                # "C": -273.15,
+                # "celsius": -273.15,
+            }[new_x_unit]
+        except KeyError:
+            warnings.warn(
+                f"Can't convert original unit '{original_unit_name}' to new unit "
+                f"'{new_x_unit}'. Plotting using original unit '{new_x_unit}' with "
+                "unit_factor=1 (one).",
+                stacklevel=2,
+            )
+            x_unit_factor = 1
+            new_x_unit = original_unit_name
+        return x_unit_factor, new_x_unit
+
+    def _parse_overloaded_inputs(
+        self,
+        mass_list,
+        mass_lists,
+        mol_list,
+        mol_lists,
+        unit,
+        tspan_bg,
+        ax,
+        axes,
+        measurement,
+    ):
+        """From the overloaded function inputs, figure out what the user wants to do.
+
+        This includes:
+        1. determine if we're doing quantifed results (mols) or raw (masses)
+        2. figure out if there's one or two axes (remaining) and what goes on them.
+        3. figure out what to multiply numbers by when plotting to match the unit.
+        """
+        # TODO: Maybe there's a way to do this function as a decorator?
+        # So this function is overloaded in the sense that the user can give
+        #   exactly one of mol_list, mol_lists, mass_list, mass_lists.
+        # To manage that complexity, first we reduce it to two options, that down to
+        #   either v_list or v_lists and a boolean "quantified":
+        quantified = False  # default, if they give nothing
+        v_lists = None  # default, if they give nothing
+        v_list = measurement.mass_list  # default, if they give nothing
+        if mol_list:
+            quantified = True
+            v_list = mol_list
+        elif mol_lists:
+            quantified = True
+            v_lists = mol_lists
+        elif mass_list:
+            quantified = False
+            v_list = mass_list
+        elif mass_lists:
+            quantified = False
+            v_lists = mass_lists
+
+        if not ax:
+            ax = (
+                axes[0]
+                if axes
+                else self.new_ax(ylabel=f"signal / [{unit}]", xlabel="time / [s]")
+            )
+        # as the next simplification, if they give two things (v_lists), we pretend we
+        #   got one (v_list) but prepare an axis for a recursive call of this function.
+        if v_lists:
+            axes = axes or [ax, ax.twinx()]  # prepare an axis unless we were given two.
+            ax_right = axes[-1]
+            ax = axes[0]
+            v_list = v_lists[0]
+            v_list_right = v_lists[1]
+            # ah, and to enable different background tspans for the two axes:
+            try:
+                tspan_bg_right = tspan_bg[1]
+                if isinstance(tspan_bg_right, (float, int)):
+                    raise TypeError
+            except (KeyError, TypeError):
+                tspan_bg_right = None
+            else:
+                tspan_bg = tspan_bg[0]
+            if isinstance(unit, str) or not unit:
+                unit_right = unit
+            else:
+                unit_right = unit[1]
+                unit = unit[0]
+            specs_next_axis = {
+                "ax": ax_right,
+                "unit": unit_right,
+                "mass_list": None if quantified else v_list_right,
+                "mol_list": v_list_right if quantified else None,
+                "tspan_bg": tspan_bg_right,
+            }
+        else:
+            specs_next_axis = None
+
+        if quantified:
+            unit = unit or "mol/s"
+            unit_factor = {
+                "pmol/s": 1e12,
+                "nmol/s": 1e9,
+                "umol/s": 1e6,
+                "mmol/s": 1e3,
+                "mol/s": 1,  # noqa
+                "pmol/s/cm^2": 1e12,
+                "nmol/s/cm^2": 1e9,
+                "umol/s/cm^2": 1e6,
+                "mmol/s/cm^2": 1e3,
+                "mol/s/cm^2": 1,  # noqa
+            }[unit]
+            if "/cm^2" in unit:
+                unit_factor = unit_factor / measurement.A_el
+        else:
+            unit = unit or "A"
+            unit_factor = {"pA": 1e12, "nA": 1e9, "uA": 1e6, "mA": 1e3, "A": 1}[unit]
+        # TODO: Real units with a unit module! This should even be able to figure out the
+        #  unit prefix to put stuff in a nice 1-to-1e3 range
+
+        specs_this_axis = {
+            "ax": ax,
+            "v_list": v_list,
+            "unit": unit,
+            "unit_factor": unit_factor,
+            "tspan_bg": tspan_bg,
+        }
+
+        return quantified, specs_this_axis, specs_next_axis
+
+
+class MSSpectroPlotter(MPLPlotter):
+    """A matplotlib plotter specialized in mass spectrometry MID measurements."""
+
+    def __init__(self, measurement=None):
+        """Initiate the SpectroMSPlotter with its default Meausurement to plot"""
+        super().__init__()
+        self.measurement = measurement
+        self.ms_plotter = MSPlotter(measurement=measurement)
+
+    def plot_measurement(
+        self,
+        *,
+        measurement=None,
+        axes=None,
+        mass_list=None,
+        mass_lists=None,
+        mol_list=None,
+        mol_lists=None,
+        tspan=None,
+        tspan_bg=None,
+        remove_background=None,
+        unit=None,
+        x_unit=None,
+        logplot=True,
+        logdata=False,
+        legend=True,
+        xspan=None,
+        cmap_name="inferno",
+        make_colorbar=False,
+        emphasis="top",
+        ms_data="top",
+        max_threshold=None,
+        min_threshold=None,
+        scanning_mask=None,
+        vmin=None,
+        vmax=None,
+        **kwargs,
+    ):
+        """Plot m/z signal, mass spectra vs time (MID) data and return the axes of a two
+        panel figure.
+
+        There are four ways to specify what to plot. Only specify one of these::
+            mass_list: Uncalibrated signals in [(u/n/p)A] on on axis
+            mass_lists: Uncalibrated signals in [(u/n/p)A] on two axes
+            mol_list: Calibrated signals in [(u/n/p)mol/s] on on axis
+            mol_lists: Calibrated signals in [(u/n/p)mol/s] on two axes
+
+        Two axes refers to separate left and right y-axes. Default is to use all
+        available masses as mass_list.
+
+        Args:
+            measurement (SpectroMSMeasurement): Defaults to the one that initiated the
+                plotter
+            axes (list of matplotlib axis): Left and right y-axes if mass_lists are given
+                default to axes[0] as left and axes[2] as right axis for plotting masses
+                and axes[1] for plotting MSSpectra
+            mass_list (list of str): The names of the m/z values, eg. ["M2", ...] to
+                plot. Defaults to all of them (measurement.mass_list)
+            mass_lists (list of list of str): Alternately, two lists can be given for
+                masses in which case one list is plotted on the left y-axis and the other
+                on the right y-axis of the top panel.
+            mol_list (list of str): The names of the molecules, eg. ["H2", ...] to
+                plot. Defaults to all of them (measurement.mass_list)
+            mol_lists (list of list of str): Alternately, two lists can be given for
+                molecules in which case one list is plotted on the left y-axis and the
+                other on the right y-axis of the top panel.
+            tspan (iter of float): The time interval to plot, wrt measurement.tstamp
+            tspan_bg (timespan): A timespan for which to assume the signal is at its
+                background. The average signals during this timespan are subtracted.
+                If `mass_lists` are given rather than a single `mass_list`, `tspan_bg`
+                must also be two timespans - one for each axis. Default is `None` for no
+                background subtraction.
+            remove_background (bool): Whether otherwise to subtract pre-determined
+                background signals if available. Defaults to (not logplot)
+            unit (str): defaults to "A" or "mol/s"
+            x_unit (str): defaults to "s"
+            logplot (bool): Whether to plot the MS data on a log scale (default True)
+            logdata (bool): Whether to plot the natural logarithm of MS data on a
+                linear scale (default False)
+            legend (bool): Whether to use a legend for the MS data (default True)
+            xspan (iter of float): The physical span for spectra to plot
+            cmap_name (str): Colour map to pass to heat_plot method
+            make_colorbar (bool): Include a colour bar. Misalignes time axis with other
+                panels in same figure
+            emphasis (str): Whether to emphasise top or bottom panel 3/5 fig size or eq.
+            ms_data (str): Whether to plot ms_data on "top" or "bottom" panel
+            max_threshold (int or float): Only applies to spectra plotted with heat_plot.
+                All values above max threshold is set to 0 (zero).
+            min_threshold (int or float): Only applies to spectra plotted with heat_plot.
+                All values below threshold is set to 0 (zero).
+            scanning_mask (boolean list): Only applies to spectra plotted with heat_plot.
+                List of booleans of same shape as SpectrumSeries.data to exclude specific
+                data prior to plotting
+            vmin (int or float): Shift minimum value in color bar. Default lowest value
+                in measurement.spectrum_series.
+            vmax (int or float): Shift maximum value in color bar. Default highest value
+                in measurement.spectrum_series.
+            kwargs: extra key-word args are passed on to matplotlib's plot()
+        """
+
+        if logplot is None:
+            logplot = not mol_lists and not mass_lists
+
+        if not axes:
+            if ms_data == "top":
+                n_bottom = 1
+                n_top = 2 if (mass_lists or mol_lists) else 1
+                ms_axes = 0
+                ms_spec_axes = 1
+            else:
+                n_top = 1
+                n_bottom = 2 if (mass_lists or mol_lists) else 1
+                ms_axes = 1
+                ms_spec_axes = 0
+            axes = self.new_two_panel_axes(
+                n_bottom=n_bottom,
+                n_top=n_top,
+                emphasis=emphasis,
+            )
+
+        measurement = measurement or self.measurement
+
+        if (
+            mass_list
+            or mass_lists
+            or mol_list
+            or mol_lists
+            or hasattr(measurement, "mass_list")
+        ):
+            # then we have MS data!
+            self.ms_plotter.plot_measurement(
+                measurement=measurement,
+                axes=[axes[ms_axes], axes[2]]
+                if (mass_lists or mol_lists)
+                else [axes[ms_axes]],
+                tspan=tspan,
+                tspan_bg=tspan_bg,
+                remove_background=remove_background,
+                mass_list=mass_list,
+                mass_lists=mass_lists,
+                mol_list=mol_list,
+                mol_lists=mol_lists,
+                unit=unit,
+                x_unit=x_unit,
+                logplot=logplot,
+                logdata=logdata,
+                legend=legend,
+                **kwargs,
+            )
+
+        # then we have the spectrum series to plot
+        ax_heat_plot = measurement.spectrum_series.heat_plot(
+            ax=axes[ms_spec_axes],
+            tspan=tspan,
+            xspan=xspan,
+            cmap_name=cmap_name,
+            make_colorbar=make_colorbar,
+            max_threshold=max_threshold,
+            min_threshold=min_threshold,
+            scanning_mask=scanning_mask,
+            vmin=vmin,
+            vmax=vmax,
+        )
+
+        # Get the time variables aligned!
+        ax_heat_plot.set_xlim(axes[ms_axes].get_xlim())
+
+        return axes
+
+    def plot_measurement_vs(
+        self,
+        *,
+        vs_name,
+        measurement=None,
+        axes=None,
+        mass_list=None,
+        mass_lists=None,
+        mol_list=None,
+        mol_lists=None,
+        vspan=None,
+        tspan=None,
+        tspan_bg=None,
+        remove_background=None,
+        unit=None,
+        vs_unit=None,
+        logplot=True,
+        logdata=False,
+        legend=True,
+        xspan=None,
+        cmap_name="inferno",
+        make_colorbar=False,
+        vmin=None,
+        vmax=None,
+        emphasis="top",
+        ms_data="top",
+        max_threshold=None,
+        min_threshold=None,
+        scanning_mask=None,
+        sort_spectra="linear",
+        **kwargs,
+    ):
+        """Plot m/z signal and MSSpectra data in a two panel subfigure vs a specified
+        variable and return the axes.
+
+        There are four ways to specify which (MID) signals to plot in panel.
+        Only specify one of these:
+            mass_list: Uncalibrated signals in [(u/n/p)A] on on axis
+            mass_lists: Uncalibrated signals in [(u/n/p)A] on two axes
+            mol_list: Calibrated signals in [(u/n/p)mol/s] on on axis
+            mol_lists: Calibrated signals in [(u/n/p)mol/s] on two axes
+
+        Two axes refers to separate left and right y-axes. Default is to use all
+        available masses as mass_list.
+
+
+        Args:
+            measurement (SpectroMSMeasurement): Defaults to the one that initiated the
+                plotter
+            vs_name (str): Name of the series to plot versus.
+            axes (list of matplotlib axis): Defaults to axes[0], axes[2] for left and
+                right axis for plotting masses and axes[1] for MSSpectra data.
+            mass_list (list of str): The names of the m/z values, eg. ["M2", ...] to
+                plot. Defaults to all of them (measurement.mass_list)
+            mass_lists (list of list of str): Alternately, two lists can be given for
+                masses in which case one list is plotted on the left y-axis and the other
+                on the right y-axis of the top panel.
+            mol_list (list of str): The names of the molecules, eg. ["H2", ...] to
+                plot. Defaults to all of them (measurement.mass_list)
+            mol_lists (list of list of str): Alternately, two lists can be given for
+                molecules in which case one list is plotted on the left y-axis and the
+                other on the right y-axis of the top panel.
+            vspan (iter of float): The value interval to plot on x_axis
+            tspan (iter of float): The time interval to plot, wrt measurement.tstamp
+            tspan_bg (timespan): A timespan for which to assume the signal is at its
+                background. The average signals during this timespan are subtracted.
+                If `mass_lists` are given rather than a single `mass_list`, `tspan_bg`
+                must also be two timespans - one for each axis. Default is `None` for no
+                background subtraction.
+            remove_background (bool): Whether otherwise to subtract pre-determined
+                background signals if available. Defaults to (not logplot)
+            unit (str): defaults to "A" or "mol/s"
+            vs_unit (str): defaults to v_name.unit.name
+            logplot (bool): Whether to plot the MS data on a log scale (default True)
+            logdata (bool): Whether to plot the natural logarithm of MS data on a
+                linear scale (default False)
+            legend (bool): Whether to use a legend for the MS data (default True)
+            xspan (iter of float): The physical span for spectra to plot
+            cmap_name (str): Colour map to pass to heat_plot method
+            make_colorbar (bool): Include a colour bar. Misalignes time axis with other
+                panels in same figure
+            vmin (int or float): Shift minimum value in color bar. Default lowest value
+                in measurement.spectrum_series.
+            vmax (int or float): Shift maximum value in color bar. Default highest value
+                in measurement.spectrum_series.
+            emphasis (str): Whether to emphasise top or bottom panel 3/5 fig size or eq.
+                Default 'top'
+            ms_data (str): Whether to plot ms_data on "top" or "bottom" panel.
+                Default 'top'
+            max_threshold (int or float): Only applies to spectra plotted with heat_plot.
+                All values above max threshold is set to 0 (zero).
+            min_threshold (int or float): Only applies to spectra plotted with heat_plot.
+                All values below threshold is set to 0 (zero).
+            scanning_mask (boolean list): Only applies to spectra plotted with heat_plot.
+                List of booleans of same shape as SpectrumSeries.data to exclude specific
+                data prior to plotting
+            sort_spectra (list or str): Whether or not to sort the spectra data prior to
+                plotting.
+                There is three specifers:
+                    'none':
+                        This gives no new sorting. Effectively the spectras are sorted
+                        by time. (tstamp for each of the spectrum).
+                    'linear' (default):
+                        the spectras are sorted linear to v_name from low to high.
+                    a list of same shape as field.data to be sorted:
+                        This list is passed directly as the indices to sort the spectras.
+                        Defaults to sort lowest to highest value. Example
+                Note: If tspan spans a time span of a measurement with up and down
+                cycles in v_name, this might yield funny looking heat_plots.
+
+                    Example:
+                    Scanning up and down in temperature from T_low to T_high the spectras
+                    obtained wil be plotted from [T_low_start ..  T_high .. T_low_end].
+
+                    - If 'none' sorting is specified leads to heat_plot xaxis linearly
+                    from T_low_start to T_low_end missing representation of the high
+                    values in the middle of the axis.
+
+                    - If 'linear' sorting is specified all spectras obtained
+                        are sorted linearly from lowest v_name_value to highest v_name.
+                        When data is assymetric from scanning up or down in v_name this
+                        leads to abrupt looking figures since two non similair spectras
+                        are obtained at similair v_name value and hence plotted next to
+                        eachother.
+
+            kwargs: extra key-word args are passed on to matplotlib's plot()
+        """
+
+        if logplot is None:
+            logplot = not mol_lists and not mass_lists
+
+        if not axes:
+            if ms_data == "top":
+                n_bottom = 1
+                n_top = 2 if (mass_lists or mol_lists) else 1
+                ms_axes = 0
+                ms_spec_axes = 1
+            else:
+                n_top = 1
+                n_bottom = 2 if (mass_lists or mol_lists) else 1
+                ms_axes = 1
+                ms_spec_axes = 0
+
+            axes = self.new_two_panel_axes(
+                n_bottom=n_bottom,
+                n_top=n_top,
+                emphasis=emphasis,
+            )
+
+        measurement = measurement or self.measurement
+
+        if (
+            mass_list
+            or mass_lists
+            or mol_list
+            or mol_lists
+            or hasattr(measurement, "mass_list")
+        ):
+            # define where one or two axes is plotted for MS data!
+            ms_plot_axes = (
+                [axes[ms_axes], axes[2]]
+                if (mass_lists or mol_lists)
+                else [axes[ms_axes]]
+            )
+
+            # then we have MS data!
+            self.ms_plotter.plot_vs(
+                x_name=vs_name,
+                measurement=measurement,
+                axes=ms_plot_axes,
+                tspan=tspan,
+                tspan_bg=tspan_bg,
+                remove_background=remove_background,
+                mass_list=mass_list,
+                mass_lists=mass_lists,
+                mol_list=mol_list,
+                mol_lists=mol_lists,
+                unit=unit,
+                x_unit=vs_unit,
+                logplot=logplot,
+                logdata=logdata,
+                legend=legend,
+                **kwargs,
+            )
+
+        # To plot heat plot.
+        # First get all values for v_name at all spectrum times
+        field = measurement.spectrum_series.field
+        _data = field.data.copy()
+
+        _t = field.axes_series[0].t
+        _v = measurement.grab_for_t(item=vs_name, t=_t)
+
+        if tspan:
+            # create t_mask from tspan
+            t_mask = np.logical_and(tspan[0] < _t, _t < tspan[-1])
+            # apply t_mask to field.data and vs_name.data
+            _data = _data[t_mask]
+            _v = _v[t_mask]
+
+        if isinstance(sort_spectra, str):
+            if sort_spectra == "linear":
+                sorted_indicies = np.argsort(_v)
+
+            elif sort_spectra == "none":
+                sorted_indicies = np.array([])
+
+            else:
+                warnings.warn(
+                    f"Recieved {sort_spectra} for sort_spectra."
+                    "sort_spectra has to be 'linear',  'none' or "
+                    "of type list with same length as spectrum_series.",
+                    stacklevel=2,
+                )
+
+        elif isinstance(sort_spectra, list):
+            if len(_t) == len(sort_spectra):
+                sorted_indicies = sort_spectra
+            else:
+                warnings.warn(
+                    f"length [{len(sort_spectra)}] of 'sort_spectra' has to be equal"
+                    f"to [{len(_t)}]."
+                    "sort_spectra can be 'linear', 'none' or "
+                    "of type list with same length as spectrum_series.",
+                    stacklevel=2,
+                )
+        else:
+            warnings.warn(
+                "sort_spectra has to be 'linear', 'none' or "
+                "of type list with same length as spectrum_series."
+                "Recived {sort_spectra}.",
+                stacklevel=2,
+            )
+        # sort field data and x_axis equal
+        new_field_data = _data[sorted_indicies, :] if len(sorted_indicies) > 0 else _data
+        new_x_axis = _v[sorted_indicies] if len(sorted_indicies) > 0 else _v
+
+        new_field = Field(
+            name=field.name + f"_sorted_vs_{vs_name}_for_{tspan}",
+            unit_name=field.unit_name,
+            axes_series=field.axes_series,
+            data=new_field_data,
+        )
+        # Now we can plot the heat plot
+        measurement.spectrum_series.heat_plot(
+            ax=axes[ms_spec_axes],
+            t=new_x_axis,
+            field=new_field,
+            t_name=vs_name,
+            tspan=vspan,
+            xspan=xspan,
+            cmap_name=cmap_name,
+            make_colorbar=make_colorbar,
+            vmin=vmin,
+            vmax=vmax,
+            max_threshold=max_threshold,
+            min_threshold=min_threshold,
+            scanning_mask=scanning_mask,
+        )
+
+        axes[ms_spec_axes].set_xlim(axes[ms_axes].get_xlim())
+
+        if vspan:
+
+            axes[ms_axes].set_xlim([vspan[0], vspan[-1]])
+            axes[ms_spec_axes].set_xlim([vspan[0], vspan[-1]])
+
+        return axes
+
+
+#  ----- These are the standard colors for EC-MS plots! ------- #
+
+MIN_SIGNAL = 1e-14  # So that the bottom half of the plot isn't wasted on log(noise)
+# TODO: This should probably be customizeable from a settings file.
+
+STANDARD_COLORS = {
+    "M2": "b",
+    "M4": "m",
+    "M18": "y",
+    "M28": "0.5",
+    "M32": "k",
+    "M40": "c",
+    "M44": "brown",
+    "M15": "r",
+    "M26": "g",
+    "M27": "limegreen",
+    "M30": "darkorange",
+    "M31": "yellowgreen",
+    "M43": "tan",
+    "M45": "darkgreen",
+    "M34": "r",
+    "M36": "g",
+    "M46": "purple",
+    "M48": "darkslategray",
+    "M20": "slateblue",
+    "M16": "steelblue",
+    "M19": "teal",
+    "M17": "chocolate",
+    "M41": "#FF2E2E",
+    "M42": "olive",
+    "M29": "#001146",
+    "M70": "purple",
+    "M3": "orange",
+    "M73": "crimson",
+    "M74": "r",
+    "M60": "g",
+    "M58": "darkcyan",
+    "M88": "darkred",
+    "M89": "darkmagenta",
+    "M130": "purple",
+    "M132": "purple",
+    # and now, molecules:
+    "H2": "b",
+    "He": "m",
+    "H2O": "y",
+    "CO": "0.5",
+    "N2": "#8f8fffff",  # light blue-ish-purple
+    "O2": "k",
+    "Ar": "c",
+    "CO2": "brown",
+    "CH4": "r",
+    "C2H4": "g",
+    "NH3": "steelblue",
+    "O2@M32": "k",
+    "O2@M34": "r",
+    "O2@M36": "g",
+    "CO2@M44": "brown",
+    "CO2@M46": "purple",
+    "CO2@M48": "darkslategray",
+    # FIXME: Upgrade to include user defined colours from file or other module
+    # https://github.com/ixdat/ixdat/pull/101/files#r1088739480
+    # Inset of meta channels #
+    "TC temperature": "#808000",
+    "RTD temperature": "#808000",
+    "Reactor pressure": "#808000",
+    "Baratron pressure": "#808000",
+    "Containment pressure": "#808000",
+    "Flow1": "k",
+    "Flow2": "brown",
+    "Flow3": "c",
+    "Flow4": "b",
+    "Flow5": "r",
+    "Flow6": "0.5",
+    # Inset for anodic bonding #
+    "TC anodic bonding (top)": "#000075",  # "#808000",
+    "TC anodic bonding (bottom)": "#4363d8",  # "#9A6324",
+    "Total power": "#800000",
+    "Heater voltage 1": "#fabed4",
+    "Heater current 1": "#ffd8b1",
+}
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat/plotters/plotting_tools.py` & `ixdat-0.2.9.dev3/src/ixdat/plotters/plotting_tools.py`

 * *Ordering differences only*

 * *Files 25% similar despite different names*

```diff
@@ -1,96 +1,96 @@
-"""This module contains loose functions and stuff useful for ixdat plotting."""
-
-import numpy as np
-import matplotlib as mpl
-from matplotlib import pyplot as plt
-
-
-def color_axis(ax, color, lr="right", xy="y"):
-    """Color the spine, ticks, and labels of an axis.
-
-    ax (matplotlib.pyplot.axis): the axis to color (a part of)
-    color (str): The color to color the axis.
-    lr (str): whether to color the "left" spine or the "right". Defaults to "right".
-    xy (str): whether to color the "x" axis or the "y". Defaults to "y".
-    """
-    ax.spines[lr].set_color(color)
-    ax.tick_params(axis=xy, color=color, labelcolor=color)
-    if xy == "y":
-        ax.yaxis.label.set_color(color)
-    if xy == "x":
-        ax.xaxis.label.set_color(color)
-
-
-def get_color_from_cmap(x, cmap_name):
-    """Return the color as a 4-tuple, given a value btwn 0 and 1, and color map name
-
-    Args:
-        x (float): Value between 0 and 1 defining a location on a matplotlib colormap
-        cmap_name (str): The name of a matplotlib colormap. Popular ones include
-            "inferno" (0=black --> 1=yellow) and "jet" (0=blue --> 1=red)
-            See https://matplotlib.org/3.5.0/tutorials/colors/colormaps.html
-    """
-
-    cmap = mpl.cm.get_cmap(cmap_name)
-
-    rgba = cmap(x)
-    return rgba
-
-
-def add_colorbar(ax, cmap_name, vmin, vmax, label="intensity"):
-    """Add a colorbar to a matplotlib axis
-
-    ax (matplotlib Axis): The axis (with data plotted on it) to add a colorbar for
-    cmap_name (str): The name of the color map
-    vmin (float): The minimum value in the plotted data
-    vmax (float): The maximum value in the plotted data
-    label (str): A label for the color bar (the name of the value represented by color)
-    """
-
-    cmap = mpl.cm.get_cmap(cmap_name)
-    norm = mpl.colors.Normalize(vmin=vmin, vmax=vmax)
-    cb = plt.colorbar(
-        mpl.cm.ScalarMappable(norm=norm, cmap=cmap),
-        ax=ax,
-        use_gridspec=True,
-        anchor=(0.75, 0),
-    )
-    cb.set_label(label)
-
-
-def smooth_vector(y, n_points):
-    """Return copy of the vector `y` smoothed by a running average of `n_points`"""
-    # extend the vector on each side to avoid edge effects. The total extension is
-    #   n_points long, split between start and finish.
-    start_pad = int(np.floor(n_points))
-    end_pad = n_points - start_pad
-    y_extended = np.append(
-        np.append(y[0] * np.ones((start_pad,)), y), y[-1] * np.ones((end_pad,))
-    )
-    # Note, the use of cumsum is faster than convolve, according to the answer here:
-    #   https://stackoverflow.com/a/34387987
-    cumsum_vec = np.cumsum(y_extended)  # put a 0 at the front of y
-    y_smooth = (cumsum_vec[n_points:] - cumsum_vec[:-n_points]) / n_points
-    return y_smooth
-
-
-def calc_linear_background(t, y, tspans):
-    """Return a copy of the vector `y` that interpolates linearly between tspans
-
-    The vector `y - calc_linear_background(t, y, tspans)` will go to zero at the times
-    on `t` specified by `tspan
-
-    Args:
-        t (numpy Array): time
-        y (numpy Array): the value to calculate a background to
-        tspans (list of timespans): The times to interpolate the background between
-    """
-    t_bg_list = []
-    y_bg_list = []
-    for tspan in tspans:
-        mask = np.logical_and(tspan[0] < t, t < tspan[-1])
-        if True not in mask:
-            continue
-        t_bg_list.append(t[mask].mean())
-        y_bg_list.append(y[mask].mean())
-    return np.interp(t, t_bg_list, y_bg_list)
+"""This module contains loose functions and stuff useful for ixdat plotting."""
+
+import numpy as np
+import matplotlib as mpl
+from matplotlib import pyplot as plt
+
+
+def color_axis(ax, color, lr="right", xy="y"):
+    """Color the spine, ticks, and labels of an axis.
+
+    ax (matplotlib.pyplot.axis): the axis to color (a part of)
+    color (str): The color to color the axis.
+    lr (str): whether to color the "left" spine or the "right". Defaults to "right".
+    xy (str): whether to color the "x" axis or the "y". Defaults to "y".
+    """
+    ax.spines[lr].set_color(color)
+    ax.tick_params(axis=xy, color=color, labelcolor=color)
+    if xy == "y":
+        ax.yaxis.label.set_color(color)
+    if xy == "x":
+        ax.xaxis.label.set_color(color)
+
+
+def get_color_from_cmap(x, cmap_name):
+    """Return the color as a 4-tuple, given a value btwn 0 and 1, and color map name
+
+    Args:
+        x (float): Value between 0 and 1 defining a location on a matplotlib colormap
+        cmap_name (str): The name of a matplotlib colormap. Popular ones include
+            "inferno" (0=black --> 1=yellow) and "jet" (0=blue --> 1=red)
+            See https://matplotlib.org/3.5.0/tutorials/colors/colormaps.html
+    """
+
+    cmap = mpl.cm.get_cmap(cmap_name)
+
+    rgba = cmap(x)
+    return rgba
+
+
+def add_colorbar(ax, cmap_name, vmin, vmax, label="intensity"):
+    """Add a colorbar to a matplotlib axis
+
+    ax (matplotlib Axis): The axis (with data plotted on it) to add a colorbar for
+    cmap_name (str): The name of the color map
+    vmin (float): The minimum value in the plotted data
+    vmax (float): The maximum value in the plotted data
+    label (str): A label for the color bar (the name of the value represented by color)
+    """
+
+    cmap = mpl.cm.get_cmap(cmap_name)
+    norm = mpl.colors.Normalize(vmin=vmin, vmax=vmax)
+    cb = plt.colorbar(
+        mpl.cm.ScalarMappable(norm=norm, cmap=cmap),
+        ax=ax,
+        use_gridspec=True,
+        anchor=(0.75, 0),
+    )
+    cb.set_label(label)
+
+
+def smooth_vector(y, n_points):
+    """Return copy of the vector `y` smoothed by a running average of `n_points`"""
+    # extend the vector on each side to avoid edge effects. The total extension is
+    #   n_points long, split between start and finish.
+    start_pad = int(np.floor(n_points))
+    end_pad = n_points - start_pad
+    y_extended = np.append(
+        np.append(y[0] * np.ones((start_pad,)), y), y[-1] * np.ones((end_pad,))
+    )
+    # Note, the use of cumsum is faster than convolve, according to the answer here:
+    #   https://stackoverflow.com/a/34387987
+    cumsum_vec = np.cumsum(y_extended)  # put a 0 at the front of y
+    y_smooth = (cumsum_vec[n_points:] - cumsum_vec[:-n_points]) / n_points
+    return y_smooth
+
+
+def calc_linear_background(t, y, tspans):
+    """Return a copy of the vector `y` that interpolates linearly between tspans
+
+    The vector `y - calc_linear_background(t, y, tspans)` will go to zero at the times
+    on `t` specified by `tspan
+
+    Args:
+        t (numpy Array): time
+        y (numpy Array): the value to calculate a background to
+        tspans (list of timespans): The times to interpolate the background between
+    """
+    t_bg_list = []
+    y_bg_list = []
+    for tspan in tspans:
+        mask = np.logical_and(tspan[0] < t, t < tspan[-1])
+        if True not in mask:
+            continue
+        t_bg_list.append(t[mask].mean())
+        y_bg_list.append(y[mask].mean())
+    return np.interp(t, t_bg_list, y_bg_list)
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat/plotters/sec_plotter.py` & `ixdat-0.2.9.dev3/src/ixdat/plotters/sec_plotter.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,441 +1,441 @@
-"""Plotters for spectroelectrochemistry. Makes use of those in spectrum_plotter.py"""
-
-import matplotlib as mpl
-
-from .ec_plotter import ECPlotter
-from .spectrum_plotter import SpectrumSeriesPlotter, SpectroMeasurementPlotter
-from ..exceptions import SeriesNotFoundError
-
-
-class SECPlotter(SpectroMeasurementPlotter):
-    """An spectroelectrochemistry (SEC) matplotlib plotter."""
-
-    def __init__(self, measurement=None):
-        """Initiate the plotter with its default Meausurement to plot"""
-        super().__init__()
-        self.measurement = measurement
-        self.ec_plotter = ECPlotter(measurement=measurement)
-        self.spectrum_series_plotter = SpectrumSeriesPlotter()
-
-    def plot_measurement(
-        self,
-        *,
-        measurement=None,
-        field=None,
-        tspan=None,
-        xspan=None,
-        axes=None,
-        cmap_name="inferno",
-        make_colorbar=False,
-        continuous=None,
-        **kwargs,
-    ):
-        """Plot an SECMeasurement in two panels with time as x-asis.
-
-        The top panel is a heat plot with the spectral scanning variable (x) on y-axis
-        and color representing the value of the spectral data.
-        The bottom panel contains electrochemistry data.
-
-        Args:
-            measurement (Measurement): The measurement to be plotted, if different from
-                self.measurement
-            field (Field): The field with the spectral data to plot. Defaults to
-                `measurement.spectra`
-            tspan (timespan): The timespan of data to keep for the measurement.
-            xspan (iterable): The span of spectral data to plot
-            axes (list of mpl.Axis): The axes to plot on. axes[0] is for the heat
-                plot, axes[1] for potential, and axes[2] for current. The axes are
-                optional and a new set of axes, where axes[1] and axes[2] are twinned on
-                x, are generated if not provided.
-            cmap_name (str): The name of the colormap to use. Defaults to "inferno", see
-                https://matplotlib.org/3.5.0/tutorials/colors/colormaps.html#sequential
-            make_colorbar (bool): Whether to make a colorbar.
-                FIXME: colorbar at present mis-alignes axes
-            kwargs: Additional key-word arguments are passed on to
-                ECPlotter.plot_measurement().
-            continuous (bool): Optional. Whether to make a continuous heat plot (True) or
-                a discrete heat plot for each spectrum (False). In the discrete case,
-                each heat plot is a rectangle with the spectrum's duration as its width,
-                if available. If the duration is not available, each spectrum heat plot
-                extends to the start of the next one.
-                Defaults to `measurement.spectrum_series.continuous`.
-
-        Returns:
-            list of Axes: axes=[spectra, potential, None, current]
-                axes[0] is the top axis with the heat map of the spectra
-                axes[1] is the bottom left axis with electrochemical potential
-                axes[2] is None (this is where a top right axis would go)
-                axes[3] is the bottom right axis with electrode current
-        """
-        measurement = measurement or self.measurement
-
-        if not axes:
-            axes = self.new_two_panel_axes(
-                n_bottom=2,
-                n_top=1,
-                emphasis="top",
-            )
-        self.ec_plotter.plot_measurement(
-            measurement=measurement,
-            axes=[axes[1], axes[3]],
-            tspan=tspan,
-            **kwargs,
-        )
-        axes[0] = self.spectrum_series_plotter.heat_plot(
-            spectrum_series=measurement.spectrum_series,
-            field=field or measurement.spectra,
-            tspan=tspan,
-            xspan=xspan,
-            ax=axes[0],
-            cmap_name=cmap_name,
-            make_colorbar=make_colorbar,
-            continuous=continuous,
-        )
-        if make_colorbar:
-            pass  # TODO: adjust EC plot to be same width as heat plot despite colorbar.
-
-        axes[1].set_xlim(axes[0].get_xlim())
-
-        return axes
-
-    def plot_vs_potential(
-        self,
-        *,
-        measurement=None,
-        field=None,
-        tspan=None,
-        vspan=None,
-        U_name=None,
-        J_name=None,
-        xspan=None,
-        axes=None,
-        cmap_name="inferno",
-        make_colorbar=False,
-        **kwargs,
-    ):
-        """Plot an SECMeasurement in two panels with potential as x-asis.
-
-        The top panel is a heat plot with wavelength on y-axis and color representing
-        spectrum. At most one of V_ref and t_ref should be given, and if neither are
-        given the measurement's default reference_spectrum is used to calculate the
-        optical density.
-
-        Args:
-            measurement (Measurement): The measurement to be plotted, if different from
-                self.measurement
-            field (Field): The field with the spectral data to plot. Defaults to
-                `measurement.spectra`
-            tspan (timespan): The timespan of data to keep for the measurement.
-            vspan (timespan): The potential span of data to keep for the measurement.
-            U_name (str): Optional. The name of the data series to use as potential.
-            J_name (str): Optional. The name of the data series to use as current.
-            xspan (iterable): The span of spectral data to plot
-            axes (list of numpy Axes): The axes to plot on. axes[0] is for the heat
-                plot and axes[1] for potential. New are made by default.
-            cmap_name (str): The name of the colormap to use. Defaults to "inferno", see
-                https://matplotlib.org/3.5.0/tutorials/colors/colormaps.html#sequential
-            make_colorbar (bool): Whether to make a colorbar.
-            kwargs: Additional key-word arguments are passed on to
-                ECPlotter.plot_vs_potential().
-        """
-        measurement = measurement or self.measurement
-
-        if not axes:
-            axes = self.new_two_panel_axes(
-                n_bottom=1,
-                n_top=1,
-                emphasis="top",
-            )
-
-        self.ec_plotter.plot_vs_potential(
-            measurement=measurement,
-            tspan=tspan,
-            U_name=U_name,
-            J_name=J_name,
-            ax=axes[1],
-            **kwargs,
-        )
-
-        super().heat_plot_vs(
-            measurement=measurement,
-            field=field or measurement.spectra,
-            vspan=vspan,
-            xspan=xspan,
-            ax=axes[0],
-            cmap_name=cmap_name,
-            make_colorbar=make_colorbar,
-            vs=U_name or measurement.U_name,
-        )
-        axes[1].set_xlim(axes[0].get_xlim())
-        return axes
-
-
-class ECOpticalPlotter(SECPlotter):
-    def plot_measurement(
-        self,
-        *,
-        measurement=None,
-        tspan=None,
-        wlspan=None,
-        axes=None,
-        V_ref=None,
-        t_ref=None,
-        cmap_name="inferno",
-        make_colorbar=False,
-        **kwargs,
-    ):
-        """Plot an SECMeasurement in two panels with time as x-asis.
-
-        The top panel is a heat plot with wavelength on y-axis and color representing
-        spectrum. At most one of V_ref and t_ref should be given, and if neither are
-        given the measurement's default reference_spectrum is used to calculate the
-        optical density.
-
-        Args:
-            measurement (Measurement): The measurement to be plotted, if different from
-                self.measurement
-            tspan (timespan): The timespan of data to keep for the measurement.
-            wlspan (iterable): The wavelength span of spectral data to plot
-            axes (list of mpl.Axis): The axes to plot on. axes[0] is for the heat
-                plot, axes[1] for potential, and axes[2] for current. The axes are
-                optional and a new set of axes, where axes[1] and axes[2] are twinned on
-                x, are generated if not provided.
-            V_ref (float): Potential to use as reference for calculating optical density
-            t_ref (float): Time to use as a reference for calculating optical density
-            cmap_name (str): The name of the colormap to use. Defaults to "inferno", see
-                https://matplotlib.org/3.5.0/tutorials/colors/colormaps.html#sequential
-            make_colorbar (bool): Whether to make a colorbar.
-                FIXME: colorbar at present misaligns axes
-            kwargs: Additional key-word arguments are passed on to
-                ECPlotter.plot_measurement().
-
-        Returns:
-            list of Axes: axes=[spectra, potential, None, current]
-                axes[0] is the top axis with the heat map of the spectra
-                axes[1] is the bottom left axis with electrochemical potential
-                axes[2] is None (this is where a top right axis would go)
-                axes[3] is the bottom right axis with electrode current
-        """
-        measurement = measurement or self.measurement
-
-        dOD_series = measurement.calc_dOD(V_ref=V_ref, t_ref=t_ref)
-
-        return super().plot_measurement(
-            measurement=measurement,
-            tspan=tspan,
-            xspan=wlspan,
-            axes=axes,
-            cmap_name=cmap_name,
-            make_colorbar=make_colorbar,
-            field=dOD_series,
-            **kwargs,
-        )
-
-    def plot_vs_potential(
-        self,
-        *,
-        measurement=None,
-        tspan=None,
-        vspan=None,
-        U_name=None,
-        J_name=None,
-        wlspan=None,
-        axes=None,
-        V_ref=None,
-        t_ref=None,
-        cmap_name="inferno",
-        make_colorbar=False,
-        **kwargs,
-    ):
-        """Plot an SECMeasurement in two panels with time as x-asis.
-
-        The top panel is a heat plot with wavelength on y-axis and color representing
-        spectrum. At most one of V_ref and t_ref should be given, and if neither are
-        given the measurement's default reference_spectrum is used to calculate the
-        optical density.
-
-        Args:
-            measurement (Measurement): The measurement to be plotted, if different from
-                self.measurement
-            tspan (timespan): The timespan of data to keep for the measurement.
-            vspan (timespan): The potential span of data to keep for the measurement.
-            U_name (str): Optional. The name of the data series to use as potential.
-            J_name (str): Optional. The name of the data series to use as current.
-            wlspan (iterable): The wavelength span of spectral data to plot
-            axes (list of mpl.Axis): The axes to plot on. axes[0] is for the heat
-                plot, axes[1] for potential, and axes[2] for current. The axes are
-                optional and a new set of axes, where axes[1] and axes[2] are twinned on
-                x, are generated if not provided.
-            V_ref (float): Potential to use as reference for calculating optical density
-            t_ref (float): Time to use as a reference for calculating optical density
-            cmap_name (str): The name of the colormap to use. Defaults to "inferno", see
-                https://matplotlib.org/3.5.0/tutorials/colors/colormaps.html#sequential
-            make_colorbar (bool): Whether to make a colorbar.
-                FIXME: colorbar at present mis-alignes axes
-            kwargs: Additional key-word arguments are passed on to
-                ECPlotter.plot_measurement().
-
-        Returns:
-            list of Axes: axes=[spectra, potential, None, current]
-                axes[0] is the top axis with the heat map of the spectra
-                axes[1] is the bottom left axis with electrochemical potential
-                axes[2] is None (this is where a top right axis would go)
-                axes[3] is the bottom right axis with electrode current
-        """
-        measurement = measurement or self.measurement
-
-        dOD_series = measurement.calc_dOD(V_ref=V_ref, t_ref=t_ref)
-
-        return super().plot_vs_potential(
-            measurement=measurement,
-            tspan=tspan,
-            vspan=vspan,
-            U_name=U_name,
-            J_name=J_name,
-            xspan=wlspan,
-            axes=axes,
-            cmap_name=cmap_name,
-            make_colorbar=make_colorbar,
-            field=dOD_series,
-            **kwargs,
-        )
-
-    def plot_waterfall(
-        self,
-        *,
-        measurement=None,
-        ax=None,
-        V_ref=None,
-        t_ref=None,
-        cmap_name="jet",
-        make_colorbar=True,
-    ):
-        """Plot an SECMeasurement as spectra colored based on potential.
-
-        The top panel is a heat plot with wavelength on y-axis and color representing
-        spectrum. At most one of V_ref and t_ref should be given, and if neither are
-        given the measurement's default reference_spectrum is used to calculate the
-        optical density.
-
-        This uses :func:`~spectrum_plotter.SpectrumSeriesPlotter.plot_waterfall()`
-
-        Args:
-            measurement (Measurement): The measurement to be plotted, if different from
-                self.measurement
-            tspan (timespan): The timespan of data to keep for the measurement.
-            wlspan (iterable): The wavelength span of spectral data to plot
-            ax (matplotlib Axis): The axes to plot on. A new one is made by default.
-            V_ref (float): potential to use as reference for calculating optical density
-            t_ref (float): time to use as a reference for calculating optical density
-            cmap_name (str): The name of the colormap to use. Defaults to "jet", see
-                https://matplotlib.org/3.5.0/tutorials/colors/colormaps.html
-            make_colorbar (bool): Whether to make a colorbar.
-        """
-        measurement = measurement or self.measurement
-        dOD = measurement.calc_dOD(V_ref=V_ref, t_ref=t_ref)
-
-        return super().plot_waterfall_vs(
-            measurement=self.measurement,
-            field=dOD,
-            cmap_name=cmap_name,
-            make_colorbar=make_colorbar,
-            ax=ax,
-            vs=measurement.U_name,
-        )
-
-    def plot_wavelengths(
-        self,
-        *,
-        measurement=None,
-        wavelengths=None,
-        axes=None,
-        cmap_name="jet",
-        tspan=None,
-        **kwargs,
-    ):
-        """Plot the dO.D. for specific wavelength in the top panel and EC in bottom
-
-        Args:
-            measurement (Measurement): The measurement to be plotted, if different from
-                self.measurement
-            wavelengths (list of str): The names of the wavelengths to track as strings,
-                e.g. "w400" for 400 nm
-            axes (list of Ax): The axes to plot on, defaults to new matplotlib axes
-            cmap_name (str): The name of the colormap to use. Defaults to "jet", see
-                https://matplotlib.org/3.5.0/tutorials/colors/colormaps.html
-            tspan (timespan): The timespan to plot
-            **kwargs: Additional key-word arguments are passed on to
-                ECPlotter.plot_measurement
-        """
-        measurement = measurement or self.measurement
-        wavelengths = wavelengths or measurement.tracked_wavelengths
-
-        cmap = mpl.cm.get_cmap(cmap_name)
-        norm = mpl.colors.Normalize(vmin=min(measurement.wl), vmax=max(measurement.wl))
-
-        if not axes:
-            axes = self.new_two_panel_axes(n_bottom=2)
-        for wl_str in wavelengths:
-            x = float(wl_str[1:])
-            try:
-                t, y = measurement.grab(wl_str, tspan=tspan)
-            except SeriesNotFoundError:
-                measurement.track_wavelength(x)
-                t, y = measurement.grab(wl_str, tspan=tspan)
-            axes[0].plot(t, y, color=cmap(norm(x)), label=wl_str)
-        axes[0].legend()
-        axes[0].set_ylabel(r"$\Delta$O.D.")
-
-        self.ec_plotter.plot_measurement(
-            measurement=measurement, axes=[axes[1], axes[3]], tspan=tspan, **kwargs
-        )
-        return axes
-
-    def plot_wavelengths_vs_potential(
-        self,
-        *,
-        measurement=None,
-        wavelengths=None,
-        axes=None,
-        cmap_name="jet",
-        tspan=None,
-        **kwargs,
-    ):
-        """Plot the dO.D. for specific wavelength in the top panel vs potential
-
-        Args:
-            measurement (Measurement): The measurement to be plotted, if different from
-                self.measurement
-            wavelengths (list of str): The names of the wavelengths to track as strings,
-                e.g. "w400" for 400 nm
-            axes (list of Ax): The axes to plot on, defaults to new matplotlib axes
-            cmap_name (str): The name of the colormap to use. Defaults to "jet", see
-                https://matplotlib.org/3.5.0/tutorials/colors/colormaps.html
-            tspan (timespan): The timespan to plot
-            **kwargs: Additional key-word arguments are passed on to
-                ECPlotter.plot_vs_potential
-        """
-        measurement = measurement or self.measurement
-        wavelengths = wavelengths or measurement.tracked_wavelengths
-
-        cmap = mpl.cm.get_cmap(cmap_name)
-        norm = mpl.colors.Normalize(vmin=min(measurement.wl), vmax=max(measurement.wl))
-
-        if not axes:
-            axes = self.new_two_panel_axes()
-        for wl_str in wavelengths:
-            x = float(wl_str[1:])
-            try:
-                t, y = measurement.grab(wl_str, tspan=tspan)
-            except SeriesNotFoundError:
-                measurement.track_wavelength(x)
-                t, y = measurement.grab(wl_str, tspan=tspan)
-            v = measurement.U
-            axes[0].plot(v, y, color=cmap(norm(x)), label=wl_str)
-        axes[0].legend()
-        axes[0].set_ylabel(r"$\Delta$O.D.")
-
-        self.ec_plotter.plot_vs_potential(
-            measurement=measurement, ax=axes[1], tspan=tspan, **kwargs
-        )
-        return axes
+"""Plotters for spectroelectrochemistry. Makes use of those in spectrum_plotter.py"""
+
+import matplotlib as mpl
+
+from .ec_plotter import ECPlotter
+from .spectrum_plotter import SpectrumSeriesPlotter, SpectroMeasurementPlotter
+from ..exceptions import SeriesNotFoundError
+
+
+class SECPlotter(SpectroMeasurementPlotter):
+    """An spectroelectrochemistry (SEC) matplotlib plotter."""
+
+    def __init__(self, measurement=None):
+        """Initiate the plotter with its default Meausurement to plot"""
+        super().__init__()
+        self.measurement = measurement
+        self.ec_plotter = ECPlotter(measurement=measurement)
+        self.spectrum_series_plotter = SpectrumSeriesPlotter()
+
+    def plot_measurement(
+        self,
+        *,
+        measurement=None,
+        field=None,
+        tspan=None,
+        xspan=None,
+        axes=None,
+        cmap_name="inferno",
+        make_colorbar=False,
+        continuous=None,
+        **kwargs,
+    ):
+        """Plot an SECMeasurement in two panels with time as x-asis.
+
+        The top panel is a heat plot with the spectral scanning variable (x) on y-axis
+        and color representing the value of the spectral data.
+        The bottom panel contains electrochemistry data.
+
+        Args:
+            measurement (Measurement): The measurement to be plotted, if different from
+                self.measurement
+            field (Field): The field with the spectral data to plot. Defaults to
+                `measurement.spectra`
+            tspan (timespan): The timespan of data to keep for the measurement.
+            xspan (iterable): The span of spectral data to plot
+            axes (list of mpl.Axis): The axes to plot on. axes[0] is for the heat
+                plot, axes[1] for potential, and axes[2] for current. The axes are
+                optional and a new set of axes, where axes[1] and axes[2] are twinned on
+                x, are generated if not provided.
+            cmap_name (str): The name of the colormap to use. Defaults to "inferno", see
+                https://matplotlib.org/3.5.0/tutorials/colors/colormaps.html#sequential
+            make_colorbar (bool): Whether to make a colorbar.
+                FIXME: colorbar at present mis-alignes axes
+            kwargs: Additional key-word arguments are passed on to
+                ECPlotter.plot_measurement().
+            continuous (bool): Optional. Whether to make a continuous heat plot (True) or
+                a discrete heat plot for each spectrum (False). In the discrete case,
+                each heat plot is a rectangle with the spectrum's duration as its width,
+                if available. If the duration is not available, each spectrum heat plot
+                extends to the start of the next one.
+                Defaults to `measurement.spectrum_series.continuous`.
+
+        Returns:
+            list of Axes: axes=[spectra, potential, None, current]
+                axes[0] is the top axis with the heat map of the spectra
+                axes[1] is the bottom left axis with electrochemical potential
+                axes[2] is None (this is where a top right axis would go)
+                axes[3] is the bottom right axis with electrode current
+        """
+        measurement = measurement or self.measurement
+
+        if not axes:
+            axes = self.new_two_panel_axes(
+                n_bottom=2,
+                n_top=1,
+                emphasis="top",
+            )
+        self.ec_plotter.plot_measurement(
+            measurement=measurement,
+            axes=[axes[1], axes[3]],
+            tspan=tspan,
+            **kwargs,
+        )
+        axes[0] = self.spectrum_series_plotter.heat_plot(
+            spectrum_series=measurement.spectrum_series,
+            field=field or measurement.spectra,
+            tspan=tspan,
+            xspan=xspan,
+            ax=axes[0],
+            cmap_name=cmap_name,
+            make_colorbar=make_colorbar,
+            continuous=continuous,
+        )
+        if make_colorbar:
+            pass  # TODO: adjust EC plot to be same width as heat plot despite colorbar.
+
+        axes[1].set_xlim(axes[0].get_xlim())
+
+        return axes
+
+    def plot_vs_potential(
+        self,
+        *,
+        measurement=None,
+        field=None,
+        tspan=None,
+        vspan=None,
+        U_name=None,
+        J_name=None,
+        xspan=None,
+        axes=None,
+        cmap_name="inferno",
+        make_colorbar=False,
+        **kwargs,
+    ):
+        """Plot an SECMeasurement in two panels with potential as x-asis.
+
+        The top panel is a heat plot with wavelength on y-axis and color representing
+        spectrum. At most one of V_ref and t_ref should be given, and if neither are
+        given the measurement's default reference_spectrum is used to calculate the
+        optical density.
+
+        Args:
+            measurement (Measurement): The measurement to be plotted, if different from
+                self.measurement
+            field (Field): The field with the spectral data to plot. Defaults to
+                `measurement.spectra`
+            tspan (timespan): The timespan of data to keep for the measurement.
+            vspan (timespan): The potential span of data to keep for the measurement.
+            U_name (str): Optional. The name of the data series to use as potential.
+            J_name (str): Optional. The name of the data series to use as current.
+            xspan (iterable): The span of spectral data to plot
+            axes (list of numpy Axes): The axes to plot on. axes[0] is for the heat
+                plot and axes[1] for potential. New are made by default.
+            cmap_name (str): The name of the colormap to use. Defaults to "inferno", see
+                https://matplotlib.org/3.5.0/tutorials/colors/colormaps.html#sequential
+            make_colorbar (bool): Whether to make a colorbar.
+            kwargs: Additional key-word arguments are passed on to
+                ECPlotter.plot_vs_potential().
+        """
+        measurement = measurement or self.measurement
+
+        if not axes:
+            axes = self.new_two_panel_axes(
+                n_bottom=1,
+                n_top=1,
+                emphasis="top",
+            )
+
+        self.ec_plotter.plot_vs_potential(
+            measurement=measurement,
+            tspan=tspan,
+            U_name=U_name,
+            J_name=J_name,
+            ax=axes[1],
+            **kwargs,
+        )
+
+        super().heat_plot_vs(
+            measurement=measurement,
+            field=field or measurement.spectra,
+            vspan=vspan,
+            xspan=xspan,
+            ax=axes[0],
+            cmap_name=cmap_name,
+            make_colorbar=make_colorbar,
+            vs=U_name or measurement.U_name,
+        )
+        axes[1].set_xlim(axes[0].get_xlim())
+        return axes
+
+
+class ECOpticalPlotter(SECPlotter):
+    def plot_measurement(
+        self,
+        *,
+        measurement=None,
+        tspan=None,
+        wlspan=None,
+        axes=None,
+        V_ref=None,
+        t_ref=None,
+        cmap_name="inferno",
+        make_colorbar=False,
+        **kwargs,
+    ):
+        """Plot an SECMeasurement in two panels with time as x-asis.
+
+        The top panel is a heat plot with wavelength on y-axis and color representing
+        spectrum. At most one of V_ref and t_ref should be given, and if neither are
+        given the measurement's default reference_spectrum is used to calculate the
+        optical density.
+
+        Args:
+            measurement (Measurement): The measurement to be plotted, if different from
+                self.measurement
+            tspan (timespan): The timespan of data to keep for the measurement.
+            wlspan (iterable): The wavelength span of spectral data to plot
+            axes (list of mpl.Axis): The axes to plot on. axes[0] is for the heat
+                plot, axes[1] for potential, and axes[2] for current. The axes are
+                optional and a new set of axes, where axes[1] and axes[2] are twinned on
+                x, are generated if not provided.
+            V_ref (float): Potential to use as reference for calculating optical density
+            t_ref (float): Time to use as a reference for calculating optical density
+            cmap_name (str): The name of the colormap to use. Defaults to "inferno", see
+                https://matplotlib.org/3.5.0/tutorials/colors/colormaps.html#sequential
+            make_colorbar (bool): Whether to make a colorbar.
+                FIXME: colorbar at present misaligns axes
+            kwargs: Additional key-word arguments are passed on to
+                ECPlotter.plot_measurement().
+
+        Returns:
+            list of Axes: axes=[spectra, potential, None, current]
+                axes[0] is the top axis with the heat map of the spectra
+                axes[1] is the bottom left axis with electrochemical potential
+                axes[2] is None (this is where a top right axis would go)
+                axes[3] is the bottom right axis with electrode current
+        """
+        measurement = measurement or self.measurement
+
+        dOD_series = measurement.calc_dOD(V_ref=V_ref, t_ref=t_ref)
+
+        return super().plot_measurement(
+            measurement=measurement,
+            tspan=tspan,
+            xspan=wlspan,
+            axes=axes,
+            cmap_name=cmap_name,
+            make_colorbar=make_colorbar,
+            field=dOD_series,
+            **kwargs,
+        )
+
+    def plot_vs_potential(
+        self,
+        *,
+        measurement=None,
+        tspan=None,
+        vspan=None,
+        U_name=None,
+        J_name=None,
+        wlspan=None,
+        axes=None,
+        V_ref=None,
+        t_ref=None,
+        cmap_name="inferno",
+        make_colorbar=False,
+        **kwargs,
+    ):
+        """Plot an SECMeasurement in two panels with time as x-asis.
+
+        The top panel is a heat plot with wavelength on y-axis and color representing
+        spectrum. At most one of V_ref and t_ref should be given, and if neither are
+        given the measurement's default reference_spectrum is used to calculate the
+        optical density.
+
+        Args:
+            measurement (Measurement): The measurement to be plotted, if different from
+                self.measurement
+            tspan (timespan): The timespan of data to keep for the measurement.
+            vspan (timespan): The potential span of data to keep for the measurement.
+            U_name (str): Optional. The name of the data series to use as potential.
+            J_name (str): Optional. The name of the data series to use as current.
+            wlspan (iterable): The wavelength span of spectral data to plot
+            axes (list of mpl.Axis): The axes to plot on. axes[0] is for the heat
+                plot, axes[1] for potential, and axes[2] for current. The axes are
+                optional and a new set of axes, where axes[1] and axes[2] are twinned on
+                x, are generated if not provided.
+            V_ref (float): Potential to use as reference for calculating optical density
+            t_ref (float): Time to use as a reference for calculating optical density
+            cmap_name (str): The name of the colormap to use. Defaults to "inferno", see
+                https://matplotlib.org/3.5.0/tutorials/colors/colormaps.html#sequential
+            make_colorbar (bool): Whether to make a colorbar.
+                FIXME: colorbar at present mis-alignes axes
+            kwargs: Additional key-word arguments are passed on to
+                ECPlotter.plot_measurement().
+
+        Returns:
+            list of Axes: axes=[spectra, potential, None, current]
+                axes[0] is the top axis with the heat map of the spectra
+                axes[1] is the bottom left axis with electrochemical potential
+                axes[2] is None (this is where a top right axis would go)
+                axes[3] is the bottom right axis with electrode current
+        """
+        measurement = measurement or self.measurement
+
+        dOD_series = measurement.calc_dOD(V_ref=V_ref, t_ref=t_ref)
+
+        return super().plot_vs_potential(
+            measurement=measurement,
+            tspan=tspan,
+            vspan=vspan,
+            U_name=U_name,
+            J_name=J_name,
+            xspan=wlspan,
+            axes=axes,
+            cmap_name=cmap_name,
+            make_colorbar=make_colorbar,
+            field=dOD_series,
+            **kwargs,
+        )
+
+    def plot_waterfall(
+        self,
+        *,
+        measurement=None,
+        ax=None,
+        V_ref=None,
+        t_ref=None,
+        cmap_name="jet",
+        make_colorbar=True,
+    ):
+        """Plot an SECMeasurement as spectra colored based on potential.
+
+        The top panel is a heat plot with wavelength on y-axis and color representing
+        spectrum. At most one of V_ref and t_ref should be given, and if neither are
+        given the measurement's default reference_spectrum is used to calculate the
+        optical density.
+
+        This uses :func:`~spectrum_plotter.SpectrumSeriesPlotter.plot_waterfall()`
+
+        Args:
+            measurement (Measurement): The measurement to be plotted, if different from
+                self.measurement
+            tspan (timespan): The timespan of data to keep for the measurement.
+            wlspan (iterable): The wavelength span of spectral data to plot
+            ax (matplotlib Axis): The axes to plot on. A new one is made by default.
+            V_ref (float): potential to use as reference for calculating optical density
+            t_ref (float): time to use as a reference for calculating optical density
+            cmap_name (str): The name of the colormap to use. Defaults to "jet", see
+                https://matplotlib.org/3.5.0/tutorials/colors/colormaps.html
+            make_colorbar (bool): Whether to make a colorbar.
+        """
+        measurement = measurement or self.measurement
+        dOD = measurement.calc_dOD(V_ref=V_ref, t_ref=t_ref)
+
+        return super().plot_waterfall_vs(
+            measurement=self.measurement,
+            field=dOD,
+            cmap_name=cmap_name,
+            make_colorbar=make_colorbar,
+            ax=ax,
+            vs=measurement.U_name,
+        )
+
+    def plot_wavelengths(
+        self,
+        *,
+        measurement=None,
+        wavelengths=None,
+        axes=None,
+        cmap_name="jet",
+        tspan=None,
+        **kwargs,
+    ):
+        """Plot the dO.D. for specific wavelength in the top panel and EC in bottom
+
+        Args:
+            measurement (Measurement): The measurement to be plotted, if different from
+                self.measurement
+            wavelengths (list of str): The names of the wavelengths to track as strings,
+                e.g. "w400" for 400 nm
+            axes (list of Ax): The axes to plot on, defaults to new matplotlib axes
+            cmap_name (str): The name of the colormap to use. Defaults to "jet", see
+                https://matplotlib.org/3.5.0/tutorials/colors/colormaps.html
+            tspan (timespan): The timespan to plot
+            **kwargs: Additional key-word arguments are passed on to
+                ECPlotter.plot_measurement
+        """
+        measurement = measurement or self.measurement
+        wavelengths = wavelengths or measurement.tracked_wavelengths
+
+        cmap = mpl.cm.get_cmap(cmap_name)
+        norm = mpl.colors.Normalize(vmin=min(measurement.wl), vmax=max(measurement.wl))
+
+        if not axes:
+            axes = self.new_two_panel_axes(n_bottom=2)
+        for wl_str in wavelengths:
+            x = float(wl_str[1:])
+            try:
+                t, y = measurement.grab(wl_str, tspan=tspan)
+            except SeriesNotFoundError:
+                measurement.track_wavelength(x)
+                t, y = measurement.grab(wl_str, tspan=tspan)
+            axes[0].plot(t, y, color=cmap(norm(x)), label=wl_str)
+        axes[0].legend()
+        axes[0].set_ylabel(r"$\Delta$O.D.")
+
+        self.ec_plotter.plot_measurement(
+            measurement=measurement, axes=[axes[1], axes[3]], tspan=tspan, **kwargs
+        )
+        return axes
+
+    def plot_wavelengths_vs_potential(
+        self,
+        *,
+        measurement=None,
+        wavelengths=None,
+        axes=None,
+        cmap_name="jet",
+        tspan=None,
+        **kwargs,
+    ):
+        """Plot the dO.D. for specific wavelength in the top panel vs potential
+
+        Args:
+            measurement (Measurement): The measurement to be plotted, if different from
+                self.measurement
+            wavelengths (list of str): The names of the wavelengths to track as strings,
+                e.g. "w400" for 400 nm
+            axes (list of Ax): The axes to plot on, defaults to new matplotlib axes
+            cmap_name (str): The name of the colormap to use. Defaults to "jet", see
+                https://matplotlib.org/3.5.0/tutorials/colors/colormaps.html
+            tspan (timespan): The timespan to plot
+            **kwargs: Additional key-word arguments are passed on to
+                ECPlotter.plot_vs_potential
+        """
+        measurement = measurement or self.measurement
+        wavelengths = wavelengths or measurement.tracked_wavelengths
+
+        cmap = mpl.cm.get_cmap(cmap_name)
+        norm = mpl.colors.Normalize(vmin=min(measurement.wl), vmax=max(measurement.wl))
+
+        if not axes:
+            axes = self.new_two_panel_axes()
+        for wl_str in wavelengths:
+            x = float(wl_str[1:])
+            try:
+                t, y = measurement.grab(wl_str, tspan=tspan)
+            except SeriesNotFoundError:
+                measurement.track_wavelength(x)
+                t, y = measurement.grab(wl_str, tspan=tspan)
+            v = measurement.U
+            axes[0].plot(v, y, color=cmap(norm(x)), label=wl_str)
+        axes[0].legend()
+        axes[0].set_ylabel(r"$\Delta$O.D.")
+
+        self.ec_plotter.plot_vs_potential(
+            measurement=measurement, ax=axes[1], tspan=tspan, **kwargs
+        )
+        return axes
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat/plotters/spectrum_plotter.py` & `ixdat-0.2.9.dev3/src/ixdat/plotters/spectrum_plotter.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,360 +1,360 @@
-"""Plotters for spectra and spectrumseries."""
-
-import numpy as np
-import matplotlib as mpl
-from ixdat.plotters.plotting_tools import add_colorbar
-from matplotlib import pyplot as plt
-from .base_mpl_plotter import MPLPlotter
-
-
-class SpectrumPlotter(MPLPlotter):
-    """A plotter for a spectrum"""
-
-    def __init__(self, spectrum=None):
-        super().__init__()
-        self.spectrum = spectrum
-
-    def plot(self, *, spectrum=None, ax=None, **kwargs):
-        """Plot a spectrum as y (signal) vs x (scanning variable)
-
-        Args:
-            spectrum (Spectrum): The spectrum to plot if different from self.spectrum
-            ax (mpl.Axis): The axis to plot on. A new one is made by default.
-            kwargs: additional key-word arguments are given to ax.plot()
-        """
-        spectrum = spectrum or self.spectrum
-        if not ax:
-            ax = self.new_ax()
-        ax.plot(spectrum.x, spectrum.y, **kwargs)
-        ax.set_xlabel(spectrum.x_name)
-        ax.set_ylabel(spectrum.y_name)
-        return ax
-
-
-class SpectrumSeriesPlotter(MPLPlotter):
-    """A plotter for spectrum series, f.ex. spectra taken continuously over time"""
-
-    def __init__(self, spectrum_series=None):
-        super().__init__()
-        self.spectrum_series = spectrum_series
-
-    @property
-    def plot(self):
-        """The default plot of a SpectrumSeries is heat_plot"""
-        return self.heat_plot
-
-    def plot_average(self, *, spectrum_series=None, ax=None, **kwargs):
-        """Take an average of the spectra and plot that."""
-        spectrum_series = spectrum_series or self.spectrum_series
-        if not ax:
-            ax = self.new_ax()
-        ax.plot(spectrum_series.x, spectrum_series.y_average, **kwargs)
-        ax.set_xlabel(spectrum_series.x_name)
-        ax.set_ylabel(spectrum_series.y_name + " (average)")
-        return ax
-
-    def heat_plot(
-        self,
-        *,
-        spectrum_series=None,
-        field=None,
-        tspan=None,
-        xspan=None,
-        ax=None,
-        cmap_name="inferno",
-        make_colorbar=False,
-        t=None,
-        t_name=None,
-        max_threshold=None,
-        min_threshold=None,
-        scanning_mask=None,
-        vmin=None,
-        vmax=None,
-        continuous=None,
-    ):
-        """
-        Plot a spectrum series with `t` on the horizontal axis, `x` on the vertical axis,
-        and color representing `y`.
-
-        Args:
-            spectrum_series (SpectrumSeries): The spectrum series to be plotted, if
-                different from self.spectrum_series.
-            field (Field): The field to be plotted, if different from
-                spectrum_series.field
-            tspan (iterable): The span of the time data to plot
-            xspan (iterable): The span of the spectral data to plot
-            ax (mpl.Axis): The axes to plot on. A new one is made by default
-
-            cmap_name (str): The name of the colormap to use. Defaults to "inferno", see
-                https://matplotlib.org/3.5.0/tutorials/colors/colormaps.html#sequential
-            make_colorbar (bool): Whether to make a colorbar.
-                FIXME: colorbar at present mis-alignes axes
-            t (numpy array): Time data to use if not the data in spectrum_series
-            t_name (str): Name of time variable if not the one in spectrum_series
-            max_threshold (float): Maximum value to display.
-                Values above are set to zero.
-            min_threshold (float): Minimum value to display.
-                Values below are set to 0.
-            scanning_mask (list): List of booleans to exclude from scanning variable
-                before plotting data by setting y values to 0 (zero).
-            vmin (float): minimum value to represent in colours.
-            vmax (float): maximum value to represent in colours.
-            continuous (bool): Optional. Whether to make a continuous heat plot (True) or
-                a discrete heat plot for each spectrum (False). In the discrete case,
-                each heat plot is a rectangle with the spectrum's duration as its width,
-                if available. If the duration is not available, each spectrum heat plot
-                extends to the start of the next one.
-                Defaults to the `spectrum_series.continuous`.
-        """
-        spectrum_series = spectrum_series or self.spectrum_series
-        field = field or spectrum_series.field
-        if continuous is None:
-            continuous = spectrum_series.continuous
-
-        xseries = field.axes_series[1]
-        x = xseries.data
-        t = t if t is not None else field.axes_series[0].t
-        t_name = t_name or field.axes_series[0].name
-
-        data = field.data
-
-        if max_threshold:
-            # data = np.minimum(max_threshold, data)
-            data[data > max_threshold] = 0
-        if min_threshold:
-            data[data < min_threshold] = 0
-
-        if np.any(scanning_mask):
-            data[:, scanning_mask] = 0
-
-        if tspan:
-            t_mask = np.logical_and(tspan[0] < t, t < tspan[-1])
-            t = t[t_mask]
-            data = data[t_mask, :]
-            if (t[0] < t[-1]) != (tspan[0] < tspan[-1]):  # this is an XOR.
-                # Then we need to plot the data against U in the reverse direction:
-                t = np.flip(t, axis=0)
-                data = np.flip(data, axis=0)
-
-        if xspan:
-            x_mask = np.logical_and(xspan[0] < x, x < xspan[-1])
-            x = x[x_mask]
-            data = data[:, x_mask]
-
-        if not ax:
-            ax = self.new_ax()
-
-        if continuous:
-            ax.imshow(
-                np.flip(data.swapaxes(0, 1), axis=0),
-                cmap=cmap_name,
-                aspect="auto",
-                extent=(t[0], t[-1], x[0], x[-1]),
-                vmin=vmin,
-                vmax=vmax,
-            )
-        else:
-            for i, t_i in enumerate(spectrum_series.t):
-                if tspan and (t_i < min(tspan) or t_i > max(tspan)):
-                    continue
-                try:
-                    duration = spectrum_series.durations[i]
-                    # ^ raises TypeError if durations is None.
-                    t_f = t_i + duration  # raises TypeError if durations[i] is None.
-                except TypeError:
-                    if i < len(t) - 1:
-                        t_f = t[i + 1]
-                    else:
-                        # If its duration is unknown, we don't plot the last spectrum.
-                        break
-                y = data[i]
-                yy = np.stack([y, y])
-                ax.imshow(
-                    np.flip(yy.swapaxes(0, 1), axis=0),
-                    cmap=cmap_name,
-                    aspect="auto",
-                    extent=(t_i, t_f, x[0], x[-1]),
-                    vmin=vmin,
-                    vmax=vmax,
-                )
-
-        ax.set_xlabel(t_name)
-        ax.set_ylabel(xseries.name)
-
-        if make_colorbar:
-            add_colorbar(
-                ax,
-                cmap_name,
-                vmin=(vmin if vmin else np.min(data)),
-                vmax=(vmax if vmax else np.max(data)),
-            )
-        return ax
-
-    def plot_waterfall(
-        self,
-        *,
-        spectrum_series=None,
-        field=None,
-        ax=None,
-        cmap_name="jet",
-        make_colorbar=True,
-        t=None,
-        t_name=None,
-    ):
-        """Plot a SpectrumSeries as spectra colored by the time at which they are taken
-
-        Args:
-            spectrum_series (SpectrumSeries): The spectrum series to be plotted, if
-                different from self.spectrum_series.
-            field (Field): The field to be plotted, if different from
-                spectrum_series.field
-            ax (matplotlib Axis): The axes to plot on. A new one is made by default.
-
-            cmap_name (str): The name of the colormap to use. Defaults to "inferno", see
-                https://matplotlib.org/3.5.0/tutorials/colors/colormaps.html#sequential
-            make_colorbar (bool): Whether to make a colorbar.
-            t (numpy array): Time data to use if not the data in spectrum_series
-            t_name (str): Name of time variable if not the one in spectrum_series
-        """
-        spectrum_series = spectrum_series or self.spectrum_series
-        field = field or spectrum_series.field
-
-        data = field.data
-        x = field.axes_series[1].data
-        t = t if t is not None else field.axes_series[0].t
-        t_name = t_name or field.axes_series[0].name
-
-        cmap = mpl.cm.get_cmap(cmap_name)
-        norm = mpl.colors.Normalize(vmin=np.min(t), vmax=np.max(t))
-
-        if not ax:
-            ax = self.new_ax()
-
-        for i, t_i in enumerate(t):
-            spec = data[i]
-            color = cmap(norm(t_i))
-            ax.plot(x, spec, color=color)
-
-        ax.set_xlabel(field.axes_series[1].name)
-        ax.set_ylabel(field.name)
-
-        if make_colorbar:
-            cb = plt.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap), ax=ax)
-            cb.set_label(t_name)
-
-        return ax
-
-
-class SpectroMeasurementPlotter(MPLPlotter):
-    """Plotter for measurements with spectrum_series
-
-    This makes use of the methods in `SpectrumSeriesPlotter`, but allows a second
-    scanned variable (such as `potential` in the case of spectroelectrochemistry) to be
-    used from the measurement instead of time data.
-    """
-
-    def __init__(self, measurement=None):
-        super().__init__()
-        self.measurement = measurement
-        self.spectrum_series_plotter = SpectrumSeriesPlotter()
-
-    def heat_plot_vs(
-        self,
-        *,
-        measurement=None,
-        field=None,
-        vs=None,
-        vspan=None,
-        xspan=None,
-        ax=None,
-        cmap_name="inferno",
-        make_colorbar=False,
-    ):
-        """Plot a SpectroMeasurement in two panels with time as x-asis.
-
-        The top panel is a heat plot with wavelength on y-axis and color representing
-        spectrum. At most one of V_ref and t_ref should be given, and if neither are
-        given the measurement's default reference_spectrum is used to calculate the
-        optical density.
-
-        Args:
-            measurement (SpectrumSeries): The spectrum series to be plotted, if
-                different from self.spectrum_series.
-            field (Field): The field to be plotted, if different from
-                spectrum_series.field
-            vs (str): The name of the value series or time series to plot against.
-            vspan (iterable): The span of the value series or time series to include
-            xspan (iterable): The span of the spectral data to plot
-            ax (mpl.Axis): The axes to plot on. A new one is made by default
-
-            cmap_name (str): The name of the colormap to use. Defaults to "inferno", see
-                https://matplotlib.org/3.5.0/tutorials/colors/colormaps.html#sequential
-            make_colorbar (bool): Whether to make a colorbar.
-                FIXME: colorbar at present mis-alignes axes
-        """
-        measurement = measurement or self.measurement
-
-        tseries = field.axes_series[0]
-        v_name = vs
-        if vs in ("t", tseries.tseries.name):
-            v = tseries.t
-            if hasattr(measurement, "t_str") and measurement.t_str:
-                v_name = measurement.t_str
-        else:
-            v = measurement.grab_for_t(vs, t=tseries.t)
-
-        return self.spectrum_series_plotter.heat_plot(
-            spectrum_series=measurement.spectrum_series,
-            field=field,
-            tspan=vspan,
-            xspan=xspan,
-            ax=ax,
-            cmap_name=cmap_name,
-            make_colorbar=make_colorbar,
-            t=v,
-            t_name=v_name,
-        )
-
-    def plot_waterfall_vs(
-        self,
-        *,
-        measurement=None,
-        field=None,
-        vs=None,
-        ax=None,
-        cmap_name="jet",
-        make_colorbar=True,
-    ):
-        """Plot a SpectrumSeries as spectra colored by the value at which they are taken
-
-        Args:
-            measurement (SpectroMeasurement): The measurement to be plotted if different
-                from self.measurement.
-            field (Field): The field to be plotted, if different from
-                spectrum_series.field
-            vs (str): The name of the value to use for the color scale. Defaults to time
-            ax (matplotlib Axis): The axes to plot on. A new one is made by default.
-            cmap_name (str): The name of the colormap to use. Defaults to "inferno", see
-                https://matplotlib.org/3.5.0/tutorials/colors/colormaps.html#sequential
-            make_colorbar (bool): Whether to make a colorbar.
-        """
-        measurement = measurement or self.measurement
-
-        tseries = field.axes_series[0]
-        v_name = vs
-        if vs in ("t", tseries.tseries.name):
-            v = tseries.t
-            if hasattr(measurement, "t_str") and measurement.t_str:
-                v_name = measurement.t_str
-        else:
-            v = measurement.grab_for_t(vs, t=tseries.t)
-
-        return self.spectrum_series_plotter.plot_waterfall(
-            spectrum_series=measurement.spectrum_series,
-            field=field,
-            ax=ax,
-            cmap_name=cmap_name,
-            make_colorbar=make_colorbar,
-            t=v,
-            t_name=v_name,
-        )
+"""Plotters for spectra and spectrumseries."""
+
+import numpy as np
+import matplotlib as mpl
+from ixdat.plotters.plotting_tools import add_colorbar
+from matplotlib import pyplot as plt
+from .base_mpl_plotter import MPLPlotter
+
+
+class SpectrumPlotter(MPLPlotter):
+    """A plotter for a spectrum"""
+
+    def __init__(self, spectrum=None):
+        super().__init__()
+        self.spectrum = spectrum
+
+    def plot(self, *, spectrum=None, ax=None, **kwargs):
+        """Plot a spectrum as y (signal) vs x (scanning variable)
+
+        Args:
+            spectrum (Spectrum): The spectrum to plot if different from self.spectrum
+            ax (mpl.Axis): The axis to plot on. A new one is made by default.
+            kwargs: additional key-word arguments are given to ax.plot()
+        """
+        spectrum = spectrum or self.spectrum
+        if not ax:
+            ax = self.new_ax()
+        ax.plot(spectrum.x, spectrum.y, **kwargs)
+        ax.set_xlabel(spectrum.x_name)
+        ax.set_ylabel(spectrum.y_name)
+        return ax
+
+
+class SpectrumSeriesPlotter(MPLPlotter):
+    """A plotter for spectrum series, f.ex. spectra taken continuously over time"""
+
+    def __init__(self, spectrum_series=None):
+        super().__init__()
+        self.spectrum_series = spectrum_series
+
+    @property
+    def plot(self):
+        """The default plot of a SpectrumSeries is heat_plot"""
+        return self.heat_plot
+
+    def plot_average(self, *, spectrum_series=None, ax=None, **kwargs):
+        """Take an average of the spectra and plot that."""
+        spectrum_series = spectrum_series or self.spectrum_series
+        if not ax:
+            ax = self.new_ax()
+        ax.plot(spectrum_series.x, spectrum_series.y_average, **kwargs)
+        ax.set_xlabel(spectrum_series.x_name)
+        ax.set_ylabel(spectrum_series.y_name + " (average)")
+        return ax
+
+    def heat_plot(
+        self,
+        *,
+        spectrum_series=None,
+        field=None,
+        tspan=None,
+        xspan=None,
+        ax=None,
+        cmap_name="inferno",
+        make_colorbar=False,
+        t=None,
+        t_name=None,
+        max_threshold=None,
+        min_threshold=None,
+        scanning_mask=None,
+        vmin=None,
+        vmax=None,
+        continuous=None,
+    ):
+        """
+        Plot a spectrum series with `t` on the horizontal axis, `x` on the vertical axis,
+        and color representing `y`.
+
+        Args:
+            spectrum_series (SpectrumSeries): The spectrum series to be plotted, if
+                different from self.spectrum_series.
+            field (Field): The field to be plotted, if different from
+                spectrum_series.field
+            tspan (iterable): The span of the time data to plot
+            xspan (iterable): The span of the spectral data to plot
+            ax (mpl.Axis): The axes to plot on. A new one is made by default
+
+            cmap_name (str): The name of the colormap to use. Defaults to "inferno", see
+                https://matplotlib.org/3.5.0/tutorials/colors/colormaps.html#sequential
+            make_colorbar (bool): Whether to make a colorbar.
+                FIXME: colorbar at present mis-alignes axes
+            t (numpy array): Time data to use if not the data in spectrum_series
+            t_name (str): Name of time variable if not the one in spectrum_series
+            max_threshold (float): Maximum value to display.
+                Values above are set to zero.
+            min_threshold (float): Minimum value to display.
+                Values below are set to 0.
+            scanning_mask (list): List of booleans to exclude from scanning variable
+                before plotting data by setting y values to 0 (zero).
+            vmin (float): minimum value to represent in colours.
+            vmax (float): maximum value to represent in colours.
+            continuous (bool): Optional. Whether to make a continuous heat plot (True) or
+                a discrete heat plot for each spectrum (False). In the discrete case,
+                each heat plot is a rectangle with the spectrum's duration as its width,
+                if available. If the duration is not available, each spectrum heat plot
+                extends to the start of the next one.
+                Defaults to the `spectrum_series.continuous`.
+        """
+        spectrum_series = spectrum_series or self.spectrum_series
+        field = field or spectrum_series.field
+        if continuous is None:
+            continuous = spectrum_series.continuous
+
+        xseries = field.axes_series[1]
+        x = xseries.data
+        t = t if t is not None else field.axes_series[0].t
+        t_name = t_name or field.axes_series[0].name
+
+        data = field.data
+
+        if max_threshold:
+            # data = np.minimum(max_threshold, data)
+            data[data > max_threshold] = 0
+        if min_threshold:
+            data[data < min_threshold] = 0
+
+        if np.any(scanning_mask):
+            data[:, scanning_mask] = 0
+
+        if tspan:
+            t_mask = np.logical_and(tspan[0] < t, t < tspan[-1])
+            t = t[t_mask]
+            data = data[t_mask, :]
+            if (t[0] < t[-1]) != (tspan[0] < tspan[-1]):  # this is an XOR.
+                # Then we need to plot the data against U in the reverse direction:
+                t = np.flip(t, axis=0)
+                data = np.flip(data, axis=0)
+
+        if xspan:
+            x_mask = np.logical_and(xspan[0] < x, x < xspan[-1])
+            x = x[x_mask]
+            data = data[:, x_mask]
+
+        if not ax:
+            ax = self.new_ax()
+
+        if continuous:
+            ax.imshow(
+                np.flip(data.swapaxes(0, 1), axis=0),
+                cmap=cmap_name,
+                aspect="auto",
+                extent=(t[0], t[-1], x[0], x[-1]),
+                vmin=vmin,
+                vmax=vmax,
+            )
+        else:
+            for i, t_i in enumerate(spectrum_series.t):
+                if tspan and (t_i < min(tspan) or t_i > max(tspan)):
+                    continue
+                try:
+                    duration = spectrum_series.durations[i]
+                    # ^ raises TypeError if durations is None.
+                    t_f = t_i + duration  # raises TypeError if durations[i] is None.
+                except TypeError:
+                    if i < len(t) - 1:
+                        t_f = t[i + 1]
+                    else:
+                        # If its duration is unknown, we don't plot the last spectrum.
+                        break
+                y = data[i]
+                yy = np.stack([y, y])
+                ax.imshow(
+                    np.flip(yy.swapaxes(0, 1), axis=0),
+                    cmap=cmap_name,
+                    aspect="auto",
+                    extent=(t_i, t_f, x[0], x[-1]),
+                    vmin=vmin,
+                    vmax=vmax,
+                )
+
+        ax.set_xlabel(t_name)
+        ax.set_ylabel(xseries.name)
+
+        if make_colorbar:
+            add_colorbar(
+                ax,
+                cmap_name,
+                vmin=(vmin if vmin else np.min(data)),
+                vmax=(vmax if vmax else np.max(data)),
+            )
+        return ax
+
+    def plot_waterfall(
+        self,
+        *,
+        spectrum_series=None,
+        field=None,
+        ax=None,
+        cmap_name="jet",
+        make_colorbar=True,
+        t=None,
+        t_name=None,
+    ):
+        """Plot a SpectrumSeries as spectra colored by the time at which they are taken
+
+        Args:
+            spectrum_series (SpectrumSeries): The spectrum series to be plotted, if
+                different from self.spectrum_series.
+            field (Field): The field to be plotted, if different from
+                spectrum_series.field
+            ax (matplotlib Axis): The axes to plot on. A new one is made by default.
+
+            cmap_name (str): The name of the colormap to use. Defaults to "inferno", see
+                https://matplotlib.org/3.5.0/tutorials/colors/colormaps.html#sequential
+            make_colorbar (bool): Whether to make a colorbar.
+            t (numpy array): Time data to use if not the data in spectrum_series
+            t_name (str): Name of time variable if not the one in spectrum_series
+        """
+        spectrum_series = spectrum_series or self.spectrum_series
+        field = field or spectrum_series.field
+
+        data = field.data
+        x = field.axes_series[1].data
+        t = t if t is not None else field.axes_series[0].t
+        t_name = t_name or field.axes_series[0].name
+
+        cmap = mpl.cm.get_cmap(cmap_name)
+        norm = mpl.colors.Normalize(vmin=np.min(t), vmax=np.max(t))
+
+        if not ax:
+            ax = self.new_ax()
+
+        for i, t_i in enumerate(t):
+            spec = data[i]
+            color = cmap(norm(t_i))
+            ax.plot(x, spec, color=color)
+
+        ax.set_xlabel(field.axes_series[1].name)
+        ax.set_ylabel(field.name)
+
+        if make_colorbar:
+            cb = plt.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap), ax=ax)
+            cb.set_label(t_name)
+
+        return ax
+
+
+class SpectroMeasurementPlotter(MPLPlotter):
+    """Plotter for measurements with spectrum_series
+
+    This makes use of the methods in `SpectrumSeriesPlotter`, but allows a second
+    scanned variable (such as `potential` in the case of spectroelectrochemistry) to be
+    used from the measurement instead of time data.
+    """
+
+    def __init__(self, measurement=None):
+        super().__init__()
+        self.measurement = measurement
+        self.spectrum_series_plotter = SpectrumSeriesPlotter()
+
+    def heat_plot_vs(
+        self,
+        *,
+        measurement=None,
+        field=None,
+        vs=None,
+        vspan=None,
+        xspan=None,
+        ax=None,
+        cmap_name="inferno",
+        make_colorbar=False,
+    ):
+        """Plot a SpectroMeasurement in two panels with time as x-asis.
+
+        The top panel is a heat plot with wavelength on y-axis and color representing
+        spectrum. At most one of V_ref and t_ref should be given, and if neither are
+        given the measurement's default reference_spectrum is used to calculate the
+        optical density.
+
+        Args:
+            measurement (SpectrumSeries): The spectrum series to be plotted, if
+                different from self.spectrum_series.
+            field (Field): The field to be plotted, if different from
+                spectrum_series.field
+            vs (str): The name of the value series or time series to plot against.
+            vspan (iterable): The span of the value series or time series to include
+            xspan (iterable): The span of the spectral data to plot
+            ax (mpl.Axis): The axes to plot on. A new one is made by default
+
+            cmap_name (str): The name of the colormap to use. Defaults to "inferno", see
+                https://matplotlib.org/3.5.0/tutorials/colors/colormaps.html#sequential
+            make_colorbar (bool): Whether to make a colorbar.
+                FIXME: colorbar at present mis-alignes axes
+        """
+        measurement = measurement or self.measurement
+
+        tseries = field.axes_series[0]
+        v_name = vs
+        if vs in ("t", tseries.tseries.name):
+            v = tseries.t
+            if hasattr(measurement, "t_str") and measurement.t_str:
+                v_name = measurement.t_str
+        else:
+            v = measurement.grab_for_t(vs, t=tseries.t)
+
+        return self.spectrum_series_plotter.heat_plot(
+            spectrum_series=measurement.spectrum_series,
+            field=field,
+            tspan=vspan,
+            xspan=xspan,
+            ax=ax,
+            cmap_name=cmap_name,
+            make_colorbar=make_colorbar,
+            t=v,
+            t_name=v_name,
+        )
+
+    def plot_waterfall_vs(
+        self,
+        *,
+        measurement=None,
+        field=None,
+        vs=None,
+        ax=None,
+        cmap_name="jet",
+        make_colorbar=True,
+    ):
+        """Plot a SpectrumSeries as spectra colored by the value at which they are taken
+
+        Args:
+            measurement (SpectroMeasurement): The measurement to be plotted if different
+                from self.measurement.
+            field (Field): The field to be plotted, if different from
+                spectrum_series.field
+            vs (str): The name of the value to use for the color scale. Defaults to time
+            ax (matplotlib Axis): The axes to plot on. A new one is made by default.
+            cmap_name (str): The name of the colormap to use. Defaults to "inferno", see
+                https://matplotlib.org/3.5.0/tutorials/colors/colormaps.html#sequential
+            make_colorbar (bool): Whether to make a colorbar.
+        """
+        measurement = measurement or self.measurement
+
+        tseries = field.axes_series[0]
+        v_name = vs
+        if vs in ("t", tseries.tseries.name):
+            v = tseries.t
+            if hasattr(measurement, "t_str") and measurement.t_str:
+                v_name = measurement.t_str
+        else:
+            v = measurement.grab_for_t(vs, t=tseries.t)
+
+        return self.spectrum_series_plotter.plot_waterfall(
+            spectrum_series=measurement.spectrum_series,
+            field=field,
+            ax=ax,
+            cmap_name=cmap_name,
+            make_colorbar=make_colorbar,
+            t=v,
+            t_name=v_name,
+        )
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat/projects/lablogs.py` & `ixdat-0.2.9.dev3/src/ixdat/projects/lablogs.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,20 +1,20 @@
-from ixdat.db import Saveable
-
-
-class LabLog(Saveable):
-    """TODO: flush out this class"""
-
-    table_name = "lablog"
-    column_attrs = {"name": "name"}
-
-    def __init__(self, name, metadata=None, notes=None):
-        """Initiate the lablog with its name, metadata, and notes"""
-        super().__init__()
-        self.name = name
-        self.metadata = metadata or {}
-        self.notes = notes or ""
-
-    @classmethod
-    def load_or_make(cls, name):
-        """Load a lab log or make a new one if it hasn't been saved before"""
-        return cls(name)
+from ixdat.db import Saveable
+
+
+class LabLog(Saveable):
+    """TODO: flush out this class"""
+
+    table_name = "lablog"
+    column_attrs = {"name": "name"}
+
+    def __init__(self, name, metadata=None, notes=None):
+        """Initiate the lablog with its name, metadata, and notes"""
+        super().__init__()
+        self.name = name
+        self.metadata = metadata or {}
+        self.notes = notes or ""
+
+    @classmethod
+    def load_or_make(cls, name):
+        """Load a lab log or make a new one if it hasn't been saved before"""
+        return cls(name)
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat/readers/__init__.py` & `ixdat-0.2.9.dev3/src/ixdat/readers/__init__.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,67 +1,69 @@
-"""Import readers and build the READER_CLASSES dictionary for direct import
-
-Constants:
-    READER_CLASSES (dict): Dictionary of {reader_name: ReaderClass} where
-        reader_name is the name of the backend (like "directory") and ReaderClass
-        is the reader class for parsing files.
-"""
-from ..techniques import TECHNIQUE_CLASSES
-
-# ixdat
-from .ixdat_csv import IxdatCSVReader, IxdatSpectrumReader
-
-# potentiostats
-from .biologic import BiologicReader
-from .autolab import NovaASCIIReader
-from .ivium import IviumDatasetReader
-from .chi import CHInstrumentsTXTReader
-
-# mass spectrometers
-from .pfeiffer import PVMassSpecReader
-from .rgasoft import StanfordRGASoftReader
-from .cinfdata import CinfdataTXTReader
-from .cinfdata_db import CinfdataDBReader
-
-# ec-ms
-from .zilien import ZilienTSVReader, ZilienTMPReader, ZilienSpectrumReader
-from .ec_ms_pkl import EC_MS_CONVERTER
-
-# spectroelectrochemistry
-from .msrh_sec import MsrhSECReader, MsrhSECDecayReader
-
-# xrd
-from .xrdml import XRDMLReader
-
-# xps
-from .avantage import AvantageAVGReader
-
-# xas
-from .qexafs import QexafsDATReader
-
-# Measruement.read() looks for readers here:
-READER_CLASSES = {
-    "ixdat": IxdatCSVReader,
-    "biologic": BiologicReader,
-    "autolab": NovaASCIIReader,
-    "ivium": IviumDatasetReader,
-    "chi": CHInstrumentsTXTReader,
-    "pfeiffer": PVMassSpecReader,
-    "rgasoft": StanfordRGASoftReader,
-    "cinfdata": CinfdataTXTReader,
-    "cinfdata_db": CinfdataDBReader,
-    "zilien": ZilienTSVReader,
-    "zilien_tmp": ZilienTMPReader,
-    "EC_MS": EC_MS_CONVERTER,
-    "msrh_sec": MsrhSECReader,
-    "msrh_sec_decay": MsrhSECDecayReader,
-    "qexafs": QexafsDATReader,
-}
-
-
-# Spectrum.read() looks for readers here:
-SPECTRUM_READER_CLASSES = {
-    "ixdat": IxdatSpectrumReader,
-    "avantage": AvantageAVGReader,
-    "zilien": ZilienSpectrumReader,
-    "xrdml": XRDMLReader,
-}
+"""Import readers and build the READER_CLASSES dictionary for direct import
+
+Constants:
+    READER_CLASSES (dict): Dictionary of {reader_name: ReaderClass} where
+        reader_name is the name of the backend (like "directory") and ReaderClass
+        is the reader class for parsing files.
+"""
+from ..techniques import TECHNIQUE_CLASSES
+
+# ixdat
+from .ixdat_csv import IxdatCSVReader, IxdatSpectrumReader
+
+# potentiostats
+from .biologic import BiologicReader
+from .autolab import NovaASCIIReader
+from .ivium import IviumDatasetReader
+from .chi import CHInstrumentsTXTReader
+from .nordic import NordicTDMSReader
+
+# mass spectrometers
+from .pfeiffer import PVMassSpecReader
+from .rgasoft import StanfordRGASoftReader
+from .cinfdata import CinfdataTXTReader
+from .cinfdata_db import CinfdataDBReader
+
+# ec-ms
+from .zilien import ZilienTSVReader, ZilienTMPReader, ZilienSpectrumReader
+from .ec_ms_pkl import EC_MS_CONVERTER
+
+# spectroelectrochemistry
+from .msrh_sec import MsrhSECReader, MsrhSECDecayReader
+
+# xrd
+from .xrdml import XRDMLReader
+
+# xps
+from .avantage import AvantageAVGReader
+
+# xas
+from .qexafs import QexafsDATReader
+
+# Measruement.read() looks for readers here:
+READER_CLASSES = {
+    "ixdat": IxdatCSVReader,
+    "biologic": BiologicReader,
+    "autolab": NovaASCIIReader,
+    "ivium": IviumDatasetReader,
+    "chi": CHInstrumentsTXTReader,
+    "pfeiffer": PVMassSpecReader,
+    "rgasoft": StanfordRGASoftReader,
+    "cinfdata": CinfdataTXTReader,
+    "cinfdata_db": CinfdataDBReader,
+    "zilien": ZilienTSVReader,
+    "zilien_tmp": ZilienTMPReader,
+    "EC_MS": EC_MS_CONVERTER,
+    "msrh_sec": MsrhSECReader,
+    "msrh_sec_decay": MsrhSECDecayReader,
+    "qexafs": QexafsDATReader,
+    "nordic": NordicTDMSReader,
+}
+
+
+# Spectrum.read() looks for readers here:
+SPECTRUM_READER_CLASSES = {
+    "ixdat": IxdatSpectrumReader,
+    "avantage": AvantageAVGReader,
+    "zilien": ZilienSpectrumReader,
+    "xrdml": XRDMLReader,
+}
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat/readers/autolab.py` & `ixdat-0.2.9.dev3/src/ixdat/readers/autolab.py`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,83 +1,83 @@
-"""This module implements the reader for ascii exports from autolab's Nova software"""
-
-import re
-from pathlib import Path
-import pandas as pd
-from .reading_tools import (
-    prompt_for_tstamp,
-    series_list_from_dataframe,
-    STANDARD_TIMESTAMP_FORM,
-    timestamp_string_to_tstamp,
-)
-
-AUTOLAB_ALIASES = {
-    "raw_potential": ("WE(1).Potential (V)",),
-    "raw_current": ("WE(1).Current (A)",),
-    "t": ("Time (s)",),
-}
-
-
-class NovaASCIIReader:
-    """A reader for ascii files exported by Autolab's Nova software"""
-
-    def read(
-        self,
-        path_to_file,
-        cls=None,
-        name=None,
-        tstamp=None,
-        timestring=None,
-        timestring_form=STANDARD_TIMESTAMP_FORM,
-        **kwargs
-    ):
-        """Read the ASCII export from Autolab's Nova software
-
-        Args:
-            path_to_file (Path): The full absolute or relative path including the suffix
-            name (str): The name to use if not the file name
-            cls (Measurement subclass): The Measurement class to return an object of.
-                Defaults to `ECMeasurement` and should probably be a subclass thereof in
-                any case.
-            tstamp (float): timestamp of the measurement, if known
-            timestring (str): timestring describing the timestamp of the measurement
-            timestring_form (str): form of the timestring. Default is "%d/%m/%Y %H:%M:%S"
-            **kwargs (dict): Key-word arguments are passed to cls.__init__
-        """
-        self.path_to_file = Path(path_to_file)
-        name = name or self.path_to_file.name
-        if not tstamp:
-            if timestring:
-                tstamp = timestamp_string_to_tstamp(timestring, form=timestring_form)
-            else:
-                tstamp = prompt_for_tstamp(self.path_to_file)
-
-        dataframe = pd.read_csv(self.path_to_file, delimiter=";")
-
-        data_series_list = series_list_from_dataframe(
-            dataframe, "Time (s)", tstamp, get_column_unit
-        )
-        obj_as_dict = dict(
-            name=name,
-            technique="EC",
-            reader=self,
-            aliases=AUTOLAB_ALIASES,
-            series_list=data_series_list,
-            tstamp=tstamp,
-        )
-        obj_as_dict.update(kwargs)
-
-        if not cls:
-            from ..techniques.ec import ECMeasurement
-
-            cls = ECMeasurement
-        return cls.from_dict(obj_as_dict)
-
-
-def get_column_unit(column_name):
-    """Return the unit name of an autolab column, i.e the last part of the name in ()"""
-    unit_match = re.search(r"\((.+)\)$", column_name)
-    if unit_match:
-        unit_name = unit_match.group(1)
-    else:
-        unit_name = None
-    return unit_name
+"""This module implements the reader for ascii exports from autolab's Nova software"""
+
+import re
+from pathlib import Path
+import pandas as pd
+from .reading_tools import (
+    prompt_for_tstamp,
+    series_list_from_dataframe,
+    STANDARD_TIMESTAMP_FORM,
+    timestamp_string_to_tstamp,
+)
+
+AUTOLAB_ALIASES = {
+    "raw_potential": ("WE(1).Potential (V)",),
+    "raw_current": ("WE(1).Current (A)",),
+    "t": ("Time (s)",),
+}
+
+
+class NovaASCIIReader:
+    """A reader for ascii files exported by Autolab's Nova software"""
+
+    def read(
+        self,
+        path_to_file,
+        cls=None,
+        name=None,
+        tstamp=None,
+        timestring=None,
+        timestring_form=STANDARD_TIMESTAMP_FORM,
+        **kwargs
+    ):
+        """Read the ASCII export from Autolab's Nova software
+
+        Args:
+            path_to_file (Path): The full absolute or relative path including the suffix
+            name (str): The name to use if not the file name
+            cls (Measurement subclass): The Measurement class to return an object of.
+                Defaults to `ECMeasurement` and should probably be a subclass thereof in
+                any case.
+            tstamp (float): timestamp of the measurement, if known
+            timestring (str): timestring describing the timestamp of the measurement
+            timestring_form (str): form of the timestring. Default is "%d/%m/%Y %H:%M:%S"
+            **kwargs (dict): Key-word arguments are passed to cls.__init__
+        """
+        self.path_to_file = Path(path_to_file)
+        name = name or self.path_to_file.name
+        if not tstamp:
+            if timestring:
+                tstamp = timestamp_string_to_tstamp(timestring, form=timestring_form)
+            else:
+                tstamp = prompt_for_tstamp(self.path_to_file)
+
+        dataframe = pd.read_csv(self.path_to_file, delimiter=";")
+
+        data_series_list = series_list_from_dataframe(
+            dataframe, "Time (s)", tstamp, get_column_unit
+        )
+        obj_as_dict = dict(
+            name=name,
+            technique="EC",
+            reader=self,
+            aliases=AUTOLAB_ALIASES,
+            series_list=data_series_list,
+            tstamp=tstamp,
+        )
+        obj_as_dict.update(kwargs)
+
+        if not cls:
+            from ..techniques.ec import ECMeasurement
+
+            cls = ECMeasurement
+        return cls.from_dict(obj_as_dict)
+
+
+def get_column_unit(column_name):
+    """Return the unit name of an autolab column, i.e the last part of the name in ()"""
+    unit_match = re.search(r"\((.+)\)$", column_name)
+    if unit_match:
+        unit_name = unit_match.group(1)
+    else:
+        unit_name = None
+    return unit_name
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat/readers/avantage.py` & `ixdat-0.2.9.dev3/src/ixdat/readers/avantage.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,129 +1,129 @@
-from pathlib import Path
-import numpy as np
-from ..spectra import Spectrum
-from ..data_series import DataSeries, Field
-
-
-class AvantageAVGReader:
-    """A class for importing a .avg file exported by DataSpace_BatchDump.exe"""
-
-    def __init__(self, path_to_file=None):
-        self.path_to_file = path_to_file
-
-    def read(self, path_to_file, cls=None, **kwargs):
-        """Load data stored as text by Advantage's default exporting mode
-
-        Copied from pyThetaProbe, written by Anna Winiwarter and Soren Scott in 2019
-        TODO: Improve this code. See suggestions here:
-                https://github.com/ixdat/ixdat/pull/73#discussion_r892233369
-            See also more powerful reader ideas here:
-                https://github.com/CINF/PyExpLabSys/blob/master/PyExpLabSys/file_parsers/avantage.py#L384
-            and here:
-                https://github.com/ixdat/LowOverpotentialRegime/blob/main/src/pyOER/iss.py
-        Written for simple intensity-vs-energy, but with possible future expansion in
-        mind.
-        Returns the dataset as a python dictionary.
-
-        Args:
-            path_to_file (str or Path): Path to the .avg data
-            cls (Spectrum subclass): Class of spectrum to return an object of
-            kwargs: Additional keyword arguments are passed to cls.__init__
-        """
-
-        path_to_file = Path(path_to_file or self.path_to_file)
-        cls = cls or Spectrum
-
-        get_data_ax_keys, get_data_ax_vals, get_space_ax_keys, get_space_ax_vals = (
-            False,
-            False,
-            False,
-            False,
-        )
-        header_lines = []
-
-        with open(path_to_file, "r") as f:
-            in_header = True
-            while in_header:
-                line = f.readline()
-                header_lines += [line]
-
-                if get_data_ax_keys:
-                    data_ax_key_str = line.split("=")[-1]
-                    data_ax_keys = [k.strip() for k in data_ax_key_str.split(",")]
-                    data_axes = {}
-                    get_data_ax_keys = False
-
-                if get_space_ax_keys:
-                    space_ax_key_str = line.split("=")[-1]
-                    space_ax_keys = [k.strip() for k in space_ax_key_str.split(",")]
-                    space_axes = {}
-                    get_space_ax_keys = False
-
-                if get_data_ax_vals:
-                    try:
-                        data_ax_nr = int(line.split("=")[0])
-                    except ValueError:
-                        get_data_ax_vals = False
-                        continue
-                    data_ax_val_str = line.split("=")[-1]
-                    data_ax_vals = [k.strip() for k in data_ax_val_str.split(",")]
-                    data_axes[data_ax_nr] = dict(zip(data_ax_keys, data_ax_vals))
-
-                if get_space_ax_vals:
-                    try:
-                        space_ax_nr = int(line.split("=")[0])
-                    except ValueError:
-                        get_space_ax_vals = False
-                        continue
-                    space_ax_val_str = line.split("=")[-1]
-                    space_ax_vals = [k.strip() for k in space_ax_val_str.split(",")]
-                    space_axes[space_ax_nr] = dict(zip(space_ax_keys, space_ax_vals))
-
-                if "$DATA=" in line:
-                    in_header = False
-                    in_data = True
-                if "data ax" in line:
-                    get_data_ax_keys = True
-                if "$DATAAXES" in line:
-                    get_data_ax_vals = True
-                if "space ax" in line:
-                    get_space_ax_keys = True
-                if "$SPACEAXES" in line:
-                    get_space_ax_vals = True
-
-            y_vec = np.array([])
-            while in_data:
-                line = f.readline()
-                data_str = line.split("=")[-1]
-                data_str = data_str.replace("#empty#", "nan")
-                try:
-                    y_i = np.array([float(y_str) for y_str in data_str.split(",")])
-                except ValueError:
-                    if len(line.strip()) > 0:
-                        print(
-                            "found no data on this line: \n"
-                            + line
-                            + "\n Ending data import!"
-                        )
-                    in_data = False
-                else:
-                    y_vec = np.append(y_vec, y_i)
-
-            # looks from start and finish values like, for XPS, data axis refers to
-            # kinetic energy and space axis refers to binding energy
-
-            x_start = float(space_axes[0]["start"])
-            x_width = float(space_axes[0]["width"])
-            x_numPoints = int(space_axes[0]["numPoints"])
-
-            x_vec = x_start + np.arange(0, x_numPoints) * x_width
-
-        xseries = DataSeries(
-            name=space_axes[0]["label"], unit_name=space_axes[0]["unit"], data=x_vec
-        )
-        field = Field(name="counts", unit_name="", data=y_vec, axes_series=[xseries])
-
-        if "name" not in kwargs:
-            kwargs["name"] = path_to_file.stem
-
-        return cls.from_field(field, **kwargs)
+from pathlib import Path
+import numpy as np
+from ..spectra import Spectrum
+from ..data_series import DataSeries, Field
+
+
+class AvantageAVGReader:
+    """A class for importing a .avg file exported by DataSpace_BatchDump.exe"""
+
+    def __init__(self, path_to_file=None):
+        self.path_to_file = path_to_file
+
+    def read(self, path_to_file, cls=None, **kwargs):
+        """Load data stored as text by Advantage's default exporting mode
+
+        Copied from pyThetaProbe, written by Anna Winiwarter and Soren Scott in 2019
+        TODO: Improve this code. See suggestions here:
+                https://github.com/ixdat/ixdat/pull/73#discussion_r892233369
+            See also more powerful reader ideas here:
+                https://github.com/CINF/PyExpLabSys/blob/master/PyExpLabSys/file_parsers/avantage.py#L384
+            and here:
+                https://github.com/ixdat/LowOverpotentialRegime/blob/main/src/pyOER/iss.py
+        Written for simple intensity-vs-energy, but with possible future expansion in
+        mind.
+        Returns the dataset as a python dictionary.
+
+        Args:
+            path_to_file (str or Path): Path to the .avg data
+            cls (Spectrum subclass): Class of spectrum to return an object of
+            kwargs: Additional keyword arguments are passed to cls.__init__
+        """
+
+        path_to_file = Path(path_to_file or self.path_to_file)
+        cls = cls or Spectrum
+
+        get_data_ax_keys, get_data_ax_vals, get_space_ax_keys, get_space_ax_vals = (
+            False,
+            False,
+            False,
+            False,
+        )
+        header_lines = []
+
+        with open(path_to_file, "r") as f:
+            in_header = True
+            while in_header:
+                line = f.readline()
+                header_lines += [line]
+
+                if get_data_ax_keys:
+                    data_ax_key_str = line.split("=")[-1]
+                    data_ax_keys = [k.strip() for k in data_ax_key_str.split(",")]
+                    data_axes = {}
+                    get_data_ax_keys = False
+
+                if get_space_ax_keys:
+                    space_ax_key_str = line.split("=")[-1]
+                    space_ax_keys = [k.strip() for k in space_ax_key_str.split(",")]
+                    space_axes = {}
+                    get_space_ax_keys = False
+
+                if get_data_ax_vals:
+                    try:
+                        data_ax_nr = int(line.split("=")[0])
+                    except ValueError:
+                        get_data_ax_vals = False
+                        continue
+                    data_ax_val_str = line.split("=")[-1]
+                    data_ax_vals = [k.strip() for k in data_ax_val_str.split(",")]
+                    data_axes[data_ax_nr] = dict(zip(data_ax_keys, data_ax_vals))
+
+                if get_space_ax_vals:
+                    try:
+                        space_ax_nr = int(line.split("=")[0])
+                    except ValueError:
+                        get_space_ax_vals = False
+                        continue
+                    space_ax_val_str = line.split("=")[-1]
+                    space_ax_vals = [k.strip() for k in space_ax_val_str.split(",")]
+                    space_axes[space_ax_nr] = dict(zip(space_ax_keys, space_ax_vals))
+
+                if "$DATA=" in line:
+                    in_header = False
+                    in_data = True
+                if "data ax" in line:
+                    get_data_ax_keys = True
+                if "$DATAAXES" in line:
+                    get_data_ax_vals = True
+                if "space ax" in line:
+                    get_space_ax_keys = True
+                if "$SPACEAXES" in line:
+                    get_space_ax_vals = True
+
+            y_vec = np.array([])
+            while in_data:
+                line = f.readline()
+                data_str = line.split("=")[-1]
+                data_str = data_str.replace("#empty#", "nan")
+                try:
+                    y_i = np.array([float(y_str) for y_str in data_str.split(",")])
+                except ValueError:
+                    if len(line.strip()) > 0:
+                        print(
+                            "found no data on this line: \n"
+                            + line
+                            + "\n Ending data import!"
+                        )
+                    in_data = False
+                else:
+                    y_vec = np.append(y_vec, y_i)
+
+            # looks from start and finish values like, for XPS, data axis refers to
+            # kinetic energy and space axis refers to binding energy
+
+            x_start = float(space_axes[0]["start"])
+            x_width = float(space_axes[0]["width"])
+            x_numPoints = int(space_axes[0]["numPoints"])
+
+            x_vec = x_start + np.arange(0, x_numPoints) * x_width
+
+        xseries = DataSeries(
+            name=space_axes[0]["label"], unit_name=space_axes[0]["unit"], data=x_vec
+        )
+        field = Field(name="counts", unit_name="", data=y_vec, axes_series=[xseries])
+
+        if "name" not in kwargs:
+            kwargs["name"] = path_to_file.stem
+
+        return cls.from_field(field, **kwargs)
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat/readers/chi.py` & `ixdat-0.2.9.dev3/src/ixdat/readers/chi.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,47 +1,47 @@
-"""A reader for text exports from the RGA Software of Stanford Instruments"""
-
-from .ec_ms_pkl import measurement_from_ec_ms_dataset
-from ..techniques import ECMeasurement
-
-
-CHI_LEGACY_ALIASES = {
-    # TODO: These should change to what Zilien calls them. Right now the alias's
-    #   reflect the way the lagacy EC_MS code renames essential series
-    "t": ["time/s"],
-    "raw_potential": ["Ewe/V", "<Ewe>/V"],
-    "raw_current": ["I/mA", "<I>/mA"],
-    "cycle": ["cycle number"],
-}
-
-
-class CHInstrumentsTXTReader:
-    path_to_file = None
-
-    def read(self, path_to_file, cls=None):
-        """Read a .txt file exported by CH Instruments software.
-
-        TODO: Write a new reader that doesn't use the old EC_MS package
-
-        Args:
-            path_to_file (Path or str): The file to read
-            cls (Measurement subclass): The class to return. Defaults to ECMeasuremnt
-        """
-        try:
-            from EC_MS import Dataset
-        except ImportError:
-            print(
-                "The ixdat CHInstrumentsTXTReader relies on the EC_MS package.\n"
-                "Use `pip install EC_MS`. \n"
-                "Alternatively considering writing a new Reader for ixdat!"
-            )
-
-        self.path_to_file = path_to_file
-        cls = cls if (cls and not issubclass(ECMeasurement, cls)) else ECMeasurement
-        ec_ms_dataset = Dataset(path_to_file, data_type="CHI")
-        return measurement_from_ec_ms_dataset(
-            ec_ms_dataset.data,
-            cls=cls,
-            reader=self,
-            technique="EC",
-            aliases=CHI_LEGACY_ALIASES,
-        )
+"""A reader for text exports from the RGA Software of Stanford Instruments"""
+
+from .ec_ms_pkl import measurement_from_ec_ms_dataset
+from ..techniques import ECMeasurement
+
+
+CHI_LEGACY_ALIASES = {
+    # TODO: These should change to what Zilien calls them. Right now the alias's
+    #   reflect the way the lagacy EC_MS code renames essential series
+    "t": ["time/s"],
+    "raw_potential": ["Ewe/V", "<Ewe>/V"],
+    "raw_current": ["I/mA", "<I>/mA"],
+    "cycle": ["cycle number"],
+}
+
+
+class CHInstrumentsTXTReader:
+    path_to_file = None
+
+    def read(self, path_to_file, cls=None):
+        """Read a .txt file exported by CH Instruments software.
+
+        TODO: Write a new reader that doesn't use the old EC_MS package
+
+        Args:
+            path_to_file (Path or str): The file to read
+            cls (Measurement subclass): The class to return. Defaults to ECMeasuremnt
+        """
+        try:
+            from EC_MS import Dataset
+        except ImportError:
+            print(
+                "The ixdat CHInstrumentsTXTReader relies on the EC_MS package.\n"
+                "Use `pip install EC_MS`. \n"
+                "Alternatively considering writing a new Reader for ixdat!"
+            )
+
+        self.path_to_file = path_to_file
+        cls = cls if (cls and not issubclass(ECMeasurement, cls)) else ECMeasurement
+        ec_ms_dataset = Dataset(path_to_file, data_type="CHI")
+        return measurement_from_ec_ms_dataset(
+            ec_ms_dataset.data,
+            cls=cls,
+            reader=self,
+            technique="EC",
+            aliases=CHI_LEGACY_ALIASES,
+        )
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat/readers/cinfdata.py` & `ixdat-0.2.9.dev3/src/ixdat/readers/cinfdata.py`

 * *Ordering differences only*

 * *Files 11% similar despite different names*

```diff
@@ -1,211 +1,211 @@
-"""Module defining readers for DTU Surfcat's legendary cinfdata system"""
-
-from pathlib import Path
-import numpy as np
-from ..exceptions import ReadError
-from ..data_series import ValueSeries, TimeSeries
-from ..techniques import MSMeasurement
-from .reading_tools import timestamp_string_to_tstamp
-
-
-class CinfdataTXTReader:
-    """A class that reads the text exported by cinfdata's text export functionality
-
-    TODO: We should also have a reader class that downloads the data from cinfdata like
-        `EC_MS`'s `download_cinfdata_set`:
-        https://github.com/ScottSoren/EC_MS/blob/master/src/EC_MS/Data_Importing.py#L711
-
-    Attributes:
-        path_to_file (Path): the location and name of the file read by the reader
-        n_line (int): the number of the last line read by the reader
-        place_in_file (str): The last location in the file read by the reader. This
-            is used internally to tell the reader how to parse each line. Options are:
-            "header", "column names", and "data".
-        header_lines (list of str): a list of the header lines of the files. This
-            includes the column name line. The header can be nicely viewed with the
-            print_header() function.
-        tstamp (str): The unix time corresponding to t=0 for the measurement
-        tstamp_list (list of float): list of epoch tstamps in the file's timestamp line
-        column_tstamps (dict): The unix time corresponding to t=0 for each time column
-        technique (str): The name of the technique
-        column_names (list of str): The names of the data columns in the file
-        t_and_v_cols (dict): {name: (tcol, vcol)} where name is the name of the
-            ValueSeries (e.g. "M2"), tcol is the name of the corresponding time column
-            in the file (e.g. "M2-x"), and vcol is the the name of the value column in
-            the file (e.g. "M2-y).
-        column_data (dict of str: np.array): The data in the file as a dict.
-            Note that the np arrays are the same ones as in the measurement's DataSeries,
-            so this does not waste memory.
-        file_has_been_read (bool): This is used to make sure read() is only successfully
-            called once by the Reader. False until read() is called, then True.
-        measurement (Measurement): The measurement returned by read() when the file is
-            read. self.measureemnt is None before read() is called.
-    """
-
-    delim = "\t"
-
-    def __init__(self):
-        """Initialize a Reader for cinfdata-exported text files. See class docstring."""
-        self.name = None
-        self.path_to_file = None
-        self.n_line = 0
-        self.place_in_file = "header"
-        self.header_lines = []
-        self.tstamp = None
-        self.tstamp_list = []
-        self.column_tstamps = {}
-        self.column_names = []
-        self.t_and_v_cols = {}
-        self.column_data = {}
-        self.technique = "MS"  # TODO: Figure out how to tell if it's something else
-        self.measurement_class = MSMeasurement
-        self.file_has_been_read = False
-        self.measurement = None
-
-    def read(self, path_to_file, name=None, cls=None, **kwargs):
-        """Return an MSMeasurement with the data and metadata recorded in path_to_file
-
-        This loops through the lines of the file, processing one at a time. For header
-        lines, this involves searching for metadata. For the column name line, this
-        involves creating empty arrays for each data series. For the data lines, this
-        involves appending to these arrays. After going through all the lines, it
-        converts the arrays to DataSeries.
-        For cinfdata text files, each value column has its own timecolumn, and they are
-        not necessarily all the same length.
-        Finally, the method returns an ECMeasurement with these DataSeries. The
-        ECMeasurement contains a reference to the reader.
-        All attributes of this reader can be accessed from the
-        measurement as `measurement.reader.attribute_name`.
-
-        Args:
-            path_to_file (Path): The full abs or rel path including the ".txt" extension
-            **kwargs (dict): Key-word arguments are passed to ECMeasurement.__init__
-        """
-        path_to_file = Path(path_to_file) if path_to_file else self.path_to_file
-        if self.file_has_been_read:
-            print(
-                f"This {self.__class__.__name__} has already read {self.path_to_file}."
-                " Returning the measurement resulting from the original read. "
-                "Use a new Reader if you want to read another file."
-            )
-            return self.measurement
-        self.name = name or path_to_file.name
-        self.path_to_file = path_to_file
-        with open(self.path_to_file, "r") as f:
-            for line in f:
-                self.process_line(line)
-        for name in self.column_names:
-            self.column_data[name] = np.array(self.column_data[name])
-
-        data_series_list = []
-        for name, (tcol, vcol) in self.t_and_v_cols.items():
-            tseries = TimeSeries(
-                name=tcol,
-                unit_name=get_column_unit(tcol) or "s",
-                data=self.column_data[tcol],
-                tstamp=self.column_tstamps[tcol],
-            )
-            vseries = ValueSeries(
-                name=name,
-                data=self.column_data[vcol],
-                tseries=tseries,
-                unit_name=get_column_unit(vcol),
-            )
-            data_series_list.append(tseries)
-            data_series_list.append(vseries)
-
-        obj_as_dict = dict(
-            name=self.name,
-            technique=self.technique,
-            reader=self,
-            series_list=data_series_list,
-            tstamp=self.tstamp,
-        )
-        # normally MSMeasurement requires mass aliases, but not cinfdata since it uses
-        # the ixdat convention (actually, ixdat uses the cinfdata convention) of M<x>
-        obj_as_dict.update(kwargs)
-
-        if issubclass(cls, self.measurement_class):
-            self.measurement_class = cls
-
-        self.measurement = self.measurement_class.from_dict(obj_as_dict)
-        self.file_has_been_read = True
-        return self.measurement
-
-    def process_line(self, line):
-        """Call the correct line processing method depending on self.place_in_file"""
-        if self.place_in_file == "header":
-            self.process_header_line(line)
-        elif self.place_in_file == "post_header":
-            if line.strip():  # then we're in the column headers!
-                self.process_column_line(line)
-        elif self.place_in_file == "data":
-            self.process_data_line(line)
-        else:  # just for debugging
-            raise ReadError(f"place_in_file = {self.place_in_file}")
-        self.n_line += 1
-
-    def process_header_line(self, line):
-        """Search line for important metadata and set the relevant attribute of self"""
-        self.header_lines.append(line)
-        if not line.strip():  # the blank lines between the header and the column names
-            self.place_in_file = "post_header"
-        elif "Recorded at" in line:
-            for s in line.split(self.delim):
-                if "Recorded at" not in s:
-                    self.tstamp_list.append(
-                        timestamp_string_to_tstamp(
-                            s.strip()[1:-1],  # remove edge whitespace and quotes.
-                            form="%Y-%m-%d %H:%M:%S",  # like "2017-09-20 13:06:00"
-                        )
-                    )
-            self.tstamp = self.tstamp_list[0]
-
-    def process_column_line(self, line):
-        """Split the line to get the names of the file's data columns"""
-        self.header_lines.append(line)
-        self.column_names = [name.strip() for name in line.split(self.delim)]
-        self.column_data.update({name: [] for name in self.column_names})
-        i = 0  # need a counter to map tstamps to timecols.
-        for col in self.column_names:
-            if col.endswith("-y"):
-                name = col[:-2]
-                tcol = f"{name}-x"
-                if tcol not in self.column_names:
-                    print(f"Warning! No timecol for {col}. Expected {tcol}. Ignoring.")
-                    continue
-                self.t_and_v_cols[name] = (tcol, col)
-                self.column_tstamps[tcol] = self.tstamp_list[i]
-                i += 1
-
-        self.place_in_file = "data"
-
-    def process_data_line(self, line):
-        """Split the line and append the numbers the corresponding data column arrays"""
-        data_strings_from_line = line.strip().split(self.delim)
-        for name, value_string in zip(self.column_names, data_strings_from_line):
-            if value_string:
-                try:
-                    value = float(value_string)
-                except ValueError:
-                    raise ReadError(f"can't parse value string '{value_string}'")
-                self.column_data[name].append(value)
-
-    def print_header(self):
-        """Print the file header including column names. read() must be called first."""
-        header = "".join(self.header_lines)
-        print(header)
-
-
-def get_column_unit(column_name):
-    """Return the unit name of an ixdat column, i.e the part of the name after the '/'"""
-    if column_name.startswith("M") and column_name.endswith("-y"):
-        unit_name = "A"
-    elif column_name.startswith("M") and column_name.endswith("-x"):
-        unit_name = "s"
-    else:
-        # TODO: Figure out how cinfdata represents units for other stuff.
-        #    see https://github.com/ixdat/ixdat/pull/30/files#r811432543, and
-        #    https://github.com/CINF/cinfdata/blob/master/sym-files2/export_data.py#L125
-        unit_name = None
-    return unit_name
+"""Module defining readers for DTU Surfcat's legendary cinfdata system"""
+
+from pathlib import Path
+import numpy as np
+from ..exceptions import ReadError
+from ..data_series import ValueSeries, TimeSeries
+from ..techniques import MSMeasurement
+from .reading_tools import timestamp_string_to_tstamp
+
+
+class CinfdataTXTReader:
+    """A class that reads the text exported by cinfdata's text export functionality
+
+    TODO: We should also have a reader class that downloads the data from cinfdata like
+        `EC_MS`'s `download_cinfdata_set`:
+        https://github.com/ScottSoren/EC_MS/blob/master/src/EC_MS/Data_Importing.py#L711
+
+    Attributes:
+        path_to_file (Path): the location and name of the file read by the reader
+        n_line (int): the number of the last line read by the reader
+        place_in_file (str): The last location in the file read by the reader. This
+            is used internally to tell the reader how to parse each line. Options are:
+            "header", "column names", and "data".
+        header_lines (list of str): a list of the header lines of the files. This
+            includes the column name line. The header can be nicely viewed with the
+            print_header() function.
+        tstamp (str): The unix time corresponding to t=0 for the measurement
+        tstamp_list (list of float): list of epoch tstamps in the file's timestamp line
+        column_tstamps (dict): The unix time corresponding to t=0 for each time column
+        technique (str): The name of the technique
+        column_names (list of str): The names of the data columns in the file
+        t_and_v_cols (dict): {name: (tcol, vcol)} where name is the name of the
+            ValueSeries (e.g. "M2"), tcol is the name of the corresponding time column
+            in the file (e.g. "M2-x"), and vcol is the the name of the value column in
+            the file (e.g. "M2-y).
+        column_data (dict of str: np.array): The data in the file as a dict.
+            Note that the np arrays are the same ones as in the measurement's DataSeries,
+            so this does not waste memory.
+        file_has_been_read (bool): This is used to make sure read() is only successfully
+            called once by the Reader. False until read() is called, then True.
+        measurement (Measurement): The measurement returned by read() when the file is
+            read. self.measureemnt is None before read() is called.
+    """
+
+    delim = "\t"
+
+    def __init__(self):
+        """Initialize a Reader for cinfdata-exported text files. See class docstring."""
+        self.name = None
+        self.path_to_file = None
+        self.n_line = 0
+        self.place_in_file = "header"
+        self.header_lines = []
+        self.tstamp = None
+        self.tstamp_list = []
+        self.column_tstamps = {}
+        self.column_names = []
+        self.t_and_v_cols = {}
+        self.column_data = {}
+        self.technique = "MS"  # TODO: Figure out how to tell if it's something else
+        self.measurement_class = MSMeasurement
+        self.file_has_been_read = False
+        self.measurement = None
+
+    def read(self, path_to_file, name=None, cls=None, **kwargs):
+        """Return an MSMeasurement with the data and metadata recorded in path_to_file
+
+        This loops through the lines of the file, processing one at a time. For header
+        lines, this involves searching for metadata. For the column name line, this
+        involves creating empty arrays for each data series. For the data lines, this
+        involves appending to these arrays. After going through all the lines, it
+        converts the arrays to DataSeries.
+        For cinfdata text files, each value column has its own timecolumn, and they are
+        not necessarily all the same length.
+        Finally, the method returns an ECMeasurement with these DataSeries. The
+        ECMeasurement contains a reference to the reader.
+        All attributes of this reader can be accessed from the
+        measurement as `measurement.reader.attribute_name`.
+
+        Args:
+            path_to_file (Path): The full abs or rel path including the ".txt" extension
+            **kwargs (dict): Key-word arguments are passed to ECMeasurement.__init__
+        """
+        path_to_file = Path(path_to_file) if path_to_file else self.path_to_file
+        if self.file_has_been_read:
+            print(
+                f"This {self.__class__.__name__} has already read {self.path_to_file}."
+                " Returning the measurement resulting from the original read. "
+                "Use a new Reader if you want to read another file."
+            )
+            return self.measurement
+        self.name = name or path_to_file.name
+        self.path_to_file = path_to_file
+        with open(self.path_to_file, "r") as f:
+            for line in f:
+                self.process_line(line)
+        for name in self.column_names:
+            self.column_data[name] = np.array(self.column_data[name])
+
+        data_series_list = []
+        for name, (tcol, vcol) in self.t_and_v_cols.items():
+            tseries = TimeSeries(
+                name=tcol,
+                unit_name=get_column_unit(tcol) or "s",
+                data=self.column_data[tcol],
+                tstamp=self.column_tstamps[tcol],
+            )
+            vseries = ValueSeries(
+                name=name,
+                data=self.column_data[vcol],
+                tseries=tseries,
+                unit_name=get_column_unit(vcol),
+            )
+            data_series_list.append(tseries)
+            data_series_list.append(vseries)
+
+        obj_as_dict = dict(
+            name=self.name,
+            technique=self.technique,
+            reader=self,
+            series_list=data_series_list,
+            tstamp=self.tstamp,
+        )
+        # normally MSMeasurement requires mass aliases, but not cinfdata since it uses
+        # the ixdat convention (actually, ixdat uses the cinfdata convention) of M<x>
+        obj_as_dict.update(kwargs)
+
+        if issubclass(cls, self.measurement_class):
+            self.measurement_class = cls
+
+        self.measurement = self.measurement_class.from_dict(obj_as_dict)
+        self.file_has_been_read = True
+        return self.measurement
+
+    def process_line(self, line):
+        """Call the correct line processing method depending on self.place_in_file"""
+        if self.place_in_file == "header":
+            self.process_header_line(line)
+        elif self.place_in_file == "post_header":
+            if line.strip():  # then we're in the column headers!
+                self.process_column_line(line)
+        elif self.place_in_file == "data":
+            self.process_data_line(line)
+        else:  # just for debugging
+            raise ReadError(f"place_in_file = {self.place_in_file}")
+        self.n_line += 1
+
+    def process_header_line(self, line):
+        """Search line for important metadata and set the relevant attribute of self"""
+        self.header_lines.append(line)
+        if not line.strip():  # the blank lines between the header and the column names
+            self.place_in_file = "post_header"
+        elif "Recorded at" in line:
+            for s in line.split(self.delim):
+                if "Recorded at" not in s:
+                    self.tstamp_list.append(
+                        timestamp_string_to_tstamp(
+                            s.strip()[1:-1],  # remove edge whitespace and quotes.
+                            form="%Y-%m-%d %H:%M:%S",  # like "2017-09-20 13:06:00"
+                        )
+                    )
+            self.tstamp = self.tstamp_list[0]
+
+    def process_column_line(self, line):
+        """Split the line to get the names of the file's data columns"""
+        self.header_lines.append(line)
+        self.column_names = [name.strip() for name in line.split(self.delim)]
+        self.column_data.update({name: [] for name in self.column_names})
+        i = 0  # need a counter to map tstamps to timecols.
+        for col in self.column_names:
+            if col.endswith("-y"):
+                name = col[:-2]
+                tcol = f"{name}-x"
+                if tcol not in self.column_names:
+                    print(f"Warning! No timecol for {col}. Expected {tcol}. Ignoring.")
+                    continue
+                self.t_and_v_cols[name] = (tcol, col)
+                self.column_tstamps[tcol] = self.tstamp_list[i]
+                i += 1
+
+        self.place_in_file = "data"
+
+    def process_data_line(self, line):
+        """Split the line and append the numbers the corresponding data column arrays"""
+        data_strings_from_line = line.strip().split(self.delim)
+        for name, value_string in zip(self.column_names, data_strings_from_line):
+            if value_string:
+                try:
+                    value = float(value_string)
+                except ValueError:
+                    raise ReadError(f"can't parse value string '{value_string}'")
+                self.column_data[name].append(value)
+
+    def print_header(self):
+        """Print the file header including column names. read() must be called first."""
+        header = "".join(self.header_lines)
+        print(header)
+
+
+def get_column_unit(column_name):
+    """Return the unit name of an ixdat column, i.e the part of the name after the '/'"""
+    if column_name.startswith("M") and column_name.endswith("-y"):
+        unit_name = "A"
+    elif column_name.startswith("M") and column_name.endswith("-x"):
+        unit_name = "s"
+    else:
+        # TODO: Figure out how cinfdata represents units for other stuff.
+        #    see https://github.com/ixdat/ixdat/pull/30/files#r811432543, and
+        #    https://github.com/CINF/cinfdata/blob/master/sym-files2/export_data.py#L125
+        unit_name = None
+    return unit_name
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat/readers/cinfdata_db.py` & `ixdat-0.2.9.dev3/src/ixdat/readers/ixdat_csv.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,404 +1,361 @@
-"""Module defining direct DB reader connection to Surfcat's cinfdata system"""
-import warnings
-from .. import Measurement
-from ..data_series import DataSeries, ValueSeries, TimeSeries, Field
-from ..techniques.ms import MSSpectrum
-from ..spectra import Spectrum, SpectrumSeries
-from ..config import plugins
-
-SCALE_TIME_TO_SECONDS = 1e-3
-
-
-class CinfdataDBReader:
-    """A class that connects to cinf_database or read from cache
-    https://cinfdata-dababase-client.readthedocs.io/en/latest/index.html
-
-    Attributes:
-        setup_name (str): The setup name in the DB
-        timestamp (str): Timestamp when the experiment started in YYYY-MM-DD HH:MM:SS
-        units (dict): Dictionary of columns names with corresponding units
-        tstamp (str): The unix time corresponding to t=0 for the measurement
-        technique (str): The name of the technique
-        measurement (Measurement): The measurement returned by read() when the database
-            is read. self.measurement is None before read() is called.
-    """
-
-    def __init__(self):
-        """Initialize a Reader for cinf_database. See class docstring."""
-        self.name = None
-        self.sample_name = None
-        self.setup_name = None
-        self.timestamp = None
-        self.comment = None
-        self.grouping_column = None
-        self.tstamp = None
-        self.data_has_been_fetched = False
-        self.metadata = {}
-        self.technique = "reactor"
-        self.measurement_class = None
-        self.measurement = None
-        self.cinf_db = None
-        self.include_mass_scans = False
-        self.spectrum_list = []
-        self.verbose = False
-
-        plugins.activate_cinfdata()
-
-    def read(self, path_to_file, name=None, cls=None, units=None, **kwargs):
-        """
-        Return a xx-Measurement or Spectrum with the data and metadata recorded from
-        a setup at SurfCat at given timestamp
-        All attributes of this reader can be accessed from the
-        measurement as `measurement.reader.attribute_name`.
-
-        Args:
-        path_to_file (str): Named argument from Measurement Class.
-            Can be used as the setup name in the cinfdatabase
-        **kwargs (dict): Key-word arguments are passed to cinf Measurement.__init__
-            setup_name (str): The setup name in the database default to path_to_file
-            timestamp (str): Timestamp the measurement started given as
-                (YYYY-MM-DD HH:MM:SS)
-        """
-
-        if self.data_has_been_fetched:
-            print(
-                f"This {self.__class__.__name__} has already fetched data from "
-                f" {self.token} grouped by {self.grouping_column}"
-                " Returning the measurement resulting from the original read. "
-                "Use a new Reader if you want to read another file."
-            )
-
-            return self.measurement
-
-        self.measurement_class = kwargs.pop("measurement_class", cls)
-        self.setup_name = kwargs.pop("setup_name", path_to_file)
-        self.timestamp = kwargs.pop("timestamp", None)
-        self.comment = kwargs.pop("comment", None)
-        self.include_mass_scans = kwargs.pop("include_mass_scans", False)
-        self.verbose = kwargs.pop("verbose", False)
-        self.grouping_column = kwargs.pop("group", None)
-
-        # figure out whether to collect data group by 'comment' or 'timestamp'
-
-        if not self.grouping_column:
-            if self.timestamp and not self.comment:
-                self.grouping_column = "time"
-                self.token = self.timestamp
-            elif self.comment and not self.timestamp:
-                self.grouping_column = "comment"
-                self.token = self.comment
-            else:
-                warnings.warn(
-                    "Both a comment and a timestamp is given "
-                    "but no explicit grouping column is set. \n"
-                    f"Defaults to 'time' using '{self.timestamp}'",
-                    stacklevel=2,
-                )
-                self.grouping_column = "time"
-                self.token = self.timestamp
-
-        self.data_has_been_fetched = True
-
-        if issubclass(self.measurement_class, Spectrum):
-            return self.read_spectrums()
-
-        if issubclass(self.measurement_class, Measurement):
-            obj_as_dict = self.read_ms()
-            if issubclass(cls, self.measurement_class):
-                self.measurement_class = cls
-            obj_as_dict.update(kwargs)
-            self.measurement = self.measurement_class.from_dict(obj_as_dict)
-
-            if self.include_mass_scans:
-                if self.verbose:
-                    print("adding mass scans to the measurement")
-                self.add_mass_scans()
-
-            return self.measurement
-
-    def read_ms(self):
-        """Download MS data from cinfdata_database and make corresponding tseries and
-        vseries to place in a dictionary to return.
-        return obj_as_dict (dict)
-        """
-
-        with plugins.cinfdata(
-            setup_name=self.setup_name, grouping_column=self.grouping_column
-        ) as cinf_db:
-
-            self.group_data = cinf_db.get_data_group(
-                self.token, scaling_factors=(SCALE_TIME_TO_SECONDS, None)
-            )
-
-            self.group_meta = cinf_db.get_metadata_group(self.token)
-
-        self.set_sample_name_tstamp_and_name()
-
-        if self.verbose:
-            print("Retriving data from measurement named: ", self.sample_name)
-            print("Measurement started recording on: ", self.timestamp)
-
-        data_series_list = []
-
-        if self.verbose:
-            print("Column names in measurement: ")
-
-        for key in self.group_data.keys():
-            meta = self.group_meta[key]
-            if meta["type"] != 5:  # 5 is specific mass_time measurements
-                continue
-
-            column_name = meta["mass_label"]
-            if self.verbose:
-                print("Col name: ", column_name)
-
-            tcol = self.group_data[key][:, 0]
-            vcol = self.group_data[key][:, 1]
-
-            tseries = TimeSeries(
-                name=column_name + "-x",
-                unit_name=get_column_unit(column_name + "-x") or "s",
-                data=tcol,
-                tstamp=self.tstamp,
-            )
-
-            vseries = ValueSeries(
-                name=column_name,
-                data=vcol,
-                tseries=tseries,
-                unit_name=get_column_unit(column_name + "-y"),
-            )
-            data_series_list.append(tseries)
-            data_series_list.append(vseries)
-
-        obj_as_dict = dict(
-            name=self.name,
-            sample_name=self.sample_name,
-            technique=self.technique,
-            reader=self,
-            series_list=data_series_list,
-            tstamp=self.tstamp,
-            metadata=self.group_meta,
-        )
-
-        if not data_series_list:
-            warnings.warn(
-                f"No mass spec data was found using '{self.token}' "
-                f" and group_column: '{self.grouping_column}'",
-                stacklevel=2,
-            )
-            return None
-
-        return obj_as_dict
-
-    def read_spectrums(self, **kwargs):
-        """
-        Download spectrums from cinfdata_database with either a timestamp or a comment.
-        When all associated spectras have been downloaded and added to
-        self.spectrum_list, this method will try to:
-            1. figure out if any spectras were able to be downloaded; if none,return None
-            2. figure out if this method was used to add mass_scans to another
-               measurement; return list of Spectrum to be added to that measurement
-            3. figure out if one and only one spectrum was downloaded; return that single
-               Spectrum as the first spectrum in self.spectrum_list[0]
-            4. figure out if all spectras downloaded are associated enough to be a
-               SpectrumSeries (They have to be equal in dimensions like mass scans)
-               (multiple mass scans over time versus XPS spectra from different regions);
-               return SpectrumSeries
-            5. If not one and only one Spectrum was read and the multiple of spectras
-               read were not possible to combine in a SpectrumSeries,
-               return self.spectrum_list
-        """
-
-        with plugins.cinfdata(
-            setup_name=self.setup_name, grouping_column=self.grouping_column
-        ) as cinf_db:
-
-            # return dict with measurements as key containing x,y values in a np.array
-            self.group_data = cinf_db.get_data_group(self.token)
-
-            # return dict of meta data associated with the key associated (measurement)
-            self.group_meta = cinf_db.get_metadata_group(self.token)
-
-        # set sample_name, tstamp and measurement name from meta data from cinfdatabase
-        self.set_sample_name_tstamp_and_name()
-
-        self.spectrum_list = []
-        for key in self.group_meta:  # key is unique to each measurement
-            group_type = self.group_meta[key]["type"]
-            if group_type == 2:  # type 2 is unique describing XPS data
-                obj_as_dict = self.get_spectrum_as_dict(key, group_type)
-                obj_as_dict["name"] = self.group_meta[key]["name"]
-                self.spectrum_list.append(self.measurement_class.from_dict(obj_as_dict))
-            elif group_type == 4:  # type 4 is unique describing mass scan
-                obj_as_dict = self.get_spectrum_as_dict(key, group_type)
-                obj_as_dict["name"] = self.sample_name
-                self.spectrum_list.append(self.measurement_class.from_dict(obj_as_dict))
-
-        if not self.spectrum_list:
-            warnings.warn(
-                f"No spectrum was found using '{self.token}' and group_column:"
-                f"'{self.grouping_column}'",
-                stacklevel=2,
-            )
-            return None
-        elif self.include_mass_scans:
-            return self.spectrum_list
-        elif len(self.spectrum_list) == 1:
-            return self.spectrum_list[0]
-        else:
-            try:
-                return SpectrumSeries.from_spectrum_list(self.spectrum_list)
-            except ValueError:
-                warnings.warn(
-                    "Could not return SpectrumSeries from list of spectrums "
-                    f"using '{self.token}' and group column: "
-                    f"'{self.grouping_column}'."
-                    "\nReturn list of all Spectrums.",
-                    stacklevel=2,
-                )
-                return self.spectrum_list
-
-    def get_spectrum_as_dict(self, key, group_type):
-        """Convert spectrum data from cinfdatabase to ixdat dict format with xseries and
-        fields.
-        Return spectrum_as_dict (dict of xseries and Fields)
-        """
-        # Extract x and y data columns and timestamp from group data and metadata
-        x_col = self.group_data[key][:, 0]
-        y_col = self.group_data[key][:, 1]
-        metadata = self.group_meta[key]
-        tstamp = self.group_meta[key]["unixtime"]
-
-        # Create x DataSeries object with appropriate metadata
-        x_series = DataSeries(
-            data=x_col,
-            name=SPECTRUM_METADATA[group_type]["x_name"],
-            unit_name=SPECTRUM_METADATA[group_type]["x_unit_name"],
-        )
-
-        # Create y Field obj with appropriate metadata and x DataSeries as its only axis
-        y_field = Field(
-            data=y_col,
-            name=SPECTRUM_METADATA[group_type]["field_name"],
-            unit_name=SPECTRUM_METADATA[group_type]["field_unit"],
-            axes_series=[x_series],
-        )
-
-        # Create dictionary with spectrum object metadata and x-y Field object
-        spectrum_as_dict = {
-            "sample_name": self.sample_name,
-            "technique": SPECTRUM_METADATA[group_type]["technique"],
-            "field": y_field,
-            "tstamp": tstamp,
-            "reader": self,
-            "metadata": metadata,
-        }
-
-        return spectrum_as_dict
-
-    def add_mass_scans(self):
-        """Add corresponding mass scans to mass_time from 'comment'"""
-        self.include_mass_scans = True
-        self.measurement_class = MSSpectrum
-        self.grouping_column = "comment"
-        self.token = self.sample_name
-        self.spectrum_list = self.read_spectrums()
-
-        if self.verbose:
-            print(f"Using {self.measurement.time_series[-1]} to find end of experiment")
-            print(
-                "Unixtime end of exp "
-                f"{self.measurement.time_series[-1].data[-1] + self.tstamp}"
-            )
-
-        # Find index of the first spectrum to include as mass scans of this measurement
-        first_index = 0
-        if self.spectrum_list[0].tstamp < self.tstamp:
-            for i, spectrum in enumerate(self.spectrum_list):
-                if spectrum.tstamp < self.tstamp:
-                    first_index = i
-                else:
-                    break
-        # Find index of the last spectrum to include as mass scans
-        last_index = -1
-        measurement_end_time = self.measurement.time_series[-1].data[-1] + self.tstamp
-
-        if self.spectrum_list[-1].tstamp > measurement_end_time:
-            for i, spectrum in reversed(list(enumerate(self.spectrum_list))):
-                if spectrum.tstamp >= measurement_end_time:
-                    last_index = i
-                else:
-                    break
-
-        if self.verbose:
-            print(f"Start and End index of spectrum list: {first_index}, {last_index}\n")
-            print(
-                "Tstamp of first and last spectrum in list: "
-                f"{self.spectrum_list[first_index].tstamp}, "
-                f"{self.spectrum_list[last_index].tstamp}"
-            )
-
-        # Create spectrum series and add it to the measurement
-        spectrums_to_add = self.spectrum_list[first_index:last_index]
-        ms_spectra = SpectrumSeries.from_spectrum_list(spectrums_to_add)
-        self.measurement = self.measurement + ms_spectra
-
-    def set_sample_name_tstamp_and_name(self):
-        """Set the sample name and measurement name and tstamp from meta data retrieved
-        from cinfdata"""
-
-        metadata = list(self.group_meta.items())[0][1]
-
-        self.sample_name = None
-        for key_name in ("Comment", "comment"):
-            if key_name in metadata:
-                self.sample_name = metadata[key_name]
-
-        if self.sample_name is None:
-            print("No comment to set as sample_name.")
-
-        self.name = metadata["time"].strftime("%Y-%m-%d %H:%M:%S")
-        self.tstamp = float(metadata["unixtime"])
-
-
-def get_column_unit(column_name):
-    """Return the unit name of an ixdat column, i.e the part of the name after the '/'"""
-    if column_name.startswith("M") and column_name.endswith("-y"):
-        unit_name = "A"
-    elif column_name.startswith("M") and column_name.endswith("-x"):
-        unit_name = "s"
-    elif column_name.startswith("Reactor") and column_name.endswith("pressure-y"):
-        unit_name = "bar"
-    elif not column_name.startswith("Reactor") and column_name.endswith("pressure-y"):
-        unit_name = "mbar"
-    elif column_name.endswith("temperature-y"):
-        unit_name = "celcius"
-    elif column_name.startswith("Flow"):
-        unit_name = "ml/min"
-
-    else:
-        # TODO: Figure out how cinfdata represents units for other stuff.
-        #    see https://github.com/ixdat/ixdat/pull/30/files#r811432543, and
-        #    https://github.com/CINF/cinfdata/blob/master/sym-files2/export_data.py#L125
-        unit_name = None
-
-    return unit_name
-
-
-SPECTRUM_METADATA = {
-    2: {
-        "x_name": "Binding energy / eV",
-        "x_unit_name": "eV",
-        "field_name": "Counts per second",
-        "field_unit": "n/s",
-        "technique": "XPS",
-    },
-    4: {  # type 4 is unique describing msscan
-        "x_name": "Mass [AMU]",
-        "x_unit_name": "m/z",
-        "field_name": "Current",
-        "field_unit": "[A]",
-        "technique": "MS_spectrum",
-    },
-}
+"""Module defining the ixdat csv reader, so ixdat can read the files it exports."""
+
+from pathlib import Path
+import json
+import numpy as np
+import re
+import pandas as pd
+from ..exceptions import ReadError
+from ..data_series import ValueSeries, TimeSeries, DataSeries, Field
+from ..measurements import Measurement
+from ..spectra import Spectrum, SpectrumSeries
+from ..techniques import TECHNIQUE_CLASSES
+
+regular_expressions = {
+    "name": r"^name = (.+)\n",
+    "tstamp": r"tstamp = ([0-9\.]+)\n",
+    "technique": r"technique = (.+)\n",
+    "N_header_lines": r"N_header_lines = ([0-9]+)\n",
+    "backend_name": r"backend_name = (.+)\n",
+    "id": r"id = ([0-9]+)",
+    "timecol": r"timecol '(.+)' for: (?:'(.+)')$",
+    "unit": r"/ \[(.+)\]",
+    "aux_file": r"'(.*)' in file: '(.*)'",
+}
+bad_keys = ("time_step",)
+
+
+class IxdatCSVReader:
+    """A class that reads the csv's made by ixdat.exporters.csv_exporter.CSVExporter
+
+    read() is the important method - it takes the path to the mpt file as argument
+    and returns an ECMeasurement object (ec_measurement) representing that file.
+    The ECMeasurement contains a reference to the BiologicMPTReader object, as
+    ec_measurement.reader. This makes available all the following stuff, likely
+    useful for debugging.
+
+    Attributes:
+        path_to_file (Path): the location and name of the file read by the reader
+        n_line (int): the number of the last line read by the reader
+        place_in_file (str): The last location in the file read by the reader. This
+            is used internally to tell the reader how to parse each line. Options are:
+            "header", "column names", and "data".
+        header_lines (list of str): a list of the header lines of the files. This
+            includes the column name line. The header can be nicely viewed with the
+            print_header() function.
+        tstamp (str): The unix time corresponding to t=0
+        technique (str): The name of the technique
+        N_header_lines (int): The number of lines in the header of the file
+        column_names (list of str): The names of the data columns in the file
+        column_data (dict of str: np.array): The data in the file as a dict.
+            Note that the np arrays are the same ones as in the measurement's DataSeries,
+            so this does not waste memory.
+        file_has_been_read (bool): This is used to make sure read() is only successfully
+            called once by the Reader. False until read() is called, then True.
+        measurement (Measurement): The measurement returned by read() when the file is
+            read. self.measureemnt is None before read() is called.
+    """
+
+    delim = ","
+
+    def __init__(self):
+        """Initialize a Reader for ixdat-exported .csv files. See class docstring."""
+        self.name = None
+        self.path_to_file = None
+        self.n_line = 0  # TODO: decide if this is part of API.
+        # as per https://github.com/ixdat/ixdat/pull/30/files#r816204939
+        self.place_in_file = "header"
+        self.header_lines = []
+        self.tstamp = None
+        self.N_header_lines = None
+        self.timecols = {}
+        self.column_names = []
+        self.column_data = {}
+        self.technique = None
+        self.aux_file_objects = {}
+        self.measurement_class = Measurement
+        self.file_has_been_read = False
+        self.measurement = None
+        self.meas_as_dict = {}
+
+    def read(self, path_to_file, name=None, cls=None, **kwargs):
+        """Return a Measurement with the data and metadata recorded in path_to_file
+
+        This loops through the lines of the file, processing one at a time. For header
+        lines, this involves searching for metadata. For the column name line, this
+        involves creating empty arrays for each data series. For the data lines, this
+        involves appending to these arrays. After going through all the lines, it
+        converts the arrays to DataSeries.
+        The technique is specified in the header, and used to pick the
+        TechniqueMeasurement class.
+        Finally, the method returns a TechniqueMeasurement object `measurement`
+        with these DataSeries. All attributes of this reader can be accessed from the
+        measurement as `measurement.reader.attribute_name`.
+
+        Args:
+            path_to_file (Path): The full abs or rel path including the ".mpt" extension
+            name (str): The name of the measurement to return (defaults to path_to_file)
+            cls (Measurement subclass): The class of measurement to return. By default,
+                cls will be determined from the technique specified in the header of
+                path_to_file.
+            **kwargs (dict): Key-word arguments are passed to ECMeasurement.__init__
+
+        Returns cls: a Measurement of type cls
+        """
+        path_to_file = Path(path_to_file) if path_to_file else self.path_to_file
+        if self.file_has_been_read:
+            print(
+                f"This {self.__class__.__name__} has already read {self.path_to_file}."
+                " Returning the measurement resulting from the original read. "
+                "Use a new Reader if you want to read another file."
+            )
+            return self.measurement
+        self.name = name or path_to_file.name
+        self.path_to_file = path_to_file
+
+        with open(self.path_to_file, "r") as f:
+            for line in f:
+                self.process_line(line)
+
+        for name in self.column_names:
+            self.column_data[name] = np.array(self.column_data[name])
+
+        data_series_dict = {}
+
+        for tcol_name in self.timecols:  # then it's time!
+            data_series_dict[tcol_name] = TimeSeries(
+                name=tcol_name,
+                unit_name=get_column_unit(tcol_name) or "s",
+                data=self.column_data[tcol_name],
+                tstamp=self.tstamp,
+            )
+
+        for column_name, data in self.column_data.items():
+            if column_name in self.timecols:
+                continue
+            try:
+                tcol_name = next(
+                    tcol_name
+                    for tcol_name in self.timecols
+                    if column_name in self.timecols[tcol_name]
+                )
+            except StopIteration:  # debugging
+                raise ReadError(
+                    f"can't find tcol for {column_name}. timecols={self.timecols}"
+                )
+
+            tseries = data_series_dict[tcol_name]
+            vseries = ValueSeries(
+                name=column_name,
+                data=data,
+                tseries=tseries,
+                unit_name=get_column_unit(column_name),
+            )
+            data_series_dict[column_name] = vseries
+
+        data_series_list = list(data_series_dict.values())
+        self.meas_as_dict.update(
+            name=self.name,
+            technique=self.technique,
+            reader=self,
+            series_list=data_series_list,
+            tstamp=self.tstamp,
+        )
+        self.meas_as_dict.update(self.aux_file_objects)
+        self.meas_as_dict.update(kwargs)
+
+        if issubclass(cls, self.measurement_class):
+            self.measurement_class = cls
+
+        self.measurement = self.measurement_class.from_dict(self.meas_as_dict)
+        self.file_has_been_read = True
+        return self.measurement
+
+    def process_line(self, line):
+        """Call the correct line processing method depending on self.place_in_file"""
+        if self.place_in_file == "header":
+            self.process_header_line(line)
+        elif self.place_in_file == "column names":
+            self.process_column_line(line)
+        elif self.place_in_file == "data":
+            self.process_data_line(line)
+        else:  # just for debugging
+            raise ReadError(f"place_in_file = {self.place_in_file}")
+        self.n_line += 1
+
+    def process_header_line(self, line):
+        """Search line for important metadata and set the relevant attribute of self"""
+        self.header_lines.append(line)
+        name_match = re.search(regular_expressions["name"], line)
+        if name_match:
+            self.name = name_match.group(1)
+            return
+        N_head_match = re.search(regular_expressions["N_header_lines"], line)
+        if N_head_match:
+            self.N_header_lines = int(N_head_match.group(1))
+            return
+        timestamp_match = re.search(regular_expressions["tstamp"], line)
+        if timestamp_match:
+            self.tstamp = float(timestamp_match.group(1))
+            return
+        technique_match = re.search(regular_expressions["technique"], line)
+        if technique_match:
+            self.technique = technique_match.group(1)
+            if self.technique in TECHNIQUE_CLASSES:
+                if issubclass(TECHNIQUE_CLASSES[self.technique], self.measurement_class):
+                    self.measurement_class = TECHNIQUE_CLASSES[self.technique]
+            return
+        timecol_match = re.search(regular_expressions["timecol"], line)
+        if timecol_match:
+            tcol = timecol_match.group(1)
+            self.timecols[tcol] = []
+            for vcol in timecol_match.group(2).split("' and '"):
+                self.timecols[tcol].append(vcol)
+            return
+        aux_file_match = re.search(regular_expressions["aux_file"], line)
+        if aux_file_match:
+            aux_file_name = aux_file_match.group(1)
+            aux_file = self.path_to_file.parent / aux_file_match.group(2)
+            self.read_aux_file(aux_file, name=aux_file_name)
+            return
+        if " = " in line:
+            key, value = line.strip().split(" = ")
+            if key in bad_keys:
+                return
+            if key in ("name", "id"):
+                return
+            try:
+                self.meas_as_dict[key] = json.loads(value)
+            except json.decoder.JSONDecodeError:
+                print(f"skipping the following line:\n{line}")
+            return
+
+        if self.N_header_lines and self.n_line >= self.N_header_lines - 2:
+            self.place_in_file = "column names"
+
+    def process_column_line(self, line):
+        """Split the line to get the names of the file's data columns"""
+        self.header_lines.append(line)
+        self.column_names = [name.strip() for name in line.split(self.delim)]
+        self.column_data.update({name: [] for name in self.column_names})
+        self.place_in_file = "data"
+
+    def process_data_line(self, line):
+        """Split the line and append the numbers the corresponding data column arrays"""
+        data_strings_from_line = line.strip().split(self.delim)
+        for name, value_string in zip(self.column_names, data_strings_from_line):
+            if value_string:
+                try:
+                    value = float(value_string)
+                except ValueError:
+                    # That is probably because different columns are different length.
+                    #  so we just skip it!
+                    continue
+                    # raise ReadError(f"can't parse value string '{value_string}'")
+                self.column_data[name].append(value)
+
+    def read_aux_file(self, path_to_aux_file, name):
+        """Read an auxiliary file and include its series list in the measurement"""
+        spec = IxdatSpectrumReader().read(path_to_aux_file, name=name)
+        self.aux_file_objects[name] = spec
+
+    def print_header(self):
+        """Print the file header including column names. read() must be called first."""
+        header = "".join(self.header_lines)
+        print(header)
+
+
+def get_column_unit(column_name):
+    """Return the unit name of an ixdat column, i.e the part of the name after the '/'"""
+    unit_match = re.search(regular_expressions["unit"], column_name)
+    if unit_match:
+        unit_name = unit_match.group(1)
+    else:
+        unit_name = None
+    return unit_name
+
+
+class IxdatSpectrumReader(IxdatCSVReader):
+    """A reader for ixdat spectra."""
+
+    def read(self, path_to_file, name=None, cls=SpectrumSeries, **kwargs):
+        """Read an ixdat spectrum.
+
+        This reads the header with the process_line() function inherited from
+        IxdatCSVReader. Then it uses pandas to read the data.
+
+        Args:
+            path_to_file (Path): The full absolute or relative path including extension
+            name (str): The name of the measurement to return (defaults to path_to_file)
+            cls (Spectrum subclass): The class of measurement to return. By default,
+                cls will be determined from the technique specified in the header of
+                path_to_file.
+            **kwargs (dict): Key-word arguments are passed to ECMeasurement.__init__
+
+        Returns cls: a Spectrum of type cls
+        """
+        path_to_file = Path(path_to_file)
+        self.name = name or path_to_file.name
+        with open(path_to_file, "r") as f:
+            for line in f:
+                if self.place_in_file == "header":
+                    self.process_line(line)
+                else:
+                    break
+        df = pd.read_csv(path_to_file, sep=",", header=self.N_header_lines - 2)
+        if self.technique.endswith("spectrum"):
+            # FIXME: in the future, this needs to cover all spectrum classes
+            x_name, y_name = tuple(df.keys())
+            x = df[x_name].to_numpy()
+            y = df[y_name].to_numpy()
+            cls = cls if issubclass(cls, Spectrum) else Spectrum
+            return cls.from_data(  # see Spectrum.from_data()
+                x,
+                y,
+                self.tstamp,
+                x_name,
+                y_name,
+                name=self.name,
+                technique=self.technique,
+                reader=self,
+            )
+
+        elif self.technique.endswith("spectra"):
+            # FIXME: in the future, this needs to cover all spectrum series classes
+            names = {}
+            units = {}
+            swap_axes = False
+            for line in self.header_lines:
+                for line_start in ("values", "first row", "first column"):
+                    if line.startswith(line_start):
+                        t_x_or_y = re.search("([yxt])=", line).group(1)
+                        names[t_x_or_y] = re.search(r"\'(.*)\'", line).group(1)
+                        units[t_x_or_y] = re.search(r"\[(.*)\]", line).group(1)
+                        if "row" in line_start and t_x_or_y == "t":  # check!
+                            swap_axes = True
+            z1 = np.array([float(key) for key in list(df.keys())[1:]])
+            z1_and_y = df.to_numpy()
+            z0 = z1_and_y[:, 0]
+            y = z1_and_y[:, 1:]
+            if swap_axes:
+                # This is the case if the file was export with spectra_as_rows = False.
+                t = z1
+                x = z0
+                y = y.swapaxes(0, 1)
+            else:
+                t = z0
+                x = z1
+            tseries = TimeSeries(
+                name=names["t"], unit_name=units["t"], data=t, tstamp=self.tstamp
+            )
+            xseries = DataSeries(name=names["x"], unit_name=units["x"], data=x)
+            field = Field(
+                name=names["y"],
+                unit_name=units["y"],
+                data=y,
+                axes_series=[tseries, xseries],
+            )
+            cls = cls if issubclass(cls, SpectrumSeries) else SpectrumSeries
+            return cls.from_field(  # see SpectrumSeries.from_field()
+                field, name=self.name, technique=self.technique, tstamp=self.tstamp
+            )
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat/readers/ivium.py` & `ixdat-0.2.9.dev3/src/ixdat/readers/ivium.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,142 +1,142 @@
-"""This module implements the reader for the text export of Ivium's software"""
-
-import re
-from pathlib import Path
-import pandas as pd
-from ..techniques.ec import ECMeasurement
-from .reading_tools import timestamp_string_to_tstamp, series_list_from_dataframe
-
-IVIUM_ALIASES = {
-    "raw_potential": ("E/V",),
-    "raw_current": ("I/A",),
-    "t": ("time/s",),
-}
-
-
-class IviumDataReader:
-    """Class for reading single ivium files"""
-
-    def read(self, path_to_file, cls=None, name=None, cycle_number=0, **kwargs):
-        """Read the ASCII export from the Ivium software
-
-        Args:
-            path_to_file (Path): The full abs or rel path including the suffix (.txt)
-            cls (Measurement subclass): The Measurement class to return an object of.
-                Defaults to `ECMeasurement`.
-            name (str): The name to use if not the file name
-            cycle_number (int): The cycle number of the data in the file (default is 0)
-
-            **kwargs (dict): Key-word arguments are passed to cls.__init__
-
-        Returns:
-            cls: technique measurement object with the ivium data
-        """
-        self.path_to_file = Path(path_to_file)
-        name = name or self.path_to_file.name
-
-        with open(self.path_to_file, "r") as f:
-            timestring_line = f.readline()  # we need this for tstamp
-            columns_line = f.readline()  # we need this to get the column names
-            first_data_line = f.readline()  # we need this to check the column names
-        tstamp = timestamp_string_to_tstamp(
-            timestring_line.strip(),
-            form="%d/%m/%Y %H:%M:%S",  # like '04/03/2021 19:42:30'
-        )
-
-        # ivium files do something really dumb. They add an extra column of data, which
-        # looks like the measured potential (to complement 'E/V' which is presumably the
-        # setpoint), but don't add the name of this column in the column name line.
-        # So in order for pandas' csv reader to read it, we need assign a name to this
-        # extra column (it becomes 'Unlabeled_1') and specify the column names.
-        # Here we prepare the thus-corrected column name list, `column_names`:
-        column_names = [col.strip() for col in columns_line.split(" ") if col.strip()]
-        first_dat = [dat.strip() for dat in first_data_line.split(" ") if dat.strip()]
-        if len(first_dat) > len(column_names):
-            for i in range(len(first_dat) - len(column_names)):
-                column_names.append(f"unlabeled_{i}")
-
-        # And now we can read the data. Notice also the variable whitespace delimiter.
-        dataframe = pd.read_csv(
-            self.path_to_file, delimiter=r"\s+", header=1, names=column_names
-        )
-
-        # All that's left is getting the data from the dataframe into DataSeries and
-        # into the Measurement, starting with the TimeSeries:
-
-        data_series_list = series_list_from_dataframe(
-            dataframe,
-            time_name="time/s",
-            tstamp=tstamp,
-            unit_finding_function=get_column_unit,
-            cycle=cycle_number,
-        )
-        # With the `series_list` ready, we prepare the Measurement dictionary and
-        # return the Measurement object:
-        obj_as_dict = dict(
-            name=name,
-            technique="EC",
-            reader=self,
-            aliases=IVIUM_ALIASES,
-            series_list=data_series_list,
-            tstamp=tstamp,
-        )
-        obj_as_dict.update(kwargs)
-
-        if not issubclass(ECMeasurement, cls):
-            cls = ECMeasurement
-        return cls.from_dict(obj_as_dict)
-
-
-class IviumDatasetReader:
-    """Class for reading sets of ivium files exported together"""
-
-    def read(self, path_to_file, cls=None, name=None, **kwargs):
-        """Return a measurement containing the data of an ivium dataset,
-
-        An ivium dataset is a group of ivium files exported together. They share a
-        folder and a base name, and are suffixed "_1", "_2", etc.
-
-        Args:
-            path_to_file (Path or str): `Path(path_to_file).parent` is interpreted as the
-                folder where the files of the ivium dataset is. `Path(path_to_file).name`
-                up to the first "_" is interpreted as the shared start of the files in
-                the dataset. You can thus use the base name of the exported files or
-                the full path of any one of them.
-            cls (Measurement class): The measurement class. Defaults to ECMeasurement.
-            name (str): The name of the dataset. Defaults to the base name of the dataset
-            kwargs: key-word arguments are included in the dictionary for cls.from_dict()
-
-        Returns cls or ECMeasurement: A measurement object with the ivium data
-        """
-        self.path_to_file = Path(path_to_file)
-
-        folder = self.path_to_file.parent
-        base_name = self.path_to_file.name
-        if re.search(r"_[0-9]", base_name):
-            base_name = base_name.rpartition("_")[0]
-        name = name or base_name
-
-        # With two list comprehensions, we get the Measurement object for each file
-        # in the folder who's name starts with base_name:
-        all_file_paths = [f for f in folder.iterdir() if f.name.startswith(base_name)]
-        component_measurements = [
-            IviumDataReader().read(f, cls=cls, cycle_number=i)
-            for i, f in enumerate(all_file_paths)
-        ]
-
-        # Now we append these using the from_component_measurements class method of the
-        # right TechniqueMeasurement class, and return the result.
-        if not cls:
-            from ..techniques.ec import ECMeasurement
-
-            cls = ECMeasurement
-        measurement = cls.from_component_measurements(
-            component_measurements, name=name, **kwargs
-        )
-        return measurement
-
-
-def get_column_unit(column_name):
-    """Return the unit name of an ivium column, i.e what follows the first '/'."""
-    if "/" in column_name:
-        return column_name.split("/", 1)[1]
+"""This module implements the reader for the text export of Ivium's software"""
+
+import re
+from pathlib import Path
+import pandas as pd
+from ..techniques.ec import ECMeasurement
+from .reading_tools import timestamp_string_to_tstamp, series_list_from_dataframe
+
+IVIUM_ALIASES = {
+    "raw_potential": ("E/V",),
+    "raw_current": ("I/A",),
+    "t": ("time/s",),
+}
+
+
+class IviumDataReader:
+    """Class for reading single ivium files"""
+
+    def read(self, path_to_file, cls=None, name=None, cycle_number=0, **kwargs):
+        """Read the ASCII export from the Ivium software
+
+        Args:
+            path_to_file (Path): The full abs or rel path including the suffix (.txt)
+            cls (Measurement subclass): The Measurement class to return an object of.
+                Defaults to `ECMeasurement`.
+            name (str): The name to use if not the file name
+            cycle_number (int): The cycle number of the data in the file (default is 0)
+
+            **kwargs (dict): Key-word arguments are passed to cls.__init__
+
+        Returns:
+            cls: technique measurement object with the ivium data
+        """
+        self.path_to_file = Path(path_to_file)
+        name = name or self.path_to_file.name
+
+        with open(self.path_to_file, "r") as f:
+            timestring_line = f.readline()  # we need this for tstamp
+            columns_line = f.readline()  # we need this to get the column names
+            first_data_line = f.readline()  # we need this to check the column names
+        tstamp = timestamp_string_to_tstamp(
+            timestring_line.strip(),
+            form="%d/%m/%Y %H:%M:%S",  # like '04/03/2021 19:42:30'
+        )
+
+        # ivium files do something really dumb. They add an extra column of data, which
+        # looks like the measured potential (to complement 'E/V' which is presumably the
+        # setpoint), but don't add the name of this column in the column name line.
+        # So in order for pandas' csv reader to read it, we need assign a name to this
+        # extra column (it becomes 'Unlabeled_1') and specify the column names.
+        # Here we prepare the thus-corrected column name list, `column_names`:
+        column_names = [col.strip() for col in columns_line.split(" ") if col.strip()]
+        first_dat = [dat.strip() for dat in first_data_line.split(" ") if dat.strip()]
+        if len(first_dat) > len(column_names):
+            for i in range(len(first_dat) - len(column_names)):
+                column_names.append(f"unlabeled_{i}")
+
+        # And now we can read the data. Notice also the variable whitespace delimiter.
+        dataframe = pd.read_csv(
+            self.path_to_file, delimiter=r"\s+", header=1, names=column_names
+        )
+
+        # All that's left is getting the data from the dataframe into DataSeries and
+        # into the Measurement, starting with the TimeSeries:
+
+        data_series_list = series_list_from_dataframe(
+            dataframe,
+            time_name="time/s",
+            tstamp=tstamp,
+            unit_finding_function=get_column_unit,
+            cycle=cycle_number,
+        )
+        # With the `series_list` ready, we prepare the Measurement dictionary and
+        # return the Measurement object:
+        obj_as_dict = dict(
+            name=name,
+            technique="EC",
+            reader=self,
+            aliases=IVIUM_ALIASES,
+            series_list=data_series_list,
+            tstamp=tstamp,
+        )
+        obj_as_dict.update(kwargs)
+
+        if not issubclass(ECMeasurement, cls):
+            cls = ECMeasurement
+        return cls.from_dict(obj_as_dict)
+
+
+class IviumDatasetReader:
+    """Class for reading sets of ivium files exported together"""
+
+    def read(self, path_to_file, cls=None, name=None, **kwargs):
+        """Return a measurement containing the data of an ivium dataset,
+
+        An ivium dataset is a group of ivium files exported together. They share a
+        folder and a base name, and are suffixed "_1", "_2", etc.
+
+        Args:
+            path_to_file (Path or str): `Path(path_to_file).parent` is interpreted as the
+                folder where the files of the ivium dataset is. `Path(path_to_file).name`
+                up to the first "_" is interpreted as the shared start of the files in
+                the dataset. You can thus use the base name of the exported files or
+                the full path of any one of them.
+            cls (Measurement class): The measurement class. Defaults to ECMeasurement.
+            name (str): The name of the dataset. Defaults to the base name of the dataset
+            kwargs: key-word arguments are included in the dictionary for cls.from_dict()
+
+        Returns cls or ECMeasurement: A measurement object with the ivium data
+        """
+        self.path_to_file = Path(path_to_file)
+
+        folder = self.path_to_file.parent
+        base_name = self.path_to_file.name
+        if re.search(r"_[0-9]", base_name):
+            base_name = base_name.rpartition("_")[0]
+        name = name or base_name
+
+        # With two list comprehensions, we get the Measurement object for each file
+        # in the folder who's name starts with base_name:
+        all_file_paths = [f for f in folder.iterdir() if f.name.startswith(base_name)]
+        component_measurements = [
+            IviumDataReader().read(f, cls=cls, cycle_number=i)
+            for i, f in enumerate(all_file_paths)
+        ]
+
+        # Now we append these using the from_component_measurements class method of the
+        # right TechniqueMeasurement class, and return the result.
+        if not cls:
+            from ..techniques.ec import ECMeasurement
+
+            cls = ECMeasurement
+        measurement = cls.from_component_measurements(
+            component_measurements, name=name, **kwargs
+        )
+        return measurement
+
+
+def get_column_unit(column_name):
+    """Return the unit name of an ivium column, i.e what follows the first '/'."""
+    if "/" in column_name:
+        return column_name.split("/", 1)[1]
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat/readers/msrh_sec.py` & `ixdat-0.2.9.dev3/src/ixdat/readers/msrh_sec.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,258 +1,258 @@
-from pathlib import Path  # noqa
-import numpy as np
-import pandas as pd
-import time
-from .reading_tools import prompt_for_tstamp
-from ..techniques import TECHNIQUE_CLASSES
-from ..data_series import DataSeries, TimeSeries, ValueSeries, Field
-from ..spectra import Spectrum, SpectrumSeries
-from ..techniques.analysis_tools import calc_t_using_scan_rate
-
-
-class MsrhSECReader:
-    """A reader for SEC saved in three files: spectra vs U; wavelengths; current vs U"""
-
-    def read(
-        self,
-        path_to_file,
-        path_to_ref_spec_file,
-        path_to_U_J_file,
-        scan_rate,
-        tstamp=None,
-        cls=None,
-    ):
-        """Read potential-dep. SEC data from 3 csv's to return a SpectroECMeasurement
-
-        The function is well-commented so take a look at the source
-
-        Args:
-            path_to_file (Path or str): The full path to the file containing the
-                spectra data. This file has voltage in the first row, and a first
-                column with an arbitrary counter which has to be replaced by wavelength.
-            path_to_ref_spec_file (Path or str): The full path to the file containing
-                the wavelenth data, together usually with the adsorption-free spectrum.
-                The length of the columns should be the same as in the spectrum data
-                but in practice is a few points longer. The excess points at the starts
-                of the columns are discarded.
-            path_to_U_J_file (Path or str): The full path to the file containing the
-                current data vs potential. The columns may be reversed in order. In the
-                end the potential in the spectra file will be retained and the potential
-                here used to interpolate the current onto the spectra file's potential.
-            scan_rate (float): Scan rate in [mV/s]. This is used to figure out the
-                measurement's time variable, as time is bizarrely not included in any
-                of the data files.
-            tstamp (float): Timestamp. If None, the user will be prompted for the
-                measurement start time or whether to use the file creation time. This is
-                necessary because tstamp is also not included in any of the files but is
-                central to how ixdat organizes data. If you're sure that tstamp doesn't
-                matter for you, put e.g. tstamp=1 to suppress the prompt.
-            cls (Measurement subclass): The class of measurement to return. Defaults to
-                SpectroECMeasurement.
-        """
-        # us pandas to load the data from the csv files into dataframes:
-        sec_df = pd.read_csv(path_to_file)
-        # ^ Note the first row, containing potential, will become the keys. The first
-        #   column, containing an arbitrary counter, is included in the data.
-        ref_df = pd.read_csv(path_to_ref_spec_file, names=["wavelength", "counts"])
-        EI_df = pd.read_csv(path_to_U_J_file, names=["U", "J"])
-
-        # The spectra need (i) the first colum with the arbitrary counter to be
-        #    discarded and (ii) axes switched so that wavelength is the inner axis
-        #    (axis=1). The outer axis (axis=0) then spans potential or, eq., time:
-        spectra = sec_df.to_numpy()[:, 1:].swapaxes(0, 1)
-        # The potential comes from the keys of that data, discarding the first column:
-        U = np.array([float(key) for key in sec_df.keys()])[1:]
-        # We get time from this potential and the scan rate, with a helper function:
-        t = calc_t_using_scan_rate(U, dvdt=scan_rate * 1e-3)
-        # If they didn't provide a tstamp, we have to prompt for it.
-        tstamp = tstamp or prompt_for_tstamp(path_to_file)
-        if tstamp == "now":
-            tstamp = time.time()
-        # Ready to define the measurement's TimeSeries:
-        tseries = TimeSeries("time from scan rate", unit_name="s", data=t, tstamp=tstamp)
-
-        # The wavelength comes from the reference spectrum file.
-        wl = ref_df["wavelength"].to_numpy()
-        excess_wl_points = len(wl) - spectra.shape[1]
-        # ^ This is how many points to discard to line up with sec data
-        #   (1 or 2 points in the example data).
-        wl = wl[excess_wl_points:]
-        ref_signal = ref_df["counts"].to_numpy()[excess_wl_points:]
-
-        # Now we're ready to define all the spectrum DataSeries:
-
-        # wavelength is independent variable --> simple DataSeries
-        wl_series = DataSeries("wavelength / [nm]", "nm", wl)
-        # The reference spectrum spans a space defined by wavelength:
-        reference = Field(
-            name="reference",
-            unit_name="counts",
-            axes_series=[wl_series],
-            data=ref_signal,
-        )
-        reference_spectrum = Spectrum.from_field(reference)
-        # The spectra span a space defined by time and wavelength:
-        spectra = Field(
-            name="spectra",
-            unit_name="counts",
-            axes_series=[tseries, wl_series],
-            data=spectra,
-        )
-        spectrum_series = SpectrumSeries.from_field(
-            spectra, tstamp=tstamp, continuous=True
-        )
-
-        # Now we process the current and potential:
-        U_0 = EI_df["U"].to_numpy()  # ... but we'll actually use U from the sec data
-        J_0 = EI_df["J"].to_numpy() * 1e3  # 1e3 converts [A] to [mA]
-        if U_0[0] > U_0[-1]:  # Need the potential in the EC file to be increasing:
-            U_0 = np.flip(U_0)
-            J_0 = np.flip(J_0)
-        # Since the "real" potential is in the sec data, we need to interpolate the
-        #   current onto it:
-        J = np.interp(U, U_0, J_0)
-        # and now we're ready to define the electrochemical DataSeries:
-        U_series = ValueSeries("raw potential / [V]", "V", U, tseries=tseries)
-        J_series = ValueSeries("raw current / [mA]", "mA", J, tseries=tseries)
-
-        # put all our DataSeries together:
-        series_list = [
-            tseries,
-            U_series,
-            J_series,
-        ]
-
-        # Figure out which measurement class to return. Use SEC unless this read
-        #   function is provided an even more specific technique class:
-        measurement_class = TECHNIQUE_CLASSES["EC-Optical"]
-        if issubclass(cls, measurement_class):
-            measurement_class = cls
-
-        # and initiate the measurement:
-        measurement = measurement_class(
-            name=str(path_to_file),
-            tstamp=tstamp,
-            series_list=series_list,
-            aliases={
-                "raw_potential": (U_series.name,),
-                "raw_current": (J_series.name,),
-                "t": (tseries.name,),
-            },
-            spectrum_series=spectrum_series,
-            reference_spectrum=reference_spectrum,
-        )
-
-        return measurement
-
-
-class MsrhSECDecayReader:
-    def read(
-        self,
-        path_to_file,
-        path_to_ref_spec_file,
-        path_to_t_J_file,
-        path_to_t_U_file,
-        tstamp=None,
-        cls=None,
-    ):
-        """Read time-dependent SEC data from 4 csv's to return a SpectroECMeasurement
-
-        The function works in a very similar way to MsrhSECReader.read().
-
-        Args:
-            path_to_file (Path or str): The full path to the file containing the
-                spectra data. This file has time in the first row, and a first
-                column with an arbitrary counter which has to be replaced by wavelength.
-            path_to_ref_spec_file (Path or str): The full path to the file containing
-                the wavelenth data, together usually with the adsorption-free spectrum.
-                The length of the columns should be the same as in the spectrum data
-                but in practice is a few points longer. The excess points at the starts
-                of the columns are discarded.
-            path_to_t_U_file (Path or str): The full path to the file containing the
-                potential data vs time.
-            path_to_t_J_file (Path or str): The full path to the file containing the
-                current data vs time.
-            tstamp (float): Timestamp. If None, the user will be prompted for the
-                measurement start time or whether to use the file creation time. This is
-                necessary because tstamp is also not included in any of the files but is
-                central to how ixdat organizes data. If you're sure that tstamp doesn't
-                matter for you, put e.g. tstamp=1 to suppress the prompt.
-            cls (Measurement subclass): The class of measurement to return. Defaults to
-                SpectroECMeasurement.
-        """
-
-        sec_df = pd.read_csv(path_to_file)
-        ref_df = pd.read_csv(path_to_ref_spec_file, names=["wavelength", "counts"])
-        t_U_df = pd.read_csv(path_to_t_U_file, names=["t", "U"])
-        t_J_df = pd.read_csv(path_to_t_J_file, names=["t", "J"])
-
-        t_and_spectra = sec_df.to_numpy()
-        spectra = t_and_spectra[:, 1:].swapaxes(0, 1)
-        t_spectra = np.array([float(key) for key in sec_df.keys()])[1:]
-
-        wl = ref_df["wavelength"].to_numpy()
-        excess_wl_points = len(wl) - spectra.shape[1]
-        wl = wl[excess_wl_points:]
-        ref_signal = ref_df["counts"].to_numpy()[excess_wl_points:]
-
-        wl_series = DataSeries("wavelength / [nm]", "nm", wl)
-        reference_spectrum = Spectrum.from_field(
-            Field(
-                name="reference",
-                unit_name="counts",
-                axes_series=[wl_series],
-                data=np.array(ref_signal),
-            ),
-            tstamp=tstamp,
-        )
-
-        U = t_U_df["U"].to_numpy()
-        t_U = t_U_df["t"].to_numpy()
-        J = t_J_df["J"].to_numpy() * 1e3  # Convert [A] to [mA]
-        t_E = t_J_df["t"].to_numpy()
-
-        tstamp = tstamp or prompt_for_tstamp(path_to_file)
-        if tstamp == "now":
-            tstamp = time.time()
-
-        tseries_J = TimeSeries("t for current", "s", data=t_E, tstamp=tstamp)
-        tseries_U = TimeSeries("t for potential", "s", data=t_U, tstamp=tstamp)
-        tseries_spectra = TimeSeries("t for spectra", "s", t_spectra, tstamp)
-        U_series = ValueSeries("raw potential / [V]", "V", U, tseries=tseries_U)
-        J_series = ValueSeries("raw current / [mA]", "mA", J, tseries=tseries_J)
-        spectrum_series = SpectrumSeries.from_field(
-            Field(
-                name="spectra",
-                unit_name="counts",
-                axes_series=[tseries_spectra, wl_series],
-                data=spectra,
-            ),
-            tstamp=tstamp,
-        )
-        series_list = [
-            tseries_J,
-            tseries_U,
-            tseries_spectra,
-            U_series,
-            J_series,
-            wl_series,
-        ]
-
-        measurement_class = TECHNIQUE_CLASSES["EC-Optical"]
-        if issubclass(cls, measurement_class):
-            measurement_class = cls
-
-        measurement = measurement_class(
-            name=str(path_to_file),
-            tstamp=tstamp,
-            series_list=series_list,
-            aliases={
-                "raw_potential": (U_series.name,),
-                "raw_current": (J_series.name,),
-                "t": (tseries_U.name,),
-            },
-            spectrum_series=spectrum_series,
-            reference_spectrum=reference_spectrum,
-        )
-
-        return measurement
+from pathlib import Path  # noqa
+import numpy as np
+import pandas as pd
+import time
+from .reading_tools import prompt_for_tstamp
+from ..techniques import TECHNIQUE_CLASSES
+from ..data_series import DataSeries, TimeSeries, ValueSeries, Field
+from ..spectra import Spectrum, SpectrumSeries
+from ..techniques.analysis_tools import calc_t_using_scan_rate
+
+
+class MsrhSECReader:
+    """A reader for SEC saved in three files: spectra vs U; wavelengths; current vs U"""
+
+    def read(
+        self,
+        path_to_file,
+        path_to_ref_spec_file,
+        path_to_U_J_file,
+        scan_rate,
+        tstamp=None,
+        cls=None,
+    ):
+        """Read potential-dep. SEC data from 3 csv's to return a SpectroECMeasurement
+
+        The function is well-commented so take a look at the source
+
+        Args:
+            path_to_file (Path or str): The full path to the file containing the
+                spectra data. This file has voltage in the first row, and a first
+                column with an arbitrary counter which has to be replaced by wavelength.
+            path_to_ref_spec_file (Path or str): The full path to the file containing
+                the wavelenth data, together usually with the adsorption-free spectrum.
+                The length of the columns should be the same as in the spectrum data
+                but in practice is a few points longer. The excess points at the starts
+                of the columns are discarded.
+            path_to_U_J_file (Path or str): The full path to the file containing the
+                current data vs potential. The columns may be reversed in order. In the
+                end the potential in the spectra file will be retained and the potential
+                here used to interpolate the current onto the spectra file's potential.
+            scan_rate (float): Scan rate in [mV/s]. This is used to figure out the
+                measurement's time variable, as time is bizarrely not included in any
+                of the data files.
+            tstamp (float): Timestamp. If None, the user will be prompted for the
+                measurement start time or whether to use the file creation time. This is
+                necessary because tstamp is also not included in any of the files but is
+                central to how ixdat organizes data. If you're sure that tstamp doesn't
+                matter for you, put e.g. tstamp=1 to suppress the prompt.
+            cls (Measurement subclass): The class of measurement to return. Defaults to
+                SpectroECMeasurement.
+        """
+        # us pandas to load the data from the csv files into dataframes:
+        sec_df = pd.read_csv(path_to_file)
+        # ^ Note the first row, containing potential, will become the keys. The first
+        #   column, containing an arbitrary counter, is included in the data.
+        ref_df = pd.read_csv(path_to_ref_spec_file, names=["wavelength", "counts"])
+        EI_df = pd.read_csv(path_to_U_J_file, names=["U", "J"])
+
+        # The spectra need (i) the first colum with the arbitrary counter to be
+        #    discarded and (ii) axes switched so that wavelength is the inner axis
+        #    (axis=1). The outer axis (axis=0) then spans potential or, eq., time:
+        spectra = sec_df.to_numpy()[:, 1:].swapaxes(0, 1)
+        # The potential comes from the keys of that data, discarding the first column:
+        U = np.array([float(key) for key in sec_df.keys()])[1:]
+        # We get time from this potential and the scan rate, with a helper function:
+        t = calc_t_using_scan_rate(U, dvdt=scan_rate * 1e-3)
+        # If they didn't provide a tstamp, we have to prompt for it.
+        tstamp = tstamp or prompt_for_tstamp(path_to_file)
+        if tstamp == "now":
+            tstamp = time.time()
+        # Ready to define the measurement's TimeSeries:
+        tseries = TimeSeries("time from scan rate", unit_name="s", data=t, tstamp=tstamp)
+
+        # The wavelength comes from the reference spectrum file.
+        wl = ref_df["wavelength"].to_numpy()
+        excess_wl_points = len(wl) - spectra.shape[1]
+        # ^ This is how many points to discard to line up with sec data
+        #   (1 or 2 points in the example data).
+        wl = wl[excess_wl_points:]
+        ref_signal = ref_df["counts"].to_numpy()[excess_wl_points:]
+
+        # Now we're ready to define all the spectrum DataSeries:
+
+        # wavelength is independent variable --> simple DataSeries
+        wl_series = DataSeries("wavelength / [nm]", "nm", wl)
+        # The reference spectrum spans a space defined by wavelength:
+        reference = Field(
+            name="reference",
+            unit_name="counts",
+            axes_series=[wl_series],
+            data=ref_signal,
+        )
+        reference_spectrum = Spectrum.from_field(reference)
+        # The spectra span a space defined by time and wavelength:
+        spectra = Field(
+            name="spectra",
+            unit_name="counts",
+            axes_series=[tseries, wl_series],
+            data=spectra,
+        )
+        spectrum_series = SpectrumSeries.from_field(
+            spectra, tstamp=tstamp, continuous=True
+        )
+
+        # Now we process the current and potential:
+        U_0 = EI_df["U"].to_numpy()  # ... but we'll actually use U from the sec data
+        J_0 = EI_df["J"].to_numpy() * 1e3  # 1e3 converts [A] to [mA]
+        if U_0[0] > U_0[-1]:  # Need the potential in the EC file to be increasing:
+            U_0 = np.flip(U_0)
+            J_0 = np.flip(J_0)
+        # Since the "real" potential is in the sec data, we need to interpolate the
+        #   current onto it:
+        J = np.interp(U, U_0, J_0)
+        # and now we're ready to define the electrochemical DataSeries:
+        U_series = ValueSeries("raw potential / [V]", "V", U, tseries=tseries)
+        J_series = ValueSeries("raw current / [mA]", "mA", J, tseries=tseries)
+
+        # put all our DataSeries together:
+        series_list = [
+            tseries,
+            U_series,
+            J_series,
+        ]
+
+        # Figure out which measurement class to return. Use SEC unless this read
+        #   function is provided an even more specific technique class:
+        measurement_class = TECHNIQUE_CLASSES["EC-Optical"]
+        if issubclass(cls, measurement_class):
+            measurement_class = cls
+
+        # and initiate the measurement:
+        measurement = measurement_class(
+            name=str(path_to_file),
+            tstamp=tstamp,
+            series_list=series_list,
+            aliases={
+                "raw_potential": (U_series.name,),
+                "raw_current": (J_series.name,),
+                "t": (tseries.name,),
+            },
+            spectrum_series=spectrum_series,
+            reference_spectrum=reference_spectrum,
+        )
+
+        return measurement
+
+
+class MsrhSECDecayReader:
+    def read(
+        self,
+        path_to_file,
+        path_to_ref_spec_file,
+        path_to_t_J_file,
+        path_to_t_U_file,
+        tstamp=None,
+        cls=None,
+    ):
+        """Read time-dependent SEC data from 4 csv's to return a SpectroECMeasurement
+
+        The function works in a very similar way to MsrhSECReader.read().
+
+        Args:
+            path_to_file (Path or str): The full path to the file containing the
+                spectra data. This file has time in the first row, and a first
+                column with an arbitrary counter which has to be replaced by wavelength.
+            path_to_ref_spec_file (Path or str): The full path to the file containing
+                the wavelenth data, together usually with the adsorption-free spectrum.
+                The length of the columns should be the same as in the spectrum data
+                but in practice is a few points longer. The excess points at the starts
+                of the columns are discarded.
+            path_to_t_U_file (Path or str): The full path to the file containing the
+                potential data vs time.
+            path_to_t_J_file (Path or str): The full path to the file containing the
+                current data vs time.
+            tstamp (float): Timestamp. If None, the user will be prompted for the
+                measurement start time or whether to use the file creation time. This is
+                necessary because tstamp is also not included in any of the files but is
+                central to how ixdat organizes data. If you're sure that tstamp doesn't
+                matter for you, put e.g. tstamp=1 to suppress the prompt.
+            cls (Measurement subclass): The class of measurement to return. Defaults to
+                SpectroECMeasurement.
+        """
+
+        sec_df = pd.read_csv(path_to_file)
+        ref_df = pd.read_csv(path_to_ref_spec_file, names=["wavelength", "counts"])
+        t_U_df = pd.read_csv(path_to_t_U_file, names=["t", "U"])
+        t_J_df = pd.read_csv(path_to_t_J_file, names=["t", "J"])
+
+        t_and_spectra = sec_df.to_numpy()
+        spectra = t_and_spectra[:, 1:].swapaxes(0, 1)
+        t_spectra = np.array([float(key) for key in sec_df.keys()])[1:]
+
+        wl = ref_df["wavelength"].to_numpy()
+        excess_wl_points = len(wl) - spectra.shape[1]
+        wl = wl[excess_wl_points:]
+        ref_signal = ref_df["counts"].to_numpy()[excess_wl_points:]
+
+        wl_series = DataSeries("wavelength / [nm]", "nm", wl)
+        reference_spectrum = Spectrum.from_field(
+            Field(
+                name="reference",
+                unit_name="counts",
+                axes_series=[wl_series],
+                data=np.array(ref_signal),
+            ),
+            tstamp=tstamp,
+        )
+
+        U = t_U_df["U"].to_numpy()
+        t_U = t_U_df["t"].to_numpy()
+        J = t_J_df["J"].to_numpy() * 1e3  # Convert [A] to [mA]
+        t_E = t_J_df["t"].to_numpy()
+
+        tstamp = tstamp or prompt_for_tstamp(path_to_file)
+        if tstamp == "now":
+            tstamp = time.time()
+
+        tseries_J = TimeSeries("t for current", "s", data=t_E, tstamp=tstamp)
+        tseries_U = TimeSeries("t for potential", "s", data=t_U, tstamp=tstamp)
+        tseries_spectra = TimeSeries("t for spectra", "s", t_spectra, tstamp)
+        U_series = ValueSeries("raw potential / [V]", "V", U, tseries=tseries_U)
+        J_series = ValueSeries("raw current / [mA]", "mA", J, tseries=tseries_J)
+        spectrum_series = SpectrumSeries.from_field(
+            Field(
+                name="spectra",
+                unit_name="counts",
+                axes_series=[tseries_spectra, wl_series],
+                data=spectra,
+            ),
+            tstamp=tstamp,
+        )
+        series_list = [
+            tseries_J,
+            tseries_U,
+            tseries_spectra,
+            U_series,
+            J_series,
+            wl_series,
+        ]
+
+        measurement_class = TECHNIQUE_CLASSES["EC-Optical"]
+        if issubclass(cls, measurement_class):
+            measurement_class = cls
+
+        measurement = measurement_class(
+            name=str(path_to_file),
+            tstamp=tstamp,
+            series_list=series_list,
+            aliases={
+                "raw_potential": (U_series.name,),
+                "raw_current": (J_series.name,),
+                "t": (tseries_U.name,),
+            },
+            spectrum_series=spectrum_series,
+            reference_spectrum=reference_spectrum,
+        )
+
+        return measurement
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat/readers/pfeiffer.py` & `ixdat-0.2.9.dev3/src/ixdat/readers/pfeiffer.py`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,79 +1,79 @@
-"""This module implements the reader for Pfeiffer Vacuum's PV Mass Spec software"""
-
-import re
-from pathlib import Path
-import pandas as pd
-from .reading_tools import timestamp_string_to_tstamp, series_list_from_dataframe
-from ..techniques import MSMeasurement
-
-
-class PVMassSpecReader:
-    """A reader for (advanced) MID files exported from PVMassSpec ('... - Bin.dat')"""
-
-    def read(self, path_to_file, cls=None, name=None, **kwargs):
-        """Return a Measurement with the (advanced) MID data in the PVMassSpec file
-
-        Args:
-            path_to_file (Path or str): a path to the file exported by PVMassSpec with
-                (advanced) MID data. This file is typically exported with a name that
-                ends in '- Bin.dat', and with the timestamp in the file name. Note
-                that the file can be renamed, as the original name is in the file,
-                and the timestamp is read from there.
-            cls (Measurement subclass): The technique class of which to return an object.
-                Defaults to MSMeasurement.
-            name (str): The name of the measurement. Defaults to Path(path_to_file).name
-            kwargs: key-word args are used to initiate the measurement via cls.as_dict()
-
-        Return cls: The measurement object
-        """
-        self.path_to_file = Path(path_to_file)
-        name = name or self.path_to_file.name
-        with open(path_to_file, "r") as f:
-            # timestamp is on the the third line, which we select here:
-            tstamp_line = [f.readline() for _ in range(3)][-1]
-        tstamp = timestamp_string_to_tstamp(
-            tstamp_line.split(".")[-2][-19:],  # last 19 characters before the last '.'
-            form="%m-%d-%Y %H'%M'%S",  # like "03-02-2021 12'58'40"
-        )
-        df = pd.read_csv(self.path_to_file, header=6, delimiter="\t")
-        # PV MassSpec calls masses <x>_amu, information we need to pass on to
-        # MSMeasurement, so that the data will be accessible by the 'M<x>' mass string.
-        aliases = {
-            mass_from_column_name(key): [key] for key in df.keys() if "_amu" in key
-        }
-        series_list = series_list_from_dataframe(
-            df,
-            tstamp=tstamp,
-            time_name="Time Relative (sec)",
-            unit_finding_function=get_column_unit,
-        )
-        meas_as_dict = {
-            "name": name,
-            "tstamp": tstamp,
-            "series_list": series_list,
-            "aliases": aliases,
-            "technique": "MS",
-        }
-        meas_as_dict.update(kwargs)
-        cls = cls or MSMeasurement
-        return cls.from_dict(meas_as_dict)
-
-
-class PVMassSpecScanReader:
-    """A reader for mass spectra files exported from PVMassSpec ('... - Scan.dat')"""
-
-    pass
-
-
-def mass_from_column_name(mass):
-    """Return the PVMassSpec mass 'M<x>' given the column name '<x>_amu' as string"""
-    return f"M{mass.split('_')[0]}"
-
-
-def get_column_unit(column_name):
-    """Return the unit name of an ivium column, i.e what follows the first '/'."""
-    unit_match = re.search(r"\((.*)\)$", column_name)
-    if unit_match:
-        return unit_match.group(1)
-    elif "amu" in column_name:
-        return "A"
+"""This module implements the reader for Pfeiffer Vacuum's PV Mass Spec software"""
+
+import re
+from pathlib import Path
+import pandas as pd
+from .reading_tools import timestamp_string_to_tstamp, series_list_from_dataframe
+from ..techniques import MSMeasurement
+
+
+class PVMassSpecReader:
+    """A reader for (advanced) MID files exported from PVMassSpec ('... - Bin.dat')"""
+
+    def read(self, path_to_file, cls=None, name=None, **kwargs):
+        """Return a Measurement with the (advanced) MID data in the PVMassSpec file
+
+        Args:
+            path_to_file (Path or str): a path to the file exported by PVMassSpec with
+                (advanced) MID data. This file is typically exported with a name that
+                ends in '- Bin.dat', and with the timestamp in the file name. Note
+                that the file can be renamed, as the original name is in the file,
+                and the timestamp is read from there.
+            cls (Measurement subclass): The technique class of which to return an object.
+                Defaults to MSMeasurement.
+            name (str): The name of the measurement. Defaults to Path(path_to_file).name
+            kwargs: key-word args are used to initiate the measurement via cls.as_dict()
+
+        Return cls: The measurement object
+        """
+        self.path_to_file = Path(path_to_file)
+        name = name or self.path_to_file.name
+        with open(path_to_file, "r") as f:
+            # timestamp is on the the third line, which we select here:
+            tstamp_line = [f.readline() for _ in range(3)][-1]
+        tstamp = timestamp_string_to_tstamp(
+            tstamp_line.split(".")[-2][-19:],  # last 19 characters before the last '.'
+            form="%m-%d-%Y %H'%M'%S",  # like "03-02-2021 12'58'40"
+        )
+        df = pd.read_csv(self.path_to_file, header=6, delimiter="\t")
+        # PV MassSpec calls masses <x>_amu, information we need to pass on to
+        # MSMeasurement, so that the data will be accessible by the 'M<x>' mass string.
+        aliases = {
+            mass_from_column_name(key): [key] for key in df.keys() if "_amu" in key
+        }
+        series_list = series_list_from_dataframe(
+            df,
+            tstamp=tstamp,
+            time_name="Time Relative (sec)",
+            unit_finding_function=get_column_unit,
+        )
+        meas_as_dict = {
+            "name": name,
+            "tstamp": tstamp,
+            "series_list": series_list,
+            "aliases": aliases,
+            "technique": "MS",
+        }
+        meas_as_dict.update(kwargs)
+        cls = cls or MSMeasurement
+        return cls.from_dict(meas_as_dict)
+
+
+class PVMassSpecScanReader:
+    """A reader for mass spectra files exported from PVMassSpec ('... - Scan.dat')"""
+
+    pass
+
+
+def mass_from_column_name(mass):
+    """Return the PVMassSpec mass 'M<x>' given the column name '<x>_amu' as string"""
+    return f"M{mass.split('_')[0]}"
+
+
+def get_column_unit(column_name):
+    """Return the unit name of an ivium column, i.e what follows the first '/'."""
+    unit_match = re.search(r"\((.*)\)$", column_name)
+    if unit_match:
+        return unit_match.group(1)
+    elif "amu" in column_name:
+        return "A"
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat/readers/qexafs.py` & `ixdat-0.2.9.dev3/src/ixdat/readers/qexafs.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,95 +1,95 @@
-"""Readers for 'qexafs' files exported by Diamond's B18-Core"""
-
-import pandas as pd
-from .reading_tools import timestamp_string_to_tstamp
-from .. import Spectrum
-from ..spectra import MultiSpectrum
-from ..data_series import DataSeries, Field
-
-
-class QexafsDATReader:
-    def read(
-        self,
-        path_to_file,
-        cls=Spectrum,
-        technique=None,
-        y_name=None,
-        ref_name=None,
-        **kwargs
-    ):
-        """Read the .dat file exported by Diamond B18-Core.
-
-        Args:
-            path_to_file (str or Path): The path to the .dat file
-            cls (Spectrum subclass): The class to return an object of (if you use this
-                reader via the read() method of a Spectrum sub-class, cls will
-                automatically be that subclass. Defaults to `None`, meaning that the
-                class will be determined by `technique` and will be `MultiSpectrum` if
-                no technique is provided.
-            technique (str): The technique to read. This may determine which data is
-                incorporated into a `Spectrum` object. Options:
-                    i. "XAS". Returns a `Spectrum` object with `x` from the first column
-                       (e.g. "qexafs_energy") and `y` based on `y_name` and `ref_name`
-                If no technique is requested and cls is None or `Spectrum`, a
-                `MultiSpectrum` will be returned including all the data of the .dat file.
-            y_name (str): The name of the column to use as the y data. Defaults to
-                "QexafsFFI0" for "xas".
-            ref_name (str): The name of the column by which the y data should be
-                normalized. By default it becomes "I0" unless `y_name` ends with "I0".
-                Set `ref_name` to "none" (as a string) to suppress normalization to "I0".
-        """
-        tstamp = None
-        with open(path_to_file, "r") as f:
-            for i, line in enumerate(f):
-                if not line.startswith("#"):
-                    # we're out of the header.
-                    break
-                if "Date:" in line:
-                    timestamp_string = line.split("Date:")[1].strip()
-                    tstamp = timestamp_string_to_tstamp(
-                        timestamp_string,
-                        form="%a, %d %B %Y %H:%M:%S BST"
-                        # like "Fri, 13 May 2022 19:21:24 BST"
-                    )
-
-        df = pd.read_csv(path_to_file, sep="\t", header=i - 1)
-
-        # these lines remove "#" and whitespace from the beginning of column names:
-        # see https://pandas.pydata.org/docs/user_guide/text.html#string-methods
-        df.columns = df.columns.str.strip("#")
-        df.columns = df.columns.str.strip()
-
-        x_name = df.keys()[0]
-        x = df[x_name].to_numpy()
-        x_unit = "eV"
-        xseries = DataSeries(name=x_name, unit_name=x_unit, data=x)
-        if technique == "XAS":
-            y_name = y_name or "QexafsFFI0"
-            if not y_name.endswith("I0"):
-                ref_name = ref_name or "I0"
-            y = df[y_name].to_numpy()
-            if ref_name and not ref_name == "none":
-                y = y / df[ref_name].to_numpy()
-                unit_name = ""
-            else:
-                unit_name = "counts"
-            yseries = DataSeries(name=y_name, unit_name=unit_name, data=y)
-            return cls.from_series(
-                xseries,
-                yseries,
-                tstamp=tstamp,
-                technique=technique,
-                name=str(path_to_file),
-                **kwargs
-            )
-
-        fields = []
-        for y_name in df.keys()[1:]:
-            y = df[y_name].to_numpy()
-            fields.append(
-                Field(name=y_name, unit_name="", data=y, axes_series=[xseries])
-                # TODO: a unit-finding function
-            )
-        return MultiSpectrum(
-            name=str(path_to_file), fields=fields, tstamp=tstamp, technique=technique
-        )
+"""Readers for 'qexafs' files exported by Diamond's B18-Core"""
+
+import pandas as pd
+from .reading_tools import timestamp_string_to_tstamp
+from .. import Spectrum
+from ..spectra import MultiSpectrum
+from ..data_series import DataSeries, Field
+
+
+class QexafsDATReader:
+    def read(
+        self,
+        path_to_file,
+        cls=Spectrum,
+        technique=None,
+        y_name=None,
+        ref_name=None,
+        **kwargs
+    ):
+        """Read the .dat file exported by Diamond B18-Core.
+
+        Args:
+            path_to_file (str or Path): The path to the .dat file
+            cls (Spectrum subclass): The class to return an object of (if you use this
+                reader via the read() method of a Spectrum sub-class, cls will
+                automatically be that subclass. Defaults to `None`, meaning that the
+                class will be determined by `technique` and will be `MultiSpectrum` if
+                no technique is provided.
+            technique (str): The technique to read. This may determine which data is
+                incorporated into a `Spectrum` object. Options:
+                    i. "XAS". Returns a `Spectrum` object with `x` from the first column
+                       (e.g. "qexafs_energy") and `y` based on `y_name` and `ref_name`
+                If no technique is requested and cls is None or `Spectrum`, a
+                `MultiSpectrum` will be returned including all the data of the .dat file.
+            y_name (str): The name of the column to use as the y data. Defaults to
+                "QexafsFFI0" for "xas".
+            ref_name (str): The name of the column by which the y data should be
+                normalized. By default it becomes "I0" unless `y_name` ends with "I0".
+                Set `ref_name` to "none" (as a string) to suppress normalization to "I0".
+        """
+        tstamp = None
+        with open(path_to_file, "r") as f:
+            for i, line in enumerate(f):
+                if not line.startswith("#"):
+                    # we're out of the header.
+                    break
+                if "Date:" in line:
+                    timestamp_string = line.split("Date:")[1].strip()
+                    tstamp = timestamp_string_to_tstamp(
+                        timestamp_string,
+                        form="%a, %d %B %Y %H:%M:%S BST"
+                        # like "Fri, 13 May 2022 19:21:24 BST"
+                    )
+
+        df = pd.read_csv(path_to_file, sep="\t", header=i - 1)
+
+        # these lines remove "#" and whitespace from the beginning of column names:
+        # see https://pandas.pydata.org/docs/user_guide/text.html#string-methods
+        df.columns = df.columns.str.strip("#")
+        df.columns = df.columns.str.strip()
+
+        x_name = df.keys()[0]
+        x = df[x_name].to_numpy()
+        x_unit = "eV"
+        xseries = DataSeries(name=x_name, unit_name=x_unit, data=x)
+        if technique == "XAS":
+            y_name = y_name or "QexafsFFI0"
+            if not y_name.endswith("I0"):
+                ref_name = ref_name or "I0"
+            y = df[y_name].to_numpy()
+            if ref_name and not ref_name == "none":
+                y = y / df[ref_name].to_numpy()
+                unit_name = ""
+            else:
+                unit_name = "counts"
+            yseries = DataSeries(name=y_name, unit_name=unit_name, data=y)
+            return cls.from_series(
+                xseries,
+                yseries,
+                tstamp=tstamp,
+                technique=technique,
+                name=str(path_to_file),
+                **kwargs
+            )
+
+        fields = []
+        for y_name in df.keys()[1:]:
+            y = df[y_name].to_numpy()
+            fields.append(
+                Field(name=y_name, unit_name="", data=y, axes_series=[xseries])
+                # TODO: a unit-finding function
+            )
+        return MultiSpectrum(
+            name=str(path_to_file), fields=fields, tstamp=tstamp, technique=technique
+        )
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat/readers/reading_tools.py` & `ixdat-0.2.9.dev3/src/ixdat/readers/reading_tools.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,184 +1,184 @@
-"""Module with possibly general-use tools for readers"""
-
-from pathlib import Path
-import time
-import urllib.request
-from ..config import config
-from ..exceptions import ReadError
-from ..measurements import TimeSeries, ValueSeries, ConstantValue
-
-
-STANDARD_TIMESTAMP_FORM = "%d/%m/%Y %H:%M:%S"  # like '31/12/2020 23:59:59'
-USA_TIMESTAMP_FORM = "%m/%d/%Y %H:%M:%S"  # like '12/31/2020 23:59:59'
-FLOAT_MATCH = "[-]?\\d+[\\.]?\\d*(?:e[-+]?\\d+)?"  # matches floats like '5' or '-2.3e+5'
-DEFAULT_READER_NAMES = {
-    ".mpt": "biologic",
-    ".mpr": "biologic",
-    ".tsv": "zilien",
-    ".xrdml": "xrdml",
-    ".avg": "avantage",
-}
-
-
-def get_default_reader_name(path_to_file):
-    """Return a default reader if available given a file's full name with suffix"""
-    return DEFAULT_READER_NAMES.get(Path(path_to_file).suffix)
-
-
-def timestamp_string_to_tstamp(
-    timestamp_string,
-    form=None,
-    forms=(STANDARD_TIMESTAMP_FORM,),
-):
-    """Return the unix timestamp as a float by parsing timestamp_string
-
-    Args:
-        timestamp_string (str): The timestamp as read in the .mpt file
-        form (str): The format string used by time.strptime (string-parse time). This is
-            optional and overrides `forms` if given.
-        forms (iter of str): The formats you want to try for time.strptime, defaults to
-            the standard timestamp form.
-    """
-    if form:
-        forms = (form,)
-    for form in forms:
-        try:
-            return time.mktime(time.strptime(timestamp_string, form))
-        except ValueError:
-            continue
-
-    raise ReadError(f"couldn't parse timestamp_string='{timestamp_string}')")
-
-
-def prompt_for_tstamp(path_to_file, default="creation", form=STANDARD_TIMESTAMP_FORM):
-    """Return the tstamp resulting from a prompt to enter a timestamp, or a default
-
-    Args:
-        path_to_file (Path): The file of the measurement that we're getting a tstamp for
-        default (str or float): What to use as the tstamp if the user does not enter one.
-            This can be a tstamp as a float, or "creation" to use the file creation time,
-            or "now" to use `time.time()`.
-        form (str): The specification string for the timestamp format. Defaults to
-            `ixdat.readers.reading_tools.STANDARD_TIMESTAMP_FORM`
-    """
-    path_to_file = Path(path_to_file)
-
-    if default == "creation":
-        default_tstamp = path_to_file.stat().st_mtime
-    elif default == "now":
-        default_tstamp = time.time()
-    elif type(default) in (int, float):
-        default_tstamp = default
-    else:
-        raise TypeError("`default` must be a number or 'creation' or 'now'.")
-    default_timestring = time.strftime(form, time.localtime(default_tstamp))
-
-    tstamp = None
-    timestamp_string = "Try at least once."
-    while timestamp_string:
-        timestamp_string = input(
-            f"Please input the timestamp for the measurement at {path_to_file}.\n"
-            f"Please use the format {form}.\n"
-            "Enter nothing to use the default default,"
-            f" '{default}', which is '{default_timestring}'."
-        )
-        if timestamp_string:
-            try:
-                tstamp = time.mktime(time.strptime(timestamp_string, form))
-            except ValueError:
-                print(
-                    f"Could not parse '{timestamp_string}' according as '{form}'.\n"
-                    f"Try again or enter nothing to use the default."
-                )
-            else:
-                break
-    return tstamp or default_tstamp
-
-
-def series_list_from_dataframe(
-    dataframe, time_name, tstamp, unit_finding_function, **kwargs
-):
-    """Return a list of DataSeries with the data in a pandas dataframe.
-
-    The first series in the returned list is the one shared TimeSeries.
-
-    Args:
-        dataframe (pandas dataframe): The dataframe. Column names are used as series
-            names, data is taken with series.to_numpy(). The dataframe can only have one
-            TimeSeries (if there are more than one, pandas is probably not the right
-            format anyway, since it requires columns be the same length).
-        time_name (str): The name of the column to use as the TimeSeries
-        tstamp (float): The timestamp
-        unit_finding_function (function): A function which takes a column name as a
-            string and returns its unit.
-        kwargs: Additional key-word arguments are interpreted as constants to include
-            in the data series list as `ConstantValue`s.
-    """
-    t = dataframe[time_name].to_numpy()
-    tseries = TimeSeries(name=time_name, unit_name="s", data=t, tstamp=tstamp)
-    data_series_list = [tseries]
-    for column_name, series in dataframe.items():
-        if column_name == time_name:
-            continue
-        data_series_list.append(
-            ValueSeries(
-                name=column_name,
-                unit_name=unit_finding_function(column_name),
-                data=series.to_numpy(),
-                tseries=tseries,
-            )
-        )
-    for key, value in kwargs.items():
-        data_series_list.append(
-            ConstantValue(name=key, unit_name="", data=value, tseries=tseries)
-        )
-    return data_series_list
-
-
-def url_to_file(url, file_name="temp", directory=None):
-    """Copy the contents of the url to a temporary file and return that file's Path."""
-    directory = directory or config.ixdat_temp_dir
-    suffix = "." + str(url).split(".")[-1]
-    path_to_file = (directory / file_name).with_suffix(suffix)
-    urllib.request.urlretrieve(url, path_to_file)
-    return path_to_file
-
-
-def get_file_list(path_to_file_start=None, part=None, suffix=None):
-    """Get a list of files given their shared start of part.
-
-    Use either `path_to_file_start` OR `part`.
-
-    Args:
-        path_to_file_start (Path or str): The path to the files to read including
-            the shared start of the file name: `Path(path_to_file).parent` is
-            interpreted as the folder where the file are.
-            `Path(path_to_file).name` is interpreted as the shared start of the files
-            to be appended.
-            Alternatively, path_to_file_start can be a folder, in which case all
-            files in that folder (with the specified suffix) are included.
-        part (Path or str): A path where the folder is the folder containing data
-            and the name is a part of the name of each of the files to be read and
-            combined. Not to be used together with `path_to_file_start`.
-        suffix (str): If a suffix is given, only files with the specified ending are
-            added to the file list
-    """
-    file_list = []
-    if path_to_file_start:
-        path_to_file_start = Path(path_to_file_start)
-        if path_to_file_start.is_dir():
-            file_list = [f for f in path_to_file_start.iterdir() if f.is_file()]
-        else:
-            folder = path_to_file_start.parent
-            base_name = path_to_file_start.name
-            file_list = [f for f in folder.iterdir() if f.name.startswith(base_name)]
-    elif part:
-        folder = Path(part).parent
-        part_name = Path(part).name
-        file_list = [f for f in folder.iterdir() if part_name in f.name]
-    if suffix:
-        if not suffix.startswith("."):
-            # So that the user can type e.g. `suffix="mpt"` as well as `suffix=".mpt"`
-            suffix = "." + suffix
-        file_list = [f for f in file_list if f.suffix == suffix]
-    return file_list
+"""Module with possibly general-use tools for readers"""
+
+from pathlib import Path
+import time
+import urllib.request
+from ..config import config
+from ..exceptions import ReadError
+from ..measurements import TimeSeries, ValueSeries, ConstantValue
+
+
+STANDARD_TIMESTAMP_FORM = "%d/%m/%Y %H:%M:%S"  # like '31/12/2020 23:59:59'
+USA_TIMESTAMP_FORM = "%m/%d/%Y %H:%M:%S"  # like '12/31/2020 23:59:59'
+FLOAT_MATCH = "[-]?\\d+[\\.]?\\d*(?:e[-+]?\\d+)?"  # matches floats like '5' or '-2.3e+5'
+DEFAULT_READER_NAMES = {
+    ".mpt": "biologic",
+    ".mpr": "biologic",
+    ".tsv": "zilien",
+    ".xrdml": "xrdml",
+    ".avg": "avantage",
+}
+
+
+def get_default_reader_name(path_to_file):
+    """Return a default reader if available given a file's full name with suffix"""
+    return DEFAULT_READER_NAMES.get(Path(path_to_file).suffix)
+
+
+def timestamp_string_to_tstamp(
+    timestamp_string,
+    form=None,
+    forms=(STANDARD_TIMESTAMP_FORM,),
+):
+    """Return the unix timestamp as a float by parsing timestamp_string
+
+    Args:
+        timestamp_string (str): The timestamp as read in the .mpt file
+        form (str): The format string used by time.strptime (string-parse time). This is
+            optional and overrides `forms` if given.
+        forms (iter of str): The formats you want to try for time.strptime, defaults to
+            the standard timestamp form.
+    """
+    if form:
+        forms = (form,)
+    for form in forms:
+        try:
+            return time.mktime(time.strptime(timestamp_string, form))
+        except ValueError:
+            continue
+
+    raise ReadError(f"couldn't parse timestamp_string='{timestamp_string}')")
+
+
+def prompt_for_tstamp(path_to_file, default="creation", form=STANDARD_TIMESTAMP_FORM):
+    """Return the tstamp resulting from a prompt to enter a timestamp, or a default
+
+    Args:
+        path_to_file (Path): The file of the measurement that we're getting a tstamp for
+        default (str or float): What to use as the tstamp if the user does not enter one.
+            This can be a tstamp as a float, or "creation" to use the file creation time,
+            or "now" to use `time.time()`.
+        form (str): The specification string for the timestamp format. Defaults to
+            `ixdat.readers.reading_tools.STANDARD_TIMESTAMP_FORM`
+    """
+    path_to_file = Path(path_to_file)
+
+    if default == "creation":
+        default_tstamp = path_to_file.stat().st_mtime
+    elif default == "now":
+        default_tstamp = time.time()
+    elif type(default) in (int, float):
+        default_tstamp = default
+    else:
+        raise TypeError("`default` must be a number or 'creation' or 'now'.")
+    default_timestring = time.strftime(form, time.localtime(default_tstamp))
+
+    tstamp = None
+    timestamp_string = "Try at least once."
+    while timestamp_string:
+        timestamp_string = input(
+            f"Please input the timestamp for the measurement at {path_to_file}.\n"
+            f"Please use the format {form}.\n"
+            "Enter nothing to use the default default,"
+            f" '{default}', which is '{default_timestring}'."
+        )
+        if timestamp_string:
+            try:
+                tstamp = time.mktime(time.strptime(timestamp_string, form))
+            except ValueError:
+                print(
+                    f"Could not parse '{timestamp_string}' according as '{form}'.\n"
+                    f"Try again or enter nothing to use the default."
+                )
+            else:
+                break
+    return tstamp or default_tstamp
+
+
+def series_list_from_dataframe(
+    dataframe, time_name, tstamp, unit_finding_function, **kwargs
+):
+    """Return a list of DataSeries with the data in a pandas dataframe.
+
+    The first series in the returned list is the one shared TimeSeries.
+
+    Args:
+        dataframe (pandas dataframe): The dataframe. Column names are used as series
+            names, data is taken with series.to_numpy(). The dataframe can only have one
+            TimeSeries (if there are more than one, pandas is probably not the right
+            format anyway, since it requires columns be the same length).
+        time_name (str): The name of the column to use as the TimeSeries
+        tstamp (float): The timestamp
+        unit_finding_function (function): A function which takes a column name as a
+            string and returns its unit.
+        kwargs: Additional key-word arguments are interpreted as constants to include
+            in the data series list as `ConstantValue`s.
+    """
+    t = dataframe[time_name].to_numpy()
+    tseries = TimeSeries(name=time_name, unit_name="s", data=t, tstamp=tstamp)
+    data_series_list = [tseries]
+    for column_name, series in dataframe.items():
+        if column_name == time_name:
+            continue
+        data_series_list.append(
+            ValueSeries(
+                name=column_name,
+                unit_name=unit_finding_function(column_name),
+                data=series.to_numpy(),
+                tseries=tseries,
+            )
+        )
+    for key, value in kwargs.items():
+        data_series_list.append(
+            ConstantValue(name=key, unit_name="", data=value, tseries=tseries)
+        )
+    return data_series_list
+
+
+def url_to_file(url, file_name="temp", directory=None):
+    """Copy the contents of the url to a temporary file and return that file's Path."""
+    directory = directory or config.ixdat_temp_dir
+    suffix = "." + str(url).split(".")[-1]
+    path_to_file = (directory / file_name).with_suffix(suffix)
+    urllib.request.urlretrieve(url, path_to_file)
+    return path_to_file
+
+
+def get_file_list(path_to_file_start=None, part=None, suffix=None):
+    """Get a list of files given their shared start of part.
+
+    Use either `path_to_file_start` OR `part`.
+
+    Args:
+        path_to_file_start (Path or str): The path to the files to read including
+            the shared start of the file name: `Path(path_to_file).parent` is
+            interpreted as the folder where the file are.
+            `Path(path_to_file).name` is interpreted as the shared start of the files
+            to be appended.
+            Alternatively, path_to_file_start can be a folder, in which case all
+            files in that folder (with the specified suffix) are included.
+        part (Path or str): A path where the folder is the folder containing data
+            and the name is a part of the name of each of the files to be read and
+            combined. Not to be used together with `path_to_file_start`.
+        suffix (str): If a suffix is given, only files with the specified ending are
+            added to the file list
+    """
+    file_list = []
+    if path_to_file_start:
+        path_to_file_start = Path(path_to_file_start)
+        if path_to_file_start.is_dir():
+            file_list = [f for f in path_to_file_start.iterdir() if f.is_file()]
+        else:
+            folder = path_to_file_start.parent
+            base_name = path_to_file_start.name
+            file_list = [f for f in folder.iterdir() if f.name.startswith(base_name)]
+    elif part:
+        folder = Path(part).parent
+        part_name = Path(part).name
+        file_list = [f for f in folder.iterdir() if part_name in f.name]
+    if suffix:
+        if not suffix.startswith("."):
+            # So that the user can type e.g. `suffix="mpt"` as well as `suffix=".mpt"`
+            suffix = "." + suffix
+        file_list = [f for f in file_list if f.suffix == suffix]
+    return file_list
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat/readers/rgasoft.py` & `ixdat-0.2.9.dev3/src/ixdat/readers/rgasoft.py`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,43 +1,43 @@
-"""A reader for text exports from the potentiostat software of CH Instruments"""
-
-from .ec_ms_pkl import measurement_from_ec_ms_dataset
-from ..techniques import MSMeasurement
-
-
-class StanfordRGASoftReader:
-    path_to_file = None
-
-    def read(self, path_to_file, cls=None):
-        """Read a .txt file exported by CH Instruments software.
-
-        TODO: Write a new reader that doesn't use the old EC_MS package
-
-        Args:
-            path_to_file (Path or str): The file to read
-            cls (Measurement subclass): The class to return. Defaults to ECMeasuremnt
-        """
-        try:
-            from EC_MS import Dataset
-        except ImportError:
-            print(
-                "The ixdat StanfordRGASoftReader relies on the EC_MS package.\n"
-                "Use `pip install EC_MS`. \n"
-                "Alternatively considering writing a new Reader for ixdat!"
-            )
-
-        # with open(path_to_file, "r") as f:
-        #     timestamp_string = f.readline().strip()
-        # tstamp = timestamp_string_to_tstamp(
-        #     timestamp_string,
-        #     form="%b %d, %Y  %I:%M:%S %p",  # like "Mar 05, 2020  09:50:34 AM"
-        # )   # ^ For later. EC_MS actually gets this right.
-
-        self.path_to_file = path_to_file
-        cls = cls if (cls and not issubclass(MSMeasurement, cls)) else MSMeasurement
-        ec_ms_dataset = Dataset(
-            path_to_file,
-            data_type="RGA",  # tstamp=tstamp
-        )
-        return measurement_from_ec_ms_dataset(
-            ec_ms_dataset.data, cls=cls, reader=self, technique="MS"
-        )
+"""A reader for text exports from the potentiostat software of CH Instruments"""
+
+from .ec_ms_pkl import measurement_from_ec_ms_dataset
+from ..techniques import MSMeasurement
+
+
+class StanfordRGASoftReader:
+    path_to_file = None
+
+    def read(self, path_to_file, cls=None):
+        """Read a .txt file exported by CH Instruments software.
+
+        TODO: Write a new reader that doesn't use the old EC_MS package
+
+        Args:
+            path_to_file (Path or str): The file to read
+            cls (Measurement subclass): The class to return. Defaults to ECMeasuremnt
+        """
+        try:
+            from EC_MS import Dataset
+        except ImportError:
+            print(
+                "The ixdat StanfordRGASoftReader relies on the EC_MS package.\n"
+                "Use `pip install EC_MS`. \n"
+                "Alternatively considering writing a new Reader for ixdat!"
+            )
+
+        # with open(path_to_file, "r") as f:
+        #     timestamp_string = f.readline().strip()
+        # tstamp = timestamp_string_to_tstamp(
+        #     timestamp_string,
+        #     form="%b %d, %Y  %I:%M:%S %p",  # like "Mar 05, 2020  09:50:34 AM"
+        # )   # ^ For later. EC_MS actually gets this right.
+
+        self.path_to_file = path_to_file
+        cls = cls if (cls and not issubclass(MSMeasurement, cls)) else MSMeasurement
+        ec_ms_dataset = Dataset(
+            path_to_file,
+            data_type="RGA",  # tstamp=tstamp
+        )
+        return measurement_from_ec_ms_dataset(
+            ec_ms_dataset.data, cls=cls, reader=self, technique="MS"
+        )
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat/readers/xrdml.py` & `ixdat-0.2.9.dev3/src/ixdat/readers/xrdml.py`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,48 +1,48 @@
-"""This module defines the reader of .xrdml files from, for example, Empyrion XRD"""
-
-from ixdat import Spectrum
-from ixdat.data_series import DataSeries, Field
-from xml.dom.minidom import parse
-import numpy as np
-
-
-class XRDMLReader:
-    def read(self, path_to_file, cls=None, **kwargs):
-        """Read an XRDML file.
-
-        TODO: Finish tutorial here and improve use of xml parser:
-          https://realpython.com/python-xml-parser/
-
-        Args:
-            path_to_file (str or Path): The path to the .xrdml file to read
-            cls (Spectrum class): The class to return an object of.
-                Defaults to `Spectrum`.
-            kwargs: Additional keyword arguments are passed on to `cls.from_field`
-        """
-        cls = cls or Spectrum
-        with open(path_to_file, "r") as f:
-            document = parse(f)
-        datapoint_node = document.getElementsByTagName("dataPoints")[0]
-        position_nodes = [
-            node for node in datapoint_node.childNodes if "positions" in str(node)
-        ]
-        start_position_element = position_nodes[0].getElementsByTagName("startPosition")[
-            0
-        ]
-        end_position_element = position_nodes[0].getElementsByTagName("endPosition")[0]
-        x_min = float(start_position_element.childNodes[0].data)
-        x_max = float(end_position_element.childNodes[0].data)
-        intensity_parent_node = [
-            node
-            for node in datapoint_node.childNodes
-            if "intensities" in str(node) or "counts" in str(node)
-        ][0]
-        intensity_node = intensity_parent_node.childNodes[0]
-        intensity_string = intensity_node.data
-        y_vec = np.fromstring(intensity_string, dtype=float, sep=" ")
-        x_vec = np.linspace(x_min, x_max, len(y_vec))
-        xseries = DataSeries(name="two theta", unit_name="degree", data=x_vec)
-        field = Field(
-            name="intensity", unit_name="counts", data=y_vec, axes_series=[xseries]
-        )
-        return cls.from_field(field, **kwargs)
+"""This module defines the reader of .xrdml files from, for example, Empyrion XRD"""
+
+from ixdat import Spectrum
+from ixdat.data_series import DataSeries, Field
+from xml.dom.minidom import parse
+import numpy as np
+
+
+class XRDMLReader:
+    def read(self, path_to_file, cls=None, **kwargs):
+        """Read an XRDML file.
+
+        TODO: Finish tutorial here and improve use of xml parser:
+          https://realpython.com/python-xml-parser/
+
+        Args:
+            path_to_file (str or Path): The path to the .xrdml file to read
+            cls (Spectrum class): The class to return an object of.
+                Defaults to `Spectrum`.
+            kwargs: Additional keyword arguments are passed on to `cls.from_field`
+        """
+        cls = cls or Spectrum
+        with open(path_to_file, "r") as f:
+            document = parse(f)
+        datapoint_node = document.getElementsByTagName("dataPoints")[0]
+        position_nodes = [
+            node for node in datapoint_node.childNodes if "positions" in str(node)
+        ]
+        start_position_element = position_nodes[0].getElementsByTagName("startPosition")[
+            0
+        ]
+        end_position_element = position_nodes[0].getElementsByTagName("endPosition")[0]
+        x_min = float(start_position_element.childNodes[0].data)
+        x_max = float(end_position_element.childNodes[0].data)
+        intensity_parent_node = [
+            node
+            for node in datapoint_node.childNodes
+            if "intensities" in str(node) or "counts" in str(node)
+        ][0]
+        intensity_node = intensity_parent_node.childNodes[0]
+        intensity_string = intensity_node.data
+        y_vec = np.fromstring(intensity_string, dtype=float, sep=" ")
+        x_vec = np.linspace(x_min, x_max, len(y_vec))
+        xseries = DataSeries(name="two theta", unit_name="degree", data=x_vec)
+        field = Field(
+            name="intensity", unit_name="counts", data=y_vec, axes_series=[xseries]
+        )
+        return cls.from_field(field, **kwargs)
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat/readers/zilien.py` & `ixdat-0.2.9.dev3/src/ixdat/readers/zilien.py`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,821 +1,821 @@
-"""Readers for files produces by the Zilien software from Spectro Inlets.
-
-Zilien tsv files have two data header lines to define each of the data columns.
-The first one is referred to as "series header" and explains what the data describes,
-and the second one is called "column header" and specifies the specific column.
-It is done in order to keep the columns headers more readable.
-Typically, a series header will specify the measuring device (e.g. "iongauge value")
-or MS channel (e.g. "C0M2") and will apply for two or more column headers
-where the first is time ("Time [s]", "time/s") and the subsequent are the corresponding
-value(s) ("Pressure [mbar]" or "M2-H2 [A]" etc.).
-Zilien files version 2 and higher may also include all the data from an integrated
-Biologic dataset. These are grouped under the series header "EC-lab".
-"""
-
-import re
-import sys
-import time
-from collections import defaultdict
-from itertools import groupby, zip_longest
-from pathlib import Path
-
-import pandas as pd
-import numpy as np
-from matplotlib import pyplot as plt
-
-from ..data_series import DataSeries, TimeSeries, ValueSeries, Field
-from ..techniques import (
-    ECMSMeasurement,
-    MSMeasurement,
-    ECMeasurement,
-    Measurement,
-    TECHNIQUE_CLASSES,
-)
-from ..techniques.ms import MSSpectrum, MSSpectrumSeries, MSSpectroMeasurement
-from .reading_tools import timestamp_string_to_tstamp
-from ..exceptions import ReadError, TechniqueError
-
-
-ZILIEN_TIMESTAMP_FORM = "%Y-%m-%d %H_%M_%S"  # like 2021-03-15 18_50_10
-ZILIEN_MASS_COLUMN_NAMES = ["Mass  [AMU]", "Mass [AMU]"]
-ZILIEN_EC_ALIASES = {
-    "t": ["Potential time [s]"],
-    "raw_potential": ["Voltage [V]"],
-    "raw_current": ["Current [mA]"],
-    "cycle": ["Cycle [n]"],
-}
-# The Zilien .tsv files can be loaded as three different experiment types. These are the
-# aliases for each of them
-ZILIEN_ALIASES = {
-    ECMSMeasurement: ZILIEN_EC_ALIASES,
-    MSMeasurement: {},
-    ECMeasurement: ZILIEN_EC_ALIASES,
-    MSSpectroMeasurement: {},
-}
-
-BIOLOGIC_SERIES_NAME = "EC-lab"
-
-# TODO: When, in the future, Zilien files include the whole EC dataset, remove the
-#    unflattering example presently in the docs.
-#    https://github.com/ixdat/ixdat/pull/30/files#r810087496
-
-
-def parse_metadata_line(line):
-    """Parse a single metadata line and return the name, value"""
-    # The metadata format is a 5 column format:
-    name, comment, attach_to_series, type_as_str, value = line.strip("\n").split("\t")
-
-    # Since, as yet, ixdat doesn't support per-series metadata, we prefix the per-series
-    # metadata item names with the name of the series, to avoid name clashes while still
-    # preserving the data
-    if attach_to_series:
-        full_name = f"{attach_to_series}_{name}"
-    else:
-        full_name = name
-
-    # Type convert the metadata (the specification for version 1 also has a color type,
-    # but it is not used yet)
-    if type_as_str == "string":
-        return full_name, value
-    elif type_as_str == "int":
-        # Python does not seem able to directly evaluate a string like "0.0" as an
-        # integer. Therefore, we evaluate it as a float first and convert to int:
-        return full_name, int(float(value))
-    elif type_as_str == "double":
-        return full_name, float(value)
-    elif type_as_str == "bool":
-        return full_name, value == "true"
-    else:
-        raise TypeError(f"Unknown metadata type {type_as_str} for {name}")
-
-
-def to_snake_case(string):
-    """Turn a space separated string into a snake_case string"""
-    return string.lower().replace(" ", "_")
-
-
-# Matches: "{name} [{unit}]"
-ZILIEN_COLUMN_HEADER_RE = re.compile(r"^(.+?) \[(.+?)\]$")
-# Matches: "{name}/{unit}" and "{name1/name2}/{unit}"
-BIOLOGIC_COLUMN_HEADER_RE = re.compile(r"^(.+)/(.+)$")
-# Matches: "C??M{mass}"
-MASS_SERIES_RE = re.compile(r"^C[0-9]+M([0-9]+)$")
-
-
-def to_mass(string):
-    """Return mass (i.e. "18") if `string` matches the C0M18 mass series form or None"""
-    possible_match = MASS_SERIES_RE.match(string)
-    if possible_match:
-        return possible_match.group(1)
-    return None
-
-
-def determine_class(technique):
-    """Choose appropriate measurement class according to a given technique."""
-
-    if technique in ("EC-MS", "EC", "MS", "MS-MS_spectra"):
-        if technique == "MS-MS_spectra":
-            # We read it as a MSMeasurement and then add the SpectrumSeries
-            return MSMeasurement
-        return TECHNIQUE_CLASSES[technique]
-    else:
-        raise TechniqueError(
-            f'Unknown technique given: "{technique}". '
-            "Use one of the following (in upper-case): "
-            '"EC-MS", "EC, "MS", "MS-MS_spectra".'
-        )
-
-
-class ZilienTSVReader:
-    """Class for reading files saved by Spectro Inlets' Zilien software"""
-
-    def __init__(self):
-        self._path_to_file = None
-        self._cls = None
-        self._measurement = None
-
-        # start time of the Zilien measurement
-        self._timestamp = None
-        # a dictionary with metadata general information about the Zilien measurement
-        self._metadata = None
-        # a list with the Zilien TSV series headers,
-        # such as "Iongauge value" (see module docstring)
-        self._series_headers = None
-        # a list with the Zilien TSV columns headers,
-        # such as "Time [s]" and "Pressure [mbar]"
-        self._column_headers = None
-        # a numpy array of the parsed Zilien data (a big rectangle with NaN filling)
-        self._data = None
-
-    def read(
-        self,
-        path_to_file,
-        cls=None,
-        name=None,
-        include_mass_scans=None,
-        **kwargs,
-    ):
-        """Read a Zilien file
-
-        Args:
-            path_to_file (Path or str): The path of the file to read
-            cls (Measurement): The measurement class to read the file as. Zilien tsv
-                files can be read both as an EC-MS measurement, an MS measurement (which
-                will exclude the EC series from the measurement) and as an EC measurement
-                (which will exclude the MS series from the measurement). To avoid
-                importing classes, this behavior can also be controlled by setting the
-                `technique` argument to either 'EC-MS', 'MS' or 'EC'. The default is
-                determined according to what is parsed from the dataset.
-            name (str): The name of the measurement. Will default to the part of the
-                filename before the '.tsv' extension
-            include_mass_scans (bool): Whether to include mass scans (if available) and
-                thereby return a `SpectroMSMeasurement` which can be indexed to give
-                the spectrum objects. (Defaults to True if technique not specified.)
-            kwargs: All remaining keyword-arguments will be passed onto the `__init__`
-                of the Measurement
-        """
-        if self._path_to_file:
-            print(
-                f"This {self.__class__.__name__} has already read {self._path_to_file}. "
-                "Returning the measurement resulting from the original read. "
-                "Use a new Reader if you want to read another file."
-            )
-            return self._measurement
-
-        self._path_to_file = Path(path_to_file)
-
-        # Parse metadata items
-        with open(self._path_to_file, encoding="utf-8") as file_handle:
-            (
-                self._metadata,
-                self._series_headers,
-                self._column_headers,
-            ) = self._read_metadata(file_handle)
-            file_position = file_handle.tell()
-
-        # Read raw data
-        with open(self._path_to_file, "rb") as file_handle:
-            file_handle.seek(file_position)
-            self._data = np.genfromtxt(file_handle, delimiter="\t")
-
-        if "technique" in kwargs:
-            technique = kwargs["technique"]
-        else:
-            # We don't put "technique directly into kwargs because that would prevent
-            # reading of mass scans by default.
-            technique = self._get_technique()
-
-        if cls is Measurement or cls is None:
-            self._cls = determine_class(technique)
-        else:
-            self._cls = cls
-
-        if include_mass_scans is None:
-            # This becomes True if neither a class nor a technique was specified,
-            # or if a technique or class including spectra was specified.
-            include_mass_scans = (
-                not cls or cls is Measurement or issubclass(cls, MSSpectroMeasurement)
-            ) and ("MS-MS_spectra" in kwargs.get("technique", "MS-MS_spectra"))
-
-        if "technique" not in kwargs:
-            # So that technique gets passed on to the Measurement's __init__:
-            kwargs["technique"] = technique
-
-        # Part of filename before the extension
-        file_stem = self._path_to_file.stem
-
-        if "start_time_unix" in self._metadata:
-            # Extract unix timestamp from metadata
-            self._timestamp = float(self._metadata["start_time_unix"])
-        else:
-            # Extract timestamp from filename on form:
-            # 2021-04-20 11_16_18 Measurement name
-            self._timestamp = timestamp_string_to_tstamp(
-                timestamp_string=" ".join(file_stem.split(" ")[:2]),
-                form=ZILIEN_TIMESTAMP_FORM,
-            )
-
-        # Extract series data and form series
-        series, aliases = self._form_series()
-        for standard_name, general_aliases in ZILIEN_ALIASES[self._cls].items():
-            aliases[standard_name] += general_aliases
-        aliases = dict(aliases)  # Convert from defaultdict to normal dict
-
-        measurement_kwargs = {
-            "name": name or file_stem,
-            "series_list": series,
-            "aliases": aliases,
-            "tstamp": self._timestamp,
-            "metadata": self._metadata,
-        }
-        measurement_kwargs.update(kwargs)
-        self._measurement = self._cls(**measurement_kwargs)
-
-        if include_mass_scans:
-            # Check if there are MS spectra.
-            # If the file name is "YYYY-MM-DD hh_mm_ss my_file_name.tsv", then
-            # the spectra are the .tsv files in the folder "my_file_name mass scans"
-            spectra_folder = self._path_to_file.parent / (
-                self._path_to_file.stem[20:] + " mass scans"
-            )
-            if spectra_folder.exists():  # Then we have a spectra folder!
-                spectrum_series = MSSpectrumSeries.read_set(
-                    spectra_folder,
-                    suffix=".tsv",
-                    reader="zilien",
-                    t_zero=self._timestamp,
-                )
-                self._measurement = self._measurement + spectrum_series
-
-        return self._measurement
-
-    @staticmethod
-    def _read_metadata(file_handle):
-        """Read metadata from `file_handle`.
-
-        Description of the returned variables:
-        - metadata: A dictionary of meta dataset information
-        - series_headers: A list with string series headers (the first header line).
-          A series header is a group of column headers. They have possible one
-          or more empty cells (empty strings) inbetween them, so extra columns
-          in the group can be aligned. E.g. "Iongauge value", "MFC1 setpoint",
-          "MFC1 value", etc.
-        - column_headers: A list with string column headers (the second header line).
-          A column header is a name of one data column in a series. E.g. "Time [s]",
-          "Pressure [mbar]", "Flow [ml/min]", etc.
-
-        The length of the "series_headers" and the "column_headers" is the same.
-
-        Returns:
-            Tuple[Dict[str, str | int | float | bool], List[String], List[String]]:
-            Three variables with metadata, series and column headers described above.
-
-        """
-        # The first 4 lines always include the file version, number of header lines,
-        # number of data header lines and data start line in this order.
-        # Backwards compatibility is ensured, because the one extra line read will be
-        # just added to the 'metadata' dict and ignored later.
-        metadata = {}
-        fixed_metadata_lines_amount = 4
-        for _ in range(fixed_metadata_lines_amount):
-            key, value = parse_metadata_line(file_handle.readline())
-            metadata[key] = value
-
-        # read the rest when the total amount is known
-        for _ in range(metadata["num_header_lines"] - fixed_metadata_lines_amount):
-            key, value = parse_metadata_line(file_handle.readline())
-            metadata[key] = value
-
-        # version 1 of the file format is sometimes missing this value
-        if "file_format_version" not in metadata:
-            metadata["file_format_version"] = 1
-
-        series_headers = file_handle.readline().strip("\n").split("\t")
-        column_headers = file_handle.readline().strip("\n").split("\t")
-
-        return metadata, series_headers, column_headers
-
-    def _get_technique(self):
-        """Get technique according to parsed series headers and column headers.
-
-        This method covers the following cases:
-          - "pot" and "EC-lab" is in the metadata and in the headers.
-            There is "EC-lab" data.
-          - "pot" and "EC-lab" is in the metadata and in the headers.
-            There is no "EC-lab" data. (bug)
-          - "pot" is not in the metadata, but it is in the headers and the data
-            part is filled with NaN. (bug)
-          - "pot" is neither in the metadata, nor in the headers.
-
-        """
-
-        # Should stay "pot_pot_count", because the series name is prepended in
-        # order to keep unique series names. See `parse_metadata_line()`.
-        pot_in_metadata = "pot_pot_count" in self._metadata
-        pot_in_series_headers = "pot" in self._series_headers
-
-        result = ""
-        # there was EC measurement
-        if pot_in_metadata and pot_in_series_headers:
-            result = "EC-MS"
-
-            # BUG CASE: EC-lab series header gets included, but .mpt data don't
-            missing_column_headers = len(self._column_headers) < len(
-                self._series_headers
-            )
-            empty_eclab_series = self._series_headers[-1] == "EC-lab"
-            if missing_column_headers and empty_eclab_series:
-                result = "MS"
-                sys.stderr.write(
-                    "EC-lab data is missing in the dataset. That means Zilien did not"
-                    "find .mpt files when creating it. You can convert .mpr files into "
-                    ".mpt files in EC-lab (or read the .mpr files directly) and then "
-                    "connect the two Measurement objects.\n"
-                    "Reading only the Mass Scan part of the dataset.\n\n"
-                )
-        # BUG CASE: Zilien internal bug (https://github.com/ixdat/ixdat/issues/114)
-        elif not pot_in_metadata and pot_in_series_headers:
-            result = "MS"
-        # no EC did run and EC is not included in the settings dialog
-        elif not pot_in_metadata and not pot_in_series_headers:
-            result = "MS"
-        else:
-            # unexpected case, so go with the safest option
-            result = "MS"
-
-        return result
-
-    def _form_series(self):
-        """Form the series and series aliases
-
-        Returns:
-            List[Series], DefaultDict(str, List[str]): List of series and dict of aliases
-        """
-        aliases = defaultdict(list)
-        series = []
-
-        # Get non-empty series headers and their indices
-        # in order to process whole series chunks
-        series_split_indices, nonempty_headers = self._get_series_splits(
-            self._series_headers
-        )
-
-        for series_header, (begin, end) in zip(nonempty_headers, series_split_indices):
-            # Skip series not relevant for the type of measurement
-            if not issubclass(self._cls, ECMeasurement) and series_header in (
-                "pot",
-                BIOLOGIC_SERIES_NAME,
-            ):
-                continue
-            elif not issubclass(self._cls, MSMeasurement) and to_mass(series_header):
-                continue
-
-            column_headers_split = self._column_headers[begin:end]
-            data_columns_split = self._data[:, begin:end]
-
-            if series_header == BIOLOGIC_SERIES_NAME:
-                column_series = self._biologic_dataset_part(
-                    column_headers_split, data_columns_split
-                )
-            else:
-                column_series, aliases_part = self._zilien_dataset_part(
-                    series_header, column_headers_split, data_columns_split
-                )
-                # update aliases
-                for standard_name, series_name in aliases_part.items():
-                    aliases[standard_name] += series_name
-
-            series += column_series
-
-        return series, aliases
-
-    def _zilien_dataset_part(self, series_header, column_headers, data_columns_split):
-        """Process necessary data for a Zilien dataset part.
-
-        Args:
-            series_header (str): The current series name.
-            column_headers (list): A list with column names from the current series.
-            data_columns_split (np.array): A columns taken out from the parsed dataset,
-                that represents the current series.
-
-        Returns:
-            list, DefaultDict[List]: A list of Ixdat series objects and
-            a default dict with standard names for a Mass series.
-        """
-
-        count = self._metadata[f"{series_header}_{series_header}_count"]
-        names_and_units = [
-            self._form_names_and_unit(series_header, column_header)
-            for column_header in column_headers
-        ]
-
-        # Fill Mass aliases
-        aliases = defaultdict(list)
-        for series_name, _, standard_name in names_and_units:
-            if standard_name:
-                aliases[standard_name].append(series_name)
-
-        # Create Ixdat series
-        column_series = self._create_series_objects(
-            column_headers, names_and_units, data_columns_split[:count, :]
-        )
-
-        return column_series, aliases
-
-    def _biologic_dataset_part(self, column_headers, data_columns_split):
-        """Process necessary data for a Biologic dataset part.
-
-        The `experiment_number` and the `technique_number` columns are used only
-        to create an information how to split the given data part into rows.
-        After that they are not used anymore and no Ixdat series objects
-        are created from them.
-
-        Args:
-            column_headers (list): A list with column names from the current series.
-            data_columns_split (np.array): A columns taken out from the parsed dataset,
-                that represents the current series.
-
-        Returns:
-            list: A list of Ixdat series objects.
-        """
-
-        count = self._metadata[f"{BIOLOGIC_SERIES_NAME}_{BIOLOGIC_SERIES_NAME}_count"]
-        names_and_units = [
-            self._form_names_and_unit(BIOLOGIC_SERIES_NAME, column_header)
-            for column_header in column_headers
-        ]
-
-        # Split rows according to techniques used according to experiments
-        # Combine the experiment numbers and the technique numbers
-        # in order to create unique identifiers for a successful split
-        exp_nums = data_columns_split[:count, column_headers.index("experiment_number")]
-        tech_nums = data_columns_split[:count, column_headers.index("technique_number")]
-        split_vector = exp_nums * 1000 + tech_nums
-        splits = self._get_biologic_splits(split_vector)
-
-        # Create Ixdat series
-        column_series = []
-        for begin, end in splits:
-            column_series += self._create_series_objects(
-                column_headers, names_and_units, data_columns_split[begin:end, :]
-            )
-
-        return column_series
-
-    def _create_series_objects(self, column_headers, names_and_units, data_rows_split):
-        """Create an Ixdat series objects from a given portion of a dataset.
-
-        The `experiment_number` and the `technique_number` columns are skipped,
-        because they are used only to split rows in the Biologic dataset by the
-        Zilien reader, in order to create the same series as Ixdat would.
-
-        Args:
-            column_headers (list): A list with column names from the current series.
-            names_and_units (list): A tuple with three elements. A series name,
-                a unit and a standard name for every given column. (The series name
-                is same for all columns here.)
-            data_rows_split (np.array): A rows taken out from the columns part.
-                In Zilien part it represents the Zilien measurement.
-                In Biologic part it represents the current technique.
-
-        Returns:
-            list: A list of Ixdat series objects.
-        """
-
-        series_objects = []
-        time_series = None
-
-        for column_number, column_header in enumerate(column_headers):
-            # Skip meta columns in the EC-lab dataset
-            if column_header in ("experiment_number", "technique_number"):
-                continue
-
-            column_data = data_rows_split[:, column_number]
-
-            # Skip holes in the EC-lab dataset
-            if np.isnan(column_data).all():
-                continue
-
-            # Form series kwargs
-            series_name, unit, standard_name = names_and_units[column_number]
-            series_kwargs = {
-                "name": series_name,
-                "unit_name": unit,
-                "data": column_data,
-            }
-
-            # Create the series
-            if column_header in ("Time [s]", "time/s"):
-                series_object = TimeSeries(**series_kwargs, tstamp=self._timestamp)
-                time_series = series_object
-            else:
-                if time_series is None:
-                    raise ValueError("Time column must be first in a dataset series.")
-
-                series_object = ValueSeries(**series_kwargs, tseries=time_series)
-
-            series_objects.append(series_object)
-
-        return series_objects
-
-    # --- UTILS ---
-
-    @staticmethod
-    def _get_series_splits(series_headers):
-        """Create series names and their index pairs in the whole read dataset.
-
-        Args:
-            series_headers (list): Series name headers (even empty).
-
-        Returns:
-            list, list: List of tuples with index pairs
-            and a list with non-empty series name headers.
-        """
-        series_split_indices = []
-        nonempty_headers = []
-
-        for index, series_header in enumerate(series_headers):
-            if series_header != "":
-                series_split_indices.append(index)
-                nonempty_headers.append(series_header)
-
-        # create split pairs
-        # zip_longest to have the last pair with the index of the last
-        # series header to the end
-        series_split_indices = [
-            (begin, end)
-            for begin, end in zip_longest(series_split_indices, series_split_indices[1:])
-        ]
-
-        return series_split_indices, nonempty_headers
-
-    @staticmethod
-    def _get_biologic_splits(technique_numbers):
-        """Create index pairs of row splits in the biologic dataset columns.
-
-        Args:
-            technique_numbers (np.array): Numbers of techniques during
-                an EC-lab experiment(s).
-
-        Returns:
-            list: List of tuples with index pairs.
-        """
-        index = 0
-        splits = []
-
-        for _, group in groupby(technique_numbers):
-            group_length = len(list(group))
-            splits.append((index, index + group_length))
-            index += group_length
-
-        return splits
-
-    @staticmethod
-    def _form_names_and_unit(series_header, column_header):
-        """Form names and unit from headers.
-
-        Args:
-            series_header (str): Something like "Iongauge value" or "C0M18"
-            column_header (str): Something like "Time [s]" or "Flow [ml/min]"
-
-        Returns:
-            str, str, Optional[str]: Return series_name, unit, standard_name
-        """
-        standard_name = None
-        if column_header in ("Time [s]", "time/s"):  # Form TimeSeries
-            unit = "s"
-            if series_header == "pot":
-                name = f"Potential {column_header.lower()}"
-            elif series_header == BIOLOGIC_SERIES_NAME:
-                name = f"Biologic {column_header.lower()}"
-            else:
-                name = f"{series_header} {column_header.lower()}"
-        else:  # ValueSeries
-            # Perform a bit of reasonable name adaption, first break name and unit out
-            # from the column header on the form: Pressure [mbar]
-            zilien_components_match = ZILIEN_COLUMN_HEADER_RE.match(column_header)
-            biologic_components_match = BIOLOGIC_COLUMN_HEADER_RE.match(column_header)
-
-            if zilien_components_match:
-                _, unit = zilien_components_match.groups()
-            elif biologic_components_match:
-                _, unit = biologic_components_match.groups()
-            else:
-                _, unit = column_header, ""
-
-            # Is the column a "setpoint" or "value" type
-            setpoint_or_value = None
-            for option in ("setpoint", "value"):
-                if series_header.endswith(option):
-                    setpoint_or_value = option
-
-            mass = to_mass(series_header)
-            if setpoint_or_value:
-                # In that case, the column header is something like "Flow [ml/min]" where
-                # "Flow" is unnecessary, because that is apparent from the unit
-                # The name will look for example like this "MFC setpoint [ml/min]"
-                name = f"{series_header} [{unit}]"
-            elif mass is not None:
-                # e.g. from series header "C1M4" and column header "M4-He [A]"
-                # the name will be "M4 [A]" and standard name will be "M4"
-                name = f"M{mass} [{unit}]"
-                standard_name = f"M{mass}"
-            else:
-                name = column_header
-
-        return name, unit, standard_name
-
-
-class ZilienTMPReader:
-    """A class for stitching the files in a Zilien tmp directory to an ECMSMeasurement
-
-    This is necessary because Zilien often crashes, leaving only the tmp directory.
-    This is less advanced but more readable than the Spectro Inlets stitching solution.
-    """
-
-    def __init__(self, path_to_tmp_dir=None):
-        self.path_to_tmp_dir = Path(path_to_tmp_dir) if path_to_tmp_dir else None
-
-    def read(self, path_to_tmp_dir, cls=None, **kwargs):
-        """Make a measurement from all the single-value .tsv files in a Zilien tmp dir
-
-        Args:
-            path_to_tmp_dir (Path or str): The path to the tmp dir
-            cls (Measurement class): Defaults to ECMSMeasurement
-        """
-        if path_to_tmp_dir:
-            self.path_to_tmp_dir = Path(path_to_tmp_dir)
-        cls = cls or ECMSMeasurement
-        name = self.path_to_tmp_dir.parent.name
-        timestamp_string = name[:19]  # the zilien timestamp is the first 19 chars
-        tstamp = timestamp_string_to_tstamp(timestamp_string, form=ZILIEN_TIMESTAMP_FORM)
-        series_list = []
-        for tmp_file in self.path_to_tmp_dir.iterdir():
-            series_list += series_list_from_tmp(tmp_file)
-        obj_as_dict = {
-            "name": name,
-            "tstamp": tstamp,
-            "series_list": series_list,
-            "technique": "EC-MS",
-            "reader": self,
-        }
-        obj_as_dict.update(kwargs)
-        return cls.from_dict(obj_as_dict)
-
-
-def series_list_from_tmp(path_to_file):
-    """Return [ValueSeries, TimeSeries] with the data in a zilien tmp .tsv file"""
-    file_name = Path(path_to_file).name
-    timestamp_string = file_name[:19]  # the zilien timestamp form is 19 chars long
-    tstamp = timestamp_string_to_tstamp(timestamp_string, form=ZILIEN_TIMESTAMP_FORM)
-    column_match = re.search(r"\.([^\.]+)\.data", file_name)
-    if not column_match:
-        print(f"could not find column name in {path_to_file}")
-        return []
-    v_name = column_match.group(1)
-    mass_match = re.search("M[0-9]+", v_name)
-    if mass_match:
-        v_name = mass_match.group()
-        unit = "A"
-    else:
-        unit = None
-    t_name = v_name + "-x"
-    df = pd.read_csv(path_to_file, delimiter="\t", names=[t_name, v_name], header=0)
-    t_data, v_data = df[t_name].to_numpy(), df[v_name].to_numpy()
-    tseries = TimeSeries(name=t_name, unit_name="s", data=t_data, tstamp=tstamp)
-    vseries = ValueSeries(name=v_name, unit_name=unit, data=v_data, tseries=tseries)
-    return [tseries, vseries]
-
-
-class ZilienSpectrumReader:
-    """A reader for individual Zilien spectra
-    TODO: A Zilien reader which loads all spectra at once in a SpectrumSeries object
-    """
-
-    def __init__(self, path_to_spectrum=None):
-        self.path_to_spectrum = Path(path_to_spectrum) if path_to_spectrum else None
-
-    def read(self, path_to_spectrum, cls=None, t_zero=None, **kwargs):
-        """Read a Zilien spectrum.
-        FIXME: This reader was written hastily and could be designed better.
-
-        Args:
-            path_to_spectrum(Path or str): the path to the spectrum file
-            cls (Spectrum class): Defaults to MSSpectrum
-            t_zero (float): The unix timestamp which the mass scan start time
-                is referenced to. Should be the tstamp of the corresponding
-                Zilien measurement. If the Spectrum is read individually, it needs
-                to be input or defaults to `time.time()`, i.e., now.
-            kwargs: Key-word arguments are passed on ultimately to cls.__init__
-        """
-        if path_to_spectrum:
-            self.path_to_spectrum = Path(path_to_spectrum)
-        cls = cls or MSSpectrum
-        t_zero = t_zero or time.time()
-
-        with open(self.path_to_spectrum, "r") as f:
-            in_header = True
-            num_line = 1
-            metadata_dict = {}
-            while in_header:
-                name, value = parse_metadata_line(f.readline())
-                metadata_dict[name] = value
-                if num_line == metadata_dict.get("num_header_lines", 0):
-                    in_header = False
-                num_line += 1
-
-        tstamp = t_zero + metadata_dict["mass_scan_started_at"]
-        duration = (
-            (metadata_dict["stop_mass"] - metadata_dict["start_mass"])
-            * metadata_dict["points_per_amu"]
-            * metadata_dict["dwell_time"]
-            * 1e-3
-        )
-
-        df = pd.read_csv(
-            self.path_to_spectrum,
-            header=metadata_dict["data_start"] - 1,
-            delimiter="\t",
-        )
-        y_name = "Current [A]"
-
-        for x_name in ZILIEN_MASS_COLUMN_NAMES:
-            try:
-                x = df[x_name].to_numpy()
-            except KeyError:
-                continue
-            break
-        else:
-            raise ReadError(
-                f"Can't find a mass column in {self.path_to_spectrum}. "
-                f"Looked for one of {ZILIEN_MASS_COLUMN_NAMES}"
-            )
-        y = df[y_name].to_numpy()
-
-        xseries = DataSeries(data=x, name=x_name, unit_name="m/z")
-        field = Field(
-            data=np.array(y),
-            name=y_name,
-            unit_name="A",
-            axes_series=[
-                xseries,
-            ],
-        )
-        obj_as_dict = {
-            "name": self.path_to_spectrum.name,
-            "technique": "MS_spectrum",
-            "field": field,
-            "reader": self,
-            "tstamp": tstamp,
-            "duration": duration,
-        }
-        obj_as_dict.update(kwargs)
-        return cls.from_dict(obj_as_dict)
-
-
-def module_demo():
-    """Module demo here.
-
-    To run this module in PyCharm, open Run Configuration and set
-        Module name = ixdat.readers.zilien,
-    and *not*
-        Script path = ...
-    """
-    path_to_test_file = (
-        Path(__file__).parent.resolve().parent.parent.parent
-        / "test_data"
-        / "Zilien version 1"
-        / "2022-04-06 16_17_23 full set.tsv"
-    )
-
-    ecms_measurement = Measurement.read(
-        reader="zilien",
-        path_to_file=path_to_test_file,
-    )
-
-    ecms_measurement.plot_measurement()
-    plt.show()
-
-
-if __name__ == "__main__":
-    module_demo()
+"""Readers for files produces by the Zilien software from Spectro Inlets.
+
+Zilien tsv files have two data header lines to define each of the data columns.
+The first one is referred to as "series header" and explains what the data describes,
+and the second one is called "column header" and specifies the specific column.
+It is done in order to keep the columns headers more readable.
+Typically, a series header will specify the measuring device (e.g. "iongauge value")
+or MS channel (e.g. "C0M2") and will apply for two or more column headers
+where the first is time ("Time [s]", "time/s") and the subsequent are the corresponding
+value(s) ("Pressure [mbar]" or "M2-H2 [A]" etc.).
+Zilien files version 2 and higher may also include all the data from an integrated
+Biologic dataset. These are grouped under the series header "EC-lab".
+"""
+
+import re
+import sys
+import time
+from collections import defaultdict
+from itertools import groupby, zip_longest
+from pathlib import Path
+
+import pandas as pd
+import numpy as np
+from matplotlib import pyplot as plt
+
+from ..data_series import DataSeries, TimeSeries, ValueSeries, Field
+from ..techniques import (
+    ECMSMeasurement,
+    MSMeasurement,
+    ECMeasurement,
+    Measurement,
+    TECHNIQUE_CLASSES,
+)
+from ..techniques.ms import MSSpectrum, MSSpectrumSeries, MSSpectroMeasurement
+from .reading_tools import timestamp_string_to_tstamp
+from ..exceptions import ReadError, TechniqueError
+
+
+ZILIEN_TIMESTAMP_FORM = "%Y-%m-%d %H_%M_%S"  # like 2021-03-15 18_50_10
+ZILIEN_MASS_COLUMN_NAMES = ["Mass  [AMU]", "Mass [AMU]"]
+ZILIEN_EC_ALIASES = {
+    "t": ["Potential time [s]"],
+    "raw_potential": ["Voltage [V]"],
+    "raw_current": ["Current [mA]"],
+    "cycle": ["Cycle [n]"],
+}
+# The Zilien .tsv files can be loaded as three different experiment types. These are the
+# aliases for each of them
+ZILIEN_ALIASES = {
+    ECMSMeasurement: ZILIEN_EC_ALIASES,
+    MSMeasurement: {},
+    ECMeasurement: ZILIEN_EC_ALIASES,
+    MSSpectroMeasurement: {},
+}
+
+BIOLOGIC_SERIES_NAME = "EC-lab"
+
+# TODO: When, in the future, Zilien files include the whole EC dataset, remove the
+#    unflattering example presently in the docs.
+#    https://github.com/ixdat/ixdat/pull/30/files#r810087496
+
+
+def parse_metadata_line(line):
+    """Parse a single metadata line and return the name, value"""
+    # The metadata format is a 5 column format:
+    name, comment, attach_to_series, type_as_str, value = line.strip("\n").split("\t")
+
+    # Since, as yet, ixdat doesn't support per-series metadata, we prefix the per-series
+    # metadata item names with the name of the series, to avoid name clashes while still
+    # preserving the data
+    if attach_to_series:
+        full_name = f"{attach_to_series}_{name}"
+    else:
+        full_name = name
+
+    # Type convert the metadata (the specification for version 1 also has a color type,
+    # but it is not used yet)
+    if type_as_str == "string":
+        return full_name, value
+    elif type_as_str == "int":
+        # Python does not seem able to directly evaluate a string like "0.0" as an
+        # integer. Therefore, we evaluate it as a float first and convert to int:
+        return full_name, int(float(value))
+    elif type_as_str == "double":
+        return full_name, float(value)
+    elif type_as_str == "bool":
+        return full_name, value == "true"
+    else:
+        raise TypeError(f"Unknown metadata type {type_as_str} for {name}")
+
+
+def to_snake_case(string):
+    """Turn a space separated string into a snake_case string"""
+    return string.lower().replace(" ", "_")
+
+
+# Matches: "{name} [{unit}]"
+ZILIEN_COLUMN_HEADER_RE = re.compile(r"^(.+?) \[(.+?)\]$")
+# Matches: "{name}/{unit}" and "{name1/name2}/{unit}"
+BIOLOGIC_COLUMN_HEADER_RE = re.compile(r"^(.+)/(.+)$")
+# Matches: "C??M{mass}"
+MASS_SERIES_RE = re.compile(r"^C[0-9]+M([0-9]+)$")
+
+
+def to_mass(string):
+    """Return mass (i.e. "18") if `string` matches the C0M18 mass series form or None"""
+    possible_match = MASS_SERIES_RE.match(string)
+    if possible_match:
+        return possible_match.group(1)
+    return None
+
+
+def determine_class(technique):
+    """Choose appropriate measurement class according to a given technique."""
+
+    if technique in ("EC-MS", "EC", "MS", "MS-MS_spectra"):
+        if technique == "MS-MS_spectra":
+            # We read it as a MSMeasurement and then add the SpectrumSeries
+            return MSMeasurement
+        return TECHNIQUE_CLASSES[technique]
+    else:
+        raise TechniqueError(
+            f'Unknown technique given: "{technique}". '
+            "Use one of the following (in upper-case): "
+            '"EC-MS", "EC, "MS", "MS-MS_spectra".'
+        )
+
+
+class ZilienTSVReader:
+    """Class for reading files saved by Spectro Inlets' Zilien software"""
+
+    def __init__(self):
+        self._path_to_file = None
+        self._cls = None
+        self._measurement = None
+
+        # start time of the Zilien measurement
+        self._timestamp = None
+        # a dictionary with metadata general information about the Zilien measurement
+        self._metadata = None
+        # a list with the Zilien TSV series headers,
+        # such as "Iongauge value" (see module docstring)
+        self._series_headers = None
+        # a list with the Zilien TSV columns headers,
+        # such as "Time [s]" and "Pressure [mbar]"
+        self._column_headers = None
+        # a numpy array of the parsed Zilien data (a big rectangle with NaN filling)
+        self._data = None
+
+    def read(
+        self,
+        path_to_file,
+        cls=None,
+        name=None,
+        include_mass_scans=None,
+        **kwargs,
+    ):
+        """Read a Zilien file
+
+        Args:
+            path_to_file (Path or str): The path of the file to read
+            cls (Measurement): The measurement class to read the file as. Zilien tsv
+                files can be read both as an EC-MS measurement, an MS measurement (which
+                will exclude the EC series from the measurement) and as an EC measurement
+                (which will exclude the MS series from the measurement). To avoid
+                importing classes, this behavior can also be controlled by setting the
+                `technique` argument to either 'EC-MS', 'MS' or 'EC'. The default is
+                determined according to what is parsed from the dataset.
+            name (str): The name of the measurement. Will default to the part of the
+                filename before the '.tsv' extension
+            include_mass_scans (bool): Whether to include mass scans (if available) and
+                thereby return a `SpectroMSMeasurement` which can be indexed to give
+                the spectrum objects. (Defaults to True if technique not specified.)
+            kwargs: All remaining keyword-arguments will be passed onto the `__init__`
+                of the Measurement
+        """
+        if self._path_to_file:
+            print(
+                f"This {self.__class__.__name__} has already read {self._path_to_file}. "
+                "Returning the measurement resulting from the original read. "
+                "Use a new Reader if you want to read another file."
+            )
+            return self._measurement
+
+        self._path_to_file = Path(path_to_file)
+
+        # Parse metadata items
+        with open(self._path_to_file, encoding="utf-8") as file_handle:
+            (
+                self._metadata,
+                self._series_headers,
+                self._column_headers,
+            ) = self._read_metadata(file_handle)
+            file_position = file_handle.tell()
+
+        # Read raw data
+        with open(self._path_to_file, "rb") as file_handle:
+            file_handle.seek(file_position)
+            self._data = np.genfromtxt(file_handle, delimiter="\t")
+
+        if "technique" in kwargs:
+            technique = kwargs["technique"]
+        else:
+            # We don't put "technique directly into kwargs because that would prevent
+            # reading of mass scans by default.
+            technique = self._get_technique()
+
+        if cls is Measurement or cls is None:
+            self._cls = determine_class(technique)
+        else:
+            self._cls = cls
+
+        if include_mass_scans is None:
+            # This becomes True if neither a class nor a technique was specified,
+            # or if a technique or class including spectra was specified.
+            include_mass_scans = (
+                not cls or cls is Measurement or issubclass(cls, MSSpectroMeasurement)
+            ) and ("MS-MS_spectra" in kwargs.get("technique", "MS-MS_spectra"))
+
+        if "technique" not in kwargs:
+            # So that technique gets passed on to the Measurement's __init__:
+            kwargs["technique"] = technique
+
+        # Part of filename before the extension
+        file_stem = self._path_to_file.stem
+
+        if "start_time_unix" in self._metadata:
+            # Extract unix timestamp from metadata
+            self._timestamp = float(self._metadata["start_time_unix"])
+        else:
+            # Extract timestamp from filename on form:
+            # 2021-04-20 11_16_18 Measurement name
+            self._timestamp = timestamp_string_to_tstamp(
+                timestamp_string=" ".join(file_stem.split(" ")[:2]),
+                form=ZILIEN_TIMESTAMP_FORM,
+            )
+
+        # Extract series data and form series
+        series, aliases = self._form_series()
+        for standard_name, general_aliases in ZILIEN_ALIASES[self._cls].items():
+            aliases[standard_name] += general_aliases
+        aliases = dict(aliases)  # Convert from defaultdict to normal dict
+
+        measurement_kwargs = {
+            "name": name or file_stem,
+            "series_list": series,
+            "aliases": aliases,
+            "tstamp": self._timestamp,
+            "metadata": self._metadata,
+        }
+        measurement_kwargs.update(kwargs)
+        self._measurement = self._cls(**measurement_kwargs)
+
+        if include_mass_scans:
+            # Check if there are MS spectra.
+            # If the file name is "YYYY-MM-DD hh_mm_ss my_file_name.tsv", then
+            # the spectra are the .tsv files in the folder "my_file_name mass scans"
+            spectra_folder = self._path_to_file.parent / (
+                self._path_to_file.stem[20:] + " mass scans"
+            )
+            if spectra_folder.exists():  # Then we have a spectra folder!
+                spectrum_series = MSSpectrumSeries.read_set(
+                    spectra_folder,
+                    suffix=".tsv",
+                    reader="zilien",
+                    t_zero=self._timestamp,
+                )
+                self._measurement = self._measurement + spectrum_series
+
+        return self._measurement
+
+    @staticmethod
+    def _read_metadata(file_handle):
+        """Read metadata from `file_handle`.
+
+        Description of the returned variables:
+        - metadata: A dictionary of meta dataset information
+        - series_headers: A list with string series headers (the first header line).
+          A series header is a group of column headers. They have possible one
+          or more empty cells (empty strings) inbetween them, so extra columns
+          in the group can be aligned. E.g. "Iongauge value", "MFC1 setpoint",
+          "MFC1 value", etc.
+        - column_headers: A list with string column headers (the second header line).
+          A column header is a name of one data column in a series. E.g. "Time [s]",
+          "Pressure [mbar]", "Flow [ml/min]", etc.
+
+        The length of the "series_headers" and the "column_headers" is the same.
+
+        Returns:
+            Tuple[Dict[str, str | int | float | bool], List[String], List[String]]:
+            Three variables with metadata, series and column headers described above.
+
+        """
+        # The first 4 lines always include the file version, number of header lines,
+        # number of data header lines and data start line in this order.
+        # Backwards compatibility is ensured, because the one extra line read will be
+        # just added to the 'metadata' dict and ignored later.
+        metadata = {}
+        fixed_metadata_lines_amount = 4
+        for _ in range(fixed_metadata_lines_amount):
+            key, value = parse_metadata_line(file_handle.readline())
+            metadata[key] = value
+
+        # read the rest when the total amount is known
+        for _ in range(metadata["num_header_lines"] - fixed_metadata_lines_amount):
+            key, value = parse_metadata_line(file_handle.readline())
+            metadata[key] = value
+
+        # version 1 of the file format is sometimes missing this value
+        if "file_format_version" not in metadata:
+            metadata["file_format_version"] = 1
+
+        series_headers = file_handle.readline().strip("\n").split("\t")
+        column_headers = file_handle.readline().strip("\n").split("\t")
+
+        return metadata, series_headers, column_headers
+
+    def _get_technique(self):
+        """Get technique according to parsed series headers and column headers.
+
+        This method covers the following cases:
+          - "pot" and "EC-lab" is in the metadata and in the headers.
+            There is "EC-lab" data.
+          - "pot" and "EC-lab" is in the metadata and in the headers.
+            There is no "EC-lab" data. (bug)
+          - "pot" is not in the metadata, but it is in the headers and the data
+            part is filled with NaN. (bug)
+          - "pot" is neither in the metadata, nor in the headers.
+
+        """
+
+        # Should stay "pot_pot_count", because the series name is prepended in
+        # order to keep unique series names. See `parse_metadata_line()`.
+        pot_in_metadata = "pot_pot_count" in self._metadata
+        pot_in_series_headers = "pot" in self._series_headers
+
+        result = ""
+        # there was EC measurement
+        if pot_in_metadata and pot_in_series_headers:
+            result = "EC-MS"
+
+            # BUG CASE: EC-lab series header gets included, but .mpt data don't
+            missing_column_headers = len(self._column_headers) < len(
+                self._series_headers
+            )
+            empty_eclab_series = self._series_headers[-1] == "EC-lab"
+            if missing_column_headers and empty_eclab_series:
+                result = "MS"
+                sys.stderr.write(
+                    "EC-lab data is missing in the dataset. That means Zilien did not"
+                    "find .mpt files when creating it. You can convert .mpr files into "
+                    ".mpt files in EC-lab (or read the .mpr files directly) and then "
+                    "connect the two Measurement objects.\n"
+                    "Reading only the Mass Scan part of the dataset.\n\n"
+                )
+        # BUG CASE: Zilien internal bug (https://github.com/ixdat/ixdat/issues/114)
+        elif not pot_in_metadata and pot_in_series_headers:
+            result = "MS"
+        # no EC did run and EC is not included in the settings dialog
+        elif not pot_in_metadata and not pot_in_series_headers:
+            result = "MS"
+        else:
+            # unexpected case, so go with the safest option
+            result = "MS"
+
+        return result
+
+    def _form_series(self):
+        """Form the series and series aliases
+
+        Returns:
+            List[Series], DefaultDict(str, List[str]): List of series and dict of aliases
+        """
+        aliases = defaultdict(list)
+        series = []
+
+        # Get non-empty series headers and their indices
+        # in order to process whole series chunks
+        series_split_indices, nonempty_headers = self._get_series_splits(
+            self._series_headers
+        )
+
+        for series_header, (begin, end) in zip(nonempty_headers, series_split_indices):
+            # Skip series not relevant for the type of measurement
+            if not issubclass(self._cls, ECMeasurement) and series_header in (
+                "pot",
+                BIOLOGIC_SERIES_NAME,
+            ):
+                continue
+            elif not issubclass(self._cls, MSMeasurement) and to_mass(series_header):
+                continue
+
+            column_headers_split = self._column_headers[begin:end]
+            data_columns_split = self._data[:, begin:end]
+
+            if series_header == BIOLOGIC_SERIES_NAME:
+                column_series = self._biologic_dataset_part(
+                    column_headers_split, data_columns_split
+                )
+            else:
+                column_series, aliases_part = self._zilien_dataset_part(
+                    series_header, column_headers_split, data_columns_split
+                )
+                # update aliases
+                for standard_name, series_name in aliases_part.items():
+                    aliases[standard_name] += series_name
+
+            series += column_series
+
+        return series, aliases
+
+    def _zilien_dataset_part(self, series_header, column_headers, data_columns_split):
+        """Process necessary data for a Zilien dataset part.
+
+        Args:
+            series_header (str): The current series name.
+            column_headers (list): A list with column names from the current series.
+            data_columns_split (np.array): A columns taken out from the parsed dataset,
+                that represents the current series.
+
+        Returns:
+            list, DefaultDict[List]: A list of Ixdat series objects and
+            a default dict with standard names for a Mass series.
+        """
+
+        count = self._metadata[f"{series_header}_{series_header}_count"]
+        names_and_units = [
+            self._form_names_and_unit(series_header, column_header)
+            for column_header in column_headers
+        ]
+
+        # Fill Mass aliases
+        aliases = defaultdict(list)
+        for series_name, _, standard_name in names_and_units:
+            if standard_name:
+                aliases[standard_name].append(series_name)
+
+        # Create Ixdat series
+        column_series = self._create_series_objects(
+            column_headers, names_and_units, data_columns_split[:count, :]
+        )
+
+        return column_series, aliases
+
+    def _biologic_dataset_part(self, column_headers, data_columns_split):
+        """Process necessary data for a Biologic dataset part.
+
+        The `experiment_number` and the `technique_number` columns are used only
+        to create an information how to split the given data part into rows.
+        After that they are not used anymore and no Ixdat series objects
+        are created from them.
+
+        Args:
+            column_headers (list): A list with column names from the current series.
+            data_columns_split (np.array): A columns taken out from the parsed dataset,
+                that represents the current series.
+
+        Returns:
+            list: A list of Ixdat series objects.
+        """
+
+        count = self._metadata[f"{BIOLOGIC_SERIES_NAME}_{BIOLOGIC_SERIES_NAME}_count"]
+        names_and_units = [
+            self._form_names_and_unit(BIOLOGIC_SERIES_NAME, column_header)
+            for column_header in column_headers
+        ]
+
+        # Split rows according to techniques used according to experiments
+        # Combine the experiment numbers and the technique numbers
+        # in order to create unique identifiers for a successful split
+        exp_nums = data_columns_split[:count, column_headers.index("experiment_number")]
+        tech_nums = data_columns_split[:count, column_headers.index("technique_number")]
+        split_vector = exp_nums * 1000 + tech_nums
+        splits = self._get_biologic_splits(split_vector)
+
+        # Create Ixdat series
+        column_series = []
+        for begin, end in splits:
+            column_series += self._create_series_objects(
+                column_headers, names_and_units, data_columns_split[begin:end, :]
+            )
+
+        return column_series
+
+    def _create_series_objects(self, column_headers, names_and_units, data_rows_split):
+        """Create an Ixdat series objects from a given portion of a dataset.
+
+        The `experiment_number` and the `technique_number` columns are skipped,
+        because they are used only to split rows in the Biologic dataset by the
+        Zilien reader, in order to create the same series as Ixdat would.
+
+        Args:
+            column_headers (list): A list with column names from the current series.
+            names_and_units (list): A tuple with three elements. A series name,
+                a unit and a standard name for every given column. (The series name
+                is same for all columns here.)
+            data_rows_split (np.array): A rows taken out from the columns part.
+                In Zilien part it represents the Zilien measurement.
+                In Biologic part it represents the current technique.
+
+        Returns:
+            list: A list of Ixdat series objects.
+        """
+
+        series_objects = []
+        time_series = None
+
+        for column_number, column_header in enumerate(column_headers):
+            # Skip meta columns in the EC-lab dataset
+            if column_header in ("experiment_number", "technique_number"):
+                continue
+
+            column_data = data_rows_split[:, column_number]
+
+            # Skip holes in the EC-lab dataset
+            if np.isnan(column_data).all():
+                continue
+
+            # Form series kwargs
+            series_name, unit, standard_name = names_and_units[column_number]
+            series_kwargs = {
+                "name": series_name,
+                "unit_name": unit,
+                "data": column_data,
+            }
+
+            # Create the series
+            if column_header in ("Time [s]", "time/s"):
+                series_object = TimeSeries(**series_kwargs, tstamp=self._timestamp)
+                time_series = series_object
+            else:
+                if time_series is None:
+                    raise ValueError("Time column must be first in a dataset series.")
+
+                series_object = ValueSeries(**series_kwargs, tseries=time_series)
+
+            series_objects.append(series_object)
+
+        return series_objects
+
+    # --- UTILS ---
+
+    @staticmethod
+    def _get_series_splits(series_headers):
+        """Create series names and their index pairs in the whole read dataset.
+
+        Args:
+            series_headers (list): Series name headers (even empty).
+
+        Returns:
+            list, list: List of tuples with index pairs
+            and a list with non-empty series name headers.
+        """
+        series_split_indices = []
+        nonempty_headers = []
+
+        for index, series_header in enumerate(series_headers):
+            if series_header != "":
+                series_split_indices.append(index)
+                nonempty_headers.append(series_header)
+
+        # create split pairs
+        # zip_longest to have the last pair with the index of the last
+        # series header to the end
+        series_split_indices = [
+            (begin, end)
+            for begin, end in zip_longest(series_split_indices, series_split_indices[1:])
+        ]
+
+        return series_split_indices, nonempty_headers
+
+    @staticmethod
+    def _get_biologic_splits(technique_numbers):
+        """Create index pairs of row splits in the biologic dataset columns.
+
+        Args:
+            technique_numbers (np.array): Numbers of techniques during
+                an EC-lab experiment(s).
+
+        Returns:
+            list: List of tuples with index pairs.
+        """
+        index = 0
+        splits = []
+
+        for _, group in groupby(technique_numbers):
+            group_length = len(list(group))
+            splits.append((index, index + group_length))
+            index += group_length
+
+        return splits
+
+    @staticmethod
+    def _form_names_and_unit(series_header, column_header):
+        """Form names and unit from headers.
+
+        Args:
+            series_header (str): Something like "Iongauge value" or "C0M18"
+            column_header (str): Something like "Time [s]" or "Flow [ml/min]"
+
+        Returns:
+            str, str, Optional[str]: Return series_name, unit, standard_name
+        """
+        standard_name = None
+        if column_header in ("Time [s]", "time/s"):  # Form TimeSeries
+            unit = "s"
+            if series_header == "pot":
+                name = f"Potential {column_header.lower()}"
+            elif series_header == BIOLOGIC_SERIES_NAME:
+                name = f"Biologic {column_header.lower()}"
+            else:
+                name = f"{series_header} {column_header.lower()}"
+        else:  # ValueSeries
+            # Perform a bit of reasonable name adaption, first break name and unit out
+            # from the column header on the form: Pressure [mbar]
+            zilien_components_match = ZILIEN_COLUMN_HEADER_RE.match(column_header)
+            biologic_components_match = BIOLOGIC_COLUMN_HEADER_RE.match(column_header)
+
+            if zilien_components_match:
+                _, unit = zilien_components_match.groups()
+            elif biologic_components_match:
+                _, unit = biologic_components_match.groups()
+            else:
+                _, unit = column_header, ""
+
+            # Is the column a "setpoint" or "value" type
+            setpoint_or_value = None
+            for option in ("setpoint", "value"):
+                if series_header.endswith(option):
+                    setpoint_or_value = option
+
+            mass = to_mass(series_header)
+            if setpoint_or_value:
+                # In that case, the column header is something like "Flow [ml/min]" where
+                # "Flow" is unnecessary, because that is apparent from the unit
+                # The name will look for example like this "MFC setpoint [ml/min]"
+                name = f"{series_header} [{unit}]"
+            elif mass is not None:
+                # e.g. from series header "C1M4" and column header "M4-He [A]"
+                # the name will be "M4 [A]" and standard name will be "M4"
+                name = f"M{mass} [{unit}]"
+                standard_name = f"M{mass}"
+            else:
+                name = column_header
+
+        return name, unit, standard_name
+
+
+class ZilienTMPReader:
+    """A class for stitching the files in a Zilien tmp directory to an ECMSMeasurement
+
+    This is necessary because Zilien often crashes, leaving only the tmp directory.
+    This is less advanced but more readable than the Spectro Inlets stitching solution.
+    """
+
+    def __init__(self, path_to_tmp_dir=None):
+        self.path_to_tmp_dir = Path(path_to_tmp_dir) if path_to_tmp_dir else None
+
+    def read(self, path_to_tmp_dir, cls=None, **kwargs):
+        """Make a measurement from all the single-value .tsv files in a Zilien tmp dir
+
+        Args:
+            path_to_tmp_dir (Path or str): The path to the tmp dir
+            cls (Measurement class): Defaults to ECMSMeasurement
+        """
+        if path_to_tmp_dir:
+            self.path_to_tmp_dir = Path(path_to_tmp_dir)
+        cls = cls or ECMSMeasurement
+        name = self.path_to_tmp_dir.parent.name
+        timestamp_string = name[:19]  # the zilien timestamp is the first 19 chars
+        tstamp = timestamp_string_to_tstamp(timestamp_string, form=ZILIEN_TIMESTAMP_FORM)
+        series_list = []
+        for tmp_file in self.path_to_tmp_dir.iterdir():
+            series_list += series_list_from_tmp(tmp_file)
+        obj_as_dict = {
+            "name": name,
+            "tstamp": tstamp,
+            "series_list": series_list,
+            "technique": "EC-MS",
+            "reader": self,
+        }
+        obj_as_dict.update(kwargs)
+        return cls.from_dict(obj_as_dict)
+
+
+def series_list_from_tmp(path_to_file):
+    """Return [ValueSeries, TimeSeries] with the data in a zilien tmp .tsv file"""
+    file_name = Path(path_to_file).name
+    timestamp_string = file_name[:19]  # the zilien timestamp form is 19 chars long
+    tstamp = timestamp_string_to_tstamp(timestamp_string, form=ZILIEN_TIMESTAMP_FORM)
+    column_match = re.search(r"\.([^\.]+)\.data", file_name)
+    if not column_match:
+        print(f"could not find column name in {path_to_file}")
+        return []
+    v_name = column_match.group(1)
+    mass_match = re.search("M[0-9]+", v_name)
+    if mass_match:
+        v_name = mass_match.group()
+        unit = "A"
+    else:
+        unit = None
+    t_name = v_name + "-x"
+    df = pd.read_csv(path_to_file, delimiter="\t", names=[t_name, v_name], header=0)
+    t_data, v_data = df[t_name].to_numpy(), df[v_name].to_numpy()
+    tseries = TimeSeries(name=t_name, unit_name="s", data=t_data, tstamp=tstamp)
+    vseries = ValueSeries(name=v_name, unit_name=unit, data=v_data, tseries=tseries)
+    return [tseries, vseries]
+
+
+class ZilienSpectrumReader:
+    """A reader for individual Zilien spectra
+    TODO: A Zilien reader which loads all spectra at once in a SpectrumSeries object
+    """
+
+    def __init__(self, path_to_spectrum=None):
+        self.path_to_spectrum = Path(path_to_spectrum) if path_to_spectrum else None
+
+    def read(self, path_to_spectrum, cls=None, t_zero=None, **kwargs):
+        """Read a Zilien spectrum.
+        FIXME: This reader was written hastily and could be designed better.
+
+        Args:
+            path_to_spectrum(Path or str): the path to the spectrum file
+            cls (Spectrum class): Defaults to MSSpectrum
+            t_zero (float): The unix timestamp which the mass scan start time
+                is referenced to. Should be the tstamp of the corresponding
+                Zilien measurement. If the Spectrum is read individually, it needs
+                to be input or defaults to `time.time()`, i.e., now.
+            kwargs: Key-word arguments are passed on ultimately to cls.__init__
+        """
+        if path_to_spectrum:
+            self.path_to_spectrum = Path(path_to_spectrum)
+        cls = cls or MSSpectrum
+        t_zero = t_zero or time.time()
+
+        with open(self.path_to_spectrum, "r") as f:
+            in_header = True
+            num_line = 1
+            metadata_dict = {}
+            while in_header:
+                name, value = parse_metadata_line(f.readline())
+                metadata_dict[name] = value
+                if num_line == metadata_dict.get("num_header_lines", 0):
+                    in_header = False
+                num_line += 1
+
+        tstamp = t_zero + metadata_dict["mass_scan_started_at"]
+        duration = (
+            (metadata_dict["stop_mass"] - metadata_dict["start_mass"])
+            * metadata_dict["points_per_amu"]
+            * metadata_dict["dwell_time"]
+            * 1e-3
+        )
+
+        df = pd.read_csv(
+            self.path_to_spectrum,
+            header=metadata_dict["data_start"] - 1,
+            delimiter="\t",
+        )
+        y_name = "Current [A]"
+
+        for x_name in ZILIEN_MASS_COLUMN_NAMES:
+            try:
+                x = df[x_name].to_numpy()
+            except KeyError:
+                continue
+            break
+        else:
+            raise ReadError(
+                f"Can't find a mass column in {self.path_to_spectrum}. "
+                f"Looked for one of {ZILIEN_MASS_COLUMN_NAMES}"
+            )
+        y = df[y_name].to_numpy()
+
+        xseries = DataSeries(data=x, name=x_name, unit_name="m/z")
+        field = Field(
+            data=np.array(y),
+            name=y_name,
+            unit_name="A",
+            axes_series=[
+                xseries,
+            ],
+        )
+        obj_as_dict = {
+            "name": self.path_to_spectrum.name,
+            "technique": "MS_spectrum",
+            "field": field,
+            "reader": self,
+            "tstamp": tstamp,
+            "duration": duration,
+        }
+        obj_as_dict.update(kwargs)
+        return cls.from_dict(obj_as_dict)
+
+
+def module_demo():
+    """Module demo here.
+
+    To run this module in PyCharm, open Run Configuration and set
+        Module name = ixdat.readers.zilien,
+    and *not*
+        Script path = ...
+    """
+    path_to_test_file = (
+        Path(__file__).parent.resolve().parent.parent.parent
+        / "test_data"
+        / "Zilien version 1"
+        / "2022-04-06 16_17_23 full set.tsv"
+    )
+
+    ecms_measurement = Measurement.read(
+        reader="zilien",
+        path_to_file=path_to_test_file,
+    )
+
+    ecms_measurement.plot_measurement()
+    plt.show()
+
+
+if __name__ == "__main__":
+    module_demo()
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat/spectra.py` & `ixdat-0.2.9.dev3/src/ixdat/spectra.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,894 +1,894 @@
-"""Base classes for spectra and spectrum series
-
-
-Note on grammar:
-----------------
-The spectrum class corresponds to a database table which we call "spectrums". This
-is an intentional misspelling of the plural of "spectrum". The correctly spelled
-plural, "spectra", is reserved for a Field wrapping a 2-D array, each row of which
-is the y values of a spectrum. This use of two plurals of "spectrum" is analogous
-to the use of "persons" and "people" as distinct plurals of the word "person". While
-"persons" implies that each person referred to should be considered individually,
-"people" can be considered as a group.
-"""
-
-import warnings
-import numpy as np
-from .db import Saveable, fill_object_list, PlaceHolderObject
-from .data_series import DataSeries, TimeSeries, Field, time_shifted, append_series
-from .exceptions import BuildError
-from .plotters.spectrum_plotter import SpectrumPlotter, SpectrumSeriesPlotter
-from .exporters.spectrum_exporter import SpectrumExporter
-from .measurements import Measurement, get_combined_technique
-
-
-class Spectrum(Saveable):
-    """The Spectrum class.
-
-    A spectrum is a data structure including one-dimensional arrays of x and y variables
-    of equal length. Typically, information about the state of a sample can be obtained
-    from a plot of y (e.g. absorbtion OR intensity OR counts) vs x (e.g energy OR
-    wavelength OR angle OR mass-to-charge ratio). Even though in reality it takes time
-    to require a spectrum, a spectrum is considered to represent one instance in time.
-
-    In ixdat, the data of a spectrum is organized into a 1-Dimensional Field, where the
-    y-data is considered to span a space defined by the x-data.
-
-    The Spectrum class makes the data in this field intuitively available. If spec
-    is a spectrum, spec.x and spec.y give access to the x and y data, respectively,
-    while spec.xseries and spec.yseries give the corresponding DataSeries.
-    """
-
-    table_name = "spectrums"  # The misspelling is intentional. See :module:`~spectra`
-    column_attrs = {
-        "name",
-        "technique",
-        "metadata",
-        "tstamp",
-        "sample_name",
-        "field_id",
-    }
-    child_attrs = ["fields"]
-
-    def __init__(
-        self,
-        *,
-        name,
-        technique="spectrum",
-        metadata=None,
-        sample_name=None,
-        reader=None,
-        tstamp=None,
-        field=None,
-        field_id=None,
-        duration=None,
-    ):
-        """Initiate a spectrum
-
-        Args:
-            name (str): The name of the spectrum
-            metadata (dict): Free-form spectrum metadata. Must be json-compatible.
-            technique (str): The spectrum technique
-            sample_name (str): The sample name
-            reader (Reader): The reader, if read from file
-            tstamp (float): The unix epoch timestamp of the spectrum
-            field (Field): The Field containing the data (x, y, and tstamp)
-            field_id (id): The id in the data_series table of the Field with the data,
-                if the field is not yet loaded from backend.
-            duration (float): Optional. The duration of the spectrum measurement in [s]
-        """
-        super().__init__()
-        self.name = name
-        self.technique = technique
-        self.metadata = metadata
-        self.tstamp = tstamp
-        self.sample_name = sample_name
-        self.reader = reader
-        self.duration = duration
-        # Note: the PlaceHolderObject can be initiated without the backend because
-        #     if field_id is provided, then the relevant backend is the active one,
-        #     which PlaceHolderObject uses by default.
-        self._field = field or PlaceHolderObject(field_id, cls=Field)
-
-        self.plotter = SpectrumPlotter(spectrum=self)
-        # defining this method here gets it the right docstrings :D
-        self.plot = self.plotter.plot
-        self.exporter = SpectrumExporter(spectrum=self)
-        self.export = self.exporter.export
-
-    @classmethod
-    def read(cls, path_to_file, reader, **kwargs):
-        """Return a Measurement object from parsing a file with the specified reader
-
-        Args:
-            path_to_file (Path or str): The path to the file to read
-            reader (str or Reader class): The (name of the) reader to read the file with.
-            kwargs: key-word arguments are passed on to the reader's read() method.
-        """
-        if isinstance(reader, str):
-            # TODO: see if there isn't a way to put the import at the top of the module.
-            #    see: https://github.com/ixdat/ixdat/pull/1#discussion_r546437471
-            from .readers import SPECTRUM_READER_CLASSES
-
-            reader = SPECTRUM_READER_CLASSES[reader]()
-        # print(f"{__name__}. cls={cls}")  # debugging
-        return reader.read(path_to_file, cls=cls, **kwargs)
-
-    @classmethod
-    def read_set(
-        cls,
-        path_to_file_start=None,
-        part=None,
-        suffix=None,
-        file_list=None,
-        reader=None,
-        **kwargs,
-    ):
-        """Read a set of spectrum files and append them to return SpectrumSeries
-
-        Note: The list of spectrums is sorted by time.
-
-        Args:
-            path_to_file_start (Path or str): The path to the files to read including
-                the shared start of the file name: `Path(path_to_file).parent` is
-                interpreted as the folder where the file are.
-                `Path(path_to_file).name` is interpreted as the shared start of the files
-                to be appended.
-            part (Path or str): A path where the folder is the folder containing data
-                and the name is a part of the name of each of the files to be read and
-                combined.
-            suffix (str): If a suffix is given, only files with the specified ending are
-                added to the file list
-            file_list (list of Path): As an alternative to path_to_file_start or part,
-                the exact files to append can be specified in a list
-            reader (str or Reader class): The (name of the) reader to read the files with
-            kwargs: Key-word arguments are passed via cls.read() to the reader's read()
-                method, AND to SpectrumSeries.from_spectrum_list()
-        """
-        from .readers.reading_tools import get_file_list
-
-        file_list = file_list or get_file_list(path_to_file_start, part, suffix)
-        spectrum_list = []
-        for path_to_spectrum in file_list:
-            spectrum = Spectrum.read(path_to_spectrum, reader=reader, **kwargs)
-            spectrum_list.append(spectrum)
-
-        t_list = [spectrum.tstamp for spectrum in spectrum_list]
-        indeces = np.argsort(t_list)
-        spectrum_list = [spectrum_list[i] for i in indeces]
-
-        if issubclass(cls, SpectrumSeries):
-            spectra_class = cls
-        else:
-            spectra_class = SpectrumSeries
-
-        return spectra_class.from_spectrum_list(spectrum_list, **kwargs)
-
-    @property
-    def data_objects(self):
-        """The data-containing objects that need to be saved when the spectrum is saved.
-
-        For a field to be correctly saved and loaded, its axes_series must be saved
-        first. So there are three series in the data_objects to return
-        FIXME: with backend-specifying id's, field could check for itself whether
-        FIXME:  its axes_series are already in the database.
-        """
-        return self.series_list
-
-    @classmethod
-    def from_data(
-        cls,
-        x,
-        y,
-        tstamp=None,
-        x_name="x",
-        y_name="y",
-        x_unit_name=None,
-        y_unit_name=None,
-        **kwargs,
-    ):
-        """Initiate a spectrum from data. Does so via cls.from_series
-
-        Args:
-            x (np array): x data
-            y (np array): y data
-            tstamp (timestamp): The timestamp of the spectrum. Defaults to None.
-            x_name (str): Name of the x variable. Defaults to 'x'
-            y_name (str): Name of the y variable. Defaults to 'y'
-            x_unit_name (str): Name of the x unit. Defaults to None
-            y_unit_name (str): Name of the y unit. Defaults to None
-            kwargs: Key-word arguments are passed on ultimately to cls.__init__
-        """
-        xseries = DataSeries(data=x, name=x_name, unit_name=x_unit_name)
-        yseries = DataSeries(data=y, name=y_name, unit_name=y_unit_name)
-        return cls.from_series(xseries, yseries, tstamp, **kwargs)
-
-    @classmethod
-    def from_series(cls, xseries, yseries, tstamp, **kwargs):
-        """Initiate a spectrum from data. Does so via cls.from_field
-
-        Args:
-            xseries (DataSeries): A series with the x data
-            yseries (DataSeries): A series with the y data. The y data should be a
-                vector of the same length as the x data.
-            tstamp (timestamp): The timestamp of the spectrum. Defaults to None.
-            kwargs: Key-word arguments are passed on ultimately to cls.__init__
-        """
-        field = Field(
-            data=yseries.data,
-            axes_series=[xseries],
-            name=yseries.name,
-            unit_name=yseries.unit_name,
-        )
-        kwargs.update(tstamp=tstamp)
-        return cls.from_field(field, **kwargs)
-
-    @classmethod
-    def from_field(cls, field, **kwargs):
-        """Initiate a spectrum from data. Does so via cls.from_field
-
-        Args:
-            field (Field): The field containing all the data of the spectrum.
-                field.data is the y-data, which is considered to span x and t.
-                field.axes_series[0] is a DataSeries with the x data.
-                field.axes_series[1] is a TimeSeries with one time point.
-            kwargs: key-word arguments are passed on ultimately to cls.__init__
-        """
-        spectrum_as_dict = kwargs
-        spectrum_as_dict["field"] = field
-        if "name" not in spectrum_as_dict:
-            spectrum_as_dict["name"] = field.name
-        return cls.from_dict(spectrum_as_dict)
-
-    @property
-    def field(self):
-        """Since a spectrum can be loaded lazily, we make sure the field is loaded"""
-        if isinstance(self._field, PlaceHolderObject):
-            self._field = self._field.get_object()
-        return self._field
-
-    @property
-    def fields(self):
-        return [self.field]
-
-    @property
-    def field_id(self):
-        """The id of the field"""
-        return self.field.id
-
-    @property
-    def xseries(self):
-        """The x DataSeries is the first axis of the field"""
-        return self.field.axes_series[0]
-
-    @property
-    def series_list(self):
-        """A Spectrum's series list includes its field and its axes_series."""
-        return [self.field] + self.field.axes_series
-
-    @property
-    def x(self):
-        """The x data is the data attribute of the xseries"""
-        return self.xseries.data
-
-    @property
-    def x_name(self):
-        """The name of the x variable is the name attribute of the xseries"""
-        return self.xseries.name
-
-    @property
-    def yseries(self):
-        """The yseries is a DataSeries reduction of the field"""
-        return DataSeries(
-            name=self.field.name, data=self.y, unit_name=self.field.unit_name
-        )
-
-    @property
-    def y(self):
-        """The y data is the one-dimensional data attribute of the field"""
-        return self.field.data
-
-    @property
-    def y_name(self):
-        """The name of the y variable is the name attribute of the field"""
-        return self.field.name
-
-    @property
-    def tseries(self):
-        """The TimeSeries of a spectrum is a single point [0] and its tstamp"""
-        return TimeSeries(
-            name="time / [s]", unit_name="s", data=np.array([0]), tstamp=self.tstamp
-        )
-
-    def __add__(self, other):
-        """Adding spectra makes a (2)x(N_x) SpectrumSeries. self comes before other."""
-        if not self.x == other.x:  # FIXME: Some depreciation here. How else?
-            raise BuildError(
-                "can't add spectra with different `x`. "
-                # "Consider the function `append_spectra` instead."
-            )
-        t = np.array([0, other.tstamp - self.tstamp])
-        tseries = TimeSeries(
-            name="time / [s]", unit_name="s", data=t, tstamp=self.tstamp
-        )
-        new_field = Field(
-            name=self.name,
-            unit_name=self.field.unit_name,
-            data=np.array([self.y, other.y]),
-            axes_series=[tseries, self.xseries],
-        )
-        spectrum_series_as_dict = self.as_dict()
-        technique = self.technique
-        if technique.endswith("spectrum"):
-            technique = technique.rstrip("spectrum") + "spectra"
-        spectrum_series_as_dict.update(technique=technique)
-        spectrum_series_as_dict["field"] = new_field
-        del spectrum_series_as_dict["field_id"]
-
-        return SpectrumSeries.from_dict(spectrum_series_as_dict)
-
-
-class MultiSpectrum(Saveable):
-    """The MultiSpectrum class.
-
-    A collection of spectra having the same x values and tstamp. The y values of the
-    spectra in a MultiSpectrum can describe the same kind of thing, such as in the
-    multiple scans of an XPS measurement, where the average of the spectra is the
-    most-used quantity; or can different things, like fluorescence and transmission
-    measured simultaneously while varying the incident x-ray energy on a beamline.
-
-    Indexing with a spectrum name returns a `Spectrum` object with that thing, or a
-    smaller `MultiSpectrum` if there are multiple spectra with that name.
-    """
-
-    table_name = "multispectrum"
-    column_attrs = {
-        "name",
-        "technique",
-        "metadata",
-        "tstamp",
-        "sample_name",
-    }
-    extra_linkers = {"multispectrum_fields": {"data_series", "field_ids"}}
-    child_attrs = ["fields"]
-
-    def __init__(
-        self,
-        *,
-        name,
-        technique=None,
-        tstamp=None,
-        sample_name=None,
-        metadata=None,
-        fields=None,
-        field_ids=None,
-    ):
-        """Initiate a multi-spectrum
-
-        Args:
-            name (str): The name of the multi-spectrum
-            technique (str): The spectrum technique
-            tstamp (float): The unix epoch timestamp of the spectrum
-            sample_name (str): The sample name
-            metadata (dict): Free-form spectrum metadata. Must be json-compatible.
-            fields (list of Field): The Fields containing the data (x, y)
-            field_ids (list of int): The id's of Fields if available from the backend.
-        """
-        super().__init__()
-        self.name = name
-        self.technique = technique
-        self.metadata = metadata
-        self.tstamp = tstamp
-        self.sample_name = sample_name
-        self._fields = fill_object_list(object_list=fields, obj_ids=field_ids, cls=Field)
-        self._xseries = None
-        self._spectrum_list = None
-
-    @property
-    def fields(self):
-        """Make sure Fields are loaded and have the same xseries"""
-        xseries = None  # Enter the loop without an x series
-        for i, f in enumerate(self._fields):
-            if isinstance(f, PlaceHolderObject):
-                # load or "unpack" any fields for which only the id's were loaded:
-                self._fields[i] = f.get_object()
-            if i > 0:
-                # If all the xseries are the same, every field after the first should
-                # have an equivalent xseries to that of the previous field:
-                assert self._fields[i].axes_series[0] == xseries
-            # use the xseries of this field for comparison with the xseries of the next:
-            xseries = self._fields[i].axes_series[0]
-        # Now we've loaded any place-holder fields and checked their xseries are equal.
-        return self._fields
-
-    @property
-    def xseries(self):
-        """The shared xseries of all the spectra in the multi-spectrum"""
-        if not self._xseries:
-            self._xseries = self._fields[0].axes_series[0]
-        return self._xseries
-
-    @property
-    def spectrum_list(self):
-        """The spectra of the multi-spectrum as a list of Spectrum objects."""
-        if not self._spectrum_list:
-            self._spectrum_list = []
-            for field in self.fields:
-                s = Spectrum.from_field(
-                    field,
-                    name=field.name,
-                    technique=self.technique,
-                    metadata=self.metadata,
-                    tstamp=self.tstamp,
-                    sample_name=self.sample_name,
-                )
-                self._spectrum_list.append(s)
-        return self._spectrum_list
-
-    def __getitem__(self, name):
-        """Indexing a MultiSpectrum returns the spectrum with the requested name."""
-        spectrum_list = [s for s in self.spectrum_list if s.name == name]
-        if len(spectrum_list) == 1:
-            return spectrum_list[0]
-        elif len(spectrum_list) > 1:
-            return self.__class__.from_spectrum_list(
-                spectrum_list,
-                technique=self.technique,
-                metadata=self.metadata,
-            )
-
-    @classmethod
-    def from_spectrum_list(
-        cls, spectrum_list, technique=None, metadata=None, sample_name=None
-    ):
-        """Build a MultiSpectrum from a list of Spectrums"""
-        fields = [spectrum.field for spectrum in spectrum_list]
-        tstamp = spectrum_list[0].tstamp
-        if not technique:
-            technique = spectrum_list[0].technique
-            if technique.endswith("spectrum"):
-                technique = technique.rstrip("spectrum") + "spectra"
-        obj_as_dict = {
-            "fields": fields,
-            "technique": technique,
-            "metadata": metadata,
-            "tstamp": tstamp,
-            "sample_name": sample_name,
-        }
-        return cls.from_dict(obj_as_dict)
-
-
-class SpectrumSeries(Spectrum):
-    """The SpectrumSeries class.
-
-    A spectrum series is a data structure including a two-dimensional array, each row of
-    which is a spectrum, and each column of which is one spot in the spectrum as it
-    changes with some other variable.
-
-    In ixdat, the data of a spectrum series is organized into a Field, where the y-data
-    is considered to span a space defined by a DataSeries which is the x data, and a
-    DataSeries (typically a TimeSeries) which enumerates or specifies when or under
-    which conditions each spectrum was taken. The spectrum series will consider this
-    its "time" variable even if it is not actually time.
-
-    The SpectrumSeries class makes the data in this field intuitively available. If
-    spec is a spectrum series, spec.x is the x data with shape (N, ), spec.t is the
-    time data with shape (M, ), and spec.y is the spectrum data with shape (M, N).
-    """
-
-    def __init__(self, *args, **kwargs):
-        """Initiate a spectrum series
-
-        Args:
-            name (str): The name of the spectrum series
-            metadata (dict): Free-form spectrum metadata. Must be json-compatible.
-            technique (str): The spectrum technique
-            sample_name (str): The sample name
-            reader (Reader): The reader, if read from file
-            tstamp (float): The unix epoch timestamp of the spectrum
-            field (Field): The Field containing the data (x, y, and tstamp)
-            field_id (id): The id in the data_series table of the Field with the data,
-                if the field is not yet loaded from backend.
-            t_tolerance (float): The minimum relevant time difference between spectra
-                in [s]. Should correspond roughly to the time it takes for a spectrum
-                to be acquired. Defaults to the minimum of 1 second or 1/1000'th of the
-                average time between recorded spectra.
-            durations (list of float): The durations of each of the spectra in [s].
-            continuous (bool): Whether the spectra should be considered continuous, i.e.
-                whether plotting and grabbing functions should interpolate between
-                spectrums. Defaults to False.
-        """
-        if "technique" not in kwargs:
-            kwargs["technique"] = "spectra"
-        self._t_tolerance = kwargs.pop("t_tolerance", None)
-        # FIXME: durations and continuous are not in the serialization:
-        self.durations = kwargs.pop("durations", None)
-        self.continuous = kwargs.pop("continuous", False)
-        super().__init__(*args, **kwargs)
-        self.plotter = SpectrumSeriesPlotter(spectrum_series=self)
-        self.heat_plot = self.plotter.heat_plot
-        # can be overwritten in inheriting classes with e.g. plot_waterfall:
-        self.plot = self.plotter.heat_plot
-
-    @classmethod
-    def from_spectrum_list(cls, spectrum_list, **kwargs):
-        """Build a SpectrumSeries from a list of Spectrum objects."""
-        xseries = None
-        tstamp_list = []
-        ys = []
-        technique = kwargs.get("technique", None)
-        if not technique:
-            technique = spectrum_list[0].technique
-
-        for spectrum in spectrum_list:
-            tstamp_list.append(spectrum.tstamp)
-            xseries = xseries or spectrum.xseries
-            ys.append(spectrum.y)
-
-        tseries = TimeSeries(
-            name="Spectrum Time",
-            unit_name="s",
-            data=np.array(tstamp_list) - tstamp_list[0],
-            tstamp=tstamp_list[0],
-        )
-        field = Field(
-            name=spectrum_list[0].field.name,
-            unit_name=spectrum_list[0].field.unit_name,
-            axes_series=[tseries, xseries],
-            data=np.stack(ys),
-        )
-        if technique.endswith("spectrum"):
-            technique = technique.rstrip("spectrum") + "spectra"
-
-        obj_as_dict = spectrum_list[0].as_dict()
-        obj_as_dict["field"] = field
-        obj_as_dict["technique"] = technique
-        del obj_as_dict["field_id"]
-        # Any attribute we want in the SpecrumSeries for each spectrum should go here:
-        obj_as_dict["durations"] = [s.duration for s in spectrum_list]
-        return cls.from_dict(obj_as_dict)
-
-    @property
-    def field(self):
-        """Since a spectrum can be loaded lazily, we make sure the field is loaded
-
-        We also want to make sure that the field has the tstamp of the SpectrumSeries.
-        """
-        if isinstance(self._field, PlaceHolderObject):
-            self._field = self._field.get_object()
-
-        if abs(self._field.axes_series[0].tstamp - self.tstamp) > self.t_tolerance:
-            # self.t_tolerance is the resolution on the time axis of the spectra.
-            # If the t=0 of the SpectrumSeries differs from the tstamp of the field
-            # by more than this amount, it can become unclear which spectrum is which.
-            # Therefore, we shift the t=0 of the time axis of the field to match the t=0
-            # of the spectrum series. This doesn't change anything except the absolute
-            # time considered to be t=0.
-            self._field = Field(
-                name=self._field.name,
-                data=self._field.data,
-                unit_name=self._field.unit_name,
-                axes_series=[
-                    time_shifted(self._field.axes_series[0], tstamp=self.tstamp),
-                    self._field.axes_series[1],
-                ],
-            )
-        return self._field
-
-    @property
-    def yseries(self):
-        # Should this return an average or would that be counterintuitive?
-        raise BuildError(
-            f"{self!r} has no single y-series. Index it to get a Spectrum "
-            "or see `y_average`"
-        )
-
-    @property
-    def tseries(self):
-        """The TimeSeries of a SectrumSeries is the 0'th axis of its field.
-        Note that its data is not sorted!
-        """
-        return self.field.axes_series[0]
-
-    @property
-    def t(self):
-        """The time array of a SectrumSeries is the data of its tseries.
-
-        FIXME: It is the reader's job to make sure that `t` is increasing,
-           i.e. that the spectra are sorted by time.
-        """
-
-        return self.tseries.data
-
-    @property
-    def t_name(self):
-        """The name of the time variable of the spectrum series"""
-        return self.tseries.name
-
-    @property
-    def t_tolerance(self):
-        if self._t_tolerance is None:
-            # Note, accessing `self.t` here would lead to infinite recursion
-            #   due to `t_tolerance`'s use in the `field` property.
-            try:
-                t = self._field.axes_series[0].t
-            except AttributeError:
-                raise AttributeError(
-                    "`SpectrumSeries` object tried to use time data from its `Field`"
-                    " to determine its time tolerance but its `_field` was "
-                    f"{self._field}."
-                )
-            tolerance_by_spacing = 1 / 1000 * (t[-1] - t[0]) / len(t)
-            self._t_tolerance = min(tolerance_by_spacing, 1)
-        return self._t_tolerance
-
-    @property
-    def xseries(self):
-        """The x-axis DataSeries of a SectrumSeries is the 1'st axis of its field"""
-        return self.field.axes_series[1]
-
-    @property
-    def x(self):
-        """The x (scanning variable) data"""
-        return self.xseries.data
-
-    @property
-    def x_name(self):
-        """The name of the scanning variable"""
-        return self.xseries.name
-
-    @property
-    def y(self):
-        """The y data is the multi-dimensional data attribute of the field"""
-        return self.field.data
-
-    def __getitem__(self, key):
-        """Indexing a SpectrumSeries with an int n returns its n'th spectrum"""
-        from .techniques import TECHNIQUE_CLASSES
-
-        if self.technique in TECHNIQUE_CLASSES:
-            # TECHNIQUE_CLASSES points, as a rule, to the Spectrum clas, not the
-            # SpectrumSeries class of a given technique.
-            cls = TECHNIQUE_CLASSES[self.technique]
-            if issubclass(cls, SpectrumSeries):
-                # ... however, if TECHNIQUE_CLASSES does point to a SpectrumSeries
-                # class, we need indexing to return a default spectrum.
-                cls = Spectrum
-        else:
-            cls = Spectrum
-
-        if isinstance(key, int):
-            spectrum_as_dict = self.as_dict()
-            del spectrum_as_dict["field_id"]
-            spectrum_as_dict["field"] = Field(
-                # note that it's important in some cases that the spectrum does not have
-                # the same name as the spectrum series:
-                name=self.y_name + "_" + str(key),
-                unit_name=self.field.unit_name,
-                data=self.y[key],
-                axes_series=[self.xseries],
-            )
-            spectrum_as_dict["tstamp"] = self.tstamp + self.t[key]
-            if self.durations:
-                spectrum_as_dict["duration"] = self.durations[key]
-            return cls.from_dict(spectrum_as_dict)
-        raise KeyError
-
-    def cut(self, tspan, t_zero=None):
-        """Return a subset with the spectrums falling in a specified timespan
-
-        Args:
-            tspan (timespan): The timespan within which you want spectrums
-            t_zero (float): The shift in t=0 with respect to the original
-                spectrum series, in [s].
-        """
-        t_mask = np.logical_and(tspan[0] < self.t, self.t < tspan[1])
-        new_t = self.t[t_mask]
-        new_tseries = TimeSeries(
-            name=self.tseries.name,
-            unit_name=self.tseries.unit_name,
-            data=new_t,
-            tstamp=self.tseries.tstamp,
-        )
-        new_y = self.y[t_mask, :]
-        new_field = Field(
-            name=self.field.name,
-            unit_name=self.field.unit_name,
-            data=new_y,
-            axes_series=[new_tseries, self.xseries],
-        )
-        new_durations = np.array(self.durations)[t_mask]
-        cut_spectrum_series_as_dict = self.as_dict()
-        cut_spectrum_series_as_dict.update(
-            field=new_field,
-            durations=new_durations,
-        )
-        cut_spectrum_series = self.__class__.from_dict(cut_spectrum_series_as_dict)
-        if t_zero:
-            if t_zero == "start":
-                cut_spectrum_series.tstamp += tspan[0]
-            else:
-                cut_spectrum_series.tstamp += t_zero
-        return cut_spectrum_series
-
-    @property
-    def y_average(self):
-        """The y-data of the average spectrum"""
-        return np.mean(self.y, axis=0)
-
-    def __add__(self, other):
-        if type(other) is type(self):  # Then we are appending!
-            obj_as_dict = self.as_dict()
-            new_y_name = self.y_name
-            if not self.y_name == other.y_name:
-                new_y_name = self.y_name + "AND" + other.y_name
-                warnings.warn(
-                    "Appending spectra with different names: \n"
-                    f"{new_y_name}.\n"
-                    "Result might not be meaningful."
-                )
-            new_x_name = self.x_name
-            new_xseries = self.xseries
-            if not self.xseries.shape == other.xseries.shape:
-                raise TypeError(
-                    "Cannot append SpectrumSeries with different shapes "
-                    "of their x series!"
-                )
-            if not self.xseries.unit_name == other.xseries.unit_name:
-                raise TypeError(
-                    "Cannot append SpectrumSeries with different units "
-                    "of their x series!"
-                )
-            if not self.x_name == other.x_name:
-                new_x_name = self.x_name + "OR" + other.x_name
-                warnings.warn(
-                    "Appending spectra with different names: \n"
-                    f"{new_x_name}.\n"
-                    "Result might not be meaningful."
-                )
-                new_xseries = DataSeries(
-                    name=new_x_name, unit_name=self.xseries.unit_name, data=self.x
-                )
-            new_tseries = append_series([self.tseries, other.tseries])
-            new_y = np.append(self.y, other.y, axis=0)
-            new_field = Field(
-                name=new_y_name,
-                unit_name=self.field.unit_name,
-                data=new_y,
-                axes_series=[new_tseries, new_xseries],
-            )
-            new_durations = np.append(self.durations, other.durations)
-            obj_as_dict.update(
-                field=new_field,
-                durations=new_durations,
-            )
-            return self.__class__(**obj_as_dict)
-
-        if isinstance(other, Measurement):
-            return add_spectrum_series_to_measurement(other, self)
-
-
-def add_spectrum_series_to_measurement(measurement, spectrum_series, **kwargs):
-    """Add a measurement and a spectrum measurement.
-
-    Args:
-        measurement (Measurement): The `Measurement` object containing the time-resolved
-            scalar values.
-        spectrum_series (SpectrumSeries): The `SpectrumSeries` object containing the 2-D
-            time-resolved spectral data.
-        kwargs: Additional key-word arguments are passed on to the `from_dict`
-            constructor of the resulting object.
-
-    Returns SpectroMeasurement: The addition results in an object of SpectroMeasurement
-        or a subclass thereof if ixdat supports the hyphenated technique. For example,
-        addition of an `ECMeasurement` and an XAS `SpectrumSeries` results in an
-        `ECXASMeasurement` object.
-    """
-    new_name = measurement.name + " AND " + spectrum_series.name
-    new_technique = get_combined_technique(
-        measurement.technique, spectrum_series.technique
-    )
-
-    # TODO: see if there isn't a way to put the import at the top of the module.
-    #    see: https://github.com/ixdat/ixdat/pull/1#discussion_r546437410
-    from .techniques import TECHNIQUE_CLASSES
-
-    obj_as_dict = measurement.as_dict()
-    obj_as_dict["spectrum_series"] = spectrum_series
-    obj_as_dict["name"] = new_name
-    obj_as_dict["technique"] = new_technique
-
-    if new_technique in TECHNIQUE_CLASSES:
-        cls = TECHNIQUE_CLASSES[new_technique]
-    else:
-        cls = SpectroMeasurement
-    if issubclass(cls, TECHNIQUE_CLASSES["EC-Optical"]):
-        # Then we need a reference spectrum!
-        # But so far the only EC-Optical reader doesn't support reading Optical and
-        # EC parts separately, so this needs not be implemented yet.
-        raise NotImplementedError("addition of EC and Optical not yet supported.")
-
-    obj_as_dict.update(kwargs)
-    return cls.from_dict(obj_as_dict)
-
-
-class SpectroMeasurement(Measurement):
-    child_attrs = ["spectrum_series_list"] + Measurement.child_attrs
-    extra_column_attrs = {"spectro_measurements": {"spectrum_id"}}
-
-    def __init__(self, *args, spectrum_series=None, spectrum_id=None, **kwargs):
-        super().__init__(*args, **kwargs)
-        if spectrum_series:
-            self._spectrum_series = spectrum_series
-            self._spectrum_series.tstamp = self.tstamp
-        elif spectrum_id:
-            self._spectrum_series = PlaceHolderObject(spectrum_id, cls=SpectrumSeries)
-        else:
-            raise TypeError(
-                "A SpectroMeasurement must be "
-                "initialized with a `spectrum_series` or `spectrum_id`"
-            )
-
-    @property
-    def spectrum_series(self):
-        """The `SpectrumSeries` with the spectral data"""
-        if isinstance(self._spectrum_series, PlaceHolderObject):
-            self._spectrum_series = self._spectrum_series.get_object()
-            self._spectrum_series.tstamp = self.tstamp
-        return self._spectrum_series
-
-    # FIXME: The attribute below is needed in order to correctly
-    #   pass the spectrum_series between objects using its id,
-    #   because "child_attrs" only works on lists.
-    @property
-    def spectrum_series_list(self):
-        return [self.spectrum_series]
-
-    @property
-    def spectrum_id(self):
-        """The id of the `SpectrumSeries`"""
-        return self.spectrum_series.short_identity
-
-    @property
-    def spectra(self):
-        """The field of the `SpectrumSeries`. `spectra.data` is a 2-D array"""
-        return self.spectrum_series.field
-
-    def set_spectrum_series(self, spectrum_series):
-        """(Re-)set the `spectrum_series` to a provided `spectrum_series`"""
-        self._spectrum_series = spectrum_series
-
-    @property
-    def continuous(self):
-        return self.spectrum_series.continuous
-
-    @Measurement.tstamp.setter
-    def tstamp(self, tstamp):
-        self._tstamp = tstamp
-        # Resetting the tstamp needs to reset the `spectrum_series`' tstamp as well:
-        self.spectrum_series.tstamp = tstamp
-        # As before it also needs to clear the cache, so series are returned wrt the
-        # new timestamp.
-        self.clear_cache()
-
-    def __getitem__(self, item):
-        if isinstance(item, int) or isinstance(item, slice):
-            return self.spectrum_series[item]
-        return super().__getitem__(item)
-
-    def __add__(self, other):
-        added_measurement = super().__add__(other)
-        if isinstance(other, SpectroMeasurement):
-            spectrum_series = self.spectrum_series + other.spectrum_series
-            added_measurement.set_spectrum_series(spectrum_series)
-        return added_measurement
-
-    def cut(self, tspan, t_zero=None):
-        """Select the portion of the data in a given tspan.
-
-        See :func:`~measurements.Measurement.cut`
-        """
-        cut_measurement = super().cut(tspan, t_zero=t_zero)
-        spectrum_series = self.spectrum_series.cut(tspan=tspan, t_zero=t_zero)
-        cut_measurement.set_spectrum_series(spectrum_series)
-        return cut_measurement
+"""Base classes for spectra and spectrum series
+
+
+Note on grammar:
+----------------
+The spectrum class corresponds to a database table which we call "spectrums". This
+is an intentional misspelling of the plural of "spectrum". The correctly spelled
+plural, "spectra", is reserved for a Field wrapping a 2-D array, each row of which
+is the y values of a spectrum. This use of two plurals of "spectrum" is analogous
+to the use of "persons" and "people" as distinct plurals of the word "person". While
+"persons" implies that each person referred to should be considered individually,
+"people" can be considered as a group.
+"""
+
+import warnings
+import numpy as np
+from .db import Saveable, fill_object_list, PlaceHolderObject
+from .data_series import DataSeries, TimeSeries, Field, time_shifted, append_series
+from .exceptions import BuildError
+from .plotters.spectrum_plotter import SpectrumPlotter, SpectrumSeriesPlotter
+from .exporters.spectrum_exporter import SpectrumExporter
+from .measurements import Measurement, get_combined_technique
+
+
+class Spectrum(Saveable):
+    """The Spectrum class.
+
+    A spectrum is a data structure including one-dimensional arrays of x and y variables
+    of equal length. Typically, information about the state of a sample can be obtained
+    from a plot of y (e.g. absorbtion OR intensity OR counts) vs x (e.g energy OR
+    wavelength OR angle OR mass-to-charge ratio). Even though in reality it takes time
+    to require a spectrum, a spectrum is considered to represent one instance in time.
+
+    In ixdat, the data of a spectrum is organized into a 1-Dimensional Field, where the
+    y-data is considered to span a space defined by the x-data.
+
+    The Spectrum class makes the data in this field intuitively available. If spec
+    is a spectrum, spec.x and spec.y give access to the x and y data, respectively,
+    while spec.xseries and spec.yseries give the corresponding DataSeries.
+    """
+
+    table_name = "spectrums"  # The misspelling is intentional. See :module:`~spectra`
+    column_attrs = {
+        "name",
+        "technique",
+        "metadata",
+        "tstamp",
+        "sample_name",
+        "field_id",
+    }
+    child_attrs = ["fields"]
+
+    def __init__(
+        self,
+        *,
+        name,
+        technique="spectrum",
+        metadata=None,
+        sample_name=None,
+        reader=None,
+        tstamp=None,
+        field=None,
+        field_id=None,
+        duration=None,
+    ):
+        """Initiate a spectrum
+
+        Args:
+            name (str): The name of the spectrum
+            metadata (dict): Free-form spectrum metadata. Must be json-compatible.
+            technique (str): The spectrum technique
+            sample_name (str): The sample name
+            reader (Reader): The reader, if read from file
+            tstamp (float): The unix epoch timestamp of the spectrum
+            field (Field): The Field containing the data (x, y, and tstamp)
+            field_id (id): The id in the data_series table of the Field with the data,
+                if the field is not yet loaded from backend.
+            duration (float): Optional. The duration of the spectrum measurement in [s]
+        """
+        super().__init__()
+        self.name = name
+        self.technique = technique
+        self.metadata = metadata
+        self.tstamp = tstamp
+        self.sample_name = sample_name
+        self.reader = reader
+        self.duration = duration
+        # Note: the PlaceHolderObject can be initiated without the backend because
+        #     if field_id is provided, then the relevant backend is the active one,
+        #     which PlaceHolderObject uses by default.
+        self._field = field or PlaceHolderObject(field_id, cls=Field)
+
+        self.plotter = SpectrumPlotter(spectrum=self)
+        # defining this method here gets it the right docstrings :D
+        self.plot = self.plotter.plot
+        self.exporter = SpectrumExporter(spectrum=self)
+        self.export = self.exporter.export
+
+    @classmethod
+    def read(cls, path_to_file, reader, **kwargs):
+        """Return a Measurement object from parsing a file with the specified reader
+
+        Args:
+            path_to_file (Path or str): The path to the file to read
+            reader (str or Reader class): The (name of the) reader to read the file with.
+            kwargs: key-word arguments are passed on to the reader's read() method.
+        """
+        if isinstance(reader, str):
+            # TODO: see if there isn't a way to put the import at the top of the module.
+            #    see: https://github.com/ixdat/ixdat/pull/1#discussion_r546437471
+            from .readers import SPECTRUM_READER_CLASSES
+
+            reader = SPECTRUM_READER_CLASSES[reader]()
+        # print(f"{__name__}. cls={cls}")  # debugging
+        return reader.read(path_to_file, cls=cls, **kwargs)
+
+    @classmethod
+    def read_set(
+        cls,
+        path_to_file_start=None,
+        part=None,
+        suffix=None,
+        file_list=None,
+        reader=None,
+        **kwargs,
+    ):
+        """Read a set of spectrum files and append them to return SpectrumSeries
+
+        Note: The list of spectrums is sorted by time.
+
+        Args:
+            path_to_file_start (Path or str): The path to the files to read including
+                the shared start of the file name: `Path(path_to_file).parent` is
+                interpreted as the folder where the file are.
+                `Path(path_to_file).name` is interpreted as the shared start of the files
+                to be appended.
+            part (Path or str): A path where the folder is the folder containing data
+                and the name is a part of the name of each of the files to be read and
+                combined.
+            suffix (str): If a suffix is given, only files with the specified ending are
+                added to the file list
+            file_list (list of Path): As an alternative to path_to_file_start or part,
+                the exact files to append can be specified in a list
+            reader (str or Reader class): The (name of the) reader to read the files with
+            kwargs: Key-word arguments are passed via cls.read() to the reader's read()
+                method, AND to SpectrumSeries.from_spectrum_list()
+        """
+        from .readers.reading_tools import get_file_list
+
+        file_list = file_list or get_file_list(path_to_file_start, part, suffix)
+        spectrum_list = []
+        for path_to_spectrum in file_list:
+            spectrum = Spectrum.read(path_to_spectrum, reader=reader, **kwargs)
+            spectrum_list.append(spectrum)
+
+        t_list = [spectrum.tstamp for spectrum in spectrum_list]
+        indeces = np.argsort(t_list)
+        spectrum_list = [spectrum_list[i] for i in indeces]
+
+        if issubclass(cls, SpectrumSeries):
+            spectra_class = cls
+        else:
+            spectra_class = SpectrumSeries
+
+        return spectra_class.from_spectrum_list(spectrum_list, **kwargs)
+
+    @property
+    def data_objects(self):
+        """The data-containing objects that need to be saved when the spectrum is saved.
+
+        For a field to be correctly saved and loaded, its axes_series must be saved
+        first. So there are three series in the data_objects to return
+        FIXME: with backend-specifying id's, field could check for itself whether
+        FIXME:  its axes_series are already in the database.
+        """
+        return self.series_list
+
+    @classmethod
+    def from_data(
+        cls,
+        x,
+        y,
+        tstamp=None,
+        x_name="x",
+        y_name="y",
+        x_unit_name=None,
+        y_unit_name=None,
+        **kwargs,
+    ):
+        """Initiate a spectrum from data. Does so via cls.from_series
+
+        Args:
+            x (np array): x data
+            y (np array): y data
+            tstamp (timestamp): The timestamp of the spectrum. Defaults to None.
+            x_name (str): Name of the x variable. Defaults to 'x'
+            y_name (str): Name of the y variable. Defaults to 'y'
+            x_unit_name (str): Name of the x unit. Defaults to None
+            y_unit_name (str): Name of the y unit. Defaults to None
+            kwargs: Key-word arguments are passed on ultimately to cls.__init__
+        """
+        xseries = DataSeries(data=x, name=x_name, unit_name=x_unit_name)
+        yseries = DataSeries(data=y, name=y_name, unit_name=y_unit_name)
+        return cls.from_series(xseries, yseries, tstamp, **kwargs)
+
+    @classmethod
+    def from_series(cls, xseries, yseries, tstamp, **kwargs):
+        """Initiate a spectrum from data. Does so via cls.from_field
+
+        Args:
+            xseries (DataSeries): A series with the x data
+            yseries (DataSeries): A series with the y data. The y data should be a
+                vector of the same length as the x data.
+            tstamp (timestamp): The timestamp of the spectrum. Defaults to None.
+            kwargs: Key-word arguments are passed on ultimately to cls.__init__
+        """
+        field = Field(
+            data=yseries.data,
+            axes_series=[xseries],
+            name=yseries.name,
+            unit_name=yseries.unit_name,
+        )
+        kwargs.update(tstamp=tstamp)
+        return cls.from_field(field, **kwargs)
+
+    @classmethod
+    def from_field(cls, field, **kwargs):
+        """Initiate a spectrum from data. Does so via cls.from_field
+
+        Args:
+            field (Field): The field containing all the data of the spectrum.
+                field.data is the y-data, which is considered to span x and t.
+                field.axes_series[0] is a DataSeries with the x data.
+                field.axes_series[1] is a TimeSeries with one time point.
+            kwargs: key-word arguments are passed on ultimately to cls.__init__
+        """
+        spectrum_as_dict = kwargs
+        spectrum_as_dict["field"] = field
+        if "name" not in spectrum_as_dict:
+            spectrum_as_dict["name"] = field.name
+        return cls.from_dict(spectrum_as_dict)
+
+    @property
+    def field(self):
+        """Since a spectrum can be loaded lazily, we make sure the field is loaded"""
+        if isinstance(self._field, PlaceHolderObject):
+            self._field = self._field.get_object()
+        return self._field
+
+    @property
+    def fields(self):
+        return [self.field]
+
+    @property
+    def field_id(self):
+        """The id of the field"""
+        return self.field.id
+
+    @property
+    def xseries(self):
+        """The x DataSeries is the first axis of the field"""
+        return self.field.axes_series[0]
+
+    @property
+    def series_list(self):
+        """A Spectrum's series list includes its field and its axes_series."""
+        return [self.field] + self.field.axes_series
+
+    @property
+    def x(self):
+        """The x data is the data attribute of the xseries"""
+        return self.xseries.data
+
+    @property
+    def x_name(self):
+        """The name of the x variable is the name attribute of the xseries"""
+        return self.xseries.name
+
+    @property
+    def yseries(self):
+        """The yseries is a DataSeries reduction of the field"""
+        return DataSeries(
+            name=self.field.name, data=self.y, unit_name=self.field.unit_name
+        )
+
+    @property
+    def y(self):
+        """The y data is the one-dimensional data attribute of the field"""
+        return self.field.data
+
+    @property
+    def y_name(self):
+        """The name of the y variable is the name attribute of the field"""
+        return self.field.name
+
+    @property
+    def tseries(self):
+        """The TimeSeries of a spectrum is a single point [0] and its tstamp"""
+        return TimeSeries(
+            name="time / [s]", unit_name="s", data=np.array([0]), tstamp=self.tstamp
+        )
+
+    def __add__(self, other):
+        """Adding spectra makes a (2)x(N_x) SpectrumSeries. self comes before other."""
+        if not self.x == other.x:  # FIXME: Some depreciation here. How else?
+            raise BuildError(
+                "can't add spectra with different `x`. "
+                # "Consider the function `append_spectra` instead."
+            )
+        t = np.array([0, other.tstamp - self.tstamp])
+        tseries = TimeSeries(
+            name="time / [s]", unit_name="s", data=t, tstamp=self.tstamp
+        )
+        new_field = Field(
+            name=self.name,
+            unit_name=self.field.unit_name,
+            data=np.array([self.y, other.y]),
+            axes_series=[tseries, self.xseries],
+        )
+        spectrum_series_as_dict = self.as_dict()
+        technique = self.technique
+        if technique.endswith("spectrum"):
+            technique = technique.rstrip("spectrum") + "spectra"
+        spectrum_series_as_dict.update(technique=technique)
+        spectrum_series_as_dict["field"] = new_field
+        del spectrum_series_as_dict["field_id"]
+
+        return SpectrumSeries.from_dict(spectrum_series_as_dict)
+
+
+class MultiSpectrum(Saveable):
+    """The MultiSpectrum class.
+
+    A collection of spectra having the same x values and tstamp. The y values of the
+    spectra in a MultiSpectrum can describe the same kind of thing, such as in the
+    multiple scans of an XPS measurement, where the average of the spectra is the
+    most-used quantity; or can different things, like fluorescence and transmission
+    measured simultaneously while varying the incident x-ray energy on a beamline.
+
+    Indexing with a spectrum name returns a `Spectrum` object with that thing, or a
+    smaller `MultiSpectrum` if there are multiple spectra with that name.
+    """
+
+    table_name = "multispectrum"
+    column_attrs = {
+        "name",
+        "technique",
+        "metadata",
+        "tstamp",
+        "sample_name",
+    }
+    extra_linkers = {"multispectrum_fields": {"data_series", "field_ids"}}
+    child_attrs = ["fields"]
+
+    def __init__(
+        self,
+        *,
+        name,
+        technique=None,
+        tstamp=None,
+        sample_name=None,
+        metadata=None,
+        fields=None,
+        field_ids=None,
+    ):
+        """Initiate a multi-spectrum
+
+        Args:
+            name (str): The name of the multi-spectrum
+            technique (str): The spectrum technique
+            tstamp (float): The unix epoch timestamp of the spectrum
+            sample_name (str): The sample name
+            metadata (dict): Free-form spectrum metadata. Must be json-compatible.
+            fields (list of Field): The Fields containing the data (x, y)
+            field_ids (list of int): The id's of Fields if available from the backend.
+        """
+        super().__init__()
+        self.name = name
+        self.technique = technique
+        self.metadata = metadata
+        self.tstamp = tstamp
+        self.sample_name = sample_name
+        self._fields = fill_object_list(object_list=fields, obj_ids=field_ids, cls=Field)
+        self._xseries = None
+        self._spectrum_list = None
+
+    @property
+    def fields(self):
+        """Make sure Fields are loaded and have the same xseries"""
+        xseries = None  # Enter the loop without an x series
+        for i, f in enumerate(self._fields):
+            if isinstance(f, PlaceHolderObject):
+                # load or "unpack" any fields for which only the id's were loaded:
+                self._fields[i] = f.get_object()
+            if i > 0:
+                # If all the xseries are the same, every field after the first should
+                # have an equivalent xseries to that of the previous field:
+                assert self._fields[i].axes_series[0] == xseries
+            # use the xseries of this field for comparison with the xseries of the next:
+            xseries = self._fields[i].axes_series[0]
+        # Now we've loaded any place-holder fields and checked their xseries are equal.
+        return self._fields
+
+    @property
+    def xseries(self):
+        """The shared xseries of all the spectra in the multi-spectrum"""
+        if not self._xseries:
+            self._xseries = self._fields[0].axes_series[0]
+        return self._xseries
+
+    @property
+    def spectrum_list(self):
+        """The spectra of the multi-spectrum as a list of Spectrum objects."""
+        if not self._spectrum_list:
+            self._spectrum_list = []
+            for field in self.fields:
+                s = Spectrum.from_field(
+                    field,
+                    name=field.name,
+                    technique=self.technique,
+                    metadata=self.metadata,
+                    tstamp=self.tstamp,
+                    sample_name=self.sample_name,
+                )
+                self._spectrum_list.append(s)
+        return self._spectrum_list
+
+    def __getitem__(self, name):
+        """Indexing a MultiSpectrum returns the spectrum with the requested name."""
+        spectrum_list = [s for s in self.spectrum_list if s.name == name]
+        if len(spectrum_list) == 1:
+            return spectrum_list[0]
+        elif len(spectrum_list) > 1:
+            return self.__class__.from_spectrum_list(
+                spectrum_list,
+                technique=self.technique,
+                metadata=self.metadata,
+            )
+
+    @classmethod
+    def from_spectrum_list(
+        cls, spectrum_list, technique=None, metadata=None, sample_name=None
+    ):
+        """Build a MultiSpectrum from a list of Spectrums"""
+        fields = [spectrum.field for spectrum in spectrum_list]
+        tstamp = spectrum_list[0].tstamp
+        if not technique:
+            technique = spectrum_list[0].technique
+            if technique.endswith("spectrum"):
+                technique = technique.rstrip("spectrum") + "spectra"
+        obj_as_dict = {
+            "fields": fields,
+            "technique": technique,
+            "metadata": metadata,
+            "tstamp": tstamp,
+            "sample_name": sample_name,
+        }
+        return cls.from_dict(obj_as_dict)
+
+
+class SpectrumSeries(Spectrum):
+    """The SpectrumSeries class.
+
+    A spectrum series is a data structure including a two-dimensional array, each row of
+    which is a spectrum, and each column of which is one spot in the spectrum as it
+    changes with some other variable.
+
+    In ixdat, the data of a spectrum series is organized into a Field, where the y-data
+    is considered to span a space defined by a DataSeries which is the x data, and a
+    DataSeries (typically a TimeSeries) which enumerates or specifies when or under
+    which conditions each spectrum was taken. The spectrum series will consider this
+    its "time" variable even if it is not actually time.
+
+    The SpectrumSeries class makes the data in this field intuitively available. If
+    spec is a spectrum series, spec.x is the x data with shape (N, ), spec.t is the
+    time data with shape (M, ), and spec.y is the spectrum data with shape (M, N).
+    """
+
+    def __init__(self, *args, **kwargs):
+        """Initiate a spectrum series
+
+        Args:
+            name (str): The name of the spectrum series
+            metadata (dict): Free-form spectrum metadata. Must be json-compatible.
+            technique (str): The spectrum technique
+            sample_name (str): The sample name
+            reader (Reader): The reader, if read from file
+            tstamp (float): The unix epoch timestamp of the spectrum
+            field (Field): The Field containing the data (x, y, and tstamp)
+            field_id (id): The id in the data_series table of the Field with the data,
+                if the field is not yet loaded from backend.
+            t_tolerance (float): The minimum relevant time difference between spectra
+                in [s]. Should correspond roughly to the time it takes for a spectrum
+                to be acquired. Defaults to the minimum of 1 second or 1/1000'th of the
+                average time between recorded spectra.
+            durations (list of float): The durations of each of the spectra in [s].
+            continuous (bool): Whether the spectra should be considered continuous, i.e.
+                whether plotting and grabbing functions should interpolate between
+                spectrums. Defaults to False.
+        """
+        if "technique" not in kwargs:
+            kwargs["technique"] = "spectra"
+        self._t_tolerance = kwargs.pop("t_tolerance", None)
+        # FIXME: durations and continuous are not in the serialization:
+        self.durations = kwargs.pop("durations", None)
+        self.continuous = kwargs.pop("continuous", False)
+        super().__init__(*args, **kwargs)
+        self.plotter = SpectrumSeriesPlotter(spectrum_series=self)
+        self.heat_plot = self.plotter.heat_plot
+        # can be overwritten in inheriting classes with e.g. plot_waterfall:
+        self.plot = self.plotter.heat_plot
+
+    @classmethod
+    def from_spectrum_list(cls, spectrum_list, **kwargs):
+        """Build a SpectrumSeries from a list of Spectrum objects."""
+        xseries = None
+        tstamp_list = []
+        ys = []
+        technique = kwargs.get("technique", None)
+        if not technique:
+            technique = spectrum_list[0].technique
+
+        for spectrum in spectrum_list:
+            tstamp_list.append(spectrum.tstamp)
+            xseries = xseries or spectrum.xseries
+            ys.append(spectrum.y)
+
+        tseries = TimeSeries(
+            name="Spectrum Time",
+            unit_name="s",
+            data=np.array(tstamp_list) - tstamp_list[0],
+            tstamp=tstamp_list[0],
+        )
+        field = Field(
+            name=spectrum_list[0].field.name,
+            unit_name=spectrum_list[0].field.unit_name,
+            axes_series=[tseries, xseries],
+            data=np.stack(ys),
+        )
+        if technique.endswith("spectrum"):
+            technique = technique.rstrip("spectrum") + "spectra"
+
+        obj_as_dict = spectrum_list[0].as_dict()
+        obj_as_dict["field"] = field
+        obj_as_dict["technique"] = technique
+        del obj_as_dict["field_id"]
+        # Any attribute we want in the SpecrumSeries for each spectrum should go here:
+        obj_as_dict["durations"] = [s.duration for s in spectrum_list]
+        return cls.from_dict(obj_as_dict)
+
+    @property
+    def field(self):
+        """Since a spectrum can be loaded lazily, we make sure the field is loaded
+
+        We also want to make sure that the field has the tstamp of the SpectrumSeries.
+        """
+        if isinstance(self._field, PlaceHolderObject):
+            self._field = self._field.get_object()
+
+        if abs(self._field.axes_series[0].tstamp - self.tstamp) > self.t_tolerance:
+            # self.t_tolerance is the resolution on the time axis of the spectra.
+            # If the t=0 of the SpectrumSeries differs from the tstamp of the field
+            # by more than this amount, it can become unclear which spectrum is which.
+            # Therefore, we shift the t=0 of the time axis of the field to match the t=0
+            # of the spectrum series. This doesn't change anything except the absolute
+            # time considered to be t=0.
+            self._field = Field(
+                name=self._field.name,
+                data=self._field.data,
+                unit_name=self._field.unit_name,
+                axes_series=[
+                    time_shifted(self._field.axes_series[0], tstamp=self.tstamp),
+                    self._field.axes_series[1],
+                ],
+            )
+        return self._field
+
+    @property
+    def yseries(self):
+        # Should this return an average or would that be counterintuitive?
+        raise BuildError(
+            f"{self!r} has no single y-series. Index it to get a Spectrum "
+            "or see `y_average`"
+        )
+
+    @property
+    def tseries(self):
+        """The TimeSeries of a SectrumSeries is the 0'th axis of its field.
+        Note that its data is not sorted!
+        """
+        return self.field.axes_series[0]
+
+    @property
+    def t(self):
+        """The time array of a SectrumSeries is the data of its tseries.
+
+        FIXME: It is the reader's job to make sure that `t` is increasing,
+           i.e. that the spectra are sorted by time.
+        """
+
+        return self.tseries.data
+
+    @property
+    def t_name(self):
+        """The name of the time variable of the spectrum series"""
+        return self.tseries.name
+
+    @property
+    def t_tolerance(self):
+        if self._t_tolerance is None:
+            # Note, accessing `self.t` here would lead to infinite recursion
+            #   due to `t_tolerance`'s use in the `field` property.
+            try:
+                t = self._field.axes_series[0].t
+            except AttributeError:
+                raise AttributeError(
+                    "`SpectrumSeries` object tried to use time data from its `Field`"
+                    " to determine its time tolerance but its `_field` was "
+                    f"{self._field}."
+                )
+            tolerance_by_spacing = 1 / 1000 * (t[-1] - t[0]) / len(t)
+            self._t_tolerance = min(tolerance_by_spacing, 1)
+        return self._t_tolerance
+
+    @property
+    def xseries(self):
+        """The x-axis DataSeries of a SectrumSeries is the 1'st axis of its field"""
+        return self.field.axes_series[1]
+
+    @property
+    def x(self):
+        """The x (scanning variable) data"""
+        return self.xseries.data
+
+    @property
+    def x_name(self):
+        """The name of the scanning variable"""
+        return self.xseries.name
+
+    @property
+    def y(self):
+        """The y data is the multi-dimensional data attribute of the field"""
+        return self.field.data
+
+    def __getitem__(self, key):
+        """Indexing a SpectrumSeries with an int n returns its n'th spectrum"""
+        from .techniques import TECHNIQUE_CLASSES
+
+        if self.technique in TECHNIQUE_CLASSES:
+            # TECHNIQUE_CLASSES points, as a rule, to the Spectrum clas, not the
+            # SpectrumSeries class of a given technique.
+            cls = TECHNIQUE_CLASSES[self.technique]
+            if issubclass(cls, SpectrumSeries):
+                # ... however, if TECHNIQUE_CLASSES does point to a SpectrumSeries
+                # class, we need indexing to return a default spectrum.
+                cls = Spectrum
+        else:
+            cls = Spectrum
+
+        if isinstance(key, int):
+            spectrum_as_dict = self.as_dict()
+            del spectrum_as_dict["field_id"]
+            spectrum_as_dict["field"] = Field(
+                # note that it's important in some cases that the spectrum does not have
+                # the same name as the spectrum series:
+                name=self.y_name + "_" + str(key),
+                unit_name=self.field.unit_name,
+                data=self.y[key],
+                axes_series=[self.xseries],
+            )
+            spectrum_as_dict["tstamp"] = self.tstamp + self.t[key]
+            if self.durations:
+                spectrum_as_dict["duration"] = self.durations[key]
+            return cls.from_dict(spectrum_as_dict)
+        raise KeyError
+
+    def cut(self, tspan, t_zero=None):
+        """Return a subset with the spectrums falling in a specified timespan
+
+        Args:
+            tspan (timespan): The timespan within which you want spectrums
+            t_zero (float): The shift in t=0 with respect to the original
+                spectrum series, in [s].
+        """
+        t_mask = np.logical_and(tspan[0] < self.t, self.t < tspan[1])
+        new_t = self.t[t_mask]
+        new_tseries = TimeSeries(
+            name=self.tseries.name,
+            unit_name=self.tseries.unit_name,
+            data=new_t,
+            tstamp=self.tseries.tstamp,
+        )
+        new_y = self.y[t_mask, :]
+        new_field = Field(
+            name=self.field.name,
+            unit_name=self.field.unit_name,
+            data=new_y,
+            axes_series=[new_tseries, self.xseries],
+        )
+        new_durations = np.array(self.durations)[t_mask]
+        cut_spectrum_series_as_dict = self.as_dict()
+        cut_spectrum_series_as_dict.update(
+            field=new_field,
+            durations=new_durations,
+        )
+        cut_spectrum_series = self.__class__.from_dict(cut_spectrum_series_as_dict)
+        if t_zero:
+            if t_zero == "start":
+                cut_spectrum_series.tstamp += tspan[0]
+            else:
+                cut_spectrum_series.tstamp += t_zero
+        return cut_spectrum_series
+
+    @property
+    def y_average(self):
+        """The y-data of the average spectrum"""
+        return np.mean(self.y, axis=0)
+
+    def __add__(self, other):
+        if type(other) is type(self):  # Then we are appending!
+            obj_as_dict = self.as_dict()
+            new_y_name = self.y_name
+            if not self.y_name == other.y_name:
+                new_y_name = self.y_name + "AND" + other.y_name
+                warnings.warn(
+                    "Appending spectra with different names: \n"
+                    f"{new_y_name}.\n"
+                    "Result might not be meaningful."
+                )
+            new_x_name = self.x_name
+            new_xseries = self.xseries
+            if not self.xseries.shape == other.xseries.shape:
+                raise TypeError(
+                    "Cannot append SpectrumSeries with different shapes "
+                    "of their x series!"
+                )
+            if not self.xseries.unit_name == other.xseries.unit_name:
+                raise TypeError(
+                    "Cannot append SpectrumSeries with different units "
+                    "of their x series!"
+                )
+            if not self.x_name == other.x_name:
+                new_x_name = self.x_name + "OR" + other.x_name
+                warnings.warn(
+                    "Appending spectra with different names: \n"
+                    f"{new_x_name}.\n"
+                    "Result might not be meaningful."
+                )
+                new_xseries = DataSeries(
+                    name=new_x_name, unit_name=self.xseries.unit_name, data=self.x
+                )
+            new_tseries = append_series([self.tseries, other.tseries])
+            new_y = np.append(self.y, other.y, axis=0)
+            new_field = Field(
+                name=new_y_name,
+                unit_name=self.field.unit_name,
+                data=new_y,
+                axes_series=[new_tseries, new_xseries],
+            )
+            new_durations = np.append(self.durations, other.durations)
+            obj_as_dict.update(
+                field=new_field,
+                durations=new_durations,
+            )
+            return self.__class__(**obj_as_dict)
+
+        if isinstance(other, Measurement):
+            return add_spectrum_series_to_measurement(other, self)
+
+
+def add_spectrum_series_to_measurement(measurement, spectrum_series, **kwargs):
+    """Add a measurement and a spectrum measurement.
+
+    Args:
+        measurement (Measurement): The `Measurement` object containing the time-resolved
+            scalar values.
+        spectrum_series (SpectrumSeries): The `SpectrumSeries` object containing the 2-D
+            time-resolved spectral data.
+        kwargs: Additional key-word arguments are passed on to the `from_dict`
+            constructor of the resulting object.
+
+    Returns SpectroMeasurement: The addition results in an object of SpectroMeasurement
+        or a subclass thereof if ixdat supports the hyphenated technique. For example,
+        addition of an `ECMeasurement` and an XAS `SpectrumSeries` results in an
+        `ECXASMeasurement` object.
+    """
+    new_name = measurement.name + " AND " + spectrum_series.name
+    new_technique = get_combined_technique(
+        measurement.technique, spectrum_series.technique
+    )
+
+    # TODO: see if there isn't a way to put the import at the top of the module.
+    #    see: https://github.com/ixdat/ixdat/pull/1#discussion_r546437410
+    from .techniques import TECHNIQUE_CLASSES
+
+    obj_as_dict = measurement.as_dict()
+    obj_as_dict["spectrum_series"] = spectrum_series
+    obj_as_dict["name"] = new_name
+    obj_as_dict["technique"] = new_technique
+
+    if new_technique in TECHNIQUE_CLASSES:
+        cls = TECHNIQUE_CLASSES[new_technique]
+    else:
+        cls = SpectroMeasurement
+    if issubclass(cls, TECHNIQUE_CLASSES["EC-Optical"]):
+        # Then we need a reference spectrum!
+        # But so far the only EC-Optical reader doesn't support reading Optical and
+        # EC parts separately, so this needs not be implemented yet.
+        raise NotImplementedError("addition of EC and Optical not yet supported.")
+
+    obj_as_dict.update(kwargs)
+    return cls.from_dict(obj_as_dict)
+
+
+class SpectroMeasurement(Measurement):
+    child_attrs = ["spectrum_series_list"] + Measurement.child_attrs
+    extra_column_attrs = {"spectro_measurements": {"spectrum_id"}}
+
+    def __init__(self, *args, spectrum_series=None, spectrum_id=None, **kwargs):
+        super().__init__(*args, **kwargs)
+        if spectrum_series:
+            self._spectrum_series = spectrum_series
+            self._spectrum_series.tstamp = self.tstamp
+        elif spectrum_id:
+            self._spectrum_series = PlaceHolderObject(spectrum_id, cls=SpectrumSeries)
+        else:
+            raise TypeError(
+                "A SpectroMeasurement must be "
+                "initialized with a `spectrum_series` or `spectrum_id`"
+            )
+
+    @property
+    def spectrum_series(self):
+        """The `SpectrumSeries` with the spectral data"""
+        if isinstance(self._spectrum_series, PlaceHolderObject):
+            self._spectrum_series = self._spectrum_series.get_object()
+            self._spectrum_series.tstamp = self.tstamp
+        return self._spectrum_series
+
+    # FIXME: The attribute below is needed in order to correctly
+    #   pass the spectrum_series between objects using its id,
+    #   because "child_attrs" only works on lists.
+    @property
+    def spectrum_series_list(self):
+        return [self.spectrum_series]
+
+    @property
+    def spectrum_id(self):
+        """The id of the `SpectrumSeries`"""
+        return self.spectrum_series.short_identity
+
+    @property
+    def spectra(self):
+        """The field of the `SpectrumSeries`. `spectra.data` is a 2-D array"""
+        return self.spectrum_series.field
+
+    def set_spectrum_series(self, spectrum_series):
+        """(Re-)set the `spectrum_series` to a provided `spectrum_series`"""
+        self._spectrum_series = spectrum_series
+
+    @property
+    def continuous(self):
+        return self.spectrum_series.continuous
+
+    @Measurement.tstamp.setter
+    def tstamp(self, tstamp):
+        self._tstamp = tstamp
+        # Resetting the tstamp needs to reset the `spectrum_series`' tstamp as well:
+        self.spectrum_series.tstamp = tstamp
+        # As before it also needs to clear the cache, so series are returned wrt the
+        # new timestamp.
+        self.clear_cache()
+
+    def __getitem__(self, item):
+        if isinstance(item, int) or isinstance(item, slice):
+            return self.spectrum_series[item]
+        return super().__getitem__(item)
+
+    def __add__(self, other):
+        added_measurement = super().__add__(other)
+        if isinstance(other, SpectroMeasurement):
+            spectrum_series = self.spectrum_series + other.spectrum_series
+            added_measurement.set_spectrum_series(spectrum_series)
+        return added_measurement
+
+    def cut(self, tspan, t_zero=None):
+        """Select the portion of the data in a given tspan.
+
+        See :func:`~measurements.Measurement.cut`
+        """
+        cut_measurement = super().cut(tspan, t_zero=t_zero)
+        spectrum_series = self.spectrum_series.cut(tspan=tspan, t_zero=t_zero)
+        cut_measurement.set_spectrum_series(spectrum_series)
+        return cut_measurement
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat/techniques/__init__.py` & `ixdat-0.2.9.dev3/src/ixdat/techniques/__init__.py`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,52 +1,52 @@
-"""Import techniques and build the technique_classes dictionary for direct import
-
-Constants:
-    technique_classes (dict): Dictionary of {technique_name: technique_class} where
-        technique_name is the name of the technique (like "EC") and technique_class
-        is the technique class (inheriting from Measurement) which implements the
-        technique-specific functionality.
-"""
-
-from .ec import ECMeasurement, ECCalibration
-from .cv import CyclicVoltammogram, CyclicVoltammagram  # The latter is deprecated.
-from .ms import MSMeasurement, MSCalibration, MSSpectrum, MSSpectroMeasurement
-from .ec_ms import ECMSMeasurement, ECMSCalibration, ECMSSpectroMeasurement
-from .spectroelectrochemistry import (
-    SpectroECMeasurement,
-    ECXASMeasurement,
-    ECOpticalMeasurement,
-)
-from .reactor import ReactorMeasurement, ReactorSpectroMeasurement, ReactorCalibration
-
-from ..spectra import Spectrum
-from ..measurements import Measurement  # for importing in the technique modules
-
-# TODO: Is something like DecoMeasurement a Measurement or something else?
-
-TECHNIQUE_CLASSES = {
-    "simple": Measurement,
-    "EC": ECMeasurement,
-    "CV": CyclicVoltammogram,
-    "MS": MSMeasurement,
-    "EC-MS": ECMSMeasurement,
-    "XRD": Spectrum,
-    "XPS": Spectrum,
-    "XAS": Spectrum,
-    "MS_spectra": MSSpectrum,
-    "SEC": SpectroECMeasurement,
-    "EC-Optical": ECOpticalMeasurement,
-    "EC-XAS": ECXASMeasurement,
-    "MS-MS_spectra": MSSpectroMeasurement,
-    "reactor": ReactorMeasurement,
-    "reactor-MS_spectra": ReactorSpectroMeasurement,
-    "S-EC": SpectroECMeasurement,
-    "EC-MS-MS_spectra": ECMSSpectroMeasurement,
-}
-
-CALIBRATION_CLASSES = {
-    "EC": ECCalibration,
-    "CV": ECCalibration,
-    "MS": MSCalibration,
-    "EC-MS": ECMSCalibration,
-    "reactor": ReactorCalibration,
-}
+"""Import techniques and build the technique_classes dictionary for direct import
+
+Constants:
+    technique_classes (dict): Dictionary of {technique_name: technique_class} where
+        technique_name is the name of the technique (like "EC") and technique_class
+        is the technique class (inheriting from Measurement) which implements the
+        technique-specific functionality.
+"""
+
+from .ec import ECMeasurement, ECCalibration
+from .cv import CyclicVoltammogram, CyclicVoltammagram  # The latter is deprecated.
+from .ms import MSMeasurement, MSCalibration, MSSpectrum, MSSpectroMeasurement
+from .ec_ms import ECMSMeasurement, ECMSCalibration, ECMSSpectroMeasurement
+from .spectroelectrochemistry import (
+    SpectroECMeasurement,
+    ECXASMeasurement,
+    ECOpticalMeasurement,
+)
+from .reactor import ReactorMeasurement, ReactorSpectroMeasurement, ReactorCalibration
+
+from ..spectra import Spectrum
+from ..measurements import Measurement  # for importing in the technique modules
+
+# TODO: Is something like DecoMeasurement a Measurement or something else?
+
+TECHNIQUE_CLASSES = {
+    "simple": Measurement,
+    "EC": ECMeasurement,
+    "CV": CyclicVoltammogram,
+    "MS": MSMeasurement,
+    "EC-MS": ECMSMeasurement,
+    "XRD": Spectrum,
+    "XPS": Spectrum,
+    "XAS": Spectrum,
+    "MS_spectra": MSSpectrum,
+    "SEC": SpectroECMeasurement,
+    "EC-Optical": ECOpticalMeasurement,
+    "EC-XAS": ECXASMeasurement,
+    "MS-MS_spectra": MSSpectroMeasurement,
+    "reactor": ReactorMeasurement,
+    "reactor-MS_spectra": ReactorSpectroMeasurement,
+    "S-EC": SpectroECMeasurement,
+    "EC-MS-MS_spectra": ECMSSpectroMeasurement,
+}
+
+CALIBRATION_CLASSES = {
+    "EC": ECCalibration,
+    "CV": ECCalibration,
+    "MS": MSCalibration,
+    "EC-MS": ECMSCalibration,
+    "reactor": ReactorCalibration,
+}
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat/techniques/analysis_tools.py` & `ixdat-0.2.9.dev3/src/ixdat/techniques/analysis_tools.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,199 +1,199 @@
-"""Miscellaneous tools for data analysis used in measurement techniques"""
-
-import numpy as np
-from scipy.optimize import minimize
-
-
-def tspan_passing_through(t, v, vspan, direction=None, t_i=None, v_res=None):
-    """Return the tspan corresponding to t when v first passes through vspan
-
-    Args:
-        t (np.array): independent varible data (usually time)
-        v (np.array): dependent variable data
-        vspan (iter of float): The range of v that we are interested in.
-        direction (bool): Whether v should be increasing (True) or decreasing
-            (False) as it passes through vspan. By default, the direction is
-            defined by whether vspan is increasing or decreasing.
-        t_i (float): The lowest value of t acceptable for tspan. Optional.
-        v_res (float): The uncertainty or resolution of the v data. v must be
-            at in or out of vspan by at least v_res to be considered in or out.
-    """
-
-    t_i = t_i if t_i is not None else t[0] - 1
-
-    # define some things to generalize between anodic and cathodic
-    if direction is None:
-        direction = vspan[0] < vspan[-1]
-
-    v_res = v_res if v_res is not None else np.abs(vspan[-1] - vspan[0]) / 100
-    v_res = np.abs(v_res) if direction else -np.abs(v_res)
-
-    def before(a, b):
-        if direction:
-            # before means more cathodic if we want the anodic sweep
-            return a < b
-        else:
-            # and more anodic if we want the cathodic sweep
-            return a > b
-
-    if direction:
-        # we start with the lower limit of V_span if we want the anodic sweep
-        vspan = np.sort(np.array(vspan))
-    else:
-        # and with the upper limit of V_span if we want the cathodic sweep
-        vspan = -np.sort(-np.array(vspan))
-
-    t_before = t[
-        np.argmax(
-            np.logical_and(
-                t > t_i, before(v, vspan[0] - v_res)
-            )  # True if after t_i and comfortably out on start side
-        )  # first index for which V is comfortably out on start side
-    ]  # corresponding time
-    t_just_before = t[
-        np.argmax(
-            np.logical_and(
-                t > t_before, np.logical_not(before(v, vspan[0]))
-            )  # True if after t_i and in on start side
-        )
-        - 1  # last index for which V is out on start side
-    ]  # corresponding time
-    i_start = np.argmax(np.logical_and(t > t_just_before, before(vspan[0], v)))
-    # ^ first index of full sweep through range
-    t_start = t[i_start]
-    # ^ corresponding time
-    i_finish = np.argmax(np.logical_and(t > t_start, before(vspan[1], v))) - 1
-    # ^ last index of full sweep through range
-    t_finish = t[i_finish]
-    # ^ corresponding time
-    return [t_start, t_finish]
-
-
-def calc_sharp_v_scan(t, v, res_points=10):
-    """Calculate the discontinuous rate of change of v with respect to t
-
-    Args:
-        t (np.array): the data of the independent variable, typically time
-        v (np.array): the data of the dependent variable
-        res_points (int): the resolution in data points, i.e. the spacing used in
-            the slope equation v_scan = (v2 - v1) / (t2 - t1)
-    """
-    # the scan rate is dV/dt. This is a numerical calculation of dV/dt:
-    v_behind = np.append(np.tile(v[0], res_points), v[:-res_points])
-    v_ahead = np.append(v[res_points:], np.tile(v[-1], res_points))
-
-    t_behind = np.append(np.tile(t[0], res_points), t[:-res_points])
-    t_ahead = np.append(t[res_points:], np.tile(t[-1], res_points))
-
-    v_scan_middle = (v_ahead - v_behind) / (t_ahead - t_behind)
-    # ^ this is "softened" at the anodic and cathodic turns.
-
-    # We can "sharpen" it by selectively looking ahead and behind:
-    v_scan_behind = (v - v_behind) / (t - t_behind)
-    v_scan_ahead = (v_ahead - v) / (t_ahead - t)
-
-    # but this gives problems right at the beginning, so set those to zeros
-    v_scan_behind[:res_points] = np.zeros(res_points)
-    v_scan_ahead[-res_points:] = np.zeros(res_points)
-
-    # now sharpen the scan rate!
-    v_scan = v_scan_middle
-    mask_use_ahead = np.logical_and(
-        np.abs(v_scan_ahead) > np.abs(v_scan),
-        np.abs(v_scan_ahead) > np.abs(v_scan_behind),
-    )
-    v_scan[mask_use_ahead] = v_scan_ahead[mask_use_ahead]
-
-    mask_use_behind = np.logical_and(
-        np.abs(v_scan_behind) > np.abs(v_scan),
-        np.abs(v_scan_behind) > np.abs(v_scan_ahead),
-    )
-    v_scan[mask_use_behind] = v_scan_behind[mask_use_behind]
-
-    return v_scan
-
-
-def find_signed_sections(x, x_res=0.001, res_points=10):
-    """Return list of tuples ((i_start, i_finish), section_type) describing the vector x
-
-    `i_start` and `i_finish` are indexes in x defining where sections start and end.
-    `section_type` can be "positive" (x>0), "negative" (x<0) or "zero" (x~0).
-
-    Args:
-        x (np array): The data as a vector
-        x_res (float): The minimum value in x to be considered different from zero,
-            i.e. the uncertainty or resolution of the data
-        res_points (int): The minimum number of consecutive data points in x that must
-            have the same sign (or be ~0) to constitute a section of the data.
-    """
-    mask_negative = x < -x_res
-    mask_positive = x > x_res
-    mask_zero = abs(x) < x_res
-
-    section_types = ["negative", "positive", "zero"]
-    the_masks = [mask_negative, mask_positive, mask_zero]
-
-    for mask in the_masks:
-        mask[-2] = False
-        mask[-1] = True
-    N = len(x)
-    i_start = 0
-    i_finish = 0
-    n_sweep = 0
-
-    the_next_starts = [np.argmax(mask) for mask in the_masks]
-    section_id = int(np.argmin(the_next_starts))
-
-    sections = []
-    while i_start < N - 1:
-        I_out = np.argmin(the_masks[section_id][i_finish:])
-        the_next_start = i_finish + I_out + res_points
-
-        try:
-            I_in_again = np.argmax(the_masks[section_id][the_next_start:])
-        except ValueError:
-            the_next_starts[section_id] = N
-        else:
-            the_next_starts[section_id] = the_next_start + I_in_again
-            # ^ and add it.
-
-        next_section_id = int(np.argmin(the_next_starts))
-        i_finish = the_next_starts[next_section_id]
-
-        if next_section_id != section_id:
-            sections.append(((i_start, i_finish), section_types[section_id]))
-            section_id = next_section_id
-            n_sweep += 1
-            i_start = i_finish
-        else:
-            i_start += res_points
-
-    return sections
-
-
-def calc_t_using_scan_rate(v, dvdt):
-    """Return a numpy array describing the time corresponding to v given scan rate dvdt
-
-    This is useful for data sets where time is missing. It depends on another value
-    having a constant absolute rate of change (such as electrode potential in cyclic
-    voltammatry).
-    It uses the `calc_sharp_v_scan` algorithm to match the scan rate implied by the
-    timevector returned with the given scan rate.
-    Args:
-        v (np array): The value
-        dvdt (float): The scan rate in units of v's unit per second
-    Returns:
-        np array: t, the time vector corresponding to v
-    """
-
-    def error(t_tot):
-        t = np.linspace(0, t_tot[0], v.size)
-        dvdt_calc = np.abs(calc_sharp_v_scan(t, v))
-        error = np.sum(dvdt_calc**2 - dvdt**2)
-        return error
-
-    t_total_guess = (max(v) - min(v)) / dvdt
-    result = minimize(error, np.array(t_total_guess))
-
-    t_total = result.x[0]
-    return np.linspace(0, t_total, v.size)
+"""Miscellaneous tools for data analysis used in measurement techniques"""
+
+import numpy as np
+from scipy.optimize import minimize
+
+
+def tspan_passing_through(t, v, vspan, direction=None, t_i=None, v_res=None):
+    """Return the tspan corresponding to t when v first passes through vspan
+
+    Args:
+        t (np.array): independent varible data (usually time)
+        v (np.array): dependent variable data
+        vspan (iter of float): The range of v that we are interested in.
+        direction (bool): Whether v should be increasing (True) or decreasing
+            (False) as it passes through vspan. By default, the direction is
+            defined by whether vspan is increasing or decreasing.
+        t_i (float): The lowest value of t acceptable for tspan. Optional.
+        v_res (float): The uncertainty or resolution of the v data. v must be
+            at in or out of vspan by at least v_res to be considered in or out.
+    """
+
+    t_i = t_i if t_i is not None else t[0] - 1
+
+    # define some things to generalize between anodic and cathodic
+    if direction is None:
+        direction = vspan[0] < vspan[-1]
+
+    v_res = v_res if v_res is not None else np.abs(vspan[-1] - vspan[0]) / 100
+    v_res = np.abs(v_res) if direction else -np.abs(v_res)
+
+    def before(a, b):
+        if direction:
+            # before means more cathodic if we want the anodic sweep
+            return a < b
+        else:
+            # and more anodic if we want the cathodic sweep
+            return a > b
+
+    if direction:
+        # we start with the lower limit of V_span if we want the anodic sweep
+        vspan = np.sort(np.array(vspan))
+    else:
+        # and with the upper limit of V_span if we want the cathodic sweep
+        vspan = -np.sort(-np.array(vspan))
+
+    t_before = t[
+        np.argmax(
+            np.logical_and(
+                t > t_i, before(v, vspan[0] - v_res)
+            )  # True if after t_i and comfortably out on start side
+        )  # first index for which V is comfortably out on start side
+    ]  # corresponding time
+    t_just_before = t[
+        np.argmax(
+            np.logical_and(
+                t > t_before, np.logical_not(before(v, vspan[0]))
+            )  # True if after t_i and in on start side
+        )
+        - 1  # last index for which V is out on start side
+    ]  # corresponding time
+    i_start = np.argmax(np.logical_and(t > t_just_before, before(vspan[0], v)))
+    # ^ first index of full sweep through range
+    t_start = t[i_start]
+    # ^ corresponding time
+    i_finish = np.argmax(np.logical_and(t > t_start, before(vspan[1], v))) - 1
+    # ^ last index of full sweep through range
+    t_finish = t[i_finish]
+    # ^ corresponding time
+    return [t_start, t_finish]
+
+
+def calc_sharp_v_scan(t, v, res_points=10):
+    """Calculate the discontinuous rate of change of v with respect to t
+
+    Args:
+        t (np.array): the data of the independent variable, typically time
+        v (np.array): the data of the dependent variable
+        res_points (int): the resolution in data points, i.e. the spacing used in
+            the slope equation v_scan = (v2 - v1) / (t2 - t1)
+    """
+    # the scan rate is dV/dt. This is a numerical calculation of dV/dt:
+    v_behind = np.append(np.tile(v[0], res_points), v[:-res_points])
+    v_ahead = np.append(v[res_points:], np.tile(v[-1], res_points))
+
+    t_behind = np.append(np.tile(t[0], res_points), t[:-res_points])
+    t_ahead = np.append(t[res_points:], np.tile(t[-1], res_points))
+
+    v_scan_middle = (v_ahead - v_behind) / (t_ahead - t_behind)
+    # ^ this is "softened" at the anodic and cathodic turns.
+
+    # We can "sharpen" it by selectively looking ahead and behind:
+    v_scan_behind = (v - v_behind) / (t - t_behind)
+    v_scan_ahead = (v_ahead - v) / (t_ahead - t)
+
+    # but this gives problems right at the beginning, so set those to zeros
+    v_scan_behind[:res_points] = np.zeros(res_points)
+    v_scan_ahead[-res_points:] = np.zeros(res_points)
+
+    # now sharpen the scan rate!
+    v_scan = v_scan_middle
+    mask_use_ahead = np.logical_and(
+        np.abs(v_scan_ahead) > np.abs(v_scan),
+        np.abs(v_scan_ahead) > np.abs(v_scan_behind),
+    )
+    v_scan[mask_use_ahead] = v_scan_ahead[mask_use_ahead]
+
+    mask_use_behind = np.logical_and(
+        np.abs(v_scan_behind) > np.abs(v_scan),
+        np.abs(v_scan_behind) > np.abs(v_scan_ahead),
+    )
+    v_scan[mask_use_behind] = v_scan_behind[mask_use_behind]
+
+    return v_scan
+
+
+def find_signed_sections(x, x_res=0.001, res_points=10):
+    """Return list of tuples ((i_start, i_finish), section_type) describing the vector x
+
+    `i_start` and `i_finish` are indexes in x defining where sections start and end.
+    `section_type` can be "positive" (x>0), "negative" (x<0) or "zero" (x~0).
+
+    Args:
+        x (np array): The data as a vector
+        x_res (float): The minimum value in x to be considered different from zero,
+            i.e. the uncertainty or resolution of the data
+        res_points (int): The minimum number of consecutive data points in x that must
+            have the same sign (or be ~0) to constitute a section of the data.
+    """
+    mask_negative = x < -x_res
+    mask_positive = x > x_res
+    mask_zero = abs(x) < x_res
+
+    section_types = ["negative", "positive", "zero"]
+    the_masks = [mask_negative, mask_positive, mask_zero]
+
+    for mask in the_masks:
+        mask[-2] = False
+        mask[-1] = True
+    N = len(x)
+    i_start = 0
+    i_finish = 0
+    n_sweep = 0
+
+    the_next_starts = [np.argmax(mask) for mask in the_masks]
+    section_id = int(np.argmin(the_next_starts))
+
+    sections = []
+    while i_start < N - 1:
+        I_out = np.argmin(the_masks[section_id][i_finish:])
+        the_next_start = i_finish + I_out + res_points
+
+        try:
+            I_in_again = np.argmax(the_masks[section_id][the_next_start:])
+        except ValueError:
+            the_next_starts[section_id] = N
+        else:
+            the_next_starts[section_id] = the_next_start + I_in_again
+            # ^ and add it.
+
+        next_section_id = int(np.argmin(the_next_starts))
+        i_finish = the_next_starts[next_section_id]
+
+        if next_section_id != section_id:
+            sections.append(((i_start, i_finish), section_types[section_id]))
+            section_id = next_section_id
+            n_sweep += 1
+            i_start = i_finish
+        else:
+            i_start += res_points
+
+    return sections
+
+
+def calc_t_using_scan_rate(v, dvdt):
+    """Return a numpy array describing the time corresponding to v given scan rate dvdt
+
+    This is useful for data sets where time is missing. It depends on another value
+    having a constant absolute rate of change (such as electrode potential in cyclic
+    voltammatry).
+    It uses the `calc_sharp_v_scan` algorithm to match the scan rate implied by the
+    timevector returned with the given scan rate.
+    Args:
+        v (np array): The value
+        dvdt (float): The scan rate in units of v's unit per second
+    Returns:
+        np array: t, the time vector corresponding to v
+    """
+
+    def error(t_tot):
+        t = np.linspace(0, t_tot[0], v.size)
+        dvdt_calc = np.abs(calc_sharp_v_scan(t, v))
+        error = np.sum(dvdt_calc**2 - dvdt**2)
+        return error
+
+    t_total_guess = (max(v) - min(v)) / dvdt
+    result = minimize(error, np.array(t_total_guess))
+
+    t_total = result.x[0]
+    return np.linspace(0, t_total, v.size)
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat/techniques/cv.py` & `ixdat-0.2.9.dev3/src/ixdat/techniques/cv.py`

 * *Ordering differences only*

 * *Files 27% similar despite different names*

```diff
@@ -1,379 +1,379 @@
-import numpy as np
-from .ec import ECMeasurement
-from ..data_series import ValueSeries, TimeSeries
-from ..exceptions import BuildError, SeriesNotFoundError
-from .analysis_tools import (
-    tspan_passing_through,
-    calc_sharp_v_scan,
-    find_signed_sections,
-)
-from ..plotters.ec_plotter import CVDiffPlotter
-from ..plotters.plotting_tools import get_color_from_cmap, add_colorbar
-from ..tools import deprecate
-
-
-class CyclicVoltammogram(ECMeasurement):
-    """Class for cyclic voltammetry measurements.
-
-    Onto ECMeasurement, this adds:
-    - a property `cycle` which is a ValueSeries on the same TimeSeries as potential,
-    which counts cycles. "cycle" becomes the Measurement's `sel_str`. Indexing with
-    integer or iterable selects according to `cycle`.
-    - functions for quantitatively comparing cycles (like a stripping cycle, base cycle)
-    - the default plot() is plot_vs_potential()
-    """
-
-    essential_series_names = ("t", "raw_potential", "raw_current", "cycle")
-    selector_name = "cycle"
-
-    series_constructors = ECMeasurement.series_constructors
-    series_constructors["scan_rate"] = "_build_scan_rate"
-
-    """Name of the default selector"""
-
-    def __init__(self, *args, **kwargs):
-        """Only reason to have an __init__ here is to set the default plot()"""
-        super().__init__(*args, **kwargs)
-        self.plot = self.plotter.plot_vs_potential  # gets the right docstrings! :D
-
-        try:
-            _ = self["cycle"]
-        except SeriesNotFoundError:
-            median_potential = 1 / 2 * (np.max(self.U) + np.min(self.U))
-            self.redefine_cycle(start_potential=median_potential, redox=True)
-
-        self.start_potential = None  # see `redefine_cycle`
-        self.redox = None  # see `redefine_cycle`
-
-    def __getitem__(self, key):
-        """Given int list or slice key, return a CyclicVoltammogram with those cycles"""
-        if isinstance(key, slice):
-            start, stop, step = key.start, key.stop, key.step
-            if step is None:
-                step = 1
-            key = list(range(start, stop, step))
-        if isinstance(key, (int, list)):
-            if isinstance(key, list) and not all([isinstance(i, int) for i in key]):
-                print("can't get an item of type list unless all elements are int")
-                print(f"you tried to get key = {key}.")
-                raise AttributeError
-            return self.select(key)
-        return super().__getitem__(key)
-
-    def redefine_cycle(self, start_potential=None, redox=None, N_points=5):
-        """Build `cycle` which iterates when passing through start_potential
-
-        Args:
-            start_potential (float): The potential in [V] at which the cycle counter will
-                iterate. If start_potential is not given, the cycle is just the
-                `selector` inherited from ECMeasurement shifted to start at 0.
-            redox (bool): True (or 1) for anodic, False (or 0) for cathodic. The
-                direction in which the potential is scanning through start_potential to
-                trigger an iteration of `cycle`.
-            N_points (int): The number of consecutive points for which the potential
-                needs to be above (redox=True) or below (redox=False) the
-                start_potential for the new cycle to register.
-        """
-        self.start_potential = start_potential
-        self.redox = redox
-        if start_potential is None:
-            old_cycle_series = self["cycle_number"]
-            new_cycle_series = ValueSeries(
-                name="cycle",
-                unit_name=old_cycle_series.unit_name,
-                data=old_cycle_series.data - min(old_cycle_series.data),
-                tseries=old_cycle_series.tseries,
-            )
-        else:
-            cycle_vec = np.zeros(self.t.shape)
-            c = 0
-            n = 0
-            N = len(self.t)
-            v = self.U
-            if not redox:
-                # easiest way to reverse directions is to use the same > < operators
-                # but negate the arguments
-                start_potential = -start_potential
-                v = -v
-            while n < N:
-                # mask on remaining potential, True wherever behind the start potential:
-                mask_behind = v[n:] < start_potential
-                if True not in mask_behind:
-                    # if the potenential doesn't go behind start potential again, then
-                    # there are no more cycles
-                    break
-                else:
-                    # the potential has to get behind the start potential for at least
-                    # N_points data points before a new cycle can start.
-                    n += np.argmax(mask_behind) + N_points
-
-                # a mask on remaining potential, True wherever ahead of start potential:
-                mask_in_front = v[n:] > start_potential
-                if True not in mask_in_front:  # again, no more cycles.
-                    break
-                else:
-                    # We've already been behind for N_points, so as soon as the
-                    # potential gets ahead of the start_potential, a new cycle begins!
-                    n += np.argmax(mask_in_front)
-                c += 1
-                cycle_vec[n:] = c  # and subsequent points increase in cycle number
-                n += N_points  # have to be above start_potential for N_points
-                # datapoints before getting behind it for this to count as a cycle.
-            new_cycle_series = ValueSeries(
-                name="cycle",
-                unit_name="",
-                data=cycle_vec,
-                tseries=self.potential.tseries,
-            )
-        self.replace_series("cycle", new_cycle_series)
-
-    def select_sweep(self, vspan, t_i=None):
-        """Return the cut of the CV for which the potential is sweeping through vspan
-
-        Args:
-            vspan (iter of float): The range of self.potential for which to select data.
-                Vspan defines the direction of the sweep. If vspan[0] < vspan[-1], an
-                oxidative sweep is returned, i.e. one where potential is increasing.
-                If vspan[-1] < vspan[0], a reductive sweep is returned.
-            t_i (float): Optional. Time before which the sweep can't start
-        """
-        tspan = tspan_passing_through(
-            t=self.t,
-            v=self.U,
-            vspan=vspan,
-            t_i=t_i,
-        )
-        return self.cut(tspan=tspan)
-
-    def integrate(self, item, tspan=None, vspan=None, ax=None):
-        """Return the time integral of item while time in tspan or potential in vspan
-
-        Args:
-            item (str): The name of the ValueSeries to integrate
-            tspan (iter of float): A time interval over which to integrate it
-            vspan (iter of float): A potential interval over which to integrate it
-        """
-        if vspan:
-            return self.select_sweep(
-                vspan=vspan, t_i=tspan[0] if tspan else None
-            ).integrate(item, ax=ax)
-        return super().integrate(item, tspan, ax=ax)
-
-    def _build_scan_rate(self, res_points=10):
-        """The scan rate as a ValueSeries"""
-        t, v = self.grab("potential")
-        scan_rate_vec = calc_sharp_v_scan(t, v, res_points=res_points)
-        scan_rate_series = ValueSeries(
-            name="scan rate",
-            unit_name="V/s",  # TODO: unit = potential.unit / potential.tseries.unit
-            data=scan_rate_vec,
-            tseries=self.potential.tseries,
-        )
-        return scan_rate_series
-
-    @property
-    @deprecate("0.1", "Use a look-up, i.e. `ec_meas['scan_rate']`, instead.", "0.3")
-    def scan_rate(self):
-        return self["scan_rate"]
-
-    def get_timed_sweeps(self, v_scan_res=5e-4, res_points=10):
-        """Return list of [(tspan, type)] for all the potential sweeps in self.
-
-        There are three types: "anodic" (positive scan rate), "cathodic" (negative scan
-        rate), and "hold" (zero scan rate)
-
-        Args:
-            v_scan_res (float): The minimum scan rate considered significantly different
-                than zero, in [V/s]. Defaults to 5e-4 V/s (0.5 mV/s). May need be higher
-                for noisy potential, and lower for very low scan rates.
-            res_points (int): The minimum number of points to be considered a sweep.
-                During a sweep, a potential difference of at least `v_res` should be
-                scanned through every `res_points` points.
-        """
-        t = self.t
-        ec_sweep_types = {
-            "positive": "anodic",
-            "negative": "cathodic",
-            "zero": "hold",
-        }
-        indexed_sweeps = find_signed_sections(
-            self["scan_rate"].data, x_res=v_scan_res, res_points=res_points
-        )
-        timed_sweeps = []
-        for (i_start, i_finish), general_sweep_type in indexed_sweeps:
-            timed_sweeps.append(
-                ((t[i_start], t[i_finish]), ec_sweep_types[general_sweep_type])
-            )
-        return timed_sweeps
-
-    def calc_capacitance(self, vspan):
-        """Return the capacitance in [F], calculated by the first sweeps through vspan
-
-        Args:
-            vspan (iter of floats): The potential range in [V] to use for capacitance
-        """
-        sweep_1 = self.select_sweep(vspan)
-        v_scan_1 = np.mean(sweep_1.grab("scan_rate")[1])  # [V/s]
-        I_1 = np.mean(sweep_1.grab("raw_current")[1]) * 1e-3  # [mA] -> [A]
-
-        sweep_2 = self.select_sweep([vspan[-1], vspan[0]], t_i=max(sweep_1.t + 1))
-        v_scan_2 = np.mean(sweep_2.grab("scan_rate")[1])  # [V/s]
-        I_2 = np.mean(sweep_2.grab("raw_current")[1]) * 1e-3  # [mA] -> [A]
-
-        cap = 1 / 2 * (I_1 / v_scan_1 + I_2 / v_scan_2)  # [A] / [V/s] = [C/V] = [F]
-        return cap
-
-    def diff_with(self, other, v_list=None, cls=None, v_scan_res=0.001, res_points=10):
-        """Return a CyclicVotammagramDiff of this CyclicVotammagram with another one
-
-        Each anodic and cathodic sweep in other is lined up with a corresponding sweep
-        in self. Each variable given in v_list (defaults to just "current") is
-        interpolated onto self's potential and subtracted from self.
-
-        Args:
-            other (CyclicVoltammogram): The cyclic voltammogram to subtract from self.
-            v_list (list of str): The names of the series to calculate a difference
-                between self and other for (defaults to just "current").
-            cls (ECMeasurement subclass): The class to return an object of. Defaults to
-                CyclicVoltammogramDiff.
-            v_scan_res (float): see :meth:`get_timed_sweeps`
-            res_points (int):  see :meth:`get_timed_sweeps`
-        """
-
-        if not type(self) is CyclicVoltammogram:
-            raise NotImplementedError(
-                "CyclicVoltammogram.diff_with() is not implemented for "
-                f"cyclic voltammograms of type {type(self)}"
-            )
-
-        vseries = self.potential
-        tseries = vseries.tseries
-        series_list = [tseries, self["raw_potential"], self["cycle"]]
-
-        v_list = v_list or ["current", "raw_current"]
-        if "potential" in v_list:
-            raise BuildError(
-                f"v_list={v_list} is invalid. 'potential' is used to interpolate."
-            )
-
-        my_sweep_specs = [
-            spec
-            for spec in self.get_timed_sweeps(
-                v_scan_res=v_scan_res, res_points=res_points
-            )
-            if spec[1] in ["anodic", "cathodic"]
-        ]
-        others_sweep_specs = [
-            spec
-            for spec in other.get_timed_sweeps(
-                v_scan_res=v_scan_res, res_points=res_points
-            )
-            if spec[1] in ["anodic", "cathodic"]
-        ]
-        if not len(my_sweep_specs) == len(others_sweep_specs):
-            raise BuildError(
-                "Can only make diff of CyclicVoltammograms with same number of sweeps."
-                f"{self!r} has {my_sweep_specs} and {other!r} has {others_sweep_specs}."
-            )
-
-        diff_values = {name: np.array([]) for name in v_list}
-        t_diff = np.array([])
-
-        for my_spec, other_spec in zip(my_sweep_specs, others_sweep_specs):
-            sweep_type = my_spec[1]
-            if not other_spec[1] == sweep_type:
-                raise BuildError(
-                    "Corresponding sweeps must be of same type when making diff."
-                    f"Can't align {self!r}'s {my_spec} with {other!r}'s {other_spec}."
-                )
-            my_tspan = my_spec[0]
-            other_tspan = other_spec[0]
-            my_t, my_potential = self.grab(
-                "potential", my_tspan, include_endpoints=False
-            )
-            t_diff = np.append(t_diff, my_t)
-            other_t, other_potential = other.grab(
-                "potential", other_tspan, include_endpoints=False
-            )
-            if sweep_type == "anodic":
-                other_t_interp = np.interp(
-                    np.sort(my_potential), np.sort(other_potential), other_t
-                )
-            elif sweep_type == "cathodic":
-                other_t_interp = np.interp(
-                    np.sort(-my_potential), np.sort(-other_potential), other_t
-                )
-            else:
-                continue
-            for name in v_list:
-                my_v = self.grab_for_t(name, my_t)
-                other_v = other.grab_for_t(name, other_t_interp)
-                diff_v = my_v - other_v
-                diff_values[name] = np.append(diff_values[name], diff_v)
-
-        t_diff_series = TimeSeries(
-            name="time/[s] for diffs", unit_name="s", data=t_diff, tstamp=self.tstamp
-        )  # I think this is the same as self.potential.tseries
-
-        series_list.append(t_diff_series)
-        for name, data in diff_values.items():
-            series_list.append(
-                ValueSeries(
-                    name=name,
-                    unit_name=self[name].unit_name,
-                    data=data,
-                    tseries=t_diff_series,
-                )
-            )
-
-        diff_as_dict = self.as_dict()
-        del diff_as_dict["s_ids"]
-
-        diff_as_dict["series_list"] = series_list
-
-        cls = cls or CyclicVoltammogramDiff
-        diff = cls.from_dict(diff_as_dict)
-        # TODO: pass cv_compare_1 and cv_compare_2 to CyclicVoltammogramDiff as dicts
-        diff.cv_compare_1 = self
-        diff.cv_compare_2 = other
-        return diff
-
-    def plot_cycles(self, ax=None, cmap_name="jet"):
-        """Plot the cycles on a color scale.
-
-        Args:
-            ax (mpl.Axis): The axes to plot on. A new one is made by default
-            cmap_name (str): The name of the colormap to use. Defaults to "jet", which
-                ranges from blue to red
-        """
-        cycle_numbers = set(self["cycle"].data)
-        c_max = max(cycle_numbers)
-        for c in cycle_numbers:
-            color = get_color_from_cmap(c / c_max, cmap_name=cmap_name)
-            ax = self[int(c)].plot(ax=ax, color=color)
-        add_colorbar(
-            ax, cmap_name, vmin=min(cycle_numbers), vmax=c_max, label="cycle number"
-        )
-        return ax
-
-
-class CyclicVoltammagram(CyclicVoltammogram):
-
-    # FIXME: decorating the class itself doesn't work because the callable returned
-    #   by the decorator does not have the class methods. But this works fine.
-    @deprecate("0.1", "Use `CyclicVoltammogram` instead ('o' replaces 'a').", "0.3")
-    def __init__(self, *args, **kwargs):
-        super().__init__(*args, **kwargs)
-
-
-class CyclicVoltammogramDiff(CyclicVoltammogram):
-
-    default_plotter = CVDiffPlotter
-    cv_compare_1 = None
-    cv_compare_2 = None
-
-    def __init__(self, *args, **kwargs):
-        super().__init__(*args, **kwargs)
-        self.plot = self.plotter.plot
-        self.plot_diff = self.plotter.plot_diff
-        self.plotter = CVDiffPlotter(measurement=self)
+import numpy as np
+from .ec import ECMeasurement
+from ..data_series import ValueSeries, TimeSeries
+from ..exceptions import BuildError, SeriesNotFoundError
+from .analysis_tools import (
+    tspan_passing_through,
+    calc_sharp_v_scan,
+    find_signed_sections,
+)
+from ..plotters.ec_plotter import CVDiffPlotter
+from ..plotters.plotting_tools import get_color_from_cmap, add_colorbar
+from ..tools import deprecate
+
+
+class CyclicVoltammogram(ECMeasurement):
+    """Class for cyclic voltammetry measurements.
+
+    Onto ECMeasurement, this adds:
+    - a property `cycle` which is a ValueSeries on the same TimeSeries as potential,
+    which counts cycles. "cycle" becomes the Measurement's `sel_str`. Indexing with
+    integer or iterable selects according to `cycle`.
+    - functions for quantitatively comparing cycles (like a stripping cycle, base cycle)
+    - the default plot() is plot_vs_potential()
+    """
+
+    essential_series_names = ("t", "raw_potential", "raw_current", "cycle")
+    selector_name = "cycle"
+
+    series_constructors = ECMeasurement.series_constructors
+    series_constructors["scan_rate"] = "_build_scan_rate"
+
+    """Name of the default selector"""
+
+    def __init__(self, *args, **kwargs):
+        """Only reason to have an __init__ here is to set the default plot()"""
+        super().__init__(*args, **kwargs)
+        self.plot = self.plotter.plot_vs_potential  # gets the right docstrings! :D
+
+        try:
+            _ = self["cycle"]
+        except SeriesNotFoundError:
+            median_potential = 1 / 2 * (np.max(self.U) + np.min(self.U))
+            self.redefine_cycle(start_potential=median_potential, redox=True)
+
+        self.start_potential = None  # see `redefine_cycle`
+        self.redox = None  # see `redefine_cycle`
+
+    def __getitem__(self, key):
+        """Given int list or slice key, return a CyclicVoltammogram with those cycles"""
+        if isinstance(key, slice):
+            start, stop, step = key.start, key.stop, key.step
+            if step is None:
+                step = 1
+            key = list(range(start, stop, step))
+        if isinstance(key, (int, list)):
+            if isinstance(key, list) and not all([isinstance(i, int) for i in key]):
+                print("can't get an item of type list unless all elements are int")
+                print(f"you tried to get key = {key}.")
+                raise AttributeError
+            return self.select(key)
+        return super().__getitem__(key)
+
+    def redefine_cycle(self, start_potential=None, redox=None, N_points=5):
+        """Build `cycle` which iterates when passing through start_potential
+
+        Args:
+            start_potential (float): The potential in [V] at which the cycle counter will
+                iterate. If start_potential is not given, the cycle is just the
+                `selector` inherited from ECMeasurement shifted to start at 0.
+            redox (bool): True (or 1) for anodic, False (or 0) for cathodic. The
+                direction in which the potential is scanning through start_potential to
+                trigger an iteration of `cycle`.
+            N_points (int): The number of consecutive points for which the potential
+                needs to be above (redox=True) or below (redox=False) the
+                start_potential for the new cycle to register.
+        """
+        self.start_potential = start_potential
+        self.redox = redox
+        if start_potential is None:
+            old_cycle_series = self["cycle_number"]
+            new_cycle_series = ValueSeries(
+                name="cycle",
+                unit_name=old_cycle_series.unit_name,
+                data=old_cycle_series.data - min(old_cycle_series.data),
+                tseries=old_cycle_series.tseries,
+            )
+        else:
+            cycle_vec = np.zeros(self.t.shape)
+            c = 0
+            n = 0
+            N = len(self.t)
+            v = self.U
+            if not redox:
+                # easiest way to reverse directions is to use the same > < operators
+                # but negate the arguments
+                start_potential = -start_potential
+                v = -v
+            while n < N:
+                # mask on remaining potential, True wherever behind the start potential:
+                mask_behind = v[n:] < start_potential
+                if True not in mask_behind:
+                    # if the potenential doesn't go behind start potential again, then
+                    # there are no more cycles
+                    break
+                else:
+                    # the potential has to get behind the start potential for at least
+                    # N_points data points before a new cycle can start.
+                    n += np.argmax(mask_behind) + N_points
+
+                # a mask on remaining potential, True wherever ahead of start potential:
+                mask_in_front = v[n:] > start_potential
+                if True not in mask_in_front:  # again, no more cycles.
+                    break
+                else:
+                    # We've already been behind for N_points, so as soon as the
+                    # potential gets ahead of the start_potential, a new cycle begins!
+                    n += np.argmax(mask_in_front)
+                c += 1
+                cycle_vec[n:] = c  # and subsequent points increase in cycle number
+                n += N_points  # have to be above start_potential for N_points
+                # datapoints before getting behind it for this to count as a cycle.
+            new_cycle_series = ValueSeries(
+                name="cycle",
+                unit_name="",
+                data=cycle_vec,
+                tseries=self.potential.tseries,
+            )
+        self.replace_series("cycle", new_cycle_series)
+
+    def select_sweep(self, vspan, t_i=None):
+        """Return the cut of the CV for which the potential is sweeping through vspan
+
+        Args:
+            vspan (iter of float): The range of self.potential for which to select data.
+                Vspan defines the direction of the sweep. If vspan[0] < vspan[-1], an
+                oxidative sweep is returned, i.e. one where potential is increasing.
+                If vspan[-1] < vspan[0], a reductive sweep is returned.
+            t_i (float): Optional. Time before which the sweep can't start
+        """
+        tspan = tspan_passing_through(
+            t=self.t,
+            v=self.U,
+            vspan=vspan,
+            t_i=t_i,
+        )
+        return self.cut(tspan=tspan)
+
+    def integrate(self, item, tspan=None, vspan=None, ax=None):
+        """Return the time integral of item while time in tspan or potential in vspan
+
+        Args:
+            item (str): The name of the ValueSeries to integrate
+            tspan (iter of float): A time interval over which to integrate it
+            vspan (iter of float): A potential interval over which to integrate it
+        """
+        if vspan:
+            return self.select_sweep(
+                vspan=vspan, t_i=tspan[0] if tspan else None
+            ).integrate(item, ax=ax)
+        return super().integrate(item, tspan, ax=ax)
+
+    def _build_scan_rate(self, res_points=10):
+        """The scan rate as a ValueSeries"""
+        t, v = self.grab("potential")
+        scan_rate_vec = calc_sharp_v_scan(t, v, res_points=res_points)
+        scan_rate_series = ValueSeries(
+            name="scan rate",
+            unit_name="V/s",  # TODO: unit = potential.unit / potential.tseries.unit
+            data=scan_rate_vec,
+            tseries=self.potential.tseries,
+        )
+        return scan_rate_series
+
+    @property
+    @deprecate("0.1", "Use a look-up, i.e. `ec_meas['scan_rate']`, instead.", "0.3")
+    def scan_rate(self):
+        return self["scan_rate"]
+
+    def get_timed_sweeps(self, v_scan_res=5e-4, res_points=10):
+        """Return list of [(tspan, type)] for all the potential sweeps in self.
+
+        There are three types: "anodic" (positive scan rate), "cathodic" (negative scan
+        rate), and "hold" (zero scan rate)
+
+        Args:
+            v_scan_res (float): The minimum scan rate considered significantly different
+                than zero, in [V/s]. Defaults to 5e-4 V/s (0.5 mV/s). May need be higher
+                for noisy potential, and lower for very low scan rates.
+            res_points (int): The minimum number of points to be considered a sweep.
+                During a sweep, a potential difference of at least `v_res` should be
+                scanned through every `res_points` points.
+        """
+        t = self.t
+        ec_sweep_types = {
+            "positive": "anodic",
+            "negative": "cathodic",
+            "zero": "hold",
+        }
+        indexed_sweeps = find_signed_sections(
+            self["scan_rate"].data, x_res=v_scan_res, res_points=res_points
+        )
+        timed_sweeps = []
+        for (i_start, i_finish), general_sweep_type in indexed_sweeps:
+            timed_sweeps.append(
+                ((t[i_start], t[i_finish]), ec_sweep_types[general_sweep_type])
+            )
+        return timed_sweeps
+
+    def calc_capacitance(self, vspan):
+        """Return the capacitance in [F], calculated by the first sweeps through vspan
+
+        Args:
+            vspan (iter of floats): The potential range in [V] to use for capacitance
+        """
+        sweep_1 = self.select_sweep(vspan)
+        v_scan_1 = np.mean(sweep_1.grab("scan_rate")[1])  # [V/s]
+        I_1 = np.mean(sweep_1.grab("raw_current")[1]) * 1e-3  # [mA] -> [A]
+
+        sweep_2 = self.select_sweep([vspan[-1], vspan[0]], t_i=max(sweep_1.t + 1))
+        v_scan_2 = np.mean(sweep_2.grab("scan_rate")[1])  # [V/s]
+        I_2 = np.mean(sweep_2.grab("raw_current")[1]) * 1e-3  # [mA] -> [A]
+
+        cap = 1 / 2 * (I_1 / v_scan_1 + I_2 / v_scan_2)  # [A] / [V/s] = [C/V] = [F]
+        return cap
+
+    def diff_with(self, other, v_list=None, cls=None, v_scan_res=0.001, res_points=10):
+        """Return a CyclicVotammagramDiff of this CyclicVotammagram with another one
+
+        Each anodic and cathodic sweep in other is lined up with a corresponding sweep
+        in self. Each variable given in v_list (defaults to just "current") is
+        interpolated onto self's potential and subtracted from self.
+
+        Args:
+            other (CyclicVoltammogram): The cyclic voltammogram to subtract from self.
+            v_list (list of str): The names of the series to calculate a difference
+                between self and other for (defaults to just "current").
+            cls (ECMeasurement subclass): The class to return an object of. Defaults to
+                CyclicVoltammogramDiff.
+            v_scan_res (float): see :meth:`get_timed_sweeps`
+            res_points (int):  see :meth:`get_timed_sweeps`
+        """
+
+        if not type(self) is CyclicVoltammogram:
+            raise NotImplementedError(
+                "CyclicVoltammogram.diff_with() is not implemented for "
+                f"cyclic voltammograms of type {type(self)}"
+            )
+
+        vseries = self.potential
+        tseries = vseries.tseries
+        series_list = [tseries, self["raw_potential"], self["cycle"]]
+
+        v_list = v_list or ["current", "raw_current"]
+        if "potential" in v_list:
+            raise BuildError(
+                f"v_list={v_list} is invalid. 'potential' is used to interpolate."
+            )
+
+        my_sweep_specs = [
+            spec
+            for spec in self.get_timed_sweeps(
+                v_scan_res=v_scan_res, res_points=res_points
+            )
+            if spec[1] in ["anodic", "cathodic"]
+        ]
+        others_sweep_specs = [
+            spec
+            for spec in other.get_timed_sweeps(
+                v_scan_res=v_scan_res, res_points=res_points
+            )
+            if spec[1] in ["anodic", "cathodic"]
+        ]
+        if not len(my_sweep_specs) == len(others_sweep_specs):
+            raise BuildError(
+                "Can only make diff of CyclicVoltammograms with same number of sweeps."
+                f"{self!r} has {my_sweep_specs} and {other!r} has {others_sweep_specs}."
+            )
+
+        diff_values = {name: np.array([]) for name in v_list}
+        t_diff = np.array([])
+
+        for my_spec, other_spec in zip(my_sweep_specs, others_sweep_specs):
+            sweep_type = my_spec[1]
+            if not other_spec[1] == sweep_type:
+                raise BuildError(
+                    "Corresponding sweeps must be of same type when making diff."
+                    f"Can't align {self!r}'s {my_spec} with {other!r}'s {other_spec}."
+                )
+            my_tspan = my_spec[0]
+            other_tspan = other_spec[0]
+            my_t, my_potential = self.grab(
+                "potential", my_tspan, include_endpoints=False
+            )
+            t_diff = np.append(t_diff, my_t)
+            other_t, other_potential = other.grab(
+                "potential", other_tspan, include_endpoints=False
+            )
+            if sweep_type == "anodic":
+                other_t_interp = np.interp(
+                    np.sort(my_potential), np.sort(other_potential), other_t
+                )
+            elif sweep_type == "cathodic":
+                other_t_interp = np.interp(
+                    np.sort(-my_potential), np.sort(-other_potential), other_t
+                )
+            else:
+                continue
+            for name in v_list:
+                my_v = self.grab_for_t(name, my_t)
+                other_v = other.grab_for_t(name, other_t_interp)
+                diff_v = my_v - other_v
+                diff_values[name] = np.append(diff_values[name], diff_v)
+
+        t_diff_series = TimeSeries(
+            name="time/[s] for diffs", unit_name="s", data=t_diff, tstamp=self.tstamp
+        )  # I think this is the same as self.potential.tseries
+
+        series_list.append(t_diff_series)
+        for name, data in diff_values.items():
+            series_list.append(
+                ValueSeries(
+                    name=name,
+                    unit_name=self[name].unit_name,
+                    data=data,
+                    tseries=t_diff_series,
+                )
+            )
+
+        diff_as_dict = self.as_dict()
+        del diff_as_dict["s_ids"]
+
+        diff_as_dict["series_list"] = series_list
+
+        cls = cls or CyclicVoltammogramDiff
+        diff = cls.from_dict(diff_as_dict)
+        # TODO: pass cv_compare_1 and cv_compare_2 to CyclicVoltammogramDiff as dicts
+        diff.cv_compare_1 = self
+        diff.cv_compare_2 = other
+        return diff
+
+    def plot_cycles(self, ax=None, cmap_name="jet"):
+        """Plot the cycles on a color scale.
+
+        Args:
+            ax (mpl.Axis): The axes to plot on. A new one is made by default
+            cmap_name (str): The name of the colormap to use. Defaults to "jet", which
+                ranges from blue to red
+        """
+        cycle_numbers = set(self["cycle"].data)
+        c_max = max(cycle_numbers)
+        for c in cycle_numbers:
+            color = get_color_from_cmap(c / c_max, cmap_name=cmap_name)
+            ax = self[int(c)].plot(ax=ax, color=color)
+        add_colorbar(
+            ax, cmap_name, vmin=min(cycle_numbers), vmax=c_max, label="cycle number"
+        )
+        return ax
+
+
+class CyclicVoltammagram(CyclicVoltammogram):
+
+    # FIXME: decorating the class itself doesn't work because the callable returned
+    #   by the decorator does not have the class methods. But this works fine.
+    @deprecate("0.1", "Use `CyclicVoltammogram` instead ('o' replaces 'a').", "0.3")
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+
+
+class CyclicVoltammogramDiff(CyclicVoltammogram):
+
+    default_plotter = CVDiffPlotter
+    cv_compare_1 = None
+    cv_compare_2 = None
+
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        self.plot = self.plotter.plot
+        self.plot_diff = self.plotter.plot_diff
+        self.plotter = CVDiffPlotter(measurement=self)
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat/techniques/deconvolution.py` & `ixdat-0.2.9.dev3/src/ixdat/techniques/deconvolution.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,249 +1,249 @@
-"""Module for deconvolution of mass transport effects."""
-
-from .ec_ms import ECMSMeasurement
-from scipy.optimize import curve_fit  # noqa
-from scipy.interpolate import interp1d  # noqa
-from scipy import signal  # noqa
-from mpmath import invertlaplace, sinh, cosh, sqrt, exp, erfc, pi, tanh, coth  # noqa
-import matplotlib.pyplot as plt
-from numpy.fft import fft, ifft, ifftshift, fftfreq  # noqa
-import numpy as np
-
-# FIXME: too much abbreviation in this module.
-# TODO: Implement the PR review here: https://github.com/ixdat/ixdat/pull/4
-#  Perhaps best to merge [master] into [deconvolution], improve the module on the
-#  latter branch, and then reopen the PR.
-
-
-class DecoMeasurement(ECMSMeasurement):
-    """Class implementing deconvolution of EC-MS data"""
-
-    def __init__(self, name, **kwargs):
-        """Initialize a deconvolution EC-MS measurement
-
-        Args:
-            name (str): The name of the measurement"""
-        super().__init__(name, **kwargs)
-
-    def grab_partial_current(
-        self, signal_name, kernel_obj, tspan=None, tspan_bg=None, snr=10
-    ):
-        """Return the deconvoluted partial current for a given signal
-
-        Args:
-            signal_name (str): Name of signal for which deconvolution is to
-                be carried out.
-            kernel_obj (Kernel): Kernel object which contains the mass transport
-                parameters
-            tspan (list): Timespan for which the partial current is returned.
-            tspan_bg (list): Timespan that corresponds to the background signal.
-            snr (int): signal-to-noise ratio used for Wiener deconvolution.
-        """
-        # TODO: comments in this method so someone can tell what's going on!
-
-        t_sig, v_sig = self.grab_cal_signal(signal_name, tspan=tspan, tspan_bg=tspan_bg)
-
-        kernel = kernel_obj.calculate_kernel(
-            dt=t_sig[1] - t_sig[0], duration=t_sig[-1] - t_sig[0]
-        )
-        kernel = np.hstack((kernel, np.zeros(len(v_sig) - len(kernel))))
-        H = fft(kernel)
-        # TODO: store this as well.
-        partial_current = np.real(
-            ifft(fft(v_sig) * np.conj(H) / (H * np.conj(H) + (1 / snr) ** 2))
-        )
-        partial_current = partial_current * sum(kernel)
-        return t_sig, partial_current
-
-    def extract_kernel(self, signal_name, cutoff_pot=0, tspan=None, tspan_bg=None):
-        """Extracts a Kernel object from a measurement.
-
-        Args:
-            signal_name (str): Signal name from which the kernel/impule
-                response is to be extracted.
-            cutoff_pot (int): Potential which the defines the onset of the
-                impulse. Must be larger than the resting potential before the
-                impulse.
-            tspan(list): Timespan from which the kernel/impulse response is
-                extracted.
-            tspan_bg (list): Timespan that corresponds to the background signal.
-        """
-        x_curr, y_curr = self.grab_current(tspan=tspan)
-        x_pot, y_pot = self.grab_potential(tspan=tspan)
-        x_sig, y_sig = self.grab_signal(signal_name, tspan=tspan, tspan_bg=tspan_bg)
-
-        if signal_name == "M32":
-            t0 = x_curr[np.argmax(y_pot > cutoff_pot)]  # time of impulse
-        elif signal_name == "M2" or signal_name == "M17":
-            t0 = x_curr[np.argmax(y_pot < cutoff_pot)]
-        else:
-            print("mass not found")
-
-        x_sig = x_sig - t0
-
-        y_sig = y_sig[x_sig > 0]
-        x_sig = x_sig[x_sig > 0]
-
-        y_curr = y_curr[x_curr > t0]
-        x_curr = x_curr[x_curr > t0]
-        y_pot = y_pot[x_pot > t0]
-        x_pot = x_pot[x_pot > t0]
-
-        kernel = Kernel(
-            MS_data=np.array([x_sig, y_sig]),
-            EC_data=np.array([x_curr, y_curr, x_pot, y_pot]),
-        )
-
-        return kernel
-
-
-class Kernel:
-    """Kernel class implementing datatreatment of kernel/impulse response data."""
-
-    # TODO: Make class inherit from Measurement, add properties to store kernel
-    # TODO: Reference equations to paper.
-    def __init__(
-        self,
-        parameters={},  # FIXME: no mutable default arguments!
-        MS_data=None,
-        EC_data=None,
-    ):
-        """Initializes a Kernel object either in functional form by defining the
-        mass transport parameters or in the measured form by passing of EC-MS
-        data.
-
-        Args:
-            parameters (dict): Dictionary containing the mass transport
-                parameters with the following keys:
-                    diff_const: Diffusion constant in liquid
-                    work_dist: Working distance between electrode and gas/liq interface
-                    vol_gas: Gas sampling volume of the chip
-                    volflow_cap: Volumetric capillary flow
-                    henry_vola: Dimensionless Henry volatility
-                MS_data (list): List of numpy arrays containing the MS signal
-                    data.
-                EC_data (list): List of numpy arrays containing the EC (time,
-                    current, potential).
-        """
-
-        if MS_data and parameters:  # TODO: Make two different classes
-            raise Exception(
-                "Kernel can only be initialized with data OR parameters, not both"
-            )
-        if EC_data and MS_data:
-            print("Generating kernel from measured data")
-            self.type = "measured"
-        elif parameters:
-            print("Generating kernel from parameters")
-            self.type = "functional"
-        else:
-            print("Generating blank kernel")
-            self.type = None
-
-        self.params = parameters
-        self.MS_data = MS_data
-        self.EC_data = EC_data  # x_curr, y_curr, x_pot, y_pot
-
-    @property
-    def sig_area(self):
-        """Integrates a measured impulse response and returns the area."""
-        delta_sig = self.MS_data[1] - self.MS_data[1][-1]
-        sig_area = np.trapz(delta_sig, self.MS_data[0])
-
-        return sig_area
-
-    @property
-    def charge(self):
-        """Integrates the measured current over the time."""
-        y_curr = self.EC_data[1]
-
-        mask = np.isclose(y_curr, y_curr[0], rtol=1e-1)
-
-        Q = np.trapz(y_curr[mask], self.EC_data[0][mask])
-
-        return Q
-
-    def plot(self, dt=0.1, duration=100, ax=None, norm=True, **kwargs):
-        """Returns a plot of the kernel/impulse response."""
-        if ax is None:
-            fig1 = plt.figure()
-            ax = fig1.add_subplot(111)
-
-        if self.type == "functional":
-            t_kernel = np.arange(0, duration, dt)
-            ax.plot(
-                t_kernel,
-                self.calculate_kernel(dt=dt, duration=duration, norm=norm),
-                **kwargs,
-            )
-
-        elif self.type == "measured":
-            ax.plot(
-                self.MS_data[0],
-                self.calculate_kernel(dt=dt, duration=duration, norm=norm),
-                **kwargs,
-            )
-
-        else:
-            raise Exception("Nothing to plot with blank kernel")
-
-        return ax
-
-    def calculate_kernel(self, dt=0.1, duration=100, norm=True, matrix=False):
-        """Calculates a kernel/impulse response.
-
-        Args:
-            dt (int): Timestep for which the kernel/impulse response is calculated.
-                Has to match the timestep of the measured data for deconvolution.
-            duration(int): Duration in seconds for which the kernel/impulse response is
-                calculated. Must be long enough to reach zero.
-            norm (bool): If true the kernel/impulse response is normalized to its
-                area.
-            matrix (bool): If true the circulant matrix constructed from the kernel/
-                impulse reponse is returned.
-        """
-        if self.type == "functional":
-
-            t_kernel = np.arange(0, duration, dt)
-            t_kernel[0] = 1e-6
-
-            diff_const = self.params["diff_const"]
-            work_dist = self.params["work_dist"]
-            vol_gas = self.params["vol_gas"]
-            volflow_cap = self.params["volflow_cap"]
-            henry_vola = self.params["henry_vola"]
-
-            tdiff = t_kernel * diff_const / (work_dist**2)
-
-            def fs(s):
-                # See Krempl et al, 2021. Equation 6.
-                #     https://pubs.acs.org/doi/abs/10.1021/acs.analchem.1c00110
-                return 1 / (
-                    sqrt(s) * sinh(sqrt(s))
-                    + (vol_gas * henry_vola / 0.196e-4 / work_dist)
-                    * (s + volflow_cap / vol_gas * work_dist**2 / diff_const)
-                    * cosh(sqrt(s))
-                )
-
-            kernel = np.zeros(len(t_kernel))
-            for i in range(len(t_kernel)):
-                kernel[i] = invertlaplace(fs, tdiff[i], method="talbot")
-                print(tdiff[i])
-                print(kernel[i])
-
-        elif self.type == "measured":
-            kernel = self.MS_data[1]
-            t_kernel = self.MS_data[0]
-
-        if norm:
-            area = np.trapz(kernel, t_kernel)
-            kernel = kernel / area
-
-        if matrix:
-            kernel = np.tile(kernel, (len(kernel), 1))
-            i = 1
-            while i < len(t_kernel):
-                kernel[i] = np.concatenate((kernel[0][i:], kernel[0][:i]))
-                i = i + 1
-
-        return kernel
+"""Module for deconvolution of mass transport effects."""
+
+from .ec_ms import ECMSMeasurement
+from scipy.optimize import curve_fit  # noqa
+from scipy.interpolate import interp1d  # noqa
+from scipy import signal  # noqa
+from mpmath import invertlaplace, sinh, cosh, sqrt, exp, erfc, pi, tanh, coth  # noqa
+import matplotlib.pyplot as plt
+from numpy.fft import fft, ifft, ifftshift, fftfreq  # noqa
+import numpy as np
+
+# FIXME: too much abbreviation in this module.
+# TODO: Implement the PR review here: https://github.com/ixdat/ixdat/pull/4
+#  Perhaps best to merge [master] into [deconvolution], improve the module on the
+#  latter branch, and then reopen the PR.
+
+
+class DecoMeasurement(ECMSMeasurement):
+    """Class implementing deconvolution of EC-MS data"""
+
+    def __init__(self, name, **kwargs):
+        """Initialize a deconvolution EC-MS measurement
+
+        Args:
+            name (str): The name of the measurement"""
+        super().__init__(name, **kwargs)
+
+    def grab_partial_current(
+        self, signal_name, kernel_obj, tspan=None, tspan_bg=None, snr=10
+    ):
+        """Return the deconvoluted partial current for a given signal
+
+        Args:
+            signal_name (str): Name of signal for which deconvolution is to
+                be carried out.
+            kernel_obj (Kernel): Kernel object which contains the mass transport
+                parameters
+            tspan (list): Timespan for which the partial current is returned.
+            tspan_bg (list): Timespan that corresponds to the background signal.
+            snr (int): signal-to-noise ratio used for Wiener deconvolution.
+        """
+        # TODO: comments in this method so someone can tell what's going on!
+
+        t_sig, v_sig = self.grab_cal_signal(signal_name, tspan=tspan, tspan_bg=tspan_bg)
+
+        kernel = kernel_obj.calculate_kernel(
+            dt=t_sig[1] - t_sig[0], duration=t_sig[-1] - t_sig[0]
+        )
+        kernel = np.hstack((kernel, np.zeros(len(v_sig) - len(kernel))))
+        H = fft(kernel)
+        # TODO: store this as well.
+        partial_current = np.real(
+            ifft(fft(v_sig) * np.conj(H) / (H * np.conj(H) + (1 / snr) ** 2))
+        )
+        partial_current = partial_current * sum(kernel)
+        return t_sig, partial_current
+
+    def extract_kernel(self, signal_name, cutoff_pot=0, tspan=None, tspan_bg=None):
+        """Extracts a Kernel object from a measurement.
+
+        Args:
+            signal_name (str): Signal name from which the kernel/impule
+                response is to be extracted.
+            cutoff_pot (int): Potential which the defines the onset of the
+                impulse. Must be larger than the resting potential before the
+                impulse.
+            tspan(list): Timespan from which the kernel/impulse response is
+                extracted.
+            tspan_bg (list): Timespan that corresponds to the background signal.
+        """
+        x_curr, y_curr = self.grab_current(tspan=tspan)
+        x_pot, y_pot = self.grab_potential(tspan=tspan)
+        x_sig, y_sig = self.grab_signal(signal_name, tspan=tspan, tspan_bg=tspan_bg)
+
+        if signal_name == "M32":
+            t0 = x_curr[np.argmax(y_pot > cutoff_pot)]  # time of impulse
+        elif signal_name == "M2" or signal_name == "M17":
+            t0 = x_curr[np.argmax(y_pot < cutoff_pot)]
+        else:
+            print("mass not found")
+
+        x_sig = x_sig - t0
+
+        y_sig = y_sig[x_sig > 0]
+        x_sig = x_sig[x_sig > 0]
+
+        y_curr = y_curr[x_curr > t0]
+        x_curr = x_curr[x_curr > t0]
+        y_pot = y_pot[x_pot > t0]
+        x_pot = x_pot[x_pot > t0]
+
+        kernel = Kernel(
+            MS_data=np.array([x_sig, y_sig]),
+            EC_data=np.array([x_curr, y_curr, x_pot, y_pot]),
+        )
+
+        return kernel
+
+
+class Kernel:
+    """Kernel class implementing datatreatment of kernel/impulse response data."""
+
+    # TODO: Make class inherit from Measurement, add properties to store kernel
+    # TODO: Reference equations to paper.
+    def __init__(
+        self,
+        parameters={},  # FIXME: no mutable default arguments!
+        MS_data=None,
+        EC_data=None,
+    ):
+        """Initializes a Kernel object either in functional form by defining the
+        mass transport parameters or in the measured form by passing of EC-MS
+        data.
+
+        Args:
+            parameters (dict): Dictionary containing the mass transport
+                parameters with the following keys:
+                    diff_const: Diffusion constant in liquid
+                    work_dist: Working distance between electrode and gas/liq interface
+                    vol_gas: Gas sampling volume of the chip
+                    volflow_cap: Volumetric capillary flow
+                    henry_vola: Dimensionless Henry volatility
+                MS_data (list): List of numpy arrays containing the MS signal
+                    data.
+                EC_data (list): List of numpy arrays containing the EC (time,
+                    current, potential).
+        """
+
+        if MS_data and parameters:  # TODO: Make two different classes
+            raise Exception(
+                "Kernel can only be initialized with data OR parameters, not both"
+            )
+        if EC_data and MS_data:
+            print("Generating kernel from measured data")
+            self.type = "measured"
+        elif parameters:
+            print("Generating kernel from parameters")
+            self.type = "functional"
+        else:
+            print("Generating blank kernel")
+            self.type = None
+
+        self.params = parameters
+        self.MS_data = MS_data
+        self.EC_data = EC_data  # x_curr, y_curr, x_pot, y_pot
+
+    @property
+    def sig_area(self):
+        """Integrates a measured impulse response and returns the area."""
+        delta_sig = self.MS_data[1] - self.MS_data[1][-1]
+        sig_area = np.trapz(delta_sig, self.MS_data[0])
+
+        return sig_area
+
+    @property
+    def charge(self):
+        """Integrates the measured current over the time."""
+        y_curr = self.EC_data[1]
+
+        mask = np.isclose(y_curr, y_curr[0], rtol=1e-1)
+
+        Q = np.trapz(y_curr[mask], self.EC_data[0][mask])
+
+        return Q
+
+    def plot(self, dt=0.1, duration=100, ax=None, norm=True, **kwargs):
+        """Returns a plot of the kernel/impulse response."""
+        if ax is None:
+            fig1 = plt.figure()
+            ax = fig1.add_subplot(111)
+
+        if self.type == "functional":
+            t_kernel = np.arange(0, duration, dt)
+            ax.plot(
+                t_kernel,
+                self.calculate_kernel(dt=dt, duration=duration, norm=norm),
+                **kwargs,
+            )
+
+        elif self.type == "measured":
+            ax.plot(
+                self.MS_data[0],
+                self.calculate_kernel(dt=dt, duration=duration, norm=norm),
+                **kwargs,
+            )
+
+        else:
+            raise Exception("Nothing to plot with blank kernel")
+
+        return ax
+
+    def calculate_kernel(self, dt=0.1, duration=100, norm=True, matrix=False):
+        """Calculates a kernel/impulse response.
+
+        Args:
+            dt (int): Timestep for which the kernel/impulse response is calculated.
+                Has to match the timestep of the measured data for deconvolution.
+            duration(int): Duration in seconds for which the kernel/impulse response is
+                calculated. Must be long enough to reach zero.
+            norm (bool): If true the kernel/impulse response is normalized to its
+                area.
+            matrix (bool): If true the circulant matrix constructed from the kernel/
+                impulse reponse is returned.
+        """
+        if self.type == "functional":
+
+            t_kernel = np.arange(0, duration, dt)
+            t_kernel[0] = 1e-6
+
+            diff_const = self.params["diff_const"]
+            work_dist = self.params["work_dist"]
+            vol_gas = self.params["vol_gas"]
+            volflow_cap = self.params["volflow_cap"]
+            henry_vola = self.params["henry_vola"]
+
+            tdiff = t_kernel * diff_const / (work_dist**2)
+
+            def fs(s):
+                # See Krempl et al, 2021. Equation 6.
+                #     https://pubs.acs.org/doi/abs/10.1021/acs.analchem.1c00110
+                return 1 / (
+                    sqrt(s) * sinh(sqrt(s))
+                    + (vol_gas * henry_vola / 0.196e-4 / work_dist)
+                    * (s + volflow_cap / vol_gas * work_dist**2 / diff_const)
+                    * cosh(sqrt(s))
+                )
+
+            kernel = np.zeros(len(t_kernel))
+            for i in range(len(t_kernel)):
+                kernel[i] = invertlaplace(fs, tdiff[i], method="talbot")
+                print(tdiff[i])
+                print(kernel[i])
+
+        elif self.type == "measured":
+            kernel = self.MS_data[1]
+            t_kernel = self.MS_data[0]
+
+        if norm:
+            area = np.trapz(kernel, t_kernel)
+            kernel = kernel / area
+
+        if matrix:
+            kernel = np.tile(kernel, (len(kernel), 1))
+            i = 1
+            while i < len(t_kernel):
+                kernel[i] = np.concatenate((kernel[0][i:], kernel[0][:i]))
+                i = i + 1
+
+        return kernel
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat/techniques/ec.py` & `ixdat-0.2.9.dev3/src/ixdat/techniques/ec.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,391 +1,391 @@
-"""Module for representation and analysis of EC measurements"""
-
-from ..measurements import Measurement, Calibration
-from ..data_series import ValueSeries
-from ..exporters.ec_exporter import ECExporter
-from ..plotters.ec_plotter import ECPlotter
-from ..tools import deprecate
-
-EC_FANCY_NAMES = {
-    "t": "time / [s]",
-    "raw_potential": "raw potential / [V]",
-    "potential": "$U_{RHE}$ / [V]",
-    "raw_current": "raw current / [mA]",
-    "current": "J / [mA cm$^{-2}$]",
-}
-
-
-class ECMeasurement(Measurement):
-    """Class implementing electrochemistry measurements
-
-    TODO: Implement a unit library for current and potential, A_el and RE_vs_RHE
-      so that e.g. current can be seamlessly normalized to mass OR area.
-
-    The main job of this class is making sure that the ValueSeries most essential for
-    visualizing and normal electrochemistry measurements (i.e. excluding impedance
-    spec., RRDE, etc, which would need new classes) are always available in the
-    correct form as the measurement is added with others, reduced to a selection,
-    calibrated and normalized, etc. These most important ValueSeries are:
-
-    - `potential`: The working-electrode potential typically in [V].
-      If `ec_meas` is an `ECMeasurement`, then `ec_meas["potential"]` always returns a
-      `ValueSeries` characterized by:
-
-        - calibrated and/or corrected, if the measurement has been calibrated with the
-          reference electrode potential (`RE_vs_RHE`, see `calibrate`) and/or corrected
-          for ohmic drop (`R_Ohm`, see `correct_ohmic_drop`).
-        - A name that makes clear any calibration and/or correction
-        - Data which spans the entire timespan of the measurement - i.e. whenever EC
-          data is being recorded, `potential` is there, even if the name of the raw
-          `ValueSeries` (what the acquisition software calls it) changes. Indeed
-          `ec_meas["potential"].tseries` is the measurement's definitive time variable.
-
-    - `current`: The working-electrode current typically in [mA] or [mA/cm^2].
-      `ec_meas["current"]` always returns a `ValueSeries` characterized by:
-
-        - normalized if the measurement has been normalized with the electrode area
-          (`A_el`, see `normalize`)
-        - A name that makes clear whether it is normalized
-        - Data which spans the entire timespan of the measurement
-
-    - `selector`: A counter series distinguishing sections of the measurement program.
-      This is essential for analysis of complex measurements as it allows for
-      corresponding parts of experiments to be isolated and treated identically.
-      `selector` in `ECMeasurement` is defined to increment each time one or more of
-      the following changes:
-
-        - `loop_number`: A parameter saved by some potentiostats (e.g. BioLogic) which
-          allow complex looped electrochemistry programs.
-        - `file_number`: The id of the component measurement from which each section of
-          the data (the origin of each `ValueSeries` concatenated to `potential`)
-        - `cycle_number`: An incrementer within a file saved by a potentiostat.
-
-    The names of these ValueSeries, which can also be used to index the measurement, are
-    conveniently available as properties:
-
-    - `ec_meas.t_name` is the name of the definitive time, i.e. that of the potential
-    - `ec_meas.E_name` is the name of the raw potential
-    - `ec_meas.U_name` is the name of the calibrated and/or corrected potential
-    - `ec_meas.I_name` is the name of the raw current
-    - `ec_meas.J_name` is the name of the normalized current
-    - `ec_meas.selector_name` is the name of the default selector, i.e. "selector"
-
-    Numpy arrays from important `DataSeries` are directly accessible via attributes:
-
-    - `ec_meas.t` for `ec_meas["potential"].t`
-    - `ec_meas.U` for `ec_meas["potential"].data`
-    - `ec_meas.J` for `ec_meas["current"].data`
-
-    `ECMeasurement` comes with an `ECPlotter` which either plots `potential` and
-    `current` against time (`ec_meas.plot_measurement()`) or plots `current` against
-    `potential (`ec_meas.plot_vs_potential()`).
-
-    It turns out that keeping track of current, potential, and selector when combining
-    datasets is enough of a job to fill a class. Thus, the more exciting
-    electrochemistry-related functionality should be implemented in inheriting classes
-    such as `CyclicVoltammogram`.
-    """
-
-    extra_column_attrs = {
-        "ec_meaurements": {
-            "ec_technique",
-        }
-    }
-    control_series_name = "raw_potential"
-    essential_series_names = ("t", "raw_potential", "raw_current")
-    selection_series_names = ("file_number", "loop_number", "cycle number", "Ns")
-    default_exporter = ECExporter
-    default_plotter = ECPlotter
-
-    def __init__(
-        self,
-        name,
-        *,
-        ec_technique=None,
-        RE_vs_RHE=None,
-        R_Ohm=None,
-        A_el=None,
-        **kwargs,
-    ):
-        """initialize an electrochemistry measurement
-
-        Args:
-            name (str): The name of the measurement
-            ec_technique (str): The electrochemistry sub-technique
-            RE_vs_RHE (float): The reference electrode potential on the RHE scale in [V]
-            R_Ohm (float): The ohmic drop resistance in [Ohm]
-            A_el (float): The electrode area in [cm^2]
-
-        Kwargs, passed on to `Measurement.__init__` (see :class:`.Measurement`):
-            metadata (dict): Free-form measurement metadata. Must be json-compatible.
-            technique (str): The measurement technique
-            s_ids (list of int): The id's of the measurement's DataSeries, if
-                to be loaded (instead of given directly in series_list)
-            series_list (list of DataSeries): The measurement's DataSeries
-            m_ids (list of int): The id's of the component measurements, if to be
-                loaded. None unless this is a combined measurement (typically
-                corresponding to more than one file).
-            component_measurements (list of Measurements): The measurements of which
-                this measurement is a combination
-            reader (Reader): The file reader (None unless read from a file)
-            plotter (Plotter): The visualization tool for the measurement
-            exporter (Exporter): The exporting tool for the measurement
-            sample (Sample or str): The sample being measured
-            lablog (LabLog): The log entry with e.g. notes taken during the measurement
-            tstamp (float): The nominal starting time of the measurement, used for
-                data selection, visualization, and exporting.
-        """
-        super().__init__(name, **kwargs)
-
-        self.ec_technique = ec_technique
-        if RE_vs_RHE is not None or A_el is not None or R_Ohm is not None:
-            self.calibrate(RE_vs_RHE, A_el, R_Ohm)
-        self.plot_vs_potential = self.plotter.plot_vs_potential
-
-    @property
-    def E_name(self):
-        return self["raw_potential"].name
-
-    @property
-    def I_name(self):
-        return self["raw_current"].name
-
-    @property
-    def U_name(self):
-        return self.potential.name
-
-    @property
-    def J_name(self):
-        return self.current.name
-
-    @property
-    @deprecate("0.1", "Use `E_name` instead.", "0.3")
-    def E_str(self):
-        return self.E_name
-
-    @property
-    @deprecate("0.1", "Use `I_name` instead.", "0.3")
-    def I_str(self):
-        return self.I_name
-
-    @property
-    @deprecate("0.1", "Use `U_name` instead.", "0.3")
-    def V_str(self):
-        return self.U_name
-
-    @property
-    @deprecate("0.1", "Use `J_name` instead.", "0.3")
-    def J_str(self):
-        return self.J_name
-
-    @property
-    def aliases(self):
-        """A dictionary with the names of other data series a given name can refer to"""
-        a = super().aliases.copy()
-        return a
-
-    @property
-    def calibrations(self):
-        """The list of calibrations of the measurement.
-
-        The following is necessary to ensure that all EC Calibration parameters are
-        joined in a single calibration when processing. So that "potential" is both
-        calibrated to RHE and ohmic drop corrected, even if the two calibration
-        parameters were added separately.
-        """
-        full_calibration_list = self.calibration_list
-        good_calibration_list = [self.ec_calibration]
-        for calibration in full_calibration_list:
-            if calibration.__class__ is ECCalibration:
-                # Then we have all we need from it
-                continue
-            good_calibration_list.append(calibration)
-        return good_calibration_list
-
-    @property
-    def ec_calibration(self):
-        """A calibration joining the first RE_vs_RHE, A_el, and R_Ohm"""
-        return ECCalibration(RE_vs_RHE=self.RE_vs_RHE, A_el=self.A_el, R_Ohm=self.R_Ohm)
-
-    @property
-    def RE_vs_RHE(self):
-        """The refernce electrode potential on the RHE scale in [V]"""
-        for calibration in self.calibration_list:
-            if getattr(calibration, "RE_vs_RHE", None) is not None:
-                return calibration.RE_vs_RHE
-
-    @property
-    def A_el(self):
-        """The electrode area in [cm^2]"""
-        for calibration in self.calibration_list:
-            if getattr(calibration, "A_el", None) is not None:
-                return calibration.A_el
-
-    @property
-    def R_Ohm(self):
-        """The ohmic drop resistance in [Ohm]"""
-        for calibration in self.calibration_list:
-            if getattr(calibration, "R_Ohm", None) is not None:
-                return calibration.R_Ohm
-
-    def calibrate_RE(self, RE_vs_RHE):
-        """Calibrate the reference electrode by providing `RE_vs_RHE` in [V]."""
-        new_calibration = ECCalibration(
-            RE_vs_RHE=RE_vs_RHE,
-            measurement=self,
-        )
-        self.add_calibration(new_calibration)
-
-    def normalize_current(self, A_el):
-        """Normalize current to electrode surface area by providing `A_el` in [cm^2]."""
-        new_calibration = ECCalibration(
-            A_el=A_el,
-            measurement=self,
-        )
-        self.add_calibration(new_calibration)
-
-    def correct_ohmic_drop(self, R_Ohm):
-        """Correct for ohmic drop by providing `R_Ohm` in [Ohm]."""
-        new_calibration = ECCalibration(
-            R_Ohm=R_Ohm,
-            measurement=self,
-        )
-        self.add_calibration(new_calibration)
-
-    @property
-    def potential(self):
-        return self["potential"]
-
-    @property
-    def current(self):
-        return self["current"]
-
-    @property
-    @deprecate("0.1", "Use a look-up, i.e. `ec_meas['raw_potential']`, instead.", "0.3")
-    def raw_potential(self):
-        return self["raw_potential"]
-
-    @property
-    @deprecate("0.1", "Use a look-up, i.e. `ec_meas['raw_current']`, instead.", "0.3")
-    def raw_current(self):
-        return self["raw_current"]
-
-    @property
-    def U(self):
-        """The potential [V] numpy array of the measurement"""
-        return self.potential.data.copy()
-
-    @property
-    def J(self):
-        """The current ([mA] or [mA/cm^2]) numpy array of the measurement"""
-        return self.current.data.copy()
-
-    @property
-    @deprecate("0.1", "Use `U` instead.", "0.3")
-    def v(self):
-        """The potential [V] numpy array of the measurement"""
-        return self.potential.data.copy()
-
-    @property
-    @deprecate("0.1", "Use `J` instead.", "0.3")
-    def j(self):
-        """The current ([mA] or [mA/cm^2]) numpy array of the measurement"""
-        return self.current.data.copy()
-
-    def as_cv(self):
-        """Convert self to a CyclicVoltammogram"""
-        from .cv import CyclicVoltammogram
-
-        cv_as_dict = self.as_dict()
-        cv_as_dict["technique"] = "CV"
-        # Note, this works perfectly! All needed information is in self_as_dict :)
-        return CyclicVoltammogram.from_dict(cv_as_dict)
-
-
-class ECCalibration(Calibration):
-    """An electrochemical calibration with RE_vs_RHE, A_el, and/or R_Ohm"""
-
-    extra_column_attrs = {"ec_calibration": {"RE_vs_RHE", "A_el", "R_Ohm"}}
-    # TODO: https://github.com/ixdat/ixdat/pull/11#discussion_r677552828
-
-    def __init__(
-        self,
-        name=None,
-        technique="EC",
-        tstamp=None,
-        measurement=None,
-        RE_vs_RHE=None,
-        A_el=None,
-        R_Ohm=None,
-    ):
-        """Initiate a Calibration
-
-        Args:
-            name (str): The name of the calibration
-            technique (str): The technique of the calibration
-            tstamp (float): The time at which the calibration took place or is valid
-            measurement (ECMeasurement): Optional. A measurement to calibrate by default
-            RE_vs_RHE (float): The reference electrode potential on the RHE scale in [V]
-            A_el (float): The electrode area in [cm^2]
-            R_Ohm (float): The ohmic drop resistance in [Ohm]
-        """
-        super().__init__(
-            name=name, technique=technique, tstamp=tstamp, measurement=measurement
-        )
-        self.RE_vs_RHE = RE_vs_RHE
-        self.A_el = A_el
-        self.R_Ohm = R_Ohm
-
-    def __repr__(self):
-        # TODO: make __repr__'s consistent throught ixdat
-        return (
-            f"{self.__class__.__name__}"
-            f"(RE_vs_RHE={self.RE_vs_RHE}, A_el={self.A_el}, R_Ohm={self.R_Ohm})"
-        )
-
-    def calibrate_series(self, key, measurement=None):
-        """Return a calibrated series for key based on the raw data in the measurement.
-
-        Key should be "potential" or "current". Anything else will return None.
-
-        - "potential": the calibration looks up "raw_potential" in the measurement,
-        shifts it to the RHE potential if RE_vs_RHE is available, corrects it for
-        Ohmic drop if R_Ohm is available, and then returns a calibrated potential
-        series with a name indicative of the corrections done.
-        - "current": The calibration looks up "raw_current" in the measurement,
-        normalizes it to the electrode area if A_el is available, and returns a
-        calibrated current series with a name indicative of whether the normalization
-        was done.
-        """
-        measurement = measurement or self.measurement
-        if key == "potential":
-            raw_potential = measurement["raw_potential"]
-            name = raw_potential.name
-            U = raw_potential.data
-            if self.RE_vs_RHE is not None:
-                U = U + self.RE_vs_RHE
-                name = EC_FANCY_NAMES["potential"]
-            if self.R_Ohm is not None:
-                I_mA = measurement.grab_for_t("raw_current", t=raw_potential.t)
-                U = U - self.R_Ohm * I_mA * 1e-3  # [V] = [Ohm*mA*(A/mA)]
-                name = name + " $_{ohm. corr.}$"
-            return ValueSeries(
-                name=name,
-                unit_name=raw_potential.unit_name,
-                data=U,
-                tseries=raw_potential.tseries,
-            )
-
-        if key == "current":
-            raw_current = measurement["raw_current"]
-            name = raw_current.name
-            J = raw_current.data
-            if self.A_el is not None:
-                J = J / self.A_el
-                name = EC_FANCY_NAMES["current"]
-            return ValueSeries(
-                name=name,
-                unit_name=raw_current.unit_name,
-                data=J,
-                tseries=raw_current.tseries,
-            )
+"""Module for representation and analysis of EC measurements"""
+
+from ..measurements import Measurement, Calibration
+from ..data_series import ValueSeries
+from ..exporters.ec_exporter import ECExporter
+from ..plotters.ec_plotter import ECPlotter
+from ..tools import deprecate
+
+EC_FANCY_NAMES = {
+    "t": "time / [s]",
+    "raw_potential": "raw potential / [V]",
+    "potential": "$U_{RHE}$ / [V]",
+    "raw_current": "raw current / [mA]",
+    "current": "J / [mA cm$^{-2}$]",
+}
+
+
+class ECMeasurement(Measurement):
+    """Class implementing electrochemistry measurements
+
+    TODO: Implement a unit library for current and potential, A_el and RE_vs_RHE
+      so that e.g. current can be seamlessly normalized to mass OR area.
+
+    The main job of this class is making sure that the ValueSeries most essential for
+    visualizing and normal electrochemistry measurements (i.e. excluding impedance
+    spec., RRDE, etc, which would need new classes) are always available in the
+    correct form as the measurement is added with others, reduced to a selection,
+    calibrated and normalized, etc. These most important ValueSeries are:
+
+    - `potential`: The working-electrode potential typically in [V].
+      If `ec_meas` is an `ECMeasurement`, then `ec_meas["potential"]` always returns a
+      `ValueSeries` characterized by:
+
+        - calibrated and/or corrected, if the measurement has been calibrated with the
+          reference electrode potential (`RE_vs_RHE`, see `calibrate`) and/or corrected
+          for ohmic drop (`R_Ohm`, see `correct_ohmic_drop`).
+        - A name that makes clear any calibration and/or correction
+        - Data which spans the entire timespan of the measurement - i.e. whenever EC
+          data is being recorded, `potential` is there, even if the name of the raw
+          `ValueSeries` (what the acquisition software calls it) changes. Indeed
+          `ec_meas["potential"].tseries` is the measurement's definitive time variable.
+
+    - `current`: The working-electrode current typically in [mA] or [mA/cm^2].
+      `ec_meas["current"]` always returns a `ValueSeries` characterized by:
+
+        - normalized if the measurement has been normalized with the electrode area
+          (`A_el`, see `normalize`)
+        - A name that makes clear whether it is normalized
+        - Data which spans the entire timespan of the measurement
+
+    - `selector`: A counter series distinguishing sections of the measurement program.
+      This is essential for analysis of complex measurements as it allows for
+      corresponding parts of experiments to be isolated and treated identically.
+      `selector` in `ECMeasurement` is defined to increment each time one or more of
+      the following changes:
+
+        - `loop_number`: A parameter saved by some potentiostats (e.g. BioLogic) which
+          allow complex looped electrochemistry programs.
+        - `file_number`: The id of the component measurement from which each section of
+          the data (the origin of each `ValueSeries` concatenated to `potential`)
+        - `cycle_number`: An incrementer within a file saved by a potentiostat.
+
+    The names of these ValueSeries, which can also be used to index the measurement, are
+    conveniently available as properties:
+
+    - `ec_meas.t_name` is the name of the definitive time, i.e. that of the potential
+    - `ec_meas.E_name` is the name of the raw potential
+    - `ec_meas.U_name` is the name of the calibrated and/or corrected potential
+    - `ec_meas.I_name` is the name of the raw current
+    - `ec_meas.J_name` is the name of the normalized current
+    - `ec_meas.selector_name` is the name of the default selector, i.e. "selector"
+
+    Numpy arrays from important `DataSeries` are directly accessible via attributes:
+
+    - `ec_meas.t` for `ec_meas["potential"].t`
+    - `ec_meas.U` for `ec_meas["potential"].data`
+    - `ec_meas.J` for `ec_meas["current"].data`
+
+    `ECMeasurement` comes with an `ECPlotter` which either plots `potential` and
+    `current` against time (`ec_meas.plot_measurement()`) or plots `current` against
+    `potential (`ec_meas.plot_vs_potential()`).
+
+    It turns out that keeping track of current, potential, and selector when combining
+    datasets is enough of a job to fill a class. Thus, the more exciting
+    electrochemistry-related functionality should be implemented in inheriting classes
+    such as `CyclicVoltammogram`.
+    """
+
+    extra_column_attrs = {
+        "ec_meaurements": {
+            "ec_technique",
+        }
+    }
+    control_series_name = "raw_potential"
+    essential_series_names = ("t", "raw_potential", "raw_current")
+    selection_series_names = ("file_number", "loop_number", "cycle number", "Ns")
+    default_exporter = ECExporter
+    default_plotter = ECPlotter
+
+    def __init__(
+        self,
+        name,
+        *,
+        ec_technique=None,
+        RE_vs_RHE=None,
+        R_Ohm=None,
+        A_el=None,
+        **kwargs,
+    ):
+        """initialize an electrochemistry measurement
+
+        Args:
+            name (str): The name of the measurement
+            ec_technique (str): The electrochemistry sub-technique
+            RE_vs_RHE (float): The reference electrode potential on the RHE scale in [V]
+            R_Ohm (float): The ohmic drop resistance in [Ohm]
+            A_el (float): The electrode area in [cm^2]
+
+        Kwargs, passed on to `Measurement.__init__` (see :class:`.Measurement`):
+            metadata (dict): Free-form measurement metadata. Must be json-compatible.
+            technique (str): The measurement technique
+            s_ids (list of int): The id's of the measurement's DataSeries, if
+                to be loaded (instead of given directly in series_list)
+            series_list (list of DataSeries): The measurement's DataSeries
+            m_ids (list of int): The id's of the component measurements, if to be
+                loaded. None unless this is a combined measurement (typically
+                corresponding to more than one file).
+            component_measurements (list of Measurements): The measurements of which
+                this measurement is a combination
+            reader (Reader): The file reader (None unless read from a file)
+            plotter (Plotter): The visualization tool for the measurement
+            exporter (Exporter): The exporting tool for the measurement
+            sample (Sample or str): The sample being measured
+            lablog (LabLog): The log entry with e.g. notes taken during the measurement
+            tstamp (float): The nominal starting time of the measurement, used for
+                data selection, visualization, and exporting.
+        """
+        super().__init__(name, **kwargs)
+
+        self.ec_technique = ec_technique
+        if RE_vs_RHE is not None or A_el is not None or R_Ohm is not None:
+            self.calibrate(RE_vs_RHE, A_el, R_Ohm)
+        self.plot_vs_potential = self.plotter.plot_vs_potential
+
+    @property
+    def E_name(self):
+        return self["raw_potential"].name
+
+    @property
+    def I_name(self):
+        return self["raw_current"].name
+
+    @property
+    def U_name(self):
+        return self.potential.name
+
+    @property
+    def J_name(self):
+        return self.current.name
+
+    @property
+    @deprecate("0.1", "Use `E_name` instead.", "0.3")
+    def E_str(self):
+        return self.E_name
+
+    @property
+    @deprecate("0.1", "Use `I_name` instead.", "0.3")
+    def I_str(self):
+        return self.I_name
+
+    @property
+    @deprecate("0.1", "Use `U_name` instead.", "0.3")
+    def V_str(self):
+        return self.U_name
+
+    @property
+    @deprecate("0.1", "Use `J_name` instead.", "0.3")
+    def J_str(self):
+        return self.J_name
+
+    @property
+    def aliases(self):
+        """A dictionary with the names of other data series a given name can refer to"""
+        a = super().aliases.copy()
+        return a
+
+    @property
+    def calibrations(self):
+        """The list of calibrations of the measurement.
+
+        The following is necessary to ensure that all EC Calibration parameters are
+        joined in a single calibration when processing. So that "potential" is both
+        calibrated to RHE and ohmic drop corrected, even if the two calibration
+        parameters were added separately.
+        """
+        full_calibration_list = self.calibration_list
+        good_calibration_list = [self.ec_calibration]
+        for calibration in full_calibration_list:
+            if calibration.__class__ is ECCalibration:
+                # Then we have all we need from it
+                continue
+            good_calibration_list.append(calibration)
+        return good_calibration_list
+
+    @property
+    def ec_calibration(self):
+        """A calibration joining the first RE_vs_RHE, A_el, and R_Ohm"""
+        return ECCalibration(RE_vs_RHE=self.RE_vs_RHE, A_el=self.A_el, R_Ohm=self.R_Ohm)
+
+    @property
+    def RE_vs_RHE(self):
+        """The refernce electrode potential on the RHE scale in [V]"""
+        for calibration in self.calibration_list:
+            if getattr(calibration, "RE_vs_RHE", None) is not None:
+                return calibration.RE_vs_RHE
+
+    @property
+    def A_el(self):
+        """The electrode area in [cm^2]"""
+        for calibration in self.calibration_list:
+            if getattr(calibration, "A_el", None) is not None:
+                return calibration.A_el
+
+    @property
+    def R_Ohm(self):
+        """The ohmic drop resistance in [Ohm]"""
+        for calibration in self.calibration_list:
+            if getattr(calibration, "R_Ohm", None) is not None:
+                return calibration.R_Ohm
+
+    def calibrate_RE(self, RE_vs_RHE):
+        """Calibrate the reference electrode by providing `RE_vs_RHE` in [V]."""
+        new_calibration = ECCalibration(
+            RE_vs_RHE=RE_vs_RHE,
+            measurement=self,
+        )
+        self.add_calibration(new_calibration)
+
+    def normalize_current(self, A_el):
+        """Normalize current to electrode surface area by providing `A_el` in [cm^2]."""
+        new_calibration = ECCalibration(
+            A_el=A_el,
+            measurement=self,
+        )
+        self.add_calibration(new_calibration)
+
+    def correct_ohmic_drop(self, R_Ohm):
+        """Correct for ohmic drop by providing `R_Ohm` in [Ohm]."""
+        new_calibration = ECCalibration(
+            R_Ohm=R_Ohm,
+            measurement=self,
+        )
+        self.add_calibration(new_calibration)
+
+    @property
+    def potential(self):
+        return self["potential"]
+
+    @property
+    def current(self):
+        return self["current"]
+
+    @property
+    @deprecate("0.1", "Use a look-up, i.e. `ec_meas['raw_potential']`, instead.", "0.3")
+    def raw_potential(self):
+        return self["raw_potential"]
+
+    @property
+    @deprecate("0.1", "Use a look-up, i.e. `ec_meas['raw_current']`, instead.", "0.3")
+    def raw_current(self):
+        return self["raw_current"]
+
+    @property
+    def U(self):
+        """The potential [V] numpy array of the measurement"""
+        return self.potential.data.copy()
+
+    @property
+    def J(self):
+        """The current ([mA] or [mA/cm^2]) numpy array of the measurement"""
+        return self.current.data.copy()
+
+    @property
+    @deprecate("0.1", "Use `U` instead.", "0.3")
+    def v(self):
+        """The potential [V] numpy array of the measurement"""
+        return self.potential.data.copy()
+
+    @property
+    @deprecate("0.1", "Use `J` instead.", "0.3")
+    def j(self):
+        """The current ([mA] or [mA/cm^2]) numpy array of the measurement"""
+        return self.current.data.copy()
+
+    def as_cv(self):
+        """Convert self to a CyclicVoltammogram"""
+        from .cv import CyclicVoltammogram
+
+        cv_as_dict = self.as_dict()
+        cv_as_dict["technique"] = "CV"
+        # Note, this works perfectly! All needed information is in self_as_dict :)
+        return CyclicVoltammogram.from_dict(cv_as_dict)
+
+
+class ECCalibration(Calibration):
+    """An electrochemical calibration with RE_vs_RHE, A_el, and/or R_Ohm"""
+
+    extra_column_attrs = {"ec_calibration": {"RE_vs_RHE", "A_el", "R_Ohm"}}
+    # TODO: https://github.com/ixdat/ixdat/pull/11#discussion_r677552828
+
+    def __init__(
+        self,
+        name=None,
+        technique="EC",
+        tstamp=None,
+        measurement=None,
+        RE_vs_RHE=None,
+        A_el=None,
+        R_Ohm=None,
+    ):
+        """Initiate a Calibration
+
+        Args:
+            name (str): The name of the calibration
+            technique (str): The technique of the calibration
+            tstamp (float): The time at which the calibration took place or is valid
+            measurement (ECMeasurement): Optional. A measurement to calibrate by default
+            RE_vs_RHE (float): The reference electrode potential on the RHE scale in [V]
+            A_el (float): The electrode area in [cm^2]
+            R_Ohm (float): The ohmic drop resistance in [Ohm]
+        """
+        super().__init__(
+            name=name, technique=technique, tstamp=tstamp, measurement=measurement
+        )
+        self.RE_vs_RHE = RE_vs_RHE
+        self.A_el = A_el
+        self.R_Ohm = R_Ohm
+
+    def __repr__(self):
+        # TODO: make __repr__'s consistent throught ixdat
+        return (
+            f"{self.__class__.__name__}"
+            f"(RE_vs_RHE={self.RE_vs_RHE}, A_el={self.A_el}, R_Ohm={self.R_Ohm})"
+        )
+
+    def calibrate_series(self, key, measurement=None):
+        """Return a calibrated series for key based on the raw data in the measurement.
+
+        Key should be "potential" or "current". Anything else will return None.
+
+        - "potential": the calibration looks up "raw_potential" in the measurement,
+        shifts it to the RHE potential if RE_vs_RHE is available, corrects it for
+        Ohmic drop if R_Ohm is available, and then returns a calibrated potential
+        series with a name indicative of the corrections done.
+        - "current": The calibration looks up "raw_current" in the measurement,
+        normalizes it to the electrode area if A_el is available, and returns a
+        calibrated current series with a name indicative of whether the normalization
+        was done.
+        """
+        measurement = measurement or self.measurement
+        if key == "potential":
+            raw_potential = measurement["raw_potential"]
+            name = raw_potential.name
+            U = raw_potential.data
+            if self.RE_vs_RHE is not None:
+                U = U + self.RE_vs_RHE
+                name = EC_FANCY_NAMES["potential"]
+            if self.R_Ohm is not None:
+                I_mA = measurement.grab_for_t("raw_current", t=raw_potential.t)
+                U = U - self.R_Ohm * I_mA * 1e-3  # [V] = [Ohm*mA*(A/mA)]
+                name = name + " $_{ohm. corr.}$"
+            return ValueSeries(
+                name=name,
+                unit_name=raw_potential.unit_name,
+                data=U,
+                tseries=raw_potential.tseries,
+            )
+
+        if key == "current":
+            raw_current = measurement["raw_current"]
+            name = raw_current.name
+            J = raw_current.data
+            if self.A_el is not None:
+                J = J / self.A_el
+                name = EC_FANCY_NAMES["current"]
+            return ValueSeries(
+                name=name,
+                unit_name=raw_current.unit_name,
+                data=J,
+                tseries=raw_current.tseries,
+            )
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat/techniques/ec_ms.py` & `ixdat-0.2.9.dev3/src/ixdat/techniques/ec_ms.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,475 +1,475 @@
-"""Module for representation and analysis of EC-MS measurements"""
-import numpy as np
-import warnings
-from scipy.optimize import minimize
-from ..constants import FARADAY_CONSTANT
-from .ec import ECMeasurement, ECCalibration
-from .ms import MSMeasurement, MSSpectroMeasurement, MSCalResult, MSCalibration
-from .ms import _with_siq_quantifier  # FIXME: see #164
-from .cv import CyclicVoltammogram
-from ..exceptions import QuantificationError
-from ..exporters.ecms_exporter import ECMSExporter
-from ..plotters.ecms_plotter import ECMSPlotter
-from ..plotters.ms_plotter import STANDARD_COLORS
-from ..config import plugins
-
-
-class ECMSMeasurement(ECMeasurement, MSMeasurement):
-    """Class for raw EC-MS functionality. Parents: ECMeasurement and MSMeasurement"""
-
-    extra_column_attrs = {
-        "ecms_meaurements": {"ec_technique", "tspan_bg"},
-    }
-    # FIXME: It would be much more elegant if this carried over automatically from
-    #  *both* parents, by appending the table columns...
-    #  We'll see how the problem changes with the metaprogramming work.
-
-    default_plotter = ECMSPlotter
-    default_exporter = ECMSExporter
-
-    @property
-    def ec_plotter(self):
-        """A plotter for just plotting the ec data"""
-        return self.plotter.ec_plotter  # the ECPlotter of the measurement's ECMSPlotter
-
-    @property
-    def ms_plotter(self):
-        """A plotter for just plotting the ms data"""
-        return self.plotter.ms_plotter  # the MSPlotter of the measurement's ECMSPlotter
-
-    @classmethod
-    def from_dict(cls, obj_as_dict):
-        """Initiate an ECMSMeasurement from a dictionary representation.
-
-        This unpacks the ECMSCalibration from its own nested dictionary
-        TODO: Figure out a way for that to happen automatically.
-        """
-
-        if "calibration" in obj_as_dict:
-            if isinstance(obj_as_dict["calibration"], dict):
-                # FIXME: This is a mess
-                obj_as_dict["calibration"] = ECMSCalibration.from_dict(
-                    obj_as_dict["calibration"]
-                )
-        obj = super(ECMSMeasurement, cls).from_dict(obj_as_dict)
-        return obj
-
-    @property
-    def tspan(self):
-        """The tspan of an MS measurement is the tspan of its potential data"""
-        return [self.t[0], self.t[-1]]
-
-    @_with_siq_quantifier
-    def as_cv(self):
-        self_as_dict = self.as_dict()
-
-        # FIXME: The following lines are only necessary because
-        #  PlaceHolderObject.get_object isn't able to find things in the MemoryBackend
-        del self_as_dict["s_ids"]
-        self_as_dict["series_list"] = self.series_list
-
-        ecms_cv = ECMSCyclicVoltammogram.from_dict(self_as_dict)
-
-        return ecms_cv
-
-    def ecms_calibration(self, mol, mass, n_el, tspan, tspan_bg=None):
-        """Calibrate for mol and mass based on one period of steady electrolysis
-
-        Args:
-            mol (str): Name of the molecule to calibrate
-            mass (str): Name of the mass at which to calibrate
-            n_el (str): Number of electrons passed per molecule produced (remember the
-                sign! e.g. +4 for O2 by OER and -2 for H2 by HER)
-            tspan (tspan): The timespan of steady electrolysis
-            tspan_bg (tspan): The time to use as a background
-
-        Return MSCalResult: The result of the ecms_calibration
-        """
-        if plugins.use_siq:
-            warnings.warn(
-                "spectro_inlets_quantification is active but you are using the native "
-                "ixdat version of `ECMSMeasurement.ecms_calibration`"
-            )
-        Y = self.integrate_signal(mass, tspan=tspan, tspan_bg=tspan_bg)
-        Q = self.integrate("raw_current", tspan=tspan) * 1e-3
-        n = Q / (n_el * FARADAY_CONSTANT)
-        F = Y / n
-        cal = MSCalResult(
-            name=f"{mol}@{mass}",
-            mol=mol,
-            mass=mass,
-            cal_type="ecms_calibration",
-            F=F,
-        )
-        return cal
-
-    def ecms_calibration_curve(
-        self,
-        mol,
-        mass,
-        n_el,
-        tspan_list=None,
-        selector_name=None,
-        selector_list=None,
-        t_steady_pulse=None,
-        tspan_bg=None,
-        force_through_zero=False,
-        ax="new",
-        axes_measurement=None,
-        axes_measurement_J_name="raw_current",
-        return_ax=False,
-    ):
-        """Fit mol's sensitivity at mass based on steady periods of EC production.
-
-        Args:
-            mol (str): Name of the molecule to calibrate
-            mass (str): Name of the mass at which to calibrate
-            n_el (str): Number of electrons passed per molecule produced (remember the
-                sign! e.g. +4 for O2 by OER and -2 for H2 by HER)
-            tspan_list (list of tspan): The timespans of steady electrolysis
-            selector_name (str): Name of selector which identifies the periods
-                of steady electrolysis for automatic selection of timespans of steady
-                electrolysis. E.g. "selector" or "Ns" for biologic EC data
-            selector_list (list): List of values for selector_name for automatic
-                selection of timespans of steady electrolysis
-            t_steady_pulse (float): Length of steady electrolysis for each segment
-                given by selector_list. Defaults to None = entire length of segment
-            tspan_bg (tspan): The time to use as a background
-            force_through_zero (boolean): Whether to force the calibration curve through
-                zero. This can be done when confident in the background subtraction.
-            ax (Axis): The axis on which to plot the ms_calibration curve result.
-                Defaults to a new axis.
-            axes_measurement (list of Axes): The EC-MS plot axes to highlight the
-                ms_calibration on. Defaults to None. These axes are not returned.
-            axes_measurement_J_name (str): The J_name used in the axis passed
-                to axes_measurement. Must be passed manually as the axis does not "know"
-                its J_name. Defaults to "raw_current". IMPORTANT: the method still uses
-                "raw_current" to calculate the sensitivity factor, this J_name is only
-                used for plotting.
-            return_ax (bool): Whether to return the axis on which the calibration curve
-                is plotted together with the MSCalResult. Defaults to False.
-
-        Return MSCalResult(, Axis): The result of the ms_calibration (and calibration
-            curve axis if requested) based on integration of selected time periods.
-        """
-        if plugins.use_siq:
-            warnings.warn(
-                "spectro_inlets_quantification is active but you are using the native "
-                "ixdat version of `ECMSMeasurement.ecms_calibration_curve`"
-            )
-        return self._ecms_calibration_curve(
-            mol=mol,
-            mass=mass,
-            n_el=n_el,
-            tspan_list=tspan_list,
-            selector_name=selector_name,
-            selector_list=selector_list,
-            t_steady_pulse=t_steady_pulse,
-            tspan_bg=tspan_bg,
-            force_through_zero=force_through_zero,
-            ax=ax,
-            axes_measurement=axes_measurement,
-            axes_measurement_J_name=axes_measurement_J_name,
-            return_ax=return_ax,
-        )
-
-    def _ecms_calibration_curve(
-        self,
-        mol,
-        mass,
-        n_el,
-        tspan_list=None,
-        selector_name=None,
-        selector_list=None,
-        t_steady_pulse=None,
-        tspan_bg=None,
-        force_through_zero=False,
-        ax="new",
-        axes_measurement=None,
-        axes_measurement_J_name="raw_current",
-        return_ax=False,
-    ):
-        """Helper function. See ecms_calibration_curve for argument descriptions."""
-
-        axis_ms = axes_measurement[0] if axes_measurement else None
-        axis_current = axes_measurement[3] if axes_measurement else None
-        Y_list = []
-        n_list = []
-        if not tspan_list:
-            tspan_list = self._get_tspan_list(
-                selector_list, selector_name, t_steady_pulse
-            )
-        for tspan in tspan_list:
-            Y = self.integrate_signal(mass, tspan=tspan, tspan_bg=tspan_bg, ax=axis_ms)
-            # FIXME: plotting current by giving integrate() an axis doesn't work great.
-            if (
-                axes_measurement
-            ):  # FIXME: need to run twice, once to plot, once to calculate Q
-                self.integrate(axes_measurement_J_name, tspan=tspan, ax=axis_current)
-            Q = self.integrate("raw_current", tspan=tspan)
-            Q *= 1e-3  # mC --> [C]
-            n = Q / (n_el * FARADAY_CONSTANT)
-            Y_list.append(Y)
-            n_list.append(n)
-        n_vec = np.array(n_list)
-        Y_vec = np.array(Y_list)
-        n_fit = np.array([0, max(n_vec)])
-        if force_through_zero:
-
-            def rms_error(F_guess):
-                return np.mean((Y_vec - F_guess * n_vec) ** 2)
-
-            F_guess_0 = np.sum(Y_vec) / np.sum(n_vec)
-            res = minimize(rms_error, F_guess_0)
-            F = res.x[0]
-            Y_fit = n_fit * F
-        else:
-            pfit = np.polyfit(n_vec, Y_vec, deg=1)
-            F = pfit[0]
-            Y_fit = n_fit * pfit[0] + pfit[1]
-
-        if ax:
-            color = STANDARD_COLORS[mass]
-            if ax == "new":
-                ax = self.plotter.new_ax()
-                ax.set_xlabel("amount produced / [nmol]")
-                ax.set_ylabel("integrated signal / [nC]")
-            ax.plot(n_vec * 1e9, Y_vec * 1e9, "o", color=color)
-            ax.plot(n_fit * 1e9, Y_fit * 1e9, "--", color=color)
-
-        cal = MSCalResult(
-            name=f"{mol}@{mass}",
-            mol=mol,
-            mass=mass,
-            cal_type="ecms_calibration_curve",
-            F=F,
-        )
-
-        if return_ax:
-            return cal, ax
-        return cal
-
-    def _get_tspan_list(
-        self,
-        selector_list,
-        selector_name=None,
-        t_steady_pulse=None,
-    ):
-        """
-        Generate a t_span list from input of selectors.
-
-        Args:
-            selector_list (list of selector): selector numbers that define the
-                                            tspans over which data should be integrated
-            selector_name (str): name of selector that will be used to determine sections
-                                of data. Will refer to data['selector'] by default.
-                                selector_name cannot contain a space character due to
-                                limitations of self.select_values().
-            t_steady_pulse (float): length of steady state pulse period to integrate
-                                    (will choose the last x seconds of the period).
-                                    Defaults to None: uses entire steady state pulse
-
-        Returns tspan_list(list of tspan)
-        """
-        selector_name = selector_name or "selector"
-        t_idx = -1
-        if not t_steady_pulse:
-            t_idx = 0
-            t_steady_pulse = 0
-        tspan_list = [
-            [
-                self.select_values(**{selector_name: selector_value}).grab("t")[0][t_idx]
-                - t_steady_pulse,
-                self.select_values(**{selector_name: selector_value}).grab("t")[0][-1],
-            ]
-            for selector_value in selector_list
-        ]
-        print("Following tspans were selected for calibration: " + str(tspan_list))
-        return tspan_list
-
-    def siq_ecms_calibration(self, mol, mass, n_el, tspan, tspan_bg=None):
-        """Calibrate for mol and mass based on one period of steady electrolysis
-
-        Use `spectro_inlets_quantification` package.
-        Args:
-            mol (str): Name of the molecule to calibrate
-            mass (str): Name of the mass at which to calibrate
-            n_el (str): Number of electrons passed per molecule produced (remember the
-                sign! e.g. +4 for O2 by OER and -2 for H2 by HER)
-            tspan (tspan): The timespan of steady electrolysis
-            tspan_bg (tspan): The time to use as a background
-
-        Return siq.CalPoint: The result of the ecms_calibration
-        """
-        if not plugins.use_siq:
-            raise QuantificationError(
-                "`ECMSMeasurement.siq_ecms_calibration` only works when using "
-                "`spectro_inlets_quantification`"
-                "(`ixdat.options.activate_siq()`). "
-                "For native ixdat MS quantification, use `ecms_calibration`"
-                "instead."
-            )
-        Y = self.integrate_signal(mass, tspan=tspan, tspan_bg=tspan_bg)
-        Q = self.integrate("raw_current", tspan=tspan) * 1e-3
-        n = Q / (n_el * FARADAY_CONSTANT)
-        F = Y / n
-        cal = plugins.siq.CalPoint(
-            name=f"{mol}@{mass}",
-            mol=mol,
-            mass=mass,
-            F_type="ecms_calibration",
-            F=F,
-        )
-        return cal
-
-    def siq_ecms_calibration_curve(
-        self,
-        mol,
-        mass,
-        n_el,
-        tspan_list=None,
-        selector_name=None,
-        selector_list=None,
-        t_steady_pulse=None,
-        tspan_bg=None,
-        force_through_zero=False,
-        ax="new",
-        axes_measurement=None,
-        axes_measurement_J_name="raw_current",
-        return_ax=False,
-    ):
-        """Fit mol's sensitivity at mass based on steady periods of EC production.
-
-        Use `spectro_inlets_quantification`.
-
-        Args:
-            mol (str): Name of the molecule to calibrate
-            mass (str): Name of the mass at which to calibrate
-            n_el (str): Number of electrons passed per molecule produced (remember the
-                sign! e.g. +4 for O2 by OER and -2 for H2 by HER)
-            tspan_list (list of tspan): The timespans of steady electrolysis
-            selector_name (str): Name of selector which identifies the periods
-                of steady electrolysis for automatic selection of timespans of steady
-                electrolysis. E.g. "selector" or "Ns" for biologic EC data
-            selector_list (list): List of values for selector_name for automatic
-                selection of timespans of steady electrolysis
-            t_steady_pulse (float): Length of steady electrolysis for each segment
-                given by selector_list. Defaults to None = entire length of segment
-            tspan_bg (tspan): The time to use as a background
-            force_through_zero (boolean): Whether to force the calibration curve through
-                zero. This can be done when confident in the background subtraction.
-            ax (Axis): The axis on which to plot the ms_calibration curve result.
-                Defaults to a new axis.
-            axes_measurement (list of Axes): The EC-MS plot axes to highlight the
-                ms_calibration on. Defaults to None. These axes are not returned.
-            axes_measurement_J_name (str): The J_name used in the axis passed
-                to axes_measurement. Must be passed manually as the axis does not "know"
-                its J_name. Defaults to "raw_current". IMPORTANT: the method still uses
-                "raw_current" to calculate the sensitivity factor, this J_name is only
-                used for plotting.
-            return_ax (bool): Whether to return the axis on which the calibration curve
-                is plotted together with the MSCalResult. Defaults to False.
-
-        Return MSCalResult(, Axis): The result of the ms_calibration (and calibration
-            curve axis if requested) based on integration of selected time periods.
-        """
-        if not plugins.use_siq:
-            raise QuantificationError(
-                "`ECMSMeasurement.siq_ecms_calibration_curve` only works when using "
-                "`spectro_inlets_quantification`"
-                "(`ixdat.options.activate_siq()`). "
-                "For native ixdat MS quantification, use `ecms_calibration_curve`"
-                "instead."
-            )
-        ms_cal_result, ax = self._ecms_calibration_curve(
-            mol=mol,
-            mass=mass,
-            n_el=n_el,
-            tspan_list=tspan_list,
-            selector_name=selector_name,
-            selector_list=selector_list,
-            t_steady_pulse=t_steady_pulse,
-            tspan_bg=tspan_bg,
-            force_through_zero=force_through_zero,
-            ax=ax,
-            axes_measurement=axes_measurement,
-            axes_measurement_J_name=axes_measurement_J_name,
-            return_ax=True,
-        )
-        cal = ms_cal_result.to_siq()
-        if return_ax:
-            return cal, ax
-        else:
-            return cal
-
-
-class ECMSCyclicVoltammogram(CyclicVoltammogram, ECMSMeasurement):
-    """Class for raw EC-MS functionality. Parents: CyclicVoltammogram, ECMSMeasurement"""
-
-
-class ECMSCalibration(ECCalibration, MSCalibration):
-    """Class for calibrations useful for ECMSMeasurements"""
-
-    extra_column_attrs = {
-        "ecms_calibrations": {"date", "setup", "RE_vs_RHE", "A_el", "L"}
-    }
-    # FIXME: The above should be covered by the parent classes. Needs metaprogramming!
-    # NOTE: technique, name, and tstamp in column_attrs are inherited from Calibration
-    # NOTE: ms_results_ids in extra_linkers is inherited from MSCalibration.
-    # NOTE: signal_bgs is left out
-
-    def __init__(
-        self,
-        name=None,
-        date=None,
-        tstamp=None,
-        setup=None,
-        ms_cal_results=None,
-        signal_bgs=None,
-        RE_vs_RHE=None,
-        A_el=None,
-        R_Ohm=None,
-        L=None,
-        technique="EC-MS",
-    ):
-        """
-        Args:
-            name (str): Name of the ms_calibration
-            date (str): Date of the ms_calibration
-            setup (str): Name of the setup where the ms_calibration is made
-            ms_cal_results (list of MSCalResult): The mass spec calibrations
-            RE_vs_RHE (float): the RE potential in [V]
-            A_el (float): The geometric electrode area in [cm^2]
-            R_Ohm (float): The Ohmic drop in [Ohm]
-            L (float): The working distance in [m]
-        """
-        ECCalibration.__init__(
-            self,
-            A_el=A_el,
-            RE_vs_RHE=RE_vs_RHE,
-            R_Ohm=R_Ohm,
-        )
-        MSCalibration.__init__(
-            self,
-            name=name,
-            date=date,
-            tstamp=tstamp,
-            setup=setup,
-            ms_cal_results=ms_cal_results,
-            signal_bgs=signal_bgs,
-        )
-        self.technique = technique
-        self.L = L
-
-    def calibrate_series(self, key, measurement=None):
-        measurement = measurement or self.measurement
-        try_1 = ECCalibration.calibrate_series(self, key, measurement)
-        if try_1:
-            return try_1
-        try_2 = MSCalibration.calibrate_series(self, key, measurement)
-        if try_2:
-            return try_2
-
-
-class ECMSSpectroMeasurement(ECMSMeasurement, MSSpectroMeasurement):
-    pass
+"""Module for representation and analysis of EC-MS measurements"""
+import numpy as np
+import warnings
+from scipy.optimize import minimize
+from ..constants import FARADAY_CONSTANT
+from .ec import ECMeasurement, ECCalibration
+from .ms import MSMeasurement, MSSpectroMeasurement, MSCalResult, MSCalibration
+from .ms import _with_siq_quantifier  # FIXME: see #164
+from .cv import CyclicVoltammogram
+from ..exceptions import QuantificationError
+from ..exporters.ecms_exporter import ECMSExporter
+from ..plotters.ecms_plotter import ECMSPlotter
+from ..plotters.ms_plotter import STANDARD_COLORS
+from ..config import plugins
+
+
+class ECMSMeasurement(ECMeasurement, MSMeasurement):
+    """Class for raw EC-MS functionality. Parents: ECMeasurement and MSMeasurement"""
+
+    extra_column_attrs = {
+        "ecms_meaurements": {"ec_technique", "tspan_bg"},
+    }
+    # FIXME: It would be much more elegant if this carried over automatically from
+    #  *both* parents, by appending the table columns...
+    #  We'll see how the problem changes with the metaprogramming work.
+
+    default_plotter = ECMSPlotter
+    default_exporter = ECMSExporter
+
+    @property
+    def ec_plotter(self):
+        """A plotter for just plotting the ec data"""
+        return self.plotter.ec_plotter  # the ECPlotter of the measurement's ECMSPlotter
+
+    @property
+    def ms_plotter(self):
+        """A plotter for just plotting the ms data"""
+        return self.plotter.ms_plotter  # the MSPlotter of the measurement's ECMSPlotter
+
+    @classmethod
+    def from_dict(cls, obj_as_dict):
+        """Initiate an ECMSMeasurement from a dictionary representation.
+
+        This unpacks the ECMSCalibration from its own nested dictionary
+        TODO: Figure out a way for that to happen automatically.
+        """
+
+        if "calibration" in obj_as_dict:
+            if isinstance(obj_as_dict["calibration"], dict):
+                # FIXME: This is a mess
+                obj_as_dict["calibration"] = ECMSCalibration.from_dict(
+                    obj_as_dict["calibration"]
+                )
+        obj = super(ECMSMeasurement, cls).from_dict(obj_as_dict)
+        return obj
+
+    @property
+    def tspan(self):
+        """The tspan of an MS measurement is the tspan of its potential data"""
+        return [self.t[0], self.t[-1]]
+
+    @_with_siq_quantifier
+    def as_cv(self):
+        self_as_dict = self.as_dict()
+
+        # FIXME: The following lines are only necessary because
+        #  PlaceHolderObject.get_object isn't able to find things in the MemoryBackend
+        del self_as_dict["s_ids"]
+        self_as_dict["series_list"] = self.series_list
+
+        ecms_cv = ECMSCyclicVoltammogram.from_dict(self_as_dict)
+
+        return ecms_cv
+
+    def ecms_calibration(self, mol, mass, n_el, tspan, tspan_bg=None):
+        """Calibrate for mol and mass based on one period of steady electrolysis
+
+        Args:
+            mol (str): Name of the molecule to calibrate
+            mass (str): Name of the mass at which to calibrate
+            n_el (str): Number of electrons passed per molecule produced (remember the
+                sign! e.g. +4 for O2 by OER and -2 for H2 by HER)
+            tspan (tspan): The timespan of steady electrolysis
+            tspan_bg (tspan): The time to use as a background
+
+        Return MSCalResult: The result of the ecms_calibration
+        """
+        if plugins.use_siq:
+            warnings.warn(
+                "spectro_inlets_quantification is active but you are using the native "
+                "ixdat version of `ECMSMeasurement.ecms_calibration`"
+            )
+        Y = self.integrate_signal(mass, tspan=tspan, tspan_bg=tspan_bg)
+        Q = self.integrate("raw_current", tspan=tspan) * 1e-3
+        n = Q / (n_el * FARADAY_CONSTANT)
+        F = Y / n
+        cal = MSCalResult(
+            name=f"{mol}@{mass}",
+            mol=mol,
+            mass=mass,
+            cal_type="ecms_calibration",
+            F=F,
+        )
+        return cal
+
+    def ecms_calibration_curve(
+        self,
+        mol,
+        mass,
+        n_el,
+        tspan_list=None,
+        selector_name=None,
+        selector_list=None,
+        t_steady_pulse=None,
+        tspan_bg=None,
+        force_through_zero=False,
+        ax="new",
+        axes_measurement=None,
+        axes_measurement_J_name="raw_current",
+        return_ax=False,
+    ):
+        """Fit mol's sensitivity at mass based on steady periods of EC production.
+
+        Args:
+            mol (str): Name of the molecule to calibrate
+            mass (str): Name of the mass at which to calibrate
+            n_el (str): Number of electrons passed per molecule produced (remember the
+                sign! e.g. +4 for O2 by OER and -2 for H2 by HER)
+            tspan_list (list of tspan): The timespans of steady electrolysis
+            selector_name (str): Name of selector which identifies the periods
+                of steady electrolysis for automatic selection of timespans of steady
+                electrolysis. E.g. "selector" or "Ns" for biologic EC data
+            selector_list (list): List of values for selector_name for automatic
+                selection of timespans of steady electrolysis
+            t_steady_pulse (float): Length of steady electrolysis for each segment
+                given by selector_list. Defaults to None = entire length of segment
+            tspan_bg (tspan): The time to use as a background
+            force_through_zero (boolean): Whether to force the calibration curve through
+                zero. This can be done when confident in the background subtraction.
+            ax (Axis): The axis on which to plot the ms_calibration curve result.
+                Defaults to a new axis.
+            axes_measurement (list of Axes): The EC-MS plot axes to highlight the
+                ms_calibration on. Defaults to None. These axes are not returned.
+            axes_measurement_J_name (str): The J_name used in the axis passed
+                to axes_measurement. Must be passed manually as the axis does not "know"
+                its J_name. Defaults to "raw_current". IMPORTANT: the method still uses
+                "raw_current" to calculate the sensitivity factor, this J_name is only
+                used for plotting.
+            return_ax (bool): Whether to return the axis on which the calibration curve
+                is plotted together with the MSCalResult. Defaults to False.
+
+        Return MSCalResult(, Axis): The result of the ms_calibration (and calibration
+            curve axis if requested) based on integration of selected time periods.
+        """
+        if plugins.use_siq:
+            warnings.warn(
+                "spectro_inlets_quantification is active but you are using the native "
+                "ixdat version of `ECMSMeasurement.ecms_calibration_curve`"
+            )
+        return self._ecms_calibration_curve(
+            mol=mol,
+            mass=mass,
+            n_el=n_el,
+            tspan_list=tspan_list,
+            selector_name=selector_name,
+            selector_list=selector_list,
+            t_steady_pulse=t_steady_pulse,
+            tspan_bg=tspan_bg,
+            force_through_zero=force_through_zero,
+            ax=ax,
+            axes_measurement=axes_measurement,
+            axes_measurement_J_name=axes_measurement_J_name,
+            return_ax=return_ax,
+        )
+
+    def _ecms_calibration_curve(
+        self,
+        mol,
+        mass,
+        n_el,
+        tspan_list=None,
+        selector_name=None,
+        selector_list=None,
+        t_steady_pulse=None,
+        tspan_bg=None,
+        force_through_zero=False,
+        ax="new",
+        axes_measurement=None,
+        axes_measurement_J_name="raw_current",
+        return_ax=False,
+    ):
+        """Helper function. See ecms_calibration_curve for argument descriptions."""
+
+        axis_ms = axes_measurement[0] if axes_measurement else None
+        axis_current = axes_measurement[3] if axes_measurement else None
+        Y_list = []
+        n_list = []
+        if not tspan_list:
+            tspan_list = self._get_tspan_list(
+                selector_list, selector_name, t_steady_pulse
+            )
+        for tspan in tspan_list:
+            Y = self.integrate_signal(mass, tspan=tspan, tspan_bg=tspan_bg, ax=axis_ms)
+            # FIXME: plotting current by giving integrate() an axis doesn't work great.
+            if (
+                axes_measurement
+            ):  # FIXME: need to run twice, once to plot, once to calculate Q
+                self.integrate(axes_measurement_J_name, tspan=tspan, ax=axis_current)
+            Q = self.integrate("raw_current", tspan=tspan)
+            Q *= 1e-3  # mC --> [C]
+            n = Q / (n_el * FARADAY_CONSTANT)
+            Y_list.append(Y)
+            n_list.append(n)
+        n_vec = np.array(n_list)
+        Y_vec = np.array(Y_list)
+        n_fit = np.array([0, max(n_vec)])
+        if force_through_zero:
+
+            def rms_error(F_guess):
+                return np.mean((Y_vec - F_guess * n_vec) ** 2)
+
+            F_guess_0 = np.sum(Y_vec) / np.sum(n_vec)
+            res = minimize(rms_error, F_guess_0)
+            F = res.x[0]
+            Y_fit = n_fit * F
+        else:
+            pfit = np.polyfit(n_vec, Y_vec, deg=1)
+            F = pfit[0]
+            Y_fit = n_fit * pfit[0] + pfit[1]
+
+        if ax:
+            color = STANDARD_COLORS[mass]
+            if ax == "new":
+                ax = self.plotter.new_ax()
+                ax.set_xlabel("amount produced / [nmol]")
+                ax.set_ylabel("integrated signal / [nC]")
+            ax.plot(n_vec * 1e9, Y_vec * 1e9, "o", color=color)
+            ax.plot(n_fit * 1e9, Y_fit * 1e9, "--", color=color)
+
+        cal = MSCalResult(
+            name=f"{mol}@{mass}",
+            mol=mol,
+            mass=mass,
+            cal_type="ecms_calibration_curve",
+            F=F,
+        )
+
+        if return_ax:
+            return cal, ax
+        return cal
+
+    def _get_tspan_list(
+        self,
+        selector_list,
+        selector_name=None,
+        t_steady_pulse=None,
+    ):
+        """
+        Generate a t_span list from input of selectors.
+
+        Args:
+            selector_list (list of selector): selector numbers that define the
+                                            tspans over which data should be integrated
+            selector_name (str): name of selector that will be used to determine sections
+                                of data. Will refer to data['selector'] by default.
+                                selector_name cannot contain a space character due to
+                                limitations of self.select_values().
+            t_steady_pulse (float): length of steady state pulse period to integrate
+                                    (will choose the last x seconds of the period).
+                                    Defaults to None: uses entire steady state pulse
+
+        Returns tspan_list(list of tspan)
+        """
+        selector_name = selector_name or "selector"
+        t_idx = -1
+        if not t_steady_pulse:
+            t_idx = 0
+            t_steady_pulse = 0
+        tspan_list = [
+            [
+                self.select_values(**{selector_name: selector_value}).grab("t")[0][t_idx]
+                - t_steady_pulse,
+                self.select_values(**{selector_name: selector_value}).grab("t")[0][-1],
+            ]
+            for selector_value in selector_list
+        ]
+        print("Following tspans were selected for calibration: " + str(tspan_list))
+        return tspan_list
+
+    def siq_ecms_calibration(self, mol, mass, n_el, tspan, tspan_bg=None):
+        """Calibrate for mol and mass based on one period of steady electrolysis
+
+        Use `spectro_inlets_quantification` package.
+        Args:
+            mol (str): Name of the molecule to calibrate
+            mass (str): Name of the mass at which to calibrate
+            n_el (str): Number of electrons passed per molecule produced (remember the
+                sign! e.g. +4 for O2 by OER and -2 for H2 by HER)
+            tspan (tspan): The timespan of steady electrolysis
+            tspan_bg (tspan): The time to use as a background
+
+        Return siq.CalPoint: The result of the ecms_calibration
+        """
+        if not plugins.use_siq:
+            raise QuantificationError(
+                "`ECMSMeasurement.siq_ecms_calibration` only works when using "
+                "`spectro_inlets_quantification`"
+                "(`ixdat.options.activate_siq()`). "
+                "For native ixdat MS quantification, use `ecms_calibration`"
+                "instead."
+            )
+        Y = self.integrate_signal(mass, tspan=tspan, tspan_bg=tspan_bg)
+        Q = self.integrate("raw_current", tspan=tspan) * 1e-3
+        n = Q / (n_el * FARADAY_CONSTANT)
+        F = Y / n
+        cal = plugins.siq.CalPoint(
+            name=f"{mol}@{mass}",
+            mol=mol,
+            mass=mass,
+            F_type="ecms_calibration",
+            F=F,
+        )
+        return cal
+
+    def siq_ecms_calibration_curve(
+        self,
+        mol,
+        mass,
+        n_el,
+        tspan_list=None,
+        selector_name=None,
+        selector_list=None,
+        t_steady_pulse=None,
+        tspan_bg=None,
+        force_through_zero=False,
+        ax="new",
+        axes_measurement=None,
+        axes_measurement_J_name="raw_current",
+        return_ax=False,
+    ):
+        """Fit mol's sensitivity at mass based on steady periods of EC production.
+
+        Use `spectro_inlets_quantification`.
+
+        Args:
+            mol (str): Name of the molecule to calibrate
+            mass (str): Name of the mass at which to calibrate
+            n_el (str): Number of electrons passed per molecule produced (remember the
+                sign! e.g. +4 for O2 by OER and -2 for H2 by HER)
+            tspan_list (list of tspan): The timespans of steady electrolysis
+            selector_name (str): Name of selector which identifies the periods
+                of steady electrolysis for automatic selection of timespans of steady
+                electrolysis. E.g. "selector" or "Ns" for biologic EC data
+            selector_list (list): List of values for selector_name for automatic
+                selection of timespans of steady electrolysis
+            t_steady_pulse (float): Length of steady electrolysis for each segment
+                given by selector_list. Defaults to None = entire length of segment
+            tspan_bg (tspan): The time to use as a background
+            force_through_zero (boolean): Whether to force the calibration curve through
+                zero. This can be done when confident in the background subtraction.
+            ax (Axis): The axis on which to plot the ms_calibration curve result.
+                Defaults to a new axis.
+            axes_measurement (list of Axes): The EC-MS plot axes to highlight the
+                ms_calibration on. Defaults to None. These axes are not returned.
+            axes_measurement_J_name (str): The J_name used in the axis passed
+                to axes_measurement. Must be passed manually as the axis does not "know"
+                its J_name. Defaults to "raw_current". IMPORTANT: the method still uses
+                "raw_current" to calculate the sensitivity factor, this J_name is only
+                used for plotting.
+            return_ax (bool): Whether to return the axis on which the calibration curve
+                is plotted together with the MSCalResult. Defaults to False.
+
+        Return MSCalResult(, Axis): The result of the ms_calibration (and calibration
+            curve axis if requested) based on integration of selected time periods.
+        """
+        if not plugins.use_siq:
+            raise QuantificationError(
+                "`ECMSMeasurement.siq_ecms_calibration_curve` only works when using "
+                "`spectro_inlets_quantification`"
+                "(`ixdat.options.activate_siq()`). "
+                "For native ixdat MS quantification, use `ecms_calibration_curve`"
+                "instead."
+            )
+        ms_cal_result, ax = self._ecms_calibration_curve(
+            mol=mol,
+            mass=mass,
+            n_el=n_el,
+            tspan_list=tspan_list,
+            selector_name=selector_name,
+            selector_list=selector_list,
+            t_steady_pulse=t_steady_pulse,
+            tspan_bg=tspan_bg,
+            force_through_zero=force_through_zero,
+            ax=ax,
+            axes_measurement=axes_measurement,
+            axes_measurement_J_name=axes_measurement_J_name,
+            return_ax=True,
+        )
+        cal = ms_cal_result.to_siq()
+        if return_ax:
+            return cal, ax
+        else:
+            return cal
+
+
+class ECMSCyclicVoltammogram(CyclicVoltammogram, ECMSMeasurement):
+    """Class for raw EC-MS functionality. Parents: CyclicVoltammogram, ECMSMeasurement"""
+
+
+class ECMSCalibration(ECCalibration, MSCalibration):
+    """Class for calibrations useful for ECMSMeasurements"""
+
+    extra_column_attrs = {
+        "ecms_calibrations": {"date", "setup", "RE_vs_RHE", "A_el", "L"}
+    }
+    # FIXME: The above should be covered by the parent classes. Needs metaprogramming!
+    # NOTE: technique, name, and tstamp in column_attrs are inherited from Calibration
+    # NOTE: ms_results_ids in extra_linkers is inherited from MSCalibration.
+    # NOTE: signal_bgs is left out
+
+    def __init__(
+        self,
+        name=None,
+        date=None,
+        tstamp=None,
+        setup=None,
+        ms_cal_results=None,
+        signal_bgs=None,
+        RE_vs_RHE=None,
+        A_el=None,
+        R_Ohm=None,
+        L=None,
+        technique="EC-MS",
+    ):
+        """
+        Args:
+            name (str): Name of the ms_calibration
+            date (str): Date of the ms_calibration
+            setup (str): Name of the setup where the ms_calibration is made
+            ms_cal_results (list of MSCalResult): The mass spec calibrations
+            RE_vs_RHE (float): the RE potential in [V]
+            A_el (float): The geometric electrode area in [cm^2]
+            R_Ohm (float): The Ohmic drop in [Ohm]
+            L (float): The working distance in [m]
+        """
+        ECCalibration.__init__(
+            self,
+            A_el=A_el,
+            RE_vs_RHE=RE_vs_RHE,
+            R_Ohm=R_Ohm,
+        )
+        MSCalibration.__init__(
+            self,
+            name=name,
+            date=date,
+            tstamp=tstamp,
+            setup=setup,
+            ms_cal_results=ms_cal_results,
+            signal_bgs=signal_bgs,
+        )
+        self.technique = technique
+        self.L = L
+
+    def calibrate_series(self, key, measurement=None):
+        measurement = measurement or self.measurement
+        try_1 = ECCalibration.calibrate_series(self, key, measurement)
+        if try_1:
+            return try_1
+        try_2 = MSCalibration.calibrate_series(self, key, measurement)
+        if try_2:
+            return try_2
+
+
+class ECMSSpectroMeasurement(ECMSMeasurement, MSSpectroMeasurement):
+    pass
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat/techniques/ms.py` & `ixdat-0.2.9.dev3/src/ixdat/techniques/ms.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,1409 +1,1409 @@
-"""Module for representation and analysis of MS measurements"""
-
-import re
-import numpy as np
-import json  # FIXME: This is for MSCalibration.export, but shouldn't have to be here.
-import warnings
-
-from ..measurements import Measurement, Calibration
-from ..spectra import Spectrum, SpectrumSeries, SpectroMeasurement
-from ..plotters.ms_plotter import MSPlotter, MSSpectroPlotter, STANDARD_COLORS
-from ..exporters.ms_exporter import MSExporter, MSSpectroExporter
-from ..exceptions import QuantificationError
-from ..constants import (
-    AVOGADRO_CONSTANT,
-    BOLTZMANN_CONSTANT,
-    STANDARD_TEMPERATURE,
-    STANDARD_PRESSURE,
-    DYNAMIC_VISCOSITIES,
-    MOLECULAR_DIAMETERS,
-    MOLAR_MASSES,
-)
-from ..data_series import ValueSeries
-from ..db import Saveable
-from ..tools import deprecate
-from ..config import plugins
-
-
-def _with_siq_quantifier(method):
-    """Decorate a Measurement-copying method to copy the quantifier to the new one.
-
-    This means that the quantifier doesn't need to be re-applied when getting a
-    subset of the measurement.
-
-    Method is a method of a Measurement which returns a Measurement of the same type,
-    such as cut() and multicut() and the methods that use these ones, such as select,
-    and indexing for CyclicVoltammogram objects.
-    """
-
-    def method_with_siq_quantifier(*args, **kwargs):
-        new_measurement = method(*args, **kwargs)
-        old_measurement = args[0]
-        if old_measurement.siq_quantifier:
-            new_measurement.set_siq_quantifier(old_measurement.siq_quantifier)
-        return new_measurement
-
-    return method_with_siq_quantifier
-
-
-class MSMeasurement(Measurement):
-    """Class implementing raw MS functionality"""
-
-    # FIXME: tspan_bg should be column of a DataTreater, not MSMeasurement:
-    extra_column_attrs = {"ms_measurement": ("tspan_bg",)}
-    default_plotter = MSPlotter
-    default_exporter = MSExporter
-
-    def __init__(self, name, **kwargs):
-        tspan_bg = kwargs.pop("tspan_bg", None)
-        super().__init__(name, **kwargs)
-        self.tspan_bg = tspan_bg
-        self._siq_quantifier = None  # Used with external quantification package
-
-    @property
-    def ms_calibration(self):
-        ms_cal_list = []
-        tspan_bg = None
-        signal_bgs = {}
-        for cal in self.calibration_list:
-            ms_cal_list = ms_cal_list + getattr(cal, "ms_cal_list", [])
-            for mass, bg in getattr(cal, "signal_bgs", {}).items():
-                if mass not in signal_bgs:
-                    signal_bgs[mass] = bg
-            tspan_bg = tspan_bg or getattr(cal, "tspan_bg", None)
-        return MSCalibration(ms_cal_results=ms_cal_list, signal_bgs=signal_bgs)
-
-    @property
-    def signal_bgs(self):
-        return self.ms_calibration.signal_bgs
-
-    def set_bg(self, tspan_bg=None, mass_list=None):
-        """Set background values for mass_list to the average signal during tspan_bg."""
-        mass_list = mass_list or self.mass_list
-        tspan_bg = tspan_bg or self.tspan_bg
-        signal_bgs = {}
-        for mass in mass_list:
-            t, v = self.grab(mass, tspan_bg)
-            signal_bgs[mass] = np.mean(v)
-        self.add_calibration(MSCalibration(signal_bgs=signal_bgs))
-
-    def reset_bg(self, mass_list=None):
-        """Reset background values for the masses in mass_list"""
-        mass_list = mass_list or self.mass_list
-        new_signal_bgs = {}
-        for mass in mass_list:
-            if mass in self.signal_bgs:
-                new_signal_bgs[mass] = 0
-        self.add_calibration(MSCalibration(signal_bgs=new_signal_bgs))
-
-    def grab(
-        self,
-        item,
-        tspan=None,
-        tspan_bg=None,
-        include_endpoints=False,
-        remove_background=False,
-    ):
-        """Returns t, S where S is raw signal in [A] for a given signal name (ie mass)
-
-        Args:
-            item (str): Name of the signal. If `item` has the form f"n_dot_{mol}", then
-                grab_flux(mol) is returned.
-            tspan (list): Timespan for which the signal is returned.
-            tspan_bg (list): Timespan that corresponds to the background signal.
-                If not given, no background is subtracted.
-            remove_background (bool): Whether to remove a pre-set background if
-                available. This is special to MSMeasurement.
-                Defaults to False, but in grab_flux it defaults to True.
-            include_endpoints (bool): Whether to ensure tspan[0] and tspan[-1] are in t
-        """
-        if plugins.use_siq and item.startswith("n_dot_"):
-            return self.grab_flux(
-                item.removeprefix("n_dot_"),
-                tspan=tspan,
-                tspan_bg=tspan_bg,
-                include_endpoints=include_endpoints,
-            )
-        time, value = super().grab(
-            item, tspan=tspan, include_endpoints=include_endpoints
-        )
-        if tspan_bg:
-            _, bg = self.grab(item, tspan=tspan_bg)
-            return time, value - np.average(bg)
-        elif remove_background:
-            if item in self.signal_bgs:
-                return time, value - self.signal_bgs[item]
-            elif self.tspan_bg:
-                _, bg = self.grab(item, tspan=self.tspan_bg)
-                return time, value - np.average(bg)
-        return time, value
-
-    def grab_for_t(self, item, t, tspan_bg=None, remove_background=False):
-        """Return a numpy array with the value of item interpolated to time t
-
-        Args:
-            item (str): The name of the value to grab
-            t (np array): The time vector to grab the value for
-            tspan_bg (iterable): Optional. A timespan defining when `item` is at its
-                baseline level. The average value of `item` in this interval will be
-                subtracted from what is returned.
-            remove_background (bool): Whether to remove a pre-set background if
-                available. This is special to MSMeasurement.
-                Defaults to False, but in grab_flux it defaults to True.
-        """
-        t_0, v_0 = self.grab(
-            item, tspan_bg=tspan_bg, remove_background=remove_background
-        )
-        v = np.interp(t, t_0, v_0)
-        return v
-
-    def grab_signal(self, *args, **kwargs):
-        """Alias for grab()"""
-        return self.grab(*args, **kwargs)
-
-    @deprecate(
-        "0.1", "Use `remove_background` instead.", "0.3", kwarg_name="removebackground"
-    )
-    def grab_flux(
-        self,
-        mol,
-        tspan=None,
-        tspan_bg=None,
-        remove_background=True,
-        removebackground=None,
-        include_endpoints=False,
-    ):
-        """Return the flux of mol (calibrated signal) in [mol/s]
-
-        Note:
-        - With native ixdat quantification (use_siq=False),
-          `grab_flux(mol, ...)` is identical to `grab(f"n_dot_{mol}", ...)` with
-          remove_background=True by default. An MSCalibration does the maths.
-        - With an external quantification package (use_siq=True), the maths are done
-          here with the help of self.quantifier
-
-        Args:
-            mol (str or MSCalResult): Name of the molecule or a ms_calibration thereof
-            tspan (list): Timespan for which the signal is returned.
-            tspan_bg (list): Timespan that corresponds to the background signal.
-                If not given, no background is subtracted.
-            remove_background (bool): Whether to remove a pre-set background if available
-                Defaults to True.
-            removebackground (bool): DEPRECATED. Use `remove_background`.
-            include_endpoints (bool): Whether to interpolate for tspan[0] and tspan[-1]
-        """
-        if removebackground is not None:
-            remove_background = removebackground
-
-        if plugins.use_siq:
-            # We have to calculate the fluxes of all the mols and masses in the
-            # quantifier's sensitivity matrix. But this method only returns one.
-            # TODO: The results should therefore be cached. But how to know when they
-            #   need to be recalculated?
-            t, n_dots = self.grab_siq_fluxes(
-                tspan=tspan,
-                tspan_bg=tspan_bg,
-                remove_background=remove_background,
-                include_endpoints=include_endpoints,
-            )
-            return t, n_dots[mol]
-
-        if isinstance(mol, MSCalResult):
-            t, signal = self.grab(
-                mol.mass,
-                tspan=tspan,
-                tspan_bg=tspan_bg,
-                remove_background=remove_background,
-                include_endpoints=include_endpoints,
-            )
-            return t, signal / mol.F
-        return self.grab(
-            # grab() invokes __getitem__, which invokes the `Calibration`. Specifically,
-            # `MSCalibration.calibrate_series()` interprets item names starting with
-            # "n_" as molecule fluxes, and checks itself for a sensitivity factor.
-            f"n_dot_{mol}",
-            tspan=tspan,
-            tspan_bg=tspan_bg,
-            remove_background=remove_background,
-            include_endpoints=include_endpoints,
-        )
-
-    def grab_siq_fluxes(
-        self, tspan=None, tspan_bg=None, remove_background=False, include_endpoints=False
-    ):
-        """Return a time vector and a dictionary with all the quantified fluxes
-
-        Args:
-            tspan (list): Timespan for which the signal is returned.
-            tspan_bg (list): Timespan that corresponds to the background signal.
-                If not given, no background is subtracted.
-            remove_background (bool): Whether to remove a pre-set background if available
-                Defaults to True..
-            include_endpoints (bool): Whether to interpolate for tspan[0] and tspan[-1]
-        """
-        if not plugins.use_siq:
-            raise QuantificationError(
-                "`MSMeasurement.grab_siq_fluxes` only works when using "
-                "`spectro_inlets_quantification` "
-                "(`ixdat.plugins.activate_siq()`). "
-            )
-        sm = self._siq_quantifier.sm
-        signals = {}
-        t = None
-        for mass in sm.mass_list:
-            if t is None:
-                t, S = self.grab(
-                    mass,
-                    tspan=tspan,
-                    tspan_bg=tspan_bg,
-                    remove_background=remove_background,
-                    include_endpoints=include_endpoints,
-                )
-            else:
-                S = self.grab_for_t(
-                    mass,
-                    t=t,
-                    tspan_bg=tspan_bg,
-                    remove_background=remove_background,
-                )
-            signals[mass] = S
-        n_dots = sm.calc_n_dot(signals=signals)
-        return t, n_dots
-
-    @deprecate(
-        "0.1", "Use `remove_background` instead.", "0.3", kwarg_name="removebackground"
-    )
-    def grab_flux_for_t(
-        self,
-        mol,
-        t,
-        tspan_bg=None,
-        remove_background=False,
-        removebackground=None,
-    ):
-        """Return the flux of mol (calibrated signal) in [mol/s] for a given time vec
-
-        Args:
-            mol (str): Name of the molecule.
-            t (np.array): The time vector along which to give the flux
-            tspan_bg (tspan): Timespan that corresponds to the background signal.
-                If not given, no background is subtracted.
-            remove_background (bool): Whether to remove a pre-set background if available
-            removebackground (bool): DEPRECATED. Use `remove_background`.
-        """
-        if removebackground is not None:
-            remove_background = removebackground
-        t_0, y_0 = self.grab_flux(
-            mol,
-            tspan_bg=tspan_bg,
-            remove_background=remove_background,
-        )
-        y = np.interp(t, t_0, y_0)
-        return y
-
-    def get_flux_series(self, mol):
-        """Return a ValueSeries with the calibrated flux of mol"""
-        return self[f"n_dot_{mol}"]
-
-    def integrate_signal(self, mass, tspan, tspan_bg, ax=None):
-        """Integrate a ms signal with background subtraction and evt. plotting
-
-        TODO: Should this, like grab_signal does now, have the option of using a
-            background saved in the object rather than calculating a new one?
-
-        Args:
-            mass (str): The mass for which to integrate the signal
-            tspan (tspan): The timespan over which to integrate
-            tspan_bg (tspan): Timespan at which the signal is at its background value
-            ax (Axis): axis to plot on. Defaults to None
-        """
-        t, S = self.grab_signal(mass, tspan=tspan, include_endpoints=True)
-        if tspan_bg:
-            t_bg, S_bg_0 = self.grab_signal(mass, tspan=tspan_bg, include_endpoints=True)
-            S_bg = np.mean(S_bg_0) * np.ones(t.shape)
-        else:
-            S_bg = np.zeros(t.shape)
-        if ax:
-            if ax == "new":
-                fig, ax = self.plotter.new_ax()
-            ax.fill_between(t, S_bg, S, color=STANDARD_COLORS[mass], alpha=0.2)
-        return np.trapz(S - S_bg, t)
-
-    @property
-    def mass_list(self):
-        """List of the masses for which ValueSeries are contained in the measurement"""
-        return [self.as_mass(col) for col in self.series_names if self.is_mass(col)]
-
-    def is_mass(self, item):
-        if re.search("^M[0-9]+$", item):
-            return True
-        if item in self.reverse_aliases and self.is_mass(self.reverse_aliases[item][0]):
-            return True
-        return False
-
-    def as_mass(self, item):
-        if re.search("^M[0-9]+$", item):
-            return item
-        new_item = self.reverse_aliases[item][0]
-        if self.is_mass(new_item):
-            return self.as_mass(new_item)
-        raise TypeError(f"{self!r} does not recognize '{item}' as a mass.")
-
-    @deprecate(
-        "0.2.6",
-        "Use `inlet` instead. Or consider using `siq_gas_flux_calibration` "
-        "with the `spectro_inlets_quantification` package.",
-        "0.3",
-        kwarg_name="chip",
-    )
-    def gas_flux_calibration(
-        self,
-        mol,
-        mass,
-        inlet=None,
-        chip=None,
-        tspan=None,
-        tspan_bg=None,
-        ax=None,
-        carrier_mol=None,
-        mol_conc_ppm=None,
-    ):
-        """
-        Fit mol's sensitivity at mass based on one period with steady gas composition.
-
-        Args:
-            mol (str): The name of the molecule to calibrate
-            mass (str): The mass to calibrate at
-            inlet (MSInlet): An object with a `calc_n_dot_0` method for total flux to
-                the vacuum chamber containing the mass spectrometer.
-            chip (MSInlet): DEPRECATED. Old name for `inlet`.
-            tspan (iter): The timespan to average the signal over. Defaults to all
-            tspan_bg (iter): Optional timespan at which the signal is at its background.
-            ax (matplotlib axis): The axis on which to indicate what signal is used
-                with a thicker line. Defaults to none
-            carrier_mol (str): The name of the molecule of the carrier gas if
-                a dilute analyte is used. Calibration assumes total flux of the
-                capillary is the same as the flux of pure carrier gas. Defaults
-                to None.
-            mol_conc_ppm (float): Concentration of the dilute analyte in the carrier gas
-                in ppm. Defaults to None.
-
-        Returns MSCalResult: a MS calibration result containing the sensitivity factor
-            for mol at mass
-        """
-        if plugins.use_siq:
-            warnings.warn(
-                "spectro_inlets_quantification is active but you are using the native "
-                "ixdat version of `MSMeasurement.gas_flux_calibration`"
-            )
-        t, S = self.grab_signal(mass, tspan=tspan, tspan_bg=tspan_bg)
-        if ax:
-            ax.plot(t, S, color=STANDARD_COLORS[mass], linewidth=5)
-        if carrier_mol:
-            if mol_conc_ppm:
-                cal_type = "carrier_gas_flux_calibration"
-            else:
-                raise QuantificationError(
-                    "Cannot use carrier gas calibration without analyte"
-                    " concentration. mol_conc_ppm is missing."
-                )
-        elif mol_conc_ppm:
-            raise QuantificationError(
-                "Cannot use carrier gas calibration without carrier"
-                " gas definition. carrier_mol is missing."
-            )
-        else:
-            cal_type = "gas_flux_calibration"
-            mol_conc_ppm = 10**6
-            carrier_mol = mol
-        n_dot = inlet.calc_n_dot_0(gas=carrier_mol) * mol_conc_ppm / 10**6
-        F = np.mean(S) / n_dot
-        return MSCalResult(
-            name=f"{mol}@{mass}",
-            mol=mol,
-            mass=mass,
-            cal_type=cal_type,
-            F=F,
-        )
-
-    def gas_flux_calibration_curve(
-        self,
-        mol,
-        mass,
-        inlet=None,
-        chip=None,
-        tspan_list=None,
-        carrier_mol=None,
-        mol_conc_ppm=None,
-        p_inlet=None,
-        tspan_bg=None,
-        ax="new",
-        axis_measurement=None,
-        remove_bg_on_axis_measurement=True,
-        return_ax=False,
-    ):
-        """Fit mol's sensitivity at mass from 2+ periods of steady gas composition.
-
-        Args:
-            inlet (MSInlet): An object with a `calc_n_dot_0` method for total flux to
-                the vacuum chamber containing the mass spectrometer.
-            mol (str): Name of the molecule to calibrate
-            mass (str): Name of the mass at which to calibrate
-            inlet (MSInlet): An object with a `calc_n_dot_0` method for total flux to
-                the vacuum chamber containing the mass spectrometer.
-            chip (MSInlet): DEPRECATED. Old name for `inlet`.
-            tspan_list (list of tspan): The timespans of steady concentration
-                or pressure
-            carrier_mol (str): The name of the molecule of the carrier gas if
-                a dilute analyte is used. Calibration assumes total flux of the
-                capillary is the same as the flux of pure carrier gas. Defaults
-                to None.
-            mol_conc_ppm (float or list): Concentration of the dilute analyte in
-                the carrier gas in ppm. Defaults to None. Accepts float (for pressure
-                calibration) or list for concentration calibration. If list needs
-                to be same length as tspan_list or selector_list.
-            p_inlet (float, list): Pressure at the inlet (Pa). Overwrites the pressure
-                inherent to self (i.e. the MSInlet object). Accepts float (for conc.
-                calibration) or list for pressure calibration. If list, then
-                needs to be same length as tspan_list or selector_list.
-            tspan_bg (tspan): The time to use as a background
-            ax (Axis): The axis on which to plot the ms_calibration curve result.
-                Defaults to a new axis.
-            axis_measurement (Axis): The MS plot axes to highlight the
-                ms_calibration on. Defaults to None.
-            remove_bg_on_axis_measurement (bool):
-                Whether the plot on axis_measurement is showing raw data or bg
-                subtracted data. Defaults to True, i.e. plotting data with the
-                same bg subtraction as used for the calibration.
-            return_ax (bool): Whether to return the axis on which the calibration
-                curve is plotted together with the MSCalResult. Defaults to False.
-
-        Return MSCalResult(, Axis): The result of the MS calibration (and calibration
-            curve axis if requested) based on flux calculation during selected time
-            periods.
-        TODO: automatically recognize the pressure from measurement (if available)
-        """
-        if plugins.use_siq:
-            warnings.warn(
-                "spectro_inlets_quantification is active but you are using the native "
-                "ixdat version of `MSMeasurement.siq_gas_flux_calibration_curve`"
-            )
-        return self._gas_flux_calibration_curve(
-            mol=mol,
-            mass=mass,
-            inlet=inlet,
-            chip=chip,
-            tspan_list=tspan_list,
-            carrier_mol=carrier_mol,
-            mol_conc_ppm=mol_conc_ppm,
-            p_inlet=p_inlet,
-            tspan_bg=tspan_bg,
-            ax="new",
-            axis_measurement=axis_measurement,
-            remove_bg_on_axis_measurement=remove_bg_on_axis_measurement,
-            return_ax=return_ax,
-        )
-
-    @deprecate(
-        "0.2.6",
-        "Use `inlet` instead. Or consider using `siq_gas_flux_calibration_curve` "
-        "with the `spectro_inlets_quantification` package.",
-        "0.3",
-        kwarg_name="chip",
-    )
-    def _gas_flux_calibration_curve(
-        self,
-        mol,
-        mass,
-        inlet=None,
-        chip=None,
-        tspan_list=None,
-        carrier_mol=None,
-        mol_conc_ppm=None,
-        p_inlet=None,
-        tspan_bg=None,
-        ax="new",
-        axis_measurement=None,
-        remove_bg_on_axis_measurement=True,
-        return_ax=False,
-    ):
-        """Helper function. See gas_flux_calibraiton_curve for argument descriptions."""
-
-        # prepare three lists to loop over to determine molecule flux in the
-        # different periods of steady gas composition
-        if not isinstance(mol_conc_ppm, list):
-            mol_conc_ppm_list = [mol_conc_ppm for x in tspan_list]
-        else:
-            mol_conc_ppm_list = mol_conc_ppm
-        if isinstance(p_inlet, list):
-            p_list = p_inlet
-        else:
-            p_list = [p_inlet for _ in tspan_list]
-        if not len(mol_conc_ppm_list) == len(p_list) == len(tspan_list):
-            raise ValueError(
-                "Length of input lists for concentrations"
-                " and tspan or pressures and tspan is not equal"
-            )
-        S_list = []
-        n_dot_list = []
-        if carrier_mol:
-            if None not in mol_conc_ppm_list:
-                cal_type = "carrier_gas_flux_calibration_curve"
-            else:
-                raise QuantificationError(
-                    "Cannot use carrier gas calibration without analyte"
-                    " concentration. 'mol_conc_ppm' is missing. For a pure gas,"
-                    "use 'mol' instead of 'carrier_mol' and don't give a 'mol_conc_ppm'"
-                )
-        elif None not in mol_conc_ppm_list:
-            raise QuantificationError(
-                "Cannot use carrier gas calibration without carrier"
-                " gas definition. 'carrier_mol' is missing. For a pure gas,"
-                "use 'mol' instead of 'carrier_mol' and don't give a 'mol_conc_ppm'"
-            )
-        else:
-            cal_type = "gas_flux_calibration_curve"
-            # redefine mol_conc_ppm_list to compensate for unit correction done
-            # in the calculation of n_dot below
-            mol_conc_ppm = 10**6
-            mol_conc_ppm_list = [mol_conc_ppm for x in tspan_list]
-            # specify that the gas given as mol is now the carrier_mol
-            carrier_mol = mol
-        for tspan, mol_conc_ppm, pressure in zip(tspan_list, mol_conc_ppm_list, p_list):
-            t, S = self.grab_signal(mass, tspan=tspan, tspan_bg=tspan_bg)
-            if axis_measurement:
-                if remove_bg_on_axis_measurement:
-                    t_plot, S_plot = t, S
-                else:
-                    t_plot, S_plot = self.grab_signal(mass, tspan=tspan)
-                axis_measurement.plot(
-                    t_plot, S_plot, color=STANDARD_COLORS[mass], linewidth=5
-                )
-            n_dot = (
-                inlet.calc_n_dot_0(gas=carrier_mol, p=pressure) * mol_conc_ppm / 10**6
-            )
-            S_list.append(np.mean(S))
-            n_dot_list.append(n_dot)
-        n_dot_vec = np.array(n_dot_list)
-        S_vec = np.array(S_list)
-        pfit = np.polyfit(n_dot_vec, S_vec, deg=1)
-        F = pfit[0]
-        if ax:
-            color = STANDARD_COLORS[mass]
-            if ax == "new":
-                ax = self.plotter.new_ax(
-                    xlabel="molecule flux / [nmol/s]", ylabel="signal / [nA]"
-                )
-            ax.plot(n_dot_vec * 1e9, S_vec * 1e9, "o", color=color)
-            n_dot_fit = np.array([0, max(n_dot_vec)])
-            S_fit = n_dot_fit * pfit[0] + pfit[1]
-            ax.plot(n_dot_fit * 1e9, S_fit * 1e9, "--", color=color)
-        cal = MSCalResult(
-            name=f"{mol}@{mass}",
-            mol=mol,
-            mass=mass,
-            cal_type=cal_type,
-            F=F,
-        )
-        if return_ax:
-            return cal, ax
-        return cal
-
-    def siq_gas_flux_calibration(self, mol, mass, tspan, chip=None):
-        """Simple pure-gas flux calibration, using `spectro_inlets_quantification`
-
-        Args:
-            mol (str): Name of molecule to be calibrated (e.g. "He")
-            mass (str): Mass at which to calibrate it (e.g. "M4")
-            tspan (timespan): A timespan during which the pure gas is in the chip
-            chip (Chip, optional): An object defining the capillary inlet, if different
-                than the standard chip assumed by the external package.
-
-        Returns CalPoint: An object from `spectro_inlets_quantification`,
-           representing the calibration result
-        """
-        if not plugins.use_siq:
-            raise QuantificationError(
-                "`MSMeasurement.siq_gas_flux_calibration` only works when using "
-                "`spectro_inlets_quantification` "
-                "(`ixdat.options.activate_siq()`). For native ixdat MS quantification, "
-                "use `gas_flux_calibration` instead."
-            )
-        Chip = plugins.siq.Chip
-        CalPoint = plugins.siq.CalPoint
-
-        chip = chip or Chip()
-        n_dot = chip.calc_n_dot_0(gas=mol)
-        S = self.grab_signal(mass, tspan=tspan)[1].mean()
-        F = S / n_dot
-        return CalPoint(mol=mol, mass=mass, F=F, F_type="capillary", date=self.yyMdd)
-
-    def siq_gas_flux_calibration_curve(
-        self,
-        mol,
-        mass,
-        chip=None,
-        tspan_list=None,
-        carrier_mol=None,
-        mol_conc_ppm=None,
-        p_inlet=None,
-        tspan_bg=None,
-        ax="new",
-        axis_measurement=None,
-        remove_bg_on_axis_measurement=True,
-        return_ax=False,
-    ):
-        """Fit mol's sensitivity at mass from 2+ periods of steady gas composition.
-
-        Args:
-            mol (str): Name of the molecule to calibrate
-            mass (str): Name of the mass at which to calibrate
-            tspan_list (list of tspan): The timespans of steady concentration
-                or pressure
-            carrier_mol (str): The name of the molecule of the carrier gas if
-                a dilute analyte is used. Calibration assumes total flux of the
-                capillary is the same as the flux of pure carrier gas. Defaults
-                to None.
-            mol_conc_ppm (float, list): Concentration of the dilute analyte in
-                the carrier gas in ppm. Defaults to None. Accepts float (for pressure
-                calibration) or list for concentration calibration. If list needs
-                to be same length as tspan_list or selector_list.
-            p_inlet (float, list): Pressure at the inlet (Pa). Overwrites the pressure
-                inherent to self (i.e. the MSInlet object). Accepts float (for conc.
-                calibration) or list for pressure calibration. If list, then
-                needs to be same length as tspan_list or selector_list.
-            tspan_bg (tspan): The time to use as a background
-            ax (Axis): The axis on which to plot the ms_calibration curve result.
-                Defaults to a new axis.
-            axis_measurement (Axes): The MS plot axes to highlight the
-                ms_calibration on. Defaults to None. These axes are not returned.
-            remove_bg_on_axis_measurement (bool):
-                Whether the plot on axis_measurement is showing raw data or bg
-                subtracted data. Defaults to True, i.e. plotting data with the
-                same bg subtraction as used for the calibration.
-            return_ax (bool): Whether to return the axis on which the calibration
-                curve is plotted together with the MSCalResult. Defaults to False.
-
-        Returns CalPoint: An object from `spectro_inlets_quantification`,
-           representing the calibration result
-
-        TODO: automatically recognize the pressure from measurement (if available)
-        """
-        if not plugins.use_siq:
-            raise QuantificationError(
-                "`MSMeasurement.siq_gas_flux_calibration` only works when using "
-                "`spectro_inlets_quantification`"
-                "(`ixdat.options.activate_siq()`). "
-                "For native ixdat MS quantification, use `gas_flux_calibration`"
-                "instead."
-            )
-        Chip = plugins.siq.Chip
-
-        chip = chip or Chip()
-
-        cal, ax = self._gas_flux_calibration_curve(
-            inlet=chip,
-            mol=mol,
-            mass=mass,
-            tspan_list=tspan_list,
-            carrier_mol=carrier_mol,
-            mol_conc_ppm=mol_conc_ppm,
-            p_inlet=p_inlet,
-            tspan_bg=tspan_bg,
-            ax=ax,
-            axis_measurement=axis_measurement,
-            remove_bg_on_axis_measurement=remove_bg_on_axis_measurement,
-            return_ax=True,
-        )
-
-        cal = cal.to_siq()
-        if return_ax:
-            return cal, ax
-        return cal
-
-    def siq_multicomp_gas_flux_calibration(
-        self, mol_list, mass_list, gas, tspan, gas_bg=None, tspan_bg=None, chip=None
-    ):
-        """Calibration of multiple components of a calibration gas simultaneously
-
-        Uses a matrix equation and the reference spectra in the molecule data files.
-
-        The results are only as accurate as the reference spectrum used. For this reason,
-        this method is a last resort and it is recommended *not* to use a multicomponent
-        calibration gas. Instead, get a separate calibration gas for each molecule to
-        be calibrated.
-
-        Here is an explanation of the math used in this method:
-
-        The fundamental matrix equation is:
-          S_vec = F_mat @ n_dot_vec
-        Elementwise, this is:
-         S_M = sum_i ( F^i_M * n_dot^i )
-        Rewrite to show that sensitivity factors follow each molecule's spectrum:
-         S_M = sum_i (F_weight_i * spectrum^i_M * n_dot^i)
-        And regroup the parts that only depend on the molecule (^i):
-         S_M = sum_i (spectrum^i_M * (F_weight^i * n_dot^i))
-         S_M = sum_i (spectrum^i_M * sensitivity_flux^i)
-        Change back into a matrix equation, and solve it:
-         S_vec = spectrum_mat @ sensitivity_flux_vec
-         sensitivity_flux_vec = spectrum_mat^-1 @ S_vec   # eq. 1
-        Ungroup the part we grouped before (the "sensitivity_flux"):
-         F_weight^i = sensitivity_flux^i / n_dot^i        # eq. 2
-        And, in the end, each sensitivity factor is:
-         F_M^i = F_weight^i * spectrum^i_M                # eq. 3
-
-        Equations 1, 2, and 3 are implemented in the code of this method.
-
-        Args:
-            mol_list (list of str): List of the names of the molecules to calibrate
-            mass_list (list of str): List of the masses to calibrate
-            gas (Gas, dict, or str): Composition of the calibration gas, e.g.
-               {"Ar": 0.95, "H2": 0.05} for 5% H2 in Ar
-            tspan (Timespan): Timespan during which the calibration gas is in the chip
-            gas_bg (Gas, dict, or str): Composition of the background gas
-            tspan_bg (Timespan): Timespan during which the background gas is in the chip
-            chip (Chip, optional): object describing the MS capillary, if different than
-               the standard chip in the MS quantification package
-
-        Returns Calibration: An object from `spectro_inlets_quantification`,
-           representing all the calibration results from the calibration.
-        """
-        if not plugins.use_siq:
-            raise QuantificationError(
-                "`MSMeasurement.siq_multicomp_gas_flux_calibration` "
-                "only works when using `spectro_inlets_quantification` "
-                "(`ixdat.plugins.activate_siq()`). "
-            )
-        Chip = plugins.siq.Chip
-        CalPoint = plugins.siq.CalPoint
-        Calibration = plugins.siq.Calibration
-
-        chip = chip or Chip()
-        chip.gas = gas
-        flux = chip.calc_n_dot()
-
-        chip_bg = chip or Chip()
-        chip_bg.gas = gas_bg
-        flux_bg = chip_bg.calc_n_dot()
-
-        delta_flux_list = []
-        for mol in mol_list:
-            delta_flux = flux.get(mol, 0) - flux_bg.get(mol, 0)
-            delta_flux_list.append(delta_flux)
-        delta_flux_vec = np.array(delta_flux_list)
-
-        delta_signal_list = []
-        for mass in mass_list:
-            S = self.grab_signal(mass, tspan=tspan)[1].mean()
-            S_bg = self.grab_signal(mass, tspan=tspan_bg)[1].mean()
-            delta_S = S - S_bg
-            delta_signal_list.append(delta_S)
-        delta_signal_vec = np.array(delta_signal_list)
-
-        spectrum_vec_list = []
-        for mol in mol_list:
-            spectrum = chip.gas.mdict[mol].norm_spectrum
-            spectrum_vec = np.array([spectrum.get(mass, 0) for mass in mass_list])
-            spectrum_vec_list.append(spectrum_vec)
-        spectrum_mat = np.stack(spectrum_vec_list).transpose()
-
-        inverse_spectrum_mat = np.linalg.inv(spectrum_mat)
-        sensitivity_flux_vec = inverse_spectrum_mat @ delta_signal_vec  # eq. 1
-        F_weight_vec = sensitivity_flux_vec / delta_flux_vec  # eq. 2
-
-        cal_list = []
-        for i, mol in enumerate(mol_list):
-            for M, mass in enumerate(mass_list):
-                F = F_weight_vec[i] * spectrum_mat[M, i]  # eq. 3
-                if F:
-                    cal = CalPoint(
-                        mol=mol, mass=mass, F=F, F_type="capillary", date=self.yyMdd
-                    )
-                    cal_list.append(cal)
-
-        return Calibration(cal_list=cal_list)
-
-    def set_siq_quantifier(
-        self,
-        quantifier=None,
-        calibration=None,
-        mol_list=None,
-        mass_list=None,
-        carrier="He",
-    ):
-        """Set the `spectro_inlets_quantification` quantifier.
-
-        The Quantifier is an object with the method `calc_n_dot`, which takes a
-        dictionary of signals or signal vectors in [A] and return a dictionary of
-        molecular fluxes in [mol/s].
-        The quantifier typically does this by solving the linear equations of
-        S_M = sum_i ( F_M^i * n_dot^i )
-        Where n_dot^i is the flux to the vacuum chamber of molecule i in [mol/s], S_M
-        is the signal at mass M in [A], and F_M^i is the *sensitivity factor* of molecule
-        i at mass M.
-        The quantifier thus needs access to a set of sensitivity factors.
-
-        The quantifier can be built in this method (avoiding explicit import of the
-        `spectro_inlets_quantification` package) by providing the sensitivity factors
-        in the form of a `Calibration` (which can be obtained from e.g.
-        MSMeasurement.multicomp_gas_flux_cal) and the specification of which ones to
-        use by `mol_list` and `mass_list`.
-        The quantifier will always use all the masses in `mass_list` to solve for the
-        flux of all the mols in `mol_list`.
-
-        The argument `carrier` is required by some quantifiers but only used if
-        partial pressures before the MS inlet are required (`quantifier.calc_pp`)
-
-        Quantification is only as accurate as your sensitivity factors!
-
-        Args:
-            quantifier (Quantifier): The quantifier, if prepared before method call.
-               No additional arguments needed. Otherwise, the following three are needed:
-            calibration (Calibration): The calibration to build the quantifier with
-            mol_list (list of str): The list of molecules to use in flux calculations.
-               These should all be represented in the Calibration. If not provided,
-               we'll use all the mols in the Calibration.
-            mass_list (list of str): The list of masses to use in flux calculations.
-               These should all be represented in the Calibration. If not provided,
-               we'll use all the masses in the Calibration.
-            carrier (optional, str): The carrier gas in the experiment. Defaults to "He".
-        """
-        if not plugins.use_siq:
-            raise QuantificationError(
-                "`MSMeasurement.set_siq_quantifier` only works when using "
-                "`spectro_inlets_quantification` "
-                "(`ixdat.options.activate_siq()`). "
-                "For native ixdat MS quantification, use `MSMeasurement.calibrate`"
-            )
-        Quantifier = plugins.siq.Quantifier
-
-        if quantifier:
-            self._siq_quantifier = quantifier
-        else:
-            mol_list = mol_list or calibration.mol_list
-            mass_list = mass_list or calibration.mass_list
-            self._siq_quantifier = Quantifier(
-                calibration=calibration,
-                mol_list=mol_list,
-                mass_list=mass_list,
-                carrier=carrier,
-            )
-
-    @property
-    def siq_quantifier(self):
-        return self._siq_quantifier
-
-    cut = _with_siq_quantifier(Measurement.cut)
-    multicut = _with_siq_quantifier(Measurement.multicut)
-
-
-class MSCalResult(Saveable):
-    """A class for a mass spec ms_calibration result.
-
-    FIXME: I think that something inheriting directly from Saveable does not belong in
-        a technique module.
-    """
-
-    table_name = "ms_cal_results"
-    column_attrs = {"name", "mol", "mass", "cal_type", "F"}
-
-    def __init__(
-        self,
-        name=None,
-        mol=None,
-        mass=None,
-        cal_type=None,
-        F=None,
-    ):
-        super().__init__()
-        self.name = name or f"{mol}@{mass}"
-        self.mol = mol
-        self.mass = mass
-        self.cal_type = cal_type
-        self.F = F
-
-    def __repr__(self):
-        return (
-            f"{self.__class__.__name__}(name={self.name}, mol={self.mol}, "
-            f"mass={self.mass}, F={self.F})"
-        )
-
-    @property
-    def color(self):
-        return STANDARD_COLORS[self.mass]
-
-    @classmethod
-    def from_siq(cls, siq_cal_point):
-        return cls(
-            name=siq_cal_point.mol + "@" + siq_cal_point.mass,
-            mol=siq_cal_point.mol,
-            mass=siq_cal_point.mass,
-            cal_type=siq_cal_point.F_type,
-            F=siq_cal_point.F,
-        )
-
-    def to_siq(self):
-        if not plugins.use_siq:
-            raise QuantificationError(
-                "`MSCalPoint.to_siq` only works when using "
-                "`spectro_inlets_quantification` "
-                "(`ixdat.options.activate_siq()`). "
-                "For native ixdat MS quantification, use `MSMeasurement.calibrate`"
-            )
-        return plugins.siq.CalPoint(
-            mol=self.mol,
-            mass=self.mass,
-            F=self.F,
-            F_type=self.cal_type,
-        )
-
-
-class MSCalibration(Calibration):
-    """Class for mass spec calibrations. TODO: replace with powerful external package"""
-
-    extra_linkers = {"ms_calibration_results": ("ms_cal_results", "ms_cal_result_ids")}
-    # FIXME: signal_bgs are not saved at present. Should they be a separate table
-    #   of Saveable objects like ms_cal_results or should they be a single json value?
-    child_attrs = [
-        "ms_cal_results",
-    ]
-
-    def __init__(
-        self,
-        name=None,
-        date=None,
-        tstamp=None,  # FIXME: No need to have both a date and a tstamp?
-        setup=None,
-        ms_cal_results=None,
-        signal_bgs=None,
-        technique="MS",
-        measurement=None,
-    ):
-        """
-        Args:
-            name (str): Name of the ms_calibration
-            date (str): Date of the ms_calibration
-            setup (str): Name of the setup where the ms_calibration is made
-            ms_cal_results (list of MSCalResult): The mass spec calibrations
-            measurement (MSMeasurement): The measurement
-        """
-        super().__init__(
-            name=name or f"EC-MS ms_calibration for {setup} on {date}",
-            technique=technique,
-            tstamp=tstamp,
-            measurement=measurement,
-        )
-        self.date = date
-        self.setup = setup
-        self.ms_cal_results = ms_cal_results or []
-        self.signal_bgs = signal_bgs or {}
-
-    @property
-    def ms_cal_result_ids(self):
-        return [cal.id for cal in self.ms_cal_results]
-
-    @property
-    def mol_list(self):
-        return list({cal.mol for cal in self.ms_cal_results})
-
-    @property
-    def mass_list(self):
-        return list({cal.mass for cal in self.ms_cal_results})
-
-    @property
-    def name_list(self):
-        return list({cal.name for cal in self.ms_cal_results})
-
-    def __contains__(self, mol):
-        return mol in self.mol_list or mol in self.name_list
-
-    def __iter__(self):
-        yield from self.ms_cal_results
-
-    def calibrate_series(self, key, measurement=None):
-        """Return a calibrated series for `key` if possible.
-
-        If key starts with "n_", it is interpreted as a molecule flux. This method then
-        searches the calibration for a sensitivity factor for that molecule uses it to
-        divide the relevant mass signal from the measurement. Example acceptable keys:
-        "n_H2", "n_dot_H2".
-        If the key does not start with "n_", or the calibration can't find a relevant
-        sensitivity factor and mass signal, this method returns None.
-        """
-        measurement = measurement or self.measurement
-        if key.startswith("n_"):  # it's a flux!
-            mol = key.split("_")[-1]
-            try:
-                mass, F = self.get_mass_and_F(mol)
-            except QuantificationError:
-                # Calibrations just return None when they can't get what's requested.
-                return
-            signal_series = measurement[mass]
-            y = signal_series.data
-            if mass in measurement.signal_bgs:
-                # FIXME: How to make this optional to user of MSMeasuremt.grab()?
-                y = y - measurement.signal_bgs[mass]
-            n_dot = y / F
-            return ValueSeries(
-                name=f"n_dot_{mol}",
-                unit_name="mol/s",
-                data=n_dot,
-                tseries=signal_series.tseries,
-            )
-
-    def get_mass_and_F(self, mol):
-        """Return the mass and sensitivity factor to use for simple quant. of mol"""
-        cal_list_for_mol = [cal for cal in self if cal.mol == mol or cal.name == mol]
-        Fs = [cal.F for cal in cal_list_for_mol]
-        if not Fs:
-            raise QuantificationError(f"{self!r} has no sensitivity factor for {mol}")
-        index = np.argmax(np.array(Fs))
-
-        the_good_cal = cal_list_for_mol[index]
-        return the_good_cal.mass, the_good_cal.F
-
-    def get_F(self, mol, mass):
-        """Return the sensitivity factor for mol at mass"""
-        cal_list_for_mol_at_mass = [
-            cal
-            for cal in self
-            if (cal.mol == mol or cal.name == mol) and cal.mass == mass
-        ]
-        F_list = [cal.F for cal in cal_list_for_mol_at_mass]
-        if not F_list:
-            raise QuantificationError(
-                f"{self!r} has no sensitivity factor for {mol} at {mass}"
-            )
-        return np.mean(np.array(F_list))
-
-    def scaled_to(self, ms_cal_result):
-        """Return a new ms_calibration w scaled sensitivity factors to match one given"""
-        F_0 = self.get_F(ms_cal_result.mol, ms_cal_result.mass)
-        scale_factor = ms_cal_result.F / F_0
-        calibration_as_dict = self.as_dict()
-        new_cal_list = []
-        for cal in self.ms_cal_results:
-            cal = MSCalResult(
-                name=cal.name,
-                mass=cal.mass,
-                mol=cal.mol,
-                F=cal.F * scale_factor,
-                cal_type=cal.cal_type + " scaled",
-            )
-            new_cal_list.append(cal)
-        calibration_as_dict["ms_cal_results"] = new_cal_list
-        del calibration_as_dict["ms_cal_result_ids"]
-        # ^ FIXME: ms_cal_result_ids via MemoryBackend
-        calibration_as_dict["name"] = calibration_as_dict["name"] + " scaled"
-        return self.__class__.from_dict(calibration_as_dict)
-
-    @classmethod
-    def read(cls, path_to_file):
-        """Read an MSCalibration from a json-formatted text file"""
-        with open(path_to_file) as f:
-            obj_as_dict = json.load(f)
-        # put the MSCalResults (exported as dicts) into objects:
-        obj_as_dict["ms_cal_results"] = [
-            MSCalResult.from_dict(ms_cal_as_dict)
-            for ms_cal_as_dict in obj_as_dict["ms_cal_results"]
-        ]
-        return cls.from_dict(obj_as_dict)
-
-    def export(self, path_to_file=None):
-        """Export an ECMSCalibration as a json-formatted text file"""
-        path_to_file = path_to_file or (self.name + ".ix")
-        self_as_dict = self.as_dict()
-        # replace the ms_cal_result ids with the dictionaries of the results themselves:
-        del self_as_dict["ms_cal_result_ids"]
-        self_as_dict["ms_cal_results"] = [cal.as_dict() for cal in self.ms_cal_results]
-        with open(path_to_file, "w") as f:
-            json.dump(self_as_dict, f, indent=4)
-
-    @classmethod
-    def from_siq(cls, siq_calibration):
-
-        # A complication is that it can be either a Calibration or a SensitivityList.
-        # Either way, the sensitivity factors are in `sf_list`:
-        ms_cal_results = [MSCalResult.from_siq(cal) for cal in siq_calibration.sf_list]
-        # if it's a Calibration, we want the metadata:
-        try:
-            calibration = cls(
-                name=siq_calibration.name,
-                date=siq_calibration.date,
-                setup=siq_calibration.setup,
-                ms_cal_results=ms_cal_results,
-            )
-        # if not, we just want the data:
-        except AttributeError:
-            calibration = cls(ms_cal_results=ms_cal_results)
-        return calibration
-
-    def to_siq(self):
-        if not plugins.use_siq:
-            raise QuantificationError(
-                "`MSCalPoint.to_siq` only works when using "
-                "`spectro_inlets_quantification` "
-                "(`ixdat.options.activate_siq()`). "
-                "For native ixdat MS quantification, use `MSMeasurement.calibrate`"
-            )
-        cal_list = [cal.to_siq() for cal in self.ms_cal_results]
-        return plugins.siq.Calibration(
-            name=self.name,
-            date=self.date,
-            setup=self.setup,
-            cal_list=cal_list,
-        )
-
-
-class MSInlet:
-    """A class for describing the inlet to the mass spec
-
-    Every MSInlet describes the rate and composition of the gas entering a mass
-    spectrometer. The default is a Spectro Inlets EC-MS chip.
-    """
-
-    def __init__(
-        self,
-        *,
-        l_cap=1e-3,
-        w_cap=6e-6,
-        h_cap=6e-6,
-        gas="He",
-        T=STANDARD_TEMPERATURE,
-        p=STANDARD_PRESSURE,
-        verbose=True,
-    ):
-        """Create an MSInlet object given its properties.
-
-        Args:
-            l_cap (float): capillary length [m]. Defaults to design parameter.
-            w_cap (float): capillary width [m]. Defaults to design parameter.
-            h_cap (float): capillary height [m]. Defaults to design parameter.
-            p (float): system pressure in [Pa] (if to change from that in medium)
-            T (float): system temperature in [K] (if to change from that in medium)
-            gas (str): the gas at the start of the inlet.
-            verbose (bool): whether to print stuff to the terminal
-        """
-        self.verbose = verbose
-        self.l_cap = l_cap
-        self.l_cap_eff = {}
-        self.w_cap = w_cap
-        self.h_cap = h_cap
-        self.p = p
-        self.T = T
-        self.gas = gas  # TODO: Gas mixture class. This must be a pure gas now.
-
-    def calc_l_cap_eff(
-        self, n_dot_measured, gas=None, w_cap=None, h_cap=None, T=None, p=None
-    ):
-        """Calculate gas specific effective length of the capillary in [m]
-        and add {gas:value} to l_cap_eff (dict)
-
-        Args:
-            w_cap (float): Capillary width [m], defaults to self.w_cap
-            h_cap (float): Capillary height [m], defaults to self.h_cap
-            n_dot_measured (float): Measured flux of gas [mol/s]
-            gas (dict or str): The gas in the chip, defaults to self.gas
-            T (float): Temperature [K], if to be updated
-            p (float): Pressure [Pa], if to be updated
-        Returns:
-            float: Gas specific effective length in [m]
-        """
-
-        n_dot_predicted = self.calc_n_dot_0(gas=gas, w_cap=w_cap, h_cap=h_cap, T=T, p=p)
-
-        l_cap_gas_specific_eff = self.l_cap * n_dot_predicted / n_dot_measured
-        self.l_cap_eff[
-            gas
-        ] = l_cap_gas_specific_eff  # add effective l_cap for specific gas
-
-        return l_cap_gas_specific_eff
-
-    def update_l_cap(self, gases=None):
-        """Update self.l_cap from average of values in dict l_cap_eff
-
-        Args:
-            gases (list): List of gases to average l_cap, default all
-        Returns:
-            float: Averaged effective capilllary length in [m]
-        """
-        gases = gases or []
-        if self.l_cap_eff and not gases:
-            self.l_cap = np.mean(list(self.l_cap_eff.values()))
-        elif self.l_cap_eff and gases:
-            _l_cap = 0
-            for gas in gases:
-                _l_cap += self.l_cap_eff[gas]
-            self.l_cap = _l_cap / len(gases)
-        return self.l_cap
-
-    def calc_n_dot_0(self, gas=None, w_cap=None, h_cap=None, l_cap=None, T=None, p=None):
-        """Calculate the total molecular flux through the capillary in [s^-1]
-
-        Uses Equation 4.10 of Trimarco, 2017. "Real-time detection of sub-monolayer
-        desorption phenomena during electrochemical reactions: Instrument development
-        and applications." PhD Thesis, Technical University of Denmark.
-
-        Args:
-            w_cap (float): Capillary width [m], defaults to self.w_cap
-            h_cap (float): Capillary height [m], defaults to self.h_cap
-            l_cap (float): Capillary length [m], defaults to self.l_cap
-            gas (dict or str): The gas in the chip, defaults to self.gas
-            T (float): Temperature [K], if to be updated
-            p (float): Pressure [Pa], if to be updated
-        Returns:
-            float: The total molecular flux in [s^-1] through the capillary
-        """
-
-        if w_cap is None:
-            w_cap = self.w_cap  # capillary width in [m]
-        if h_cap is None:
-            h_cap = self.h_cap  # capillary height in [m]
-        if l_cap is None:
-            l_cap = self.l_cap  # effective capillary length in [m]
-        if T is None:
-            T = self.T
-        if p is None:
-            p = self.p
-        pi = np.pi
-
-        # TODO: make it so that DYNAMIC_VISCOSITIES[gas] can just be a float if someone
-        #   enters it without having access to the temperature-dependent values.
-        if T < DYNAMIC_VISCOSITIES[gas][0, 0] or T > DYNAMIC_VISCOSITIES[gas][-1, 0]:
-            warnings.warn(
-                "Insufficient data in constants.py to appropriately estimate "
-                f"the dynamic viscosity for {gas} at temperature: {T}K",
-                stacklevel=2,
-            )
-        _eta_v = DYNAMIC_VISCOSITIES[gas][:, 1]  # list of known eta(T) for 'gas'
-        _eta_T = DYNAMIC_VISCOSITIES[gas][:, 0]  # list of paired Ts for eta(T)
-
-        eta = np.interp(T, _eta_T, _eta_v)  # dynamic viscosity of gas at T in [Pa*s]
-
-        s = MOLECULAR_DIAMETERS[gas]  # molecule diameter in [m]
-        m = MOLAR_MASSES[gas] * 1e-3 / AVOGADRO_CONSTANT  # molecule mass in [kg]
-
-        d = ((w_cap * h_cap) / pi) ** 0.5 * 2
-        # d = 4.4e-6  #used in Henriksen2009
-        a = d / 2
-        p_1 = p
-        lambda_ = d  # defining the transitional pressure
-        # ...from setting mean free path equal to capillary d
-        p_t = BOLTZMANN_CONSTANT * T / (2**0.5 * pi * s**2 * lambda_)
-        p_2 = 0
-        p_m = (p_1 + p_t) / 2  # average pressure in the transitional flow region
-        v_m = (8 * BOLTZMANN_CONSTANT * T / (pi * m)) ** 0.5
-        # a reciprocal velocity used for short-hand:
-        nu = (m / (BOLTZMANN_CONSTANT * T)) ** 0.5
-
-        # ... and now, we're ready for the capillary equation.
-        #   (need to turn of black and flake8 for tolerable format)
-        # fmt: off
-        #   Equation 4.10 of Daniel Trimarco's PhD Thesis:
-        N_dot = (                                                               # noqa
-            1 / (BOLTZMANN_CONSTANT * T) * 1 / l_cap * (                        # noqa
-                (p_t - p_2) * a**3 * 2 * pi / 3 * v_m + (p_1 - p_t) * (         # noqa
-                    a**4 * pi / (8 * eta) * p_m  + a**3 * 2 * pi / 3 * v_m * (  # noqa
-                        (1 + 2 * a * nu * p_m / eta) / (                        # noqa
-                        1 + 2.48 * a * nu * p_m / eta                           # noqa
-                        )                                                       # noqa
-                    )                                                           # noqa
-                )                                                               # noqa
-            )                                                                   # noqa
-        )                                                                       # noqa
-        # fmt: on
-        n_dot = N_dot / AVOGADRO_CONSTANT
-        return n_dot
-
-    @deprecate(
-        last_supported_release="0.2.5",
-        update_message=("`gas_flux_calibration` is now a method of `MSMeasurement`"),
-        hard_deprecation_release="0.3.0",
-        remove_release="1.0.0",
-    )
-    def gas_flux_calibration(
-        self,
-        measurement,
-        mol,
-        mass,
-        tspan=None,
-        tspan_bg=None,
-        ax=None,
-        carrier_mol=None,
-        mol_conc_ppm=None,
-    ):
-        return measurement.gas_flux_calibration(
-            mol=mol,
-            inlet=self,
-            mass=mass,
-            tspan=tspan,
-            tspan_bg=tspan_bg,
-            ax=ax,
-            carrier_mol=carrier_mol,
-            mol_conc_ppm=mol_conc_ppm,
-        )
-
-    @deprecate(
-        last_supported_release="0.2.5",
-        update_message=(
-            "`gas_flux_calibration_curve` is now a method of `MSMeasurement`"
-        ),
-        hard_deprecation_release="0.3.0",
-        remove_release="1.0.0",
-    )
-    def gas_flux_calibration_curve(
-        self,
-        measurement,
-        mol,
-        mass,
-        tspan_list=None,
-        carrier_mol=None,
-        mol_conc_ppm=None,
-        p_inlet=None,
-        tspan_bg=None,
-        ax="new",
-        axis_measurement=None,
-        return_ax=False,
-    ):
-        return measurement.gas_flux_calibration_curve(
-            mol=mol,
-            inlet=self,
-            mass=mass,
-            tspan_list=tspan_list,
-            tspan_bg=tspan_bg,
-            ax=ax,
-            carrier_mol=carrier_mol,
-            mol_conc_ppm=mol_conc_ppm,
-            p_inlet=p_inlet,
-            axis_measurement=axis_measurement,
-            return_ax=return_ax,
-        )
-
-
-class MSSpectrum(Spectrum):
-    """Nothing to add to normal Spectrum yet.
-    TODO: Methods for co-plotting ref spectra from a database
-    """
-
-    pass
-
-
-class MSSpectrumSeries(SpectrumSeries):
-    """Nothing to add to normal SpectrumSeries yet."""
-
-    pass
-
-
-class MSSpectroMeasurement(MSMeasurement, SpectroMeasurement):
-
-    # FIXME: automate this in inheritance of hyphenated techniques:
-    extra_column_attrs = {
-        "spectro_measurements": {"spectrum_id"},
-        "ms_measurement": {"tspan_bg"},
-    }
-    default_plotter = MSSpectroPlotter
-    default_exporter = MSSpectroExporter
-
-    # FIXME: this shouldn't be necessary. See #164.
-    cut = _with_siq_quantifier(SpectroMeasurement.cut)
-    multicut = _with_siq_quantifier(SpectroMeasurement.multicut)
+"""Module for representation and analysis of MS measurements"""
+
+import re
+import numpy as np
+import json  # FIXME: This is for MSCalibration.export, but shouldn't have to be here.
+import warnings
+
+from ..measurements import Measurement, Calibration
+from ..spectra import Spectrum, SpectrumSeries, SpectroMeasurement
+from ..plotters.ms_plotter import MSPlotter, MSSpectroPlotter, STANDARD_COLORS
+from ..exporters.ms_exporter import MSExporter, MSSpectroExporter
+from ..exceptions import QuantificationError
+from ..constants import (
+    AVOGADRO_CONSTANT,
+    BOLTZMANN_CONSTANT,
+    STANDARD_TEMPERATURE,
+    STANDARD_PRESSURE,
+    DYNAMIC_VISCOSITIES,
+    MOLECULAR_DIAMETERS,
+    MOLAR_MASSES,
+)
+from ..data_series import ValueSeries
+from ..db import Saveable
+from ..tools import deprecate
+from ..config import plugins
+
+
+def _with_siq_quantifier(method):
+    """Decorate a Measurement-copying method to copy the quantifier to the new one.
+
+    This means that the quantifier doesn't need to be re-applied when getting a
+    subset of the measurement.
+
+    Method is a method of a Measurement which returns a Measurement of the same type,
+    such as cut() and multicut() and the methods that use these ones, such as select,
+    and indexing for CyclicVoltammogram objects.
+    """
+
+    def method_with_siq_quantifier(*args, **kwargs):
+        new_measurement = method(*args, **kwargs)
+        old_measurement = args[0]
+        if old_measurement.siq_quantifier:
+            new_measurement.set_siq_quantifier(old_measurement.siq_quantifier)
+        return new_measurement
+
+    return method_with_siq_quantifier
+
+
+class MSMeasurement(Measurement):
+    """Class implementing raw MS functionality"""
+
+    # FIXME: tspan_bg should be column of a DataTreater, not MSMeasurement:
+    extra_column_attrs = {"ms_measurement": ("tspan_bg",)}
+    default_plotter = MSPlotter
+    default_exporter = MSExporter
+
+    def __init__(self, name, **kwargs):
+        tspan_bg = kwargs.pop("tspan_bg", None)
+        super().__init__(name, **kwargs)
+        self.tspan_bg = tspan_bg
+        self._siq_quantifier = None  # Used with external quantification package
+
+    @property
+    def ms_calibration(self):
+        ms_cal_list = []
+        tspan_bg = None
+        signal_bgs = {}
+        for cal in self.calibration_list:
+            ms_cal_list = ms_cal_list + getattr(cal, "ms_cal_list", [])
+            for mass, bg in getattr(cal, "signal_bgs", {}).items():
+                if mass not in signal_bgs:
+                    signal_bgs[mass] = bg
+            tspan_bg = tspan_bg or getattr(cal, "tspan_bg", None)
+        return MSCalibration(ms_cal_results=ms_cal_list, signal_bgs=signal_bgs)
+
+    @property
+    def signal_bgs(self):
+        return self.ms_calibration.signal_bgs
+
+    def set_bg(self, tspan_bg=None, mass_list=None):
+        """Set background values for mass_list to the average signal during tspan_bg."""
+        mass_list = mass_list or self.mass_list
+        tspan_bg = tspan_bg or self.tspan_bg
+        signal_bgs = {}
+        for mass in mass_list:
+            t, v = self.grab(mass, tspan_bg)
+            signal_bgs[mass] = np.mean(v)
+        self.add_calibration(MSCalibration(signal_bgs=signal_bgs))
+
+    def reset_bg(self, mass_list=None):
+        """Reset background values for the masses in mass_list"""
+        mass_list = mass_list or self.mass_list
+        new_signal_bgs = {}
+        for mass in mass_list:
+            if mass in self.signal_bgs:
+                new_signal_bgs[mass] = 0
+        self.add_calibration(MSCalibration(signal_bgs=new_signal_bgs))
+
+    def grab(
+        self,
+        item,
+        tspan=None,
+        tspan_bg=None,
+        include_endpoints=False,
+        remove_background=False,
+    ):
+        """Returns t, S where S is raw signal in [A] for a given signal name (ie mass)
+
+        Args:
+            item (str): Name of the signal. If `item` has the form f"n_dot_{mol}", then
+                grab_flux(mol) is returned.
+            tspan (list): Timespan for which the signal is returned.
+            tspan_bg (list): Timespan that corresponds to the background signal.
+                If not given, no background is subtracted.
+            remove_background (bool): Whether to remove a pre-set background if
+                available. This is special to MSMeasurement.
+                Defaults to False, but in grab_flux it defaults to True.
+            include_endpoints (bool): Whether to ensure tspan[0] and tspan[-1] are in t
+        """
+        if plugins.use_siq and item.startswith("n_dot_"):
+            return self.grab_flux(
+                item.removeprefix("n_dot_"),
+                tspan=tspan,
+                tspan_bg=tspan_bg,
+                include_endpoints=include_endpoints,
+            )
+        time, value = super().grab(
+            item, tspan=tspan, include_endpoints=include_endpoints
+        )
+        if tspan_bg:
+            _, bg = self.grab(item, tspan=tspan_bg)
+            return time, value - np.average(bg)
+        elif remove_background:
+            if item in self.signal_bgs:
+                return time, value - self.signal_bgs[item]
+            elif self.tspan_bg:
+                _, bg = self.grab(item, tspan=self.tspan_bg)
+                return time, value - np.average(bg)
+        return time, value
+
+    def grab_for_t(self, item, t, tspan_bg=None, remove_background=False):
+        """Return a numpy array with the value of item interpolated to time t
+
+        Args:
+            item (str): The name of the value to grab
+            t (np array): The time vector to grab the value for
+            tspan_bg (iterable): Optional. A timespan defining when `item` is at its
+                baseline level. The average value of `item` in this interval will be
+                subtracted from what is returned.
+            remove_background (bool): Whether to remove a pre-set background if
+                available. This is special to MSMeasurement.
+                Defaults to False, but in grab_flux it defaults to True.
+        """
+        t_0, v_0 = self.grab(
+            item, tspan_bg=tspan_bg, remove_background=remove_background
+        )
+        v = np.interp(t, t_0, v_0)
+        return v
+
+    def grab_signal(self, *args, **kwargs):
+        """Alias for grab()"""
+        return self.grab(*args, **kwargs)
+
+    @deprecate(
+        "0.1", "Use `remove_background` instead.", "0.3", kwarg_name="removebackground"
+    )
+    def grab_flux(
+        self,
+        mol,
+        tspan=None,
+        tspan_bg=None,
+        remove_background=True,
+        removebackground=None,
+        include_endpoints=False,
+    ):
+        """Return the flux of mol (calibrated signal) in [mol/s]
+
+        Note:
+        - With native ixdat quantification (use_siq=False),
+          `grab_flux(mol, ...)` is identical to `grab(f"n_dot_{mol}", ...)` with
+          remove_background=True by default. An MSCalibration does the maths.
+        - With an external quantification package (use_siq=True), the maths are done
+          here with the help of self.quantifier
+
+        Args:
+            mol (str or MSCalResult): Name of the molecule or a ms_calibration thereof
+            tspan (list): Timespan for which the signal is returned.
+            tspan_bg (list): Timespan that corresponds to the background signal.
+                If not given, no background is subtracted.
+            remove_background (bool): Whether to remove a pre-set background if available
+                Defaults to True.
+            removebackground (bool): DEPRECATED. Use `remove_background`.
+            include_endpoints (bool): Whether to interpolate for tspan[0] and tspan[-1]
+        """
+        if removebackground is not None:
+            remove_background = removebackground
+
+        if plugins.use_siq:
+            # We have to calculate the fluxes of all the mols and masses in the
+            # quantifier's sensitivity matrix. But this method only returns one.
+            # TODO: The results should therefore be cached. But how to know when they
+            #   need to be recalculated?
+            t, n_dots = self.grab_siq_fluxes(
+                tspan=tspan,
+                tspan_bg=tspan_bg,
+                remove_background=remove_background,
+                include_endpoints=include_endpoints,
+            )
+            return t, n_dots[mol]
+
+        if isinstance(mol, MSCalResult):
+            t, signal = self.grab(
+                mol.mass,
+                tspan=tspan,
+                tspan_bg=tspan_bg,
+                remove_background=remove_background,
+                include_endpoints=include_endpoints,
+            )
+            return t, signal / mol.F
+        return self.grab(
+            # grab() invokes __getitem__, which invokes the `Calibration`. Specifically,
+            # `MSCalibration.calibrate_series()` interprets item names starting with
+            # "n_" as molecule fluxes, and checks itself for a sensitivity factor.
+            f"n_dot_{mol}",
+            tspan=tspan,
+            tspan_bg=tspan_bg,
+            remove_background=remove_background,
+            include_endpoints=include_endpoints,
+        )
+
+    def grab_siq_fluxes(
+        self, tspan=None, tspan_bg=None, remove_background=False, include_endpoints=False
+    ):
+        """Return a time vector and a dictionary with all the quantified fluxes
+
+        Args:
+            tspan (list): Timespan for which the signal is returned.
+            tspan_bg (list): Timespan that corresponds to the background signal.
+                If not given, no background is subtracted.
+            remove_background (bool): Whether to remove a pre-set background if available
+                Defaults to True..
+            include_endpoints (bool): Whether to interpolate for tspan[0] and tspan[-1]
+        """
+        if not plugins.use_siq:
+            raise QuantificationError(
+                "`MSMeasurement.grab_siq_fluxes` only works when using "
+                "`spectro_inlets_quantification` "
+                "(`ixdat.plugins.activate_siq()`). "
+            )
+        sm = self._siq_quantifier.sm
+        signals = {}
+        t = None
+        for mass in sm.mass_list:
+            if t is None:
+                t, S = self.grab(
+                    mass,
+                    tspan=tspan,
+                    tspan_bg=tspan_bg,
+                    remove_background=remove_background,
+                    include_endpoints=include_endpoints,
+                )
+            else:
+                S = self.grab_for_t(
+                    mass,
+                    t=t,
+                    tspan_bg=tspan_bg,
+                    remove_background=remove_background,
+                )
+            signals[mass] = S
+        n_dots = sm.calc_n_dot(signals=signals)
+        return t, n_dots
+
+    @deprecate(
+        "0.1", "Use `remove_background` instead.", "0.3", kwarg_name="removebackground"
+    )
+    def grab_flux_for_t(
+        self,
+        mol,
+        t,
+        tspan_bg=None,
+        remove_background=False,
+        removebackground=None,
+    ):
+        """Return the flux of mol (calibrated signal) in [mol/s] for a given time vec
+
+        Args:
+            mol (str): Name of the molecule.
+            t (np.array): The time vector along which to give the flux
+            tspan_bg (tspan): Timespan that corresponds to the background signal.
+                If not given, no background is subtracted.
+            remove_background (bool): Whether to remove a pre-set background if available
+            removebackground (bool): DEPRECATED. Use `remove_background`.
+        """
+        if removebackground is not None:
+            remove_background = removebackground
+        t_0, y_0 = self.grab_flux(
+            mol,
+            tspan_bg=tspan_bg,
+            remove_background=remove_background,
+        )
+        y = np.interp(t, t_0, y_0)
+        return y
+
+    def get_flux_series(self, mol):
+        """Return a ValueSeries with the calibrated flux of mol"""
+        return self[f"n_dot_{mol}"]
+
+    def integrate_signal(self, mass, tspan, tspan_bg, ax=None):
+        """Integrate a ms signal with background subtraction and evt. plotting
+
+        TODO: Should this, like grab_signal does now, have the option of using a
+            background saved in the object rather than calculating a new one?
+
+        Args:
+            mass (str): The mass for which to integrate the signal
+            tspan (tspan): The timespan over which to integrate
+            tspan_bg (tspan): Timespan at which the signal is at its background value
+            ax (Axis): axis to plot on. Defaults to None
+        """
+        t, S = self.grab_signal(mass, tspan=tspan, include_endpoints=True)
+        if tspan_bg:
+            t_bg, S_bg_0 = self.grab_signal(mass, tspan=tspan_bg, include_endpoints=True)
+            S_bg = np.mean(S_bg_0) * np.ones(t.shape)
+        else:
+            S_bg = np.zeros(t.shape)
+        if ax:
+            if ax == "new":
+                fig, ax = self.plotter.new_ax()
+            ax.fill_between(t, S_bg, S, color=STANDARD_COLORS[mass], alpha=0.2)
+        return np.trapz(S - S_bg, t)
+
+    @property
+    def mass_list(self):
+        """List of the masses for which ValueSeries are contained in the measurement"""
+        return [self.as_mass(col) for col in self.series_names if self.is_mass(col)]
+
+    def is_mass(self, item):
+        if re.search("^M[0-9]+$", item):
+            return True
+        if item in self.reverse_aliases and self.is_mass(self.reverse_aliases[item][0]):
+            return True
+        return False
+
+    def as_mass(self, item):
+        if re.search("^M[0-9]+$", item):
+            return item
+        new_item = self.reverse_aliases[item][0]
+        if self.is_mass(new_item):
+            return self.as_mass(new_item)
+        raise TypeError(f"{self!r} does not recognize '{item}' as a mass.")
+
+    @deprecate(
+        "0.2.6",
+        "Use `inlet` instead. Or consider using `siq_gas_flux_calibration` "
+        "with the `spectro_inlets_quantification` package.",
+        "0.3",
+        kwarg_name="chip",
+    )
+    def gas_flux_calibration(
+        self,
+        mol,
+        mass,
+        inlet=None,
+        chip=None,
+        tspan=None,
+        tspan_bg=None,
+        ax=None,
+        carrier_mol=None,
+        mol_conc_ppm=None,
+    ):
+        """
+        Fit mol's sensitivity at mass based on one period with steady gas composition.
+
+        Args:
+            mol (str): The name of the molecule to calibrate
+            mass (str): The mass to calibrate at
+            inlet (MSInlet): An object with a `calc_n_dot_0` method for total flux to
+                the vacuum chamber containing the mass spectrometer.
+            chip (MSInlet): DEPRECATED. Old name for `inlet`.
+            tspan (iter): The timespan to average the signal over. Defaults to all
+            tspan_bg (iter): Optional timespan at which the signal is at its background.
+            ax (matplotlib axis): The axis on which to indicate what signal is used
+                with a thicker line. Defaults to none
+            carrier_mol (str): The name of the molecule of the carrier gas if
+                a dilute analyte is used. Calibration assumes total flux of the
+                capillary is the same as the flux of pure carrier gas. Defaults
+                to None.
+            mol_conc_ppm (float): Concentration of the dilute analyte in the carrier gas
+                in ppm. Defaults to None.
+
+        Returns MSCalResult: a MS calibration result containing the sensitivity factor
+            for mol at mass
+        """
+        if plugins.use_siq:
+            warnings.warn(
+                "spectro_inlets_quantification is active but you are using the native "
+                "ixdat version of `MSMeasurement.gas_flux_calibration`"
+            )
+        t, S = self.grab_signal(mass, tspan=tspan, tspan_bg=tspan_bg)
+        if ax:
+            ax.plot(t, S, color=STANDARD_COLORS[mass], linewidth=5)
+        if carrier_mol:
+            if mol_conc_ppm:
+                cal_type = "carrier_gas_flux_calibration"
+            else:
+                raise QuantificationError(
+                    "Cannot use carrier gas calibration without analyte"
+                    " concentration. mol_conc_ppm is missing."
+                )
+        elif mol_conc_ppm:
+            raise QuantificationError(
+                "Cannot use carrier gas calibration without carrier"
+                " gas definition. carrier_mol is missing."
+            )
+        else:
+            cal_type = "gas_flux_calibration"
+            mol_conc_ppm = 10**6
+            carrier_mol = mol
+        n_dot = inlet.calc_n_dot_0(gas=carrier_mol) * mol_conc_ppm / 10**6
+        F = np.mean(S) / n_dot
+        return MSCalResult(
+            name=f"{mol}@{mass}",
+            mol=mol,
+            mass=mass,
+            cal_type=cal_type,
+            F=F,
+        )
+
+    def gas_flux_calibration_curve(
+        self,
+        mol,
+        mass,
+        inlet=None,
+        chip=None,
+        tspan_list=None,
+        carrier_mol=None,
+        mol_conc_ppm=None,
+        p_inlet=None,
+        tspan_bg=None,
+        ax="new",
+        axis_measurement=None,
+        remove_bg_on_axis_measurement=True,
+        return_ax=False,
+    ):
+        """Fit mol's sensitivity at mass from 2+ periods of steady gas composition.
+
+        Args:
+            inlet (MSInlet): An object with a `calc_n_dot_0` method for total flux to
+                the vacuum chamber containing the mass spectrometer.
+            mol (str): Name of the molecule to calibrate
+            mass (str): Name of the mass at which to calibrate
+            inlet (MSInlet): An object with a `calc_n_dot_0` method for total flux to
+                the vacuum chamber containing the mass spectrometer.
+            chip (MSInlet): DEPRECATED. Old name for `inlet`.
+            tspan_list (list of tspan): The timespans of steady concentration
+                or pressure
+            carrier_mol (str): The name of the molecule of the carrier gas if
+                a dilute analyte is used. Calibration assumes total flux of the
+                capillary is the same as the flux of pure carrier gas. Defaults
+                to None.
+            mol_conc_ppm (float or list): Concentration of the dilute analyte in
+                the carrier gas in ppm. Defaults to None. Accepts float (for pressure
+                calibration) or list for concentration calibration. If list needs
+                to be same length as tspan_list or selector_list.
+            p_inlet (float, list): Pressure at the inlet (Pa). Overwrites the pressure
+                inherent to self (i.e. the MSInlet object). Accepts float (for conc.
+                calibration) or list for pressure calibration. If list, then
+                needs to be same length as tspan_list or selector_list.
+            tspan_bg (tspan): The time to use as a background
+            ax (Axis): The axis on which to plot the ms_calibration curve result.
+                Defaults to a new axis.
+            axis_measurement (Axis): The MS plot axes to highlight the
+                ms_calibration on. Defaults to None.
+            remove_bg_on_axis_measurement (bool):
+                Whether the plot on axis_measurement is showing raw data or bg
+                subtracted data. Defaults to True, i.e. plotting data with the
+                same bg subtraction as used for the calibration.
+            return_ax (bool): Whether to return the axis on which the calibration
+                curve is plotted together with the MSCalResult. Defaults to False.
+
+        Return MSCalResult(, Axis): The result of the MS calibration (and calibration
+            curve axis if requested) based on flux calculation during selected time
+            periods.
+        TODO: automatically recognize the pressure from measurement (if available)
+        """
+        if plugins.use_siq:
+            warnings.warn(
+                "spectro_inlets_quantification is active but you are using the native "
+                "ixdat version of `MSMeasurement.siq_gas_flux_calibration_curve`"
+            )
+        return self._gas_flux_calibration_curve(
+            mol=mol,
+            mass=mass,
+            inlet=inlet,
+            chip=chip,
+            tspan_list=tspan_list,
+            carrier_mol=carrier_mol,
+            mol_conc_ppm=mol_conc_ppm,
+            p_inlet=p_inlet,
+            tspan_bg=tspan_bg,
+            ax="new",
+            axis_measurement=axis_measurement,
+            remove_bg_on_axis_measurement=remove_bg_on_axis_measurement,
+            return_ax=return_ax,
+        )
+
+    @deprecate(
+        "0.2.6",
+        "Use `inlet` instead. Or consider using `siq_gas_flux_calibration_curve` "
+        "with the `spectro_inlets_quantification` package.",
+        "0.3",
+        kwarg_name="chip",
+    )
+    def _gas_flux_calibration_curve(
+        self,
+        mol,
+        mass,
+        inlet=None,
+        chip=None,
+        tspan_list=None,
+        carrier_mol=None,
+        mol_conc_ppm=None,
+        p_inlet=None,
+        tspan_bg=None,
+        ax="new",
+        axis_measurement=None,
+        remove_bg_on_axis_measurement=True,
+        return_ax=False,
+    ):
+        """Helper function. See gas_flux_calibraiton_curve for argument descriptions."""
+
+        # prepare three lists to loop over to determine molecule flux in the
+        # different periods of steady gas composition
+        if not isinstance(mol_conc_ppm, list):
+            mol_conc_ppm_list = [mol_conc_ppm for x in tspan_list]
+        else:
+            mol_conc_ppm_list = mol_conc_ppm
+        if isinstance(p_inlet, list):
+            p_list = p_inlet
+        else:
+            p_list = [p_inlet for _ in tspan_list]
+        if not len(mol_conc_ppm_list) == len(p_list) == len(tspan_list):
+            raise ValueError(
+                "Length of input lists for concentrations"
+                " and tspan or pressures and tspan is not equal"
+            )
+        S_list = []
+        n_dot_list = []
+        if carrier_mol:
+            if None not in mol_conc_ppm_list:
+                cal_type = "carrier_gas_flux_calibration_curve"
+            else:
+                raise QuantificationError(
+                    "Cannot use carrier gas calibration without analyte"
+                    " concentration. 'mol_conc_ppm' is missing. For a pure gas,"
+                    "use 'mol' instead of 'carrier_mol' and don't give a 'mol_conc_ppm'"
+                )
+        elif None not in mol_conc_ppm_list:
+            raise QuantificationError(
+                "Cannot use carrier gas calibration without carrier"
+                " gas definition. 'carrier_mol' is missing. For a pure gas,"
+                "use 'mol' instead of 'carrier_mol' and don't give a 'mol_conc_ppm'"
+            )
+        else:
+            cal_type = "gas_flux_calibration_curve"
+            # redefine mol_conc_ppm_list to compensate for unit correction done
+            # in the calculation of n_dot below
+            mol_conc_ppm = 10**6
+            mol_conc_ppm_list = [mol_conc_ppm for x in tspan_list]
+            # specify that the gas given as mol is now the carrier_mol
+            carrier_mol = mol
+        for tspan, mol_conc_ppm, pressure in zip(tspan_list, mol_conc_ppm_list, p_list):
+            t, S = self.grab_signal(mass, tspan=tspan, tspan_bg=tspan_bg)
+            if axis_measurement:
+                if remove_bg_on_axis_measurement:
+                    t_plot, S_plot = t, S
+                else:
+                    t_plot, S_plot = self.grab_signal(mass, tspan=tspan)
+                axis_measurement.plot(
+                    t_plot, S_plot, color=STANDARD_COLORS[mass], linewidth=5
+                )
+            n_dot = (
+                inlet.calc_n_dot_0(gas=carrier_mol, p=pressure) * mol_conc_ppm / 10**6
+            )
+            S_list.append(np.mean(S))
+            n_dot_list.append(n_dot)
+        n_dot_vec = np.array(n_dot_list)
+        S_vec = np.array(S_list)
+        pfit = np.polyfit(n_dot_vec, S_vec, deg=1)
+        F = pfit[0]
+        if ax:
+            color = STANDARD_COLORS[mass]
+            if ax == "new":
+                ax = self.plotter.new_ax(
+                    xlabel="molecule flux / [nmol/s]", ylabel="signal / [nA]"
+                )
+            ax.plot(n_dot_vec * 1e9, S_vec * 1e9, "o", color=color)
+            n_dot_fit = np.array([0, max(n_dot_vec)])
+            S_fit = n_dot_fit * pfit[0] + pfit[1]
+            ax.plot(n_dot_fit * 1e9, S_fit * 1e9, "--", color=color)
+        cal = MSCalResult(
+            name=f"{mol}@{mass}",
+            mol=mol,
+            mass=mass,
+            cal_type=cal_type,
+            F=F,
+        )
+        if return_ax:
+            return cal, ax
+        return cal
+
+    def siq_gas_flux_calibration(self, mol, mass, tspan, chip=None):
+        """Simple pure-gas flux calibration, using `spectro_inlets_quantification`
+
+        Args:
+            mol (str): Name of molecule to be calibrated (e.g. "He")
+            mass (str): Mass at which to calibrate it (e.g. "M4")
+            tspan (timespan): A timespan during which the pure gas is in the chip
+            chip (Chip, optional): An object defining the capillary inlet, if different
+                than the standard chip assumed by the external package.
+
+        Returns CalPoint: An object from `spectro_inlets_quantification`,
+           representing the calibration result
+        """
+        if not plugins.use_siq:
+            raise QuantificationError(
+                "`MSMeasurement.siq_gas_flux_calibration` only works when using "
+                "`spectro_inlets_quantification` "
+                "(`ixdat.options.activate_siq()`). For native ixdat MS quantification, "
+                "use `gas_flux_calibration` instead."
+            )
+        Chip = plugins.siq.Chip
+        CalPoint = plugins.siq.CalPoint
+
+        chip = chip or Chip()
+        n_dot = chip.calc_n_dot_0(gas=mol)
+        S = self.grab_signal(mass, tspan=tspan)[1].mean()
+        F = S / n_dot
+        return CalPoint(mol=mol, mass=mass, F=F, F_type="capillary", date=self.yyMdd)
+
+    def siq_gas_flux_calibration_curve(
+        self,
+        mol,
+        mass,
+        chip=None,
+        tspan_list=None,
+        carrier_mol=None,
+        mol_conc_ppm=None,
+        p_inlet=None,
+        tspan_bg=None,
+        ax="new",
+        axis_measurement=None,
+        remove_bg_on_axis_measurement=True,
+        return_ax=False,
+    ):
+        """Fit mol's sensitivity at mass from 2+ periods of steady gas composition.
+
+        Args:
+            mol (str): Name of the molecule to calibrate
+            mass (str): Name of the mass at which to calibrate
+            tspan_list (list of tspan): The timespans of steady concentration
+                or pressure
+            carrier_mol (str): The name of the molecule of the carrier gas if
+                a dilute analyte is used. Calibration assumes total flux of the
+                capillary is the same as the flux of pure carrier gas. Defaults
+                to None.
+            mol_conc_ppm (float, list): Concentration of the dilute analyte in
+                the carrier gas in ppm. Defaults to None. Accepts float (for pressure
+                calibration) or list for concentration calibration. If list needs
+                to be same length as tspan_list or selector_list.
+            p_inlet (float, list): Pressure at the inlet (Pa). Overwrites the pressure
+                inherent to self (i.e. the MSInlet object). Accepts float (for conc.
+                calibration) or list for pressure calibration. If list, then
+                needs to be same length as tspan_list or selector_list.
+            tspan_bg (tspan): The time to use as a background
+            ax (Axis): The axis on which to plot the ms_calibration curve result.
+                Defaults to a new axis.
+            axis_measurement (Axes): The MS plot axes to highlight the
+                ms_calibration on. Defaults to None. These axes are not returned.
+            remove_bg_on_axis_measurement (bool):
+                Whether the plot on axis_measurement is showing raw data or bg
+                subtracted data. Defaults to True, i.e. plotting data with the
+                same bg subtraction as used for the calibration.
+            return_ax (bool): Whether to return the axis on which the calibration
+                curve is plotted together with the MSCalResult. Defaults to False.
+
+        Returns CalPoint: An object from `spectro_inlets_quantification`,
+           representing the calibration result
+
+        TODO: automatically recognize the pressure from measurement (if available)
+        """
+        if not plugins.use_siq:
+            raise QuantificationError(
+                "`MSMeasurement.siq_gas_flux_calibration` only works when using "
+                "`spectro_inlets_quantification`"
+                "(`ixdat.options.activate_siq()`). "
+                "For native ixdat MS quantification, use `gas_flux_calibration`"
+                "instead."
+            )
+        Chip = plugins.siq.Chip
+
+        chip = chip or Chip()
+
+        cal, ax = self._gas_flux_calibration_curve(
+            inlet=chip,
+            mol=mol,
+            mass=mass,
+            tspan_list=tspan_list,
+            carrier_mol=carrier_mol,
+            mol_conc_ppm=mol_conc_ppm,
+            p_inlet=p_inlet,
+            tspan_bg=tspan_bg,
+            ax=ax,
+            axis_measurement=axis_measurement,
+            remove_bg_on_axis_measurement=remove_bg_on_axis_measurement,
+            return_ax=True,
+        )
+
+        cal = cal.to_siq()
+        if return_ax:
+            return cal, ax
+        return cal
+
+    def siq_multicomp_gas_flux_calibration(
+        self, mol_list, mass_list, gas, tspan, gas_bg=None, tspan_bg=None, chip=None
+    ):
+        """Calibration of multiple components of a calibration gas simultaneously
+
+        Uses a matrix equation and the reference spectra in the molecule data files.
+
+        The results are only as accurate as the reference spectrum used. For this reason,
+        this method is a last resort and it is recommended *not* to use a multicomponent
+        calibration gas. Instead, get a separate calibration gas for each molecule to
+        be calibrated.
+
+        Here is an explanation of the math used in this method:
+
+        The fundamental matrix equation is:
+          S_vec = F_mat @ n_dot_vec
+        Elementwise, this is:
+         S_M = sum_i ( F^i_M * n_dot^i )
+        Rewrite to show that sensitivity factors follow each molecule's spectrum:
+         S_M = sum_i (F_weight_i * spectrum^i_M * n_dot^i)
+        And regroup the parts that only depend on the molecule (^i):
+         S_M = sum_i (spectrum^i_M * (F_weight^i * n_dot^i))
+         S_M = sum_i (spectrum^i_M * sensitivity_flux^i)
+        Change back into a matrix equation, and solve it:
+         S_vec = spectrum_mat @ sensitivity_flux_vec
+         sensitivity_flux_vec = spectrum_mat^-1 @ S_vec   # eq. 1
+        Ungroup the part we grouped before (the "sensitivity_flux"):
+         F_weight^i = sensitivity_flux^i / n_dot^i        # eq. 2
+        And, in the end, each sensitivity factor is:
+         F_M^i = F_weight^i * spectrum^i_M                # eq. 3
+
+        Equations 1, 2, and 3 are implemented in the code of this method.
+
+        Args:
+            mol_list (list of str): List of the names of the molecules to calibrate
+            mass_list (list of str): List of the masses to calibrate
+            gas (Gas, dict, or str): Composition of the calibration gas, e.g.
+               {"Ar": 0.95, "H2": 0.05} for 5% H2 in Ar
+            tspan (Timespan): Timespan during which the calibration gas is in the chip
+            gas_bg (Gas, dict, or str): Composition of the background gas
+            tspan_bg (Timespan): Timespan during which the background gas is in the chip
+            chip (Chip, optional): object describing the MS capillary, if different than
+               the standard chip in the MS quantification package
+
+        Returns Calibration: An object from `spectro_inlets_quantification`,
+           representing all the calibration results from the calibration.
+        """
+        if not plugins.use_siq:
+            raise QuantificationError(
+                "`MSMeasurement.siq_multicomp_gas_flux_calibration` "
+                "only works when using `spectro_inlets_quantification` "
+                "(`ixdat.plugins.activate_siq()`). "
+            )
+        Chip = plugins.siq.Chip
+        CalPoint = plugins.siq.CalPoint
+        Calibration = plugins.siq.Calibration
+
+        chip = chip or Chip()
+        chip.gas = gas
+        flux = chip.calc_n_dot()
+
+        chip_bg = chip or Chip()
+        chip_bg.gas = gas_bg
+        flux_bg = chip_bg.calc_n_dot()
+
+        delta_flux_list = []
+        for mol in mol_list:
+            delta_flux = flux.get(mol, 0) - flux_bg.get(mol, 0)
+            delta_flux_list.append(delta_flux)
+        delta_flux_vec = np.array(delta_flux_list)
+
+        delta_signal_list = []
+        for mass in mass_list:
+            S = self.grab_signal(mass, tspan=tspan)[1].mean()
+            S_bg = self.grab_signal(mass, tspan=tspan_bg)[1].mean()
+            delta_S = S - S_bg
+            delta_signal_list.append(delta_S)
+        delta_signal_vec = np.array(delta_signal_list)
+
+        spectrum_vec_list = []
+        for mol in mol_list:
+            spectrum = chip.gas.mdict[mol].norm_spectrum
+            spectrum_vec = np.array([spectrum.get(mass, 0) for mass in mass_list])
+            spectrum_vec_list.append(spectrum_vec)
+        spectrum_mat = np.stack(spectrum_vec_list).transpose()
+
+        inverse_spectrum_mat = np.linalg.inv(spectrum_mat)
+        sensitivity_flux_vec = inverse_spectrum_mat @ delta_signal_vec  # eq. 1
+        F_weight_vec = sensitivity_flux_vec / delta_flux_vec  # eq. 2
+
+        cal_list = []
+        for i, mol in enumerate(mol_list):
+            for M, mass in enumerate(mass_list):
+                F = F_weight_vec[i] * spectrum_mat[M, i]  # eq. 3
+                if F:
+                    cal = CalPoint(
+                        mol=mol, mass=mass, F=F, F_type="capillary", date=self.yyMdd
+                    )
+                    cal_list.append(cal)
+
+        return Calibration(cal_list=cal_list)
+
+    def set_siq_quantifier(
+        self,
+        quantifier=None,
+        calibration=None,
+        mol_list=None,
+        mass_list=None,
+        carrier="He",
+    ):
+        """Set the `spectro_inlets_quantification` quantifier.
+
+        The Quantifier is an object with the method `calc_n_dot`, which takes a
+        dictionary of signals or signal vectors in [A] and return a dictionary of
+        molecular fluxes in [mol/s].
+        The quantifier typically does this by solving the linear equations of
+        S_M = sum_i ( F_M^i * n_dot^i )
+        Where n_dot^i is the flux to the vacuum chamber of molecule i in [mol/s], S_M
+        is the signal at mass M in [A], and F_M^i is the *sensitivity factor* of molecule
+        i at mass M.
+        The quantifier thus needs access to a set of sensitivity factors.
+
+        The quantifier can be built in this method (avoiding explicit import of the
+        `spectro_inlets_quantification` package) by providing the sensitivity factors
+        in the form of a `Calibration` (which can be obtained from e.g.
+        MSMeasurement.multicomp_gas_flux_cal) and the specification of which ones to
+        use by `mol_list` and `mass_list`.
+        The quantifier will always use all the masses in `mass_list` to solve for the
+        flux of all the mols in `mol_list`.
+
+        The argument `carrier` is required by some quantifiers but only used if
+        partial pressures before the MS inlet are required (`quantifier.calc_pp`)
+
+        Quantification is only as accurate as your sensitivity factors!
+
+        Args:
+            quantifier (Quantifier): The quantifier, if prepared before method call.
+               No additional arguments needed. Otherwise, the following three are needed:
+            calibration (Calibration): The calibration to build the quantifier with
+            mol_list (list of str): The list of molecules to use in flux calculations.
+               These should all be represented in the Calibration. If not provided,
+               we'll use all the mols in the Calibration.
+            mass_list (list of str): The list of masses to use in flux calculations.
+               These should all be represented in the Calibration. If not provided,
+               we'll use all the masses in the Calibration.
+            carrier (optional, str): The carrier gas in the experiment. Defaults to "He".
+        """
+        if not plugins.use_siq:
+            raise QuantificationError(
+                "`MSMeasurement.set_siq_quantifier` only works when using "
+                "`spectro_inlets_quantification` "
+                "(`ixdat.options.activate_siq()`). "
+                "For native ixdat MS quantification, use `MSMeasurement.calibrate`"
+            )
+        Quantifier = plugins.siq.Quantifier
+
+        if quantifier:
+            self._siq_quantifier = quantifier
+        else:
+            mol_list = mol_list or calibration.mol_list
+            mass_list = mass_list or calibration.mass_list
+            self._siq_quantifier = Quantifier(
+                calibration=calibration,
+                mol_list=mol_list,
+                mass_list=mass_list,
+                carrier=carrier,
+            )
+
+    @property
+    def siq_quantifier(self):
+        return self._siq_quantifier
+
+    cut = _with_siq_quantifier(Measurement.cut)
+    multicut = _with_siq_quantifier(Measurement.multicut)
+
+
+class MSCalResult(Saveable):
+    """A class for a mass spec ms_calibration result.
+
+    FIXME: I think that something inheriting directly from Saveable does not belong in
+        a technique module.
+    """
+
+    table_name = "ms_cal_results"
+    column_attrs = {"name", "mol", "mass", "cal_type", "F"}
+
+    def __init__(
+        self,
+        name=None,
+        mol=None,
+        mass=None,
+        cal_type=None,
+        F=None,
+    ):
+        super().__init__()
+        self.name = name or f"{mol}@{mass}"
+        self.mol = mol
+        self.mass = mass
+        self.cal_type = cal_type
+        self.F = F
+
+    def __repr__(self):
+        return (
+            f"{self.__class__.__name__}(name={self.name}, mol={self.mol}, "
+            f"mass={self.mass}, F={self.F})"
+        )
+
+    @property
+    def color(self):
+        return STANDARD_COLORS[self.mass]
+
+    @classmethod
+    def from_siq(cls, siq_cal_point):
+        return cls(
+            name=siq_cal_point.mol + "@" + siq_cal_point.mass,
+            mol=siq_cal_point.mol,
+            mass=siq_cal_point.mass,
+            cal_type=siq_cal_point.F_type,
+            F=siq_cal_point.F,
+        )
+
+    def to_siq(self):
+        if not plugins.use_siq:
+            raise QuantificationError(
+                "`MSCalPoint.to_siq` only works when using "
+                "`spectro_inlets_quantification` "
+                "(`ixdat.options.activate_siq()`). "
+                "For native ixdat MS quantification, use `MSMeasurement.calibrate`"
+            )
+        return plugins.siq.CalPoint(
+            mol=self.mol,
+            mass=self.mass,
+            F=self.F,
+            F_type=self.cal_type,
+        )
+
+
+class MSCalibration(Calibration):
+    """Class for mass spec calibrations. TODO: replace with powerful external package"""
+
+    extra_linkers = {"ms_calibration_results": ("ms_cal_results", "ms_cal_result_ids")}
+    # FIXME: signal_bgs are not saved at present. Should they be a separate table
+    #   of Saveable objects like ms_cal_results or should they be a single json value?
+    child_attrs = [
+        "ms_cal_results",
+    ]
+
+    def __init__(
+        self,
+        name=None,
+        date=None,
+        tstamp=None,  # FIXME: No need to have both a date and a tstamp?
+        setup=None,
+        ms_cal_results=None,
+        signal_bgs=None,
+        technique="MS",
+        measurement=None,
+    ):
+        """
+        Args:
+            name (str): Name of the ms_calibration
+            date (str): Date of the ms_calibration
+            setup (str): Name of the setup where the ms_calibration is made
+            ms_cal_results (list of MSCalResult): The mass spec calibrations
+            measurement (MSMeasurement): The measurement
+        """
+        super().__init__(
+            name=name or f"EC-MS ms_calibration for {setup} on {date}",
+            technique=technique,
+            tstamp=tstamp,
+            measurement=measurement,
+        )
+        self.date = date
+        self.setup = setup
+        self.ms_cal_results = ms_cal_results or []
+        self.signal_bgs = signal_bgs or {}
+
+    @property
+    def ms_cal_result_ids(self):
+        return [cal.id for cal in self.ms_cal_results]
+
+    @property
+    def mol_list(self):
+        return list({cal.mol for cal in self.ms_cal_results})
+
+    @property
+    def mass_list(self):
+        return list({cal.mass for cal in self.ms_cal_results})
+
+    @property
+    def name_list(self):
+        return list({cal.name for cal in self.ms_cal_results})
+
+    def __contains__(self, mol):
+        return mol in self.mol_list or mol in self.name_list
+
+    def __iter__(self):
+        yield from self.ms_cal_results
+
+    def calibrate_series(self, key, measurement=None):
+        """Return a calibrated series for `key` if possible.
+
+        If key starts with "n_", it is interpreted as a molecule flux. This method then
+        searches the calibration for a sensitivity factor for that molecule uses it to
+        divide the relevant mass signal from the measurement. Example acceptable keys:
+        "n_H2", "n_dot_H2".
+        If the key does not start with "n_", or the calibration can't find a relevant
+        sensitivity factor and mass signal, this method returns None.
+        """
+        measurement = measurement or self.measurement
+        if key.startswith("n_"):  # it's a flux!
+            mol = key.split("_")[-1]
+            try:
+                mass, F = self.get_mass_and_F(mol)
+            except QuantificationError:
+                # Calibrations just return None when they can't get what's requested.
+                return
+            signal_series = measurement[mass]
+            y = signal_series.data
+            if mass in measurement.signal_bgs:
+                # FIXME: How to make this optional to user of MSMeasuremt.grab()?
+                y = y - measurement.signal_bgs[mass]
+            n_dot = y / F
+            return ValueSeries(
+                name=f"n_dot_{mol}",
+                unit_name="mol/s",
+                data=n_dot,
+                tseries=signal_series.tseries,
+            )
+
+    def get_mass_and_F(self, mol):
+        """Return the mass and sensitivity factor to use for simple quant. of mol"""
+        cal_list_for_mol = [cal for cal in self if cal.mol == mol or cal.name == mol]
+        Fs = [cal.F for cal in cal_list_for_mol]
+        if not Fs:
+            raise QuantificationError(f"{self!r} has no sensitivity factor for {mol}")
+        index = np.argmax(np.array(Fs))
+
+        the_good_cal = cal_list_for_mol[index]
+        return the_good_cal.mass, the_good_cal.F
+
+    def get_F(self, mol, mass):
+        """Return the sensitivity factor for mol at mass"""
+        cal_list_for_mol_at_mass = [
+            cal
+            for cal in self
+            if (cal.mol == mol or cal.name == mol) and cal.mass == mass
+        ]
+        F_list = [cal.F for cal in cal_list_for_mol_at_mass]
+        if not F_list:
+            raise QuantificationError(
+                f"{self!r} has no sensitivity factor for {mol} at {mass}"
+            )
+        return np.mean(np.array(F_list))
+
+    def scaled_to(self, ms_cal_result):
+        """Return a new ms_calibration w scaled sensitivity factors to match one given"""
+        F_0 = self.get_F(ms_cal_result.mol, ms_cal_result.mass)
+        scale_factor = ms_cal_result.F / F_0
+        calibration_as_dict = self.as_dict()
+        new_cal_list = []
+        for cal in self.ms_cal_results:
+            cal = MSCalResult(
+                name=cal.name,
+                mass=cal.mass,
+                mol=cal.mol,
+                F=cal.F * scale_factor,
+                cal_type=cal.cal_type + " scaled",
+            )
+            new_cal_list.append(cal)
+        calibration_as_dict["ms_cal_results"] = new_cal_list
+        del calibration_as_dict["ms_cal_result_ids"]
+        # ^ FIXME: ms_cal_result_ids via MemoryBackend
+        calibration_as_dict["name"] = calibration_as_dict["name"] + " scaled"
+        return self.__class__.from_dict(calibration_as_dict)
+
+    @classmethod
+    def read(cls, path_to_file):
+        """Read an MSCalibration from a json-formatted text file"""
+        with open(path_to_file) as f:
+            obj_as_dict = json.load(f)
+        # put the MSCalResults (exported as dicts) into objects:
+        obj_as_dict["ms_cal_results"] = [
+            MSCalResult.from_dict(ms_cal_as_dict)
+            for ms_cal_as_dict in obj_as_dict["ms_cal_results"]
+        ]
+        return cls.from_dict(obj_as_dict)
+
+    def export(self, path_to_file=None):
+        """Export an ECMSCalibration as a json-formatted text file"""
+        path_to_file = path_to_file or (self.name + ".ix")
+        self_as_dict = self.as_dict()
+        # replace the ms_cal_result ids with the dictionaries of the results themselves:
+        del self_as_dict["ms_cal_result_ids"]
+        self_as_dict["ms_cal_results"] = [cal.as_dict() for cal in self.ms_cal_results]
+        with open(path_to_file, "w") as f:
+            json.dump(self_as_dict, f, indent=4)
+
+    @classmethod
+    def from_siq(cls, siq_calibration):
+
+        # A complication is that it can be either a Calibration or a SensitivityList.
+        # Either way, the sensitivity factors are in `sf_list`:
+        ms_cal_results = [MSCalResult.from_siq(cal) for cal in siq_calibration.sf_list]
+        # if it's a Calibration, we want the metadata:
+        try:
+            calibration = cls(
+                name=siq_calibration.name,
+                date=siq_calibration.date,
+                setup=siq_calibration.setup,
+                ms_cal_results=ms_cal_results,
+            )
+        # if not, we just want the data:
+        except AttributeError:
+            calibration = cls(ms_cal_results=ms_cal_results)
+        return calibration
+
+    def to_siq(self):
+        if not plugins.use_siq:
+            raise QuantificationError(
+                "`MSCalPoint.to_siq` only works when using "
+                "`spectro_inlets_quantification` "
+                "(`ixdat.options.activate_siq()`). "
+                "For native ixdat MS quantification, use `MSMeasurement.calibrate`"
+            )
+        cal_list = [cal.to_siq() for cal in self.ms_cal_results]
+        return plugins.siq.Calibration(
+            name=self.name,
+            date=self.date,
+            setup=self.setup,
+            cal_list=cal_list,
+        )
+
+
+class MSInlet:
+    """A class for describing the inlet to the mass spec
+
+    Every MSInlet describes the rate and composition of the gas entering a mass
+    spectrometer. The default is a Spectro Inlets EC-MS chip.
+    """
+
+    def __init__(
+        self,
+        *,
+        l_cap=1e-3,
+        w_cap=6e-6,
+        h_cap=6e-6,
+        gas="He",
+        T=STANDARD_TEMPERATURE,
+        p=STANDARD_PRESSURE,
+        verbose=True,
+    ):
+        """Create an MSInlet object given its properties.
+
+        Args:
+            l_cap (float): capillary length [m]. Defaults to design parameter.
+            w_cap (float): capillary width [m]. Defaults to design parameter.
+            h_cap (float): capillary height [m]. Defaults to design parameter.
+            p (float): system pressure in [Pa] (if to change from that in medium)
+            T (float): system temperature in [K] (if to change from that in medium)
+            gas (str): the gas at the start of the inlet.
+            verbose (bool): whether to print stuff to the terminal
+        """
+        self.verbose = verbose
+        self.l_cap = l_cap
+        self.l_cap_eff = {}
+        self.w_cap = w_cap
+        self.h_cap = h_cap
+        self.p = p
+        self.T = T
+        self.gas = gas  # TODO: Gas mixture class. This must be a pure gas now.
+
+    def calc_l_cap_eff(
+        self, n_dot_measured, gas=None, w_cap=None, h_cap=None, T=None, p=None
+    ):
+        """Calculate gas specific effective length of the capillary in [m]
+        and add {gas:value} to l_cap_eff (dict)
+
+        Args:
+            w_cap (float): Capillary width [m], defaults to self.w_cap
+            h_cap (float): Capillary height [m], defaults to self.h_cap
+            n_dot_measured (float): Measured flux of gas [mol/s]
+            gas (dict or str): The gas in the chip, defaults to self.gas
+            T (float): Temperature [K], if to be updated
+            p (float): Pressure [Pa], if to be updated
+        Returns:
+            float: Gas specific effective length in [m]
+        """
+
+        n_dot_predicted = self.calc_n_dot_0(gas=gas, w_cap=w_cap, h_cap=h_cap, T=T, p=p)
+
+        l_cap_gas_specific_eff = self.l_cap * n_dot_predicted / n_dot_measured
+        self.l_cap_eff[
+            gas
+        ] = l_cap_gas_specific_eff  # add effective l_cap for specific gas
+
+        return l_cap_gas_specific_eff
+
+    def update_l_cap(self, gases=None):
+        """Update self.l_cap from average of values in dict l_cap_eff
+
+        Args:
+            gases (list): List of gases to average l_cap, default all
+        Returns:
+            float: Averaged effective capilllary length in [m]
+        """
+        gases = gases or []
+        if self.l_cap_eff and not gases:
+            self.l_cap = np.mean(list(self.l_cap_eff.values()))
+        elif self.l_cap_eff and gases:
+            _l_cap = 0
+            for gas in gases:
+                _l_cap += self.l_cap_eff[gas]
+            self.l_cap = _l_cap / len(gases)
+        return self.l_cap
+
+    def calc_n_dot_0(self, gas=None, w_cap=None, h_cap=None, l_cap=None, T=None, p=None):
+        """Calculate the total molecular flux through the capillary in [s^-1]
+
+        Uses Equation 4.10 of Trimarco, 2017. "Real-time detection of sub-monolayer
+        desorption phenomena during electrochemical reactions: Instrument development
+        and applications." PhD Thesis, Technical University of Denmark.
+
+        Args:
+            w_cap (float): Capillary width [m], defaults to self.w_cap
+            h_cap (float): Capillary height [m], defaults to self.h_cap
+            l_cap (float): Capillary length [m], defaults to self.l_cap
+            gas (dict or str): The gas in the chip, defaults to self.gas
+            T (float): Temperature [K], if to be updated
+            p (float): Pressure [Pa], if to be updated
+        Returns:
+            float: The total molecular flux in [s^-1] through the capillary
+        """
+
+        if w_cap is None:
+            w_cap = self.w_cap  # capillary width in [m]
+        if h_cap is None:
+            h_cap = self.h_cap  # capillary height in [m]
+        if l_cap is None:
+            l_cap = self.l_cap  # effective capillary length in [m]
+        if T is None:
+            T = self.T
+        if p is None:
+            p = self.p
+        pi = np.pi
+
+        # TODO: make it so that DYNAMIC_VISCOSITIES[gas] can just be a float if someone
+        #   enters it without having access to the temperature-dependent values.
+        if T < DYNAMIC_VISCOSITIES[gas][0, 0] or T > DYNAMIC_VISCOSITIES[gas][-1, 0]:
+            warnings.warn(
+                "Insufficient data in constants.py to appropriately estimate "
+                f"the dynamic viscosity for {gas} at temperature: {T}K",
+                stacklevel=2,
+            )
+        _eta_v = DYNAMIC_VISCOSITIES[gas][:, 1]  # list of known eta(T) for 'gas'
+        _eta_T = DYNAMIC_VISCOSITIES[gas][:, 0]  # list of paired Ts for eta(T)
+
+        eta = np.interp(T, _eta_T, _eta_v)  # dynamic viscosity of gas at T in [Pa*s]
+
+        s = MOLECULAR_DIAMETERS[gas]  # molecule diameter in [m]
+        m = MOLAR_MASSES[gas] * 1e-3 / AVOGADRO_CONSTANT  # molecule mass in [kg]
+
+        d = ((w_cap * h_cap) / pi) ** 0.5 * 2
+        # d = 4.4e-6  #used in Henriksen2009
+        a = d / 2
+        p_1 = p
+        lambda_ = d  # defining the transitional pressure
+        # ...from setting mean free path equal to capillary d
+        p_t = BOLTZMANN_CONSTANT * T / (2**0.5 * pi * s**2 * lambda_)
+        p_2 = 0
+        p_m = (p_1 + p_t) / 2  # average pressure in the transitional flow region
+        v_m = (8 * BOLTZMANN_CONSTANT * T / (pi * m)) ** 0.5
+        # a reciprocal velocity used for short-hand:
+        nu = (m / (BOLTZMANN_CONSTANT * T)) ** 0.5
+
+        # ... and now, we're ready for the capillary equation.
+        #   (need to turn of black and flake8 for tolerable format)
+        # fmt: off
+        #   Equation 4.10 of Daniel Trimarco's PhD Thesis:
+        N_dot = (                                                               # noqa
+            1 / (BOLTZMANN_CONSTANT * T) * 1 / l_cap * (                        # noqa
+                (p_t - p_2) * a**3 * 2 * pi / 3 * v_m + (p_1 - p_t) * (         # noqa
+                    a**4 * pi / (8 * eta) * p_m  + a**3 * 2 * pi / 3 * v_m * (  # noqa
+                        (1 + 2 * a * nu * p_m / eta) / (                        # noqa
+                        1 + 2.48 * a * nu * p_m / eta                           # noqa
+                        )                                                       # noqa
+                    )                                                           # noqa
+                )                                                               # noqa
+            )                                                                   # noqa
+        )                                                                       # noqa
+        # fmt: on
+        n_dot = N_dot / AVOGADRO_CONSTANT
+        return n_dot
+
+    @deprecate(
+        last_supported_release="0.2.5",
+        update_message=("`gas_flux_calibration` is now a method of `MSMeasurement`"),
+        hard_deprecation_release="0.3.0",
+        remove_release="1.0.0",
+    )
+    def gas_flux_calibration(
+        self,
+        measurement,
+        mol,
+        mass,
+        tspan=None,
+        tspan_bg=None,
+        ax=None,
+        carrier_mol=None,
+        mol_conc_ppm=None,
+    ):
+        return measurement.gas_flux_calibration(
+            mol=mol,
+            inlet=self,
+            mass=mass,
+            tspan=tspan,
+            tspan_bg=tspan_bg,
+            ax=ax,
+            carrier_mol=carrier_mol,
+            mol_conc_ppm=mol_conc_ppm,
+        )
+
+    @deprecate(
+        last_supported_release="0.2.5",
+        update_message=(
+            "`gas_flux_calibration_curve` is now a method of `MSMeasurement`"
+        ),
+        hard_deprecation_release="0.3.0",
+        remove_release="1.0.0",
+    )
+    def gas_flux_calibration_curve(
+        self,
+        measurement,
+        mol,
+        mass,
+        tspan_list=None,
+        carrier_mol=None,
+        mol_conc_ppm=None,
+        p_inlet=None,
+        tspan_bg=None,
+        ax="new",
+        axis_measurement=None,
+        return_ax=False,
+    ):
+        return measurement.gas_flux_calibration_curve(
+            mol=mol,
+            inlet=self,
+            mass=mass,
+            tspan_list=tspan_list,
+            tspan_bg=tspan_bg,
+            ax=ax,
+            carrier_mol=carrier_mol,
+            mol_conc_ppm=mol_conc_ppm,
+            p_inlet=p_inlet,
+            axis_measurement=axis_measurement,
+            return_ax=return_ax,
+        )
+
+
+class MSSpectrum(Spectrum):
+    """Nothing to add to normal Spectrum yet.
+    TODO: Methods for co-plotting ref spectra from a database
+    """
+
+    pass
+
+
+class MSSpectrumSeries(SpectrumSeries):
+    """Nothing to add to normal SpectrumSeries yet."""
+
+    pass
+
+
+class MSSpectroMeasurement(MSMeasurement, SpectroMeasurement):
+
+    # FIXME: automate this in inheritance of hyphenated techniques:
+    extra_column_attrs = {
+        "spectro_measurements": {"spectrum_id"},
+        "ms_measurement": {"tspan_bg"},
+    }
+    default_plotter = MSSpectroPlotter
+    default_exporter = MSSpectroExporter
+
+    # FIXME: this shouldn't be necessary. See #164.
+    cut = _with_siq_quantifier(SpectroMeasurement.cut)
+    multicut = _with_siq_quantifier(SpectroMeasurement.multicut)
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat/techniques/spectroelectrochemistry.py` & `ixdat-0.2.9.dev3/src/ixdat/techniques/spectroelectrochemistry.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,299 +1,299 @@
-import numpy as np
-from scipy.interpolate import interp1d
-
-from .ec import ECMeasurement
-from ..db import PlaceHolderObject
-from ..spectra import Spectrum, SpectroMeasurement
-from ..data_series import Field, ValueSeries
-from ..exporters.sec_exporter import SECExporter
-from ..plotters.sec_plotter import SECPlotter, ECOpticalPlotter
-
-
-class SpectroECMeasurement(SpectroMeasurement, ECMeasurement):
-    """Electrochemistry with spectrometry."""
-
-    default_exporter = SECExporter
-    default_plotter = SECPlotter
-
-    def __init__(self, **kwargs):
-        """FIXME: Passing the right key-word arguments on is a mess"""
-        ec_kwargs = {
-            k: v for k, v in kwargs.items() if k in ECMeasurement.get_all_column_attrs()
-        }
-        spec_kwargs = {
-            k: v
-            for k, v in kwargs.items()
-            if k in SpectroMeasurement.get_all_column_attrs()
-        }
-        # FIXME: I think the lines below could be avoided with a PlaceHolderObject that
-        #  works together with MemoryBackend
-        if "series_list" in kwargs:
-            ec_kwargs.update(series_list=kwargs["series_list"])
-            spec_kwargs.update(series_list=kwargs["series_list"])
-        if "component_measurements" in kwargs:
-            ec_kwargs.update(component_measurements=kwargs["component_measurements"])
-            spec_kwargs.update(component_measurements=kwargs["component_measurements"])
-        if "calibration_list" in kwargs:
-            ec_kwargs.update(calibration_list=kwargs["calibration_list"])
-            spec_kwargs.update(calibration_list=kwargs["calibration_list"])
-        if "spectrum_series" in kwargs:
-            spec_kwargs.update(spectrum_series=kwargs["spectrum_series"])
-        SpectroMeasurement.__init__(self, **spec_kwargs)
-        ECMeasurement.__init__(self, **ec_kwargs)
-
-
-class ECXASMeasurement(SpectroECMeasurement):
-    """Electrochemistry with X-ray Absorption Spectroscopy"""
-
-    pass
-
-
-class ECOpticalMeasurement(SpectroECMeasurement):
-    """Electrochemistry with optical Spectroscopy
-
-    This adds, to the SpectroElectrochemistry base class, methods for normalizing to a
-    reference spectrum to get optical density, and for tracking intensity at specific
-    wavelengths.
-    """
-
-    default_plotter = ECOpticalPlotter
-
-    extra_linkers = SpectroECMeasurement.extra_linkers.copy()
-    extra_linkers.update({"ec_optical_measurements": ("spectra", "ref_id")})
-
-    def __init__(self, reference_spectrum=None, ref_id=None, **kwargs):
-        """Initialize an SEC measurement. All args and kwargs go to ECMeasurement."""
-        SpectroECMeasurement.__init__(self, **kwargs)
-        if reference_spectrum:
-            self._reference_spectrum = reference_spectrum
-        elif ref_id:
-            self._reference_spectrum = PlaceHolderObject(ref_id, cls=Spectrum)
-        self.tracked_wavelengths = []
-        self.plot_waterfall = self.plotter.plot_waterfall
-        self.plot_wavelengths = self.plotter.plot_wavelengths
-        self.plot_wavelengths_vs_potential = self.plotter.plot_wavelengths_vs_potential
-        self.technique = "EC-Optical"
-
-    @property
-    def reference_spectrum(self):
-        """The reference spectrum which will by default be used to calculate dOD"""
-        if isinstance(self._reference_spectrum, PlaceHolderObject):
-            self._reference_spectrum = self._reference_spectrum.get_object()
-        return self._reference_spectrum
-
-    def set_reference_spectrum(
-        self,
-        spectrum=None,
-        t_ref=None,
-        V_ref=None,
-    ):
-        """Set the spectrum used as the reference when calculating dOD.
-
-        Args:
-            spectrum (Spectrum or str): If a Spectrum is given, it becomes the reference
-                spectrum. The string "reference" can be given to make the reference
-                spectrum become (via the reference_spectrum property) one that the
-                measurement was loaded with (evt. for definition of wavelengths).
-            t_ref (float): The time (with respect to self.tstamp) to use as the
-                reference spectrum
-            V_ref (float): The potential to use as the reference spectrum. This will
-                only work if the potential is monotonically increasing.
-        """
-        if t_ref and not spectrum:
-            spectrum = self.get_spectrum(t=t_ref)
-        if V_ref and not spectrum:
-            spectrum = self.get_spectrum(V=V_ref)
-        if not spectrum:
-            raise ValueError("must provide a spectrum, t_ref, or V_ref!")
-        self._reference_spectrum = spectrum
-
-    @property
-    def wavelength(self):
-        """A DataSeries with the wavelengths for the SEC spectra"""
-        return self.spectra.axes_series[1]
-
-    @property
-    def wl(self):
-        """A numpy array with the wavelengths in [nm] for the SEC spectra"""
-        return self.wavelength.data
-
-    def calc_dOD(self, V_ref=None, t_ref=None, index_ref=None):
-        """Calculate the optical density with respect to a reference
-
-        Provide at most one of V_ref, t_ref, or index. If none are provided the default
-        reference spectrum (self.reference_spectrum) will be used.
-
-        Args:
-            V_ref (float): The potential at which to get the reference spectrum
-            t_ref (float): The time at which to get the reference spectrum
-            index_ref (int): The index of the reference spectrum
-        Return Field: the delta optical density spanning time and wavelength
-        """
-        counts = self.spectra.data
-        if V_ref or t_ref:
-            ref_spec = self.get_spectrum(V=V_ref, t=t_ref, index=index_ref)
-        else:
-            ref_spec = self.reference_spectrum
-        dOD = -np.log10(counts / ref_spec.y)
-        dOD_series = Field(
-            name=r"$\Delta$ O.D.",
-            unit_name="",
-            axes_series=self.spectra.axes_series,
-            data=dOD,
-        )
-        return dOD_series
-
-    def get_spectrum(self, V=None, t=None, index=None, name=None, interpolate=True):
-        """Return the Spectrum at a given potential V, time t, or index
-
-        Exactly one of V, t, and index should be given. If V (t) is out of the range of
-        self.U (self.t), then first or last spectrum will be returned.
-
-        Args:
-            V (float): The potential at which to get the spectrum. Measurement.U must
-                be monotonically increasing for this to work.
-            t (float): The time at which to get the spectrum
-            index (int): The index of the spectrum
-            name (str): Optional. name to give the new spectrum if interpolated
-            interpolate (bool): Optional. Set to false to grab closest spectrum rather
-                than interpolating.
-
-        Return Spectrum: The spectrum. The data is (spectrum.x, spectrum.y)
-        """
-        if V and V in self.U:  # woohoo, can skip interpolation!
-            index = int(np.argmax(self.U == V))
-        elif t and t in self.t:  # woohoo, can skip interpolation!
-            index = int(np.argmax(self.t == t))
-        if index:  # then we're done:
-            return self.spectrum_series[index]
-        # otherwise, we have to interpolate:
-        counts = self.spectra.data
-        end_spectra = (self.spectrum_series[0].y, self.spectrum_series[-1].y)
-        if V:
-            if interpolate:
-                counts_interpolater = interp1d(
-                    self.U, counts, axis=0, fill_value=end_spectra, bounds_error=False
-                )
-                # FIXME: This requires that potential and spectra have same tseries!
-                y = counts_interpolater(V)
-            else:
-                U_diff = np.abs(self.U - V)
-                index = np.argmin(U_diff)
-                y = counts[index]
-            name = name or f"{self.spectra.name}_{V}V"
-        elif t:
-            t_spec = self.spectra.axes_series[0].t
-            if interpolate:
-                counts_interpolater = interp1d(
-                    t_spec, counts, axis=0, fill_value=end_spectra, bounds_error=False
-                )
-                y = counts_interpolater(t)
-            else:
-                t_diff = np.abs(t_spec - t)
-                index = np.argmin(t_diff)
-                y = counts[index]
-            name = name or f"{self.spectra.name}_{t}s"
-        else:
-            raise ValueError("Need t or V or index to select a spectrum!")
-
-        field = Field(
-            data=y,
-            name=name,
-            unit_name=self.spectra.unit_name,
-            axes_series=[self.wavelength],
-        )
-        return Spectrum.from_field(field, tstamp=self.tstamp)
-
-    def get_dOD_spectrum(
-        self,
-        V=None,
-        t=None,
-        index=None,
-        V_ref=None,
-        t_ref=None,
-        index_ref=None,
-    ):
-        """Return the delta optical density Spectrum given a point and reference point.
-
-        Provide exactly one of V, t, and index, and at most one of V_ref, t_ref, and
-        index_ref. For V and V_ref to work, the potential in the measurement must be
-        monotonically increasing.
-
-        Args:
-            V (float): The potential at which to get the spectrum.
-            t (float): The time at which to get the spectrum
-            index (int): The index of the spectrum
-            V_ref (float): The potential at which to get the reference spectrum
-            t_ref (float): The time at which to get the reference spectrum
-            index_ref (int): The index of the reference spectrum
-        Return:
-             Spectrum: The dOD spectrum. The data is (spectrum.x, spectrum.y)
-        """
-        if V_ref or t_ref or index_ref:
-            spectrum_ref = self.get_spectrum(V=V_ref, t=t_ref, index=index_ref)
-        else:
-            spectrum_ref = self.reference_spectrum
-        spectrum = self.get_spectrum(V=V, t=t, index=index)
-        field = Field(
-            data=-np.log10(spectrum.y / spectrum_ref.y),
-            name=r"$\Delta$ OD",
-            unit_name="",
-            axes_series=[self.wavelength],
-        )
-        return Spectrum.from_field(field)
-
-    def track_wavelength(self, wl, width=10, V_ref=None, t_ref=None, index_ref=None):
-        """Return and cache a ValueSeries for the dOD for a specific wavelength.
-
-        The caching adds wl_str to the SECMeasurement's data series, where
-            wl_str = "w" + int(wl)
-            This is dOD. The raw is also added as wl_str + "_raw".
-        So, to get the raw counts for a specific wavelength, call this function and
-            then use __getitem__, as in: sec_meas[wl_str + "_raw"]
-        If V_ref, t_ref, or index_ref are provided, they specify what to reference dOD
-            to. Otherwise, dOD is referenced to the SECMeasurement's reference_spectrum.
-
-        Args:
-            wl (float): The wavelength to track in [nm]
-            width (float): The width around wl to average. For example, if wl=400 and
-                width = 20, the spectra will be averaged between 390 and 410 nm to get
-                the values. Defaults to 10. To interpolate at the exact wavelength
-                rather than averaging, specify `width=0`.
-            V_ref (float): The potential at which to get the reference spectrum
-            t_ref (float): The time at which to get the reference spectrum
-            index_ref (int): The index of the reference spectrum
-        Returns ValueSeries: The dOD value of the spectrum at wl.
-        """
-        if V_ref or t_ref or index_ref:
-            spectrum_ref = self.get_spectrum(V=V_ref, t=t_ref, index=index_ref)
-        else:
-            spectrum_ref = self.reference_spectrum
-        x = self.wl
-        if width:  # averaging
-            wl_mask = np.logical_and(wl - width / 2 < x, x < wl + width / 2)
-            counts_ref = np.mean(spectrum_ref.y[wl_mask])
-            counts_wl = np.mean(self.spectra.data[:, wl_mask], axis=1)
-        else:  # interpolation
-            counts_ref = np.interp(wl, spectrum_ref.x, spectrum_ref.y)
-            counts_wl = []
-            for counts_i in self.spectra.data:
-                c = np.interp(wl, x, counts_i)
-                counts_wl.append(c)
-            counts_wl = np.array(counts_wl)
-        dOD_wl = -np.log10(counts_wl / counts_ref)
-        raw_name = f"w{int(wl)} raw"
-        dOD_name = f"w{int(wl)}"
-        tseries = self.spectra.axes_series[0]
-        raw_vseries = ValueSeries(
-            name=raw_name, unit_name="counts", data=counts_wl, tseries=tseries
-        )
-        dOD_vseries = ValueSeries(
-            name=dOD_name, unit_name="", data=dOD_wl, tseries=tseries
-        )
-        self.replace_series(raw_name, raw_vseries)
-        # FIXME: better caching. See https://github.com/ixdat/ixdat/pull/11
-        self.replace_series(dOD_name, dOD_vseries)
-        # FIXME: better caching. See https://github.com/ixdat/ixdat/pull/11
-        self.tracked_wavelengths.append(dOD_name)  # For the exporter.
-
-        return dOD_vseries
+import numpy as np
+from scipy.interpolate import interp1d
+
+from .ec import ECMeasurement
+from ..db import PlaceHolderObject
+from ..spectra import Spectrum, SpectroMeasurement
+from ..data_series import Field, ValueSeries
+from ..exporters.sec_exporter import SECExporter
+from ..plotters.sec_plotter import SECPlotter, ECOpticalPlotter
+
+
+class SpectroECMeasurement(SpectroMeasurement, ECMeasurement):
+    """Electrochemistry with spectrometry."""
+
+    default_exporter = SECExporter
+    default_plotter = SECPlotter
+
+    def __init__(self, **kwargs):
+        """FIXME: Passing the right key-word arguments on is a mess"""
+        ec_kwargs = {
+            k: v for k, v in kwargs.items() if k in ECMeasurement.get_all_column_attrs()
+        }
+        spec_kwargs = {
+            k: v
+            for k, v in kwargs.items()
+            if k in SpectroMeasurement.get_all_column_attrs()
+        }
+        # FIXME: I think the lines below could be avoided with a PlaceHolderObject that
+        #  works together with MemoryBackend
+        if "series_list" in kwargs:
+            ec_kwargs.update(series_list=kwargs["series_list"])
+            spec_kwargs.update(series_list=kwargs["series_list"])
+        if "component_measurements" in kwargs:
+            ec_kwargs.update(component_measurements=kwargs["component_measurements"])
+            spec_kwargs.update(component_measurements=kwargs["component_measurements"])
+        if "calibration_list" in kwargs:
+            ec_kwargs.update(calibration_list=kwargs["calibration_list"])
+            spec_kwargs.update(calibration_list=kwargs["calibration_list"])
+        if "spectrum_series" in kwargs:
+            spec_kwargs.update(spectrum_series=kwargs["spectrum_series"])
+        SpectroMeasurement.__init__(self, **spec_kwargs)
+        ECMeasurement.__init__(self, **ec_kwargs)
+
+
+class ECXASMeasurement(SpectroECMeasurement):
+    """Electrochemistry with X-ray Absorption Spectroscopy"""
+
+    pass
+
+
+class ECOpticalMeasurement(SpectroECMeasurement):
+    """Electrochemistry with optical Spectroscopy
+
+    This adds, to the SpectroElectrochemistry base class, methods for normalizing to a
+    reference spectrum to get optical density, and for tracking intensity at specific
+    wavelengths.
+    """
+
+    default_plotter = ECOpticalPlotter
+
+    extra_linkers = SpectroECMeasurement.extra_linkers.copy()
+    extra_linkers.update({"ec_optical_measurements": ("spectra", "ref_id")})
+
+    def __init__(self, reference_spectrum=None, ref_id=None, **kwargs):
+        """Initialize an SEC measurement. All args and kwargs go to ECMeasurement."""
+        SpectroECMeasurement.__init__(self, **kwargs)
+        if reference_spectrum:
+            self._reference_spectrum = reference_spectrum
+        elif ref_id:
+            self._reference_spectrum = PlaceHolderObject(ref_id, cls=Spectrum)
+        self.tracked_wavelengths = []
+        self.plot_waterfall = self.plotter.plot_waterfall
+        self.plot_wavelengths = self.plotter.plot_wavelengths
+        self.plot_wavelengths_vs_potential = self.plotter.plot_wavelengths_vs_potential
+        self.technique = "EC-Optical"
+
+    @property
+    def reference_spectrum(self):
+        """The reference spectrum which will by default be used to calculate dOD"""
+        if isinstance(self._reference_spectrum, PlaceHolderObject):
+            self._reference_spectrum = self._reference_spectrum.get_object()
+        return self._reference_spectrum
+
+    def set_reference_spectrum(
+        self,
+        spectrum=None,
+        t_ref=None,
+        V_ref=None,
+    ):
+        """Set the spectrum used as the reference when calculating dOD.
+
+        Args:
+            spectrum (Spectrum or str): If a Spectrum is given, it becomes the reference
+                spectrum. The string "reference" can be given to make the reference
+                spectrum become (via the reference_spectrum property) one that the
+                measurement was loaded with (evt. for definition of wavelengths).
+            t_ref (float): The time (with respect to self.tstamp) to use as the
+                reference spectrum
+            V_ref (float): The potential to use as the reference spectrum. This will
+                only work if the potential is monotonically increasing.
+        """
+        if t_ref and not spectrum:
+            spectrum = self.get_spectrum(t=t_ref)
+        if V_ref and not spectrum:
+            spectrum = self.get_spectrum(V=V_ref)
+        if not spectrum:
+            raise ValueError("must provide a spectrum, t_ref, or V_ref!")
+        self._reference_spectrum = spectrum
+
+    @property
+    def wavelength(self):
+        """A DataSeries with the wavelengths for the SEC spectra"""
+        return self.spectra.axes_series[1]
+
+    @property
+    def wl(self):
+        """A numpy array with the wavelengths in [nm] for the SEC spectra"""
+        return self.wavelength.data
+
+    def calc_dOD(self, V_ref=None, t_ref=None, index_ref=None):
+        """Calculate the optical density with respect to a reference
+
+        Provide at most one of V_ref, t_ref, or index. If none are provided the default
+        reference spectrum (self.reference_spectrum) will be used.
+
+        Args:
+            V_ref (float): The potential at which to get the reference spectrum
+            t_ref (float): The time at which to get the reference spectrum
+            index_ref (int): The index of the reference spectrum
+        Return Field: the delta optical density spanning time and wavelength
+        """
+        counts = self.spectra.data
+        if V_ref or t_ref:
+            ref_spec = self.get_spectrum(V=V_ref, t=t_ref, index=index_ref)
+        else:
+            ref_spec = self.reference_spectrum
+        dOD = -np.log10(counts / ref_spec.y)
+        dOD_series = Field(
+            name=r"$\Delta$ O.D.",
+            unit_name="",
+            axes_series=self.spectra.axes_series,
+            data=dOD,
+        )
+        return dOD_series
+
+    def get_spectrum(self, V=None, t=None, index=None, name=None, interpolate=True):
+        """Return the Spectrum at a given potential V, time t, or index
+
+        Exactly one of V, t, and index should be given. If V (t) is out of the range of
+        self.U (self.t), then first or last spectrum will be returned.
+
+        Args:
+            V (float): The potential at which to get the spectrum. Measurement.U must
+                be monotonically increasing for this to work.
+            t (float): The time at which to get the spectrum
+            index (int): The index of the spectrum
+            name (str): Optional. name to give the new spectrum if interpolated
+            interpolate (bool): Optional. Set to false to grab closest spectrum rather
+                than interpolating.
+
+        Return Spectrum: The spectrum. The data is (spectrum.x, spectrum.y)
+        """
+        if V and V in self.U:  # woohoo, can skip interpolation!
+            index = int(np.argmax(self.U == V))
+        elif t and t in self.t:  # woohoo, can skip interpolation!
+            index = int(np.argmax(self.t == t))
+        if index:  # then we're done:
+            return self.spectrum_series[index]
+        # otherwise, we have to interpolate:
+        counts = self.spectra.data
+        end_spectra = (self.spectrum_series[0].y, self.spectrum_series[-1].y)
+        if V:
+            if interpolate:
+                counts_interpolater = interp1d(
+                    self.U, counts, axis=0, fill_value=end_spectra, bounds_error=False
+                )
+                # FIXME: This requires that potential and spectra have same tseries!
+                y = counts_interpolater(V)
+            else:
+                U_diff = np.abs(self.U - V)
+                index = np.argmin(U_diff)
+                y = counts[index]
+            name = name or f"{self.spectra.name}_{V}V"
+        elif t:
+            t_spec = self.spectra.axes_series[0].t
+            if interpolate:
+                counts_interpolater = interp1d(
+                    t_spec, counts, axis=0, fill_value=end_spectra, bounds_error=False
+                )
+                y = counts_interpolater(t)
+            else:
+                t_diff = np.abs(t_spec - t)
+                index = np.argmin(t_diff)
+                y = counts[index]
+            name = name or f"{self.spectra.name}_{t}s"
+        else:
+            raise ValueError("Need t or V or index to select a spectrum!")
+
+        field = Field(
+            data=y,
+            name=name,
+            unit_name=self.spectra.unit_name,
+            axes_series=[self.wavelength],
+        )
+        return Spectrum.from_field(field, tstamp=self.tstamp)
+
+    def get_dOD_spectrum(
+        self,
+        V=None,
+        t=None,
+        index=None,
+        V_ref=None,
+        t_ref=None,
+        index_ref=None,
+    ):
+        """Return the delta optical density Spectrum given a point and reference point.
+
+        Provide exactly one of V, t, and index, and at most one of V_ref, t_ref, and
+        index_ref. For V and V_ref to work, the potential in the measurement must be
+        monotonically increasing.
+
+        Args:
+            V (float): The potential at which to get the spectrum.
+            t (float): The time at which to get the spectrum
+            index (int): The index of the spectrum
+            V_ref (float): The potential at which to get the reference spectrum
+            t_ref (float): The time at which to get the reference spectrum
+            index_ref (int): The index of the reference spectrum
+        Return:
+             Spectrum: The dOD spectrum. The data is (spectrum.x, spectrum.y)
+        """
+        if V_ref or t_ref or index_ref:
+            spectrum_ref = self.get_spectrum(V=V_ref, t=t_ref, index=index_ref)
+        else:
+            spectrum_ref = self.reference_spectrum
+        spectrum = self.get_spectrum(V=V, t=t, index=index)
+        field = Field(
+            data=-np.log10(spectrum.y / spectrum_ref.y),
+            name=r"$\Delta$ OD",
+            unit_name="",
+            axes_series=[self.wavelength],
+        )
+        return Spectrum.from_field(field)
+
+    def track_wavelength(self, wl, width=10, V_ref=None, t_ref=None, index_ref=None):
+        """Return and cache a ValueSeries for the dOD for a specific wavelength.
+
+        The caching adds wl_str to the SECMeasurement's data series, where
+            wl_str = "w" + int(wl)
+            This is dOD. The raw is also added as wl_str + "_raw".
+        So, to get the raw counts for a specific wavelength, call this function and
+            then use __getitem__, as in: sec_meas[wl_str + "_raw"]
+        If V_ref, t_ref, or index_ref are provided, they specify what to reference dOD
+            to. Otherwise, dOD is referenced to the SECMeasurement's reference_spectrum.
+
+        Args:
+            wl (float): The wavelength to track in [nm]
+            width (float): The width around wl to average. For example, if wl=400 and
+                width = 20, the spectra will be averaged between 390 and 410 nm to get
+                the values. Defaults to 10. To interpolate at the exact wavelength
+                rather than averaging, specify `width=0`.
+            V_ref (float): The potential at which to get the reference spectrum
+            t_ref (float): The time at which to get the reference spectrum
+            index_ref (int): The index of the reference spectrum
+        Returns ValueSeries: The dOD value of the spectrum at wl.
+        """
+        if V_ref or t_ref or index_ref:
+            spectrum_ref = self.get_spectrum(V=V_ref, t=t_ref, index=index_ref)
+        else:
+            spectrum_ref = self.reference_spectrum
+        x = self.wl
+        if width:  # averaging
+            wl_mask = np.logical_and(wl - width / 2 < x, x < wl + width / 2)
+            counts_ref = np.mean(spectrum_ref.y[wl_mask])
+            counts_wl = np.mean(self.spectra.data[:, wl_mask], axis=1)
+        else:  # interpolation
+            counts_ref = np.interp(wl, spectrum_ref.x, spectrum_ref.y)
+            counts_wl = []
+            for counts_i in self.spectra.data:
+                c = np.interp(wl, x, counts_i)
+                counts_wl.append(c)
+            counts_wl = np.array(counts_wl)
+        dOD_wl = -np.log10(counts_wl / counts_ref)
+        raw_name = f"w{int(wl)} raw"
+        dOD_name = f"w{int(wl)}"
+        tseries = self.spectra.axes_series[0]
+        raw_vseries = ValueSeries(
+            name=raw_name, unit_name="counts", data=counts_wl, tseries=tseries
+        )
+        dOD_vseries = ValueSeries(
+            name=dOD_name, unit_name="", data=dOD_wl, tseries=tseries
+        )
+        self.replace_series(raw_name, raw_vseries)
+        # FIXME: better caching. See https://github.com/ixdat/ixdat/pull/11
+        self.replace_series(dOD_name, dOD_vseries)
+        # FIXME: better caching. See https://github.com/ixdat/ixdat/pull/11
+        self.tracked_wavelengths.append(dOD_name)  # For the exporter.
+
+        return dOD_vseries
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat/tools.py` & `ixdat-0.2.9.dev3/src/ixdat/tools.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,322 +1,322 @@
-"""This module contains general purpose tools"""
-import datetime
-import inspect
-import time
-import warnings
-from functools import wraps
-from string import ascii_uppercase
-from typing import Optional
-
-import numpy as np
-from packaging import version
-
-from ixdat.exceptions import DeprecationError
-import ixdat.config
-
-# from ixdat.config import CFG
-from ixdat import __version__
-
-warnings.simplefilter("default")
-
-
-def thing_is_close(thing_one, thing_two):
-    """Return whether two things are (nearly) equal, looking recursively if necessary"""
-    if type(thing_one) != type(thing_two):
-        return False
-
-    if isinstance(thing_one, list):
-        return list_is_close(thing_one, thing_two)
-    elif isinstance(thing_one, dict):
-        return dict_is_close(thing_one, thing_two)
-    else:
-        return value_is_close(thing_one, thing_two)
-
-
-def value_is_close(value_one, value_two):
-    """Return whether `value_one` and `value_two` are equal (or close for floats)"""
-    if isinstance(value_one, float) or isinstance(value_two, float):
-        return np.isclose(value_one, value_two)
-    elif isinstance(value_one, np.ndarray) and isinstance(value_two, np.ndarray):
-        return np.allclose(value_one, value_two)
-
-    return value_one == value_two
-
-
-def dict_is_close(dict_one, dict_two):
-    """Return True if the dicts are equal, except floats are allowed to just be close
-
-    Args:
-        dict_one (dict): The first dictionary to compare
-        dict_two (dict): The second dictionary to compare
-
-    .. warning::
-       This function is recursive (also together with :ref:func:`list_is_close` and
-       **will** result in an infinite loop if the values reference back to itself
-    """
-    if len(dict_one) != len(dict_two):
-        return False
-    if dict_one.keys() != dict_two.keys():
-        return False
-
-    for key, value_one in dict_one.items():
-        value_two = dict_two[key]
-        if not thing_is_close(value_one, value_two):
-            return False
-
-    return True
-
-
-def list_is_close(list_one, list_two):
-    """Return True if the lists are equal, except floats are allowed to just be close
-
-    Args:
-        list_one (list): The first list to compare
-        list_two (list): The second list to compare
-
-    .. warning::
-       This function is recursive (also together with :ref:func:`list_is_close` and
-       **will** result in an infinite loop if the values reference back to itself
-    """
-    if len(list_one) != len(list_two):
-        return False
-
-    for value_one, value_two in zip(list_one, list_two):
-        if type(value_one) != type(value_two):
-            return False
-        if not thing_is_close(value_one, value_two):
-            return False
-
-    return True
-
-
-def _construct_deprecation_message(
-    callable_,
-    last_supported_release,
-    update_message,
-    hard_deprecation_release,
-    remove_release,
-    kwarg_name,
-):
-    """Return a deprecation message
-
-    Args:
-        callable_ (Callable): The callable to form a deprecation message for
-
-    All other arguments are as in :func:`deprecate`
-
-    """
-    # Form an identity string for the object which is being deprecated, which is used
-    # in the message to the user
-    identity = f"argument named '{kwarg_name}' in " if kwarg_name else ""
-    if inspect.isclass(callable_):
-        identity += f"class '{callable_.__qualname__}'"
-    else:
-        if "." in callable_.__qualname__:
-            identity += f"method '{callable_.__qualname__}'"
-        else:
-            identity += f"function '{callable_.__qualname__}'"
-
-    message = (
-        f"The {identity} is deprecated, its last supported version being "
-        f"{last_supported_release}:\n"
-    )
-    # Add information on potential hard deprecation
-    if hard_deprecation_release is not None:
-        message += (
-            "* It will become hard deprecated, raising exceptions rather than "
-            f"issuing warnings, from version {hard_deprecation_release}\n"
-        )
-    else:
-        message += (
-            "* It will continue to be soft deprecated, issuing warnings, for the "
-            "foreseeable future\n"
-        )
-
-    # Add information of potential removal
-    if remove_release is not None:
-        message += f"* It is planned for complete removal in version {remove_release}\n"
-
-    message += (
-        "\n"
-        "See instructions below on how to update your code to avoid this message:\n"
-        f"{update_message}"
-    )
-    return message
-
-
-def deprecate(
-    last_supported_release,
-    update_message,
-    hard_deprecation_release=None,
-    remove_release=None,
-    kwarg_name=None,
-):
-    """Mark a function, method, programmed property or class for deprecation
-
-    The deprecator supports soft and hard deprecation, which will either issue warnings
-    or raise exceptions, as well as providing information about an update path and the
-    potential time the functionality will be completely removed.
-
-    Args:
-        last_supported_release (str): The name of the last version when the deprecated
-            functionality was fully supported e.g. "1.2.3"
-        update_message (str): A message to the user with instructions on how to upgrade
-            to avoid the deprecated functionality
-
-    Keyword Args:
-        hard_deprecation_release (str): The release with which the deprecation will raise
-            exceptions rather than issue warnings e.g. "1.4.0". The default None means
-            that this deprecation will remain soft for the foreseeable future.
-        remove_release (str): The release with which the deprecated functionality is
-            planned for deletion
-        kwarg_name (str): If given, is the name of the keyword argument which is
-            deprecated. If not given it is assumed that the entire callable is
-            deprecated.
-
-    Examples:
-
-    Used to deprecate a class::
-     @deprecate("1.2.3", "Please use `MyNewClass` instead", "1.4.0", "2.0.0")
-     class MyClass:
-         ...
-
-    Used to deprecate a method or an argument in a method::
-        class MyClass:
-
-            @deprecate("1.2.3", "Please use `mynewmethod` instead", "1.4.0", "2.0.0")
-            def mymethod(self):
-                ...
-
-            @classmethod
-            @deprecate(
-                "1.2.3",
-                "Please use `kwargb` instead",
-                "1.4.0",
-                "2.0.0",
-                kwarg_name="kwarga"
-            )
-            def mymethod(cls, arg, kwarga=None, kwargb=None):
-                ...
-
-    Used to decorate a property::
-        class MyClass:
-            def __init__(self):
-                self._internal = 8
-
-            @property
-            @deprecate("1.2.3", "Please use `new_external` instead")
-            def external(self):
-                return self._internal
-
-            @external.setter
-            @deprecate("1.2.3", "Please use `new_external` instead")
-            def external(self, value):
-                self._internal = value
-
-    .. note::
-       In the examples above, when this decorator is applied to a class method, static
-       method or programmed property, it must be applied as the first decorator (closest
-       to the def).
-
-    """
-
-    def decorator(callable_):
-        """Decorate a callable with"""
-
-        compound_message = _construct_deprecation_message(
-            callable_,
-            last_supported_release,
-            update_message,
-            hard_deprecation_release,
-            remove_release,
-            kwarg_name,
-        )
-
-        # Get the argument signature of the callable
-        callable_signature = inspect.signature(callable_)
-
-        @wraps(callable_)
-        def inner_function(*args, **kwargs):
-            # Form bound arguments, so both args and kwargs gets mapped to their name
-            bound_arguments = {}
-            if kwarg_name:
-                bound_arguments = callable_signature.bind(*args, **kwargs).arguments
-
-            # If something deprecated is being used, issue the warning or exception
-            if kwarg_name is None or kwarg_name in bound_arguments:
-                if hard_deprecation_release is not None and version.parse(
-                    __version__
-                ) >= version.parse(hard_deprecation_release):
-                    raise DeprecationError(compound_message)
-                else:
-                    # The stacklevel argument here is used to make the warning reference
-                    # the line at which the decorated callable was called, as the place
-                    # where the warning is raised, instead of here in `innner_function`,
-                    # which would be useless
-                    warnings.warn(compound_message, DeprecationWarning, stacklevel=2)
-
-            # Calculate the return value of the original object and return
-            return_value = callable_(*args, **kwargs)
-
-            return return_value
-
-        return inner_function
-
-    return decorator
-
-
-@deprecate(
-    last_supported_release="0.2.8",
-    update_message=(
-        "Please use `ixdat.tools.tstamp_to_string` with the keyword argument "
-        "`string_format='native_date'` instead."
-    ),
-    hard_deprecation_release="0.2.12",
-    remove_release="0.2.14",
-)
-def tstamp_to_yyMdd(tstamp: float) -> str:
-    """Return the date in compact form "yyMdd" format given the unix time (float).
-    In this format the month is given as a capital letter, starting with A for January.
-    E.g. June 4th, 2022 will become 22F04.
-    """
-    a = time.localtime(tstamp)
-    year = a.tm_year
-    month = a.tm_mon
-    day = a.tm_mday
-    date_string = "{0:02d}{1:1s}{2:02d}".format(
-        year % 100, chr(ord("A") + month - 1), day
-    )
-    return date_string
-
-
-def tstamp_to_string(tstamp: float, string_format: Optional[str] = None) -> str:
-    """Return a string representation of unix timestamps `tstamp`
-
-    Args:
-        tstamp (float): The unix time stamp to convert
-        string_format (str): Optional. The datetime string format to use. If not given,
-            the value of ``ixdat.config.config.timestamp_string_format`` will be used.
-            Accepted values are format strings accepted by `datetime.datetime.strftime`
-            or "native" or "native_date", which will produce ixdat native datetime
-            strings or date strings respectively.
-
-    """
-    dt = datetime.datetime.fromtimestamp(tstamp, ixdat.config.config.timezone)
-    if string_format is None:
-        string_format = ixdat.config.config.timestamp_string_format
-
-    if string_format in ("native", "native_date"):
-        # ixdat shows months as capital letters, where Jan->A, Feb->B etc.
-        month_letter = ascii_uppercase[dt.month - 1]
-        if string_format == "native":
-            # Brings to the total format to: 22E18 14:34:55
-            string_format = f"%y{month_letter}%d %H:%M:%S"
-        else:
-            string_format = f"%y{month_letter}%d"
-
-    return dt.strftime(string_format)
-
-
-if __name__ == "__main__":
-    t0 = time.time()
-    print(tstamp_to_string(t0))
+"""This module contains general purpose tools"""
+import datetime
+import inspect
+import time
+import warnings
+from functools import wraps
+from string import ascii_uppercase
+from typing import Optional
+
+import numpy as np
+from packaging import version
+
+from ixdat.exceptions import DeprecationError
+import ixdat.config
+
+# from ixdat.config import CFG
+from ixdat import __version__
+
+warnings.simplefilter("default")
+
+
+def thing_is_close(thing_one, thing_two):
+    """Return whether two things are (nearly) equal, looking recursively if necessary"""
+    if type(thing_one) != type(thing_two):
+        return False
+
+    if isinstance(thing_one, list):
+        return list_is_close(thing_one, thing_two)
+    elif isinstance(thing_one, dict):
+        return dict_is_close(thing_one, thing_two)
+    else:
+        return value_is_close(thing_one, thing_two)
+
+
+def value_is_close(value_one, value_two):
+    """Return whether `value_one` and `value_two` are equal (or close for floats)"""
+    if isinstance(value_one, float) or isinstance(value_two, float):
+        return np.isclose(value_one, value_two)
+    elif isinstance(value_one, np.ndarray) and isinstance(value_two, np.ndarray):
+        return np.allclose(value_one, value_two)
+
+    return value_one == value_two
+
+
+def dict_is_close(dict_one, dict_two):
+    """Return True if the dicts are equal, except floats are allowed to just be close
+
+    Args:
+        dict_one (dict): The first dictionary to compare
+        dict_two (dict): The second dictionary to compare
+
+    .. warning::
+       This function is recursive (also together with :ref:func:`list_is_close` and
+       **will** result in an infinite loop if the values reference back to itself
+    """
+    if len(dict_one) != len(dict_two):
+        return False
+    if dict_one.keys() != dict_two.keys():
+        return False
+
+    for key, value_one in dict_one.items():
+        value_two = dict_two[key]
+        if not thing_is_close(value_one, value_two):
+            return False
+
+    return True
+
+
+def list_is_close(list_one, list_two):
+    """Return True if the lists are equal, except floats are allowed to just be close
+
+    Args:
+        list_one (list): The first list to compare
+        list_two (list): The second list to compare
+
+    .. warning::
+       This function is recursive (also together with :ref:func:`list_is_close` and
+       **will** result in an infinite loop if the values reference back to itself
+    """
+    if len(list_one) != len(list_two):
+        return False
+
+    for value_one, value_two in zip(list_one, list_two):
+        if type(value_one) != type(value_two):
+            return False
+        if not thing_is_close(value_one, value_two):
+            return False
+
+    return True
+
+
+def _construct_deprecation_message(
+    callable_,
+    last_supported_release,
+    update_message,
+    hard_deprecation_release,
+    remove_release,
+    kwarg_name,
+):
+    """Return a deprecation message
+
+    Args:
+        callable_ (Callable): The callable to form a deprecation message for
+
+    All other arguments are as in :func:`deprecate`
+
+    """
+    # Form an identity string for the object which is being deprecated, which is used
+    # in the message to the user
+    identity = f"argument named '{kwarg_name}' in " if kwarg_name else ""
+    if inspect.isclass(callable_):
+        identity += f"class '{callable_.__qualname__}'"
+    else:
+        if "." in callable_.__qualname__:
+            identity += f"method '{callable_.__qualname__}'"
+        else:
+            identity += f"function '{callable_.__qualname__}'"
+
+    message = (
+        f"The {identity} is deprecated, its last supported version being "
+        f"{last_supported_release}:\n"
+    )
+    # Add information on potential hard deprecation
+    if hard_deprecation_release is not None:
+        message += (
+            "* It will become hard deprecated, raising exceptions rather than "
+            f"issuing warnings, from version {hard_deprecation_release}\n"
+        )
+    else:
+        message += (
+            "* It will continue to be soft deprecated, issuing warnings, for the "
+            "foreseeable future\n"
+        )
+
+    # Add information of potential removal
+    if remove_release is not None:
+        message += f"* It is planned for complete removal in version {remove_release}\n"
+
+    message += (
+        "\n"
+        "See instructions below on how to update your code to avoid this message:\n"
+        f"{update_message}"
+    )
+    return message
+
+
+def deprecate(
+    last_supported_release,
+    update_message,
+    hard_deprecation_release=None,
+    remove_release=None,
+    kwarg_name=None,
+):
+    """Mark a function, method, programmed property or class for deprecation
+
+    The deprecator supports soft and hard deprecation, which will either issue warnings
+    or raise exceptions, as well as providing information about an update path and the
+    potential time the functionality will be completely removed.
+
+    Args:
+        last_supported_release (str): The name of the last version when the deprecated
+            functionality was fully supported e.g. "1.2.3"
+        update_message (str): A message to the user with instructions on how to upgrade
+            to avoid the deprecated functionality
+
+    Keyword Args:
+        hard_deprecation_release (str): The release with which the deprecation will raise
+            exceptions rather than issue warnings e.g. "1.4.0". The default None means
+            that this deprecation will remain soft for the foreseeable future.
+        remove_release (str): The release with which the deprecated functionality is
+            planned for deletion
+        kwarg_name (str): If given, is the name of the keyword argument which is
+            deprecated. If not given it is assumed that the entire callable is
+            deprecated.
+
+    Examples:
+
+    Used to deprecate a class::
+     @deprecate("1.2.3", "Please use `MyNewClass` instead", "1.4.0", "2.0.0")
+     class MyClass:
+         ...
+
+    Used to deprecate a method or an argument in a method::
+        class MyClass:
+
+            @deprecate("1.2.3", "Please use `mynewmethod` instead", "1.4.0", "2.0.0")
+            def mymethod(self):
+                ...
+
+            @classmethod
+            @deprecate(
+                "1.2.3",
+                "Please use `kwargb` instead",
+                "1.4.0",
+                "2.0.0",
+                kwarg_name="kwarga"
+            )
+            def mymethod(cls, arg, kwarga=None, kwargb=None):
+                ...
+
+    Used to decorate a property::
+        class MyClass:
+            def __init__(self):
+                self._internal = 8
+
+            @property
+            @deprecate("1.2.3", "Please use `new_external` instead")
+            def external(self):
+                return self._internal
+
+            @external.setter
+            @deprecate("1.2.3", "Please use `new_external` instead")
+            def external(self, value):
+                self._internal = value
+
+    .. note::
+       In the examples above, when this decorator is applied to a class method, static
+       method or programmed property, it must be applied as the first decorator (closest
+       to the def).
+
+    """
+
+    def decorator(callable_):
+        """Decorate a callable with"""
+
+        compound_message = _construct_deprecation_message(
+            callable_,
+            last_supported_release,
+            update_message,
+            hard_deprecation_release,
+            remove_release,
+            kwarg_name,
+        )
+
+        # Get the argument signature of the callable
+        callable_signature = inspect.signature(callable_)
+
+        @wraps(callable_)
+        def inner_function(*args, **kwargs):
+            # Form bound arguments, so both args and kwargs gets mapped to their name
+            bound_arguments = {}
+            if kwarg_name:
+                bound_arguments = callable_signature.bind(*args, **kwargs).arguments
+
+            # If something deprecated is being used, issue the warning or exception
+            if kwarg_name is None or kwarg_name in bound_arguments:
+                if hard_deprecation_release is not None and version.parse(
+                    __version__
+                ) >= version.parse(hard_deprecation_release):
+                    raise DeprecationError(compound_message)
+                else:
+                    # The stacklevel argument here is used to make the warning reference
+                    # the line at which the decorated callable was called, as the place
+                    # where the warning is raised, instead of here in `innner_function`,
+                    # which would be useless
+                    warnings.warn(compound_message, DeprecationWarning, stacklevel=2)
+
+            # Calculate the return value of the original object and return
+            return_value = callable_(*args, **kwargs)
+
+            return return_value
+
+        return inner_function
+
+    return decorator
+
+
+@deprecate(
+    last_supported_release="0.2.8",
+    update_message=(
+        "Please use `ixdat.tools.tstamp_to_string` with the keyword argument "
+        "`string_format='native_date'` instead."
+    ),
+    hard_deprecation_release="0.2.12",
+    remove_release="0.2.14",
+)
+def tstamp_to_yyMdd(tstamp: float) -> str:
+    """Return the date in compact form "yyMdd" format given the unix time (float).
+    In this format the month is given as a capital letter, starting with A for January.
+    E.g. June 4th, 2022 will become 22F04.
+    """
+    a = time.localtime(tstamp)
+    year = a.tm_year
+    month = a.tm_mon
+    day = a.tm_mday
+    date_string = "{0:02d}{1:1s}{2:02d}".format(
+        year % 100, chr(ord("A") + month - 1), day
+    )
+    return date_string
+
+
+def tstamp_to_string(tstamp: float, string_format: Optional[str] = None) -> str:
+    """Return a string representation of unix timestamps `tstamp`
+
+    Args:
+        tstamp (float): The unix time stamp to convert
+        string_format (str): Optional. The datetime string format to use. If not given,
+            the value of ``ixdat.config.config.timestamp_string_format`` will be used.
+            Accepted values are format strings accepted by `datetime.datetime.strftime`
+            or "native" or "native_date", which will produce ixdat native datetime
+            strings or date strings respectively.
+
+    """
+    dt = datetime.datetime.fromtimestamp(tstamp, ixdat.config.config.timezone)
+    if string_format is None:
+        string_format = ixdat.config.config.timestamp_string_format
+
+    if string_format in ("native", "native_date"):
+        # ixdat shows months as capital letters, where Jan->A, Feb->B etc.
+        month_letter = ascii_uppercase[dt.month - 1]
+        if string_format == "native":
+            # Brings to the total format to: 22E18 14:34:55
+            string_format = f"%y{month_letter}%d %H:%M:%S"
+        else:
+            string_format = f"%y{month_letter}%d"
+
+    return dt.strftime(string_format)
+
+
+if __name__ == "__main__":
+    t0 = time.time()
+    print(tstamp_to_string(t0))
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat.egg-info/PKG-INFO` & `ixdat-0.2.9.dev3/README.rst`

 * *Files 24% similar despite different names*

```diff
@@ -1,154 +1,139 @@
-Metadata-Version: 2.1
-Name: ixdat
-Version: 0.2.9.dev2
-Summary: The in-situ experimental data tool
-Home-page: https://github.com/ixdat/ixdat
-Author: Soren B. Scott, Kenneth Nielsen, Anna Winiwarter, et al
-Author-email: sbs@chem.ku.dk
-License: MIT
-Classifier: Programming Language :: Python :: 3
-Classifier: License :: OSI Approved :: MIT License
-Classifier: Operating System :: OS Independent
-Requires-Python: >=3.6
-Description-Content-Type: text/x-rst
-License-File: LICENSE.txt
-
-.. figure:: docs/source/figures/logo.svg
-    :width: 200
-
-=============================================
-``ixdat``: The In-situ Experimental Data Tool
-=============================================
-
-With ``ixdat``, you can import, combine, and export complex experimental datasets
-as simply as::
-
-    ec = Measurement.read_set("awesome_EC_data", reader="biologic")
-    ec.plot_measurement()
-
-    ms = Measurement.read("2021-03-30 16_59_35 MS data.tsv", reader="zilien")
-    ms.plot_measurement()
-
-    ecms = ec + ms
-    ecms.plot_measurement()
-
-    ecms.export("my_combined_data.csv")
-
-Output:
-
-.. figure:: docs/source/figures/ixdat_example_figures.png
-    :width: 700
-
-    In-situ experimental data made easy
-
-Or rather than exporting, you can take advantage of ``ixdat``'s powerful analysis
-tools and database backends to be a one-stop tool from messy raw data to public
-repository accompanying your breakthrough publication and advancing our field.
-
-Version
--------
-This is the latest version.
-
-For changes up to this version, see `CHANGES.rst <https://github.com/ixdat/ixdat/blob/main/CHANGES.rst>`_
-
-For ixdat 0.1.9 see the `v0.1.x branch <https://github.com/ixdat/ixdat/tree/v0.1.x>`_.
-
-About
------
-
-``ixdat`` provides a powerful **object-oriented** interface to experimental data, especially in-situ experimental data for which it is of interest to combine data obtained simultaneously from multiple techniques.
-
-Documentation is at https://ixdat.readthedocs.io
-
-In addition to a **pluggable** parser interface for importing your data format, ``ixdat`` also includes
-pluggable exporters and plotters, as well as a database interface. A relational model of experimental data is
-designed into every level.
-
-.. list-table:: Techniques and Readers
-   :widths: 20 15 50
-   :header-rows: 1
-
-
-   * - Measurement technique
-     - Status
-     - Readers
-   * - Electrochemistry
-     - Released
-     - - biologic: .mpt files from Biologic's EC-Lab software
-       - autolab: ascii files from AutoLab's NOVA software
-       - ivium: .txt files from Ivium's IviumSoft software
-   * - Mass Spectrometry
-     - Released
-     - - pfeiffer: .dat files from Pfeiffer Vacuum's PVMassSpec software
-       - cinfdata: text export from DTU Physics' cinfdata system
-       - zilien: .tsv files from Spectro Inlets' Zilien software
-   * - Electrochemistry - Mass Spectrometry
-     - Released
-     - - zilien: .tsv files from Spectro Inlets' Zilien software
-       - EC_MS: .pkl files from the legacy EC_MS python package
-   * - Spectroelectrochemistry
-     - Released
-     - - msrh_sec: .csv file sets from Imperial College London's SEC system
-   * - X-ray photoelectron spectroscopy (XPS)
-     - Development
-     - - avantage: .avg files from Thermo Scientific's Avantage software
-   * - X-ray diffraction (XRD)
-     - Development
-     - - xrdml: .xrdml files from e.g. PanAnalytical's Empyereon
-   * - In-situ Electrochemistry - X-ray adsorption spectroscopy
-     - Development
-     - - qexafs: .dat files from Diamond's B18 beamline
-   * - Low-Energy Ion Scattering (LEIS)
-     - Future
-     -
-
-Tutorials are provided at https://ixdat.readthedocs.io/en/latest/tutorials/index.html
-
-Installation
-------------
-
-To use ``ixdat``, you need to have python installed. We recommend
-`Anaconda python <https://www.anaconda.com/products/individual>`_.
-
-To install ``ixdat``, just type in your terminal or Anaconda prompt::
-
-    $ pip install ixdat
-
-And hit enter.
-
-``ixdat`` is under development, and to make use of the newest features,
-you may need to upgrade to the latest version. This is also easy. Just type::
-
-    $ pip install --upgrade ixdat
-
-
-Article repositories
---------------------
-
-``ixdat`` is shown in practice in a growing number of open repositories of data and analysis
-for academic publications:
-
-- Soren B. Scott, et al.  **Tracking oxygen atoms in electrochemical CO oxidation –Part I: Oxygen exchange via CO2 hydration**. `Electrochimica Acta, 374, 137842 <https://doi.org/10.1016/j.electacta.2021.137842>`_, **2021**.
-
-  Repository: https://github.com/ScottSoren/pyCOox_public
-
-- Soren B. Scott, et al.  **Tracking oxygen atoms in electrochemical CO oxidation –Part II: Lattice oxygen reactivity in oxides of Pt and Ir**. `Electrochimica Acta, 374, 137844 <https://doi.org/10.1016/j.electacta.2021.137844>`_, **2021**.
-
-  Repository: https://github.com/ScottSoren/pyCOox_public
-
-- Kevin Krempl, et al. **Dynamic Interfacial Reaction Rates from Electrochemistry - Mass Spectrometry**. `Journal of Analytical Chemistry. 93, 7022-7028 <https://doi.org/10.1021/acs.analchem.1c00110>`_, **2021**
-
-  Repository: https://github.com/kkrempl/Dynamic-Interfacial-Reaction-Rates
-
-- Junheng Huang, et al. **Online Electrochemistry−Mass Spectrometry Evaluation of the Acidic Oxygen Evolution Reaction at Supported Catalysts**. `ACS Catal. 11, 12745-12753 <https://doi.org/10.1021/acscatal.1c03430>`_, **2021**
-
-  Repository: https://github.com/ScottSoren/Huang2021
-
-
-Join us
--------
-
-``ixdat`` is free and open source software and we welcome input and new collaborators. Please help us improve ``ixdat``!
-
-Contact us (https://github.com/ixdat/ixdat/discussions or sbscott@ic.ac.uk) or just
-`get started developing <https://ixdat.readthedocs.io/en/latest/developing/index.html>`_.
+.. figure:: docs/source/figures/logo.svg
+    :width: 200
+
+=============================================
+``ixdat``: The In-situ Experimental Data Tool
+=============================================
+
+With ``ixdat``, you can import, combine, and export complex experimental datasets
+as simply as::
+
+    ec = Measurement.read_set("awesome_EC_data", reader="biologic")
+    ec.plot_measurement()
+
+    ms = Measurement.read("2021-03-30 16_59_35 MS data.tsv", reader="zilien")
+    ms.plot_measurement()
+
+    ecms = ec + ms
+    ecms.plot_measurement()
+
+    ecms.export("my_combined_data.csv")
+
+Output:
+
+.. figure:: docs/source/figures/ixdat_example_figures.png
+    :width: 700
+
+    In-situ experimental data made easy
+
+Or rather than exporting, you can take advantage of ``ixdat``'s powerful analysis
+tools and database backends to be a one-stop tool from messy raw data to public
+repository accompanying your breakthrough publication and advancing our field.
+
+Version
+-------
+This is the latest version.
+
+For changes up to this version, see `CHANGES.rst <https://github.com/ixdat/ixdat/blob/main/CHANGES.rst>`_
+
+For ixdat 0.1.9 see the `v0.1.x branch <https://github.com/ixdat/ixdat/tree/v0.1.x>`_.
+
+About
+-----
+
+``ixdat`` provides a powerful **object-oriented** interface to experimental data, especially in-situ experimental data for which it is of interest to combine data obtained simultaneously from multiple techniques.
+
+Documentation is at https://ixdat.readthedocs.io
+
+In addition to a **pluggable** parser interface for importing your data format, ``ixdat`` also includes
+pluggable exporters and plotters, as well as a database interface. A relational model of experimental data is
+designed into every level.
+
+.. list-table:: Techniques and Readers
+   :widths: 20 15 50
+   :header-rows: 1
+
+
+   * - Measurement technique
+     - Status
+     - Readers
+   * - Electrochemistry
+     - Released
+     - - biologic: .mpt files from Biologic's EC-Lab software
+       - autolab: ascii files from AutoLab's NOVA software
+       - ivium: .txt files from Ivium's IviumSoft software
+   * - Mass Spectrometry
+     - Released
+     - - pfeiffer: .dat files from Pfeiffer Vacuum's PVMassSpec software
+       - cinfdata: text export from DTU Physics' cinfdata system
+       - zilien: .tsv files from Spectro Inlets' Zilien software
+   * - Electrochemistry - Mass Spectrometry
+     - Released
+     - - zilien: .tsv files from Spectro Inlets' Zilien software
+       - EC_MS: .pkl files from the legacy EC_MS python package
+   * - Spectroelectrochemistry
+     - Released
+     - - msrh_sec: .csv file sets from Imperial College London's SEC system
+   * - X-ray photoelectron spectroscopy (XPS)
+     - Development
+     - - avantage: .avg files from Thermo Scientific's Avantage software
+   * - X-ray diffraction (XRD)
+     - Development
+     - - xrdml: .xrdml files from e.g. PanAnalytical's Empyereon
+   * - In-situ Electrochemistry - X-ray adsorption spectroscopy
+     - Development
+     - - qexafs: .dat files from Diamond's B18 beamline
+   * - Low-Energy Ion Scattering (LEIS)
+     - Future
+     -
+
+Tutorials are provided at https://ixdat.readthedocs.io/en/latest/tutorials/index.html
+
+Installation
+------------
+
+To use ``ixdat``, you need to have python installed. We recommend
+`Anaconda python <https://www.anaconda.com/products/individual>`_.
+
+To install ``ixdat``, just type in your terminal or Anaconda prompt::
+
+    $ pip install ixdat
+
+And hit enter.
+
+``ixdat`` is under development, and to make use of the newest features,
+you may need to upgrade to the latest version. This is also easy. Just type::
+
+    $ pip install --upgrade ixdat
+
+
+Article repositories
+--------------------
+
+``ixdat`` is shown in practice in a growing number of open repositories of data and analysis
+for academic publications:
+
+- Soren B. Scott, et al.  **Tracking oxygen atoms in electrochemical CO oxidation –Part I: Oxygen exchange via CO2 hydration**. `Electrochimica Acta, 374, 137842 <https://doi.org/10.1016/j.electacta.2021.137842>`_, **2021**.
+
+  Repository: https://github.com/ScottSoren/pyCOox_public
+
+- Soren B. Scott, et al.  **Tracking oxygen atoms in electrochemical CO oxidation –Part II: Lattice oxygen reactivity in oxides of Pt and Ir**. `Electrochimica Acta, 374, 137844 <https://doi.org/10.1016/j.electacta.2021.137844>`_, **2021**.
+
+  Repository: https://github.com/ScottSoren/pyCOox_public
+
+- Kevin Krempl, et al. **Dynamic Interfacial Reaction Rates from Electrochemistry - Mass Spectrometry**. `Journal of Analytical Chemistry. 93, 7022-7028 <https://doi.org/10.1021/acs.analchem.1c00110>`_, **2021**
+
+  Repository: https://github.com/kkrempl/Dynamic-Interfacial-Reaction-Rates
+
+- Junheng Huang, et al. **Online Electrochemistry−Mass Spectrometry Evaluation of the Acidic Oxygen Evolution Reaction at Supported Catalysts**. `ACS Catal. 11, 12745-12753 <https://doi.org/10.1021/acscatal.1c03430>`_, **2021**
+
+  Repository: https://github.com/ScottSoren/Huang2021
+
+
+Join us
+-------
+
+``ixdat`` is free and open source software and we welcome input and new collaborators. Please help us improve ``ixdat``!
+
+Contact us (https://github.com/ixdat/ixdat/discussions or sbscott@ic.ac.uk) or just
+`get started developing <https://ixdat.readthedocs.io/en/latest/developing/index.html>`_.
```

### Comparing `ixdat-0.2.9.dev2/src/ixdat.egg-info/SOURCES.txt` & `ixdat-0.2.9.dev3/src/ixdat.egg-info/SOURCES.txt`

 * *Files 4% similar despite different names*

```diff
@@ -49,14 +49,15 @@
 src/ixdat/readers/chi.py
 src/ixdat/readers/cinfdata.py
 src/ixdat/readers/cinfdata_db.py
 src/ixdat/readers/ec_ms_pkl.py
 src/ixdat/readers/ivium.py
 src/ixdat/readers/ixdat_csv.py
 src/ixdat/readers/msrh_sec.py
+src/ixdat/readers/nordic.py
 src/ixdat/readers/pfeiffer.py
 src/ixdat/readers/qexafs.py
 src/ixdat/readers/reading_tools.py
 src/ixdat/readers/rgasoft.py
 src/ixdat/readers/xrdml.py
 src/ixdat/readers/zilien.py
 src/ixdat/techniques/__init__.py
```

