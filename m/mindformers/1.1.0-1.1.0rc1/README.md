# Comparing `tmp/mindformers-1.1.0-py3-none-any.whl.zip` & `tmp/mindformers-1.1.0rc1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,457 +1,455 @@
-Zip file size: 1339390 bytes, number of entries: 455
--rw-r--r--  2.0 unx    15913 b- defN 24-May-09 13:54 configs/README.md
--rw-r--r--  2.0 unx     4241 b- defN 24-May-09 13:54 configs/bert/run_bert_base_uncased.yaml
--rw-r--r--  2.0 unx     4257 b- defN 24-May-09 13:54 configs/bert/run_bert_tiny_uncased.yaml
--rw-r--r--  2.0 unx     4055 b- defN 24-May-09 13:54 configs/bloom/run_bloom_560m.yaml
--rw-r--r--  2.0 unx     4054 b- defN 24-May-09 13:54 configs/bloom/run_bloom_7.1b.yaml
--rw-r--r--  2.0 unx     4138 b- defN 24-May-09 13:54 configs/bloom/run_bloom_7.1b_910b.yaml
--rw-r--r--  2.0 unx     4156 b- defN 24-May-09 13:54 configs/bloom/run_bloom_7.1b_910b_fa.yaml
--rw-r--r--  2.0 unx     4137 b- defN 24-May-09 13:54 configs/clip/run_clip_vit_b_16_pretrain_flickr8k.yaml
--rw-r--r--  2.0 unx     4285 b- defN 24-May-09 13:54 configs/clip/run_clip_vit_b_16_zero_shot_image_classification_cifar100.yaml
--rw-r--r--  2.0 unx     4138 b- defN 24-May-09 13:54 configs/clip/run_clip_vit_b_32_pretrain_flickr8k.yaml
--rw-r--r--  2.0 unx     4285 b- defN 24-May-09 13:54 configs/clip/run_clip_vit_b_32_zero_shot_image_classification_cifar100.yaml
--rw-r--r--  2.0 unx     4148 b- defN 24-May-09 13:54 configs/clip/run_clip_vit_l_14@336_pretrain_flickr8k.yaml
--rw-r--r--  2.0 unx     4295 b- defN 24-May-09 13:54 configs/clip/run_clip_vit_l_14@336_zero_shot_image_classification_cifar100.yaml
--rw-r--r--  2.0 unx     4139 b- defN 24-May-09 13:54 configs/clip/run_clip_vit_l_14_pretrain_flickr8k.yaml
--rw-r--r--  2.0 unx     4286 b- defN 24-May-09 13:54 configs/clip/run_clip_vit_l_14_zero_shot_image_classification_cifar100.yaml
--rw-r--r--  2.0 unx     6097 b- defN 24-May-09 13:54 configs/codegeex2/run_codegeex2_6b.yaml
--rw-r--r--  2.0 unx     5811 b- defN 24-May-09 13:54 configs/codegeex2/run_codegeex2_6b_eval.yaml
--rw-r--r--  2.0 unx     5817 b- defN 24-May-09 13:54 configs/codegeex2/run_codegeex2_6b_finetune.yaml
--rw-r--r--  2.0 unx     5821 b- defN 24-May-09 13:54 configs/codegeex2/run_codegeex2_6b_finetune_2048.yaml
--rw-r--r--  2.0 unx     5201 b- defN 24-May-09 13:54 configs/codellama/finetune_codellama_34b_32p.yaml
--rw-r--r--  2.0 unx     3943 b- defN 24-May-09 13:54 configs/codellama/predict_codellama_34b_910b.yaml
--rw-r--r--  2.0 unx     5218 b- defN 24-May-09 13:54 configs/codellama/run_codellama_34b_910b.yaml
--rw-r--r--  2.0 unx      843 b- defN 24-May-09 13:54 configs/convert_config/run_convert.yaml
--rw-r--r--  2.0 unx      585 b- defN 24-May-09 13:54 configs/convert_config/run_reversed_convert.yaml
--rw-r--r--  2.0 unx     2617 b- defN 24-May-09 13:54 configs/general/run_general_task.yaml
--rw-r--r--  2.0 unx     5756 b- defN 24-May-09 13:54 configs/glm/run_glm_6b_finetune.yaml
--rw-r--r--  2.0 unx     5677 b- defN 24-May-09 13:54 configs/glm/run_glm_6b_infer.yaml
--rw-r--r--  2.0 unx     5954 b- defN 24-May-09 13:54 configs/glm/run_glm_6b_lora.yaml
--rw-r--r--  2.0 unx     5861 b- defN 24-May-09 13:54 configs/glm/run_glm_6b_lora_infer.yaml
--rw-r--r--  2.0 unx     6051 b- defN 24-May-09 13:54 configs/glm2/predict_glm2_6b.yaml
--rw-r--r--  2.0 unx     6031 b- defN 24-May-09 13:54 configs/glm2/run_glm2_6b.yaml
--rw-r--r--  2.0 unx     5924 b- defN 24-May-09 13:54 configs/glm2/run_glm2_6b_finetune_2k_800T_A2_64G.yaml
--rw-r--r--  2.0 unx     5923 b- defN 24-May-09 13:54 configs/glm2/run_glm2_6b_finetune_2k_800_32G.yaml
--rw-r--r--  2.0 unx     5870 b- defN 24-May-09 13:54 configs/glm2/run_glm2_6b_finetune_800T_A2_64G.yaml
--rw-r--r--  2.0 unx     5916 b- defN 24-May-09 13:54 configs/glm2/run_glm2_6b_finetune_800_32G.yaml
--rw-r--r--  2.0 unx     5615 b- defN 24-May-09 13:54 configs/glm2/run_glm2_6b_finetune_eval.yaml
--rw-r--r--  2.0 unx     6203 b- defN 24-May-09 13:54 configs/glm2/run_glm2_6b_lora_2k_800T_A2_64G.yaml
--rw-r--r--  2.0 unx     6205 b- defN 24-May-09 13:54 configs/glm2/run_glm2_6b_lora_2k_800_32G.yaml
--rw-r--r--  2.0 unx     6193 b- defN 24-May-09 13:54 configs/glm2/run_glm2_6b_lora_800T_A2_64G.yaml
--rw-r--r--  2.0 unx     6197 b- defN 24-May-09 13:54 configs/glm2/run_glm2_6b_lora_800_32G.yaml
--rw-r--r--  2.0 unx     5800 b- defN 24-May-09 13:54 configs/glm2/run_glm2_6b_lora_eval.yaml
--rw-r--r--  2.0 unx     6152 b- defN 24-May-09 13:54 configs/glm2/run_glm2_6b_ptuning2.yaml
--rw-r--r--  2.0 unx     6071 b- defN 24-May-09 13:54 configs/glm3/predict_glm3_6b.yaml
--rw-r--r--  2.0 unx     6031 b- defN 24-May-09 13:54 configs/glm3/run_glm3_6b.yaml
--rw-r--r--  2.0 unx     5909 b- defN 24-May-09 13:54 configs/glm3/run_glm3_6b_finetune_2k_800T_A2_64G.yaml
--rw-r--r--  2.0 unx     5903 b- defN 24-May-09 13:54 configs/glm3/run_glm3_6b_finetune_800T_A2_64G.yaml
--rw-r--r--  2.0 unx     5234 b- defN 24-May-09 13:54 configs/glm3/run_glm3_6b_multiturn_finetune_800T_A2_64G.yaml
--rw-r--r--  2.0 unx     4339 b- defN 24-May-09 13:54 configs/gpt2/run_gpt2.yaml
--rw-r--r--  2.0 unx     4716 b- defN 24-May-09 13:54 configs/gpt2/run_gpt2_13b.yaml
--rw-r--r--  2.0 unx     4860 b- defN 24-May-09 13:54 configs/gpt2/run_gpt2_13b_910b.yaml
--rw-r--r--  2.0 unx     4666 b- defN 24-May-09 13:54 configs/gpt2/run_gpt2_52b.yaml
--rw-r--r--  2.0 unx     4414 b- defN 24-May-09 13:54 configs/gpt2/run_gpt2_lora.yaml
--rw-r--r--  2.0 unx     4299 b- defN 24-May-09 13:54 configs/gpt2/run_gpt2_txtcls.yaml
--rw-r--r--  2.0 unx     4669 b- defN 24-May-09 13:54 configs/gpt2/run_gpt2_xl.yaml
--rw-r--r--  2.0 unx     4907 b- defN 24-May-09 13:54 configs/gpt2/run_gpt2_xl_lora.yaml
--rwxr-xr-x  2.0 unx     5024 b- defN 24-May-09 13:54 configs/llama/run_llama_13b.yaml
--rw-r--r--  2.0 unx     5028 b- defN 24-May-09 13:54 configs/llama/run_llama_13b_910b.yaml
--rwxr-xr-x  2.0 unx     5020 b- defN 24-May-09 13:54 configs/llama/run_llama_7b.yaml
--rw-r--r--  2.0 unx     5019 b- defN 24-May-09 13:54 configs/llama/run_llama_7b_910b.yaml
--rw-r--r--  2.0 unx     5473 b- defN 24-May-09 13:54 configs/llama/run_llama_7b_lora.yaml
--rw-r--r--  2.0 unx     4023 b- defN 24-May-09 13:54 configs/llama2/predict_llama2_13b.yaml
--rw-r--r--  2.0 unx     4246 b- defN 24-May-09 13:54 configs/llama2/predict_llama2_70b.yaml
--rw-r--r--  2.0 unx     5165 b- defN 24-May-09 13:54 configs/llama2/predict_llama2_7b.yaml
--rw-r--r--  2.0 unx     5275 b- defN 24-May-09 13:54 configs/llama2/run_llama2_13b.yaml
--rw-r--r--  2.0 unx     5086 b- defN 24-May-09 13:54 configs/llama2/run_llama2_13b_910b.yaml
--rw-r--r--  2.0 unx     5507 b- defN 24-May-09 13:54 configs/llama2/run_llama2_13b_910b_auto_parallel.yaml
--rw-r--r--  2.0 unx     5139 b- defN 24-May-09 13:54 configs/llama2/run_llama2_13b_910b_finetune.yaml
--rw-r--r--  2.0 unx     5063 b- defN 24-May-09 13:54 configs/llama2/run_llama2_13b_bf16_800T_A2_finetune.yaml
--rw-r--r--  2.0 unx     5272 b- defN 24-May-09 13:54 configs/llama2/run_llama2_13b_lora_910b.yaml
--rw-r--r--  2.0 unx     5324 b- defN 24-May-09 13:54 configs/llama2/run_llama2_70b.yaml
--rw-r--r--  2.0 unx     5334 b- defN 24-May-09 13:54 configs/llama2/run_llama2_70b_910b.yaml
--rw-r--r--  2.0 unx     5751 b- defN 24-May-09 13:54 configs/llama2/run_llama2_70b_910b_auto_parallel.yaml
--rw-r--r--  2.0 unx     5354 b- defN 24-May-09 13:54 configs/llama2/run_llama2_70b_910b_finetune.yaml
--rw-r--r--  2.0 unx     5259 b- defN 24-May-09 13:54 configs/llama2/run_llama2_70b_bf16_800T_A2.yaml
--rw-r--r--  2.0 unx     5265 b- defN 24-May-09 13:54 configs/llama2/run_llama2_7b.yaml
--rw-r--r--  2.0 unx     5100 b- defN 24-May-09 13:54 configs/llama2/run_llama2_7b_910b.yaml
--rw-r--r--  2.0 unx     5484 b- defN 24-May-09 13:54 configs/llama2/run_llama2_7b_910b_auto_parallel.yaml
--rw-r--r--  2.0 unx     5113 b- defN 24-May-09 13:54 configs/llama2/run_llama2_7b_910b_finetune.yaml
--rw-r--r--  2.0 unx     5023 b- defN 24-May-09 13:54 configs/llama2/run_llama2_7b_bf16_800T_A2_finetune.yaml
--rw-r--r--  2.0 unx     5304 b- defN 24-May-09 13:54 configs/llama2/run_llama2_7b_lora_910b.yaml
--rw-r--r--  2.0 unx     4785 b- defN 24-May-09 13:54 configs/mae/run_mae_vit_base_p16_224_800ep.yaml
--rw-r--r--  2.0 unx     4942 b- defN 24-May-09 13:54 configs/pangualpha/run_pangualpha_13b.yaml
--rw-r--r--  2.0 unx     4791 b- defN 24-May-09 13:54 configs/pangualpha/run_pangualpha_2_6b.yaml
--rw-r--r--  2.0 unx     4229 b- defN 24-May-09 13:54 configs/pangualpha/run_pangualpha_2_6b_em_f1.yaml
--rw-r--r--  2.0 unx     4495 b- defN 24-May-09 13:54 configs/pangualpha/run_pangualpha_2_6b_prompt_txtcls.yaml
--rw-r--r--  2.0 unx     5531 b- defN 24-May-09 13:54 configs/qa/run_qa_bert_base_uncased.yaml
--rwxr-xr-x  2.0 unx     6823 b- defN 24-May-09 13:54 configs/sam/run_sam_vit-b.yaml
--rwxr-xr-x  2.0 unx     6826 b- defN 24-May-09 13:54 configs/sam/run_sam_vit-h.yaml
--rw-r--r--  2.0 unx     6826 b- defN 24-May-09 13:54 configs/sam/run_sam_vit-l.yaml
--rw-r--r--  2.0 unx     6149 b- defN 24-May-09 13:54 configs/swin/run_swin_base_p4w7_224_100ep.yaml
--rw-r--r--  2.0 unx     4494 b- defN 24-May-09 13:54 configs/t5/run_t5_small_on_wmt16.yaml
--rw-r--r--  2.0 unx     4455 b- defN 24-May-09 13:54 configs/t5/run_t5_tiny_on_wmt16.yaml
--rw-r--r--  2.0 unx     5772 b- defN 24-May-09 13:54 configs/tokcls/run_tokcls_bert_base_chinese.yaml
--rw-r--r--  2.0 unx     5788 b- defN 24-May-09 13:54 configs/tokcls/run_tokcls_bert_base_chinese_cluener.yaml
--rw-r--r--  2.0 unx     4428 b- defN 24-May-09 13:54 configs/txtcls/run_txtcls_bert_base_uncased.yaml
--rw-r--r--  2.0 unx     4438 b- defN 24-May-09 13:54 configs/txtcls/run_txtcls_bert_base_uncased_mnli.yaml
--rw-r--r--  2.0 unx     6021 b- defN 24-May-09 13:54 configs/vit/run_vit_base_p16_224_100ep.yaml
--r--------  2.0 unx      243 b- defN 24-May-09 13:54 mindformers/.commit_id
--r--------  2.0 unx     1577 b- defN 24-May-09 13:54 mindformers/__init__.py
--r--------  2.0 unx    38831 b- defN 24-May-09 13:54 mindformers/auto_class.py
--r--------  2.0 unx    68882 b- defN 24-May-09 13:54 mindformers/mindformer_book.py
--r--------  2.0 unx    14332 b- defN 24-May-09 13:54 mindformers/model_runner.py
--r--------  2.0 unx    11269 b- defN 24-May-09 13:54 mindformers/version_control.py
--r--------  2.0 unx     1345 b- defN 24-May-09 13:54 mindformers/core/__init__.py
--r--------  2.0 unx     4002 b- defN 24-May-09 13:54 mindformers/core/clip_grad.py
--r--------  2.0 unx     4315 b- defN 24-May-09 13:54 mindformers/core/parallel_config.py
--r--------  2.0 unx      809 b- defN 24-May-09 13:54 mindformers/core/callback/__init__.py
--r--------  2.0 unx     3309 b- defN 24-May-09 13:54 mindformers/core/callback/build_callback.py
--r--------  2.0 unx    46230 b- defN 24-May-09 13:54 mindformers/core/callback/callback.py
--r--------  2.0 unx      795 b- defN 24-May-09 13:54 mindformers/core/context/__init__.py
--r--------  2.0 unx     7754 b- defN 24-May-09 13:54 mindformers/core/context/build_context.py
--r--------  2.0 unx      789 b- defN 24-May-09 13:54 mindformers/core/loss/__init__.py
--r--------  2.0 unx     2866 b- defN 24-May-09 13:54 mindformers/core/loss/build_loss.py
--r--------  2.0 unx    17965 b- defN 24-May-09 13:54 mindformers/core/loss/loss.py
--r--------  2.0 unx      802 b- defN 24-May-09 13:54 mindformers/core/lr/__init__.py
--r--------  2.0 unx     4178 b- defN 24-May-09 13:54 mindformers/core/lr/build_lr.py
--r--------  2.0 unx    24986 b- defN 24-May-09 13:54 mindformers/core/lr/lr_schedule.py
--r--------  2.0 unx      800 b- defN 24-May-09 13:54 mindformers/core/metric/__init__.py
--r--------  2.0 unx     2683 b- defN 24-May-09 13:54 mindformers/core/metric/build_metric.py
--r--------  2.0 unx    35009 b- defN 24-May-09 13:54 mindformers/core/metric/metric.py
--r--------  2.0 unx     1768 b- defN 24-May-09 13:54 mindformers/core/metric/utils.py
--r--------  2.0 unx      847 b- defN 24-May-09 13:54 mindformers/core/optim/__init__.py
--r--------  2.0 unx     4623 b- defN 24-May-09 13:54 mindformers/core/optim/build_optim.py
--r--------  2.0 unx    21317 b- defN 24-May-09 13:54 mindformers/core/optim/came.py
--r--------  2.0 unx    31399 b- defN 24-May-09 13:54 mindformers/core/optim/optim.py
--r--------  2.0 unx     2608 b- defN 24-May-09 13:54 mindformers/dataset/__init__.py
--r--------  2.0 unx     3379 b- defN 24-May-09 13:54 mindformers/dataset/base_dataset.py
--r--------  2.0 unx     2560 b- defN 24-May-09 13:54 mindformers/dataset/build_dataset.py
--r--------  2.0 unx    12910 b- defN 24-May-09 13:54 mindformers/dataset/causal_language_model_dataset.py
--r--------  2.0 unx     9758 b- defN 24-May-09 13:54 mindformers/dataset/contrastive_language_image_pretrain_dataset.py
--r--------  2.0 unx     2998 b- defN 24-May-09 13:54 mindformers/dataset/general_dataset.py
--r--------  2.0 unx     9994 b- defN 24-May-09 13:54 mindformers/dataset/img_cls_dataset.py
--r--------  2.0 unx    21959 b- defN 24-May-09 13:54 mindformers/dataset/keyword_gen_dataset.py
--r--------  2.0 unx    16192 b- defN 24-May-09 13:54 mindformers/dataset/labels.py
--r--------  2.0 unx     8478 b- defN 24-May-09 13:54 mindformers/dataset/mask_language_model_dataset.py
--r--------  2.0 unx     9100 b- defN 24-May-09 13:54 mindformers/dataset/mim_dataset.py
--r--------  2.0 unx     9150 b- defN 24-May-09 13:54 mindformers/dataset/multi_turn_dataset.py
--r--------  2.0 unx     7973 b- defN 24-May-09 13:54 mindformers/dataset/question_answering_dataset.py
--r--------  2.0 unx    13957 b- defN 24-May-09 13:54 mindformers/dataset/reward_model_dataset.py
--r--------  2.0 unx     7515 b- defN 24-May-09 13:54 mindformers/dataset/text_classification_dataset.py
--r--------  2.0 unx     9984 b- defN 24-May-09 13:54 mindformers/dataset/token_classification_dataset.py
--r--------  2.0 unx    11610 b- defN 24-May-09 13:54 mindformers/dataset/translation_dataset.py
--r--------  2.0 unx     2662 b- defN 24-May-09 13:54 mindformers/dataset/utils.py
--r--------  2.0 unx     8502 b- defN 24-May-09 13:54 mindformers/dataset/zero_shot_image_classification_dataset.py
--r--------  2.0 unx     1580 b- defN 24-May-09 13:54 mindformers/dataset/dataloader/__init__.py
--r--------  2.0 unx     6200 b- defN 24-May-09 13:54 mindformers/dataset/dataloader/adgen_dataloader.py
--r--------  2.0 unx     2930 b- defN 24-May-09 13:54 mindformers/dataset/dataloader/build_dataloader.py
--r--------  2.0 unx     7893 b- defN 24-May-09 13:54 mindformers/dataset/dataloader/cifar100_dataloader.py
--r--------  2.0 unx     6961 b- defN 24-May-09 13:54 mindformers/dataset/dataloader/cluener_dataloader.py
--r--------  2.0 unx     5344 b- defN 24-May-09 13:54 mindformers/dataset/dataloader/datareaders.py
--r--------  2.0 unx     7099 b- defN 24-May-09 13:54 mindformers/dataset/dataloader/flickr8k_dataloader.py
--r--------  2.0 unx     6483 b- defN 24-May-09 13:54 mindformers/dataset/dataloader/multi_image_cap_dataloader.py
--r--------  2.0 unx    11335 b- defN 24-May-09 13:54 mindformers/dataset/dataloader/multi_source_dataloader.py
--r--------  2.0 unx    12659 b- defN 24-May-09 13:54 mindformers/dataset/dataloader/sft_dataloader.py
--r--------  2.0 unx     7423 b- defN 24-May-09 13:54 mindformers/dataset/dataloader/sft_map_functions.py
--r--------  2.0 unx    23889 b- defN 24-May-09 13:54 mindformers/dataset/dataloader/squad_dataloader.py
--r--------  2.0 unx     7902 b- defN 24-May-09 13:54 mindformers/dataset/dataloader/toolaplaca_dataloader.py
--r--------  2.0 unx    20483 b- defN 24-May-09 13:54 mindformers/dataset/dataloader/training_dataloader.py
--r--------  2.0 unx     4360 b- defN 24-May-09 13:54 mindformers/dataset/dataloader/wmt16_dataloader.py
--r--------  2.0 unx      808 b- defN 24-May-09 13:54 mindformers/dataset/mask/__init__.py
--r--------  2.0 unx     2196 b- defN 24-May-09 13:54 mindformers/dataset/mask/build_mask.py
--r--------  2.0 unx     3760 b- defN 24-May-09 13:54 mindformers/dataset/mask/vision_mask.py
--r--------  2.0 unx      754 b- defN 24-May-09 13:54 mindformers/dataset/sampler/__init__.py
--r--------  2.0 unx     2719 b- defN 24-May-09 13:54 mindformers/dataset/sampler/build_sampler.py
--r--------  2.0 unx     1192 b- defN 24-May-09 13:54 mindformers/dataset/transforms/__init__.py
--r--------  2.0 unx    33409 b- defN 24-May-09 13:54 mindformers/dataset/transforms/auto_augment.py
--r--------  2.0 unx     3364 b- defN 24-May-09 13:54 mindformers/dataset/transforms/build_transforms.py
--r--------  2.0 unx    11513 b- defN 24-May-09 13:54 mindformers/dataset/transforms/mixup.py
--r--------  2.0 unx     4846 b- defN 24-May-09 13:54 mindformers/dataset/transforms/random_erasing.py
--r--------  2.0 unx     6231 b- defN 24-May-09 13:54 mindformers/dataset/transforms/text_transforms.py
--r--------  2.0 unx    12281 b- defN 24-May-09 13:54 mindformers/dataset/transforms/vision_transforms.py
--r--------  2.0 unx      855 b- defN 24-May-09 13:54 mindformers/experimental/__init__.py
--r--------  2.0 unx      787 b- defN 24-May-09 13:54 mindformers/experimental/distri_ckpt/__init__.py
--r--------  2.0 unx     7136 b- defN 24-May-09 13:54 mindformers/experimental/distri_ckpt/checkpointing.py
--r--------  2.0 unx      786 b- defN 24-May-09 13:54 mindformers/experimental/distri_cores/__init__.py
--r--------  2.0 unx      712 b- defN 24-May-09 13:54 mindformers/experimental/distri_cores/activation_checkpointing/__init__.py
--r--------  2.0 unx      776 b- defN 24-May-09 13:54 mindformers/experimental/distri_cores/optimizer/__init__.py
--r--------  2.0 unx      712 b- defN 24-May-09 13:54 mindformers/experimental/distri_cores/optimizer/grads_accumulate/__init__.py
--r--------  2.0 unx      789 b- defN 24-May-09 13:54 mindformers/experimental/distri_cores/optimizer/zero/__init__.py
--r--------  2.0 unx    14836 b- defN 24-May-09 13:54 mindformers/experimental/distri_cores/optimizer/zero/adamw.py
--r--------  2.0 unx      712 b- defN 24-May-09 13:54 mindformers/experimental/distri_cores/pipeline_parallel/__init__.py
--r--------  2.0 unx      712 b- defN 24-May-09 13:54 mindformers/experimental/distri_cores/sequence_parallel/__init__.py
--r--------  2.0 unx    19713 b- defN 24-May-09 13:54 mindformers/experimental/distri_cores/sequence_parallel/ring_attention.py
--r--------  2.0 unx     6853 b- defN 24-May-09 13:54 mindformers/experimental/distri_cores/sequence_parallel/utils.py
--r--------  2.0 unx      712 b- defN 24-May-09 13:54 mindformers/experimental/distri_cores/tensor_parallel/__init__.py
--r--------  2.0 unx      984 b- defN 24-May-09 13:54 mindformers/generation/__init__.py
--r--------  2.0 unx    18947 b- defN 24-May-09 13:54 mindformers/generation/beam_search.py
--r--------  2.0 unx     9241 b- defN 24-May-09 13:54 mindformers/generation/generation_config.py
--r--------  2.0 unx    14147 b- defN 24-May-09 13:54 mindformers/generation/logits_process.py
--r--------  2.0 unx    11881 b- defN 24-May-09 13:54 mindformers/generation/streamers.py
--r--------  2.0 unx    57851 b- defN 24-May-09 13:54 mindformers/generation/text_generator.py
--r--------  2.0 unx     2956 b- defN 24-May-09 13:54 mindformers/generation/utils.py
--r--------  2.0 unx     2354 b- defN 24-May-09 13:54 mindformers/models/__init__.py
--r--------  2.0 unx    10530 b- defN 24-May-09 13:54 mindformers/models/base_config.py
--r--------  2.0 unx    16983 b- defN 24-May-09 13:54 mindformers/models/base_model.py
--r--------  2.0 unx    12083 b- defN 24-May-09 13:54 mindformers/models/base_processor.py
--r--------  2.0 unx     2681 b- defN 24-May-09 13:54 mindformers/models/build_config.py
--r--------  2.0 unx     5053 b- defN 24-May-09 13:54 mindformers/models/build_model.py
--r--------  2.0 unx     2461 b- defN 24-May-09 13:54 mindformers/models/build_processor.py
--r--------  2.0 unx     3570 b- defN 24-May-09 13:54 mindformers/models/build_tokenizer.py
--r--------  2.0 unx    35454 b- defN 24-May-09 13:54 mindformers/models/configuration_utils.py
--r--------  2.0 unx    16629 b- defN 24-May-09 13:54 mindformers/models/convert_slow_tokenizer.py
--r--------  2.0 unx    32672 b- defN 24-May-09 13:54 mindformers/models/image_processing_utils.py
--r--------  2.0 unx    70053 b- defN 24-May-09 13:54 mindformers/models/modeling_utils.py
--r--------  2.0 unx    21177 b- defN 24-May-09 13:54 mindformers/models/processing_utils.py
--r--------  2.0 unx    50997 b- defN 24-May-09 13:54 mindformers/models/sentencepiece_model_pb2.py
--r--------  2.0 unx     6645 b- defN 24-May-09 13:54 mindformers/models/sentencepiece_model_pb2_new.py
--r--------  2.0 unx    47092 b- defN 24-May-09 13:54 mindformers/models/tokenization_utils.py
--r--------  2.0 unx   210149 b- defN 24-May-09 13:54 mindformers/models/tokenization_utils_base.py
--r--------  2.0 unx    38916 b- defN 24-May-09 13:54 mindformers/models/tokenization_utils_fast.py
--r--------  2.0 unx     4969 b- defN 24-May-09 13:54 mindformers/models/utils.py
--r--------  2.0 unx     2334 b- defN 24-May-09 13:54 mindformers/models/auto/__init__.py
--r--------  2.0 unx    42029 b- defN 24-May-09 13:54 mindformers/models/auto/auto_factory.py
--r--------  2.0 unx    19644 b- defN 24-May-09 13:54 mindformers/models/auto/configuration_auto.py
--r--------  2.0 unx    17407 b- defN 24-May-09 13:54 mindformers/models/auto/image_processing_auto.py
--r--------  2.0 unx    11169 b- defN 24-May-09 13:54 mindformers/models/auto/modeling_auto.py
--r--------  2.0 unx    18227 b- defN 24-May-09 13:54 mindformers/models/auto/processing_auto.py
--r--------  2.0 unx    32868 b- defN 24-May-09 13:54 mindformers/models/auto/tokenization_auto.py
--r--------  2.0 unx     1191 b- defN 24-May-09 13:54 mindformers/models/bert/__init__.py
--r--------  2.0 unx    29699 b- defN 24-May-09 13:54 mindformers/models/bert/bert.py
--r--------  2.0 unx     8583 b- defN 24-May-09 13:54 mindformers/models/bert/bert_config.py
--r--------  2.0 unx     3764 b- defN 24-May-09 13:54 mindformers/models/bert/bert_processor.py
--r--------  2.0 unx    25732 b- defN 24-May-09 13:54 mindformers/models/bert/bert_tokenizer.py
--r--------  2.0 unx     8228 b- defN 24-May-09 13:54 mindformers/models/bert/bert_tokenizer_fast.py
--r--------  2.0 unx     8638 b- defN 24-May-09 13:54 mindformers/models/bert/convert_weight.py
--r--------  2.0 unx     1189 b- defN 24-May-09 13:54 mindformers/models/blip2/__init__.py
--r--------  2.0 unx     4045 b- defN 24-May-09 13:54 mindformers/models/blip2/blip2.py
--r--------  2.0 unx     7102 b- defN 24-May-09 13:54 mindformers/models/blip2/blip2_config.py
--r--------  2.0 unx     9582 b- defN 24-May-09 13:54 mindformers/models/blip2/blip2_itm_evaluator.py
--r--------  2.0 unx     7861 b- defN 24-May-09 13:54 mindformers/models/blip2/blip2_llama.py
--r--------  2.0 unx    11459 b- defN 24-May-09 13:54 mindformers/models/blip2/blip2_llm.py
--r--------  2.0 unx     8689 b- defN 24-May-09 13:54 mindformers/models/blip2/blip2_processor.py
--r--------  2.0 unx    23783 b- defN 24-May-09 13:54 mindformers/models/blip2/blip2_qformer.py
--r--------  2.0 unx     1501 b- defN 24-May-09 13:54 mindformers/models/blip2/blip2_vit.py
--r--------  2.0 unx     4365 b- defN 24-May-09 13:54 mindformers/models/blip2/convert_reversed.py
--r--------  2.0 unx     5482 b- defN 24-May-09 13:54 mindformers/models/blip2/convert_weight.py
--r--------  2.0 unx     4115 b- defN 24-May-09 13:54 mindformers/models/blip2/layers.py
--r--------  2.0 unx    70439 b- defN 24-May-09 13:54 mindformers/models/blip2/qformer.py
--r--------  2.0 unx     5191 b- defN 24-May-09 13:54 mindformers/models/blip2/qformer_config.py
--r--------  2.0 unx     1137 b- defN 24-May-09 13:54 mindformers/models/bloom/__init__.py
--r--------  2.0 unx    17832 b- defN 24-May-09 13:54 mindformers/models/bloom/bloom.py
--r--------  2.0 unx     9617 b- defN 24-May-09 13:54 mindformers/models/bloom/bloom_config.py
--r--------  2.0 unx     3632 b- defN 24-May-09 13:54 mindformers/models/bloom/bloom_processor.py
--r--------  2.0 unx     4576 b- defN 24-May-09 13:54 mindformers/models/bloom/bloom_reward.py
--r--------  2.0 unx    10525 b- defN 24-May-09 13:54 mindformers/models/bloom/bloom_tokenizer.py
--r--------  2.0 unx     6814 b- defN 24-May-09 13:54 mindformers/models/bloom/bloom_tokenizer_fast.py
--r--------  2.0 unx     5263 b- defN 24-May-09 13:54 mindformers/models/bloom/convert_reversed.py
--r--------  2.0 unx     5668 b- defN 24-May-09 13:54 mindformers/models/bloom/convert_weight.py
--r--------  2.0 unx    32828 b- defN 24-May-09 13:54 mindformers/models/bloom/layers.py
--r--------  2.0 unx     1030 b- defN 24-May-09 13:54 mindformers/models/clip/__init__.py
--r--------  2.0 unx    10413 b- defN 24-May-09 13:54 mindformers/models/clip/clip.py
--r--------  2.0 unx     9192 b- defN 24-May-09 13:54 mindformers/models/clip/clip_config.py
--r--------  2.0 unx    10123 b- defN 24-May-09 13:54 mindformers/models/clip/clip_modules.py
--r--------  2.0 unx     6379 b- defN 24-May-09 13:54 mindformers/models/clip/clip_processor.py
--r--------  2.0 unx    11888 b- defN 24-May-09 13:54 mindformers/models/clip/clip_tokenizer.py
--r--------  2.0 unx     2989 b- defN 24-May-09 13:54 mindformers/models/clip/convert_weight.py
--r--------  2.0 unx     1036 b- defN 24-May-09 13:54 mindformers/models/glm/__init__.py
--r--------  2.0 unx    20335 b- defN 24-May-09 13:54 mindformers/models/glm/attention.py
--r--------  2.0 unx    16157 b- defN 24-May-09 13:54 mindformers/models/glm/chatglm_6b_tokenizer.py
--r--------  2.0 unx     2308 b- defN 24-May-09 13:54 mindformers/models/glm/convert_reversed.py
--r--------  2.0 unx     2458 b- defN 24-May-09 13:54 mindformers/models/glm/convert_weight.py
--r--------  2.0 unx    23408 b- defN 24-May-09 13:54 mindformers/models/glm/glm.py
--r--------  2.0 unx    12201 b- defN 24-May-09 13:54 mindformers/models/glm/glm_config.py
--r--------  2.0 unx     4446 b- defN 24-May-09 13:54 mindformers/models/glm/glm_processor.py
--r--------  2.0 unx    13117 b- defN 24-May-09 13:54 mindformers/models/glm/layers.py
--r--------  2.0 unx      901 b- defN 24-May-09 13:54 mindformers/models/glm2/__init__.py
--r--------  2.0 unx     2069 b- defN 24-May-09 13:54 mindformers/models/glm2/convert_reversed.py
--r--------  2.0 unx     2172 b- defN 24-May-09 13:54 mindformers/models/glm2/convert_weight.py
--r--------  2.0 unx    15785 b- defN 24-May-09 13:54 mindformers/models/glm2/glm2.py
--r--------  2.0 unx     6147 b- defN 24-May-09 13:54 mindformers/models/glm2/glm2_config.py
--r--------  2.0 unx     7779 b- defN 24-May-09 13:54 mindformers/models/glm2/glm2_modules.py
--r--------  2.0 unx    10952 b- defN 24-May-09 13:54 mindformers/models/glm2/glm2_tokenizer.py
--r--------  2.0 unx    25284 b- defN 24-May-09 13:54 mindformers/models/glm2/glm2_transformer.py
--r--------  2.0 unx      789 b- defN 24-May-09 13:54 mindformers/models/glm3/__init__.py
--r--------  2.0 unx    17380 b- defN 24-May-09 13:54 mindformers/models/glm3/glm3_tokenizer.py
--r--------  2.0 unx     1106 b- defN 24-May-09 13:54 mindformers/models/gpt2/__init__.py
--r--------  2.0 unx     5661 b- defN 24-May-09 13:54 mindformers/models/gpt2/convert_reversed.py
--r--------  2.0 unx     7211 b- defN 24-May-09 13:54 mindformers/models/gpt2/convert_weight.py
--r--------  2.0 unx    28509 b- defN 24-May-09 13:54 mindformers/models/gpt2/gpt2.py
--r--------  2.0 unx     9790 b- defN 24-May-09 13:54 mindformers/models/gpt2/gpt2_config.py
--r--------  2.0 unx     3629 b- defN 24-May-09 13:54 mindformers/models/gpt2/gpt2_processor.py
--r--------  2.0 unx    14404 b- defN 24-May-09 13:54 mindformers/models/gpt2/gpt2_tokenizer.py
--r--------  2.0 unx     9082 b- defN 24-May-09 13:54 mindformers/models/gpt2/gpt2_tokenizer_fast.py
--r--------  2.0 unx    11209 b- defN 24-May-09 13:54 mindformers/models/gpt2/gpt_modules.py
--r--------  2.0 unx     1061 b- defN 24-May-09 13:54 mindformers/models/llama/__init__.py
--r--------  2.0 unx     2833 b- defN 24-May-09 13:54 mindformers/models/llama/convert_reversed.py
--r--------  2.0 unx     7035 b- defN 24-May-09 13:54 mindformers/models/llama/convert_weight.py
--r--------  2.0 unx    21015 b- defN 24-May-09 13:54 mindformers/models/llama/llama.py
--r--------  2.0 unx    11794 b- defN 24-May-09 13:54 mindformers/models/llama/llama_config.py
--r--------  2.0 unx    34134 b- defN 24-May-09 13:54 mindformers/models/llama/llama_interleave.py
--r--------  2.0 unx    20921 b- defN 24-May-09 13:54 mindformers/models/llama/llama_layer.py
--r--------  2.0 unx    13664 b- defN 24-May-09 13:54 mindformers/models/llama/llama_moe.py
--r--------  2.0 unx     3641 b- defN 24-May-09 13:54 mindformers/models/llama/llama_processor.py
--r--------  2.0 unx    17126 b- defN 24-May-09 13:54 mindformers/models/llama/llama_tokenizer.py
--r--------  2.0 unx     9130 b- defN 24-May-09 13:54 mindformers/models/llama/llama_tokenizer_fast.py
--r--------  2.0 unx    29001 b- defN 24-May-09 13:54 mindformers/models/llama/llama_transformer.py
--r--------  2.0 unx      878 b- defN 24-May-09 13:54 mindformers/models/mae/__init__.py
--r--------  2.0 unx     3416 b- defN 24-May-09 13:54 mindformers/models/mae/convert_weight.py
--r--------  2.0 unx    18363 b- defN 24-May-09 13:54 mindformers/models/mae/mae.py
--r--------  2.0 unx     7851 b- defN 24-May-09 13:54 mindformers/models/mae/mae_config.py
--r--------  2.0 unx    36274 b- defN 24-May-09 13:54 mindformers/models/mae/mae_modules.py
--r--------  2.0 unx     8056 b- defN 24-May-09 13:54 mindformers/models/mae/mae_processor.py
--r--------  2.0 unx     1022 b- defN 24-May-09 13:54 mindformers/models/pangualpha/__init__.py
--r--------  2.0 unx     5469 b- defN 24-May-09 13:54 mindformers/models/pangualpha/convert_weight.py
--r--------  2.0 unx    28704 b- defN 24-May-09 13:54 mindformers/models/pangualpha/pangualpha.py
--r--------  2.0 unx     4905 b- defN 24-May-09 13:54 mindformers/models/pangualpha/pangualpha_config.py
--r--------  2.0 unx     2523 b- defN 24-May-09 13:54 mindformers/models/pangualpha/pangualpha_processor.py
--r--------  2.0 unx     8201 b- defN 24-May-09 13:54 mindformers/models/pangualpha/pangualpha_tokenizer.py
--r--------  2.0 unx     1190 b- defN 24-May-09 13:54 mindformers/models/sam/__init__.py
--r--------  2.0 unx     3489 b- defN 24-May-09 13:54 mindformers/models/sam/conver_weight.py
--r--------  2.0 unx     6289 b- defN 24-May-09 13:54 mindformers/models/sam/sam.py
--r--------  2.0 unx     8641 b- defN 24-May-09 13:54 mindformers/models/sam/sam_config.py
--r--------  2.0 unx    17601 b- defN 24-May-09 13:54 mindformers/models/sam/sam_image_encoder.py
--r--------  2.0 unx     2944 b- defN 24-May-09 13:54 mindformers/models/sam/sam_layers.py
--r--------  2.0 unx    22840 b- defN 24-May-09 13:54 mindformers/models/sam/sam_mask_decoder.py
--r--------  2.0 unx    10277 b- defN 24-May-09 13:54 mindformers/models/sam/sam_processor.py
--r--------  2.0 unx    10597 b- defN 24-May-09 13:54 mindformers/models/sam/sam_prompt_encoder.py
--r--------  2.0 unx    22757 b- defN 24-May-09 13:54 mindformers/models/sam/sam_utils.py
--r--------  2.0 unx      885 b- defN 24-May-09 13:54 mindformers/models/swin/__init__.py
--r--------  2.0 unx     4949 b- defN 24-May-09 13:54 mindformers/models/swin/convert_weight.py
--r--------  2.0 unx    16602 b- defN 24-May-09 13:54 mindformers/models/swin/swin.py
--r--------  2.0 unx     8141 b- defN 24-May-09 13:54 mindformers/models/swin/swin_config.py
--r--------  2.0 unx    31100 b- defN 24-May-09 13:54 mindformers/models/swin/swin_modules.py
--r--------  2.0 unx     7139 b- defN 24-May-09 13:54 mindformers/models/swin/swin_processor.py
--r--------  2.0 unx     1192 b- defN 24-May-09 13:54 mindformers/models/t5/__init__.py
--r--------  2.0 unx     8077 b- defN 24-May-09 13:54 mindformers/models/t5/convert_weight.py
--r--------  2.0 unx    10956 b- defN 24-May-09 13:54 mindformers/models/t5/mt5.py
--r--------  2.0 unx    90581 b- defN 24-May-09 13:54 mindformers/models/t5/t5.py
--r--------  2.0 unx    12127 b- defN 24-May-09 13:54 mindformers/models/t5/t5_config.py
--r--------  2.0 unx     4350 b- defN 24-May-09 13:54 mindformers/models/t5/t5_processor.py
--r--------  2.0 unx    20272 b- defN 24-May-09 13:54 mindformers/models/t5/t5_tokenizer.py
--r--------  2.0 unx     9885 b- defN 24-May-09 13:54 mindformers/models/t5/t5_tokenizer_fast.py
--r--------  2.0 unx      948 b- defN 24-May-09 13:54 mindformers/models/vit/__init__.py
--r--------  2.0 unx     3364 b- defN 24-May-09 13:54 mindformers/models/vit/convert_weight.py
--r--------  2.0 unx    13786 b- defN 24-May-09 13:54 mindformers/models/vit/vit.py
--r--------  2.0 unx     8809 b- defN 24-May-09 13:54 mindformers/models/vit/vit_config.py
--r--------  2.0 unx    37548 b- defN 24-May-09 13:54 mindformers/models/vit/vit_modules.py
--r--------  2.0 unx     6797 b- defN 24-May-09 13:54 mindformers/models/vit/vit_processor.py
--r--------  2.0 unx      992 b- defN 24-May-09 13:54 mindformers/modules/__init__.py
--r--------  2.0 unx    50360 b- defN 24-May-09 13:54 mindformers/modules/activation.py
--r--------  2.0 unx     6108 b- defN 24-May-09 13:54 mindformers/modules/block_tables.py
--r--------  2.0 unx     3234 b- defN 24-May-09 13:54 mindformers/modules/cache_engine.py
--r--------  2.0 unx    12130 b- defN 24-May-09 13:54 mindformers/modules/flash_attention.py
--r--------  2.0 unx    16303 b- defN 24-May-09 13:54 mindformers/modules/infer_attention.py
--r--------  2.0 unx    13108 b- defN 24-May-09 13:54 mindformers/modules/kvcache_mgr.py
--r--------  2.0 unx    47993 b- defN 24-May-09 13:54 mindformers/modules/layers.py
--r--------  2.0 unx    14414 b- defN 24-May-09 13:54 mindformers/modules/local_block_sparse_attention.py
--r--------  2.0 unx     3810 b- defN 24-May-09 13:54 mindformers/modules/paged_attention_mgr.py
--r--------  2.0 unx     1335 b- defN 24-May-09 13:54 mindformers/modules/transformer/__init__.py
--r--------  2.0 unx    72609 b- defN 24-May-09 13:54 mindformers/modules/transformer/moe.py
--r--------  2.0 unx     9066 b- defN 24-May-09 13:54 mindformers/modules/transformer/op_parallel_config.py
--r--------  2.0 unx   213326 b- defN 24-May-09 13:54 mindformers/modules/transformer/transformer.py
--r--------  2.0 unx      897 b- defN 24-May-09 13:54 mindformers/pet/__init__.py
--r--------  2.0 unx     1176 b- defN 24-May-09 13:54 mindformers/pet/constants.py
--r--------  2.0 unx     4907 b- defN 24-May-09 13:54 mindformers/pet/pet_config.py
--r--------  2.0 unx     4376 b- defN 24-May-09 13:54 mindformers/pet/pet_model.py
--r--------  2.0 unx     1096 b- defN 24-May-09 13:54 mindformers/pet/utils.py
--r--------  2.0 unx      696 b- defN 24-May-09 13:54 mindformers/pet/models/__init__.py
--r--------  2.0 unx     3988 b- defN 24-May-09 13:54 mindformers/pet/models/lora.py
--r--------  2.0 unx      955 b- defN 24-May-09 13:54 mindformers/pet/tuners/__init__.py
--r--------  2.0 unx     1176 b- defN 24-May-09 13:54 mindformers/pet/tuners/ada_adapter.py
--r--------  2.0 unx     1194 b- defN 24-May-09 13:54 mindformers/pet/tuners/adalora_adapter.py
--r--------  2.0 unx     5497 b- defN 24-May-09 13:54 mindformers/pet/tuners/lora_adapter.py
--r--------  2.0 unx     1649 b- defN 24-May-09 13:54 mindformers/pet/tuners/pet_adapter.py
--r--------  2.0 unx     1165 b- defN 24-May-09 13:54 mindformers/pet/tuners/prefix_tuning_adapter.py
--r--------  2.0 unx     1470 b- defN 24-May-09 13:54 mindformers/pet/tuners/ptuning2_adapter.py
--r--------  2.0 unx     2142 b- defN 24-May-09 13:54 mindformers/pipeline/__init__.py
--r--------  2.0 unx    13012 b- defN 24-May-09 13:54 mindformers/pipeline/base_pipeline.py
--r--------  2.0 unx     2393 b- defN 24-May-09 13:54 mindformers/pipeline/build_pipeline.py
--r--------  2.0 unx     5452 b- defN 24-May-09 13:54 mindformers/pipeline/fill_mask_pipeline.py
--r--------  2.0 unx     6648 b- defN 24-May-09 13:54 mindformers/pipeline/image_classification_pipeline.py
--r--------  2.0 unx     6249 b- defN 24-May-09 13:54 mindformers/pipeline/image_to_text_generation_pipeline.py
--r--------  2.0 unx     5695 b- defN 24-May-09 13:54 mindformers/pipeline/masked_image_modeling_pipeline.py
--r--------  2.0 unx    27730 b- defN 24-May-09 13:54 mindformers/pipeline/pipeline.py
--r--------  2.0 unx     3369 b- defN 24-May-09 13:54 mindformers/pipeline/pipeline_registry.py
--r--------  2.0 unx    17599 b- defN 24-May-09 13:54 mindformers/pipeline/question_answering_pipeline.py
--r--------  2.0 unx     5728 b- defN 24-May-09 13:54 mindformers/pipeline/registry_constant.py
--r--------  2.0 unx    25379 b- defN 24-May-09 13:54 mindformers/pipeline/segment_anything_pipeline.py
--r--------  2.0 unx     9668 b- defN 24-May-09 13:54 mindformers/pipeline/text_classification_pipeline.py
--r--------  2.0 unx     9744 b- defN 24-May-09 13:54 mindformers/pipeline/text_generation_pipeline.py
--r--------  2.0 unx     9160 b- defN 24-May-09 13:54 mindformers/pipeline/token_classification_pipeline.py
--r--------  2.0 unx     7484 b- defN 24-May-09 13:54 mindformers/pipeline/translation_pipeline.py
--r--------  2.0 unx     8281 b- defN 24-May-09 13:54 mindformers/pipeline/zero_shot_image_classification_pipeline.py
--r--------  2.0 unx     1194 b- defN 24-May-09 13:54 mindformers/tools/__init__.py
--r--------  2.0 unx    10734 b- defN 24-May-09 13:54 mindformers/tools/check_rules.py
--r--------  2.0 unx     4399 b- defN 24-May-09 13:54 mindformers/tools/download_tools.py
--r--------  2.0 unx     6243 b- defN 24-May-09 13:54 mindformers/tools/download_tools_multithread.py
--r--------  2.0 unx     3029 b- defN 24-May-09 13:54 mindformers/tools/generic.py
--r--------  2.0 unx     6862 b- defN 24-May-09 13:54 mindformers/tools/hccl_tools.py
--r--------  2.0 unx     1925 b- defN 24-May-09 13:54 mindformers/tools/image_tools.py
--r--------  2.0 unx    23336 b- defN 24-May-09 13:54 mindformers/tools/logger.py
--r--------  2.0 unx     2316 b- defN 24-May-09 13:54 mindformers/tools/merge_hccl.py
--r--------  2.0 unx     5653 b- defN 24-May-09 13:54 mindformers/tools/moe_token_distribution_tools.py
--r--------  2.0 unx     3164 b- defN 24-May-09 13:54 mindformers/tools/transform_ckpt.py
--r--------  2.0 unx     5383 b- defN 24-May-09 13:54 mindformers/tools/transform_ckpt_lora.py
--r--------  2.0 unx    13940 b- defN 24-May-09 13:54 mindformers/tools/utils.py
--r--------  2.0 unx      842 b- defN 24-May-09 13:54 mindformers/tools/cloud_adapter/__init__.py
--r--------  2.0 unx     8522 b- defN 24-May-09 13:54 mindformers/tools/cloud_adapter/cloud_adapter.py
--r--------  2.0 unx     3464 b- defN 24-May-09 13:54 mindformers/tools/cloud_adapter/cloud_monitor.py
--r--------  2.0 unx      152 b- defN 24-May-09 13:54 mindformers/tools/hub/__init__.py
--r--------  2.0 unx    27730 b- defN 24-May-09 13:54 mindformers/tools/hub/dynamic_module_utils.py
--r--------  2.0 unx    32184 b- defN 24-May-09 13:54 mindformers/tools/hub/hub.py
--r--------  2.0 unx      942 b- defN 24-May-09 13:54 mindformers/tools/register/__init__.py
--r--------  2.0 unx    11052 b- defN 24-May-09 13:54 mindformers/tools/register/config.py
--r--------  2.0 unx     7056 b- defN 24-May-09 13:54 mindformers/tools/register/register.py
--r--------  2.0 unx     1982 b- defN 24-May-09 13:54 mindformers/trainer/__init__.py
--r--------  2.0 unx    50692 b- defN 24-May-09 13:54 mindformers/trainer/base_trainer.py
--r--------  2.0 unx     4428 b- defN 24-May-09 13:54 mindformers/trainer/build_trainer.py
--r--------  2.0 unx    55851 b- defN 24-May-09 13:54 mindformers/trainer/config_args.py
--r--------  2.0 unx     5949 b- defN 24-May-09 13:54 mindformers/trainer/optimizer_grouped_parameters.py
--r--------  2.0 unx    69793 b- defN 24-May-09 13:54 mindformers/trainer/trainer.py
--r--------  2.0 unx    82738 b- defN 24-May-09 13:54 mindformers/trainer/training_args.py
--r--------  2.0 unx    35540 b- defN 24-May-09 13:54 mindformers/trainer/utils.py
--r--------  2.0 unx      821 b- defN 24-May-09 13:54 mindformers/trainer/causal_language_modeling/__init__.py
--r--------  2.0 unx    21478 b- defN 24-May-09 13:54 mindformers/trainer/causal_language_modeling/causal_language_modeling.py
--r--------  2.0 unx      866 b- defN 24-May-09 13:54 mindformers/trainer/contrastive_language_image_pretrain/__init__.py
--r--------  2.0 unx     4480 b- defN 24-May-09 13:54 mindformers/trainer/contrastive_language_image_pretrain/contrastive_language_image_pretrain.py
--r--------  2.0 unx      795 b- defN 24-May-09 13:54 mindformers/trainer/general_task_trainer/__init__.py
--r--------  2.0 unx     8976 b- defN 24-May-09 13:54 mindformers/trainer/general_task_trainer/general_task_trainer.py
--r--------  2.0 unx      928 b- defN 24-May-09 13:54 mindformers/trainer/image_classification/__init__.py
--r--------  2.0 unx     4622 b- defN 24-May-09 13:54 mindformers/trainer/image_classification/group_ic_params.py
--r--------  2.0 unx     9361 b- defN 24-May-09 13:54 mindformers/trainer/image_classification/image_classification.py
--r--------  2.0 unx     8106 b- defN 24-May-09 13:54 mindformers/trainer/image_classification/zero_shot_image_classification.py
--r--------  2.0 unx      823 b- defN 24-May-09 13:54 mindformers/trainer/image_to_text_generation/__init__.py
--r--------  2.0 unx     5982 b- defN 24-May-09 13:54 mindformers/trainer/image_to_text_generation/image_to_text_generation.py
--r--------  2.0 unx      818 b- defN 24-May-09 13:54 mindformers/trainer/image_to_text_retrieval/__init__.py
--r--------  2.0 unx    12082 b- defN 24-May-09 13:54 mindformers/trainer/image_to_text_retrieval/eval_utils.py
--r--------  2.0 unx     7317 b- defN 24-May-09 13:54 mindformers/trainer/image_to_text_retrieval/image_to_text_retrieval.py
--r--------  2.0 unx      818 b- defN 24-May-09 13:54 mindformers/trainer/masked_image_modeling/__init__.py
--r--------  2.0 unx     2197 b- defN 24-May-09 13:54 mindformers/trainer/masked_image_modeling/group_mim_parameters.py
--r--------  2.0 unx     7482 b- defN 24-May-09 13:54 mindformers/trainer/masked_image_modeling/masked_image_modeling_pretrain.py
--r--------  2.0 unx      830 b- defN 24-May-09 13:54 mindformers/trainer/masked_language_modeling/__init__.py
--r--------  2.0 unx     6931 b- defN 24-May-09 13:54 mindformers/trainer/masked_language_modeling/masked_language_modeling_pretrain.py
--r--------  2.0 unx      799 b- defN 24-May-09 13:54 mindformers/trainer/question_answering/__init__.py
--r--------  2.0 unx     9308 b- defN 24-May-09 13:54 mindformers/trainer/question_answering/question_answering.py
--r--------  2.0 unx      803 b- defN 24-May-09 13:54 mindformers/trainer/text_classfication/__init__.py
--r--------  2.0 unx     9476 b- defN 24-May-09 13:54 mindformers/trainer/text_classfication/text_classification.py
--r--------  2.0 unx      807 b- defN 24-May-09 13:54 mindformers/trainer/token_classification/__init__.py
--r--------  2.0 unx     9590 b- defN 24-May-09 13:54 mindformers/trainer/token_classification/token_classification.py
--r--------  2.0 unx      795 b- defN 24-May-09 13:54 mindformers/trainer/translation/__init__.py
--r--------  2.0 unx     6810 b- defN 24-May-09 13:54 mindformers/trainer/translation/translation_finetune.py
--r--------  2.0 unx      122 b- defN 24-May-09 13:54 mindformers/utils/__init__.py
--r--------  2.0 unx     1734 b- defN 24-May-09 13:54 mindformers/utils/convert_utils.py
--r--------  2.0 unx    14764 b- defN 24-May-09 13:54 mindformers/utils/image_transforms.py
--r--------  2.0 unx     3153 b- defN 24-May-09 13:54 mindformers/utils/image_utils.py
--r--------  2.0 unx     1739 b- defN 24-May-09 13:54 mindformers/utils/import_utils.py
--r--------  2.0 unx      913 b- defN 24-May-09 13:54 mindformers/wrapper/__init__.py
--r--------  2.0 unx    14329 b- defN 24-May-09 13:54 mindformers/wrapper/adaptive_loss_scale.py
--r--------  2.0 unx     4117 b- defN 24-May-09 13:54 mindformers/wrapper/build_wrapper.py
--r--------  2.0 unx    12419 b- defN 24-May-09 13:54 mindformers/wrapper/wrapper.py
--rw-r--r--  2.0 unx    11357 b- defN 24-May-09 13:54 mindformers-1.1.0.dist-info/LICENSE
--rw-r--r--  2.0 unx    22642 b- defN 24-May-09 13:54 mindformers-1.1.0.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-May-09 13:54 mindformers-1.1.0.dist-info/WHEEL
--r--------  2.0 unx       12 b- defN 24-May-09 13:54 mindformers-1.1.0.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx    45027 b- defN 24-May-09 13:54 mindformers-1.1.0.dist-info/RECORD
-455 files, 4951540 bytes uncompressed, 1266642 bytes compressed:  74.4%
+Zip file size: 1337854 bytes, number of entries: 453
+-rw-r--r--  2.0 unx    15518 b- defN 24-Apr-12 09:49 configs/README.md
+-rw-r--r--  2.0 unx     4241 b- defN 24-Apr-12 09:49 configs/bert/run_bert_base_uncased.yaml
+-rw-r--r--  2.0 unx     4257 b- defN 24-Apr-12 09:49 configs/bert/run_bert_tiny_uncased.yaml
+-rw-r--r--  2.0 unx     6074 b- defN 24-Apr-12 09:49 configs/blip2/run_blip2_stage1_vit_g_qformer_pretrain.yaml
+-rw-r--r--  2.0 unx     6139 b- defN 24-Apr-12 09:49 configs/blip2/run_blip2_stage1_vit_g_retrieval_flickr30k.yaml
+-rw-r--r--  2.0 unx     4951 b- defN 24-Apr-12 09:49 configs/blip2/run_blip2_stage1_vit_g_zero_shot_image_classification_cifar100.yaml
+-rw-r--r--  2.0 unx     6214 b- defN 24-Apr-12 09:49 configs/blip2/run_blip2_stage2_vit_g_baichuan_7b.yaml
+-rw-r--r--  2.0 unx     4705 b- defN 24-Apr-12 09:49 configs/blip2/run_blip2_stage2_vit_g_baichuan_7b_image_to_text_generation.yaml
+-rw-r--r--  2.0 unx     6272 b- defN 24-Apr-12 09:49 configs/blip2/run_blip2_stage2_vit_g_llama_7b.yaml
+-rw-r--r--  2.0 unx     6272 b- defN 24-Apr-12 09:49 configs/blip2/run_blip2_stage2_vit_g_llama_7b_910b.yaml
+-rw-r--r--  2.0 unx     4761 b- defN 24-Apr-12 09:49 configs/blip2/run_blip2_stage2_vit_g_llama_7b_image_to_text_generation.yaml
+-rw-r--r--  2.0 unx     4055 b- defN 24-Apr-12 09:49 configs/bloom/run_bloom_560m.yaml
+-rw-r--r--  2.0 unx     4054 b- defN 24-Apr-12 09:49 configs/bloom/run_bloom_7.1b.yaml
+-rw-r--r--  2.0 unx     4138 b- defN 24-Apr-12 09:49 configs/bloom/run_bloom_7.1b_910b.yaml
+-rw-r--r--  2.0 unx     4156 b- defN 24-Apr-12 09:49 configs/bloom/run_bloom_7.1b_910b_fa.yaml
+-rw-r--r--  2.0 unx     4137 b- defN 24-Apr-12 09:49 configs/clip/run_clip_vit_b_16_pretrain_flickr8k.yaml
+-rw-r--r--  2.0 unx     4285 b- defN 24-Apr-12 09:49 configs/clip/run_clip_vit_b_16_zero_shot_image_classification_cifar100.yaml
+-rw-r--r--  2.0 unx     4138 b- defN 24-Apr-12 09:49 configs/clip/run_clip_vit_b_32_pretrain_flickr8k.yaml
+-rw-r--r--  2.0 unx     4285 b- defN 24-Apr-12 09:49 configs/clip/run_clip_vit_b_32_zero_shot_image_classification_cifar100.yaml
+-rw-r--r--  2.0 unx     4148 b- defN 24-Apr-12 09:49 configs/clip/run_clip_vit_l_14@336_pretrain_flickr8k.yaml
+-rw-r--r--  2.0 unx     4295 b- defN 24-Apr-12 09:49 configs/clip/run_clip_vit_l_14@336_zero_shot_image_classification_cifar100.yaml
+-rw-r--r--  2.0 unx     4139 b- defN 24-Apr-12 09:49 configs/clip/run_clip_vit_l_14_pretrain_flickr8k.yaml
+-rw-r--r--  2.0 unx     4286 b- defN 24-Apr-12 09:49 configs/clip/run_clip_vit_l_14_zero_shot_image_classification_cifar100.yaml
+-rw-r--r--  2.0 unx     6097 b- defN 24-Apr-12 09:49 configs/codegeex2/run_codegeex2_6b.yaml
+-rw-r--r--  2.0 unx     5811 b- defN 24-Apr-12 09:49 configs/codegeex2/run_codegeex2_6b_eval.yaml
+-rw-r--r--  2.0 unx     5817 b- defN 24-Apr-12 09:49 configs/codegeex2/run_codegeex2_6b_finetune.yaml
+-rw-r--r--  2.0 unx     5821 b- defN 24-Apr-12 09:49 configs/codegeex2/run_codegeex2_6b_finetune_2048.yaml
+-rw-r--r--  2.0 unx     4008 b- defN 24-Apr-12 09:49 configs/codellama/predict_codellama_34b_910b.yaml
+-rw-r--r--  2.0 unx     5330 b- defN 24-Apr-12 09:49 configs/codellama/run_codellama_34b_910b.yaml
+-rw-r--r--  2.0 unx     5303 b- defN 24-Apr-12 09:49 configs/codellama/run_codellama_34b_910b_32p.yaml
+-rw-r--r--  2.0 unx      843 b- defN 24-Apr-12 09:49 configs/convert_config/run_convert.yaml
+-rw-r--r--  2.0 unx      585 b- defN 24-Apr-12 09:49 configs/convert_config/run_reversed_convert.yaml
+-rw-r--r--  2.0 unx     2617 b- defN 24-Apr-12 09:49 configs/general/run_general_task.yaml
+-rw-r--r--  2.0 unx     5756 b- defN 24-Apr-12 09:49 configs/glm/run_glm_6b_finetune.yaml
+-rw-r--r--  2.0 unx     5677 b- defN 24-Apr-12 09:49 configs/glm/run_glm_6b_infer.yaml
+-rw-r--r--  2.0 unx     5954 b- defN 24-Apr-12 09:49 configs/glm/run_glm_6b_lora.yaml
+-rw-r--r--  2.0 unx     5861 b- defN 24-Apr-12 09:49 configs/glm/run_glm_6b_lora_infer.yaml
+-rw-r--r--  2.0 unx     1639 b- defN 24-Apr-12 09:49 configs/glm2/export_glm2_6b.yaml
+-rw-r--r--  2.0 unx     6029 b- defN 24-Apr-12 09:49 configs/glm2/run_glm2_6b.yaml
+-rw-r--r--  2.0 unx     5922 b- defN 24-Apr-12 09:49 configs/glm2/run_glm2_6b_finetune_2k_800T_A2_64G.yaml
+-rw-r--r--  2.0 unx     5923 b- defN 24-Apr-12 09:49 configs/glm2/run_glm2_6b_finetune_2k_800_32G.yaml
+-rw-r--r--  2.0 unx     5870 b- defN 24-Apr-12 09:49 configs/glm2/run_glm2_6b_finetune_800T_A2_64G.yaml
+-rw-r--r--  2.0 unx     5916 b- defN 24-Apr-12 09:49 configs/glm2/run_glm2_6b_finetune_800_32G.yaml
+-rw-r--r--  2.0 unx     5615 b- defN 24-Apr-12 09:49 configs/glm2/run_glm2_6b_finetune_eval.yaml
+-rw-r--r--  2.0 unx     6203 b- defN 24-Apr-12 09:49 configs/glm2/run_glm2_6b_lora_2k_800T_A2_64G.yaml
+-rw-r--r--  2.0 unx     6205 b- defN 24-Apr-12 09:49 configs/glm2/run_glm2_6b_lora_2k_800_32G.yaml
+-rw-r--r--  2.0 unx     6191 b- defN 24-Apr-12 09:49 configs/glm2/run_glm2_6b_lora_800T_A2_64G.yaml
+-rw-r--r--  2.0 unx     6197 b- defN 24-Apr-12 09:49 configs/glm2/run_glm2_6b_lora_800_32G.yaml
+-rw-r--r--  2.0 unx     5800 b- defN 24-Apr-12 09:49 configs/glm2/run_glm2_6b_lora_eval.yaml
+-rw-r--r--  2.0 unx     6150 b- defN 24-Apr-12 09:49 configs/glm2/run_glm2_6b_ptuning2.yaml
+-rw-r--r--  2.0 unx     1640 b- defN 24-Apr-12 09:49 configs/glm3/export_glm3_6b.yaml
+-rw-r--r--  2.0 unx     6029 b- defN 24-Apr-12 09:49 configs/glm3/run_glm3_6b.yaml
+-rw-r--r--  2.0 unx     5907 b- defN 24-Apr-12 09:49 configs/glm3/run_glm3_6b_finetune_2k_800T_A2_64G.yaml
+-rw-r--r--  2.0 unx     5903 b- defN 24-Apr-12 09:49 configs/glm3/run_glm3_6b_finetune_800T_A2_64G.yaml
+-rw-r--r--  2.0 unx     5234 b- defN 24-Apr-12 09:49 configs/glm3/run_glm3_6b_multiturn_finetune_800T_A2_64G.yaml
+-rw-r--r--  2.0 unx     4376 b- defN 24-Apr-12 09:49 configs/gpt2/run_gpt2.yaml
+-rw-r--r--  2.0 unx     4753 b- defN 24-Apr-12 09:49 configs/gpt2/run_gpt2_13b.yaml
+-rw-r--r--  2.0 unx     4897 b- defN 24-Apr-12 09:49 configs/gpt2/run_gpt2_13b_910b.yaml
+-rw-r--r--  2.0 unx     4703 b- defN 24-Apr-12 09:49 configs/gpt2/run_gpt2_52b.yaml
+-rw-r--r--  2.0 unx     4451 b- defN 24-Apr-12 09:49 configs/gpt2/run_gpt2_lora.yaml
+-rw-r--r--  2.0 unx     4336 b- defN 24-Apr-12 09:49 configs/gpt2/run_gpt2_txtcls.yaml
+-rw-r--r--  2.0 unx     4706 b- defN 24-Apr-12 09:49 configs/gpt2/run_gpt2_xl.yaml
+-rw-r--r--  2.0 unx     4944 b- defN 24-Apr-12 09:49 configs/gpt2/run_gpt2_xl_lora.yaml
+-rwxr-xr-x  2.0 unx     5173 b- defN 24-Apr-12 09:49 configs/llama/run_llama_13b.yaml
+-rw-r--r--  2.0 unx     5176 b- defN 24-Apr-12 09:49 configs/llama/run_llama_13b_910b.yaml
+-rwxr-xr-x  2.0 unx     5169 b- defN 24-Apr-12 09:49 configs/llama/run_llama_7b.yaml
+-rw-r--r--  2.0 unx     5167 b- defN 24-Apr-12 09:49 configs/llama/run_llama_7b_910b.yaml
+-rw-r--r--  2.0 unx     5625 b- defN 24-Apr-12 09:49 configs/llama/run_llama_7b_lora.yaml
+-rw-r--r--  2.0 unx     2890 b- defN 24-Apr-12 09:49 configs/llama2/export_llama2_13b.yaml
+-rw-r--r--  2.0 unx     2721 b- defN 24-Apr-12 09:49 configs/llama2/export_llama2_7b.yaml
+-rw-r--r--  2.0 unx     4056 b- defN 24-Apr-12 09:49 configs/llama2/predict_llama2_70b_910b.yaml
+-rw-r--r--  2.0 unx     5424 b- defN 24-Apr-12 09:49 configs/llama2/run_llama2_13b.yaml
+-rw-r--r--  2.0 unx     5087 b- defN 24-Apr-12 09:49 configs/llama2/run_llama2_13b_910b.yaml
+-rw-r--r--  2.0 unx     5655 b- defN 24-Apr-12 09:49 configs/llama2/run_llama2_13b_910b_auto_parallel.yaml
+-rw-r--r--  2.0 unx     5137 b- defN 24-Apr-12 09:49 configs/llama2/run_llama2_13b_910b_finetune.yaml
+-rw-r--r--  2.0 unx     5420 b- defN 24-Apr-12 09:49 configs/llama2/run_llama2_13b_lora_910b.yaml
+-rw-r--r--  2.0 unx     5473 b- defN 24-Apr-12 09:49 configs/llama2/run_llama2_70b.yaml
+-rw-r--r--  2.0 unx     5332 b- defN 24-Apr-12 09:49 configs/llama2/run_llama2_70b_910b.yaml
+-rw-r--r--  2.0 unx     5897 b- defN 24-Apr-12 09:49 configs/llama2/run_llama2_70b_910b_auto_parallel.yaml
+-rw-r--r--  2.0 unx     5378 b- defN 24-Apr-12 09:49 configs/llama2/run_llama2_70b_910b_finetune.yaml
+-rw-r--r--  2.0 unx     5469 b- defN 24-Apr-12 09:49 configs/llama2/run_llama2_70b_bf16_910b.yaml
+-rw-r--r--  2.0 unx     5414 b- defN 24-Apr-12 09:49 configs/llama2/run_llama2_7b.yaml
+-rw-r--r--  2.0 unx     5100 b- defN 24-Apr-12 09:49 configs/llama2/run_llama2_7b_910b.yaml
+-rw-r--r--  2.0 unx     5632 b- defN 24-Apr-12 09:49 configs/llama2/run_llama2_7b_910b_auto_parallel.yaml
+-rw-r--r--  2.0 unx     5113 b- defN 24-Apr-12 09:49 configs/llama2/run_llama2_7b_910b_finetune.yaml
+-rw-r--r--  2.0 unx     5452 b- defN 24-Apr-12 09:49 configs/llama2/run_llama2_7b_lora_910b.yaml
+-rw-r--r--  2.0 unx     4785 b- defN 24-Apr-12 09:49 configs/mae/run_mae_vit_base_p16_224_800ep.yaml
+-rw-r--r--  2.0 unx     4942 b- defN 24-Apr-12 09:49 configs/pangualpha/run_pangualpha_13b.yaml
+-rw-r--r--  2.0 unx     4791 b- defN 24-Apr-12 09:49 configs/pangualpha/run_pangualpha_2_6b.yaml
+-rw-r--r--  2.0 unx     4229 b- defN 24-Apr-12 09:49 configs/pangualpha/run_pangualpha_2_6b_em_f1.yaml
+-rw-r--r--  2.0 unx     4495 b- defN 24-Apr-12 09:49 configs/pangualpha/run_pangualpha_2_6b_prompt_txtcls.yaml
+-rw-r--r--  2.0 unx     5531 b- defN 24-Apr-12 09:49 configs/qa/run_qa_bert_base_uncased.yaml
+-rwxr-xr-x  2.0 unx     6823 b- defN 24-Apr-12 09:49 configs/sam/run_sam_vit-b.yaml
+-rwxr-xr-x  2.0 unx     6826 b- defN 24-Apr-12 09:49 configs/sam/run_sam_vit-h.yaml
+-rw-r--r--  2.0 unx     6826 b- defN 24-Apr-12 09:49 configs/sam/run_sam_vit-l.yaml
+-rw-r--r--  2.0 unx     6149 b- defN 24-Apr-12 09:49 configs/swin/run_swin_base_p4w7_224_100ep.yaml
+-rw-r--r--  2.0 unx     4494 b- defN 24-Apr-12 09:49 configs/t5/run_t5_small_on_wmt16.yaml
+-rw-r--r--  2.0 unx     4455 b- defN 24-Apr-12 09:49 configs/t5/run_t5_tiny_on_wmt16.yaml
+-rw-r--r--  2.0 unx     5772 b- defN 24-Apr-12 09:49 configs/tokcls/run_tokcls_bert_base_chinese.yaml
+-rw-r--r--  2.0 unx     5788 b- defN 24-Apr-12 09:49 configs/tokcls/run_tokcls_bert_base_chinese_cluener.yaml
+-rw-r--r--  2.0 unx     4428 b- defN 24-Apr-12 09:49 configs/txtcls/run_txtcls_bert_base_uncased.yaml
+-rw-r--r--  2.0 unx     4438 b- defN 24-Apr-12 09:49 configs/txtcls/run_txtcls_bert_base_uncased_mnli.yaml
+-rw-r--r--  2.0 unx     6021 b- defN 24-Apr-12 09:49 configs/vit/run_vit_base_p16_224_100ep.yaml
+-r--------  2.0 unx      215 b- defN 24-Apr-12 09:49 mindformers/.commit_id
+-r--------  2.0 unx     1374 b- defN 24-Apr-12 09:49 mindformers/__init__.py
+-r--------  2.0 unx    39713 b- defN 24-Apr-12 09:49 mindformers/auto_class.py
+-r--------  2.0 unx    68039 b- defN 24-Apr-12 09:49 mindformers/mindformer_book.py
+-r--------  2.0 unx    12378 b- defN 24-Apr-12 09:49 mindformers/version_control.py
+-r--------  2.0 unx     1345 b- defN 24-Apr-12 09:49 mindformers/core/__init__.py
+-r--------  2.0 unx     4002 b- defN 24-Apr-12 09:49 mindformers/core/clip_grad.py
+-r--------  2.0 unx     4315 b- defN 24-Apr-12 09:49 mindformers/core/parallel_config.py
+-r--------  2.0 unx      809 b- defN 24-Apr-12 09:49 mindformers/core/callback/__init__.py
+-r--------  2.0 unx     3309 b- defN 24-Apr-12 09:49 mindformers/core/callback/build_callback.py
+-r--------  2.0 unx    46230 b- defN 24-Apr-12 09:49 mindformers/core/callback/callback.py
+-r--------  2.0 unx      795 b- defN 24-Apr-12 09:49 mindformers/core/context/__init__.py
+-r--------  2.0 unx     7754 b- defN 24-Apr-12 09:49 mindformers/core/context/build_context.py
+-r--------  2.0 unx      789 b- defN 24-Apr-12 09:49 mindformers/core/loss/__init__.py
+-r--------  2.0 unx     2866 b- defN 24-Apr-12 09:49 mindformers/core/loss/build_loss.py
+-r--------  2.0 unx    17965 b- defN 24-Apr-12 09:49 mindformers/core/loss/loss.py
+-r--------  2.0 unx      802 b- defN 24-Apr-12 09:49 mindformers/core/lr/__init__.py
+-r--------  2.0 unx     4178 b- defN 24-Apr-12 09:49 mindformers/core/lr/build_lr.py
+-r--------  2.0 unx    21488 b- defN 24-Apr-12 09:49 mindformers/core/lr/lr_schedule.py
+-r--------  2.0 unx      800 b- defN 24-Apr-12 09:49 mindformers/core/metric/__init__.py
+-r--------  2.0 unx     2683 b- defN 24-Apr-12 09:49 mindformers/core/metric/build_metric.py
+-r--------  2.0 unx    35009 b- defN 24-Apr-12 09:49 mindformers/core/metric/metric.py
+-r--------  2.0 unx     1768 b- defN 24-Apr-12 09:49 mindformers/core/metric/utils.py
+-r--------  2.0 unx      847 b- defN 24-Apr-12 09:49 mindformers/core/optim/__init__.py
+-r--------  2.0 unx     4623 b- defN 24-Apr-12 09:49 mindformers/core/optim/build_optim.py
+-r--------  2.0 unx    21063 b- defN 24-Apr-12 09:49 mindformers/core/optim/came.py
+-r--------  2.0 unx    31399 b- defN 24-Apr-12 09:49 mindformers/core/optim/optim.py
+-r--------  2.0 unx     2608 b- defN 24-Apr-12 09:49 mindformers/dataset/__init__.py
+-r--------  2.0 unx     3379 b- defN 24-Apr-12 09:49 mindformers/dataset/base_dataset.py
+-r--------  2.0 unx     2560 b- defN 24-Apr-12 09:49 mindformers/dataset/build_dataset.py
+-r--------  2.0 unx    12908 b- defN 24-Apr-12 09:49 mindformers/dataset/causal_language_model_dataset.py
+-r--------  2.0 unx     9758 b- defN 24-Apr-12 09:49 mindformers/dataset/contrastive_language_image_pretrain_dataset.py
+-r--------  2.0 unx     2998 b- defN 24-Apr-12 09:49 mindformers/dataset/general_dataset.py
+-r--------  2.0 unx     9994 b- defN 24-Apr-12 09:49 mindformers/dataset/img_cls_dataset.py
+-r--------  2.0 unx    21959 b- defN 24-Apr-12 09:49 mindformers/dataset/keyword_gen_dataset.py
+-r--------  2.0 unx    16192 b- defN 24-Apr-12 09:49 mindformers/dataset/labels.py
+-r--------  2.0 unx     8478 b- defN 24-Apr-12 09:49 mindformers/dataset/mask_language_model_dataset.py
+-r--------  2.0 unx     9100 b- defN 24-Apr-12 09:49 mindformers/dataset/mim_dataset.py
+-r--------  2.0 unx     9150 b- defN 24-Apr-12 09:49 mindformers/dataset/multi_turn_dataset.py
+-r--------  2.0 unx     7973 b- defN 24-Apr-12 09:49 mindformers/dataset/question_answering_dataset.py
+-r--------  2.0 unx    13957 b- defN 24-Apr-12 09:49 mindformers/dataset/reward_model_dataset.py
+-r--------  2.0 unx     7515 b- defN 24-Apr-12 09:49 mindformers/dataset/text_classification_dataset.py
+-r--------  2.0 unx     9984 b- defN 24-Apr-12 09:49 mindformers/dataset/token_classification_dataset.py
+-r--------  2.0 unx    11610 b- defN 24-Apr-12 09:49 mindformers/dataset/translation_dataset.py
+-r--------  2.0 unx     2662 b- defN 24-Apr-12 09:49 mindformers/dataset/utils.py
+-r--------  2.0 unx     8502 b- defN 24-Apr-12 09:49 mindformers/dataset/zero_shot_image_classification_dataset.py
+-r--------  2.0 unx     1580 b- defN 24-Apr-12 09:49 mindformers/dataset/dataloader/__init__.py
+-r--------  2.0 unx     6200 b- defN 24-Apr-12 09:49 mindformers/dataset/dataloader/adgen_dataloader.py
+-r--------  2.0 unx     2930 b- defN 24-Apr-12 09:49 mindformers/dataset/dataloader/build_dataloader.py
+-r--------  2.0 unx     7893 b- defN 24-Apr-12 09:49 mindformers/dataset/dataloader/cifar100_dataloader.py
+-r--------  2.0 unx     6961 b- defN 24-Apr-12 09:49 mindformers/dataset/dataloader/cluener_dataloader.py
+-r--------  2.0 unx     5344 b- defN 24-Apr-12 09:49 mindformers/dataset/dataloader/datareaders.py
+-r--------  2.0 unx     7099 b- defN 24-Apr-12 09:49 mindformers/dataset/dataloader/flickr8k_dataloader.py
+-r--------  2.0 unx     6483 b- defN 24-Apr-12 09:49 mindformers/dataset/dataloader/multi_image_cap_dataloader.py
+-r--------  2.0 unx    11335 b- defN 24-Apr-12 09:49 mindformers/dataset/dataloader/multi_source_dataloader.py
+-r--------  2.0 unx    12659 b- defN 24-Apr-12 09:49 mindformers/dataset/dataloader/sft_dataloader.py
+-r--------  2.0 unx     7423 b- defN 24-Apr-12 09:49 mindformers/dataset/dataloader/sft_map_functions.py
+-r--------  2.0 unx    23889 b- defN 24-Apr-12 09:49 mindformers/dataset/dataloader/squad_dataloader.py
+-r--------  2.0 unx     7902 b- defN 24-Apr-12 09:49 mindformers/dataset/dataloader/toolaplaca_dataloader.py
+-r--------  2.0 unx    20177 b- defN 24-Apr-12 09:49 mindformers/dataset/dataloader/training_dataloader.py
+-r--------  2.0 unx     4360 b- defN 24-Apr-12 09:49 mindformers/dataset/dataloader/wmt16_dataloader.py
+-r--------  2.0 unx      808 b- defN 24-Apr-12 09:49 mindformers/dataset/mask/__init__.py
+-r--------  2.0 unx     2196 b- defN 24-Apr-12 09:49 mindformers/dataset/mask/build_mask.py
+-r--------  2.0 unx     3760 b- defN 24-Apr-12 09:49 mindformers/dataset/mask/vision_mask.py
+-r--------  2.0 unx      754 b- defN 24-Apr-12 09:49 mindformers/dataset/sampler/__init__.py
+-r--------  2.0 unx     2719 b- defN 24-Apr-12 09:49 mindformers/dataset/sampler/build_sampler.py
+-r--------  2.0 unx     1192 b- defN 24-Apr-12 09:49 mindformers/dataset/transforms/__init__.py
+-r--------  2.0 unx    33409 b- defN 24-Apr-12 09:49 mindformers/dataset/transforms/auto_augment.py
+-r--------  2.0 unx     3364 b- defN 24-Apr-12 09:49 mindformers/dataset/transforms/build_transforms.py
+-r--------  2.0 unx    11513 b- defN 24-Apr-12 09:49 mindformers/dataset/transforms/mixup.py
+-r--------  2.0 unx     4846 b- defN 24-Apr-12 09:49 mindformers/dataset/transforms/random_erasing.py
+-r--------  2.0 unx     6231 b- defN 24-Apr-12 09:49 mindformers/dataset/transforms/text_transforms.py
+-r--------  2.0 unx    12281 b- defN 24-Apr-12 09:49 mindformers/dataset/transforms/vision_transforms.py
+-r--------  2.0 unx      984 b- defN 24-Apr-12 09:49 mindformers/generation/__init__.py
+-r--------  2.0 unx    18947 b- defN 24-Apr-12 09:49 mindformers/generation/beam_search.py
+-r--------  2.0 unx     9205 b- defN 24-Apr-12 09:49 mindformers/generation/generation_config.py
+-r--------  2.0 unx    14147 b- defN 24-Apr-12 09:49 mindformers/generation/logits_process.py
+-r--------  2.0 unx    11805 b- defN 24-Apr-12 09:49 mindformers/generation/streamers.py
+-r--------  2.0 unx    57463 b- defN 24-Apr-12 09:49 mindformers/generation/text_generator.py
+-r--------  2.0 unx     2956 b- defN 24-Apr-12 09:49 mindformers/generation/utils.py
+-r--------  2.0 unx      862 b- defN 24-Apr-12 09:49 mindformers/inference/__init__.py
+-r--------  2.0 unx     1406 b- defN 24-Apr-12 09:49 mindformers/inference/context.py
+-r--------  2.0 unx     2532 b- defN 24-Apr-12 09:49 mindformers/inference/infer_config.py
+-r--------  2.0 unx     1930 b- defN 24-Apr-12 09:49 mindformers/inference/infer_task.py
+-r--------  2.0 unx    10760 b- defN 24-Apr-12 09:49 mindformers/inference/pipeline.py
+-r--------  2.0 unx     2561 b- defN 24-Apr-12 09:49 mindformers/inference/postprocess_sampler.py
+-r--------  2.0 unx      704 b- defN 24-Apr-12 09:49 mindformers/inference/infers/__init__.py
+-r--------  2.0 unx     8454 b- defN 24-Apr-12 09:49 mindformers/inference/infers/base_infer.py
+-r--------  2.0 unx     3234 b- defN 24-Apr-12 09:49 mindformers/inference/infers/cache_engine.py
+-r--------  2.0 unx    31740 b- defN 24-Apr-12 09:49 mindformers/inference/infers/text_generator_infer.py
+-r--------  2.0 unx     2390 b- defN 24-Apr-12 09:49 mindformers/models/__init__.py
+-r--------  2.0 unx    10530 b- defN 24-Apr-12 09:49 mindformers/models/base_config.py
+-r--------  2.0 unx    17400 b- defN 24-Apr-12 09:49 mindformers/models/base_model.py
+-r--------  2.0 unx    12083 b- defN 24-Apr-12 09:49 mindformers/models/base_processor.py
+-r--------  2.0 unx     2681 b- defN 24-Apr-12 09:49 mindformers/models/build_config.py
+-r--------  2.0 unx     4427 b- defN 24-Apr-12 09:49 mindformers/models/build_model.py
+-r--------  2.0 unx     2461 b- defN 24-Apr-12 09:49 mindformers/models/build_processor.py
+-r--------  2.0 unx     3570 b- defN 24-Apr-12 09:49 mindformers/models/build_tokenizer.py
+-r--------  2.0 unx    35454 b- defN 24-Apr-12 09:49 mindformers/models/configuration_utils.py
+-r--------  2.0 unx    16629 b- defN 24-Apr-12 09:49 mindformers/models/convert_slow_tokenizer.py
+-r--------  2.0 unx    32672 b- defN 24-Apr-12 09:49 mindformers/models/image_processing_utils.py
+-r--------  2.0 unx    69023 b- defN 24-Apr-12 09:49 mindformers/models/modeling_utils.py
+-r--------  2.0 unx    21177 b- defN 24-Apr-12 09:49 mindformers/models/processing_utils.py
+-r--------  2.0 unx    50997 b- defN 24-Apr-12 09:49 mindformers/models/sentencepiece_model_pb2.py
+-r--------  2.0 unx     6645 b- defN 24-Apr-12 09:49 mindformers/models/sentencepiece_model_pb2_new.py
+-r--------  2.0 unx    47042 b- defN 24-Apr-12 09:49 mindformers/models/tokenization_utils.py
+-r--------  2.0 unx   209768 b- defN 24-Apr-12 09:49 mindformers/models/tokenization_utils_base.py
+-r--------  2.0 unx    38870 b- defN 24-Apr-12 09:49 mindformers/models/tokenization_utils_fast.py
+-r--------  2.0 unx     2021 b- defN 24-Apr-12 09:49 mindformers/models/utils.py
+-r--------  2.0 unx     2334 b- defN 24-Apr-12 09:49 mindformers/models/auto/__init__.py
+-r--------  2.0 unx    42560 b- defN 24-Apr-12 09:49 mindformers/models/auto/auto_factory.py
+-r--------  2.0 unx    19463 b- defN 24-Apr-12 09:49 mindformers/models/auto/configuration_auto.py
+-r--------  2.0 unx    17195 b- defN 24-Apr-12 09:49 mindformers/models/auto/image_processing_auto.py
+-r--------  2.0 unx    11172 b- defN 24-Apr-12 09:49 mindformers/models/auto/modeling_auto.py
+-r--------  2.0 unx    18016 b- defN 24-Apr-12 09:49 mindformers/models/auto/processing_auto.py
+-r--------  2.0 unx    32587 b- defN 24-Apr-12 09:49 mindformers/models/auto/tokenization_auto.py
+-r--------  2.0 unx     1191 b- defN 24-Apr-12 09:49 mindformers/models/bert/__init__.py
+-r--------  2.0 unx    29699 b- defN 24-Apr-12 09:49 mindformers/models/bert/bert.py
+-r--------  2.0 unx     8583 b- defN 24-Apr-12 09:49 mindformers/models/bert/bert_config.py
+-r--------  2.0 unx     3764 b- defN 24-Apr-12 09:49 mindformers/models/bert/bert_processor.py
+-r--------  2.0 unx    25732 b- defN 24-Apr-12 09:49 mindformers/models/bert/bert_tokenizer.py
+-r--------  2.0 unx     8228 b- defN 24-Apr-12 09:49 mindformers/models/bert/bert_tokenizer_fast.py
+-r--------  2.0 unx     8638 b- defN 24-Apr-12 09:49 mindformers/models/bert/convert_weight.py
+-r--------  2.0 unx     1189 b- defN 24-Apr-12 09:49 mindformers/models/blip2/__init__.py
+-r--------  2.0 unx     4045 b- defN 24-Apr-12 09:49 mindformers/models/blip2/blip2.py
+-r--------  2.0 unx     6941 b- defN 24-Apr-12 09:49 mindformers/models/blip2/blip2_config.py
+-r--------  2.0 unx     9584 b- defN 24-Apr-12 09:49 mindformers/models/blip2/blip2_itm_evaluator.py
+-r--------  2.0 unx     8142 b- defN 24-Apr-12 09:49 mindformers/models/blip2/blip2_llama.py
+-r--------  2.0 unx    11459 b- defN 24-Apr-12 09:49 mindformers/models/blip2/blip2_llm.py
+-r--------  2.0 unx     8689 b- defN 24-Apr-12 09:49 mindformers/models/blip2/blip2_processor.py
+-r--------  2.0 unx    23783 b- defN 24-Apr-12 09:49 mindformers/models/blip2/blip2_qformer.py
+-r--------  2.0 unx     1501 b- defN 24-Apr-12 09:49 mindformers/models/blip2/blip2_vit.py
+-r--------  2.0 unx     4365 b- defN 24-Apr-12 09:49 mindformers/models/blip2/convert_reversed.py
+-r--------  2.0 unx     5482 b- defN 24-Apr-12 09:49 mindformers/models/blip2/convert_weight.py
+-r--------  2.0 unx     4115 b- defN 24-Apr-12 09:49 mindformers/models/blip2/layers.py
+-r--------  2.0 unx    70439 b- defN 24-Apr-12 09:49 mindformers/models/blip2/qformer.py
+-r--------  2.0 unx     5191 b- defN 24-Apr-12 09:49 mindformers/models/blip2/qformer_config.py
+-r--------  2.0 unx     1137 b- defN 24-Apr-12 09:49 mindformers/models/bloom/__init__.py
+-r--------  2.0 unx    17489 b- defN 24-Apr-12 09:49 mindformers/models/bloom/bloom.py
+-r--------  2.0 unx     9617 b- defN 24-Apr-12 09:49 mindformers/models/bloom/bloom_config.py
+-r--------  2.0 unx     3632 b- defN 24-Apr-12 09:49 mindformers/models/bloom/bloom_processor.py
+-r--------  2.0 unx     4576 b- defN 24-Apr-12 09:49 mindformers/models/bloom/bloom_reward.py
+-r--------  2.0 unx    10525 b- defN 24-Apr-12 09:49 mindformers/models/bloom/bloom_tokenizer.py
+-r--------  2.0 unx     6814 b- defN 24-Apr-12 09:49 mindformers/models/bloom/bloom_tokenizer_fast.py
+-r--------  2.0 unx     5263 b- defN 24-Apr-12 09:49 mindformers/models/bloom/convert_reversed.py
+-r--------  2.0 unx     5668 b- defN 24-Apr-12 09:49 mindformers/models/bloom/convert_weight.py
+-r--------  2.0 unx    32708 b- defN 24-Apr-12 09:49 mindformers/models/bloom/layers.py
+-r--------  2.0 unx     1030 b- defN 24-Apr-12 09:49 mindformers/models/clip/__init__.py
+-r--------  2.0 unx    10413 b- defN 24-Apr-12 09:49 mindformers/models/clip/clip.py
+-r--------  2.0 unx     9192 b- defN 24-Apr-12 09:49 mindformers/models/clip/clip_config.py
+-r--------  2.0 unx    10123 b- defN 24-Apr-12 09:49 mindformers/models/clip/clip_modules.py
+-r--------  2.0 unx     6379 b- defN 24-Apr-12 09:49 mindformers/models/clip/clip_processor.py
+-r--------  2.0 unx    11888 b- defN 24-Apr-12 09:49 mindformers/models/clip/clip_tokenizer.py
+-r--------  2.0 unx     2989 b- defN 24-Apr-12 09:49 mindformers/models/clip/convert_weight.py
+-r--------  2.0 unx     1036 b- defN 24-Apr-12 09:49 mindformers/models/glm/__init__.py
+-r--------  2.0 unx    20335 b- defN 24-Apr-12 09:49 mindformers/models/glm/attention.py
+-r--------  2.0 unx    16157 b- defN 24-Apr-12 09:49 mindformers/models/glm/chatglm_6b_tokenizer.py
+-r--------  2.0 unx     2308 b- defN 24-Apr-12 09:49 mindformers/models/glm/convert_reversed.py
+-r--------  2.0 unx     2458 b- defN 24-Apr-12 09:49 mindformers/models/glm/convert_weight.py
+-r--------  2.0 unx    22573 b- defN 24-Apr-12 09:49 mindformers/models/glm/glm.py
+-r--------  2.0 unx    12201 b- defN 24-Apr-12 09:49 mindformers/models/glm/glm_config.py
+-r--------  2.0 unx     4446 b- defN 24-Apr-12 09:49 mindformers/models/glm/glm_processor.py
+-r--------  2.0 unx    13117 b- defN 24-Apr-12 09:49 mindformers/models/glm/layers.py
+-r--------  2.0 unx      901 b- defN 24-Apr-12 09:49 mindformers/models/glm2/__init__.py
+-r--------  2.0 unx     2069 b- defN 24-Apr-12 09:49 mindformers/models/glm2/convert_reversed.py
+-r--------  2.0 unx     2172 b- defN 24-Apr-12 09:49 mindformers/models/glm2/convert_weight.py
+-r--------  2.0 unx    13392 b- defN 24-Apr-12 09:49 mindformers/models/glm2/glm2.py
+-r--------  2.0 unx     5627 b- defN 24-Apr-12 09:49 mindformers/models/glm2/glm2_config.py
+-r--------  2.0 unx     6386 b- defN 24-Apr-12 09:49 mindformers/models/glm2/glm2_modules.py
+-r--------  2.0 unx    10952 b- defN 24-Apr-12 09:49 mindformers/models/glm2/glm2_tokenizer.py
+-r--------  2.0 unx    35424 b- defN 24-Apr-12 09:49 mindformers/models/glm2/glm2_transformer.py
+-r--------  2.0 unx      789 b- defN 24-Apr-12 09:49 mindformers/models/glm3/__init__.py
+-r--------  2.0 unx    16878 b- defN 24-Apr-12 09:49 mindformers/models/glm3/glm3_tokenizer.py
+-r--------  2.0 unx     1106 b- defN 24-Apr-12 09:49 mindformers/models/gpt2/__init__.py
+-r--------  2.0 unx     5661 b- defN 24-Apr-12 09:49 mindformers/models/gpt2/convert_reversed.py
+-r--------  2.0 unx     7211 b- defN 24-Apr-12 09:49 mindformers/models/gpt2/convert_weight.py
+-r--------  2.0 unx    28921 b- defN 24-Apr-12 09:49 mindformers/models/gpt2/gpt2.py
+-r--------  2.0 unx     9701 b- defN 24-Apr-12 09:49 mindformers/models/gpt2/gpt2_config.py
+-r--------  2.0 unx     3629 b- defN 24-Apr-12 09:49 mindformers/models/gpt2/gpt2_processor.py
+-r--------  2.0 unx    14404 b- defN 24-Apr-12 09:49 mindformers/models/gpt2/gpt2_tokenizer.py
+-r--------  2.0 unx     9082 b- defN 24-Apr-12 09:49 mindformers/models/gpt2/gpt2_tokenizer_fast.py
+-r--------  2.0 unx    11324 b- defN 24-Apr-12 09:49 mindformers/models/gpt2/gpt_modules.py
+-r--------  2.0 unx     1061 b- defN 24-Apr-12 09:49 mindformers/models/llama/__init__.py
+-r--------  2.0 unx     2833 b- defN 24-Apr-12 09:49 mindformers/models/llama/convert_reversed.py
+-r--------  2.0 unx     7035 b- defN 24-Apr-12 09:49 mindformers/models/llama/convert_weight.py
+-r--------  2.0 unx    26734 b- defN 24-Apr-12 09:49 mindformers/models/llama/llama.py
+-r--------  2.0 unx    11843 b- defN 24-Apr-12 09:49 mindformers/models/llama/llama_config.py
+-r--------  2.0 unx    34597 b- defN 24-Apr-12 09:49 mindformers/models/llama/llama_interleave.py
+-r--------  2.0 unx    27060 b- defN 24-Apr-12 09:49 mindformers/models/llama/llama_layer.py
+-r--------  2.0 unx    13664 b- defN 24-Apr-12 09:49 mindformers/models/llama/llama_moe.py
+-r--------  2.0 unx     3641 b- defN 24-Apr-12 09:49 mindformers/models/llama/llama_processor.py
+-r--------  2.0 unx    17126 b- defN 24-Apr-12 09:49 mindformers/models/llama/llama_tokenizer.py
+-r--------  2.0 unx     9038 b- defN 24-Apr-12 09:49 mindformers/models/llama/llama_tokenizer_fast.py
+-r--------  2.0 unx    34843 b- defN 24-Apr-12 09:49 mindformers/models/llama/llama_transformer.py
+-r--------  2.0 unx      878 b- defN 24-Apr-12 09:49 mindformers/models/mae/__init__.py
+-r--------  2.0 unx     3416 b- defN 24-Apr-12 09:49 mindformers/models/mae/convert_weight.py
+-r--------  2.0 unx    18363 b- defN 24-Apr-12 09:49 mindformers/models/mae/mae.py
+-r--------  2.0 unx     7851 b- defN 24-Apr-12 09:49 mindformers/models/mae/mae_config.py
+-r--------  2.0 unx    36274 b- defN 24-Apr-12 09:49 mindformers/models/mae/mae_modules.py
+-r--------  2.0 unx     8056 b- defN 24-Apr-12 09:49 mindformers/models/mae/mae_processor.py
+-r--------  2.0 unx     1022 b- defN 24-Apr-12 09:49 mindformers/models/pangualpha/__init__.py
+-r--------  2.0 unx     5469 b- defN 24-Apr-12 09:49 mindformers/models/pangualpha/convert_weight.py
+-r--------  2.0 unx    28704 b- defN 24-Apr-12 09:49 mindformers/models/pangualpha/pangualpha.py
+-r--------  2.0 unx     4905 b- defN 24-Apr-12 09:49 mindformers/models/pangualpha/pangualpha_config.py
+-r--------  2.0 unx     2523 b- defN 24-Apr-12 09:49 mindformers/models/pangualpha/pangualpha_processor.py
+-r--------  2.0 unx     8201 b- defN 24-Apr-12 09:49 mindformers/models/pangualpha/pangualpha_tokenizer.py
+-r--------  2.0 unx     1190 b- defN 24-Apr-12 09:49 mindformers/models/sam/__init__.py
+-r--------  2.0 unx     3489 b- defN 24-Apr-12 09:49 mindformers/models/sam/conver_weight.py
+-r--------  2.0 unx     6281 b- defN 24-Apr-12 09:49 mindformers/models/sam/sam.py
+-r--------  2.0 unx     8641 b- defN 24-Apr-12 09:49 mindformers/models/sam/sam_config.py
+-r--------  2.0 unx    17601 b- defN 24-Apr-12 09:49 mindformers/models/sam/sam_image_encoder.py
+-r--------  2.0 unx     2944 b- defN 24-Apr-12 09:49 mindformers/models/sam/sam_layers.py
+-r--------  2.0 unx    22840 b- defN 24-Apr-12 09:49 mindformers/models/sam/sam_mask_decoder.py
+-r--------  2.0 unx    10277 b- defN 24-Apr-12 09:49 mindformers/models/sam/sam_processor.py
+-r--------  2.0 unx    10597 b- defN 24-Apr-12 09:49 mindformers/models/sam/sam_prompt_encoder.py
+-r--------  2.0 unx    22757 b- defN 24-Apr-12 09:49 mindformers/models/sam/sam_utils.py
+-r--------  2.0 unx      885 b- defN 24-Apr-12 09:49 mindformers/models/swin/__init__.py
+-r--------  2.0 unx     4949 b- defN 24-Apr-12 09:49 mindformers/models/swin/convert_weight.py
+-r--------  2.0 unx    16602 b- defN 24-Apr-12 09:49 mindformers/models/swin/swin.py
+-r--------  2.0 unx     8141 b- defN 24-Apr-12 09:49 mindformers/models/swin/swin_config.py
+-r--------  2.0 unx    31100 b- defN 24-Apr-12 09:49 mindformers/models/swin/swin_modules.py
+-r--------  2.0 unx     7139 b- defN 24-Apr-12 09:49 mindformers/models/swin/swin_processor.py
+-r--------  2.0 unx     1192 b- defN 24-Apr-12 09:49 mindformers/models/t5/__init__.py
+-r--------  2.0 unx     8077 b- defN 24-Apr-12 09:49 mindformers/models/t5/convert_weight.py
+-r--------  2.0 unx    10956 b- defN 24-Apr-12 09:49 mindformers/models/t5/mt5.py
+-r--------  2.0 unx    90581 b- defN 24-Apr-12 09:49 mindformers/models/t5/t5.py
+-r--------  2.0 unx    12127 b- defN 24-Apr-12 09:49 mindformers/models/t5/t5_config.py
+-r--------  2.0 unx     4350 b- defN 24-Apr-12 09:49 mindformers/models/t5/t5_processor.py
+-r--------  2.0 unx    20272 b- defN 24-Apr-12 09:49 mindformers/models/t5/t5_tokenizer.py
+-r--------  2.0 unx     9885 b- defN 24-Apr-12 09:49 mindformers/models/t5/t5_tokenizer_fast.py
+-r--------  2.0 unx      948 b- defN 24-Apr-12 09:49 mindformers/models/vit/__init__.py
+-r--------  2.0 unx     3364 b- defN 24-Apr-12 09:49 mindformers/models/vit/convert_weight.py
+-r--------  2.0 unx    13786 b- defN 24-Apr-12 09:49 mindformers/models/vit/vit.py
+-r--------  2.0 unx     8809 b- defN 24-Apr-12 09:49 mindformers/models/vit/vit_config.py
+-r--------  2.0 unx    37548 b- defN 24-Apr-12 09:49 mindformers/models/vit/vit_modules.py
+-r--------  2.0 unx     6797 b- defN 24-Apr-12 09:49 mindformers/models/vit/vit_processor.py
+-r--------  2.0 unx      992 b- defN 24-Apr-12 09:49 mindformers/modules/__init__.py
+-r--------  2.0 unx    50360 b- defN 24-Apr-12 09:49 mindformers/modules/activation.py
+-r--------  2.0 unx    12130 b- defN 24-Apr-12 09:49 mindformers/modules/flash_attention.py
+-r--------  2.0 unx    13465 b- defN 24-Apr-12 09:49 mindformers/modules/kvcache_mgr.py
+-r--------  2.0 unx    48081 b- defN 24-Apr-12 09:49 mindformers/modules/layers.py
+-r--------  2.0 unx    14414 b- defN 24-Apr-12 09:49 mindformers/modules/local_block_sparse_attention.py
+-r--------  2.0 unx     5310 b- defN 24-Apr-12 09:49 mindformers/modules/paged_attention_mgr.py
+-r--------  2.0 unx     1335 b- defN 24-Apr-12 09:49 mindformers/modules/transformer/__init__.py
+-r--------  2.0 unx    63615 b- defN 24-Apr-12 09:49 mindformers/modules/transformer/moe.py
+-r--------  2.0 unx     9135 b- defN 24-Apr-12 09:49 mindformers/modules/transformer/op_parallel_config.py
+-r--------  2.0 unx   214520 b- defN 24-Apr-12 09:49 mindformers/modules/transformer/transformer.py
+-r--------  2.0 unx      897 b- defN 24-Apr-12 09:49 mindformers/pet/__init__.py
+-r--------  2.0 unx     1176 b- defN 24-Apr-12 09:49 mindformers/pet/constants.py
+-r--------  2.0 unx     4907 b- defN 24-Apr-12 09:49 mindformers/pet/pet_config.py
+-r--------  2.0 unx     4196 b- defN 24-Apr-12 09:49 mindformers/pet/pet_model.py
+-r--------  2.0 unx     1096 b- defN 24-Apr-12 09:49 mindformers/pet/utils.py
+-r--------  2.0 unx      696 b- defN 24-Apr-12 09:49 mindformers/pet/models/__init__.py
+-r--------  2.0 unx     3640 b- defN 24-Apr-12 09:49 mindformers/pet/models/lora.py
+-r--------  2.0 unx      955 b- defN 24-Apr-12 09:49 mindformers/pet/tuners/__init__.py
+-r--------  2.0 unx     1176 b- defN 24-Apr-12 09:49 mindformers/pet/tuners/ada_adapter.py
+-r--------  2.0 unx     1194 b- defN 24-Apr-12 09:49 mindformers/pet/tuners/adalora_adapter.py
+-r--------  2.0 unx     5497 b- defN 24-Apr-12 09:49 mindformers/pet/tuners/lora_adapter.py
+-r--------  2.0 unx     1649 b- defN 24-Apr-12 09:49 mindformers/pet/tuners/pet_adapter.py
+-r--------  2.0 unx     1165 b- defN 24-Apr-12 09:49 mindformers/pet/tuners/prefix_tuning_adapter.py
+-r--------  2.0 unx     1350 b- defN 24-Apr-12 09:49 mindformers/pet/tuners/ptuning2_adapter.py
+-r--------  2.0 unx     2142 b- defN 24-Apr-12 09:49 mindformers/pipeline/__init__.py
+-r--------  2.0 unx    13012 b- defN 24-Apr-12 09:49 mindformers/pipeline/base_pipeline.py
+-r--------  2.0 unx     2393 b- defN 24-Apr-12 09:49 mindformers/pipeline/build_pipeline.py
+-r--------  2.0 unx     5452 b- defN 24-Apr-12 09:49 mindformers/pipeline/fill_mask_pipeline.py
+-r--------  2.0 unx     6648 b- defN 24-Apr-12 09:49 mindformers/pipeline/image_classification_pipeline.py
+-r--------  2.0 unx     6249 b- defN 24-Apr-12 09:49 mindformers/pipeline/image_to_text_generation_pipeline.py
+-r--------  2.0 unx     5695 b- defN 24-Apr-12 09:49 mindformers/pipeline/masked_image_modeling_pipeline.py
+-r--------  2.0 unx    28220 b- defN 24-Apr-12 09:49 mindformers/pipeline/pipeline.py
+-r--------  2.0 unx     3369 b- defN 24-Apr-12 09:49 mindformers/pipeline/pipeline_registry.py
+-r--------  2.0 unx    17599 b- defN 24-Apr-12 09:49 mindformers/pipeline/question_answering_pipeline.py
+-r--------  2.0 unx     5728 b- defN 24-Apr-12 09:49 mindformers/pipeline/registry_constant.py
+-r--------  2.0 unx    25379 b- defN 24-Apr-12 09:49 mindformers/pipeline/segment_anything_pipeline.py
+-r--------  2.0 unx     9668 b- defN 24-Apr-12 09:49 mindformers/pipeline/text_classification_pipeline.py
+-r--------  2.0 unx     9883 b- defN 24-Apr-12 09:49 mindformers/pipeline/text_generation_pipeline.py
+-r--------  2.0 unx     9160 b- defN 24-Apr-12 09:49 mindformers/pipeline/token_classification_pipeline.py
+-r--------  2.0 unx     7484 b- defN 24-Apr-12 09:49 mindformers/pipeline/translation_pipeline.py
+-r--------  2.0 unx     8281 b- defN 24-Apr-12 09:49 mindformers/pipeline/zero_shot_image_classification_pipeline.py
+-r--------  2.0 unx     1194 b- defN 24-Apr-12 09:49 mindformers/tools/__init__.py
+-r--------  2.0 unx    10542 b- defN 24-Apr-12 09:49 mindformers/tools/check_rules.py
+-r--------  2.0 unx     4399 b- defN 24-Apr-12 09:49 mindformers/tools/download_tools.py
+-r--------  2.0 unx     6243 b- defN 24-Apr-12 09:49 mindformers/tools/download_tools_multithread.py
+-r--------  2.0 unx    12033 b- defN 24-Apr-12 09:49 mindformers/tools/export.py
+-r--------  2.0 unx     2999 b- defN 24-Apr-12 09:49 mindformers/tools/generic.py
+-r--------  2.0 unx     6862 b- defN 24-Apr-12 09:49 mindformers/tools/hccl_tools.py
+-r--------  2.0 unx     1925 b- defN 24-Apr-12 09:49 mindformers/tools/image_tools.py
+-r--------  2.0 unx    22670 b- defN 24-Apr-12 09:49 mindformers/tools/logger.py
+-r--------  2.0 unx     2316 b- defN 24-Apr-12 09:49 mindformers/tools/merge_hccl.py
+-r--------  2.0 unx     5653 b- defN 24-Apr-12 09:49 mindformers/tools/moe_token_distribution_tools.py
+-r--------  2.0 unx     3164 b- defN 24-Apr-12 09:49 mindformers/tools/transform_ckpt.py
+-r--------  2.0 unx    13940 b- defN 24-Apr-12 09:49 mindformers/tools/utils.py
+-r--------  2.0 unx      842 b- defN 24-Apr-12 09:49 mindformers/tools/cloud_adapter/__init__.py
+-r--------  2.0 unx     8522 b- defN 24-Apr-12 09:49 mindformers/tools/cloud_adapter/cloud_adapter.py
+-r--------  2.0 unx     3464 b- defN 24-Apr-12 09:49 mindformers/tools/cloud_adapter/cloud_monitor.py
+-r--------  2.0 unx      152 b- defN 24-Apr-12 09:49 mindformers/tools/hub/__init__.py
+-r--------  2.0 unx    27727 b- defN 24-Apr-12 09:49 mindformers/tools/hub/dynamic_module_utils.py
+-r--------  2.0 unx    32184 b- defN 24-Apr-12 09:49 mindformers/tools/hub/hub.py
+-r--------  2.0 unx      942 b- defN 24-Apr-12 09:49 mindformers/tools/register/__init__.py
+-r--------  2.0 unx    11052 b- defN 24-Apr-12 09:49 mindformers/tools/register/config.py
+-r--------  2.0 unx     7056 b- defN 24-Apr-12 09:49 mindformers/tools/register/register.py
+-r--------  2.0 unx     1982 b- defN 24-Apr-12 09:49 mindformers/trainer/__init__.py
+-r--------  2.0 unx    53834 b- defN 24-Apr-12 09:49 mindformers/trainer/base_trainer.py
+-r--------  2.0 unx     4428 b- defN 24-Apr-12 09:49 mindformers/trainer/build_trainer.py
+-r--------  2.0 unx    55851 b- defN 24-Apr-12 09:49 mindformers/trainer/config_args.py
+-r--------  2.0 unx     5949 b- defN 24-Apr-12 09:49 mindformers/trainer/optimizer_grouped_parameters.py
+-r--------  2.0 unx    71479 b- defN 24-Apr-12 09:49 mindformers/trainer/trainer.py
+-r--------  2.0 unx    82482 b- defN 24-Apr-12 09:49 mindformers/trainer/training_args.py
+-r--------  2.0 unx    35774 b- defN 24-Apr-12 09:49 mindformers/trainer/utils.py
+-r--------  2.0 unx      821 b- defN 24-Apr-12 09:49 mindformers/trainer/causal_language_modeling/__init__.py
+-r--------  2.0 unx    21619 b- defN 24-Apr-12 09:49 mindformers/trainer/causal_language_modeling/causal_language_modeling.py
+-r--------  2.0 unx      866 b- defN 24-Apr-12 09:49 mindformers/trainer/contrastive_language_image_pretrain/__init__.py
+-r--------  2.0 unx     4633 b- defN 24-Apr-12 09:49 mindformers/trainer/contrastive_language_image_pretrain/contrastive_language_image_pretrain.py
+-r--------  2.0 unx      795 b- defN 24-Apr-12 09:49 mindformers/trainer/general_task_trainer/__init__.py
+-r--------  2.0 unx     9138 b- defN 24-Apr-12 09:49 mindformers/trainer/general_task_trainer/general_task_trainer.py
+-r--------  2.0 unx      928 b- defN 24-Apr-12 09:49 mindformers/trainer/image_classification/__init__.py
+-r--------  2.0 unx     4622 b- defN 24-Apr-12 09:49 mindformers/trainer/image_classification/group_ic_params.py
+-r--------  2.0 unx     9536 b- defN 24-Apr-12 09:49 mindformers/trainer/image_classification/image_classification.py
+-r--------  2.0 unx     8281 b- defN 24-Apr-12 09:49 mindformers/trainer/image_classification/zero_shot_image_classification.py
+-r--------  2.0 unx      823 b- defN 24-Apr-12 09:49 mindformers/trainer/image_to_text_generation/__init__.py
+-r--------  2.0 unx     6161 b- defN 24-Apr-12 09:49 mindformers/trainer/image_to_text_generation/image_to_text_generation.py
+-r--------  2.0 unx      818 b- defN 24-Apr-12 09:49 mindformers/trainer/image_to_text_retrieval/__init__.py
+-r--------  2.0 unx    12082 b- defN 24-Apr-12 09:49 mindformers/trainer/image_to_text_retrieval/eval_utils.py
+-r--------  2.0 unx     7801 b- defN 24-Apr-12 09:49 mindformers/trainer/image_to_text_retrieval/image_to_text_retrieval.py
+-r--------  2.0 unx      818 b- defN 24-Apr-12 09:49 mindformers/trainer/masked_image_modeling/__init__.py
+-r--------  2.0 unx     2197 b- defN 24-Apr-12 09:49 mindformers/trainer/masked_image_modeling/group_mim_parameters.py
+-r--------  2.0 unx     7482 b- defN 24-Apr-12 09:49 mindformers/trainer/masked_image_modeling/masked_image_modeling_pretrain.py
+-r--------  2.0 unx      830 b- defN 24-Apr-12 09:49 mindformers/trainer/masked_language_modeling/__init__.py
+-r--------  2.0 unx     6931 b- defN 24-Apr-12 09:49 mindformers/trainer/masked_language_modeling/masked_language_modeling_pretrain.py
+-r--------  2.0 unx      799 b- defN 24-Apr-12 09:49 mindformers/trainer/question_answering/__init__.py
+-r--------  2.0 unx     9308 b- defN 24-Apr-12 09:49 mindformers/trainer/question_answering/question_answering.py
+-r--------  2.0 unx      803 b- defN 24-Apr-12 09:49 mindformers/trainer/text_classfication/__init__.py
+-r--------  2.0 unx     9476 b- defN 24-Apr-12 09:49 mindformers/trainer/text_classfication/text_classification.py
+-r--------  2.0 unx      807 b- defN 24-Apr-12 09:49 mindformers/trainer/token_classification/__init__.py
+-r--------  2.0 unx     9590 b- defN 24-Apr-12 09:49 mindformers/trainer/token_classification/token_classification.py
+-r--------  2.0 unx      795 b- defN 24-Apr-12 09:49 mindformers/trainer/translation/__init__.py
+-r--------  2.0 unx     6810 b- defN 24-Apr-12 09:49 mindformers/trainer/translation/translation_finetune.py
+-r--------  2.0 unx      122 b- defN 24-Apr-12 09:49 mindformers/utils/__init__.py
+-r--------  2.0 unx     1626 b- defN 24-Apr-12 09:49 mindformers/utils/convert_utils.py
+-r--------  2.0 unx    14764 b- defN 24-Apr-12 09:49 mindformers/utils/image_transforms.py
+-r--------  2.0 unx     3153 b- defN 24-Apr-12 09:49 mindformers/utils/image_utils.py
+-r--------  2.0 unx     1739 b- defN 24-Apr-12 09:49 mindformers/utils/import_utils.py
+-r--------  2.0 unx      913 b- defN 24-Apr-12 09:49 mindformers/wrapper/__init__.py
+-r--------  2.0 unx    14329 b- defN 24-Apr-12 09:49 mindformers/wrapper/adaptive_loss_scale.py
+-r--------  2.0 unx     4117 b- defN 24-Apr-12 09:49 mindformers/wrapper/build_wrapper.py
+-r--------  2.0 unx    12419 b- defN 24-Apr-12 09:49 mindformers/wrapper/wrapper.py
+-rw-r--r--  2.0 unx    11357 b- defN 24-Apr-12 09:49 mindformers-1.1.0rc1.dist-info/LICENSE
+-rw-r--r--  2.0 unx    21404 b- defN 24-Apr-12 09:49 mindformers-1.1.0rc1.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-Apr-12 09:49 mindformers-1.1.0rc1.dist-info/WHEEL
+-r--------  2.0 unx       12 b- defN 24-Apr-12 09:49 mindformers-1.1.0rc1.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx    44721 b- defN 24-Apr-12 09:49 mindformers-1.1.0rc1.dist-info/RECORD
+453 files, 4963338 bytes uncompressed, 1265656 bytes compressed:  74.5%
```

## zipnote {}

```diff
@@ -3,14 +3,38 @@
 
 Filename: configs/bert/run_bert_base_uncased.yaml
 Comment: 
 
 Filename: configs/bert/run_bert_tiny_uncased.yaml
 Comment: 
 
+Filename: configs/blip2/run_blip2_stage1_vit_g_qformer_pretrain.yaml
+Comment: 
+
+Filename: configs/blip2/run_blip2_stage1_vit_g_retrieval_flickr30k.yaml
+Comment: 
+
+Filename: configs/blip2/run_blip2_stage1_vit_g_zero_shot_image_classification_cifar100.yaml
+Comment: 
+
+Filename: configs/blip2/run_blip2_stage2_vit_g_baichuan_7b.yaml
+Comment: 
+
+Filename: configs/blip2/run_blip2_stage2_vit_g_baichuan_7b_image_to_text_generation.yaml
+Comment: 
+
+Filename: configs/blip2/run_blip2_stage2_vit_g_llama_7b.yaml
+Comment: 
+
+Filename: configs/blip2/run_blip2_stage2_vit_g_llama_7b_910b.yaml
+Comment: 
+
+Filename: configs/blip2/run_blip2_stage2_vit_g_llama_7b_image_to_text_generation.yaml
+Comment: 
+
 Filename: configs/bloom/run_bloom_560m.yaml
 Comment: 
 
 Filename: configs/bloom/run_bloom_7.1b.yaml
 Comment: 
 
 Filename: configs/bloom/run_bloom_7.1b_910b.yaml
@@ -51,23 +75,23 @@
 
 Filename: configs/codegeex2/run_codegeex2_6b_finetune.yaml
 Comment: 
 
 Filename: configs/codegeex2/run_codegeex2_6b_finetune_2048.yaml
 Comment: 
 
-Filename: configs/codellama/finetune_codellama_34b_32p.yaml
-Comment: 
-
 Filename: configs/codellama/predict_codellama_34b_910b.yaml
 Comment: 
 
 Filename: configs/codellama/run_codellama_34b_910b.yaml
 Comment: 
 
+Filename: configs/codellama/run_codellama_34b_910b_32p.yaml
+Comment: 
+
 Filename: configs/convert_config/run_convert.yaml
 Comment: 
 
 Filename: configs/convert_config/run_reversed_convert.yaml
 Comment: 
 
 Filename: configs/general/run_general_task.yaml
@@ -81,15 +105,15 @@
 
 Filename: configs/glm/run_glm_6b_lora.yaml
 Comment: 
 
 Filename: configs/glm/run_glm_6b_lora_infer.yaml
 Comment: 
 
-Filename: configs/glm2/predict_glm2_6b.yaml
+Filename: configs/glm2/export_glm2_6b.yaml
 Comment: 
 
 Filename: configs/glm2/run_glm2_6b.yaml
 Comment: 
 
 Filename: configs/glm2/run_glm2_6b_finetune_2k_800T_A2_64G.yaml
 Comment: 
@@ -120,15 +144,15 @@
 
 Filename: configs/glm2/run_glm2_6b_lora_eval.yaml
 Comment: 
 
 Filename: configs/glm2/run_glm2_6b_ptuning2.yaml
 Comment: 
 
-Filename: configs/glm3/predict_glm3_6b.yaml
+Filename: configs/glm3/export_glm3_6b.yaml
 Comment: 
 
 Filename: configs/glm3/run_glm3_6b.yaml
 Comment: 
 
 Filename: configs/glm3/run_glm3_6b_finetune_2k_800T_A2_64G.yaml
 Comment: 
@@ -174,38 +198,35 @@
 
 Filename: configs/llama/run_llama_7b_910b.yaml
 Comment: 
 
 Filename: configs/llama/run_llama_7b_lora.yaml
 Comment: 
 
-Filename: configs/llama2/predict_llama2_13b.yaml
+Filename: configs/llama2/export_llama2_13b.yaml
 Comment: 
 
-Filename: configs/llama2/predict_llama2_70b.yaml
+Filename: configs/llama2/export_llama2_7b.yaml
 Comment: 
 
-Filename: configs/llama2/predict_llama2_7b.yaml
+Filename: configs/llama2/predict_llama2_70b_910b.yaml
 Comment: 
 
 Filename: configs/llama2/run_llama2_13b.yaml
 Comment: 
 
 Filename: configs/llama2/run_llama2_13b_910b.yaml
 Comment: 
 
 Filename: configs/llama2/run_llama2_13b_910b_auto_parallel.yaml
 Comment: 
 
 Filename: configs/llama2/run_llama2_13b_910b_finetune.yaml
 Comment: 
 
-Filename: configs/llama2/run_llama2_13b_bf16_800T_A2_finetune.yaml
-Comment: 
-
 Filename: configs/llama2/run_llama2_13b_lora_910b.yaml
 Comment: 
 
 Filename: configs/llama2/run_llama2_70b.yaml
 Comment: 
 
 Filename: configs/llama2/run_llama2_70b_910b.yaml
@@ -213,32 +234,29 @@
 
 Filename: configs/llama2/run_llama2_70b_910b_auto_parallel.yaml
 Comment: 
 
 Filename: configs/llama2/run_llama2_70b_910b_finetune.yaml
 Comment: 
 
-Filename: configs/llama2/run_llama2_70b_bf16_800T_A2.yaml
+Filename: configs/llama2/run_llama2_70b_bf16_910b.yaml
 Comment: 
 
 Filename: configs/llama2/run_llama2_7b.yaml
 Comment: 
 
 Filename: configs/llama2/run_llama2_7b_910b.yaml
 Comment: 
 
 Filename: configs/llama2/run_llama2_7b_910b_auto_parallel.yaml
 Comment: 
 
 Filename: configs/llama2/run_llama2_7b_910b_finetune.yaml
 Comment: 
 
-Filename: configs/llama2/run_llama2_7b_bf16_800T_A2_finetune.yaml
-Comment: 
-
 Filename: configs/llama2/run_llama2_7b_lora_910b.yaml
 Comment: 
 
 Filename: configs/mae/run_mae_vit_base_p16_224_800ep.yaml
 Comment: 
 
 Filename: configs/pangualpha/run_pangualpha_13b.yaml
@@ -297,17 +315,14 @@
 
 Filename: mindformers/auto_class.py
 Comment: 
 
 Filename: mindformers/mindformer_book.py
 Comment: 
 
-Filename: mindformers/model_runner.py
-Comment: 
-
 Filename: mindformers/version_control.py
 Comment: 
 
 Filename: mindformers/core/__init__.py
 Comment: 
 
 Filename: mindformers/core/clip_grad.py
@@ -507,75 +522,63 @@
 
 Filename: mindformers/dataset/transforms/text_transforms.py
 Comment: 
 
 Filename: mindformers/dataset/transforms/vision_transforms.py
 Comment: 
 
-Filename: mindformers/experimental/__init__.py
-Comment: 
-
-Filename: mindformers/experimental/distri_ckpt/__init__.py
-Comment: 
-
-Filename: mindformers/experimental/distri_ckpt/checkpointing.py
+Filename: mindformers/generation/__init__.py
 Comment: 
 
-Filename: mindformers/experimental/distri_cores/__init__.py
+Filename: mindformers/generation/beam_search.py
 Comment: 
 
-Filename: mindformers/experimental/distri_cores/activation_checkpointing/__init__.py
+Filename: mindformers/generation/generation_config.py
 Comment: 
 
-Filename: mindformers/experimental/distri_cores/optimizer/__init__.py
+Filename: mindformers/generation/logits_process.py
 Comment: 
 
-Filename: mindformers/experimental/distri_cores/optimizer/grads_accumulate/__init__.py
+Filename: mindformers/generation/streamers.py
 Comment: 
 
-Filename: mindformers/experimental/distri_cores/optimizer/zero/__init__.py
+Filename: mindformers/generation/text_generator.py
 Comment: 
 
-Filename: mindformers/experimental/distri_cores/optimizer/zero/adamw.py
+Filename: mindformers/generation/utils.py
 Comment: 
 
-Filename: mindformers/experimental/distri_cores/pipeline_parallel/__init__.py
+Filename: mindformers/inference/__init__.py
 Comment: 
 
-Filename: mindformers/experimental/distri_cores/sequence_parallel/__init__.py
+Filename: mindformers/inference/context.py
 Comment: 
 
-Filename: mindformers/experimental/distri_cores/sequence_parallel/ring_attention.py
+Filename: mindformers/inference/infer_config.py
 Comment: 
 
-Filename: mindformers/experimental/distri_cores/sequence_parallel/utils.py
+Filename: mindformers/inference/infer_task.py
 Comment: 
 
-Filename: mindformers/experimental/distri_cores/tensor_parallel/__init__.py
+Filename: mindformers/inference/pipeline.py
 Comment: 
 
-Filename: mindformers/generation/__init__.py
+Filename: mindformers/inference/postprocess_sampler.py
 Comment: 
 
-Filename: mindformers/generation/beam_search.py
+Filename: mindformers/inference/infers/__init__.py
 Comment: 
 
-Filename: mindformers/generation/generation_config.py
+Filename: mindformers/inference/infers/base_infer.py
 Comment: 
 
-Filename: mindformers/generation/logits_process.py
+Filename: mindformers/inference/infers/cache_engine.py
 Comment: 
 
-Filename: mindformers/generation/streamers.py
-Comment: 
-
-Filename: mindformers/generation/text_generator.py
-Comment: 
-
-Filename: mindformers/generation/utils.py
+Filename: mindformers/inference/infers/text_generator_infer.py
 Comment: 
 
 Filename: mindformers/models/__init__.py
 Comment: 
 
 Filename: mindformers/models/base_config.py
 Comment: 
@@ -1014,26 +1017,17 @@
 
 Filename: mindformers/modules/__init__.py
 Comment: 
 
 Filename: mindformers/modules/activation.py
 Comment: 
 
-Filename: mindformers/modules/block_tables.py
-Comment: 
-
-Filename: mindformers/modules/cache_engine.py
-Comment: 
-
 Filename: mindformers/modules/flash_attention.py
 Comment: 
 
-Filename: mindformers/modules/infer_attention.py
-Comment: 
-
 Filename: mindformers/modules/kvcache_mgr.py
 Comment: 
 
 Filename: mindformers/modules/layers.py
 Comment: 
 
 Filename: mindformers/modules/local_block_sparse_attention.py
@@ -1155,14 +1149,17 @@
 
 Filename: mindformers/tools/download_tools.py
 Comment: 
 
 Filename: mindformers/tools/download_tools_multithread.py
 Comment: 
 
+Filename: mindformers/tools/export.py
+Comment: 
+
 Filename: mindformers/tools/generic.py
 Comment: 
 
 Filename: mindformers/tools/hccl_tools.py
 Comment: 
 
 Filename: mindformers/tools/image_tools.py
@@ -1176,17 +1173,14 @@
 
 Filename: mindformers/tools/moe_token_distribution_tools.py
 Comment: 
 
 Filename: mindformers/tools/transform_ckpt.py
 Comment: 
 
-Filename: mindformers/tools/transform_ckpt_lora.py
-Comment: 
-
 Filename: mindformers/tools/utils.py
 Comment: 
 
 Filename: mindformers/tools/cloud_adapter/__init__.py
 Comment: 
 
 Filename: mindformers/tools/cloud_adapter/cloud_adapter.py
@@ -1344,23 +1338,23 @@
 
 Filename: mindformers/wrapper/build_wrapper.py
 Comment: 
 
 Filename: mindformers/wrapper/wrapper.py
 Comment: 
 
-Filename: mindformers-1.1.0.dist-info/LICENSE
+Filename: mindformers-1.1.0rc1.dist-info/LICENSE
 Comment: 
 
-Filename: mindformers-1.1.0.dist-info/METADATA
+Filename: mindformers-1.1.0rc1.dist-info/METADATA
 Comment: 
 
-Filename: mindformers-1.1.0.dist-info/WHEEL
+Filename: mindformers-1.1.0rc1.dist-info/WHEEL
 Comment: 
 
-Filename: mindformers-1.1.0.dist-info/top_level.txt
+Filename: mindformers-1.1.0rc1.dist-info/top_level.txt
 Comment: 
 
-Filename: mindformers-1.1.0.dist-info/RECORD
+Filename: mindformers-1.1.0rc1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## configs/README.md

```diff
@@ -132,23 +132,22 @@
         - type: 
         - checkpoint_name_or_path: 
 
           *\# *
         - top_k: top_ktokens
         - top_p: top_ptokens
         - do_sample: top_ktop_pFalsetop_ktop_p1
-        - use_past: TruePaged Attention[](https://gitee.com/mindspore/mindformers/tree/dev/docs#text-generator)
+        - use_past: True[](https://gitee.com/mindspore/mindformers/tree/dev/docs#text-generator)
         - max_decode_length: 
-        - max_length: max_decode_lengthmax_length
-        - max_new_tokens: max_lengthmax_new_tokens
-        - min_length: 
-        - min_new_tokens: min_lengthmin_new_tokens
         - repetition_penalty: 11
-        - block_size: Paged Attentionblock
-        - num_blocks: Paged Attentionblocksbatch_size*seq_length<=block_size*num_blocksPA
+        - use_paged_attention: Paged AttentionMS Lite
+        - pa_block_size: Paged Attentionblock
+        - pa_num_blocks: Paged Attentionblocks
+        - use_prompt_flash_attention: PromptFlashAttention
+        - use_incre_flash_attention: IncreFlashAttentionuse_past=True
 - lr_schedule: 
     - type: 
 - layer_scale: 
 - layer_decay: 
 - optimizer: 
     - type: 
     - weight_decay: 
```

## configs/codellama/predict_codellama_34b_910b.yaml

```diff
@@ -95,14 +95,15 @@
     softmax_compute_type: "float16"
     rotary_dtype: "float16"
     param_init_type: "float16"
     use_past: False
     scaling_factor: 1.0
     extend_method: "None" # support "None", "PI", "NTK"
     use_flash_attention: False
+    use_paged_attention: False  # PA only supported in inference
     block_size: 16
     num_blocks: 512
     is_dynamic: False
     use_kvcache_op: False
     is_flexible_shape: False
     offset: 0
     use_rope_slice: False
```

## configs/codellama/run_codellama_34b_910b.yaml

```diff
@@ -147,18 +147,21 @@
     theta: 1000000.0
     compute_dtype: "float16"
     layernorm_compute_type: "float32"
     softmax_compute_type: "float16"
     rotary_dtype: "float16"
     param_init_type: "float16"
     use_past: False
+    pretrain_seqlen: 4096 # seqlen of the pretrain checkpoint
     extend_method: "None" # support "None", "PI", "NTK"
+    compute_in_2d: True
     use_flash_attention: True
     fine_grain_interleave: 2
     offset: 0
+    use_past_shard: False
     checkpoint_name_or_path: ""
     repetition_penalty: 1
     max_decode_length: 512
     top_k: 3
     top_p: 1
     do_sample: False
   arch:
```

## configs/glm2/run_glm2_6b.yaml

```diff
@@ -46,16 +46,16 @@
     multi_query_group_num: 2
     apply_query_key_layer_scaling: True
     attention_softmax_in_fp32: True
     fp32_residual_connection: False
     quantization_bit: 0
     pre_seq_len: None
     prefix_projection: False
-    param_init_type: "bfloat16"
-    compute_dtype: "bfloat16"
+    param_init_type: "float16"
+    compute_dtype: "float16"
     layernorm_compute_type: "float32"
     use_past: False
     use_flash_attention: False # when use FlashAttention, seq_length should be multiple of 16
     use_prompt_flash_attention: False
     use_incre_flash_attention: False
     eos_token_id: 2
     pad_token_id: 0
```

## configs/glm2/run_glm2_6b_finetune_2k_800T_A2_64G.yaml

```diff
@@ -45,16 +45,16 @@
     multi_query_group_num: 2
     apply_query_key_layer_scaling: True
     attention_softmax_in_fp32: True
     fp32_residual_connection: False
     quantization_bit: 0
     pre_seq_len: None
     prefix_projection: False
-    param_init_type: "bfloat16"
-    compute_dtype: "bfloat16"
+    param_init_type: "float16"
+    compute_dtype: "float16"
     layernorm_compute_type: "float32"
     use_past: False
     use_flash_attention: True # when use FlashAttention, seq_length should be multiple of 16
     eos_token_id: 2
     pad_token_id: 0
     repetition_penalty: 1.0
     max_decode_length: 256
```

## configs/glm2/run_glm2_6b_lora_800T_A2_64G.yaml

```diff
@@ -45,16 +45,16 @@
     multi_query_group_num: 2
     apply_query_key_layer_scaling: True
     attention_softmax_in_fp32: True
     fp32_residual_connection: False
     quantization_bit: 0
     pre_seq_len: None
     prefix_projection: False
-    param_init_type: "bfloat16"
-    compute_dtype: "bfloat16"
+    param_init_type: "float16"
+    compute_dtype: "float16"
     layernorm_compute_type: "float32"
     use_past: False
     use_flash_attention: True # when use FlashAttention, seq_length should be multiple of 16
     eos_token_id: 2
     pad_token_id: 0
     repetition_penalty: 1.0
     max_decode_length: 256
```

## configs/glm2/run_glm2_6b_ptuning2.yaml

```diff
@@ -45,16 +45,16 @@
     multi_query_group_num: 2
     apply_query_key_layer_scaling: True
     attention_softmax_in_fp32: True
     fp32_residual_connection: False
     quantization_bit: 0
     pre_seq_len: 128
     prefix_projection: False
-    param_init_type: "bfloat16"
-    compute_dtype: "bfloat16"
+    param_init_type: "float16"
+    compute_dtype: "float16"
     layernorm_compute_type: "float32"
     softmax_compute_type: "float32"
     use_past: False
     use_flash_attention: False # when use FlashAttention, seq_length should be multiple of 16
     eos_token_id: 2
     pad_token_id: 0
     max_decode_length: 256
```

## configs/glm3/run_glm3_6b.yaml

```diff
@@ -46,16 +46,16 @@
     multi_query_group_num: 2
     apply_query_key_layer_scaling: True
     attention_softmax_in_fp32: True
     fp32_residual_connection: False
     quantization_bit: 0
     pre_seq_len: None
     prefix_projection: False
-    param_init_type: "bfloat16"
-    compute_dtype: "bfloat16"
+    param_init_type: "float16"
+    compute_dtype: "float16"
     layernorm_compute_type: "float32"
     use_past: False
     use_flash_attention: False # when use FlashAttention, seq_length should be multiple of 16
     use_prompt_flash_attention: False
     use_incre_flash_attention: False
     eos_token_id: 2
     pad_token_id: 0
```

## configs/glm3/run_glm3_6b_finetune_2k_800T_A2_64G.yaml

```diff
@@ -45,16 +45,16 @@
     multi_query_group_num: 2
     apply_query_key_layer_scaling: True
     attention_softmax_in_fp32: True
     fp32_residual_connection: False
     quantization_bit: 0
     pre_seq_len: None
     prefix_projection: False
-    param_init_type: "bfloat16"
-    compute_dtype: "bfloat16"
+    param_init_type: "float16"
+    compute_dtype: "float16"
     layernorm_compute_type: "float32"
     use_past: False
     use_flash_attention: False # when use FlashAttention, seq_length should be multiple of 16
     eos_token_id: 2
     pad_token_id: 0
     repetition_penalty: 1.0
     max_decode_length: 256
```

## configs/gpt2/run_gpt2.yaml

```diff
@@ -128,14 +128,15 @@
     hidden_size: 768
     num_layers: 12
     num_heads: 12
     expand_ratio: 4
     hidden_act: "gelu"
     use_flash_attention: False
     use_prompt_flash_attention: False
+    use_incre_flash_attention: False
     hidden_dropout_rate: 0.1
     attention_dropout_rate: 0.1
     param_init_type: "float32"
     layernorm_compute_type: "float32"
     softmax_compute_type: "float32"
     compute_dtype: "float16"
     checkpoint_name_or_path: "gpt2"
@@ -145,15 +146,15 @@
     top_k: 5
     top_p: 1
     do_sample: True
     use_past: False
   arch:
     type: GPT2LMHeadModel
 
-# lr schedule
+# lr sechdule
 lr_schedule:
   type: polynomial
   learning_rate: 0.0001
   lr_end: 0.00001
   warmup_steps: 0
   total_steps: -1 # -1 means it will load the total steps of the dataset
 layer_scale: False
```

## configs/gpt2/run_gpt2_13b.yaml

```diff
@@ -139,14 +139,15 @@
     hidden_size: 5120
     num_layers: 40
     num_heads: 40
     expand_ratio: 4
     hidden_act: "fast_gelu"
     use_flash_attention: False
     use_prompt_flash_attention: False
+    use_incre_flash_attention: False
     hidden_dropout_rate: 0.1
     attention_dropout_rate: 0.1
     param_init_type: "float16"
     layernorm_compute_type: "float32"
     softmax_compute_type: "float16"
     compute_dtype: "float16"
     checkpoint_name_or_path: ""
@@ -156,15 +157,15 @@
     top_k: 5
     top_p: 1
     do_sample: True
     use_past: False
   arch:
     type: GPT2LMHeadModel
 
-# lr schedule
+# lr sechdule
 lr_schedule:
   type: polynomial
   learning_rate: 0.00005
   lr_end: 0.000001
   warmup_steps: 2000
   total_steps: -1 # -1 means it will load the total steps of the dataset
 layer_scale: False
```

## configs/gpt2/run_gpt2_13b_910b.yaml

```diff
@@ -146,14 +146,15 @@
     hidden_size: 5120
     num_layers: 40
     num_heads: 40
     expand_ratio: 4
     hidden_act: "fast_gelu"
     use_flash_attention: False
     use_prompt_flash_attention: False
+    use_incre_flash_attention: False
     hidden_dropout_rate: 0.1
     attention_dropout_rate: 0.1
     param_init_type: "float16"
     layernorm_compute_type: "float32"
     softmax_compute_type: "float16"
     compute_dtype: "float16"
     checkpoint_name_or_path: ""
@@ -163,15 +164,15 @@
     top_k: 5
     top_p: 1
     do_sample: True
     use_past: False
   arch:
     type: GPT2LMHeadModel
 
-# lr schedule
+# lr sechdule
 lr_schedule:
   type: CosineWithWarmUpLR
   learning_rate: 6.e-5
   lr_end: 6.e-6
   warmup_steps: 0
   total_steps: -1 # -1 means it will load the total steps of the dataset
 layer_scale: False
```

## configs/gpt2/run_gpt2_52b.yaml

```diff
@@ -137,14 +137,15 @@
     hidden_size: 8192
     num_layers: 64
     num_heads: 32
     expand_ratio: 4
     hidden_act: "fast_gelu"
     use_flash_attention: False
     use_prompt_flash_attention: False
+    use_incre_flash_attention: False
     hidden_dropout_rate: 0.1
     attention_dropout_rate: 0.1
     param_init_type: "float16"
     layernorm_compute_type: "float32"
     softmax_compute_type: "float16"
     compute_dtype: "float16"
     checkpoint_name_or_path: ""
@@ -154,15 +155,15 @@
     top_k: 5
     top_p: 1
     do_sample: True
     use_past: False
   arch:
     type: GPT2LMHeadModel
 
-# lr schedule
+# lr sechdule
 lr_schedule:
   type: polynomial
   learning_rate: 0.00005
   lr_end: 0.000001
   warmup_steps: 2000
   total_steps: -1 # -1 means it will load the total steps of the dataset
 layer_scale: False
```

## configs/gpt2/run_gpt2_lora.yaml

```diff
@@ -129,14 +129,15 @@
     hidden_size: 768
     num_layers: 12
     num_heads: 12
     expand_ratio: 4
     hidden_act: "gelu"
     use_flash_attention: False
     use_prompt_flash_attention: False
+    use_incre_flash_attention: False
     hidden_dropout_rate: 0.1
     attention_dropout_rate: 0.1
     param_init_type: "float32"
     layernorm_compute_type: "float32"
     softmax_compute_type: "float32"
     compute_dtype: "float16"
     checkpoint_name_or_path: "gpt2_lora"
@@ -153,15 +154,15 @@
       lora_rank: 64
       lora_alpha: 128
       lora_dropout: 0.1
       target_modules: '.*dense1.*|.*dense3.*'
   arch:
     type: GPT2LMHeadModel
 
-# lr schedule
+# lr sechdule
 lr_schedule:
   type: polynomial
   learning_rate: 0.001
   lr_end: 0.0001
   warmup_steps: 0
   total_steps: -1 # -1 means it will load the total steps of the dataset
 layer_scale: False
```

## configs/gpt2/run_gpt2_txtcls.yaml

```diff
@@ -129,14 +129,15 @@
     num_labels: 2
     num_layers: 12
     num_heads: 12
     expand_ratio: 4
     hidden_act: "gelu"
     use_flash_attention: False
     use_prompt_flash_attention: False
+    use_incre_flash_attention: False
     hidden_dropout_rate: 0.1
     attention_dropout_rate: 0.1
     param_init_type: "float32"
     layernorm_compute_type: "float32"
     softmax_compute_type: "float32"
     compute_dtype: "float16"
     checkpoint_name_or_path: "gpt2"
@@ -145,15 +146,15 @@
     max_decode_length: 1024
     top_k: 5
     top_p: 1
     do_sample: True
   arch:
     type: GPT2ForSequenceClassification
 
-# lr schedule
+# lr sechdule
 lr_schedule:
   type: polynomial
   learning_rate: 0.0001
   lr_end: 0.00001
   warmup_steps: 0
   total_steps: -1 # -1 means it will load the total steps of the dataset
 layer_scale: False
```

## configs/gpt2/run_gpt2_xl.yaml

```diff
@@ -137,14 +137,15 @@
     hidden_size: 1600
     num_layers: 48
     num_heads: 25
     expand_ratio: 4
     hidden_act: "fast_gelu"
     use_flash_attention: False
     use_prompt_flash_attention: False
+    use_incre_flash_attention: False
     hidden_dropout_rate: 0.1
     attention_dropout_rate: 0.1
     param_init_type: "float16"
     layernorm_compute_type: "float32"
     softmax_compute_type: "float16"
     compute_dtype: "float16"
     checkpoint_name_or_path: "gpt2_xl"
@@ -154,15 +155,15 @@
     top_k: 5
     top_p: 1
     do_sample: True
     use_past: False
   arch:
     type: GPT2LMHeadModel
 
-# lr schedule
+# lr sechdule
 lr_schedule:
   type: polynomial
   learning_rate: 0.00005
   lr_end: 0.000001
   warmup_steps: 2000
   total_steps: -1 # -1 means it will load the total steps of the dataset
 layer_scale: False
```

## configs/gpt2/run_gpt2_xl_lora.yaml

```diff
@@ -139,14 +139,15 @@
     hidden_size: 1600
     num_layers: 48
     num_heads: 25
     expand_ratio: 4
     hidden_act: "fast_gelu"
     use_flash_attention: False
     use_prompt_flash_attention: False
+    use_incre_flash_attention: False
     hidden_dropout_rate: 0.1
     attention_dropout_rate: 0.1
     param_init_type: "float16"
     layernorm_compute_type: "float32"
     softmax_compute_type: "float16"
     compute_dtype: "float16"
     checkpoint_name_or_path: "gpt2_xl_lora"
@@ -164,15 +165,15 @@
       lora_alpha: 128
       lora_dropout: 0.1
       target_modules: '.*dense1.*|.*dense3.*'
   arch:
     type: GPT2LMHeadModel
 
 
-# lr schedule
+# lr sechdule
 lr_schedule:
   type: polynomial
   learning_rate: 0.0005
   lr_end: 0.00001
   warmup_steps: 2000
   total_steps: -1 # -1 means it will load the total steps of the dataset
 layer_scale: False
```

## configs/llama/run_llama_13b.yaml

```diff
@@ -147,17 +147,20 @@
     ignore_token_id: -100
     compute_dtype: "float16"
     layernorm_compute_type: "float32"
     softmax_compute_type: "float16"
     rotary_dtype: "float16"
     param_init_type: "float16"
     use_past: False
+    pretrain_seqlen: 2048 # seqlen of the pretrain checkpoint: 2048 for llama and 4096 for llama2
     extend_method: "None" # support "None", "PI", "NTK"
+    compute_in_2d: False
     use_flash_attention: False
     offset: 0
+    use_past_shard: False
     checkpoint_name_or_path: "llama_13b"
     repetition_penalty: 1
     max_decode_length: 512
     top_k: 3
     top_p: 1
     do_sample: False
   arch:
```

## configs/llama/run_llama_13b_910b.yaml

```diff
@@ -147,17 +147,20 @@
     ignore_token_id: -100
     compute_dtype: "float16"
     layernorm_compute_type: "float32"
     softmax_compute_type: "float16"
     rotary_dtype: "float16"
     param_init_type: "float16"
     use_past: False
+    pretrain_seqlen: 2048 # seqlen of the pretrain checkpoint: 2048 for llama and 4096 for llama2
     extend_method: "None" # support "None", "PI", "NTK"
+    compute_in_2d: True
     use_flash_attention: False
     offset: 0
+    use_past_shard: False
     checkpoint_name_or_path: "llama_13b"
     repetition_penalty: 1
     max_decode_length: 512
     top_k: 3
     top_p: 1
     do_sample: False
   arch:
```

## configs/llama/run_llama_7b.yaml

```diff
@@ -147,17 +147,20 @@
     ignore_token_id: -100
     compute_dtype: "float16"
     layernorm_compute_type: "float32"
     softmax_compute_type: "float16"
     rotary_dtype: "float16"
     param_init_type: "float16"
     use_past: False
+    pretrain_seqlen: 2048 # seqlen of the pretrain checkpoint: 2048 for llama and 4096 for llama2
     extend_method: "None" # support "None", "PI", "NTK"
+    compute_in_2d: False
     use_flash_attention: False
     offset: 0
+    use_past_shard: False
     checkpoint_name_or_path: "llama_7b"
     repetition_penalty: 1
     max_decode_length: 512
     top_k: 3
     top_p: 1
     do_sample: False
   arch:
```

## configs/llama/run_llama_7b_910b.yaml

```diff
@@ -147,17 +147,20 @@
     ignore_token_id: -100
     compute_dtype: "float16"
     layernorm_compute_type: "float32"
     softmax_compute_type: "float16"
     rotary_dtype: "float16"
     param_init_type: "float16"
     use_past: False
+    pretrain_seqlen: 2048 # seqlen of the pretrain checkpoint: 2048 for llama and 4096 for llama2
     extend_method: "None" # support "None", "PI", "NTK"
+    compute_in_2d: True
     use_flash_attention: False
     offset: 0
+    use_past_shard: False
     checkpoint_name_or_path: "llama_7b"
     repetition_penalty: 1
     max_decode_length: 512
     top_k: 3
     top_p: 1
     do_sample: False
   arch:
```

## configs/llama/run_llama_7b_lora.yaml

```diff
@@ -149,17 +149,20 @@
     ignore_token_id: -100
     compute_dtype: "float16"
     layernorm_compute_dtype: "float32"
     softmax_compute_dtype: "float16"
     rotary_dtype: "float16"
     param_init_type: "float16"
     use_past: False
+    pretrain_seqlen: 2048 # seqlen of the pretrain checkpoint: 2048 for llama and 4096 for llama2
     extend_method: "None" # support "None", "PI", "NTK"
+    compute_in_2d: False
     use_flash_attention: False
     offset: 0
+    use_past_shard: False
     checkpoint_name_or_path: "llama_7b_lora"
     repetition_penalty: 1
     max_decode_length: 512
     top_k: 3
     top_p: 1
     do_sample: False
     pet_config:
```

## configs/llama2/run_llama2_13b.yaml

```diff
@@ -149,17 +149,20 @@
     ignore_token_id: -100
     compute_dtype: "float16"
     layernorm_compute_type: "float32"
     softmax_compute_type: "float16"
     rotary_dtype: "float16"
     param_init_type: "float16"
     use_past: False
+    pretrain_seqlen: 4096 # seqlen of the pretrain checkpoint: 2048 for llama and 4096 for llama2
     extend_method: "None" # support "None", "PI", "NTK"
+    compute_in_2d: False
     use_flash_attention: False # FA can accelerate training or finetune
     offset: 0
+    use_past_shard: False
     checkpoint_name_or_path: "llama2_13b"
     repetition_penalty: 1
     max_decode_length: 512
     top_k: 3
     top_p: 1
     do_sample: False
   arch:
```

## configs/llama2/run_llama2_13b_910b.yaml

```diff
@@ -149,15 +149,15 @@
     layernorm_compute_type: "float32"
     softmax_compute_type: "float16"
     rotary_dtype: "float16"
     param_init_type: "float16"
     use_past: False
     scaling_factor: 1.0
     extend_method: "None" # support "None", "PI", "NTK"
-    use_flash_attention: True # FA can accelerate training or finetune
+    use_flash_attention: False # FA can accelerate training or finetune
     offset: 0
     checkpoint_name_or_path: "llama2_13b"
     repetition_penalty: 1
     max_decode_length: 512
     top_k: 3
     top_p: 1
     do_sample: False
```

## configs/llama2/run_llama2_13b_910b_auto_parallel.yaml

```diff
@@ -152,17 +152,20 @@
     ignore_token_id: -100
     compute_dtype: "float16"
     layernorm_compute_type: "float32"
     softmax_compute_type: "float16"
     rotary_dtype: "float16"
     param_init_type: "float16"
     use_past: False
+    pretrain_seqlen: 4096 # seqlen of the pretrain checkpoint: 2048 for llama and 4096 for llama2
     extend_method: "None" # support "None", "PI", "NTK"
+    compute_in_2d: True
     use_flash_attention: False # FA can accelerate training or finetune
     offset: 0
+    use_past_shard: False
     checkpoint_name_or_path: "llama2_13b"
     repetition_penalty: 1
     max_decode_length: 512
     top_k: 3
     top_p: 1
     do_sample: False
   arch:
```

## configs/llama2/run_llama2_13b_910b_finetune.yaml

```diff
@@ -30,15 +30,15 @@
   eps: 1.e-8 # 1e-8
   learning_rate: 1.e-5
 
 # lr sechdule
 lr_schedule:
   type: CosineWithWarmUpLR
   learning_rate: 1.e-5
-  lr_end: 0.0
+  lr_end: 0
   warmup_ratio: 0.03
   total_steps: -1 # -1 means it will load the total steps of the dataset
 
 # dataset
 train_dataset: &train_dataset
   data_loader:
     type: MindDataset
```

## configs/llama2/run_llama2_13b_lora_910b.yaml

```diff
@@ -149,17 +149,20 @@
     ignore_token_id: -100
     compute_dtype: "float16"
     layernorm_compute_type: "float32"
     softmax_compute_type: "float16"
     rotary_dtype: "float16"
     param_init_type: "float16"
     use_past: False
+    pretrain_seqlen: 4096 # seqlen of the pretrain checkpoint: 2048 for llama and 4096 for llama2
     extend_method: "None" # support "None", "PI", "NTK"
+    compute_in_2d: True
     use_flash_attention: False # FA can accelerate training or finetune
     offset: 0
+    use_past_shard: False
     checkpoint_name_or_path: "llama2_13b"
     repetition_penalty: 1
     max_decode_length: 512
     top_k: 3
     top_p: 1
     do_sample: False
     pet_config:
```

## configs/llama2/run_llama2_70b.yaml

```diff
@@ -150,17 +150,20 @@
     ignore_token_id: -100
     compute_dtype: "float16"
     layernorm_compute_type: "float32"
     softmax_compute_type: "float16"
     rotary_dtype: "float16"
     param_init_type: "float16"
     use_past: False
+    pretrain_seqlen: 4096 # seqlen of the pretrain checkpoint: 2048 for llama and 4096 for llama2
     extend_method: "None" # support "None", "PI", "NTK"
+    compute_in_2d: False
     use_flash_attention: False # FA can accelerate training or finetune
     offset: 0
+    use_past_shard: False
     checkpoint_name_or_path: "llama2_70b"
     repetition_penalty: 1
     max_decode_length: 512
     top_k: 3
     top_p: 1
     do_sample: False
   arch:
```

## configs/llama2/run_llama2_70b_910b.yaml

```diff
@@ -31,15 +31,15 @@
   eps: 1.e-8 # 1e-8
   learning_rate: 1.e-5
 
 # lr sechdule
 lr_schedule:
   type: CosineWithWarmUpLR
   learning_rate: 1.e-5
-  lr_end: 0.0
+  lr_end: 0
   warmup_ratio: 0.03
   total_steps: -1 # -1 means it will load the total steps of the dataset
 
 # dataset
 train_dataset: &train_dataset
   data_loader:
     type: MindDataset
```

## configs/llama2/run_llama2_70b_910b_auto_parallel.yaml

```diff
@@ -37,15 +37,15 @@
   eps: 1.e-8 # 1e-8
   learning_rate: 1.e-5
 
 # lr sechdule
 lr_schedule:
   type: CosineWithWarmUpLR
   learning_rate: 1.e-5
-  lr_end: 0.0
+  lr_end: 0
   warmup_ratio: 0.03
   total_steps: -1 # -1 means it will load the total steps of the dataset
 
 # dataset
 train_dataset: &train_dataset
   data_loader:
     type: MindDataset
@@ -155,17 +155,20 @@
     ignore_token_id: -100
     compute_dtype: "float16"
     layernorm_compute_type: "float32"
     softmax_compute_type: "float16"
     rotary_dtype: "float16"
     param_init_type: "float16"
     use_past: False
+    pretrain_seqlen: 4096 # seqlen of the pretrain checkpoint: 2048 for llama and 4096 for llama2
     extend_method: "None" # support "None", "PI", "NTK"
+    compute_in_2d: True
     use_flash_attention: False # FA can accelerate training or finetune
     offset: 0
+    use_past_shard: False
     checkpoint_name_or_path: "llama2_70b"
     repetition_penalty: 1
     max_decode_length: 512
     top_k: 3
     top_p: 1
     do_sample: False
   arch:
```

## configs/llama2/run_llama2_70b_910b_finetune.yaml

```diff
@@ -31,15 +31,15 @@
   eps: 1.e-8 # 1e-8
   learning_rate: 1.e-5
 
 # lr sechdule
 lr_schedule:
   type: CosineWithWarmUpLR
   learning_rate: 1.e-5
-  lr_end: 0.0
+  lr_end: 0
   warmup_ratio: 0.03
   total_steps: -1 # -1 means it will load the total steps of the dataset
 
 # dataset
 train_dataset: &train_dataset
   data_loader:
     type: MindDataset
@@ -155,14 +155,15 @@
     use_past: False
     scaling_factor: 1.0
     extend_method: "None" # support "None", "PI", "NTK"
     use_flash_attention: True  # FA can accelerate training or finetune
     fine_grain_interleave: 2
     qkv_concat: false
     offset: 0
+    use_past_shard: False
     checkpoint_name_or_path: ""
     repetition_penalty: 1
     max_decode_length: 512
     top_k: 3
     top_p: 1
     do_sample: False
   arch:
```

## configs/llama2/run_llama2_7b.yaml

```diff
@@ -149,17 +149,20 @@
     ignore_token_id: -100
     compute_dtype: "float16"
     layernorm_compute_type: "float32"
     softmax_compute_type: "float16"
     rotary_dtype: "float16"
     param_init_type: "float16"
     use_past: False
+    pretrain_seqlen: 4096 # seqlen of the pretrain checkpoint: 2048 for llama and 4096 for llama2
     extend_method: "None" # support "None", "PI", "NTK"
+    compute_in_2d: False
     use_flash_attention: False # FA can accelerate training or finetune
     offset: 0
+    use_past_shard: False
     checkpoint_name_or_path: "llama2_7b"
     repetition_penalty: 1
     max_decode_length: 512
     top_k: 3
     top_p: 1
     do_sample: False
   arch:
```

## configs/llama2/run_llama2_7b_910b_auto_parallel.yaml

```diff
@@ -152,17 +152,20 @@
     ignore_token_id: -100
     compute_dtype: "float16"
     layernorm_compute_type: "float32"
     softmax_compute_type: "float16"
     rotary_dtype: "float16"
     param_init_type: "float16"
     use_past: False
+    pretrain_seqlen: 4096 # seqlen of the pretrain checkpoint: 2048 for llama and 4096 for llama2
     extend_method: "None" # support "None", "PI", "NTK"
+    compute_in_2d: True
     use_flash_attention: True  # FA can accelerate training or finetune
     offset: 0
+    use_past_shard: False
     checkpoint_name_or_path: "llama2_7b"
     repetition_penalty: 1
     max_decode_length: 512
     top_k: 3
     top_p: 1
     do_sample: False
   arch:
```

## configs/llama2/run_llama2_7b_lora_910b.yaml

```diff
@@ -149,17 +149,20 @@
     ignore_token_id: -100
     compute_dtype: "float16"
     layernorm_compute_type: "float32"
     softmax_compute_type: "float16"
     rotary_dtype: "float16"
     param_init_type: "float16"
     use_past: False
+    pretrain_seqlen: 4096 # seqlen of the pretrain checkpoint: 2048 for llama and 4096 for llama2
     extend_method: "None" # support "None", "PI", "NTK"
+    compute_in_2d: True
     use_flash_attention: False # FA can accelerate training or finetune
     offset: 0
+    use_past_shard: False
     checkpoint_name_or_path: "llama2_7b_lora"
     repetition_penalty: 1
     max_decode_length: 512
     top_k: 3
     top_p: 1
     do_sample: False
     pet_config:
```

## mindformers/.commit_id

```diff
@@ -1,8 +1,8 @@
-r1.1.0
-commit 38f9154
-Merge: 62e3820 2f81b86
-Author: i-robot <huawei_ci_bot@163.com>
-Date:   Thu May 9 12:31:18 2024 +0000
+r1.1.rc1
+commit c06e87b
+Merge: 9504c46 2160758
+Author: Lin <heqinglin4@huawei.com>
+Date:   Fri Apr 12 09:42:21 2024 +0000
 
-    !3000 cellcherrypick
-    Merge pull request !3000 from /cherry-pick-1715253112
+    !2690 ms
+    Merge pull request !2690 from Lin/cherry-pick-1712914867
```

## mindformers/__init__.py

```diff
@@ -13,33 +13,28 @@
 # limitations under the License.
 # ============================================================================
 
 """mindformers init"""
 
 __version__ = "1.1"
 
-from mindformers import core, dataset, experimental, \
+from mindformers import core, dataset, \
     models, modules, wrapper, tools
 from mindformers.pipeline import *
 from mindformers.trainer import *
 from mindformers.core import *
 from mindformers.dataset import *
-from mindformers.experimental import *
 from mindformers.models import *
 from mindformers.modules import *
 from mindformers.wrapper import *
 from mindformers.tools import *
 from mindformers import generation
 from mindformers.generation import *
 from mindformers.pet import *
-from mindformers import model_runner
-from mindformers.model_runner import *
 from .mindformer_book import MindFormerBook
 
 __all__ = []
 __all__.extend(dataset.__all__)
-__all__.extend(experimental.__all__)
 __all__.extend(models.__all__)
 __all__.extend(core.__all__)
 __all__.extend(tools.__all__)
 __all__.extend(generation.__all__)
-__all__.extend(model_runner.__all__)
```

## mindformers/auto_class.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2024 Huawei Technologies Co., Ltd
+# Copyright 2022 Huawei Technologies Co., Ltd
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 # http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -20,16 +20,17 @@
 import shutil
 
 from mindformers.tools.utils import try_sync_file
 
 from .mindformer_book import MindFormerBook, print_dict
 from .models.build_processor import build_processor
 from .models.base_config import BaseConfig
-from .models.build_model import build_network
+from .models.build_model import build_model
 from .models.build_config import build_model_config
+from .pet import get_pet_model, is_supported_pet_type
 from .tools import logger
 from .tools.register.config import MindFormerConfig
 
 
 __all__ = ['AutoConfig', 'AutoModel', 'AutoProcessor', 'AutoTokenizer']
 
 
@@ -276,15 +277,22 @@
         else:
             raise ValueError("config should be inherited from BaseConfig,"
                              " or a path to .yaml file for model config.")
 
         config_args.model.model_config.update(**kwargs)
         if not download_checkpoint:
             config_args.model.model_config.checkpoint_name_or_path = None
-        model = build_network(config_args.model)
+        ckpt_cfg = config_args.model.model_config.checkpoint_name_or_path
+        pet_config = config_args.model.model_config.pet_config
+        if pet_config and is_supported_pet_type(pet_config.pet_type):
+            config_args.model.model_config.checkpoint_name_or_path = None
+        model = build_model(config_args.model)
+        if pet_config:
+            model.config.checkpoint_name_or_path = ckpt_cfg
+            model = get_pet_model(model, pet_config)
         logger.info("model built successfully!")
         return model
 
     @classmethod
     def _inverse_parse_config(cls, config):
         """
         Inverse parse config method, which builds yaml file content for model config.
@@ -428,15 +436,22 @@
 
         if not isinstance(pretrained_model_name_or_dir, str):
             raise TypeError(f"pretrained_model_name_or_dir should be a str,"
                             f" but got {type(pretrained_model_name_or_dir)}")
         config_args = cls._get_config_args(pretrained_model_name_or_dir, **kwargs)
         if not download_checkpoint:
             config_args.model.model_config.checkpoint_name_or_path = None
-        model = build_network(config_args.model)
+        ckpt_cfg = config_args.model.model_config.checkpoint_name_or_path
+        pet_config = config_args.model.model_config.pet_config
+        if pet_config and is_supported_pet_type(pet_config.pet_type):
+            config_args.model.model_config.checkpoint_name_or_path = None
+        model = build_model(config_args.model)
+        if pet_config:
+            model.config.checkpoint_name_or_path = ckpt_cfg
+            model = get_pet_model(model, pet_config)
         cls.default_checkpoint_download_path = model.default_checkpoint_download_path
 
         logger.info("model built successfully!")
         return model
 
     @classmethod
     def show_support_list(cls):
```

## mindformers/mindformer_book.py

```diff
@@ -269,20 +269,14 @@
                 _PROJECT_PATH, "research/internlm/run_internlm_7b.yaml")),
             ("internlm_7b_lora", os.path.join(
                 _PROJECT_PATH, "research/internlm/run_internlm_7b_lora.yaml")),
             ("qwen_7b", os.path.join(
                 _PROJECT_PATH, "research/qwen/run_qwen_7b.yaml")),
             ("qwen_7b_lora", os.path.join(
                 _PROJECT_PATH, "research/qwen/run_qwen_7b_lora.yaml")),
-            ("yi_6b", os.path.join(
-                _PROJECT_PATH, "research/yi/predict_yi_6b.yaml")),
-            ("yi_34b", os.path.join(
-                _PROJECT_PATH, "research/yi/predict_yi_34b.yaml")),
-            ("deepseek_33b", os.path.join(
-                _PROJECT_PATH, "research/deepseek/predict_deepseek_33b.yaml")),
             ("common", os.path.join(
                 _PROJECT_PATH, "configs/gpt2/run_gpt2.yaml"))])
          ),
         ("segment_anything", OrderedDict([
             ("sam_vit_b", os.path.join(
                 _PROJECT_PATH, "configs/sam/run_sam_vit-b.yaml")),
             ("sam_vit_l", os.path.join(
@@ -444,16 +438,14 @@
                 _PROJECT_PATH, "research/ziya/run_ziya_13b.yaml")),
             ("skywork_13b", os.path.join(
                 _PROJECT_PATH, "research/skywork/run_skywork_13b.yaml")),
             ("internlm_7b", os.path.join(
                 _PROJECT_PATH, "research/internlm/run_internlm_7b.yaml")),
             ("internlm_7b_lora", os.path.join(
                 _PROJECT_PATH, "research/internlm/run_internlm_7b_lora.yaml")),
-            ("deepseek_33b", os.path.join(
-                _PROJECT_PATH, "research/internlm/predict_deepseek_33b.yaml")),
             ("common", os.path.join(
                 _PROJECT_PATH, "configs/gpt2/run_gpt2.yaml"))
         ])),
         ("image_to_text_retrieval", OrderedDict([
             ("blip2_stage1_evaluator", os.path.join(
                 _PROJECT_PATH, "configs/blip2/run_blip2_stage1_vit_g_retrieval_flickr30k.yaml"))
         ])),
@@ -584,20 +576,14 @@
             'sam_vit_b',
             'sam_vit_l',
             'sam_vit_h'
         ]),
         ('qwen', [
             'qwen_7b',
         ]),
-        ('yi', [
-            'yi_6b_finetune',
-            'yi_6b_pretrain',
-            'yi_6b_text_generation',
-            'yi_34b_text_generation',
-        ])
     ])
 
     _MODEL_SUPPORT_LIST = OrderedDict([
         ('clip', [
             'clip_vit_b_32',
             'clip_vit_b_16',
             'clip_vit_l_14',
@@ -706,18 +692,14 @@
         ('codegeex2', [
             'codegeex2_6b',
         ]),
         ('internlm', [
             'internlm_7b',
             'internlm_7b_lora',
         ]),
-        ('yi', [
-            'yi_6b',
-            'yi_34b',
-        ]),
         ('sam', [
             'sam_vit_b',
             'sam_vit_l',
             'sam_vit_h'
         ])
     ])
 
@@ -840,29 +822,25 @@
         ('llama', [
             'llama_7b',
             'llama_13b',
             'llama_65b',
             'llama2_7b',
             'llama2_13b',
             'llama2_70b',
-            'llama3_8b',
             'llama_7b_lora',
             'baichuan_7b',
             'ziya_13b',
             'skywork_13b',
             'codellama_34b',
         ]),
         ('llama2', [
             'llama2_7b',
             'llama2_13b',
             'llama2_70b',
         ]),
-        ('llama3', [
-            'llama3_8b',
-        ]),
         ('codellama', [
             'codellama_34b',
         ]),
         ('pangualpha', [
             'pangualpha_2_6b',
             'pangualpha_13b'
         ]),
@@ -895,18 +873,14 @@
         ]),
         ('qwen', [
             'qwen_7b',
         ]),
         ('skywork', [
             'skywork_13b',
         ]),
-        ('yi', [
-            'yi_6b',
-            'yi_34b',
-        ])
     ])
 
     _MODEL_CONFIG_TO_NAME = OrderedDict([
     ])
 
     _MODEL_CKPT_URL_LIST = OrderedDict([
         ('clip_vit_b_32',
```

## mindformers/version_control.py

```diff
@@ -14,14 +14,16 @@
 # ============================================================================
 """MindSpore Version Control"""
 import os
 import mindspore as ms
 from mindspore import nn
 import mindspore.ops.operations as P
 import mindspore.ops.functional as F
+from mindspore.context import ParallelMode
+from mindspore.parallel._utils import _get_parallel_mode
 from .tools.utils import is_version_ge
 from .tools.logger import logger
 
 
 def get_ascend_soc_version():
     """Get ascend soc version."""
     if is_version_ge(ms.__version__, "2.2.0"):
@@ -47,15 +49,14 @@
 def is_910b():
     device = get_ascend_soc_version()
     return device in ['910b', 'ascend910b']
 
 
 def get_cell_reuse(func):
     """Cell reuse decorator."""
-
     def decorator(*args, **kwargs):
         stand_alone = ms.get_auto_parallel_context("parallel_mode") == 'stand_alone'
         pipline_parallel = ms.get_auto_parallel_context("pipeline_stages") > 1
         if os.getenv("ENABLE_CELL_REUSE", "0") == "0" or \
                 not is_version_ge(ms.__version__, "2.1.0") \
                 or stand_alone or not pipline_parallel:
             logger.info("The Cell Reuse compilation acceleration feature is not supported "
@@ -100,15 +101,14 @@
                         os.getenv("ENABLE_CELL_REUSE", "UNSET"),
                         os.getenv("MS_DEV_CELL_REUSE", "UNSET"),
                         os.getenv("MS_ENABLE_REF_MODE", "UNSET"),
                         os.getenv("MS_ENABLE_GE", "UNSET"),
                         os.getenv("MS_ENABLE_TRAIN", "UNSET"))
         from mindspore.common import lazy_inline
         lazy_inline(func)(*args, **kwargs)
-
     return decorator
 
 
 def get_dropout(dropout_prob):
     if is_version_ge(ms.__version__, '1.11.0'):
         dropout = nn.Dropout(p=dropout_prob)
     else:
@@ -122,15 +122,14 @@
     else:
         tril = nn.Tril()
     return tril
 
 
 def get_norm():
     """return ops.norm"""
-
     # pylint: disable=C0103
     # pylint: disable=W0622
     def tensor_norm1(A, ord=None, dim=None, keepdim=False, dtype=None):
         return F.norm(A, ord=ord, dim=dim, keepdim=keepdim, dtype=dtype)
 
     # pylint: disable=C0103
     # pylint: disable=W0622
@@ -174,18 +173,19 @@
 def fix_optim_global_step_sig():
     # when the version of mindspore bigger than 2.2.0, it should update global step explicitly.
     return is_version_ge(ms.__version__, "2.2.0")
 
 
 def check_valid_flash_attention(import_fa_valid=True, fa_type=None):
     """check mindspore version is valid for input flash attention"""
-    version_map = {"PromptFlashAttention": "2.2.0",
+    version_map = {"IncreFlashAttention": "2.3.0",
+                   "PromptFlashAttention": "2.2.0",
                    "FlashAttention": "2.2.0"}
     valid_version = version_map.get(fa_type)
-    if not is_910b() and fa_type in ["PromptFlashAttention"]:
+    if not is_910b() and fa_type in ["PromptFlashAttention", "IncreFlashAttention"]:
         logger.warning(f"Current device {get_ascend_soc_version()} do not support {fa_type}, "
                        f"please use 910b device.")
         return False
     if valid_version is None:
         raise ValueError(f"fa_type should be in {list(version_map.keys())}, but get {fa_type}")
     version_valid = is_version_ge(ms.__version__, valid_version)
     if not version_valid:
@@ -193,47 +193,48 @@
                        f"please upgrade to {valid_version} or later version.")
         logger.warning("Now running on self-attention mode.")
         result = False
     elif not import_fa_valid:
         logger.warning(f"Import {fa_type} ERROR, please upgrade your MindSpore to {valid_version} or later version. ")
         logger.warning("Now running on self-attention mode.")
         result = False
+    elif fa_type == "IncreFlashAttention" and _get_parallel_mode() not in ParallelMode.STAND_ALONE:
+        logger.warning("Current IncreFlashAttention does not support parallel mode, "
+                       "incremental inference will run in self attention")
+        result = False
     # both pass should return True
     else:
         result = True
     return result
 
-
 def choose_flash_attention_dtype():
     """
     attention_mask dtype should be float16 on ms 2.2.0, uint8 on 2.2.10
     ms version below 2.2.0 won't be in this func
     """
     fa_dtype = ms.uint8
     cur_ver = ms.__version__
     if is_version_ge(cur_ver, "2.2.0") and not is_version_ge(cur_ver, "2.2.1"):
         fa_dtype = ms.float16
     elif is_version_ge(cur_ver, "2.2.1"):
         fa_dtype = ms.uint8
     return fa_dtype
 
-
 def check_valid_big_kernel():
     """check mindspore version is valid for big kernel SiLU and LlamaRMSNorm Ops"""
     version_valid = is_version_ge(ms.__version__, "2.2.10")
     # below ms 2.2.10 is not support
     if not version_valid:
         logger.warning("Current MindSpore do not support big kernel SiLU and RMSNorm, "
                        "please upgrade to 2.2.10 or later version.")
         result = False
     else:
         result = True
     return result
 
-
 def is_version_python(cur_ver, tar_ver):
     """
         return cur_ver >= tar_ver.
         Check whether the current version is higher than or equal to the base version.
         for cur_ver: 3.7.10, tar_ver: 3.9.0, it return False.
         you can get python cur_ver through:
             cur_py_ver = sys.version.split(' ')[0]
@@ -245,13 +246,28 @@
     for x, y in zip(cur_ver.split(version_split_char), tar_ver.split(version_split_char)):
         if not x.isdigit() or not y.isdigit():
             continue
         if int(x) != int(y):
             return int(x) >= int(y)
     return True
 
+def check_valid_paged_attention():
+    """check mindspore version is valid for paged attention"""
+    version_valid = is_version_ge(ms.__version__, "2.2.11")
+    # below ms 2.2.11 is not support
+    if not version_valid:
+        logger.warning("Current MindSpore do not support PagedAttention, please upgrade to 2.2.11 or later version.")
+        logger.warning("Now running on self-attention mode.")
+        result = False
+    else:
+        result = True
+    return result
+
+def is_paged_attention_v2():
+    """check whether the interface of paged attention is changed"""
+    return is_version_ge(ms.__version__, "2.3.0")
 
-def check_rmsnorm_big_kernel_valid():
+def check_rmsnorm_big_kernel_valid(is_dynamic=False):
     """check whether rmsnorm big kernel is valid"""
-    if check_valid_big_kernel() and not is_910a():
+    if check_valid_big_kernel() and not is_910a() and not is_dynamic:
         return True
     return False
```

## mindformers/core/lr/lr_schedule.py

```diff
@@ -14,15 +14,14 @@
 # This file was refer to project:
 # https://github.com/huawei-noah/Pretrained-Language-Model/blob/master/PanGu-%CE%B1/utils.py
 # https://github.com/huggingface/transformers/blob/main/src/transformers/optimization.py
 # ============================================================================
 """Self-Define LR Schedule."""
 import math
 
-from mindspore._checkparam import args_type_check
 from mindspore.ops import operations as P
 import mindspore.common.dtype as mstype
 from mindspore.nn.learning_rate_schedule import LearningRateSchedule
 from mindspore.common.tensor import Tensor
 from mindformers.tools.register import MindFormerRegister, MindFormerModuleType
 from mindformers.tools.logger import logger
 
@@ -36,44 +35,33 @@
 def _get_warmup_steps(warmup_steps: int, warmup_ratio: float, total_steps: int):
     """check warmup args and get warmup steps."""
     if warmup_ratio is None:
         if not isinstance(warmup_steps, int):
             raise TypeError(f"The type of warmup_steps must be int, but got {type(warmup_steps)}")
         if warmup_steps < 0:
             raise ValueError(f"Warmup_steps must be >= 0, but got {warmup_steps}")
-        if warmup_steps > total_steps:
-            raise ValueError(f"Total_steps {total_steps} must be greater than warmup_steps {warmup_steps}")
         return warmup_steps
 
     if not isinstance(warmup_ratio, float):
         raise TypeError(f"The type of warmup_ratio must be float, but got {type(warmup_ratio)}")
 
     if warmup_ratio > 1.0 or warmup_ratio < 0.0:
         raise ValueError(f"Warmup_ratio's value range must be in [0,1], but got {warmup_ratio}")
 
     if total_steps is None:
-        raise ValueError(f"When warmup_ratio takes effect, total_steps must be set, but got {total_steps}")
+        raise ValueError(f"When warmup_ratio takes effect, total_steps must be set, but got {total_steps} ")
     if not isinstance(total_steps, int):
         raise TypeError(f"The type of total_steps must be int, but got {type(total_steps)}")
 
     warmup_steps = int(total_steps * warmup_ratio)
     logger.info("Current warmup_ratio is %s, total_steps is %s, warmup_steps will be set to %s",
                 warmup_ratio, total_steps, warmup_steps)
     return warmup_steps
 
 
-def _check_decay_method(decay_steps: int, total_steps: int):
-    """check decay method."""
-    if decay_steps is not None:
-        return
-
-    if decay_steps is None and total_steps is None:
-        raise ValueError(f"When decay_steps is None, total_steps must be set, but got {total_steps} ")
-
-
 @MindFormerRegister.register(MindFormerModuleType.LR)
 class ConstantWarmUpLR(LearningRateSchedule):
     """
     Constant Warm Up Learning Rate.
 
     Args:
         learning_rate (`float`):
@@ -87,34 +75,30 @@
         total_steps (`int`, *optional*, defaults to None):
             The number of warm up steps.
 
     Returns:
         Class, ConstantWarmUpLR
     """
 
-    @args_type_check(
-        learning_rate=(int, float), warmup_steps=int, warmup_lr_init=(int, float), warmup_ratio=(int, float),
-        total_steps=int
-    )
-    def __init__(self, learning_rate: float, warmup_steps: int = None, warmup_lr_init: float = 0.,
+    def __init__(self, learning_rate: float, warmup_steps: int = 0, warmup_lr_init: float = 0.,
                  warmup_ratio: float = None, total_steps: int = None, **kwargs):
         super(ConstantWarmUpLR, self).__init__()
         warmup_steps = _get_warmup_steps(warmup_steps, warmup_ratio, total_steps)
         self.learning_rate = learning_rate
         self.warmup_lr_init = warmup_lr_init
         self.warmup_steps = Tensor(warmup_steps, mstype.float32)
         self.one_constant = Tensor(1.0, mstype.float32)
         self.greater = P.Greater()
         self.kwargs = kwargs
 
     def construct(self, global_step):
         """compute current step lr."""
         if self.greater(self.warmup_steps, global_step):
             percent = global_step / self.warmup_steps
-            learning_rate = self.warmup_lr_init + (self.learning_rate - self.warmup_lr_init) * percent
+            learning_rate = self.warmup_lr_init + self.learning_rate * percent
         else:
             percent = self.one_constant
             learning_rate = self.learning_rate * percent
         return learning_rate
 
 
 @MindFormerRegister.register(MindFormerModuleType.LR)
@@ -134,18 +118,14 @@
         warmup_ratio (`float`, *optional*, defaults to None):
             Ratio of total training steps used for warmup.
 
     Returns:
         Class, LinearWithWarmUpLR
     """
 
-    @args_type_check(
-        learning_rate=(int, float), warmup_steps=int, warmup_lr_init=(int, float), warmup_ratio=(int, float),
-        total_steps=int
-    )
     def __init__(self, learning_rate: float, total_steps: int, warmup_steps: int = None,
                  warmup_lr_init: float = 0., warmup_ratio: float = None,
                  **kwargs):
         super(LinearWithWarmUpLR, self).__init__()
         warmup_steps = _get_warmup_steps(warmup_steps, warmup_ratio, total_steps)
         linear_steps = max(1, total_steps - warmup_steps)
         self.kwargs = kwargs
@@ -160,15 +140,15 @@
         self.cast = P.Cast()
 
     def construct(self, global_step):
         """compute current step lr."""
         global_step = self.cast(global_step, mstype.float32)
         if self.greater(self.warmup_steps, global_step):
             percent = global_step / self.warmup_steps
-            learning_rate = self.warmup_lr_init + (self.learning_rate - self.warmup_lr_init) * percent
+            learning_rate = self.warmup_lr_init + self.learning_rate * percent
         else:
             percent = self.max(self.zero_constant, (self.total_steps - global_step) / self.linear_steps)
             learning_rate = self.learning_rate * percent
         return learning_rate
 
 
 @MindFormerRegister.register(MindFormerModuleType.LR)
@@ -188,60 +168,47 @@
             following a half-cosine).
         lr_end (`float`, *optional*, defaults to 0.):
             Final value of learning rate.
         warmup_lr_init (`float`, *optional*, defaults to 0.):
             Initial learning rate in warm up steps.
         warmup_ratio (`float`, *optional*, defaults to None):
             Ratio of total training steps used for warmup.
-        decay_steps (`int`, *optional*, defaults to None):
-            The number of decay steps.
 
     Returns:
         Class, CosineWithWarmUpLR
     """
 
-    @args_type_check(
-        learning_rate=(int, float), warmup_steps=int, warmup_lr_init=(int, float), warmup_ratio=(int, float),
-        total_steps=int, num_cycles=(int, float), lr_end=(int, float)
-    )
     def __init__(self, learning_rate: float, warmup_steps: int = 0, total_steps: int = None,
                  num_cycles: float = 0.5, lr_end: float = 0., warmup_lr_init: float = 0.,
-                 warmup_ratio: float = None, decay_steps: int = None, **kwargs):
+                 warmup_ratio: float = None, **kwargs):
         super(CosineWithWarmUpLR, self).__init__()
-        _check_decay_method(decay_steps, total_steps)
         warmup_steps = _get_warmup_steps(warmup_steps, warmup_ratio, total_steps)
         cosine_steps = max(1, total_steps - warmup_steps)
-        decay_steps = max(1, decay_steps) \
-            if decay_steps is not None else max(1, total_steps)
         self.kwargs = kwargs
         self.learning_rate = learning_rate
         self.lr_end = Tensor(lr_end, mstype.float32)
         self.warmup_lr_init = warmup_lr_init
         self.warmup_steps = Tensor(warmup_steps, mstype.float32)
         self.cosine_steps = Tensor(cosine_steps, mstype.float32)
-        self.decay_steps = Tensor(decay_steps, mstype.float32)
         self.num_cycles = num_cycles
         self.greater = P.Greater()
         self.greater_equal = P.GreaterEqual()
         self.max = P.Maximum()
         self.math_pi = math.pi
         self.cos = P.Cos()
         self.zero_constant = Tensor(0.0, mstype.float32)
         self.cast = P.Cast()
 
     def construct(self, global_step):
         """compute current step lr."""
         global_step = self.cast(global_step, mstype.float32)
-        if self.greater_equal(global_step, self.decay_steps):
-            # Include global_step in computation to circumvent mindspore control flow issues
-            return global_step - global_step + self.lr_end
 
         if self.greater(self.warmup_steps, global_step):
             percent = global_step / self.warmup_steps
-            learning_rate = self.warmup_lr_init + (self.learning_rate - self.warmup_lr_init) * percent
+            learning_rate = self.warmup_lr_init + self.learning_rate * percent
         else:
             progress = (global_step - self.warmup_steps) / self.cosine_steps
             percent = self.max(
                 self.zero_constant, 0.5 * (1.0 + self.cos(self.math_pi * self.num_cycles * 2.0 * progress)))
             learning_rate = self.lr_end + (self.learning_rate - self.lr_end) * percent
         return learning_rate
 
@@ -263,61 +230,48 @@
             following a half-cosine).
         lr_end (`float`, *optional*, defaults to 0.):
             Final value of learning rate.
         warmup_lr_init (`float`, *optional*, defaults to 0.):
             Initial learning rate in warm up steps.
         warmup_ratio (`float`, *optional*, defaults to None):
             Ratio of total training steps used for warmup.
-        decay_steps (`int`, *optional*, defaults to None):
-            The number of decay steps.
 
     Returns:
         Class, CosineWithRestartsAndWarmUpLR
     """
 
-    @args_type_check(
-        learning_rate=(int, float), warmup_steps=int, warmup_lr_init=(int, float), warmup_ratio=(int, float),
-        total_steps=int, num_cycles=(int, float), lr_end=(int, float)
-    )
     def __init__(self, learning_rate: float, warmup_steps: int = None, total_steps: int = None,
                  num_cycles: float = 1., lr_end: float = 0., warmup_lr_init: float = 0.,
-                 warmup_ratio: float = None, decay_steps: int = None, **kwargs):
+                 warmup_ratio: float = None, **kwargs):
         super(CosineWithRestartsAndWarmUpLR, self).__init__()
-        _check_decay_method(decay_steps, total_steps)
         warmup_steps = _get_warmup_steps(warmup_steps, warmup_ratio, total_steps)
         cosine_steps = max(1, total_steps - warmup_steps)
-        decay_steps = max(1, decay_steps) \
-            if decay_steps is not None else max(1, total_steps)
         self.kwargs = kwargs
         self.learning_rate = learning_rate
         self.lr_end = Tensor(lr_end, mstype.float32)
         self.warmup_lr_init = warmup_lr_init
         self.warmup_steps = Tensor(warmup_steps, mstype.float32)
         self.cosine_steps = Tensor(cosine_steps, mstype.float32)
-        self.decay_steps = Tensor(decay_steps, mstype.float32)
         self.num_cycles = num_cycles
         self.greater = P.Greater()
         self.greater_equal = P.GreaterEqual()
         self.max = P.Maximum()
         self.math_pi = math.pi
         self.cos = P.Cos()
         self.zero_constant = Tensor(0.0, mstype.float32)
         self.one_constant = Tensor(1.0, mstype.float32)
         self.cast = P.Cast()
 
     def construct(self, global_step):
         """compute current step lr."""
         global_step = self.cast(global_step, mstype.float32)
-        if self.greater_equal(global_step, self.decay_steps):
-            # Include global_step in computation to circumvent mindspore control flow issues
-            return global_step - global_step + self.lr_end
 
         if self.greater(self.warmup_steps, global_step):
             percent = global_step / self.warmup_steps
-            learning_rate = self.warmup_lr_init + (self.learning_rate - self.warmup_lr_init) * percent
+            learning_rate = self.warmup_lr_init + self.learning_rate * percent
         else:
             progress = (global_step - self.warmup_steps) / self.cosine_steps
             if self.greater(self.one_constant, progress):
                 percent = self.max(
                     self.zero_constant,
                     0.5 * (1.0 + self.cos(self.math_pi * ((self.num_cycles * progress) % self.one_constant))))
                 learning_rate = self.lr_end + (self.learning_rate - self.lr_end) * percent
@@ -343,59 +297,46 @@
         lr_end (`float`, *optional*, defaults to 0.):
             Final value of learning rate.
         warmup_lr_init (`float`, *optional*, defaults to 0.):
             Initial learning rate in warm up steps.
         warmup_ratio (`float`, *optional*, defaults to None):
             Ratio of total training steps used for warmup.
         decay_steps (`int`, *optional*, defaults to None):
-            The number of decay steps, which must be smaller than total_steps - warmup_steps.
-            If the value is None, decay steps will be total_steps - warmup_steps.
+            The number of decay steps. If the value is None, decay steps will be total_steps - warmup_steps.
 
     Returns:
         Class, PolynomialWithWarmUpLR
     """
 
-    @args_type_check(
-        learning_rate=(int, float), warmup_steps=int, warmup_lr_init=(int, float), warmup_ratio=(int, float),
-        total_steps=int, lr_end=(int, float), power=(int, float)
-    )
     def __init__(self, learning_rate: float, total_steps: int, warmup_steps: int = None,
                  lr_end: float = 1e-7, power: float = 1.0, warmup_lr_init: float = 0.,
-                 warmup_ratio: float = None, decay_steps: int = None, **kwargs):
+                 warmup_ratio: float = None, **kwargs):
         super(PolynomialWithWarmUpLR, self).__init__()
-        _check_decay_method(decay_steps, total_steps)
         warmup_steps = _get_warmup_steps(warmup_steps, warmup_ratio, total_steps)
-        decay_steps = max(1, decay_steps) \
-            if decay_steps is not None else max(1, total_steps - warmup_steps)
-        if decay_steps > total_steps - warmup_steps:
-            raise ValueError(f"decay_steps ({decay_steps}) must be be smaller than \
-                             steps interval ({total_steps - warmup_steps})")
+        decay_steps = max(1, total_steps - warmup_steps)
         if not learning_rate > lr_end:
             raise ValueError(f"lr_end ({lr_end}) must be be smaller than initial lr ({learning_rate})")
         self.kwargs = kwargs
         self.learning_rate = learning_rate
         self.warmup_lr_init = warmup_lr_init
         self.lr_end = Tensor(lr_end, mstype.float32)
         self.power = power
         self.warmup_steps = Tensor(warmup_steps, mstype.float32)
         self.decay_steps = Tensor(decay_steps, mstype.float32)
-        self.total_steps = Tensor(total_steps, mstype.float32)
         self.greater = P.Greater()
+        self.greater_equal = P.GreaterEqual()
         self.cast = P.Cast()
 
     def construct(self, global_step):
         """compute current step lr."""
         global_step = self.cast(global_step, mstype.float32)
 
         if self.greater(self.warmup_steps, global_step):
             percent = global_step / self.warmup_steps
-            learning_rate = self.warmup_lr_init + (self.learning_rate - self.warmup_lr_init) * percent
-        elif self.greater(global_step, self.total_steps):
-            # Include global_step in computation to circumvent mindspore control flow issues
-            return global_step - global_step + self.lr_end
+            learning_rate = self.warmup_lr_init + self.learning_rate * percent
         else:
             lr_range = self.learning_rate - self.lr_end
             pct_remaining = 1 - (global_step - self.warmup_steps) / self.decay_steps
             decay = lr_range * pct_remaining ** self.power + self.lr_end
             percent = decay / self.learning_rate
             learning_rate = self.learning_rate * percent
         return learning_rate
@@ -463,15 +404,14 @@
         eta_min (`float`, optional):
             Minimum learning rate. Default: 0.
 
     .. _SGDR\: Stochastic Gradient Descent with Warm Restarts:
         https://arxiv.org/abs/1608.03983
     """
 
-    @args_type_check(base_lr=(int, float), t_max=int, eta_min=(int, float))
     def __init__(self, base_lr: float, t_max: int, eta_min: float = 0., **kwargs):
         super(CosineAnnealingLR, self).__init__()
         if t_max < 1 or not isinstance(t_max, int):
             raise ValueError("Expected positive integer T_max, but got {}".format(t_max))
         self.kwargs = kwargs
         self.base_lr = base_lr
         self.t_max = t_max
@@ -518,15 +458,14 @@
         eta_min (`float`, optional):
             Minimum learning rate. Default: 0.
 
     .. _SGDR\: Stochastic Gradient Descent with Warm Restarts:
         https://arxiv.org/abs/1608.03983
     """
 
-    @args_type_check(base_lr=(int, float), t_0=int, t_mult=int, eta_min=(int, float))
     def __init__(self, base_lr: float, t_0: int, t_mult: int = 1, eta_min: float = 0., **kwargs):
         super(CosineAnnealingWarmRestarts, self).__init__()
         if t_0 < 1 or not isinstance(t_0, int):
             raise ValueError("Expected positive integer t_0, but got {}".format(t_0))
         if t_mult < 1 or not isinstance(t_mult, int):
             raise ValueError("Expected positive integer t_mult, but got {}".format(t_mult))
         self.kwargs = kwargs
```

## mindformers/core/optim/came.py

```diff
@@ -189,51 +189,52 @@
 
     Args:
         params (Union[list[Parameter], list[dict]]): When the `params` is a list of `Parameter` which will be updated,
             the element in `params` must be class `Parameter`.
         learning_rate (Union[float, Tensor]): A value or a graph for the learning rate.
             When the learning_rate is a Tensor in a 1D dimension.
             If the type of `learning_rate` is int, it will be converted to float. Default: None.
-        eps (Union[list, tuple]): The regularization constans for square gradient, parameter scale and
-            instability_matrix respectively. default: (1e-30, 1e-3, 1e-16)
-        clip_threshold (float): The threshold of root mean square of final gradient update. default: 1.0
-        decay_rate (float): The coefficient used to compute running averages of square gradient.
-            Should be in range [0.0, 1.0]. default: 0.8.
-        beta1 (float): The coefficient to computing running averages of gradient. Should be in range [0.0, 1.0].
+        eps (tuple): The regularization constans for square gradient, parameter scale and instability_matrix
+            respectively. default: (1e-30, 1e-3, 1e-16)
+        clip_threshold (Union[float, Tensor]): The threshold of root mean square of final gradient update. default: 1.0
+        decay_rate (Union[float, Tensor]): The coefficient used to compute running averages of square gradient.
+            default: 0.8
+        beta1 (float): The coefficient to computing running averages of gradient. Should be in range (0.0, 1.0).
                Default: 0.9.
-        beta3 (float): The coefficient to computing running averages of gradient. Should be in range [0.0, 1.0].
+        beta3 (float): The coefficient to computing running averages of gradient. Should be in range (0.0, 1.0).
                Default: 0.99.
-        weight_decay (float): Weight decay (L2 penalty). It must be equal to or greater than 0.
-            Should be in range [0.0, 1.0]. default: 0.0.
+        weight_decay (float): Weight decay (L2 penalty). It must be equal to or greater than 0. Default: 0.0.
         scale_parameter (bool): If True, learning rate is scaled by root mean square of parameter. default: True
         relative_step (bool): If True, time-dependent learning rate is computed instead of external learning rate.
             default: True
         warmup_init (bool): The time-dependent learning rate computation depends on whether warm-up
             initialization is being used. default: False
         compression (bool): If True, the data type of the running averages exponent will be compression to float16.
             default: False
-        loss_scale (int): An integer point value for the loss scale. Should be greater than 0. In general, use the
+        loss_scale (float): A floating point value for the loss scale. Should be greater than 0. In general, use the
             default value. Only when `FixedLossScaleManager` is used for training and the `drop_overflow_update` in
             `FixedLossScaleManager` is set to False, then this value needs to be the same as the `loss_scale` in
             `FixedLossScaleManager`. Refer to class :class:`mindspore.amp.FixedLossScaleManager` for more details.
-            Default: 1.
+            Default: 1.0.
 
     Inputs:
         - **gradients** (tuple[Tensor]) - The gradients of `params`, the shape is the same as `params`.
 
     Outputs:
         Tensor[bool], the value is True.
 
     Raises:
         TypeError: If `learning_rate` is not one of int, float, Tensor, Iterable, LearningRateSchedule.
         TypeError: If element of `parameters` is neither Parameter nor dict.
-        TypeError: If `decay_rate`, `weight_decay`, `beta1`, `beta3`, `eps` or `loss_scale` is not a float.
+        TypeError: If `beta1`, `beta3`, `eps` or `loss_scale` is not a float.
+        TypeError: If `weight_decay` is neither float nor int.
         TypeError: If `use_locking` or `use_nesterov` is not a bool.
         ValueError: If `loss_scale` or `eps` is less than or equal to 0.
-        ValueError: If `decay_rate`, `weight_decay`, `beta1` or `beta3` is not in range [0.0, 1.0].
+        ValueError: If `beta1`, `beta3` is not in range (0.0, 1.0).
+        ValueError: If `weight_decay` is less than 0.
 
     Supported Platforms:
         ``Ascend``
     """
     _support_parallel_optimizer = True
 
     @opt_init_args_register
@@ -246,15 +247,15 @@
                  beta1=0.9,
                  beta3=0.99,
                  weight_decay=0.0,
                  scale_parameter=False,
                  relative_step=False,
                  warmup_init=False,
                  compression=False,
-                 loss_scale=1):
+                 loss_scale=1.0):
 
         if learning_rate is not None and relative_step:
             raise ValueError("Cannot combine manual lr and relative_step options", learning_rate)
         if warmup_init and not relative_step:
             raise ValueError("warmup_init requires relative_step=True")
         if learning_rate is None and not relative_step:
             raise ValueError("Cannot learning_rate is None and relative_step=False")
@@ -263,36 +264,33 @@
         if beta1 is None:
             beta1 = 0.0
 
         if not isinstance(learning_rate, (float, int)) and learning_rate is not None:
             if relative_step or scale_parameter:
                 logging.warning("When learning_rate is learning scheduler, it not support update learning rate!")
 
-        validator.check_value_type("loss_scale", loss_scale, [int], self.cls_name)
         super(Came, self).__init__(learning_rate, params, weight_decay, loss_scale)
         validator.check_value_type("eps", eps, [list, tuple], self.cls_name)
         if len(eps) != 3:
             raise ValueError("eps must have 3 value: (eps1, eps2, eps3).")
         for i, ele in enumerate(eps):
             validator.check_value_type("eps{}".format(i), ele, [float], self.cls_name)
             validator.check_non_negative_float(ele, "eps{}".format(i), self.cls_name)
         validator.check_value_type("clip_threshold", clip_threshold, [float], self.cls_name)
         validator.check_non_negative_float(clip_threshold, "clip_threshold", self.cls_name)
         validator.check_value_type("decay_rate", decay_rate, [float], self.cls_name)
-        validator.check_float_range(decay_rate, 0, 1, Rel.INC_BOTH, "decay_rate", self.cls_name)
-        validator.check_value_type("weight_decay", weight_decay, [float], self.cls_name)
-        validator.check_float_range(weight_decay, 0, 1, Rel.INC_BOTH, "weight_decay", self.cls_name)
+        validator.check_float_range(decay_rate, 0, 1, Rel.INC_NEITHER, "decay_rate", self.cls_name)
+        validator.check_float_range(weight_decay, 0, 1, Rel.INC_LEFT, "weight_decay", self.cls_name)
         validator.check_value_type("scale_parameter", scale_parameter, [bool], self.cls_name)
         validator.check_value_type("relative_step", relative_step, [bool], self.cls_name)
-        validator.check_value_type("warmup_init", warmup_init, [bool], self.cls_name)
         validator.check_value_type("compression", compression, [bool], self.cls_name)
-        validator.check_value_type("beta1", beta1, [float], self.cls_name)
-        validator.check_float_range(beta1, 0, 1, Rel.INC_BOTH, "beta1", self.cls_name)
-        validator.check_value_type("beta3", beta3, [float], self.cls_name)
-        validator.check_float_range(beta3, 0, 1, Rel.INC_BOTH, "beta3", self.cls_name)
+        validator.check_value_type("beta1", beta1, [int, float], self.cls_name)
+        validator.check_non_negative_float(float(beta1), "beta1", self.cls_name)
+        validator.check_value_type("beta3", beta3, [int, float], self.cls_name)
+        validator.check_non_negative_float(float(beta3), "beta3", self.cls_name)
         self.eps = trans_to_tensor(eps)
         self.clip_threshold = trans_to_tensor(clip_threshold)
         self.decay_rate = trans_to_tensor(-decay_rate)
         self.beta1 = trans_to_tensor(beta1)
         self.beta3 = trans_to_tensor(beta3)
         self.weight_decay = trans_to_tensor(weight_decay)
         self.weight_decay_flag = bool(weight_decay)
```

## mindformers/dataset/causal_language_model_dataset.py

```diff
@@ -230,15 +230,15 @@
                                                       'shard_id': dataset_config.rank_id})
         return dataset
 
     @classmethod
     def _process_mindrecord_data(cls, dataset_config):
         """Process the mindrecord data"""
         dataset_files = []
-        mind_compile = re.compile(r"mindrecord\d*$")
+        mind_compile = re.compile("mindrecord0*$")
         if dataset_config.data_loader.dataset_dir:
             data_dir = dataset_config.data_loader.pop("dataset_dir")
             if os.path.isdir(data_dir):
                 for r, _, f in os.walk(data_dir):
                     for file in f:
                         if re.findall(mind_compile, file) or file.endswith(".tfrecord"):
                             dataset_files.append(os.path.join(r, file))
```

## mindformers/dataset/dataloader/training_dataloader.py

```diff
@@ -50,15 +50,14 @@
                 text_col: str = "",
                 file_format: str = None,
                 read_function: Callable = None,
                 shuffle: bool = False,
                 samples_num: int = 10000,
                 skip_num: int = 0,
                 file_limit: int = 1,
-                process_num: int = 64,
                 **kwargs):
         r"""
         Training DataLoader API.
 
         Args:
             dataset_dir (str): The directory path to parquet text with hdfs.
             column_names(list): Column names contained in the created dataset.
@@ -102,16 +101,15 @@
         if max_length <= 0:
             raise TypeError(f"max_length should be an integer greater than 0.")
 
         logger.info("dataset_dir: %s, samples_num: %s", dataset_dir, samples_num)
         training_dataset = TrainingDataset(dataset_dir, column_names=column_names, tokenizer=tokenizer,
                                            dataset_name=dataset_name, is_align=is_align, max_length=max_length,
                                            text_col=text_col, file_format=file_format, read_function=read_function,
-                                           shuffle=shuffle, samples_num=samples_num, file_limit=file_limit,
-                                           process_num=process_num)
+                                           shuffle=shuffle, samples_num=samples_num, file_limit=file_limit)
 
         kwargs["num_shards"] = None
         kwargs["shard_id"] = None
         gen_dataset = GeneratorDataset(training_dataset, column_names=column_names, shuffle=shuffle, **kwargs)
         logger.info("NOTE: The sample of Dataset will skip %s", skip_num)
         gen_dataset = gen_dataset.skip(skip_num)
         return gen_dataset
@@ -169,27 +167,25 @@
                  is_align: bool = True,
                  max_length: int = 1025,
                  text_col: str = "",
                  file_format: str = None,
                  read_function: Callable = None,
                  shuffle: bool = True,
                  samples_num: int = 10000,
-                 file_limit: int = 1,
-                 process_num: int = 64):
+                 file_limit: int = 1):
         self.dataset_dir = dataset_dir
         self.dataset_name = dataset_name.lower() if dataset_name else None
         self.format = file_format
         self.column_names = column_names
         self.tokenizer = self._check_tokenizer(tokenizer)
         self.text_col = text_col
         self.read_function = read_function
         self.shuffle = shuffle
         self.sample_number = samples_num
         self.file_limit = file_limit
-        self.process_num = process_num if process_num != -1 else os.cpu_count()
         self.current_index = self.file_limit
         self.start_index = 0
         self.files = []
         self.current_files = []
         self.current_samples_number = 0
         self.current_samples = []
         self.iter_index = 0
@@ -338,23 +334,21 @@
         def pad_func(input_list):
             """Align the length of inpud_ids."""
             token_list = []
             token_item = []
             for token in input_list:
                 token_item.extend(token)
                 if len(token_item) >= self.max_length:
-                    stop_idx = 0
                     for idx in range(0, len(token_item) - self.max_length, self.max_length):
                         token_list.append(np.array(token_item[idx:idx + self.max_length]))
-                        stop_idx = idx
-                    token_item = token_item[stop_idx + self.max_length:]
+                    token_item = token_item[idx + self.max_length:]
             return token_list
 
         logger.info("Start Tokenizer sample")
-        pool = Pool(processes=self.process_num)
+        pool = Pool(processes=os.cpu_count())
         encoded_sentences = pool.map(self._tokenizer_func, iterable)
         pool.close()
         pool.join()
         del pool
         logger.info("Tokenizer sample completed")
         if not self.is_align:
             return encoded_sentences
```

## mindformers/generation/generation_config.py

```diff
@@ -26,20 +26,20 @@
     """Class that holds a configuration for a generation task.
     Args:
         > Parameters that control the length of the output
 
         max_length (`int`, *optional*, defaults to 20):
             The maximum length the generated tokens can have. Corresponds to the length of the input prompt +
             `max_new_tokens`. Its effect is overridden by `max_new_tokens`, if also set.
-        max_new_tokens (`int`, *optional*, defaults to None):
+        max_new_tokens (`int`, *optional*):
             The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.
         min_length (`int`, *optional*, defaults to 0):
             The minimum length of the sequence to be generated. Corresponds to the length of the input prompt +
             `min_new_tokens`. Its effect is overridden by `min_new_tokens`, if also set.
-        min_new_tokens (`int`, *optional*, defaults to None):
+        min_new_tokens (`int`, *optional*):
             The minimum numbers of tokens to generate, ignoring the number of tokens in the prompt.
 
         > Parameters that control the generation strategy used
 
         do_sample (`bool`, *optional*, defaults to `False`):
             Whether to use sampling ; use greedy decoding otherwise.
         use_past (`bool`, *optional*, defaults to `False`):
```

## mindformers/generation/streamers.py

```diff
@@ -117,16 +117,15 @@
                 # switch from batch mode to single mode
                 if self.batch_stream:
                     self.batch_stream = False
                     self.token_cache = []
                     self.print_len = 0
                 self.token_cache.extend(value)
         else:
-            raise ValueError(f"TextStreamer only supports int, or 1 ~ 2 dim numpy.ndarray/list as inputs, "
-                             f"but got {type(value)} with value {value}.")
+            raise ValueError("TextStreamer only supports int, or 1 ~ 2 dim numpy.ndarray/list as inputs.")
 
         # Add the new token to the cache and decodes the entire thing.
         if self.batch_stream:
             text = self.tokenizer.batch_decode(value, self.skip_special_tokens, **self.decode_kwargs)
         else:
             text = self.tokenizer.decode(self.token_cache, self.skip_special_tokens, **self.decode_kwargs)
```

## mindformers/generation/text_generator.py

```diff
@@ -17,30 +17,28 @@
 For text generation
 """
 import copy
 import time
 from typing import Optional, List, Union
 
 import numpy as np
-import mindspore as ms
 from mindspore.ops import functional as F
 from mindspore.ops import operations as P
 import mindspore.common.dtype as mstype
 from mindspore.common.tensor import Tensor
 
 from mindformers.generation.beam_search import BeamSearchScorer
 from mindformers.generation.generation_config import GenerationConfig
 from mindformers.generation.logits_process import (LogitNormalization, LogitsProcessorList,
                                                    RepetitionPenaltyLogitsProcessor,
                                                    TemperatureLogitsWarper, TopKLogitsWarper,
                                                    TopPLogitsWarper, MinLengthLogitsProcessor,
                                                    MinNewTokensLengthLogitsProcessor)
 from mindformers.generation.streamers import BaseStreamer
 from mindformers.generation.utils import softmax_with_threads, topk
-from mindformers.modules.block_tables import BlockTables
 from mindformers.tools import logger
 
 __all__ = ["GenerationMixin"]
 
 
 def get_valid_length_each_example(input_ids, pad_token_id):
     """get valid length and max length in a batch"""
@@ -70,32 +68,15 @@
     BEAM_SEARCH = "beam_search"
 
 
 class GenerationMixin:
     """Generator For the nlp models"""
 
     def __init__(self):
-        self.block_mgr = None
-        self.use_kbk_infer = False
-
-    def _set_block_mgr(self, batch_size):
-        """ Set model block table mgr function. """
-
-        if not self.block_mgr:
-            self.block_mgr = BlockTables(self.config.num_blocks, self.config.block_size, self.config.seq_length)
-
-        if self.block_mgr:
-            self.block_mgr.init_cache_engine(batch_size)
-
-    def _set_kbk_infer(self):
-        jit_level = self.jit_config_dict.get("jit_level")
-        infer_boost = self.jit_config_dict.get("infer_boost")
-        self.use_kbk_infer = (jit_level == "O0" and infer_boost == "on")
-        logger.info(
-            "Set kbk infer :{}".format(self.use_kbk_infer))
+        pass
 
     # pylint: disable=W0613
     def prepare_inputs_for_generation(self, input_ids, **kwargs):
         """
         prepare inputs for generation.
         A model class needs to define a `prepare_inputs_for_generation` method
         in order to use `.generate()`
@@ -104,41 +85,27 @@
             RuntimeError: Not implemented in model but call `.generate()`
         """
         raise RuntimeError(
             "A model class needs to define a `prepare_inputs_for_generation`"
             " method in order to use `.generate()`."
         )
 
-    def add_flags_custom(self, is_first_iteration):
-        """
-        Add customized attributes for specific cells in the model. If the model does not implement this method,
-        this will add customized attributes for all cells in the model recursively.
-
-        Args:
-            is_first_iteration (bool): Network configuration information.
-            Indicate whether current iteration is the first iteration in prediction.
-        """
-        self.add_flags_recursive(is_first_iteration=is_first_iteration)
-
     # pylint: disable=W0613
     def update_model_kwargs_before_generate(self, input_ids, model_kwargs: dict):
         """
         update model kwargs before generate.
         If your model needs to update model kwargs before generate, implement
         this method in your model, else do nothing.
         """
         return
 
     def slice_incremental_inputs(self, model_inputs: dict, current_index):
         """used for non-first iterations, slice the inputs to length 1."""
         input_ids = model_inputs.pop("input_ids")
         if isinstance(input_ids, Tensor):
-            if input_ids.shape[-1] == 1:
-                model_inputs["input_ids"] = input_ids
-                return
             input_ids = input_ids.asnumpy()
         inputs_tmp = []
         for i, index_value in enumerate(current_index):
             current_index_tmp = (
                 int(index_value) - i * input_ids.shape[1]
             )  # multibatch
             # use numpy to slice array to avoid complie ascend slice op
@@ -152,18 +119,18 @@
         if not keep_all and current_index is not None:
             index = current_index.view(-1,)
             logits = P.Gather()(logits, index, 0)
         outputs = P.LogSoftmax(-1)(logits)
         outputs = F.tensor_pow(np.e, outputs)
         return outputs
 
-    def get_logits_processor(self,
-                             generation_config: GenerationConfig,
-                             input_ids_seq_length: int,
-                             logits_processor: Optional[LogitsProcessorList]):
+    def _get_logits_processor(self,
+                              generation_config: GenerationConfig,
+                              input_ids_seq_length: int,
+                              logits_processor: Optional[LogitsProcessorList]):
         """
         This class returns a [`LogitsProcessorList`] list object that contains all relevant [`LogitsProcessor`]
         instances used to modify the scores of the language model head.
         """
         # instantiate processors list
         processors = LogitsProcessorList()
 
@@ -217,15 +184,15 @@
                         f" by the model's config default values. If you just want to change the default values"
                         f" of {object_type} consider passing them as arguments to `.generate()`"
                         f" instead of using a custom {object_type}."
                     )
         default_list.extend(custom_list)
         return default_list
 
-    def get_logits_warper(self, generation_config: GenerationConfig):
+    def _get_logits_warper(self, generation_config: GenerationConfig):
         """
         This class returns a [`LogitsProcessorList`] list object that contains all relevant [`LogitsWarper`] instances
         used for multinomial sampling.
         """
 
         # instantiate warpers list
         warpers = LogitsProcessorList()
@@ -285,52 +252,511 @@
             origin_inputs,
             ((0, 0), (0, pad_length)),
             "constant",
             constant_values=(0, pad_token_id),
         )
         return input_ids
 
-    def _incremental_infer(self, model_inputs: dict, prefill, current_index, valid_length_each_example, block_tables,
-                           slot_mapping):
+    def _incremental_infer(self, model_inputs: dict, current_index, valid_length_each_example):
         """model forward for incremental infer."""
         # Claim the first graph
-        if prefill:
-            self.phase = "prefill"
-            self.add_flags_custom(is_first_iteration=True)
+        if self.is_first_iteration:
+            self.add_flags_recursive(is_first_iteration=True)
             model_inputs["input_position"] = Tensor(current_index, mstype.int32)
             model_inputs["init_reset"] = Tensor([False], mstype.bool_)  # init_reset (1,) bool False
             model_inputs["batch_valid_length"] = Tensor([valid_length_each_example], mstype.int32)
-            if block_tables is not None:
-                model_inputs["block_tables"] = Tensor(block_tables, mstype.int32)
-                model_inputs["slot_mapping"] = Tensor(slot_mapping, mstype.int32)
             # pylint: disable=E1102
             res = self(
                 **model_inputs,
             )
-            ms.hal.synchronize()
-            self.phase = "increment"
             # first iter done, go to other iters
-            self.add_flags_custom(is_first_iteration=False)
+            self.is_first_iteration = False
+            self.add_flags_recursive(is_first_iteration=False)
         else:
             # slice model inputs for incremental infer
             self.slice_incremental_inputs(model_inputs, current_index)
             model_inputs["input_position"] = Tensor(current_index, mstype.int32)
             model_inputs["init_reset"] = Tensor([True], mstype.bool_)  # init_reset (1,) bool True
             model_inputs["batch_valid_length"] = Tensor([valid_length_each_example], mstype.int32)
-            if block_tables is not None:
-                model_inputs["block_tables"] = Tensor(block_tables, mstype.int32)
-                model_inputs["slot_mapping"] = Tensor(slot_mapping, mstype.int32)
             # pylint: disable=E1102
             res = self(
                 **model_inputs,
             )
-            ms.hal.synchronize()
 
         return res
 
+    def _greedy_search(self,
+                       origin_inputs,
+                       generation_config: GenerationConfig,
+                       logits_processor: Optional[LogitsProcessorList] = None,
+                       streamer: BaseStreamer = None,
+                       **model_kwargs):
+        r"""
+        Generates sequences of token ids for models with a language modeling head using **greedy decoding** and can be
+        used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.
+
+        In most cases, you do not need to call [`~generation.GenerationMixin.greedy_search`] directly. Use generate()
+        instead.
+
+        Parameters:
+            origin_inputs (`List(str), List(List(str))`):
+                The sequence used as a prompt for the generation.
+            generation_config (`GenerationConfig`, *optional*):
+                The generation configuration to be used as base parametrization for the generation
+                call. `**kwargs` passed to generate matching the attributes of `generation_config`
+                will override them. If `generation_config` is not provided, the default config
+                from the model configuration will be used. Please note that unspecified parameters
+                will inherit [`GenerationConfig`]'s default values, whose documentation should be
+                checked to parameterize generation.
+            logits_processor (`LogitsProcessorList`, *optional*):
+                An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]
+                used to modify the prediction scores of the language modeling head applied at each generation step.
+            streamer (`TextStreamer, *optional*`):
+                The streamer that generator uses.
+            model_kwargs:
+                Additional model specific kwargs will be forwarded to the `forward` function of the model. If model is
+                an encoder-decoder model the kwargs should include `encoder_outputs`.
+
+        Return:
+            A list of the generated token ids
+        """
+        total_time = time.time()
+        prepare_time = time.time()
+
+        logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()
+
+        if generation_config.pad_token_id is None:
+            generation_config.pad_token_id = 0
+
+        if streamer is not None:
+            streamer.put(origin_inputs)
+
+        batch_size = origin_inputs.shape[0]
+        is_encoder_decoder = self.config.is_encoder_decoder
+        logger.debug("The input shape is: %s", origin_inputs.shape)
+
+        valid_length_each_example, input_ids_length = \
+            get_valid_length_each_example(origin_inputs, generation_config.pad_token_id)
+
+        if generation_config.max_new_tokens is not None:
+            generation_config.max_length = generation_config.max_new_tokens + input_ids_length
+
+        if generation_config.max_length > self.config.seq_length:
+            logger.warning("max_length %s can not exceeds model seq_length %s, set max_length = seq_length.",
+                           generation_config.max_length, self.config.seq_length)
+            generation_config.max_length = self.config.seq_length
+
+        logger.debug("max length is: %s", generation_config.max_length)
+        if not is_encoder_decoder and input_ids_length >= generation_config.max_length:
+            raise ValueError(
+                f"the input_ids length {input_ids_length} exceeds the max length config {generation_config.max_length}."
+                f"check your inputs and set max_length larger than your inputs length."
+            )
+
+        input_ids = self._pad_inputs_using_max_length(
+            origin_inputs=origin_inputs, pad_token_id=generation_config.pad_token_id
+        )
+
+        logger.debug(
+            "pad the origin inputs from %s into shape: %s",
+            origin_inputs.shape,
+            input_ids.shape,
+        )
+
+        input_mask = np.zeros_like(input_ids)
+        for i in range(valid_length_each_example.shape[0]):
+            input_mask[i, :valid_length_each_example[i]] = 1
+        encoder_output = None
+        encoder_mask = None
+        if is_encoder_decoder:
+            if generation_config.max_length > self.config.max_decode_length:
+                generation_config.max_length = self.config.max_decode_length
+            logger.debug("max decode length is: %s", generation_config.max_length)
+
+            # When do encoder and decoder prediction, the encoder can be cached
+            # to speed up the inference
+            (
+                encoder_output,
+                encoder_mask,
+                input_ids,
+                target_mask,
+            ) = self._prepare_model_inputs_for_decoder(input_ids, input_mask)
+            valid_length_each_example = [1 for _ in range(batch_size)]
+        # A single loop generates one token, loop until reaching target
+        # model_origin_max_length or generating eod token
+        is_finished = [False] * batch_size
+
+        # update model kwargs once, before go into generate loop.
+        self.update_model_kwargs_before_generate(input_ids, model_kwargs)
+
+        # setup is_first_iteration flag for incremental infer
+        if generation_config.use_past:
+            self.is_first_iteration = True
+        need_gather_logits = True
+
+        origin_len = np.sum(valid_length_each_example)
+        prepare_time = time.time() - prepare_time
+        logger.debug("forward prepare time: %s s", prepare_time)
+
+        while np.sum(is_finished) != batch_size:
+            forward_time = time.time()
+            seq_length = input_ids.shape[1]
+            current_index = [
+                valid_length_each_example[i] - 1 + i * seq_length
+                for i in range(batch_size)
+            ]
+            logger.debug("validate length: %s", valid_length_each_example)
+            if is_encoder_decoder:
+                inputs = Tensor(input_ids, mstype.int32)
+                # pylint: disable=E1102
+                res = self(
+                    input_ids=None,
+                    attention_mask=encoder_mask,
+                    encoder_outputs=encoder_output,
+                    decoder_input_ids=inputs,
+                    decoder_attention_mask=Tensor(target_mask, mstype.float32),
+                )
+            else:
+                model_kwargs["current_index"] = current_index
+                # model prepare input dict
+                model_inputs = self.prepare_inputs_for_generation(  # pylint: disable=E1111
+                    input_ids, **model_kwargs
+                )
+                # incremental generate
+                if generation_config.use_past:
+                    # when first iteration, gather last logits; others keep all logits.
+                    need_gather_logits = self.is_first_iteration
+                    # incremental generate
+                    res = self._incremental_infer(
+                        model_inputs=model_inputs,
+                        current_index=current_index,
+                        valid_length_each_example=valid_length_each_example,
+                    )
+                # auto-aggressive generate
+                else:
+                    res = self(**model_inputs)  # pylint: disable=E1102
+            forward_time = time.time() - forward_time
+
+            search_time = time.time()
+            # post process logits; skip this phase if post process is done in graph
+            if not self.config.is_sample_acceleration:
+                # convert to numpy for post process
+                logits = res[0] if isinstance(res, tuple) else res
+                if isinstance(logits, Tensor):
+                    logits = logits.asnumpy()
+                logits = np.reshape(logits, (-1, logits.shape[-1]))
+                # need gather last seq logits using current_index
+                # compare length to determine if need gather; if not, gather should be done in model construct
+                if need_gather_logits and logits.shape[0] > len(current_index):
+                    logits = logits[current_index]
+
+                # post process logits, without changing logits shape and order
+                probs = logits_processor(input_ids, logits, is_finished)
+                p_args = np.tile(np.arange(logits.shape[-1]), (batch_size, 1))
+            else:
+                probs, p_args = res
+                if isinstance(probs, Tensor):
+                    probs = probs.asnumpy()
+                if isinstance(p_args, Tensor):
+                    p_args = p_args.asnumpy()
+            search_time = time.time() - search_time
+
+            update_time = time.time()
+
+            # Random select a token as final output for this round
+            target_list = [[] for _ in range(batch_size)]
+            for i in range(batch_size):
+                if is_finished[i]:
+                    continue
+
+                target_index = np.argmax(probs[i])
+
+                # get target token id
+                target = p_args[i][target_index]
+                input_ids[i, valid_length_each_example[i]] = target
+
+                if streamer is not None:
+                    # assign target element
+                    target_list[i] = [target]
+
+                if is_encoder_decoder:
+                    target_mask[i][valid_length_each_example[i]] = int(1)
+
+                valid_length_each_example[i] += int(1)
+                input_mask[i][valid_length_each_example[i] - 1] = 1
+
+                # Stop judgment
+                if p_args[i][target_index] == generation_config.eos_token_id \
+                        or valid_length_each_example[i] == generation_config.max_length:
+                    is_finished[i] = True
+                    continue
+            if streamer is not None:
+                if batch_size == 1:
+                    streamer.put(target_list[0])
+                else:
+                    streamer.put(target_list)
+            update_time = time.time() - update_time
+            logger.debug("forward time: %s s; greedy search time: %s s; update time: %s s; total count: %s s",
+                         forward_time, search_time, update_time, forward_time + search_time + update_time)
+
+        # Return valid outputs out of padded outputs
+        output_ids = []
+        for i in range(batch_size):
+            output_ids.append(
+                input_ids[i, : int(valid_length_each_example[i])].astype(np.int32)
+            )
+        logger.debug("The output is: %s", output_ids)
+        if streamer is not None:
+            streamer.end()
+
+        generate_len = np.sum(valid_length_each_example) - origin_len
+        total_time = time.time() - total_time
+        logger.info("total time: %s s; generated tokens: %s tokens; generate speed: %s tokens/s",
+                    total_time, generate_len, generate_len / total_time)
+
+        return output_ids
+
+    def _sample(self,
+                origin_inputs,
+                generation_config: GenerationConfig,
+                logits_processor: Optional[LogitsProcessorList] = None,
+                logits_warper: Optional[LogitsProcessorList] = None,
+                streamer: BaseStreamer = None,
+                **model_kwargs):
+        r"""
+        Generates sequences of token ids for models with a language modeling head using **multinomial sampling** and
+        can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.
+
+        Parameters:
+            origin_inputs (`List(str), List(List(str))`):
+                The sequence used as a prompt for the generation.
+            generation_config (`GenerationConfig`, *optional*):
+                The generation configuration to be used as base parametrization for the generation
+                call. `**kwargs` passed to generate matching the attributes of `generation_config`
+                will override them. If `generation_config` is not provided, the default config
+                from the model configuration will be used. Please note that unspecified parameters
+                will inherit [`GenerationConfig`]'s default values, whose documentation should be
+                checked to parameterize generation.
+            logits_processor (`LogitsProcessorList`, *optional*):
+                An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]
+                used to modify the prediction scores of the language modeling head applied at each generation step.
+            logits_warper (`LogitsProcessorList`, *optional*):
+                An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsWarper`] used
+                to warp the prediction score distribution of the language modeling head applied before multinomial
+                sampling at each generation step.
+            streamer (`TextStreamer, *optional*`):
+                The streamer that generator uses.
+            model_kwargs:
+                Additional model specific kwargs will be forwarded to the `forward` function of the model. If model is
+                an encoder-decoder model the kwargs should include `encoder_outputs`.
+
+        Return:
+            A list of the generated token ids
+        """
+        total_time = time.time()
+        prepare_time = time.time()
+
+        logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()
+        logits_warper = logits_warper if logits_warper is not None else LogitsProcessorList()
+
+        if generation_config.pad_token_id is None:
+            generation_config.pad_token_id = 0
+
+        if streamer is not None:
+            streamer.put(origin_inputs)
+
+        batch_size = origin_inputs.shape[0]
+        is_encoder_decoder = self.config.is_encoder_decoder
+        logger.debug("The input shape is: %s", origin_inputs.shape)
+
+        valid_length_each_example, input_ids_length = \
+            get_valid_length_each_example(origin_inputs, generation_config.pad_token_id)
+
+        if generation_config.max_new_tokens is not None:
+            generation_config.max_length = generation_config.max_new_tokens + input_ids_length
+
+        if generation_config.max_length > self.config.seq_length:
+            logger.warning("max_length %s can not exceeds model seq_length %s, set max_length = seq_length.",
+                           generation_config.max_length, self.config.seq_length)
+            generation_config.max_length = self.config.seq_length
+
+        logger.debug("max length is: %s", generation_config.max_length)
+        if not is_encoder_decoder and input_ids_length >= generation_config.max_length:
+            raise ValueError(
+                f"the input_ids length {input_ids_length} exceeds the max length config {generation_config.max_length}."
+                f"check your inputs and set max_length larger than your inputs length."
+            )
+
+        input_ids = self._pad_inputs_using_max_length(
+            origin_inputs=origin_inputs, pad_token_id=generation_config.pad_token_id
+        )
+
+        logger.debug(
+            "pad the origin inputs from %s into shape: %s",
+            origin_inputs.shape,
+            input_ids.shape,
+        )
+
+        input_mask = np.zeros_like(input_ids)
+        for i in range(valid_length_each_example.shape[0]):
+            input_mask[i, :valid_length_each_example[i]] = 1
+        encoder_output = None
+        encoder_mask = None
+        if is_encoder_decoder:
+            if generation_config.max_length > self.config.max_decode_length:
+                generation_config.max_length = self.config.max_decode_length
+            logger.debug("max decode length is: %s", generation_config.max_length)
+
+            # When do encoder and decoder prediction, the encoder can be cached
+            # to speed up the inference
+            (
+                encoder_output,
+                encoder_mask,
+                input_ids,
+                target_mask,
+            ) = self._prepare_model_inputs_for_decoder(input_ids, input_mask)
+            valid_length_each_example = [1 for _ in range(batch_size)]
+        # A single loop generates one token, loop until reaching target
+        # model_origin_max_length or generating eod token
+        is_finished = [False] * batch_size
+
+        # update model kwargs once, before go into generate loop.
+        self.update_model_kwargs_before_generate(input_ids, model_kwargs)
+
+        # setup is_first_iteration flag for incremental infer
+        if generation_config.use_past:
+            self.is_first_iteration = True
+        need_gather_logits = True
+
+        origin_len = np.sum(valid_length_each_example)
+        prepare_time = time.time() - prepare_time
+        logger.debug("forward prepare time: %s s", prepare_time)
+
+        while np.sum(is_finished) != batch_size:
+            forward_time = time.time()
+            seq_length = input_ids.shape[1]
+            current_index = [
+                valid_length_each_example[i] - 1 + i * seq_length
+                for i in range(batch_size)
+            ]
+            logger.debug("validate length: %s", valid_length_each_example)
+            if is_encoder_decoder:
+                inputs = Tensor(input_ids, mstype.int32)
+                # pylint: disable=E1102
+                res = self(
+                    input_ids=None,
+                    attention_mask=encoder_mask,
+                    encoder_outputs=encoder_output,
+                    decoder_input_ids=inputs,
+                    decoder_attention_mask=Tensor(target_mask, mstype.float32),
+                )
+            else:
+                model_kwargs["current_index"] = current_index
+                # model prepare input dict
+                model_inputs = self.prepare_inputs_for_generation(  # pylint: disable=E1111
+                    input_ids, **model_kwargs
+                )
+                # incremental generate
+                if generation_config.use_past:
+                    # when first iteration, gather last logits; others keep all logits.
+                    need_gather_logits = self.is_first_iteration
+                    # incremental generate
+                    res = self._incremental_infer(
+                        model_inputs=model_inputs,
+                        current_index=current_index,
+                        valid_length_each_example=valid_length_each_example,
+                    )
+                # auto-aggressive generate
+                else:
+                    res = self(**model_inputs)  # pylint: disable=E1102
+            forward_time = time.time() - forward_time
+
+            sample_time = time.time()
+            # post process logits; skip this phase if post process is done in graph
+            if not self.config.is_sample_acceleration:
+                # convert to numpy for post process
+                logits = res[0] if isinstance(res, tuple) else res
+                if isinstance(logits, Tensor):
+                    logits = logits.asnumpy()
+                logits = np.reshape(logits, (-1, logits.shape[-1]))
+                # need gather last seq logits using current_index
+                # compare length to determine if need gather; if not, gather should be done in model construct
+                if need_gather_logits and logits.shape[0] > len(current_index):
+                    logits = logits[current_index]
+
+                # post process logits, without changing logits shape and order
+                probs = logits_processor(input_ids, logits, is_finished)
+                probs = logits_warper(input_ids, probs, is_finished)
+                p_args = np.tile(np.arange(logits.shape[-1]), (batch_size, 1))
+            else:
+                probs, p_args = res
+                if isinstance(probs, Tensor):
+                    probs = probs.asnumpy()
+                if isinstance(p_args, Tensor):
+                    p_args = p_args.asnumpy()
+            sample_time = time.time() - sample_time
+
+            update_time = time.time()
+            p_norms = softmax_with_threads(probs, is_finished)
+
+            # Random select a token as final output for this round
+            target_list = [[] for _ in range(batch_size)]
+            for i in range(batch_size):
+                if is_finished[i]:
+                    continue
+
+                p_norm = p_norms[i]
+                target_index = np.random.choice(len(probs[i]), p=p_norm)
+
+                # get target token id
+                target = p_args[i][target_index]
+                input_ids[i, valid_length_each_example[i]] = target
+
+                if streamer is not None:
+                    # assign target element
+                    target_list[i] = [target]
+
+                if is_encoder_decoder:
+                    target_mask[i][valid_length_each_example[i]] = int(1)
+
+                valid_length_each_example[i] += int(1)
+                input_mask[i][valid_length_each_example[i] - 1] = 1
+
+                # Stop judgment
+                if p_args[i][target_index] == generation_config.eos_token_id \
+                        or valid_length_each_example[i] == generation_config.max_length:
+                    is_finished[i] = True
+                    continue
+            if streamer is not None:
+                if batch_size == 1:
+                    streamer.put(target_list[0])
+                else:
+                    streamer.put(target_list)
+            update_time = time.time() - update_time
+            logger.debug("forward time: %s s; sample time: %s s; update time: %s s; total count: %s s",
+                         forward_time, sample_time, update_time, forward_time + sample_time + update_time)
+
+        # Return valid outputs out of padded outputs
+        output_ids = []
+        for i in range(batch_size):
+            output_ids.append(
+                input_ids[i, : int(valid_length_each_example[i])].astype(np.int32)
+            )
+        logger.debug("The output is: %s", output_ids)
+
+        if streamer is not None:
+            streamer.end()
+
+        generate_len = np.sum(valid_length_each_example) - origin_len
+        total_time = time.time() - total_time
+        logger.info("total time: %s s; generated tokens: %s tokens; generate speed: %s tokens/s",
+                    total_time, generate_len, generate_len / total_time)
+
+        return output_ids
+
     def _beam_search(self,
                      origin_inputs,
                      beam_scorer: BeamSearchScorer,
                      generation_config: GenerationConfig,
                      logits_processor: Optional[LogitsProcessorList] = None,
                      streamer: BaseStreamer = None,
                      **model_kwargs):
@@ -371,26 +797,37 @@
             raise ValueError("Beam search does not support sample acceleration yet! "
                              "Please set is_sample_acceleration to False.")
 
         total_time = time.time()
         prepare_time = time.time()
         logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()
 
+        if generation_config.pad_token_id is None:
+            generation_config.pad_token_id = 0
+
         batch_size = len(beam_scorer._beam_hyps)  # pylint: disable=W0212
         num_beams = beam_scorer.num_beams
         batch_beam_size = origin_inputs.shape[0]
         logger.debug("The input shape is: %s", origin_inputs.shape)
         if num_beams * batch_size != batch_beam_size:
             raise ValueError(
                 f"Batch dimension of `input_ids` should be {num_beams * batch_size}, but is {batch_beam_size}."
             )
 
-        valid_length_each_example, _ = \
+        is_encoder_decoder = self.config.is_encoder_decoder
+
+        valid_length_each_example, input_ids_length = \
             get_valid_length_each_example(origin_inputs, generation_config.pad_token_id)
 
+        if not is_encoder_decoder and input_ids_length > generation_config.max_length:
+            raise ValueError(
+                "The max_length set is smaller than the length in the input_ids."
+                f"You shout set max_length to {input_ids_length}"
+            )
+
         target_length = (
             self.config.seq_length
             if generation_config.max_length > self.config.seq_length
             else generation_config.max_length
         )
         logger.debug("max target_length is: %s", target_length)
         input_ids = self._pad_inputs_using_max_length(
@@ -408,15 +845,15 @@
         beam_scores = beam_scores.reshape((batch_size * num_beams,))
 
         input_mask = np.zeros_like(input_ids)
         for i in range(valid_length_each_example.shape[0]):
             input_mask[i, :valid_length_each_example[i]] = 1
         encoder_output = None
         encoder_mask = None
-        if self.config.is_encoder_decoder:
+        if is_encoder_decoder:
             if target_length > self.config.max_decode_length:
                 target_length = self.config.max_decode_length
             logger.debug("target_length is: %s", target_length)
 
             # When do encoder and decoder prediction, the encoder can be cached
             # to speed up the inference
             (
@@ -426,14 +863,17 @@
                 target_mask,
             ) = self._prepare_model_inputs_for_decoder(input_ids, input_mask)
             valid_length_each_example = np.ones((batch_beam_size, 1)).astype(np.int32)
 
         # update model kwargs once, before go into generate loop.
         self.update_model_kwargs_before_generate(input_ids, model_kwargs)
 
+        # setup is_first_iteration flag for incremental infer
+        if generation_config.use_past:
+            self.is_first_iteration = True
         need_gather_logits = True
 
         is_first_token = True
 
         origin_len = np.sum(valid_length_each_example) / num_beams
         prepare_time = time.time() - prepare_time
         logger.debug("forward prepare time: %s s", prepare_time)
@@ -442,15 +882,15 @@
             forward_time = time.time()
             seq_length = input_ids.shape[1]
             current_index = [
                 valid_length_each_example[i] - 1 + i * seq_length
                 for i in range(batch_beam_size)
             ]
             logger.debug("validate length: %s", valid_length_each_example)
-            if self.config.is_encoder_decoder:
+            if is_encoder_decoder:
                 inputs = Tensor(input_ids, mstype.int32)
                 # pylint: disable=E1102
                 res = self(
                     input_ids=None,
                     attention_mask=encoder_mask,
                     encoder_outputs=encoder_output,
                     decoder_input_ids=inputs,
@@ -520,15 +960,15 @@
             old_input_ids = input_ids.copy()
             for i in range(batch_beam_size):
                 input_ids[i] = old_input_ids[beam_idx[i], :]
 
             # add new tokens to input_ids
             for i in range(batch_beam_size):
                 input_ids[i, valid_length_each_example[i]] = beam_next_tokens[i]
-                if self.config.is_encoder_decoder:
+                if is_encoder_decoder:
                     target_mask[i][valid_length_each_example[i]] = int(1)
 
                 input_mask[i][valid_length_each_example[i]] = 1
                 valid_length_each_example[i] += int(1)
 
             update_time = time.time() - update_time
             logger.debug("forward time: %s s; beam search time: %s s; update time: %s s; total count: %s s",
@@ -589,18 +1029,14 @@
                 forwarded to the `forward` function of the model. Supported `generate_config` keywords can be
                 checked in [`GenerationConfig`]'s documentation. Mainly used Keywords are shown below:
 
                 max_length(int): The maximum length the generated tokens can have. Corresponds to the length of
                     the input prompt + `max_new_tokens`. Its effect is overridden by `max_new_tokens`, if also set.
                 max_new_tokens (int): The maximum numbers of tokens to generate, ignoring the number of
                     tokens in the prompt.
-                min_length (int): The minimum length of the sequence to be generated. Corresponds to the length of the
-                    input prompt + `min_new_tokens`. Its effect is overridden by `min_new_tokens`, if also set.
-                min_new_tokens (int): The minimum numbers of tokens to generate, ignoring the number of tokens
-                    in the prompt.
                 do_sample(bool): Whether to do sampling on the candidate ids.
                     If set True it will be enabled, and set it to be False to disable the sampling,
                     equivalent to topk 1.
                     If set None, it follows the setting in the configureation in the model.
                 top_k(int): Determine the topK numbers token id as candidate. This should be a positive number.
                     If set None, it follows the setting in the configureation in the model.
                 top_p(float): The accumulation probability of the candidate token ids below the top_p
@@ -655,15 +1091,14 @@
         try:
             input_ids = np.array(input_ids)
         except ValueError as e:
             raise ValueError(str(e) + " Please check your inputs of model.generate(),"
                                       " and make sure the inputs are padded to same length.")
         input_ids = np.reshape(input_ids, (-1, np.shape(input_ids)[-1]))
         batch_size = input_ids.shape[0]
-
         seed = 0 if seed is None else seed
         np.random.seed(seed)
 
         logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()
 
         # use_past should be defined in model config
         use_past_tmp = kwargs.pop("use_past", None)
@@ -683,66 +1118,61 @@
         )  # All unused kwargs must be model kwargs
 
         if generation_config.num_beams > 1:
             logger.warning("When num_beams is set to a value greater than 1, do_sample will be set to False, "
                            "due to the current beam search does not support sampling.")
             generation_config.do_sample = False
         if not generation_config.do_sample:
-            if generation_config.top_p != 1.0:
-                logger.warning("When do_sample is set to False, top_p will be set to 1, making them inactive.")
-                generation_config.top_p = 1.0
-
-            if generation_config.top_k != 0:
-                logger.warning("When do_sample is set to False, top_k will be set to 0, making them inactive.")
-                generation_config.top_k = 0
+            logger.warning("When do_sample is set to False, top_k will be set to 1 and top_p will be set to 0, "
+                           "making them inactive.")
+            generation_config.top_p = 1.0
+            generation_config.top_k = 0
         logger.info("Generation Config is: %s", generation_config)
 
-        if generation_config.pad_token_id is None:
-            generation_config.pad_token_id = 0
-
-        if generation_config.max_length > self.config.seq_length:
-            logger.warning("max_length %s can not exceeds model seq_length %s, set max_length = seq_length.",
-                           generation_config.max_length, self.config.seq_length)
-            generation_config.max_length = self.config.seq_length
-
-        logger.debug("max length is: %s", generation_config.max_length)
-
         _, input_ids_length = get_valid_length_each_example(input_ids, generation_config.pad_token_id)
 
-        if generation_config.max_new_tokens is not None:
-            generation_config.max_length = generation_config.max_new_tokens + input_ids_length
-
-        if not self.config.is_encoder_decoder and input_ids_length > generation_config.max_length:
-            raise ValueError(
-                "The max_length set is smaller than the length in the input_ids."
-                f"You shout set max_length to {input_ids_length}"
-            )
-
-        logits_processor = self.get_logits_processor(
+        logits_processor = self._get_logits_processor(
             generation_config=generation_config,
             input_ids_seq_length=input_ids_length,
             logits_processor=logits_processor,
         )
 
         # determine generation mode
-        generation_config.generation_mode = self._get_generation_mode(generation_config)
+        generation_mode = self._get_generation_mode(generation_config)
 
         if streamer is not None and (generation_config.num_beams > 1):
             raise ValueError(
                 "`streamer` cannot be used with beam search yet. Make sure that `num_beams` is set to 1."
             )
-        self._set_kbk_infer()
 
-        if generation_config.use_past and self.use_kbk_infer:
-            self._set_block_mgr(batch_size)
-            if self.config.is_dynamic:
-                self.set_dynamic_inputs()
+        if generation_mode == GenerationMode.GREEDY_SEARCH:
+            # run greedy search
+            output_ids = self._greedy_search(
+                origin_inputs=input_ids,
+                generation_config=generation_config,
+                logits_processor=logits_processor,
+                streamer=streamer,
+                **model_kwargs,
+            )
+
+        elif generation_mode == GenerationMode.SAMPLE:
+            # prepare logits warper
+            logits_warper = self._get_logits_warper(generation_config)
 
-        # beam search
-        if generation_config.generation_mode == GenerationMode.BEAM_SEARCH:
+            # run sample
+            output_ids = self._sample(
+                origin_inputs=input_ids,
+                generation_config=generation_config,
+                logits_processor=logits_processor,
+                logits_warper=logits_warper,
+                streamer=streamer,
+                **model_kwargs,
+            )
+
+        elif generation_mode == GenerationMode.BEAM_SEARCH:
             # prepare beam search scorer
             beam_scorer = BeamSearchScorer(
                 batch_size=batch_size,
                 num_beams=generation_config.num_beams,
                 max_length=generation_config.max_length
             )
             # interleave input_ids with `num_beams` additional sequences per batch
@@ -753,428 +1183,11 @@
                 origin_inputs=input_ids,
                 beam_scorer=beam_scorer,
                 generation_config=generation_config,
                 logits_processor=logits_processor,
                 streamer=streamer,
                 **model_kwargs
             )
-        # greedy search or sample
-        else:
-            total_time = time.time()
-            prepare_time = time.time()
-
-            origin_inputs = input_ids
-            logits_warper = self.get_logits_warper(generation_config) \
-                if generation_config.generation_mode == GenerationMode.SAMPLE else None
-
-            if streamer is not None:
-                streamer.put(origin_inputs)
-
-            batch_size = origin_inputs.shape[0]
-            logger.debug("The input shape is: %s", origin_inputs.shape)
-
-            valid_length_each_example, _ = \
-                get_valid_length_each_example(origin_inputs, generation_config.pad_token_id)
-
-            input_ids = self._pad_inputs_using_max_length(
-                origin_inputs=origin_inputs, pad_token_id=generation_config.pad_token_id
-            )
-
-            logger.debug(
-                "pad the origin inputs from %s into shape: %s",
-                origin_inputs.shape,
-                input_ids.shape,
-            )
-
-            input_mask = np.zeros_like(input_ids)
-            for i in range(valid_length_each_example.shape[0]):
-                input_mask[i, :valid_length_each_example[i]] = 1
-            encoder_output = None
-            encoder_mask = None
-            target_mask = None
-            if self.config.is_encoder_decoder:
-                if generation_config.max_length > self.config.max_decode_length:
-                    generation_config.max_length = self.config.max_decode_length
-                logger.debug("max decode length is: %s", generation_config.max_length)
-
-                # When do encoder and decoder prediction, the encoder can be cached
-                # to speed up the inference
-                (
-                    encoder_output,
-                    encoder_mask,
-                    input_ids,
-                    target_mask,
-                ) = self._prepare_model_inputs_for_decoder(input_ids, input_mask)
-                valid_length_each_example = [1 for _ in range(batch_size)]
-            # A single loop generates one token, loop until reaching target
-            # model_origin_max_length or generating eod token
-            is_finished = [False] * batch_size
-
-            # update model kwargs once, before go into generate loop.
-            self.update_model_kwargs_before_generate(input_ids, model_kwargs)
-
-            origin_len = np.sum(valid_length_each_example)
-            prepare_time = time.time() - prepare_time
-            logger.debug("forward prepare time: %s s", prepare_time)
-
-            prefill = True
-            if self.use_kbk_infer:
-                model_kwargs["origin_inputs"] = origin_inputs
-            while np.sum(is_finished) != batch_size:
-                block_tables = None
-                slot_mapping = None
-                if generation_config.use_past and self.use_kbk_infer:
-                    if prefill:
-                        if self.config.is_dynamic:
-                            max_input_length = len(origin_inputs[0])
-                        else:
-                            max_input_length = self.config.seq_length
-                        block_tables, slot_mapping = self.block_mgr.assemble_pa_full_inputs(max_input_length,
-                                                                                            valid_length_each_example,
-                                                                                            is_finished)
-                    else:
-                        block_tables, slot_mapping = self.block_mgr.assemble_pa_inc_inputs(valid_length_each_example,
-                                                                                           is_finished)
-
-                target_list, is_finished = self.infer(input_ids=input_ids,
-                                                      valid_length_each_example=valid_length_each_example,
-                                                      generation_config=generation_config,
-                                                      logits_processor=logits_processor,
-                                                      logits_warper=logits_warper,
-                                                      block_tables=block_tables,
-                                                      slot_mapping=slot_mapping,
-                                                      prefill=prefill,
-                                                      is_finished=is_finished,
-                                                      encoder_mask=encoder_mask,
-                                                      encoder_output=encoder_output,
-                                                      target_mask=target_mask,
-                                                      **model_kwargs)
-                if generation_config.use_past:
-                    if prefill and "origin_inputs" in model_kwargs:
-                        model_kwargs.pop("origin_inputs")
-                    prefill = False
-                for i in range(batch_size):
-                    if is_finished[i]:
-                        continue
-
-                    input_ids[i, valid_length_each_example[i]] = target_list[i]
-
-                    if self.config.is_encoder_decoder:
-                        target_mask[i][valid_length_each_example[i]] = int(1)
-
-                    valid_length_each_example[i] += int(1)
-                    input_mask[i][valid_length_each_example[i] - 1] = 1
-
-                    # Stop judgment
-                    if target_list[i] == generation_config.eos_token_id \
-                            or valid_length_each_example[i] == generation_config.max_length:
-                        is_finished[i] = True
-
-                if streamer is not None:
-                    if batch_size == 1:
-                        streamer.put(target_list[0])
-                    else:
-                        streamer.put(target_list)
-
-            # Return valid outputs out of padded outputs
-            output_ids = []
-            for i in range(batch_size):
-                output_ids.append(
-                    input_ids[i, : int(valid_length_each_example[i])].astype(np.int32)
-                )
-            logger.debug("The output is: %s", output_ids)
-            if streamer is not None:
-                streamer.end()
-
-            generate_len = np.sum(valid_length_each_example) - origin_len
-            total_time = time.time() - total_time
-            logger.info("total time: %s s; generated tokens: %s tokens; generate speed: %s tokens/s",
-                        total_time, generate_len, generate_len / total_time)
 
         # set to original phase
         self.set_train(origin_phase == "train")
-
-        if self.block_mgr:
-            self.block_mgr.clear_cache()
-
         return output_ids
-
-    def infer(self,
-              input_ids: [Union[List[int], List[List[int]]]],
-              valid_length_each_example: [List[int]],
-              generation_config: [GenerationConfig] = None,
-              logits_processor: Optional[LogitsProcessorList] = None,
-              logits_warper: Optional[LogitsProcessorList] = None,
-              block_tables: Optional[Tensor] = None,
-              slot_mapping: Optional[Tensor] = None,
-              prefill: bool = True,
-              is_finished: List[bool] = None,
-              encoder_mask: Optional[Tensor] = None,
-              encoder_output: Optional[Tensor] = None,
-              target_mask: Optional[Tensor] = None,
-              **model_kwargs):
-        """
-        do infer and return logits on next position, can choose do prefill or decode predict.
-
-        Args:
-            input_ids (List(List(int))):
-                Input ids after padding.
-            valid_length_each_example (List(int)):
-                Valid input length except padding.
-            generation_config (`GenerationConfig`):
-                The generation configuration to be used as base parametrization for the generation call.
-            logits_processor (`LogitsProcessorList`, *optional*):
-                An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]
-                used to modify the prediction scores of the language modeling head applied at each generation step.
-            logits_warper (`LogitsProcessorList`, *optional*):
-                An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsWarper`] used
-                to warp the prediction score distribution of the language modeling head applied before multinomial
-                sampling at each generation step.
-            block_tables (Tensor):
-                Params for page attention
-            slot_mapping (Tensor):
-                Params for page attention
-            prefill (bool):
-                Whether to do prefill predict or decode predict
-            is_finished (List(bool)):
-                Whether each sequence is finished its generation.
-            encoder_mask (Tensor):
-                Use for encoder-decoder construct, do not need for decoder only construct
-            encoder_output (Tensor):
-                Use for encoder-decoder construct, do not need for decoder only construct
-            target_mask (Tensor):
-                Use for encoder-decoder construct, do not need for decoder only construct
-
-        Returns:
-            next_token, is_finished
-        """
-        max_valid_length = max(valid_length_each_example)
-        if not self.config.is_encoder_decoder and max_valid_length > self.config.seq_length:
-            raise ValueError(
-                f"The input length:{max_valid_length} is longer than the seq_length:{self.config.seq_length}, "
-                "which is not allowed."
-            )
-
-        start_time = time.time()
-
-        input_ids = np.array(input_ids)
-        res, current_index = self.forward(input_ids=input_ids,
-                                          valid_length_each_example=valid_length_each_example,
-                                          generation_config=generation_config,
-                                          block_tables=block_tables,
-                                          slot_mapping=slot_mapping,
-                                          prefill=prefill,
-                                          encoder_mask=encoder_mask,
-                                          encoder_output=encoder_output,
-                                          target_mask=target_mask,
-                                          **model_kwargs)
-
-        forward_time = time.time() - start_time
-        sample_time = time.time()
-
-        need_gather_logits = True
-        if not self.config.is_encoder_decoder and generation_config.use_past:
-            need_gather_logits = prefill
-
-        next_id, is_finished = self.postprocess(input_ids=input_ids,
-                                                is_finished=is_finished,
-                                                res=res,
-                                                generation_config=generation_config,
-                                                valid_length_each_example=valid_length_each_example,
-                                                current_index=current_index,
-                                                logits_processor=logits_processor,
-                                                logits_warper=logits_warper,
-                                                need_gather_logits=need_gather_logits)
-
-        sample_time = time.time() - sample_time
-        infer_time = time.time() - start_time
-        logger.debug("forward time: %s s; sample time: %s s; total count: %s s",
-                     forward_time, sample_time, infer_time)
-
-        return next_id, is_finished
-
-    def forward(self,
-                input_ids: [Union[List[int], List[List[int]]]],
-                valid_length_each_example: [List[int]],
-                generation_config: [GenerationConfig] = None,
-                block_tables: Optional[Tensor] = None,
-                slot_mapping: Optional[Tensor] = None,
-                prefill: bool = None,
-                encoder_mask: Optional[Tensor] = None,
-                encoder_output: Optional[Tensor] = None,
-                target_mask: Optional[Tensor] = None,
-                **model_kwargs):
-        """
-        Model forward process.
-
-        Args:
-            input_ids (List(List(int))):
-                Input ids after padding.
-            valid_length_each_example (List(int)):
-                Valid input length except padding.
-            generation_config (`GenerationConfig`):
-                The generation configuration to be used as base parametrization for the generation call.
-            block_tables (Tensor):
-                Params for page attention
-            slot_mapping (Tensor):
-                Params for page attention
-            prefill (bool):
-                Whether to do prefill predict or decode predict
-            encoder_mask (Tensor):
-                Use for encoder-decoder construct, do not need for decoder only construct
-            encoder_output (Tensor):
-                Use for encoder-decoder construct, do not need for decoder only construct
-            target_mask (Tensor):
-                Use for encoder-decoder construct, do not need for decoder only construct
-
-        Returns:
-            res, current_index
-        """
-        input_ids = np.reshape(input_ids, (-1, np.shape(input_ids)[-1]))
-        batch_size = input_ids.shape[0]
-        seq_length = input_ids.shape[1]
-        current_index = [
-            valid_length_each_example[i] - 1 + i * seq_length
-            for i in range(batch_size)
-        ]
-
-        if self.config.is_encoder_decoder:
-            inputs = Tensor(input_ids, mstype.int32)
-            # pylint: disable=E1102
-            res = self(
-                input_ids=None,
-                attention_mask=encoder_mask,
-                encoder_outputs=encoder_output,
-                decoder_input_ids=inputs,
-                decoder_attention_mask=Tensor(target_mask, mstype.float32),
-            )
-        else:
-            model_kwargs["current_index"] = current_index
-            # pylint: disable=E1111
-            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
-            real_input_ids = model_inputs["input_ids"]
-            batch_size = real_input_ids.shape[0]
-            seq_length = real_input_ids.shape[1]
-            current_index = [
-                valid_length_each_example[i] - 1 + i * seq_length
-                for i in range(batch_size)
-            ]
-            model_kwargs["current_index"] = current_index
-
-            if generation_config.use_past:
-                res = self._incremental_infer(
-                    model_inputs=model_inputs,
-                    prefill=prefill,
-                    current_index=current_index,
-                    valid_length_each_example=valid_length_each_example,
-                    block_tables=block_tables,
-                    slot_mapping=slot_mapping
-                )
-            else:
-                res = self(**model_inputs)  # pylint: disable=E1102
-
-        return res, current_index
-
-    def postprocess(self,
-                    input_ids,
-                    is_finished,
-                    res,
-                    generation_config,
-                    valid_length_each_example,
-                    current_index: Optional[Union[List[int], List[List[int]]]],
-                    logits_processor: Optional[LogitsProcessorList] = None,
-                    logits_warper: Optional[LogitsProcessorList] = None,
-                    need_gather_logits: bool = True):
-        """
-        postprocess of the output from model generation.
-
-        Args:
-            input_ids (List(List(int))):
-                Input ids after padding.
-            res (List(List(int))):
-                Logits after infer.
-            is_finished (List(bool)):
-                Whether each sequence is finished its generation.
-            generation_config (`GenerationConfig`):
-                The generation configuration to be used as base parametrization for the generation call.
-            valid_length_each_example (List(int)):
-                Valid input length except padding.
-            current_index (List(int)):
-                Current index of sequence.
-            logits_processor (`LogitsProcessorList`, *optional*):
-                An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]
-                used to modify the prediction scores of the language modeling head applied at each generation step.
-            logits_warper (`LogitsProcessorList`, *optional*):
-                An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsWarper`] used
-                to warp the prediction score distribution of the language modeling head applied before multinomial
-                sampling at each generation step.
-            need_gather_logits (bool):
-                whether gather result, when decode predict and is first iteration, set True.
-        """
-        batch_size = input_ids.shape[0]
-        target_list = [[] for _ in range(batch_size)]
-
-        if not hasattr(generation_config, "generation_mode"):
-            generation_config.generation_mode = self._get_generation_mode(generation_config)
-        if generation_config.generation_mode == GenerationMode.GREEDY_SEARCH:
-            if not self.config.is_sample_acceleration:
-                logits = res[0] if isinstance(res, tuple) else res
-                logits = P.Reshape()(logits, (-1, logits.shape[-1]))
-                if need_gather_logits and logits.shape[0] > len(current_index):
-                    logits = logits[Tensor(current_index, dtype=mstype.int32)]
-                if logits_processor:
-                    if isinstance(logits, Tensor):
-                        logits = logits.asnumpy()
-                    logits = Tensor(logits_processor(input_ids, logits, is_finished))
-                target_list = P.Argmax()(logits)
-                target_list = target_list.asnumpy().tolist()
-            else:
-                probs, p_args = res
-                if isinstance(p_args, Tensor):
-                    p_args = p_args.asnumpy()
-                target_index_list = P.Argmax()(probs)
-                target_index_list = target_index_list.asnumpy().tolist()
-            # run greedy search
-            for i in range(batch_size):
-                if is_finished[i]:
-                    continue
-                if self.config.is_sample_acceleration:
-                    target_index = target_index_list[i]
-                    target = p_args[i][target_index]
-                    target_list[i] = target
-
-        elif generation_config.generation_mode == GenerationMode.SAMPLE:
-            if not self.config.is_sample_acceleration:
-                # convert to numpy for post process
-                logits = res[0] if isinstance(res, tuple) else res
-                if isinstance(logits, Tensor):
-                    logits = logits.asnumpy()
-                logits = np.reshape(logits, (-1, logits.shape[-1]))
-                # need gather last seq logits using current_index
-                # compare length to determine if need gather; if not, gather should be done in model construct
-                if need_gather_logits and logits.shape[0] > len(current_index):
-                    logits = logits[current_index]
-
-                probs = logits_processor(input_ids, logits, is_finished)
-                p_args = np.tile(np.arange(logits.shape[-1]), (batch_size, 1))
-                probs = logits_warper(input_ids, probs, is_finished)
-            else:
-                probs, p_args = res
-                if isinstance(probs, Tensor):
-                    probs = probs.asnumpy()
-                if isinstance(p_args, Tensor):
-                    p_args = p_args.asnumpy()
-            p_norms = softmax_with_threads(probs, is_finished)
-
-            for i in range(batch_size):
-                if is_finished[i]:
-                    continue
-                p_norm = p_norms[i]
-                target_index = np.random.choice(len(probs[i]), p=p_norm)
-                # get target token id
-                target = p_args[i][target_index]
-                target_list[i] = target
-
-        elif generation_config.generation_mode == GenerationMode.BEAM_SEARCH:
-            raise ValueError("sampler method doesn't support BEAM_SEARCH. ")
-
-        return target_list, is_finished
```

## mindformers/models/__init__.py

```diff
@@ -15,14 +15,15 @@
 
 """models init"""
 from .auto import *
 from .bert import *
 from .mae import *
 from .vit import *
 from .swin import *
+from .blip2 import *
 from .clip import *
 from .t5 import *
 from .gpt2 import *
 from .glm import *
 from .glm2 import *
 from .glm3 import *
 from .llama import *
@@ -36,22 +37,23 @@
 from .base_config import BaseConfig
 from .base_model import BaseModel
 from .image_processing_utils import BaseImageProcessor
 from .processing_utils import ProcessorMixin
 from .base_processor import BaseProcessor, BaseAudioProcessor
 from .build_tokenizer import build_tokenizer
 from .build_processor import build_processor
-from .build_model import build_model_config, build_head, build_network, \
+from .build_model import build_model_config, build_head, \
     build_model, build_encoder
 from .utils import CONFIG_NAME, FEATURE_EXTRACTOR_NAME, IMAGE_PROCESSOR_NAME
 
 __all__ = ['BaseConfig', 'BaseModel', 'BaseProcessor', 'BaseImageProcessor',
            'BaseAudioProcessor', 'PreTrainedTokenizerFast']
 
 __all__.extend(auto.__all__)
+__all__.extend(blip2.__all__)
 __all__.extend(bert.__all__)
 __all__.extend(mae.__all__)
 __all__.extend(vit.__all__)
 __all__.extend(swin.__all__)
 __all__.extend(clip.__all__)
 __all__.extend(t5.__all__)
 __all__.extend(gpt2.__all__)
```

## mindformers/models/base_model.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2024 Huawei Technologies Co., Ltd
+# Copyright 2023 Huawei Technologies Co., Ltd
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 # http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -21,15 +21,15 @@
 import shutil
 import yaml
 
 import mindspore as ms
 from mindspore import nn
 from mindspore.train.serialization import load_checkpoint, load_param_into_net
 
-from .build_model import build_network
+from .build_model import build_model
 from ..generation import GenerationMixin
 from ..mindformer_book import MindFormerBook, print_path_or_list
 from .configuration_utils import PretrainedConfig
 from ..tools.register import MindFormerConfig, DictConfig
 from ..tools.download_tools import download_with_progress_bar
 from ..tools.logger import logger
 from ..tools.utils import try_sync_file, replace_tk_to_mindpet
@@ -185,14 +185,26 @@
             config.__dict__.pop("type")
 
         for key, val in config.__dict__.items():
             if isinstance(val, PretrainedConfig):
                 val.__dict__.pop("type")
                 config.__dict__.update({key: val})
 
+    def prepare_inputs_for_export(self, full_model=True):
+        """
+        prepare inputs for export.
+        A model class needs to define a `prepare_inputs_for_export` method
+
+        Raises:
+            RuntimeError: Not implemented in model but call `.prepare_inputs_for_export()`
+        """
+        raise RuntimeError(
+            "A model class needs to define a `prepare_inputs_for_export` method "
+        )
+
     def _inverse_parse_config(self, config):
         """
         Inverse parse config method, which builds yaml file content for model config.
 
         Args:
             config (PretrainedConfig): a model config inherited from PretrainedConfig.
 
@@ -332,15 +344,15 @@
 
         if not isinstance(pretrained_model_name_or_dir, str):
             raise TypeError(f"pretrained_model_name_or_dir should be a str,"
                             f" but got {type(pretrained_model_name_or_dir)}")
         config_args = cls._get_config_args(pretrained_model_name_or_dir, **kwargs)
         if not download_checkpoint:
             config_args.model.model_config.checkpoint_name_or_path = None
-        model = build_network(config_args.model)
+        model = build_model(config_args.model)
         logger.info("model built successfully!")
         return model
 
     @classmethod
     def show_support_list(cls):
         """show_support_list method"""
         logger.info("support list of %s is:", cls.__name__)
```

## mindformers/models/build_model.py

```diff
@@ -64,28 +64,14 @@
             default_args.setdefault('config', model_config)
 
             return MindFormerRegister.get_instance_from_cfg(
                 config.arch, MindFormerModuleType.MODELS, default_args=default_args)
         return None
     return None
 
-def build_network(
-        config: dict = None, default_args: dict = None):
-    """Create the pet network For MindFormer"""
-    ckpt_cfg = config.model_config.checkpoint_name_or_path
-    pet_config = config.model_config.pet_config
-    network = build_model(config, default_args=default_args)
-    if pet_config:
-        from mindformers.pet import get_pet_model, is_supported_pet_type
-        if is_supported_pet_type(pet_config.pet_type):
-            config.model_config.checkpoint_name_or_path = None
-        network.checkpoint_name_or_path = ckpt_cfg
-        network = get_pet_model(network, pet_config)
-    return network
-
 def build_encoder(
         config: dict = None, default_args: dict = None,
         module_type: str = 'encoder', class_name: str = None, **kwargs):
     """Build encoder API."""
     if config is None and class_name is None:
         return None
     if config is not None:
```

## mindformers/models/modeling_utils.py

```diff
@@ -20,15 +20,15 @@
 import json
 import shutil
 from functools import partial
 from typing import Dict, Optional, Union
 import yaml
 
 import mindspore as ms
-from mindspore import nn, JitConfig
+from mindspore import nn
 from mindspore import load_checkpoint, load_param_into_net
 
 from mindformers.tools.hub import (
     PushToHubMixin,
     cached_file,
     download_url,
     extract_commit_hash,
@@ -43,15 +43,15 @@
 from mindformers.tools.logger import logger
 from mindformers.tools.register import MindFormerConfig, DictConfig
 from ..mindformer_book import MindFormerBook, print_path_or_list
 from ..tools.download_tools import download_with_progress_bar
 from ..tools.utils import try_sync_file, replace_tk_to_mindpet
 from .configuration_utils import PretrainedConfig
 from .utils import CONFIG_NAME, WEIGHTS_NAME, WEIGHTS_INDEX_NAME
-from .build_model import build_network
+from .build_model import build_model
 
 __all__ = ["PreTrainedModel"]
 
 IGNORE_KEYS = ["_name_or_path"]
 
 
 def dtype_byte_size(dtype):
@@ -248,15 +248,14 @@
         :str: Identifies that this is a Mindspore model.
         """
         return "ms"
 
     # pylint: disable=W0613
     def __init__(self, config: PretrainedConfig, *inputs, **kwargs):
         super().__init__(**kwargs)
-        GenerationMixin.__init__(self)
         if not isinstance(config, PretrainedConfig):
             raise ValueError(
                 f"Parameter config in `{self.__class__.__name__}(config)` should be an instance of class "
                 "`PretrainedConfig`. To create a model from a pretrained model use "
                 f"`model = {self.__class__.__name__}.from_pretrained(PRETRAINED_MODEL_NAME)`"
             )
         # Save config and origin of the pretrained weights if given in model
@@ -573,46 +572,24 @@
             config.__dict__.pop("type")
 
         for key, val in config.__dict__.items():
             if isinstance(val, PretrainedConfig):
                 val.__dict__.pop("type")
                 config.__dict__.update({key: val})
 
-    def set_model_predict_config(self):
+    def prepare_inputs_for_export(self, full_model=True):
         """
-        Set predict config for model.
-        """
-        if self.config.use_past:
-            # jit_level = "O0" indicates that the operator is executed in the form of kernel by kernel mode.
-            # infer_boost = "on" indicates that the high performance inference is enabled.
-            jit_level = "O0"
-            infer_boost = "on"
-            jit_config = JitConfig(jit_level=jit_level, infer_boost=infer_boost)
-            self.set_jit_config(jit_config)
-            logger.info(
-                "Set jit config for jit level:{} and infer boost:{}.".format(jit_level, infer_boost))
+        prepare inputs for export.
+        A model class needs to define a `prepare_inputs_for_export` method
 
-    def prepare_inputs_for_predict_layout(self, input_ids, **kwargs):
-        """
-        prepare inputs for transform ckpt.
+        Raises:
+            RuntimeError: Not implemented in model but call `.prepare_inputs_for_export()`
         """
         raise RuntimeError(
-            "A model class needs to define a `prepare_inputs_for_predict_layout`"
-            " method in order to use parallel predict."
-        )
-
-    # pylint: disable=W0613
-    def set_dynamic_inputs(self):
-        """
-        Compile static graphs into dynamic shapes
-        """
-
-        raise RuntimeError(
-            "A model class needs to define a `set_dynamic_inputs`"
-            " method in order to use `model.set_inputs()`."
+            "A model class needs to define a `prepare_inputs_for_export` method "
         )
 
     def _inverse_parse_config(self, config):
         """
         Inverse parse config method, which builds yaml file content for model config.
 
         Args:
@@ -834,15 +811,15 @@
 
         if not isinstance(pretrained_model_name_or_dir, str):
             raise TypeError(f"pretrained_model_name_or_dir should be a str,"
                             f" but got {type(pretrained_model_name_or_dir)}")
         config_args = cls._get_config_args(pretrained_model_name_or_dir, **kwargs)
         if not download_checkpoint:
             config_args.model.model_config.checkpoint_name_or_path = None
-        model = build_network(config_args.model)
+        model = build_model(config_args.model)
         logger.info("model built successfully!")
         return model
 
     @classmethod
     def from_pretrained_experimental_mode(
             cls,
             pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],
@@ -1445,10 +1422,7 @@
         logger.info("support list of %s is:", cls.__name__)
         print_path_or_list(cls._support_list)
 
     @classmethod
     def get_support_list(cls):
         """get_support_list method"""
         return cls._support_list
-
-    def kvcache(self, layer_idx):
-        raise RuntimeError("Override the kvcache method to get key cache value cache for swap.")
```

## mindformers/models/tokenization_utils.py

```diff
@@ -549,15 +549,14 @@
                 token_index = new_idx + added_tokens
                 current_vocab[token.content] = token_index
                 added_tokens += 1
             else:
                 token_index = current_vocab[token.content]
 
             if token.special and str(token) not in self.all_special_tokens:
-                self.reset_special_tokens_cache()
                 self._additional_special_tokens.append(token)
             # the setter automatically updates the reverse map
             self._added_tokens_decoder[token_index] = token
             self._added_tokens_encoder[token.content] = token_index
             if self.verbose:
                 logger.info(f"Adding {token} to the vocabulary")
```

## mindformers/models/tokenization_utils_base.py

```diff
@@ -849,15 +849,14 @@
         self._sep_token = None
         self._pad_token = None
         self._cls_token = None
         self._mask_token = None
         self._pad_token_type_id = 0
         self._additional_special_tokens = []
         self.verbose = verbose
-        self.reset_special_tokens_cache()
 
         # We directly set the hidden value to allow initialization with special tokens
         # which are not yet in the vocabulary. Necessary for serialization/de-serialization
         # TODO clean this up at some point (probably by switching to fast tokenizers)
         for key, value in kwargs.items():
             if value is None:
                 continue
@@ -876,18 +875,14 @@
     def sanitize_special_tokens(self) -> int:
         """
         The `sanitize_special_tokens` is now deprecated kept for backward compatibility and will be removed in
         transformers v5.
         """
         return self.add_tokens(self.all_special_tokens_extended, special_tokens=True)
 
-    def reset_special_tokens_cache(self):
-        self._all_special_tokens = []
-        self._all_special_ids = []
-
     def add_special_tokens(
             self, special_tokens_dict: Dict[str, Union[str, AddedToken]], replace_additional_special_tokens=True
     ) -> int:
         """
         Add a dictionary of special tokens (eos, pad, cls, etc.) to the encoder and link them to class attributes. If
         special tokens are NOT in the vocabulary, they are added to it (indexed starting from the last index of the
         current vocabulary).
@@ -1348,32 +1343,24 @@
     @property
     def all_special_tokens(self) -> List[str]:
         """
         `List[str]`: A list of the unique special tokens (`'<unk>'`, `'<cls>'`, ..., etc.).
 
         Convert tokens of `tokenizers.AddedToken` type to string.
         """
-        if self._all_special_tokens:
-            return self._all_special_tokens
-
         all_toks = [str(s) for s in self.all_special_tokens_extended]
-        self._all_special_tokens = all_toks
         return all_toks
 
     @property
     def all_special_ids(self) -> List[int]:
         """
         `List[int]`: List the ids of the special tokens(`'<unk>'`, `'<cls>'`, etc.) mapped to class attributes.
         """
-        if self._all_special_ids:
-            return self._all_special_ids
-
         all_toks = self.all_special_tokens
         all_ids = self.convert_tokens_to_ids(all_toks)
-        self._all_special_ids = all_ids
         return all_ids
 
 
 ENCODE_KWARGS_DOCSTRING = r"""
             add_special_tokens (`bool`, *optional*, defaults to `True`):
                 Whether or not to add special tokens when encoding the sequences. This will use the underlying
                 `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines which tokens are
@@ -2215,15 +2202,15 @@
                 f"Can't load following files from cache: {unresolved_files} and cannot check if these "
                 "files are necessary for the tokenizer to operate."
             )
 
         if all(full_file_name is None for full_file_name in resolved_vocab_files.values()):
             raise EnvironmentError(
                 f"Can't load tokenizer for '{pretrained_model_name_or_path}'. If you were trying to load it from "
-                "'https://openmind.cn/models', make sure you don't have a local directory with "
+                "'https://modelfoundrysh.test.osinfra.cn/models', make sure you don't have a local directory with "
                 f"the same name. Otherwise, make sure '{pretrained_model_name_or_path}' is the correct path "
                 f"to a directory containing all relevant files for a {cls.__name__} tokenizer."
             )
 
         for file_id, file_path in vocab_files.items():
             if file_id not in resolved_vocab_files:
                 continue
```

## mindformers/models/tokenization_utils_fast.py

```diff
@@ -347,15 +347,14 @@
         return index
 
     def _convert_id_to_token(self, index: int) -> Optional[str]:
         return self._tokenizer.id_to_token(int(index))
 
     def _add_tokens(self, new_tokens: List[Union[str, AddedToken]], special_tokens=False) -> int:
         if special_tokens:
-            self.reset_special_tokens_cache()
             return self._tokenizer.add_special_tokens(new_tokens)
 
         return self._tokenizer.add_tokens(new_tokens)
 
     def num_special_tokens_to_add(self, pair: bool = False) -> int:
         """
         Returns the number of added tokens when encoding a sequence with special tokens.
```

## mindformers/models/utils.py

```diff
@@ -10,15 +10,14 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ============================================================================
 """Check Model Input Config."""
 import json
-import numpy as np
 import mindspore.common.dtype as mstype
 from ..version_control import get_cell_reuse
 
 CONFIG_NAME = "config.json"
 WEIGHTS_NAME = "mindspore_model.ckpt"
 WEIGHTS_INDEX_NAME = "mindspore_model.ckpt.index.json"
 FEATURE_EXTRACTOR_NAME = "preprocessor_config.json"
@@ -58,72 +57,10 @@
     try:
         json.dumps(obj)
         return True
     except TypeError:
         return False
 
 
-def check_recompute_rule(select_recompute, pp_id, layer_id, layer_list):
-    if isinstance(select_recompute, bool):
-        return select_recompute
-    if isinstance(select_recompute, (list, tuple)):
-        layer_list = np.insert(layer_list, 0, 0)
-        if layer_id < layer_list[pp_id] + select_recompute[pp_id]:
-            return True
-    return False
-
-
-def set_layer_stage_recompute(layer, layer_id, offset, parallel_config, n_layers):
-    r"""
-        Default setting for the pipeline is: `(layer_id + offset) // (layers / pipeline_stage)`.
-
-        Args:
-            layer(Cell) - Represents the transformer block
-            parallel_config(dict) - Parallel Config
-            layer_id(int) - Means the layer index for the current module, counts from zero.
-            offset(Union[int, List[int]]) - Means the layer_index needs a offset, if there are other modules in the net.
-            n_layers(int) - The total layers used for the model.
-    """
-    pp = parallel_config.pipeline_stage
-    stage_layers_list = np.array([n_layers // pp] * pp) + np.array(offset)
-    layer_list = np.array([np.sum(stage_layers_list[:i + 1]) for i in range(len(stage_layers_list))])
-    if isinstance(offset, (list, tuple)):
-        if len(offset) != pp:
-            raise ValueError(f"The length of `offset` {len(offset)} do not match `pipeline stage` {pp}.")
-        pp_id = int(np.sum(layer_list < layer_id + 1))
-        offset_layer = offset[0]
-    elif isinstance(offset, int):
-        offset_layer = offset
-        pp_dis = max(int((n_layers + 1) / pp), 1)
-        pp_id = min((layer_id + offset_layer) // pp_dis, pp - 1)
-    else:
-        raise TypeError(f"`offset` must be `int` or list/tuple of `int`, but got {type(offset)}.")
-
-    layer.pipeline_stage = pp_id
-
-    # Used for optimizer's fusion tag
-    dis = max(int((n_layers + 1) / parallel_config.gradient_aggregation_group), 1)
-    if pp > 1:
-        layer.set_comm_fusion(2)
-    else:
-        layer.set_comm_fusion(int((layer_id + offset_layer) / dis) + 1)
-    if isinstance(parallel_config.recompute, bool):
-        if parallel_config.recompute:
-            layer.recompute()
-            return
-    if parallel_config.recompute.recompute and not parallel_config.recompute.select_recompute:
-        layer.recompute(
-            recompute_slice_activation=parallel_config.recompute.recompute_slice_activation
-        )
-    else:
-        if check_recompute_rule(parallel_config.recompute.select_comm_recompute, pp_id, layer_id, layer_list):
-            if not layer.attention_norm.self_define:
-                layer.attention_norm.norm.add_prim_attr("recompute_comm_op", True)
-                layer.ffn_norm.norm.add_prim_attr("recompute_comm_op", True)
-        if check_recompute_rule(parallel_config.recompute.select_recompute, pp_id, layer_id, layer_list):
-            layer.feed_forward.mul.recompute()
-            layer.feed_forward.w1.activation.silu.recompute()
-
-
 ms_type_to_str = reverse_dict(str_to_ms_type)
 
 cell_reuse = get_cell_reuse
```

## mindformers/models/auto/auto_factory.py

```diff
@@ -18,35 +18,33 @@
 import copy
 import shutil
 import importlib
 from collections import OrderedDict
 import functools
 import types
 
+from mindformers.pet import get_pet_model, is_supported_pet_type
 from mindformers.tools.register.config import MindFormerConfig
 from mindformers.tools.logger import logger
 from mindformers.tools.utils import try_sync_file
 from mindformers.tools.hub import (
     get_class_from_dynamic_module,
     resolve_trust_remote_code,
     cached_file,
     extract_commit_hash,
 )
-from mindformers.tools.generic import experimental_mode_func_checker
 from mindformers.models.auto.configuration_auto import (
     AutoConfig,
     replace_list_option_in_docstrings,
 )
 from mindformers.models.utils import CONFIG_NAME
 from mindformers.models.configuration_utils import PretrainedConfig
 from ...mindformer_book import MindFormerBook, print_dict
-from ..build_model import build_network
+from ..build_model import build_model
 
-EXP_ERROR_MSG = "Please use AutoModel.from_pretrained(), and the input pretrained_model_name_or_dir " \
-                "should be a path to directory which has yaml file, or a model name supported, e.g. llama2_7b."
 
 CLASS_DOCSTRING = """
     This is a generic model class that will be instantiated as one of the model classes of the library when created
     with the [`~BaseAutoModelClass.from_pretrained`] class method or the [`~BaseAutoModelClass.from_config`] class
     method.
 
     This class cannot be instantiated directly using `__init__()` (throws an error).
@@ -257,29 +255,29 @@
             if pretrained_model_name_or_dir in local_model_list:
                 return False
             raise ValueError(f'\'{pretrained_model_name_or_dir}\' is not supported by \'{local_model_type}\', '
                              f'please select from {local_model_list}')
 
         local_model_names = local_model_list.keys()
         if len(pretrained_model_name_or_dir.split('_')) <= cls._model_name or \
-                not pretrained_model_name_or_dir.split('_')[cls._model_name] in local_model_names:
+            not pretrained_model_name_or_dir.split('_')[cls._model_name] in local_model_names:
             raise ValueError(f'\'{pretrained_model_name_or_dir}\' is not supported by \'{local_model_type}\', '
                              f'please select from {local_model_list}')
         local_model_name = pretrained_model_name_or_dir.split('_')[cls._model_name]
         if not pretrained_model_name_or_dir in local_model_list[local_model_name]:
             raise ValueError(f'\'{pretrained_model_name_or_dir}\' is not supported by '
                              f'\'{local_model_type}_{local_model_name}\', please select from '
                              f'{local_model_list[local_model_name]}')
         return False
 
     @classmethod
     def is_experimental_mode_from_config(cls, config):
         """Check whether AutoModel.from_config() should go into original or experimental mode."""
         if isinstance(config, PretrainedConfig):
-            model_name = config.__dict__.pop("model_name", None)
+            model_name = config.__dict__.get("model_name", None)
             if model_name is None:
                 model_name = MindFormerBook.get_model_config_to_name().get(id(config), None)
             return model_name is None
         return False
 
     @classmethod
     def from_config(cls, config, **kwargs):
@@ -357,15 +355,22 @@
         else:
             raise ValueError("config should be inherited from PretrainedConfig,"
                              " or a path to .yaml file for model config.")
 
         config_args.model.model_config.update(**kwargs)
         if not download_checkpoint:
             config_args.model.model_config.checkpoint_name_or_path = None
-        model = build_network(config_args.model)
+        ckpt_cfg = config_args.model.model_config.checkpoint_name_or_path
+        pet_config = config_args.model.model_config.pet_config
+        if pet_config and is_supported_pet_type(pet_config.pet_type):
+            config_args.model.model_config.checkpoint_name_or_path = None
+        model = build_model(config_args.model)
+        if pet_config:
+            model.config.checkpoint_name_or_path = ckpt_cfg
+            model = get_pet_model(model, pet_config)
         logger.info("model built successfully!")
         return model
 
     @classmethod
     def _inverse_parse_config(cls, config):
         """
         Inverse parse config method, which builds yaml file content for model config.
@@ -499,15 +504,15 @@
             config_list = [file for file in os.listdir(pretrained_model_name_or_dir)
                            if file == CONFIG_NAME]
             if not yaml_list and config_list:
                 return True
             return False
 
         if "/" in pretrained_model_name_or_dir and \
-                pretrained_model_name_or_dir.split("/")[0] != "mindspore":
+            pretrained_model_name_or_dir.split("/")[0] != "mindspore":
             return True
         return False
 
     @classmethod
     def from_pretrained(cls, pretrained_model_name_or_dir, *model_args, **kwargs):
         """
         From pretrain method, which instantiates a Model by pretrained model name or path.
@@ -533,15 +538,14 @@
                             f" but got {type(pretrained_model_name_or_dir)}")
 
         if cls.is_experimental_mode(pretrained_model_name_or_dir):
             return cls.from_pretrained_experimental_mode(pretrained_model_name_or_dir, *model_args, **kwargs)
         return cls.from_pretrained_origin_mode(pretrained_model_name_or_dir, **kwargs)
 
     @classmethod
-    @experimental_mode_func_checker(EXP_ERROR_MSG)
     def from_pretrained_experimental_mode(cls, pretrained_model_name_or_path, *model_args, **kwargs):
         """get models from_pretrained"""
         config = kwargs.pop("config", None)
         trust_remote_code = kwargs.pop("trust_remote_code", None)
         kwargs["_from_auto"] = True
         hub_kwargs_names = [
             "cache_dir",
@@ -652,15 +656,22 @@
 
         if not isinstance(pretrained_model_name_or_dir, str):
             raise TypeError(f"pretrained_model_name_or_dir should be a str,"
                             f" but got {type(pretrained_model_name_or_dir)}")
         config_args = cls._get_config_args(pretrained_model_name_or_dir, **kwargs)
         if not download_checkpoint:
             config_args.model.model_config.checkpoint_name_or_path = None
-        model = build_network(config_args.model)
+        ckpt_cfg = config_args.model.model_config.checkpoint_name_or_path
+        pet_config = config_args.model.model_config.pet_config
+        if pet_config and is_supported_pet_type(pet_config.pet_type):
+            config_args.model.model_config.checkpoint_name_or_path = None
+        model = build_model(config_args.model)
+        if pet_config:
+            model.config.checkpoint_name_or_path = ckpt_cfg
+            model = get_pet_model(model, pet_config)
         cls.default_checkpoint_download_path = model.default_checkpoint_download_path
 
         logger.info("model built successfully!")
         return model
 
     @classmethod
     def register(cls, config_class, model_class, exist_ok=False):
@@ -714,29 +725,26 @@
     # Now we need to copy and re-register `from_config` and `from_pretrained` as class methods otherwise we can't
     # have a specific docstrings for them.
     from_config = copy_func(_BaseAutoModelClass.from_config)
     from_config_docstring = insert_head_doc(FROM_CONFIG_DOCSTRING, head_doc=head_doc)
     from_config_docstring = from_config_docstring.replace("BaseAutoModelClass", name)
     from_config_docstring = from_config_docstring.replace("checkpoint_placeholder", checkpoint_for_example)
     from_config.__doc__ = from_config_docstring
-    from_config = replace_list_option_in_docstrings(
-        model_mapping._model_mapping,  # pylint: disable=W0212
-        use_model_types=False)(from_config)
+    from_config = replace_list_option_in_docstrings(model_mapping._model_mapping, use_model_types=False)(from_config)  # pylint: disable=W0212
     cls.from_config = classmethod(from_config)
 
     from_pretrained_docstring = FROM_PRETRAINED_MINDFORMERS_DOCSTRING
     from_pretrained = copy_func(_BaseAutoModelClass.from_pretrained)
     from_pretrained_docstring = insert_head_doc(from_pretrained_docstring, head_doc=head_doc)
     from_pretrained_docstring = from_pretrained_docstring.replace("BaseAutoModelClass", name)
     from_pretrained_docstring = from_pretrained_docstring.replace("checkpoint_placeholder", checkpoint_for_example)
     shortcut = checkpoint_for_example.split("/")[-1].split("-")[0]
     from_pretrained_docstring = from_pretrained_docstring.replace("shortcut_placeholder", shortcut)
     from_pretrained.__doc__ = from_pretrained_docstring
-    from_pretrained = replace_list_option_in_docstrings(model_mapping._model_mapping)(  # pylint: disable=W0212
-        from_pretrained)
+    from_pretrained = replace_list_option_in_docstrings(model_mapping._model_mapping)(from_pretrained)  # pylint: disable=W0212
     cls.from_pretrained = classmethod(from_pretrained)
     return cls
 
 
 def get_values(model_mapping):
     result = []
     for model in model_mapping.values():
```

## mindformers/models/auto/configuration_auto.py

```diff
@@ -66,16 +66,14 @@
         ("sam", "SamModel"),
         ("swin", "SwinModel"),
         ("t5", "T5ForConditionalGeneration"),
         ("vit", "ViTModel")
     ]
 )
 
-EXP_ERROR_MSG = "The input yaml_name_or_path should be a path to yaml file, e.g. " \
-                "'run_xxx_model.yaml', or a model name supported, e.g. llama2_7b."
 
 def config_class_to_model_type(config):
     """Converts a config class name to the corresponding model type"""
     for key, cls in CONFIG_MAPPING_NAMES.items():
         if cls == config:
             return key
     # if key not found check in extra content
@@ -361,15 +359,15 @@
         config_args.model.model_config.update(**kwargs)
         config = build_model_config(config_args.model.model_config)
         MindFormerBook.set_model_config_to_name(id(config), config_args.model.arch.type)
 
         return config
 
     @classmethod
-    @experimental_mode_func_checker(EXP_ERROR_MSG)
+    @experimental_mode_func_checker()
     def get_config_experimental_mode(cls, pretrained_model_name_or_path, **kwargs):
         """Get config object by from_pretrained with experimental mode
 
         :param pretrained_model_name_or_path: model file name or corresponding path
         :return: config object
         """
         use_auth_token = kwargs.pop("use_auth_token", None)
```

## mindformers/models/auto/image_processing_auto.py

```diff
@@ -24,19 +24,16 @@
 # Build the list of all image processors
 from mindformers.tools import get_file_from_repo, get_class_from_dynamic_module, resolve_trust_remote_code
 from mindformers.models.utils import CONFIG_NAME, IMAGE_PROCESSOR_NAME
 from mindformers.tools import logger
 from mindformers.models.configuration_utils import PretrainedConfig
 from mindformers.models.image_processing_utils import ImageProcessingMixin
 from mindformers.models.auto.auto_factory import _LazyAutoMapping
-from mindformers.tools.generic import experimental_mode_func_checker
-
 from .configuration_auto import CONFIG_MAPPING_NAMES, AutoConfig
 
-EXP_ERROR_MSG = "AutoImageProcessor is in experimental, AutoProcessor is recommended."
 
 IMAGE_PROCESSOR_MAPPING_NAMES = OrderedDict(
     [
         ("blip2", "Blip2ImageProcessor"),
         ("clip", "CLIPImageProcessor"),
         ("mae", "ViTMAEImageProcessor"),
         ("sam", "SamImageProcessor"),
@@ -173,15 +170,14 @@
     def __init__(self):
         raise EnvironmentError(
             "AutoImageProcessor is designed to be instantiated "
             "using the `AutoImageProcessor.from_pretrained(pretrained_model_name_or_path)` method."
         )
 
     @classmethod
-    @experimental_mode_func_checker(EXP_ERROR_MSG)
     def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):
         r"""
         Instantiate one of the image processor classes of the library from a pretrained model vocabulary.
 
         The image processor class to instantiate is selected based on the `model_type` property of the config object
         (either passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's
         missing, by falling back to using pattern matching on `pretrained_model_name_or_path`:
```

## mindformers/models/auto/modeling_auto.py

```diff
@@ -95,15 +95,15 @@
         ("vit", "ViTForImageClassification"),
         ("blip2", "Blip2Classifier")
     ]
 )
 
 MODEL_FOR_VISION_2_SEQ_MAPPING_NAMES = OrderedDict(
     [
-        ("blip2", "Blip2ImageToTextGeneration"),
+        ("blip2", "Blip2ForConditionalGeneration"),
     ]
 )
 
 MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES = OrderedDict(
     [
         # Model for Seq2Seq Causal LM mapping
         ("t5", "T5ForConditionalGeneration"),
```

## mindformers/models/auto/processing_auto.py

```diff
@@ -33,17 +33,14 @@
 from ...tools.generic import experimental_mode_func_checker
 from ...tools import get_class_from_dynamic_module, resolve_trust_remote_code, logger
 from ...tools.register.config import MindFormerConfig
 from ...mindformer_book import MindFormerBook, print_dict
 from ..build_processor import build_processor
 
 
-EXP_ERROR_MSG = "The input yaml_name_or_path should be a path to yaml file, or a " \
-                "path to directory which has yaml file, or a model name supported, e.g. llama2_7b."
-
 PROCESSOR_MAPPING_NAMES = OrderedDict(
     [
         ("bert", "BertProcessor"),
         ("blip2", "Blip2Processor"),
         ("bloom", "BloomProcessor"),
         ("clip", "CLIPProcessor"),
         ("glm", "GLMProcessor"),
@@ -64,17 +61,17 @@
 def is_experimental_mode(path):
     """Check if the experimental is used based on yaml name or path"""
     experimental_mode = False
     is_exists = os.path.exists(path)
     is_dir = os.path.isdir(path)
     if is_exists:
         if is_dir:
-            yaml_list = [file for file in os.listdir(path)
-                         if file.endswith(".yaml")]
-            if not yaml_list:
+            experimental_mode = True
+        else:  # file
+            if not (path.endswith(".yaml") or path.endswith(".yml")):
                 experimental_mode = True
     else:  # repo
         if "/" in path and path.split("/")[0] != "mindspore":
             experimental_mode = True
 
     return experimental_mode
 
@@ -263,15 +260,15 @@
         if not os.path.isdir(lib_path):
             lib_path = None
         processor = build_processor(config_args.processor, lib_path=lib_path)
         logger.info("processor built successfully!")
         return processor
 
     @classmethod
-    @experimental_mode_func_checker(EXP_ERROR_MSG)
+    @experimental_mode_func_checker()
     def from_pretrained_experimental(cls, pretrained_model_name_or_path, **kwargs):
         """Experimental features."""
         config = kwargs.pop("config", None)
         trust_remote_code = kwargs.pop("trust_remote_code", None)
         kwargs["_from_auto"] = True
 
         processor_class = None
```

## mindformers/models/auto/tokenization_auto.py

```diff
@@ -19,15 +19,14 @@
 import importlib
 import json
 import os
 import warnings
 import shutil
 from collections import OrderedDict
 from typing import Dict, Optional, Union
-from mindformers.tools.generic import experimental_mode_func_checker
 from ..tokenization_utils import PreTrainedTokenizer
 from ..tokenization_utils_base import TOKENIZER_CONFIG_FILE
 from ...tools import (
     cached_file,
     extract_commit_hash,
 )
 from ...tools.hub import get_class_from_dynamic_module, resolve_trust_remote_code
@@ -40,16 +39,14 @@
     config_class_to_model_type
 )
 from .auto_factory import _LazyAutoMapping
 from ...mindformer_book import MindFormerBook, print_dict
 
 TOKENIZER_SUPPORT_LIST = MindFormerBook.get_tokenizer_support_list()
 
-EXP_ERROR_MSG = "The input yaml_name_or_path should be a path to directory which has " \
-                "yaml file, or a model name supported, e.g. llama2_7b."
 
 def is_experimental_mode(path):
     """Check whether AutoTokenizer.from_pretrained() should go into original or experimental mode
 
     :param path: (str) path to AutoTokenizer.from_pretrained()
     :return: (bool) whether AutoTokenizer.from_pretrained() should go into original or experimental mode
     """
@@ -410,15 +407,14 @@
 
         dynamic_class = MindFormerRegister.get_cls(module_type='tokenizer', class_name=class_name)
         instanced_class = dynamic_class.from_pretrained(yaml_name_or_path, **kwargs)
         logger.info("%s Tokenizer built successfully!", instanced_class.__class__.__name__)
         return instanced_class
 
     @classmethod
-    @experimental_mode_func_checker(EXP_ERROR_MSG)
     def get_class_from_experimental_mode(cls, pretrained_model_name_or_path, *inputs, **kwargs):
         r"""
         Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary.
 
         The tokenizer class to instantiate is selected based on the `model_type` property of the config object (either
         passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by
         falling back to using pattern matching on `pretrained_model_name_or_path`:
```

## mindformers/models/blip2/blip2_config.py

```diff
@@ -94,25 +94,22 @@
                  softmax_compute_type: str = "float32",
                  vision_config: Union[dict, ViTConfig] = ViTConfig(),
                  qformer_config: Union[dict, QFormerConfig] = QFormerConfig(),
                  text_config: Optional[LlamaConfig] = None,
                  parallel_config: Union[dict, TransformerOpParallelConfig] = default_transformer_config,
                  is_training: bool = True,
                  micro_batch_interleave_num=1,
-                 use_past=False,
                  **kwargs):
         super(Blip2Config, self).__init__(**kwargs)
         if isinstance(parallel_config, dict):
             parallel_config = TransformerOpParallelConfig(**parallel_config)
         if isinstance(qformer_config, dict):
             qformer_config = QFormerConfig(**qformer_config)
         if isinstance(vision_config, dict):
             vision_config = ViTConfig(**vision_config)
-        if isinstance(text_config, dict):
-            text_config = LlamaConfig(**text_config)
         self.batch_size = batch_size
         self.freeze_vision = freeze_vision
         self.freeze_text = freeze_text
         self.max_txt_len = max_txt_len
         self.checkpoint_name_or_path = checkpoint_name_or_path
         self.prompt = prompt
         self.prompt_length = prompt_length
@@ -149,8 +146,7 @@
         self.qformer_config.dtype = self.dtype
 
         self.vision_config.parallel_config = parallel_config
         self.vision_config.compute_dtype = self.compute_dtype
         self.vision_config.layernorm_compute_type = self.layernorm_compute_type
         self.vision_config.softmax_compute_type = self.softmax_compute_type
         self.vision_config.dtype = self.dtype
-        self.use_past = use_past
```

## mindformers/models/blip2/blip2_itm_evaluator.py

```diff
@@ -111,15 +111,15 @@
     def __init__(self, blip2_qformer: Blip2Qformer, **kwargs):
         super(Blip2TextForwarder, self).__init__(**kwargs)
         self.qformer = blip2_qformer.qformer
         self.text_proj = blip2_qformer.text_proj
         self.pad_token_id = blip2_qformer.pad_token_id
         self.not_equal = P.NotEqual()
         self.cast = P.Cast()
-        self.normalize = ops.L2Normalize(axis=-1, epsilon=1e-7)
+        self.normalize = ops.L2Normalize(axis=-1, epsilon=1e-12)
 
     def construct(self, text_input_ids):
         """ forawrd text_ids through the bert model.
 
         Args:
             text_input_ids (Tensor): input text_ids
 
@@ -146,15 +146,15 @@
         self.qformer = blip2_qformer.qformer
         self.ln_vision = blip2_qformer.ln_vision
         self.visual_encoder = blip2_qformer.visual_encoder
         self.vision_proj = blip2_qformer.vision_proj
         self.query_tokens = blip2_qformer.query_tokens
         self.ones = ops.ones
         self.broadcast_to = ops.broadcast_to
-        self.normalize = ops.L2Normalize(axis=-1, epsilon=1e-7)
+        self.normalize = ops.L2Normalize(axis=-1, epsilon=1e-12)
 
     def construct(self, image):
         """ forawrd image through vit and the bert model.
 
         Args:
             image (Tensor): input image
```

## mindformers/models/blip2/blip2_llama.py

```diff
@@ -42,36 +42,46 @@
         """
 
     def __init__(self, config):
         super().__init__(config)
 
     # pylint: disable=W0221
     def construct(self, input_embeddings: Tensor, input_attention_masks: Tensor, batch_valid_length=None,
-                  batch_index=None, zactivate_len=None, block_tables=None, slot_mapping=None):
+                  batch_index=None, zactivate_len=None):
         """
         Forward of llama model with concat image and text embeddings
         """
         bs, seq_len, _ = self.shape(input_embeddings)
-        mask = None
-        if self.use_past:
+        if not self.use_past:
+            freqs_cis = self.freqs_mgr()
+            mask = self.casual_mask(masks=input_attention_masks)  # mask: [bs, seq, seq]
+            kvcache_inputs = None
+        else:
             if self.is_first_iteration:
-                freqs_cis = self.freqs_mgr.prefill(bs, seq_len)
+                freqs_cis = self.freqs_mgr(seq_len)
+                mask = self.casual_mask(masks=input_attention_masks)  # mask: [bs, seq, seq]
             else:
-                freqs_cis = self.freqs_mgr.increment(batch_valid_length)
-        else:
-            freqs_cis = self.freqs_mgr(seq_len)
-            mask = self.casual_mask(masks=input_attention_masks)  # mask: [bs, seq, seq]
+                freqs_cis = self.freqs_mgr.increment(batch_valid_length, bs)
+                if self.is_dynamic and self.is_flexible_shape and not self.use_kvcache_op:
+                    mask = self.casual_mask.increment_slice(
+                        self.kvcache_preprocess.range,
+                        self.kvcache_preprocess.max_cache_length // bs,
+                        batch_valid_length,
+                        zactivate_len)
+                else:
+                    mask = self.casual_mask.increment(self.kvcache_preprocess.range, batch_valid_length, zactivate_len)
+
+            kvcache_inputs = self.kvcache_preprocess(bs, batch_valid_length, batch_index, zactivate_len)
 
         # tokens: [bs, seq/1]
         h = input_embeddings
         h = self.reshape(h, (bs, seq_len, self.hidden_size))
         # h: [bs, seq/1, hidden_dim]
         for i in range(self.num_layers):
-            h = self.layers[i](h, freqs_cis, mask, batch_valid_length=batch_valid_length, block_tables=block_tables,
-                               slot_mapping=slot_mapping)
+            h = self.layers[i](h, freqs_cis, mask, kvcache_inputs=kvcache_inputs)
         output = self.norm_out(h)
         return output
 
 
 @MindFormerRegister.register(MindFormerModuleType.MODELS)
 class LlamaForBlip2(LlamaForCausalLM, ImageTextEmbeddingPreparationMixIn):
     r"""
@@ -133,17 +143,15 @@
                   input_ids=None,
                   labels=None,
                   input_position=None,
                   attention_mask=None,
                   init_reset=True,
                   batch_valid_length=None,
                   batch_index=None,
-                  zactivate_len=None,
-                  block_tables=None,
-                  slot_mapping=None):
+                  zactivate_len=None):
         """LlamaForBlip2 forward."""
         if input_embeddings is None and input_ids is not None:  # for incremental infer
             input_embeddings = self.model.tok_embeddings(input_ids)
 
         bsz, seqlen, _ = self.shape(input_embeddings)
         if self.use_past:
             if not isinstance(batch_valid_length, Tensor):
@@ -152,19 +160,15 @@
         if batch_valid_length is not None:
             batch_valid_length = self.reshape(batch_valid_length, (-1,))
         if not self.is_first_iteration:
             batch_valid_length = self.sub_batch_valid_len(batch_valid_length, 1)
 
         output = self.model(input_embeddings=input_embeddings,
                             input_attention_masks=attention_mask,
-                            batch_valid_length=batch_valid_length,
-                            batch_index=batch_index,
-                            zactivate_len=zactivate_len,
-                            block_tables=block_tables,
-                            slot_mapping=slot_mapping)
+                            batch_valid_length=batch_valid_length)
 
         pre_gather = (not self.use_past or self.is_first_iteration) and batch_valid_length is not None
         if pre_gather:
             output = self.gather(output, self.sub_batch_valid_len(batch_valid_length, 1), 1)
 
         logits = self.lm_head(output)
         logits = self.cast(logits, mstype.float32)
```

## mindformers/models/bloom/bloom.py

```diff
@@ -8,33 +8,31 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ============================================================================
+
 """Bloom model"""
 import copy
 import os
 import numpy as np
-
 import mindspore.common.dtype as mstype
 from mindspore import nn
 from mindspore import Tensor
 from mindspore.common.initializer import initializer
 from mindspore.ops import operations as P
-
-from mindformers.models.modeling_utils import PreTrainedModel
 from mindformers.modules.transformer import VocabEmbedding
-from mindformers.modules.layers import LayerNorm, AlibiTensor
+from mindformers.modules.layers import LayerNorm, AlibiTensor, AlibiTensorV2
 from mindformers.core.loss import CrossEntropyLoss
+from mindformers.models.modeling_utils import PreTrainedModel
 from mindformers.tools.logger import logger
 from mindformers.tools.register import MindFormerRegister, MindFormerModuleType
 from mindformers.mindformer_book import MindFormerBook
-
 from .layers import BloomBlocks, CausalMask
 from .bloom_config import BloomConfig
 from ..utils import convert_mstype, cell_reuse
 
 
 class BloomPreTrainedModel(PreTrainedModel):
     """
@@ -44,28 +42,25 @@
 
     config_class = BloomConfig
     base_model_prefix = "bloom"
 
 
 def jit_inference_with_condition():
     """allow jit inference"""
-
     def decorator(func):
         if os.getenv("JIT_INFERENCE", "NOT_FOUND") == "NOT_FOUND":
             return func
         from mindspore import jit, JitConfig
         dec = jit(jit_config=JitConfig(jit_level="O2"))
         return dec(func)
-
     return decorator
 
 
 class BloomEmbeddingLayer(nn.Cell):
     """The Embedding Layer of Bloom network."""
-
     def __init__(self, config=None):
         super(BloomEmbeddingLayer, self).__init__(auto_prefix=False)
         parallel_config = copy.deepcopy(config.parallel_config)
         embedding_mp = config.parallel_config.embedding_dp_mp_config.model_parallel
         vocab_size = config.vocab_size
         if vocab_size % embedding_mp != 0:
             logger.warning("The vocab size of embedding layer is: %s, it is not divide by model_parallel: %s",
@@ -142,17 +137,21 @@
 
         self.embedding = BloomEmbeddingLayer(config)
         self.embedding.pipeline_stage = 0
 
         self.make_causal_attention = CausalMask(seq_length=config.seq_length,
                                                 parallel_config=config.parallel_config.dp_mp_config)
 
-        self.build_alibi_tensor = AlibiTensor(seq_length=config.seq_length,
-                                              num_heads=config.num_heads,
-                                              parallel_config=config.parallel_config)
+        if config.use_past:
+            self.build_alibi_tensor = AlibiTensor(seq_length=config.seq_length,
+                                                  num_heads=config.num_heads,
+                                                  parallel_config=config.parallel_config)
+        else:
+            self.build_alibi_tensor = AlibiTensorV2(seq_length=config.seq_length,
+                                                    num_heads=config.num_heads)
 
         self.blocks = BloomBlocks(hidden_size=config.hidden_size,
                                   batch_size=config.batch_size,
                                   ffn_hidden_size=config.hidden_size * config.expand_ratio,
                                   seq_length=config.seq_length,
                                   num_layers=config.num_layers,
                                   num_heads=config.num_heads,
@@ -178,41 +177,37 @@
         self.ln_f.pipeline_stage = config.parallel_config.pipeline_stage - 1
 
         self.use_past = config.use_past
         self.dtype = convert_mstype(config.param_init_type)
         self.mul_init_reset = P.Mul().shard(
             ((config.parallel_config.data_parallel, config.parallel_config.model_parallel, 1, 1), (1,)))
 
-        self.cast = P.Cast()
-        self.reshape = P.Reshape()
-
     def construct(self, input_ids, input_mask, init_reset=True, batch_valid_length=None):
         """Bloom model"""
+
         input_embedding, embedding_table = self.embedding(input_ids)
         hidden_states = input_embedding
         hidden_states_shape = hidden_states.shape
-        hidden_states = self.reshape(hidden_states, (-1, hidden_states_shape[-1]))
+        hidden_states = hidden_states.reshape((-1, hidden_states_shape[-1]))
 
         causal_mask = self.make_causal_attention(input_mask)
         alibi_tensor = self.build_alibi_tensor(input_mask, hidden_states.dtype)
 
         if self.use_past:
-            init_reset = self.cast(init_reset, self.dtype)
-            init_reset = self.mul_init_reset(self.blocks[0].key_past, init_reset)
+            init_reset = self.mul_init_reset(self.blocks[0].key_past, init_reset.astype(self.dtype))
         for i in range(self.num_layers):
             hidden_states, _ = self.blocks[i](hidden_states, alibi_tensor, causal_mask, init_reset, batch_valid_length)
-        hidden_states = self.reshape(hidden_states, hidden_states_shape)
+        hidden_states = hidden_states.reshape(hidden_states_shape)
         output_state = self.ln_f(hidden_states)
 
         return output_state, embedding_table
 
 
 class BloomHead(nn.Cell):
     """Head for Bloom to get the logits of each token in the vocab."""
-
     def __init__(self,
                  hidden_size,
                  vocab_size,
                  compute_type="float16",
                  parallel_config=None):
         super(BloomHead, self).__init__()
         copied_parallel_config = copy.deepcopy(parallel_config)
@@ -300,22 +295,14 @@
         self.load_checkpoint(config)
 
     def prepare_inputs_for_generation(self, input_ids, **kwargs):
         return {
             "input_ids": Tensor(input_ids, mstype.int32)
         }
 
-    def add_flags_custom(self, is_first_iteration):
-        """Add customized attributes for specific cells in the model."""
-        self.add_flags(is_first_iteration=is_first_iteration)
-        self.transformer.add_flags(is_first_iteration=is_first_iteration)
-        for layer in self.transformer.blocks:
-            layer.add_flags(is_first_iteration=is_first_iteration)
-            layer.attention.add_flags(is_first_iteration=is_first_iteration)
-
     # pylint: disable=W0613
     @jit_inference_with_condition()
     def construct(self, input_ids, input_position=None, position_ids=None, attention_mask=None,
                   input_embeds=None, labels=None, init_reset=True, batch_valid_length=None):
         """
         construct function for Language Modeling
```

## mindformers/models/bloom/layers.py

```diff
@@ -8,40 +8,38 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ============================================================================
+
 """Bloom custom layers"""
 import math
 import numpy as np
-
 from mindspore.common.tensor import Tensor
-from mindspore import nn, Parameter
+from mindspore import nn, ops, Parameter
 import mindspore.common.dtype as mstype
-from mindspore.ops import functional as F
 from mindspore.ops import operations as P
 from mindspore.context import ParallelMode
 from mindspore.parallel._utils import _get_parallel_mode, _is_sharding_propagation
 
 from mindformers.modules import AttentionMask
 from mindformers.modules.flash_attention import FlashAttention
-from mindformers.modules.transformer import MultiHeadAttention, TransformerEncoderLayer, TransformerEncoder
 from mindformers.modules.transformer.op_parallel_config import default_dpmp_config
 from mindformers.modules.transformer.moe import default_moe_config
+from mindformers.modules.transformer import MultiHeadAttention,\
+                                            TransformerEncoderLayer, TransformerEncoder
 from mindformers.modules.transformer.transformer import default_transformer_config, _get_lambda_func
 from mindformers.version_control import check_valid_flash_attention, choose_flash_attention_dtype
 
 from mindformers.tools.logger import logger
 
-
 class BloomAttention(MultiHeadAttention):
     r"""The implementation of Bloom attention."""
-
     def __init__(self, batch_size,
                  src_seq_length,
                  tgt_seq_length,
                  hidden_size,
                  num_heads,
                  hidden_dropout_rate=0.1,
                  attention_dropout_rate=0.1,
@@ -72,15 +70,15 @@
             self.beta = Tensor([1.0])
         else:
             self.add_alibi = P.Add().shard(
                 ((parallel_config.data_parallel, parallel_config.model_parallel, 1, 1),
                  (parallel_config.data_parallel, parallel_config.model_parallel, 1, 1)))
             self.mul_alibi = P.Mul().shard(
                 ((parallel_config.data_parallel, parallel_config.model_parallel, 1, 1), (1,)))
-            self.mul_score = P.Mul().shard(
+            self.mul_alibi1 = P.Mul().shard(
                 ((parallel_config.data_parallel, parallel_config.model_parallel, 1, 1), (1,)))
             # [B, 1, S, S] + [B, H, S, S]
             self.add_atten_score = P.Add().shard(
                 ((parallel_config.data_parallel, 1, 1, 1),
                  (parallel_config.data_parallel, parallel_config.model_parallel, 1, 1)))
             # [B, H, S, D] * [B, 1, S, 1]
             self.mul_key = P.Mul().shard(
@@ -119,89 +117,85 @@
                                                       use_attention_mask=True)
             if use_select_recompute:
                 logger.info("Using select recompute mode!")
                 self.cast.recompute()
                 self.batch_matmul.recompute()
                 self.batch_matmul_trans_b.recompute()
                 self.sub.recompute()
+                self.mul_alibi1.recompute()
                 self.add.recompute()
                 self.add_alibi.recompute()
                 self.prob_dropout.recompute()
                 self.softmax.softmax.recompute()
                 self.softmax_3d.recompute()
             if use_seq_parallel:
                 logger.info("Using seq parallel mode!")
                 self.dropout.dropout.shard(((parallel_config.data_parallel * parallel_config.model_parallel, 1),))
                 self.projection.shard(
                     strategy_bias=((parallel_config.data_parallel * parallel_config.model_parallel, 1), (1,)),
                     strategy_matmul=((parallel_config.data_parallel, parallel_config.model_parallel),
                                      (parallel_config.model_parallel, 1)),
                     out_strategy_matmul=((parallel_config.data_parallel * parallel_config.model_parallel, 1),))
-        self.sub_attention = P.Sub().shard(((1,), (1, 1, 1, 1)))
-        self.tile = P.Tile()
-        self.reshape = P.Reshape()
-
+        self.sub_attention = ops.Sub().shard(((1,), (1, 1, 1, 1)))
     # pylint: disable=arguments-differ
     def construct(self, query_tensor, key_tensor, value_tensor, alibi_tensor, attention_mask,
                   key_past=None, value_past=None, batch_valid_length=None):
         """Forward process of the BloomAttention"""
         self._check_inputs(query_tensor, key_tensor, value_tensor, attention_mask, key_past,
                            value_past, batch_valid_length)
-        ori_shape = F.shape(query_tensor)
+        ori_shape = query_tensor.shape
         batch_size = self._get_batch_size_from_query(query_tensor)
         query_tensor, key_tensor, value_tensor = self._convert_to_2d_tensor(query_tensor,
                                                                             key_tensor,
                                                                             value_tensor)
-        ori_dtype = F.dtype(query_tensor)
-        query_tensor = self.cast(query_tensor, self.dtype)
-        key_tensor = self.cast(key_tensor, self.dtype)
-        value_tensor = self.cast(value_tensor, self.dtype)
+        ori_dtype = query_tensor.dtype
+        query_tensor = query_tensor.astype(self.dtype)
+        key_tensor = key_tensor.astype(self.dtype)
+        value_tensor = value_tensor.astype(self.dtype)
         # multi head attention: query, key, value are derived from the same inputs
         query = self.dense1(query_tensor)
         key = self.dense2(key_tensor)
         value = self.dense3(value_tensor)
-
-        query = self.reshape(
-            query,
-            (batch_size, self._get_seq_length_under_incremental(self.src_seq_length), self.n_head, self.size_per_head))
-        query = self.transpose(query, (0, 2, 1, 3))  # the returned shape is [bs, num_heads, seq_length, size_per_head]
-        key = self.reshape(
-            key,
-            (batch_size, self._get_seq_length_under_incremental(self.tgt_seq_length), self.n_head, self.size_per_head)
-        )
-        key = self.transpose(key, (0, 2, 1, 3))  # the returned shape is [bs, num_heads, size_per_head, seq_length]
-        value = self.reshape(
-            value,
-            (batch_size, self._get_seq_length_under_incremental(self.tgt_seq_length), self.n_head, self.size_per_head)
-        )
-        value = self.transpose(value, (0, 2, 1, 3))  # the returned shape is [bs, num_heads, seq_length, size_per_head]
-
+        # the returned shape is [bs, num_heads, seq_length, size_per_head]
+        query = self.transpose(
+            query.reshape((batch_size, self._get_seq_length_under_incremental(self.src_seq_length),
+                           self.n_head, self.size_per_head)), (0, 2, 1, 3))
+        # return query, query
+        # the returned shape is [bs, num_heads, size_per_head, seq_length]
+        key = self.transpose(
+            key.reshape((batch_size, self._get_seq_length_under_incremental(self.tgt_seq_length),
+                         self.n_head, self.size_per_head)), (0, 2, 1, 3))
+
+        # the returned shape is [bs, num_heads, seq_length, size_per_head]
+        value = self.transpose(
+            value.reshape((batch_size, self._get_seq_length_under_incremental(self.tgt_seq_length),
+                           self.n_head, self.size_per_head)), (0, 2, 1, 3))
         # support input shape is [bs, seq, seq] or [bs, heads, seq, seq]
         if attention_mask is not None and attention_mask.ndim == 3:
             # expand attention mask from [bs, seq, seq] -> [bs, 1, seq, seq]
             attention_mask = self.expand_dims(attention_mask, 1)
         # key and value for current token(s)
         key_present = key
         value_present = value
         if self.use_past:
             # The first graph with the input size of (bs, seq_length)
             if self.is_first_iteration:
                 # Get the valid input length without padding
-                valid_length_vector = self.cast((self.less(self.range, batch_valid_length.view(-1, 1, 1))), self.dtype)
+                valid_length_vector = (self.less(self.range, batch_valid_length.view(-1, 1, 1))).astype(self.dtype)
                 # Cover the key and value numbers corresponding to the padding position
                 key_present = self.mul_key(key, self.expand_dims(valid_length_vector, 3))
                 value_present = self.mul_value(value, self.expand_dims(valid_length_vector, 3))
-            # The second graph with the input size of (bs, 1)
+            # The second graph with the inpus size of (bs, 1)
             # the shape of query is (bs, num_heads, 1, size_per_head)
             # the shape of key is   (bs, num_heads, size_per_head, 1)
             # the shape of value is (bs, num_heads, 1, size_per_head)
             else:
                 current_index = batch_valid_length - 1
-                current_index = self.reshape(current_index, (-1, 1, 1))
-                current_mask = self.cast((self.equal(self.range, current_index)), self.dtype)
+                current_index = current_index.reshape((-1, 1, 1))
+                current_mask = (self.equal(self.range, current_index)).astype(self.dtype)
                 # Pad the key and value to seq_length with only the position index not zero
                 current_key = self.mul_key(key, self.expand_dims(current_mask, 3))
                 current_value = self.mul_value(value, self.expand_dims(current_mask, 3))
                 # Concat the previous saved state and current state
                 key = self.add_key(key_past, current_key)
                 value = self.add_value(value_past, current_value)
                 # Update key_present and value_present for state update
@@ -210,94 +204,94 @@
 
         layer_present = (key_present, value_present)
         # multi head attention considering attention mask
         # the return shape is [bs * seq_length, hidden_size]
         if self.use_flash_attention:
             attention_mask = self.sub_attention(Tensor((1.0,), mstype.float16), attention_mask)
             attention_mask = self.cast(attention_mask, mstype.uint8)
-            # repeat alibi tensor shape as attention_mask
-            attention_mask_dim = F.shape(attention_mask)[-1]
-            alibi_tensor = self.tile(alibi_tensor, (1, 1, attention_mask_dim, 1))
             attention = self.flash_attention(query, key, value, attention_mask, alibi_tensor)
             attention = self._merge_heads(attention)
         else:
             attention = self._attn(query, key, value, alibi_tensor, attention_mask, batch_valid_length)
 
         # Output
         output = self.projection(attention)
         output = self.dropout(output)
-        output = self.reshape(output, ori_shape)
-        output = self.cast(output, ori_dtype)
+        output = output.reshape(ori_shape)
+        output = output.astype(ori_dtype)
         return output, layer_present
 
     def _softmax(self, attention_scores):
         """
         :param attention_scores: a 3d tensor before softmax
         :return: the attention scores.
         """
+
         attention_probs = self.softmax(attention_scores)
+
         return attention_probs
 
-    def _attn(self, query, key, value, alibi_tensor, attention_mask, batch_valid_length=None):
+    def _attn(self, query, key, value, alibi_tensor, attention_mask, batch_valid_length):
         """
         Get the weighted score along the seq_length
 
         Inputs:
             query: the query matrix
             key: the key matrix
             value: the value matrix
             alibi_tensor: the alibi matrix
             attention_mask: the attention mask matrix with shape (batch_size,
             1, seq_length, seq_length)
         Outputs:
             weighted_values: Tensor, the weighted sum scores
         """
-        ori_dtype = F.dtype(query)
-        score = self.batch_matmul_trans_b(self.cast(query, self.dtype), self.cast(key, self.dtype))
+        # Normalize query and key before MatMul, default off
+        # Attention score [bs, num_heads, seq_length, seq_length]
+        ori_dtype = query.dtype
+        score = self.batch_matmul_trans_b(query.astype(self.dtype), key.astype(self.dtype))
         score = self.add_alibi(
-            self.mul_score(score, self.cast(self.inv_norm_factor, ori_dtype)),
-            self.mul_alibi(alibi_tensor, self.cast(self.beta, ori_dtype))
-        )
+            self.mul_alibi1(score, self.inv_norm_factor.astype(ori_dtype)),
+            self.mul_alibi(alibi_tensor, self.beta.astype(ori_dtype))
+            )
         attention_scores = self.cast(score, self.softmax_dtype)
         # for input size of (bs, 1) namely the second graph,
         # the shape of attention_mask matrix should be (bs, 1, 1, seq_length)
         if attention_mask is not None:
             if self.use_past and not self.is_first_iteration:
                 # Calculate the current total token
                 # bs, 1 -> bs, 1, 1
                 batch_valid_length = batch_valid_length - 1
-                index = self.reshape(batch_valid_length, (-1, 1, 1))
+                index = batch_valid_length.reshape((-1, 1, 1))
                 # Calculate the attention_mask matrix via the position index
-                attention_mask = self.cast((self.tensor_le(self.range, index)), mstype.int32)
+                attention_mask = (self.tensor_le(self.range, index)).astype(mstype.int32)
                 attention_mask = self.expand_dims(attention_mask, 2)
             # Minus 10000 for the position where masked to exclude them from softmax
-            multiple_out = self.sub(
-                self.cast(Tensor((1.0,)), F.dtype(attention_scores)),
-                self.cast(attention_mask, F.dtype(attention_scores)))
+            multiplu_out = self.sub(
+                Tensor((1.0,)).astype(attention_scores.dtype),
+                attention_mask.astype(attention_scores.dtype))
 
-            adder = self.mul(multiple_out, self.multiply_data)
+            adder = self.mul(multiplu_out, self.multiply_data)
             attention_scores = self.add_atten_score(adder, attention_scores)
 
         # attention probs
         attention_probs = self._softmax(attention_scores)
         attention_probs = self.cast(attention_probs, ori_dtype)
 
         attention_probs = self.prob_dropout(attention_probs)
         # Weighted sum output [bs, num_heads, seq_length, size_per_head]
 
-        weighted_values = self.batch_matmul(self.cast(attention_probs, self.dtype),
-                                            self.cast(value, self.dtype))
-        weighted_values = self.cast(weighted_values, self.softmax_dtype)
+        weighted_values = self.batch_matmul(attention_probs.astype(self.dtype),
+                                            value.astype(self.dtype))
+        weighted_values = weighted_values.astype(self.softmax_dtype)
         attention_merge = self._merge_heads(weighted_values)
         return attention_merge
 
 
 class BloomBlock(TransformerEncoderLayer):
     r"""A block of Bloom model."""
-
     def __init__(self,
                  batch_size,
                  hidden_size,
                  ffn_hidden_size,
                  num_heads,
                  seq_length,
                  attention_dropout_rate=0.1,
@@ -364,105 +358,103 @@
                                             use_select_recompute=use_select_recompute,
                                             use_flash_attention=use_flash_attention,
                                             parallel_config=attention_parallel_config)
         if self.use_past:
             size_per_head = hidden_size // num_heads
             self.key_shape = (batch_size, num_heads, seq_length, size_per_head)
             self.key_past = Parameter(Tensor(np.zeros(shape=self.key_shape), self.dtype), name="key_past")
-
             # [B, H, S, D]
-            self.assign_past = P.Assign().shard(
+            self.assign_key_past = P.Assign().shard(
+                ((parallel_config.data_parallel, parallel_config.model_parallel, 1, 1),
+                 (parallel_config.data_parallel, parallel_config.model_parallel, 1, 1)))
+            # [B, H, S, D]
+            self.assign_value_past = P.Assign().shard(
                 ((parallel_config.data_parallel, parallel_config.model_parallel, 1, 1),
                  (parallel_config.data_parallel, parallel_config.model_parallel, 1, 1)))
-
         if use_seq_parallel:
-            self.add.shard(((parallel_config.data_parallel * parallel_config.model_parallel, 1),
-                            (parallel_config.data_parallel * parallel_config.model_parallel, 1)))
-            self.layernorm1.shard(((parallel_config.data_parallel * parallel_config.model_parallel, 1),))
-            self.layernorm2.shard(((parallel_config.data_parallel * parallel_config.model_parallel, 1),))
+            self.add.shard(((parallel_config.data_parallel*parallel_config.model_parallel, 1),
+                            (parallel_config.data_parallel*parallel_config.model_parallel, 1)))
+            self.layernorm1.shard(((parallel_config.data_parallel*parallel_config.model_parallel, 1),))
+            self.layernorm2.shard(((parallel_config.data_parallel*parallel_config.model_parallel, 1),))
             if not self.use_moe:
                 self.output.projection.shard(
                     strategy_bias=((parallel_config.data_parallel * parallel_config.model_parallel, 1), (1,)),
                     strategy_matmul=((parallel_config.data_parallel, parallel_config.model_parallel),
                                      (parallel_config.model_parallel, 1)),
                     out_strategy_matmul=((parallel_config.data_parallel * parallel_config.model_parallel, 1),))
                 self.output.dropout.dropout.shard(
                     ((parallel_config.data_parallel * parallel_config.model_parallel, 1),))
 
-        self.depend = P.Depend()
-        self.cast = P.Cast()
-
     # pylint: disable=arguments-differ
     def construct(self, x, alibi_tensor, input_mask=None, init_reset=True, batch_valid_length=None):
         """forward process"""
         self._check_input(x, input_mask, init_reset, batch_valid_length)
         if self.post_layernorm_residual:
             input_x = x
         else:
             input_x = self.layernorm1(x)
 
-        input_x = self.cast(input_x, self.dtype)
+        input_x = input_x.astype(self.dtype)
         # indicate whether reset saved states
         key_reset = None
         value_reset = None
 
         if self.use_past and self.is_first_iteration:
             # reset states, init_reset True for reuse and False for reset
-            self.assign_past(self.key_past, self.mul(self.key_past, F.cast(init_reset, self.dtype)))
+            self.assign_key_past(self.key_past, init_reset)
             key_reset = self.key_past
-            self.assign_past(self.value_past, self.mul(self.value_past, F.cast(init_reset, self.dtype)))
+            self.assign_value_past(self.value_past, init_reset)
             value_reset = self.value_past
             # add dependency for desired execution order
-            input_x = self.depend(input_x, key_reset)
-            input_x = self.depend(input_x, value_reset)
+            input_x = ops.depend(input_x, key_reset)
+            input_x = ops.depend(input_x, value_reset)
 
         attention, layer_present = self.attention(input_x, input_x, input_x, alibi_tensor, input_mask,
                                                   self.key_past, self.value_past, batch_valid_length)
         # For post-layernorm the inputs for residual path are output of self-attention and output of layernorm
         if self.post_layernorm_residual:
             x = self.add(input_x, attention)
         # For pre-layernorm the inputs for residual path are output of self-attention and input of this layer
         else:
             x = self.add(x, attention)
 
         output_x = self.layernorm2(x)
-        output_x = F.cast(output_x, self.dtype)
+        output_x = output_x.astype(self.dtype)
         aux_loss = None
         if self.use_moe:
             mlp_logit, aux_loss = self.output(output_x)
         else:
             mlp_logit = self.output(output_x)
 
         value_update = None
         key_update = None
         if self.use_past:
             # current key and value
             key_present, value_present = layer_present
             # update key and value calculated this step
-            self.assign_past(self.key_past, key_present)
+            self.assign_key_past(self.key_past, key_present)
             key_update = self.key_past
-            self.assign_past(self.value_past, value_present)
+            self.assign_value_past(self.value_past, value_present)
             value_update = self.value_past
             # add dependency for desired execution order
-            key_update = self.depend(key_update, key_reset)
-            value_update = self.depend(value_update, value_reset)
+            key_update = ops.depend(key_update, key_reset)
+            value_update = ops.depend(value_update, value_reset)
 
         # add dependency for desired execution order
-        mlp_logit = self.depend(mlp_logit, value_update)
-        mlp_logit = self.depend(mlp_logit, key_update)
+        mlp_logit = ops.depend(mlp_logit, value_update)
+        mlp_logit = ops.depend(mlp_logit, key_update)
 
         output = self.add(x, mlp_logit)
         if self.use_moe:
             return output, layer_present, aux_loss
         return output, layer_present
 
 
 class BloomBlocks(TransformerEncoder):
     r"""All blocks of Bloom model."""
-
     def __init__(self,
                  batch_size,
                  num_layers,
                  hidden_size,
                  ffn_hidden_size,
                  seq_length,
                  num_heads,
@@ -598,21 +590,20 @@
             Tensor. The attention mask matrix with shape (batch_size, seq_length, seq_length).
     """
 
     def __init__(self, seq_length, parallel_config=default_dpmp_config):
         super(CausalMask, self).__init__(seq_length, parallel_config)
         self.seq_length = seq_length
         self.parallel_config = parallel_config
-        self.cast = P.Cast()
 
     def construct(self, input_mask):
         """Forward process of the CausalMask"""
-        input_mask = self.cast(self.not_equal(input_mask, 0), mstype.float32)
+        input_mask = self.not_equal(input_mask, 0).astype(mstype.float32)
         input_shape = input_mask.shape
         shape_right = (input_shape[0], 1, input_shape[1])
         # Mask the padded inputs
-        attention_mask = self.reshape(input_mask, shape_right)
+        attention_mask = input_mask.reshape(shape_right)
         lower_traiangle = self.expand_dim(self.lower_triangle_mask, 0)
         # the returned shape is [bs, seq_length, seq_length]
         attention_mask = self.multiply(
             attention_mask, lower_traiangle)
         return attention_mask
```

## mindformers/models/glm/glm.py

```diff
@@ -31,14 +31,15 @@
 from mindformers.modules.layers import LayerNorm
 from mindformers.version_control import get_dropout
 from mindformers.models.modeling_utils import PreTrainedModel
 
 from .glm_config import GLMConfig
 from .layers import DeepNormWithGLULayer
 
+
 #  Get MS backend: 0 vm 1 GE
 is_ge = os.getenv('MS_ENABLE_GE')
 if is_ge == '1':
     jit_level = "O3"
 else:
     jit_level = "O1"
 
@@ -81,15 +82,14 @@
     if isinstance(parallel_config.recompute, bool):
         if parallel_config.recompute:
             network.recompute()
     else:
         if parallel_config.recompute.recompute:
             network.recompute(recompute_slice_activation=parallel_config.recompute.recompute_slice_activation)
 
-
 class ProcessLogits(nn.Cell):
     r"""Process logits into probability distribution."""
 
     def __init__(self, use_past=False):
         super(ProcessLogits, self).__init__()
         self.e = ms.Tensor(np.e)
         self.gather = P.Gather()
@@ -114,15 +114,14 @@
     The backbone of GLM network
 
     Args:
         config (GLMConfig): The config of network.
         op_parallel_config (optional): Operator parallel strategy. Default: `OpParallelConfig()`.
         embed_parallel_config (optional): Operator parallel strategy. Default: `EmbeddingOpParallelConfig()`.
     """
-
     def __init__(self,
                  config,
                  op_parallel_config=default_dpmp_config,
                  embed_parallel_config=default_embedding_parallel_config):
         super(GLMModel, self).__init__(config)
         # recording parameters
         self.num_layers = config.num_layers
@@ -171,29 +170,26 @@
                 compute_dtype=config.compute_dtype,
                 use_past=self.use_past,
                 parallel_config=op_parallel_config,
             )
 
         self.layers = nn.CellList()
         for i in range(config.num_layers):
-            layer = get_layer(i + 1)
+            layer = get_layer(i+1)
             set_parallel_configure_for_layer(layer, layer_id=i, layers=config.num_layers,
                                              offset=0, parallel_config=op_parallel_config)
             self.layers.append(layer)
 
         # Final layer norm before output.
         self.use_final_layernorm = config.use_final_layernorm
         if config.use_final_layernorm:
             self.final_layernorm = layernorm(config.hidden_size, eps=config.layernorm_epsilon)
             self.final_layernorm.shard(((op_parallel_config.data_parallel, 1, 1),))
 
-    # pylint: disable=W0613
-    def construct(self, input_ids, position_ids, attention_mask, init_reset=True, batch_valid_length=None,
-                  labels=None, input_position=None, input_embeds=None, batch_index=None, zactivate_len=None,
-                  block_tables=None, slot_mapping=None):
+    def construct(self, input_ids, position_ids, attention_mask, init_reset=True, batch_valid_length=None):
         """
         Get output logits
 
         Inputs:
             input_ids (Tensor): The tokenized inputs with dtype int32.
             input_mask (Tensor): The mask indicating whether each position is a valid input.
             position_ids (Tensor): Used to identify each token's position in the list of tokens.
@@ -382,36 +378,27 @@
         position_ids_tmp = []
         attention_mask_tmp = []
         for i, index_value in enumerate(current_index):
             current_index_tmp = (
                 int(index_value) - i * input_ids.shape[1]
             )  # multibatch
             # use numpy to slice array to avoid complie ascend slice op
-            inputs_tmp.append(input_ids[i][current_index_tmp: current_index_tmp + 1])
+            inputs_tmp.append(input_ids[i][current_index_tmp : current_index_tmp + 1])
             position_ids_tmp.append(position_ids[i][..., current_index_tmp:current_index_tmp + 1])
             attention_mask_tmp.append(attention_mask[i][:, current_index_tmp:current_index_tmp + 1, :])
         inputs_tmp = np.array(inputs_tmp, dtype=np.int32)
         position_ids_tmp = np.array(position_ids_tmp, dtype=np.int32)
         attention_mask_tmp = np.array(attention_mask_tmp, dtype=np.int32)
         model_inputs["input_ids"] = Tensor(inputs_tmp, mstype.int32)
         model_inputs["position_ids"] = Tensor(position_ids_tmp, mstype.int32)
         model_inputs["attention_mask"] = Tensor(attention_mask_tmp, mstype.int32)
 
-    def add_flags_custom(self, is_first_iteration):
-        """Add customized attributes for specific cells in the model."""
-        self.add_flags(is_first_iteration=is_first_iteration)
-        self.transformer.add_flags(is_first_iteration=is_first_iteration)
-        for layer in self.transformer.layers:
-            layer.add_flags(is_first_iteration=is_first_iteration)
-            layer.attention.add_flags(is_first_iteration=is_first_iteration)
-
     # pylint: disable=W0613
     def construct(self, input_ids, labels=None, position_ids=None, attention_mask=None,
-                  input_position=None, input_embeds=None, init_reset=True, batch_valid_length=None,
-                  batch_index=None, zactivate_len=None, block_tables=None, slot_mapping=None):
+                  input_position=None, input_embeds=None, init_reset=True, batch_valid_length=None):
         """
         Extract logits and calculate loss
 
         Inputs:
             input_ids (Tensor): the tokenized inputs with dtype int32.
             labels (Tensor): the indices of input sequence tokens in the vocabulary.
             position_ids (Tensor): used to identify each token's position in the list of tokens.
@@ -511,17 +498,16 @@
         else:
             probs, p_args = self.topk(logits, self.top_k)
             p = probs
 
         return p, p_args
 
     # pylint:disable=arguments-differ,W0613
-    def construct(self, input_ids, position_ids=None, attention_mask=None, init_reset=True, batch_valid_length=None,
-                  labels=None, input_position=None, input_embeds=None, batch_index=None, zactivate_len=None,
-                  block_tables=None, slot_mapping=None):
+    def construct(self, input_ids, position_ids=None, attention_mask=None, input_position=None,
+                  labels=None, input_embeds=None, init_reset=True, batch_valid_length=None):
         """Get probs and p_args"""
         # model forward
         output_states, _ = self.transformer(input_ids, position_ids, attention_mask, init_reset, batch_valid_length)
         logits = self.lm_head(output_states)
 
         if not self.is_sample_acceleration:
             return (logits,)
```

## mindformers/models/glm2/glm2.py

```diff
@@ -29,15 +29,14 @@
 from mindformers.version_control import get_tril
 from mindformers.models.modeling_utils import PreTrainedModel
 
 from ..utils import cell_reuse
 from .glm2_config import ChatGLM2Config
 from .glm2_modules import precompute_rotary_emb_cache
 from .glm2_transformer import ChatGLM2Transformer
-from ...tools.logger import logger
 
 __all__ = ['ChatGLM2ForConditionalGeneration', 'ChatGLM2Model', 'ChatGLM2WithPtuning2']
 
 
 class GLM2PreTrainedModel(PreTrainedModel):
     """
     An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained
@@ -51,58 +50,56 @@
 class ChatGLM2Model(GLM2PreTrainedModel):
     r"""
     The backbone of ChatGLM2 network
 
     Args:
         config (GLMConfig): The config of network.
     """
-
     def __init__(self, config: ChatGLM2Config, **kwargs):
         super(ChatGLM2Model, self).__init__(config, **kwargs)
         self.num_layers = config.num_layers
         self.multi_query_group_num = config.multi_query_group_num
         self.kv_channels = config.kv_channels
         self.seq_length = config.seq_length
         self.compute_dtype = config.compute_dtype
         self.use_past = config.use_past
         self.is_first_iteration = True
-
         # vocab embedding
         embed_parallel_config = EmbeddingOpParallelConfig()
         embed_parallel_config.data_parallel = config.parallel_config.data_parallel
         embed_parallel_config.model_parallel = config.parallel_config.model_parallel
         embed_parallel_config.vocab_emb_dp = False
         self.embedding = VocabEmbedding(vocab_size=config.vocab_size, embedding_size=config.hidden_size,
-                                        parallel_config=embed_parallel_config, param_init_type=config.param_init_type)
+                                        parallel_config=embed_parallel_config)
         self.embedding.set_comm_fusion(config.parallel_config.gradient_aggregation_group)
 
         # rotary embedding
         rotary_dim = (
             config.hidden_size // config.num_attention_heads if config.kv_channels is None else config.kv_channels
         )
         self.rotary_pos_emb = precompute_rotary_emb_cache(
             seq_len=self.seq_length,
-            dim=rotary_dim // 2,
-            rope_ratio=config.rope_ratio,
-            dtype=config.compute_dtype
+            dim=rotary_dim // 2
         )
+        self.rotary_pos_emb = Tensor(self.rotary_pos_emb, config.compute_dtype)
 
         self.encoder = ChatGLM2Transformer(config)
 
         self.output_layer = Linear(config.hidden_size,
                                    config.vocab_size,
                                    has_bias=False,
                                    param_init_type=config.param_init_type,
                                    compute_dtype=config.compute_dtype)
         self.output_layer.shard(strategy_matmul=((config.parallel_config.data_parallel, 1),
                                                  (config.parallel_config.model_parallel, 1)))
         if config.parallel_config.pipeline_stage > 1:
             self.output_layer.pipeline_stage = config.parallel_config.pipeline_stage - 1
         self.output_layer.set_comm_fusion(config.parallel_config.gradient_aggregation_group)
 
+
         self.tril = get_tril()
         self.ones = P.Ones()
         self.less = P.Less()
         self.gather = P.Gather()
         self.expand_dims = P.ExpandDims()
         self.reshape = P.Reshape()
         self.mul = P.Mul()
@@ -123,32 +120,39 @@
             # [bs, 1, seq_len] for incremental infer
             attention_mask = self.gather(attention_mask.view(-1, self.seq_length), input_position, 0)
         # [bs, 1, seq_len, seq_len] for normal, [bs, 1, 1, seq_len] for incremental infer
         attention_mask = self.reshape(attention_mask, (batch_size, 1, -1, self.seq_length))
         return attention_mask
 
     def construct(self, input_ids, input_position=None, position_ids=None, attention_mask=None,
-                  input_embeds=None, batch_valid_length=None, full_attention_mask=None, prefix_key_values=None,
-                  block_tables=None, slot_mapping=None):
+                  input_embeds=None, init_reset=True, batch_valid_length=None, full_attention_mask=None,
+                  prefix_key_values=None):
         """ChatGLM2 model."""
         _ = position_ids
         batch_size, _ = input_ids.shape
         if input_embeds is None:
             input_embeds, _ = self.embedding(input_ids)  # (bs, seq_len, hs)
 
         if full_attention_mask is None:
             # (bs, 1, seq_len, seq_len)
             full_attention_mask = self.get_masks(batch_size, attention_mask, input_position)
-            full_attention_mask = full_attention_mask.type(mstype.uint8)
+
+        # (sen length, kv_channels // 4, 2)
+        rotary_pos_emb = self.rotary_pos_emb
+        if self.use_past and not self.is_first_iteration and batch_valid_length is not None:
+            # only take [bs, 1, kv_channels // 4, 2]
+            batch_gather_position = batch_valid_length.view(-1, 1) - 1  # [bs, seq_len=1]
+            rotary_pos_emb = self.gather(rotary_pos_emb, batch_gather_position, 0)
 
         # Run encoder.
         hidden_states = self.encoder(
-            input_embeds, full_attention_mask, self.rotary_pos_emb,
-            batch_valid_length=batch_valid_length, prefix_key_values=prefix_key_values, block_tables=block_tables,
-            slot_mapping=slot_mapping)
+            input_embeds, full_attention_mask, rotary_pos_emb=rotary_pos_emb,
+            init_reset=init_reset, batch_valid_length=batch_valid_length,
+            prefix_key_values=prefix_key_values)
+
         return hidden_states
 
 
 @MindFormerRegister.register(MindFormerModuleType.MODELS)
 class ChatGLM2ForConditionalGeneration(GLM2PreTrainedModel):
     r"""
     Provide gpt training loss or logits through network.
@@ -165,87 +169,53 @@
 
     @cell_reuse
     def __init__(self, config: ChatGLM2Config, **kwargs):
         super(ChatGLM2ForConditionalGeneration, self).__init__(config, **kwargs)
         self.transformer = ChatGLM2Model(config=config)
         self.cast = P.Cast()
         self.gather = P.Gather()
+        self.is_first_iteration = True
         self.loss = CrossEntropyLoss(parallel_config=config.parallel_config)
         self.gmask = config.gmask_token_id
         self.bos_token_id = config.bos_token_id
         self.use_past = config.use_past
         self.is_first_iteration = True
         self.not_equal = P.NotEqual()
         self.add = P.Add()
         self.load_checkpoint(config)
         self.vocab_size = config.padded_vocab_size
-        self.set_model_predict_config()
 
     def prepare_inputs_for_generation(self, input_ids, **kwargs):
         """prepare inputs for generation."""
         input_position = kwargs.get("current_index", None)
         if input_position is not None:
             input_position = Tensor(input_position, mstype.int32)
-        if self.config.is_dynamic and "origin_inputs" in kwargs:
-            input_ids = kwargs["origin_inputs"]
         return {
             "input_ids": Tensor(input_ids, mstype.int32),
             "input_position": input_position
         }
 
-    def prepare_inputs_for_predict_layout(self, input_ids, **kwargs):
-        """Get ChatGLM2 model input tuple for transform ckpt."""
-        input_ids = Tensor(input_ids, mstype.int32)
-        labels = Tensor(kwargs["labels"]) if "labels" in kwargs else None
-        bs = input_ids.shape[0]
-        slot_mapping = Tensor(np.ones(shape=tuple([bs])), mstype.int32)
-        return input_ids, labels, None, None, None, None, None, None, None, None, slot_mapping, None, None
-
-    def set_dynamic_inputs(self):
-        dynamic_input_ids = Tensor(shape=[None, None], dtype=mstype.int32)
-        dynamic_input_position = Tensor(shape=[None], dtype=mstype.int32)
-        dynamic_init_reset = Tensor([False], mstype.bool_)
-        dynamic_batch_valid_length = Tensor(shape=[None, None], dtype=mstype.int32)
-        dynamic_block_tables = Tensor(shape=[None, None], dtype=mstype.int32)
-        dynamic_slot_mapping = Tensor(shape=[None], dtype=mstype.int32)
-        self.set_inputs(dynamic_input_ids, None, dynamic_input_position, None, None, None, dynamic_init_reset,
-                        dynamic_batch_valid_length, None, dynamic_block_tables, dynamic_slot_mapping, None, None)
-        logger.info("Set dynamic input for glm.")
-
-    def add_flags_custom(self, is_first_iteration):
-        """Add customized attributes for specific cells in the model."""
-        self.add_flags(is_first_iteration=is_first_iteration)
-        self.transformer.add_flags(is_first_iteration=is_first_iteration)
-        for layer in self.transformer.encoder.layers:
-            layer.add_flags(is_first_iteration=is_first_iteration)
-            layer.self_attention.infer_attention.add_flags(is_first_iteration=is_first_iteration)
-
-    # pylint: disable=W0613
     def construct(self, input_ids=None, labels=None, input_position=None, position_ids=None, attention_mask=None,
-                  input_embeds=None, init_reset=True, batch_valid_length=None, prefix_key_values=None,
-                  block_tables=None, slot_mapping=None, batch_index=None, zactivate_len=None):
+                  input_embeds=None, init_reset=True, batch_valid_length=None, prefix_key_values=None):
         """ChatGLM2 for conditional generation model."""
         # input_ids: (bs, seq_len)
         # position_ids: (bs, seq_len)
         # attention_mask: (bs, seq_len)
         bs, seq_len = input_ids.shape
         hidden_states = self.transformer(
             input_ids=input_ids,
             input_position=input_position,
             position_ids=position_ids,
             attention_mask=attention_mask,
             input_embeds=input_embeds,
+            init_reset=init_reset,
             batch_valid_length=batch_valid_length,
-            prefix_key_values=prefix_key_values,
-            block_tables=block_tables,
-            slot_mapping=slot_mapping
+            prefix_key_values=prefix_key_values
         )
         lm_logits = self.transformer.output_layer(hidden_states)
-        if lm_logits.dtype == mstype.bfloat16:
-            lm_logits = self.cast(lm_logits, mstype.float32)
         outputs = (lm_logits,)
 
         # train
         if labels is not None:
             logits = lm_logits.to(mstype.float32)
             labels = labels.reshape((-1,))
             logits = logits.reshape((-1, logits.shape[-1]))
@@ -319,34 +289,25 @@
             # load ckpt
             config.checkpoint_name_or_path = ckpt_cfg
             self.load_checkpoint(config)
 
         # freeze pretrained model
         PetAdapter.freeze_pretrained_model(self, config.pet_config.pet_type)
 
-    # pylint: disable=W0613
     def construct(self, input_ids=None, labels=None, input_position=None, position_ids=None, attention_mask=None,
-                  input_embeds=None, init_reset=True, batch_valid_length=None, prefix_key_values=None,
-                  block_tables=None, slot_mapping=None, batch_index=None, zactivate_len=None):
+                  input_embeds=None, init_reset=True, batch_valid_length=None, prefix_key_values=None):
+
         if not self.use_past or self.is_first_iteration:
             batch_size = input_ids.shape[0]
             prefix_key_values = self.prefix_encoder(batch_size)
 
         return super().construct(
             input_ids=input_ids,
             labels=labels,
             input_position=input_position,
             position_ids=position_ids,
             attention_mask=attention_mask,
             input_embeds=input_embeds,
+            init_reset=init_reset,
             batch_valid_length=batch_valid_length,
-            prefix_key_values=prefix_key_values,
-            block_tables=block_tables,
-            slot_mapping=slot_mapping
+            prefix_key_values=prefix_key_values
         )
-
-    def kvcache(self, layer_idx):
-        key_cache = \
-            self.transformer.encoder.layers[layer_idx].self_attention.infer_attention.paged_attention_mgr.key_cache
-        value_cache = \
-            self.transformer.encoder.layers[layer_idx].self_attention.infer_attention.paged_attention_mgr.value_cache
-        return key_cache, value_cache
```

## mindformers/models/glm2/glm2_config.py

```diff
@@ -47,15 +47,14 @@
                  ffn_hidden_size=13696,
                  kv_channels=128,
                  num_attention_heads=32,
                  seq_length=2048,
                  hidden_dropout=0.0,
                  attention_dropout=0.0,
                  layernorm_epsilon=1e-5,
-                 rope_ratio=1,
                  rmsnorm=True,
                  apply_residual_connection_post_layernorm=False,
                  post_layer_norm=True,
                  add_bias_linear=False,
                  add_qkv_bias=True,
                  bias_dropout_fusion=True,
                  multi_query_attention=True,
@@ -69,25 +68,21 @@
                  param_init_type: str = "float16",
                  compute_dtype: str = "float16",
                  layernorm_compute_type: str = "float32",
                  use_past=False,
                  use_flash_attention=False,
                  use_prompt_flash_attention=False,
                  use_incre_flash_attention=False,
-                 block_size=16,
-                 num_blocks=128,
-                 is_dynamic=False,
                  eos_token_id=2,
                  pad_token_id=0,
                  gmask_token_id=None,
                  bos_token_id=None,
                  repetition_penalty=1.0,
                  checkpoint_name_or_path=None,
                  parallel_config: Union[dict, TransformerOpParallelConfig] = default_transformer_config,
-                 no_recompute_layers=None,
                  **kwargs):
         super().__init__(**kwargs)
         if isinstance(parallel_config, dict):
             parallel_config = TransformerOpParallelConfig(**parallel_config)
         self.batch_size = batch_size
         self.num_layers = num_layers
         self.vocab_size = padded_vocab_size
@@ -96,15 +91,14 @@
         self.ffn_hidden_size = ffn_hidden_size
         self.kv_channels = kv_channels
         self.num_attention_heads = num_attention_heads
         self.seq_length = seq_length
         self.hidden_dropout = hidden_dropout
         self.attention_dropout = attention_dropout
         self.layernorm_epsilon = layernorm_epsilon
-        self.rope_ratio = rope_ratio
         self.rmsnorm = rmsnorm
         self.apply_residual_connection_post_layernorm = apply_residual_connection_post_layernorm
         self.post_layer_norm = post_layer_norm
         self.add_bias_linear = add_bias_linear
         self.add_qkv_bias = add_qkv_bias
         self.bias_dropout_fusion = bias_dropout_fusion
         self.multi_query_attention = multi_query_attention
@@ -125,13 +119,7 @@
         self.eos_token_id = eos_token_id
         self.pad_token_id = pad_token_id
         self.repetition_penalty = repetition_penalty
         self.parallel_config = parallel_config
         self.checkpoint_name_or_path = checkpoint_name_or_path
         self.gmask_token_id = gmask_token_id
         self.bos_token_id = bos_token_id
-        self.block_size = block_size
-        self.num_blocks = num_blocks
-        self.is_dynamic = is_dynamic
-        self.num_heads = self.num_attention_heads
-        self.n_kv_heads = self.multi_query_group_num if self.multi_query_attention else None
-        self.no_recompute_layers = no_recompute_layers
```

## mindformers/models/glm2/glm2_modules.py

```diff
@@ -17,39 +17,31 @@
 
 import mindspore.common.dtype as mstype
 import mindspore.ops.operations as P
 from mindspore import nn, Parameter, Tensor
 from mindspore.common.initializer import initializer
 
 from mindformers.modules.layers import Linear
-from mindformers.version_control import check_rmsnorm_big_kernel_valid, check_valid_big_kernel
 
 from .glm2_config import ChatGLM2Config
 
 
-def precompute_rotary_emb_cache(seq_len: int, dim: int, dtype=mstype.float32, rope_ratio=1, base: int = 10000):
+def precompute_rotary_emb_cache(seq_len: int, dim: int, dtype=np.float32, base: int = 10000):
     """pre compute rotary emb cache."""
-    base = base * rope_ratio
     # $\Theta = {\theta_i = 10000^{\frac{2(i-1)}{d}}, i \in [1, 2, ..., \frac{d}{2}]}$
-    theta = 1.0 / (base ** (np.arange(0, dim, 2, dtype=np.float32) / dim))
+    theta = 1.0 / (base ** (np.arange(0, dim, 2, dtype=dtype) / dim))
 
     # Create position indexes `[0, 1, ..., seq_len - 1]`
-    seq_idx = np.arange(seq_len, dtype=np.float32)
+    seq_idx = np.arange(seq_len, dtype=dtype)
 
     # Calculate the product of position index and $\theta_i$
     idx_theta = np.outer(seq_idx, theta).astype(np.float32)
 
-    cache = Tensor(np.stack((np.cos(idx_theta), np.sin(idx_theta)), axis=-1), dtype=dtype)
-
-    freqs = np.expand_dims(idx_theta, 2)
-    emb = np.concatenate((freqs, freqs), axis=-1)
-    emb = emb.reshape(seq_len, dim)
-    freqs_cos = Tensor(np.concatenate((np.cos(emb), np.ones_like(emb)), axis=-1), dtype=dtype)
-    freqs_sin = Tensor(np.concatenate((np.sin(emb), np.zeros_like(emb)), axis=-1), dtype=dtype)
-    return freqs_cos, freqs_sin, cache
+    cache = np.stack((np.cos(idx_theta), np.sin(idx_theta)), axis=-1).astype(dtype)
+    return cache
 
 
 class ChatGLM2RMSNorm(nn.Cell):
     r"""
     A self-defined RMSNorm operation using reduce mean.
 
         Args:
@@ -58,103 +50,74 @@
             param_init_type: The param init type.
         Inputs:
             - **x** (Tensor) - Tensor of shape :math:`(batch, seq\_length, hidden\_size)`.
 
         Outputs:
             Tensor of shape :math:`(batch, seq_length, hidden_size)`.
     """
-
     def __init__(self, dim, eps=1e-6, param_init_type=mstype.float32):
         super(ChatGLM2RMSNorm, self).__init__()
         self.eps = Tensor(float(eps), dtype=param_init_type)
         self.weight = Parameter(initializer('ones', (dim,), dtype=param_init_type))
-        if not check_rmsnorm_big_kernel_valid():
-            self.square = P.Square()
-            self.mean = P.ReduceMean(keep_dims=True)
-            self.add = P.Add()
-            self.rsqrt = P.Rsqrt()
-            self.mul = P.Mul()
-            self.mul2 = P.Mul()
-            self.rms_norm = self._self_norm
-            self.self_define = True
-        else:
-            self.norm = P.RmsNorm(float(eps))
-            self.rms_norm = self._rms_norm
-            self.self_define = False
+        self.square = P.Square()
+        self.mean = P.ReduceMean(keep_dims=True)
+        self.add = P.Add()
+        self.rsqrt = P.Rsqrt()
+        self.mul = P.Mul()
+        self.mul2 = P.Mul()
 
-    def _self_norm(self, x):
+    def _norm(self, x):
         # shard:(dp, 1, 1)
         norm_factor = self.square(x)
         norm_factor = self.mean(norm_factor, -1)
         norm_factor = self.add(norm_factor, self.eps)
         norm_factor = self.rsqrt(norm_factor)
-        output = self.mul(x, norm_factor)
-        output = self.mul2(output, self.weight)
-        return output
-
-    def _rms_norm(self, x):
-        return self.norm(x, self.weight)[0]
+        return self.mul(x, norm_factor)
 
     def construct(self, x):
         """Forward of RMSNorm."""
-        return self.rms_norm(x)
+        output = self._norm(x)
+        output = self.mul2(output, self.weight)
+        return output
 
     def shard(self, strategy):
         """Parallel strategy configuratiuon interface."""
-        if self.self_define:
-            self.square.shard(strategy)
-            self.mean.shard(strategy)
-            self.rsqrt.shard(strategy)
-            self.add.shard((strategy[0], ()))
-            self.mul.shard((strategy[0], strategy[0]))
-            self.mul2.shard((strategy[0], (1,)))
-        else:
-            self.norm.shard((strategy[0], (1,)))
+        self.square.shard(strategy)
+        self.mean.shard(strategy)
+        self.rsqrt.shard(strategy)
+        self.add.shard((strategy[0], ()))
+        self.mul.shard((strategy[0], strategy[0]))
+        self.mul2.shard((strategy[0], (1,)))
 
 
 class ChatGLM2SiLU(nn.Cell):
     r"""
     A self-defined SwiGlu.
 
         Inputs:
             - **x** (Tensor) - Tensor.
 
         Outputs:
             Tensor. x = x * sigmod(x).
     """
-
     def __init__(self):
         super(ChatGLM2SiLU, self).__init__()
-        if check_valid_big_kernel():
-            # pylint: disable=W0212
-            self.silu = P._inner_ops.SiLU()
-            self.self_define = False
-        else:
-            self.sigmoid = P.Sigmoid()
-            self.mul = P.Mul()
-            self.silu = self._self_silu
-            self.self_define = True
+        self.sigmoid = P.Sigmoid()
+        self.mul = P.Mul()
 
     def shard(self, strategy):
-        if self.self_define:
-            self.sigmoid.shard(strategy)
-            self.mul.shard((strategy[0], strategy[0]))
-        else:
-            self.silu.shard(strategy)
-
-    def _self_silu(self, x):
-        return self.mul(x, self.sigmoid(x))
+        self.sigmoid.shard(strategy)
+        self.mul.shard((strategy[0], strategy[0]))
 
     def construct(self, x):
-        return self.silu(x)
+        return self.mul(x, self.sigmoid(x))
 
 
 class ChatGLM2SwiGLU(nn.Cell):
     """SwiGLU activation function."""
-
     def __init__(self):
         super(ChatGLM2SwiGLU, self).__init__()
         self.split = P.Split(axis=-1, output_num=2)
         self.silu = ChatGLM2SiLU()
         self.mul = P.Mul()
 
     def construct(self, x):
@@ -170,15 +133,14 @@
 class ChatGLM2MLP(nn.Cell):
     """MLP.
 
     MLP will take the input with h hidden state, project it to 4*h
     hidden dimension, perform nonlinear transformation, and project the
     state back into h hidden dimension.
     """
-
     def __init__(self, config: ChatGLM2Config):
         super(ChatGLM2MLP, self).__init__()
         self.add_bias = config.add_bias_linear
         self.dense_h_to_4h = Linear(
             config.hidden_size,
             config.ffn_hidden_size * 2,
             has_bias=self.add_bias,
```

## mindformers/models/glm2/glm2_transformer.py

```diff
@@ -10,47 +10,59 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ============================================================================
 """ChatGLM2 Transformer."""
 import math
+import numpy as np
 
 import mindspore.ops.functional as F
 import mindspore.ops.operations as P
-from mindspore import Tensor, nn, ops
+from mindspore import Parameter, Tensor, nn, ops
 from mindspore import dtype as mstype
-from mindformers.modules.infer_attention import InferAttention
+try:
+    from mindspore.ops.operations.nn_ops import PromptFlashAttention
+    PROMPTFLASHATTENTION_VALID = True
+except ImportError:
+    PROMPTFLASHATTENTION_VALID = False
+
+try:
+    from mindspore.ops.operations.nn_ops import IncreFlashAttention
+    INCREFLASHATTENTION_VALID = True
+except ImportError:
+    INCREFLASHATTENTION_VALID = False
+
 from mindformers.modules import LayerNorm
 from mindformers.modules.layers import Linear
 from mindformers.modules.flash_attention import FlashAttention
 from mindformers.pet.tuners.ptuning2_adapter import Ptuning2Adapter
-from mindformers.version_control import get_dropout
+from mindformers.version_control import get_dropout, check_valid_flash_attention, choose_flash_attention_dtype
 
 from .glm2_config import ChatGLM2Config
 from .glm2_modules import ChatGLM2MLP, ChatGLM2RMSNorm
 
 
-
-
 class CoreAttention(nn.Cell):
     """ChatGLM2 core attention."""
-
     def __init__(self, config: ChatGLM2Config, layer_number):
         super(CoreAttention, self).__init__()
         self.apply_query_key_layer_scaling = config.apply_query_key_layer_scaling
         self.attention_softmax_in_fp32 = config.attention_softmax_in_fp32
         if self.apply_query_key_layer_scaling:
             self.attention_softmax_in_fp32 = True
         self.layer_number = max(1, layer_number)
-        self.head_dim = config.kv_channels
+
         projection_size = config.kv_channels * config.num_attention_heads
 
-        self.n_head = config.num_attention_heads
-        self.norm_factor = math.sqrt(self.head_dim)
+        self.hidden_size_per_partition = projection_size
+        self.hidden_size_per_attention_head = projection_size // config.num_attention_heads
+        self.num_attention_heads_per_partition = config.num_attention_heads
+
+        self.norm_factor = math.sqrt(self.hidden_size_per_attention_head)
         self.mul_mask = P.Mul()
         self.add = P.Add()
 
         # Strided linear layer.
         self.attention_dropout = get_dropout(config.attention_dropout)
 
         parallel_config = config.parallel_config
@@ -65,21 +77,14 @@
         self.softmax = nn.Softmax(axis=-1)
 
         self.merger_head_transpose = P.Transpose().shard(
             ((parallel_config.data_parallel, parallel_config.model_parallel, 1, 1),))
         self.reshape = P.Reshape()
 
         self.compute_dtype = config.compute_dtype
-        self.multi_query_attention = config.multi_query_attention
-        if self.multi_query_attention:
-            self.n_kv_head = config.multi_query_group_num
-            self.qkv_hidden_size = (
-                projection_size + 2 * self.head_dim * config.multi_query_group_num)
-        self.transpose = P.Transpose()
-        self.cast = P.Cast()
 
     def construct(self, query_layer, key_layer, value_layer, attention_mask):
         """
         calculate attention function
         """
         # query_layer [b, heads, seq, hidden_size_per_head]
         # key_layer [b, heads, seq, hidden_size_per_head]
@@ -119,14 +124,15 @@
         attention_probs = F.cast(attention_probs, attention_scores_dtype)
 
         attention_probs = self.attention_dropout(attention_probs)
 
         # [bs, heads, seq_q, seq_k] x [bs, heads, seq_v, hidden_size_per_head] -> [b, heads, seq_q, hidden_size_per_head]
         context_layer = self.batch_matmul(attention_probs, value_layer)
         context_layer = F.cast(context_layer, self.compute_dtype)
+
         context_layer = self._merge_heads(context_layer)
 
         return context_layer
 
     def _merge_heads(self, x):
         """
         convert a 4d input to a 2d output
@@ -142,97 +148,115 @@
         new_shape = (x_shape[0], x_shape[1], -1)
         x_merge = self.reshape(x, new_shape)
         return x_merge
 
 
 class ChatGLM2SelfAttention(nn.Cell):
     """ChatGLM2 self-attention."""
-
     def __init__(self, config: ChatGLM2Config, layer_number):
         super(ChatGLM2SelfAttention, self).__init__()
         self.layer_number = max(1, layer_number)
-        self.head_dim = config.kv_channels
+
         self.projection_size = config.kv_channels * config.num_attention_heads
         # Per attention head and per partition values.
+        self.hidden_size_per_attention_head = self.projection_size // config.num_attention_heads
         self.apply_query_key_layer_scaling = config.apply_query_key_layer_scaling
-        self.norm_factor = math.sqrt(self.head_dim)
-        self.n_head = config.num_attention_heads
+        self.norm_factor = math.sqrt(self.hidden_size_per_attention_head)
+        self.num_attention_heads_per_partition = config.num_attention_heads
         self.params_dtype = config.param_init_type
         self.compute_dtype = config.compute_dtype
         self.batch_size = config.batch_size
+        self.seq_length = config.seq_length
         self.pre_seq_len = config.pre_seq_len
-        self.n_rep = self.n_head // config.multi_query_group_num
 
         self.multi_query_attention = config.multi_query_attention
         self.qkv_hidden_size = 3 * self.projection_size
 
         if self.multi_query_attention:
-            self.n_kv_head = config.multi_query_group_num
+            self.num_multi_query_groups_per_partition = config.multi_query_group_num
             self.qkv_hidden_size = (
-                self.projection_size + 2 * self.head_dim * config.multi_query_group_num)
+                self.projection_size + 2 * self.hidden_size_per_attention_head * config.multi_query_group_num)
 
         dp, mp = config.parallel_config.data_parallel, config.parallel_config.model_parallel
         self.query_key_value = Linear(config.hidden_size,
                                       self.qkv_hidden_size,
                                       has_bias=config.add_bias_linear or config.add_qkv_bias,
                                       param_init_type=self.params_dtype,
                                       compute_dtype=self.compute_dtype)
         self.query_key_value.shard(strategy_matmul=((dp, 1), (mp, 1)), strategy_bias=((dp, mp), (mp,)))
-        self.shape = P.Shape()
+
+        self.core_attention = CoreAttention(config, self.layer_number)
+
         self.dense = Linear(self.projection_size,
                             config.hidden_size,
                             has_bias=config.add_bias_linear,
                             param_init_type=self.params_dtype,
                             compute_dtype=self.compute_dtype)
         self.dense.shard(strategy_matmul=((dp, 1), (mp, 1)), strategy_bias=((dp, 1), (1,)))
+
+        self.reshape = P.Reshape()
+        self.stack = P.Stack(axis=-1)
+        self.gather = P.Gather()
+        self.index_0 = Tensor(0)
+        self.index_1 = Tensor(1)
+        self.mul = P.Mul()
+        self.sub = P.Sub()
+        self.add = P.Add()
+        self.concat = P.Concat(axis=-1)
+        self.split_3 = P.Split(axis=-1, output_num=3)
+        self.transpose = P.Transpose()
+
         self.use_past = config.use_past
         if self.use_past:
-            self.infer_attention = InferAttention(self.n_head,
-                                                  self.head_dim,
-                                                  self.n_kv_head,
-                                                  scale_value=1. / math.sqrt(self.head_dim),
+            self.is_first_iteration = True
+            total_seq_length = self.seq_length
+            if isinstance(config.pre_seq_len, int):
+                total_seq_length = total_seq_length + config.pre_seq_len
+            seq_range = np.arange(total_seq_length).reshape(1, 1, -1)
+            self.range = Tensor(
+                np.tile(seq_range, (self.batch_size, 1, 1)), mstype.int32)
+            self.less = P.Less()
+            self.add_past = P.Add().shard(((1, 1, 1, 1), (1, 1, 1, 1)))
+            self.mul1 = P.Mul().shard(((1, 1, 1, 1), (1, 1, 1, 1)))
+            self.expand_dims = P.ExpandDims().shard(((1, 1, 1),))
+            self.equal = P.Equal().shard(((1, 1, 1), (1, 1, 1)))
+            self.tile = P.Tile().shard(((1, 1, 1, 1),))
+        self.use_flash_attention = config.use_flash_attention
+        self.use_prompt_flash_attention = config.use_prompt_flash_attention
+        self.use_incre_flash_attention = config.use_incre_flash_attention
+
+        if self.use_flash_attention:
+            self.attention_mask_dtype = choose_flash_attention_dtype()
+            self.sub_attention = P.Sub().shard(((), (dp, 1, 1)))
+            head_dim = config.hidden_size // config.num_attention_heads
+            self.flash_attention = FlashAttention(head_num=config.num_attention_heads,
+                                                  scale_value=1. / math.sqrt(head_dim),
+                                                  input_layout='BNSD',
+                                                  keep_prob=1. - config.attention_dropout,
                                                   pre_tokens=65536,
                                                   next_tokens=0,
-                                                  block_size=config.block_size,
-                                                  num_blocks=config.num_blocks,
-                                                  rotary_cos_format=1,
-                                                  parallel_config=config.parallel_config)
-        else:
-            self.core_attention = CoreAttention(config, self.layer_number)
-            self.reshape = P.Reshape()
-            self.stack = P.Stack(axis=-1)
-            self.mul = P.Mul()
-            self.sub = P.Sub()
-            self.add = P.Add()
-            self.concat = P.Concat(axis=-1)
-            self.transpose = P.Transpose()
-            self.cast = P.Cast()
-            self.tile_kv = P.Tile()
-            self.use_flash_attention = config.use_flash_attention
-
-            if self.use_flash_attention:
-                self.flash_attention = FlashAttention(head_num=config.num_attention_heads,
-                                                      scale_value=1. / math.sqrt(self.head_dim),
-                                                      input_layout='BNSD',
-                                                      keep_prob=1. - config.attention_dropout,
-                                                      pre_tokens=65536,
-                                                      next_tokens=0,
-                                                      dp=dp,
-                                                      mp=mp)
-            self.merger_head_transpose = P.Transpose().shard(((dp, mp, 1, 1),))
-
-    def _repeat_kv(self, x, rep):
-        if rep == 1:
-            return x
-        bs, n_kv_head, seqlen, head_dim = self.shape(x)
-        x = self.reshape(x, (bs, n_kv_head, 1, seqlen * head_dim))
-        x = self.tile_kv(x, (1, 1, rep, 1))
-        x = self.reshape(x, (bs, n_kv_head * rep, seqlen, head_dim))
-        return x
+                                                  dp=dp,
+                                                  mp=mp)
 
+        if self.use_prompt_flash_attention:
+            self.attention_mask_dtype = choose_flash_attention_dtype()
+            self.prompt_flash_attention = PromptFlashAttention(num_heads=config.num_attention_heads,
+                                                               scale_value=1 / self.norm_factor,
+                                                               num_key_value_heads=0,
+                                                               input_layout='BNSD',
+                                                               pre_tokens=65536,
+                                                               next_tokens=0)
+
+        if self.use_incre_flash_attention and self.use_past:
+            self.attention_mask_dtype = choose_flash_attention_dtype()
+            self.incre_flash_attention = IncreFlashAttention(num_heads=config.num_attention_heads,
+                                                             scale_value=1 / self.norm_factor,
+                                                             input_layout='BNSD',
+                                                             num_key_value_heads=0)
+        self.merger_head_transpose = P.Transpose().shard(((dp, mp, 1, 1),))
     def _merge_heads(self, x):
         """
         convert a 4d input to a 2d output
 
         Inputs:
             x: input tensor
 
@@ -241,39 +265,35 @@
         """
         x = self.merger_head_transpose(x, (0, 2, 1, 3))  # bs, seq_length, head, size_per_head
         x_shape = x.shape
         new_shape = (x_shape[0], x_shape[1], -1)
         x_merge = self.reshape(x, new_shape)
         return x_merge
 
-    def apply_rotary_pos_emb(self, x: Tensor, rotary_pos_emb: Tensor) -> Tensor:
+    def apply_rotary_pos_emb(self, x: Tensor, rope_cache: Tensor) -> Tensor:
         """apply rotary position embedding to q,k."""
         # x: [b, heads, seq, hidden_size_per_head]
         bs, num_heads, seq_len, _ = x.shape  # 1, 324, 128
         # rope_cache: first (seq_len, kv_channels//4, 2), other (1, kv_channels//4, 2)
-        _, _, rope_cache = rotary_pos_emb
         rot_dim = rope_cache.shape[-2] * 2  # kv_channels // 2
         x, x_pass = x[..., :rot_dim], x[..., rot_dim:]
         # ms not support variable sizes
         # truncate to support variable sizes
         # rope_cache = rope_cache[:sq]
         # [bs, nh, sq, kv_channels//4, 2]
         xshaped = self.reshape(x, (bs, num_heads, seq_len, rot_dim // 2, 2))
         # [bs, 1, sq, kv_channels//4, 2]
-        if rope_cache.dtype == mstype.bfloat16:
-            rope_cache = self.cast(rope_cache, mstype.float32)
         rope_cache = self.reshape(rope_cache, (-1, 1, seq_len, xshaped.shape[3], 2))
 
         xshaped_0, xshaped_1 = ops.split(xshaped, 1, -1)
         rope_cache_0, rope_cache_1 = ops.split(rope_cache, 1, -1)
         x_out1 = self.sub(self.mul(xshaped_0, rope_cache_0), self.mul(xshaped_1, rope_cache_1))
         x_out2 = self.add(self.mul(xshaped_1, rope_cache_0), self.mul(xshaped_0, rope_cache_1))
         x_out = self.stack((x_out1, x_out2))
         x_out = self.reshape(x_out, (x_out.shape[0], x_out.shape[1], x_out.shape[2], -1))
-        x_out = self.cast(x_out, x_pass.dtype)
         # [bs, sq, nh, hidden_size_per_head]
         return self.concat((x_out, x_pass))
 
     def add_prefix_if_need(self, prefix_key_value, key_layer, value_layer, attention_mask):
         """
         add p-tuning v2 prefix if need
         """
@@ -293,88 +313,189 @@
             prefix_mask = attention_mask.new_zeros((batch_size, 1, seq_len, self.pre_seq_len))
             m_cat = P.Concat(3)
             # [bs, 1, seq_len, pre_seq_len + seq_len]
             attention_mask = m_cat((prefix_mask, attention_mask))
 
         return key_layer, value_layer, attention_mask
 
-    def construct(self, hidden_states, attention_mask, rotary_pos_emb, batch_valid_length=None, prefix_key_value=None,
-                  block_tables=None, slot_mapping=None):
+    def construct(self, hidden_states, attention_mask, rotary_pos_emb, key_past=None, value_past=None,
+                  batch_valid_length=None, prefix_key_value=None):
         """Forward process of self-attention."""
         # hidden_states: [bs, seq_len, hidden_size]
         # attention_mask (bs, 1, seq_len, seq_len)
         # rotary_pos_emb: first: (sen length, kv_channels//4, 2) after:(1, kv_channels//4, 2]
-        bs, seq_len, _ = self.shape(hidden_states)
+
         # [bs, seq_len, qkv_hidden_size]
         mixed_raw_layer = self.query_key_value(hidden_states)
 
         # not compatible with ms below 2.0
-        (query, key, value) = mixed_raw_layer.split(
-            [self.n_head * self.head_dim,
-             self.n_kv_head * self.head_dim,
-             self.n_kv_head * self.head_dim,
-             ],
-            axis=-1,
+        if self.multi_query_attention:
+            (query_layer, key_layer, value_layer) = mixed_raw_layer.split(
+                [self.num_attention_heads_per_partition * self.hidden_size_per_attention_head,
+                 self.num_multi_query_groups_per_partition * self.hidden_size_per_attention_head,
+                 self.num_multi_query_groups_per_partition * self.hidden_size_per_attention_head,
+                 ],
+                axis=-1,
+            )
+            # [bs, seq_len, nh, hidden_size_per_attention_head] -> [bs, nh, seq_len, hidden_size_per_attention_head]
+            query_layer = query_layer.view(
+                query_layer.shape[:-1] + (self.num_attention_heads_per_partition, self.hidden_size_per_attention_head)
+            )
+            query_layer = self.transpose(query_layer, (0, 2, 1, 3))
+            # [bs, seq_len, multi_query_groups, hidden_size_per_attention_head]
+            # -> [bs, multi_query_groups, seq_len, hidden_size_per_attention_head]
+            key_layer = key_layer.view(
+                key_layer.shape[:-1] + (self.num_multi_query_groups_per_partition, self.hidden_size_per_attention_head)
+            )
+            key_layer = self.transpose(key_layer, (0, 2, 1, 3))
+            # [bs, seq_len, multi_query_groups, hidden_size_per_attention_head]
+            # -> [bs, multi_query_groups, seq_len, hidden_size_per_attention_head]
+            value_layer = value_layer.view(
+                value_layer.shape[:-1]
+                + (self.num_multi_query_groups_per_partition, self.hidden_size_per_attention_head)
+            )
+            value_layer = self.transpose(value_layer, (0, 2, 1, 3))
+        else:
+            # [b, seq, (heads * 3 * hidden_size_per_head)] --> [b, seq, heads, 3 * hidden_size_per_head]
+            new_tensor_shape = mixed_raw_layer.shape[:-1] + (
+                self.num_attention_heads_per_partition, 3 * self.hidden_size_per_attention_head,
+            )
+            mixed_raw_layer = mixed_raw_layer.view(*new_tensor_shape)
+            # [b, seq, heads, hidden_size_per_head]
+            (query_layer, key_layer, value_layer) = self.split_3(mixed_raw_layer)
+            # [b, seq, heads, hidden_size_per_head] -> [bs, num_heads, seq_len, hidden_size_per_head]
+            query_layer = self.transpose(query_layer, (0, 2, 1, 3))
+            key_layer = self.transpose(key_layer, (0, 2, 1, 3))
+            value_layer = self.transpose(value_layer, (0, 2, 1, 3))
+
+        # rotary_pos_emb: first: (seq_length, kv_channels//4, 2) after:(1, kv_channels//4, 2)
+        if rotary_pos_emb is not None:
+            # [b, heads, seq, hidden_size_per_head]
+            query_layer = self.apply_rotary_pos_emb(query_layer, rotary_pos_emb)
+            # [bs, multi_query_groups, seq_len, hidden_size_per_attention_head]
+            key_layer = self.apply_rotary_pos_emb(key_layer, rotary_pos_emb)
+
+        key_layer, value_layer, attention_mask = self.add_prefix_if_need(
+            prefix_key_value,
+            key_layer,
+            value_layer,
+            attention_mask
         )
 
         # key and value for current token(s)
+        # [bs, heads, seq_len, hidden_size_per_head]
+        key_present = key_layer
+        value_present = value_layer
         if self.use_past:
-            freqs_cos, freqs_sin, _ = rotary_pos_emb
-            context_layer = self.infer_attention(query, key, value, batch_valid_length, block_tables, slot_mapping,
-                                                 freqs_cos, freqs_sin)
-        else:
-            query = self.transpose(self.reshape(query, (bs, seq_len, self.n_head, self.head_dim)), (0, 2, 1, 3))
-            key = self.transpose(self.reshape(key, (bs, seq_len, self.n_kv_head, self.head_dim)), (0, 2, 1, 3))
-            value = self.transpose(self.reshape(value, (bs, seq_len, self.n_kv_head, self.head_dim)), (0, 2, 1, 3))
-
-            query = self.apply_rotary_pos_emb(query, rotary_pos_emb)
-            key = self.apply_rotary_pos_emb(key, rotary_pos_emb)
-
-            key, value, attention_mask = self.add_prefix_if_need(
-                prefix_key_value,
-                key,
-                value,
-                attention_mask
-            )
+            # The first graph with the input size of (bs, seq_length)
+            if self.is_first_iteration:
+                # Get the valid input length without padding
+                valid_length_vector = F.cast(self.less(self.range, batch_valid_length.view(-1, 1, 1)),
+                                             self.params_dtype)  # [bs, 1, seq_len]
+                # Cover the key and value numbers corresponding to the padding position
+                key_present = self.mul1(key_present, self.expand_dims(valid_length_vector, 3))
+                value_present = self.mul1(value_present, self.expand_dims(valid_length_vector, 3))
+            # The second graph with the inpus size of (bs, 1)
+            # the shape of query is (bs, num_heads, 1, size_per_head)
+            # the shape of key is   (bs, multi_query_groups, 1, size_per_head)
+            # the shape of value is (bs, multi_query_groups, 1, size_per_head)
+            else:
+                # Get the current token position index
+                # key_past: [batch_size, multi_query_groups, seq_length, size_per_head]
+                valid_length = batch_valid_length - 1
+                valid_length = self.reshape(valid_length, (-1, 1, 1))  # [bs, 1, 1]
+                # self.range: [bs, 1, config.seq_len]
+                valid_length_vector = F.cast(self.equal(valid_length, self.range), self.params_dtype)
+                # Pad the key and value to seq_length with only the position index not zero
+                current_key = self.mul1(key_present, self.expand_dims(valid_length_vector, 3))
+                current_value = self.mul1(value_present, self.expand_dims(valid_length_vector, 3))
+                # Concat the previous saved state and current state
+                # [batch_size, multi_query_groups, seq_length, size_per_head]
+                key_present = self.add_past(key_past, current_key)
+                value_present = self.add_past(value_past, current_value)
+            # update k v for attention
+            # [batch_size, multi_query_groups, seq_length, size_per_head]
+            key_layer = key_present
+            # [batch_size, multi_query_groups, seq_length, size_per_head]
+            value_layer = value_present
 
-            if self.use_flash_attention:
-                context_layer = self.flash_attention(query, key, value, attention_mask)
+        layer_present = (key_present, value_present)
+
+        # tile k,v to num_heads
+        if self.multi_query_attention:
+            bs, heads, _, hs_ph = key_layer.shape
+            key_layer = key_layer.view((bs, heads, -1))
+
+            key_layer = key_layer.unsqueeze(2)
+            key_layer = key_layer.tile(
+                (1, 1, self.num_attention_heads_per_partition // self.num_multi_query_groups_per_partition, 1))
+            # [b, heads, seq, hidden_size_per_head]
+            key_layer = key_layer.view((bs, self.num_attention_heads_per_partition, -1, hs_ph))
+
+            value_layer = value_layer.view((bs, heads, -1))
+            value_layer = value_layer.unsqueeze(2)
+            value_layer = value_layer.tile(
+                (1, 1, self.num_attention_heads_per_partition // self.num_multi_query_groups_per_partition, 1))
+            # [b, heads, seq, hidden_size_per_head]
+            value_layer = value_layer.view((bs, self.num_attention_heads_per_partition, -1, hs_ph))
+
+        if not self.training:
+            if self.use_prompt_flash_attention and \
+               ((self.use_past and self.is_first_iteration) or (not self.use_past)):
+                attention_mask = attention_mask.squeeze(1).to(self.attention_mask_dtype)
+                context_layer = self.prompt_flash_attention(query_layer, key_layer, value_layer, attention_mask,
+                                                            None, None, None, None, None, None, None, None)
+                context_layer = self._merge_heads(context_layer)
+            elif self.use_incre_flash_attention and (self.use_past and not self.is_first_iteration):
+                attention_mask = attention_mask.squeeze(1).to(self.attention_mask_dtype)
+                context_layer = self.incre_flash_attention(query_layer, key_layer, value_layer, attention_mask,
+                                                           None, None, None, None, None, None, None, None)
                 context_layer = self._merge_heads(context_layer)
             else:
-                key = self._repeat_kv(key, self.n_rep)
-                value = self._repeat_kv(value, self.n_rep)
-                context_layer = self.core_attention(query, key, value, attention_mask)
+                context_layer = self.core_attention(query_layer, key_layer, value_layer, attention_mask)
 
+        if self.training:
+            if self.use_flash_attention:
+                attention_mask = self.cast(attention_mask, mstype.uint8)
+                context_layer = self.flash_attention(query_layer, key_layer, value_layer, attention_mask)
+                context_layer = self._merge_heads(context_layer)
+            else:
+                context_layer = self.core_attention(query_layer, key_layer, value_layer, attention_mask)
         # # =================
         # # Output. [bs, seq_len, hidden_size]
         # # =================
 
         output = self.dense(context_layer)
 
-        return output
+        return output, layer_present
 
 
 class ChatGLM2Block(nn.Cell):
     """A single transformer layer.
 
     Transformer layer takes input with size [s, b, h] and returns an
     output of the same size.
     """
 
     def __init__(self, config: ChatGLM2Config, layer_number: int):
         super(ChatGLM2Block, self).__init__()
         self.layer_number = layer_number
         self.apply_residual_connection_post_layernorm = config.apply_residual_connection_post_layernorm
+        self.fp32_residual_connection = config.fp32_residual_connection
+        self.use_past = config.use_past
+        self.params_dtype = config.param_init_type
         self.layernorm_dtype = config.layernorm_compute_type
         self.compute_dtype = config.compute_dtype
+        self.seq_length = config.seq_length
 
         layer_norm_func = ChatGLM2RMSNorm if config.rmsnorm else LayerNorm
         # Layernorm on the input data.
         self.input_layernorm = layer_norm_func(config.hidden_size, eps=config.layernorm_epsilon,
                                                param_init_type=self.layernorm_dtype)
+
         self.input_layernorm.set_comm_fusion(config.parallel_config.gradient_aggregation_group)
 
         # Self attention.
         self.self_attention = ChatGLM2SelfAttention(config, layer_number)
         self.hidden_dropout = config.hidden_dropout
 
         # Layernorm on the attention output
@@ -386,46 +507,69 @@
         self.mlp = ChatGLM2MLP(config)
 
         self.dropout = get_dropout(self.hidden_dropout)
         self.dropout.dropout.shard(((config.parallel_config.data_parallel, 1, 1),))
 
         self.cast = P.Cast()
 
+        self.key_past = None
+        self.value_past = None
+        if self.use_past:
+            size_per_head = config.hidden_size // config.num_attention_heads
+            kv_num_partition = config.num_attention_heads
+            if config.multi_query_attention:
+                kv_num_partition = config.multi_query_group_num
+
+            total_seq_length = self.seq_length
+            if isinstance(config.pre_seq_len, int):
+                total_seq_length = total_seq_length + config.pre_seq_len
+
+            kv_shape = (config.batch_size, kv_num_partition, total_seq_length, size_per_head)
+            # parameters saving key and value states
+            self.key_past = Parameter(
+                Tensor(np.zeros(shape=kv_shape), self.params_dtype), name="key_past")
+            self.value_past = Parameter(
+                Tensor(np.zeros(shape=kv_shape), self.params_dtype), name="value_past")
+            self.mul = P.Mul().shard(((1, 1, 1, 1), (1,)))
+            self.assign = P.Assign().shard(((1, 1, 1, 1), (1, 1, 1, 1)))
 
-    def set_select_recompute(self):
-        self.input_layernorm.recompute(False)
-        self.post_attention_layernorm.recompute(False)
-        self.self_attention.recompute()
-        self.mlp.recompute()
-        self.dropout.dropout.recompute(False)
-        self.cast.recompute(False)
-
-    def construct(self, hidden_states, attention_mask, rotary_pos_emb, batch_valid_length=None, prefix_key_value=None,
-                  block_tables=None, slot_mapping=None):
+    def construct(self, hidden_states, attention_mask, rotary_pos_emb,
+                  init_reset=True, batch_valid_length=None, prefix_key_value=None):
         """Forward process of the transformer layer."""
         # hidden_states: [bs, seq_len, hidden_size]
         # attention_mask first: (bs, 1, seq_len, seq_len), after: (bs, 1, 1, seq_len)
         # rotary_pos_emb: first: (seq_len, kv_channels//4, 2) after: (1, kv_channels//4, 2)
-        if batch_valid_length is not None:
-            batch_valid_length = batch_valid_length + 1
+
         # Layer norm at the beginning of the transformer layer.
         hidden_states = self.cast(hidden_states, self.layernorm_dtype)
         layernorm_output = self.input_layernorm(hidden_states)
         # fp32 -> fp16
         layernorm_output = self.cast(layernorm_output, self.compute_dtype)
 
+        key_reset = None
+        value_reset = None
+        if self.use_past:
+            # reset states, init_reset True for reuse and False for reset
+            key_reset = self.assign(self.key_past, self.mul(
+                self.key_past, F.cast(init_reset, self.params_dtype)))
+            value_reset = self.assign(self.value_past, self.mul(
+                self.value_past, F.cast(init_reset, self.params_dtype)))
+            # add dependency for desired execution order
+            layernorm_output = F.depend(layernorm_output, key_reset)
+            layernorm_output = F.depend(layernorm_output, value_reset)
+
         # Self attention.
-        attention_output = self.self_attention(
+        attention_output, layer_present = self.self_attention(
             layernorm_output,
             attention_mask,
             rotary_pos_emb,
+            self.key_past,
+            self.value_past,
             batch_valid_length,
-            prefix_key_value,
-            block_tables=block_tables,
-            slot_mapping=slot_mapping
+            prefix_key_value
         )
 
         # Residual connection.
         # False on default.
         if self.apply_residual_connection_post_layernorm:
             residual = layernorm_output
         else:
@@ -437,39 +581,57 @@
         # Layer norm post the self attention.
         layernorm_output = self.post_attention_layernorm(layernorm_input)
         layernorm_output = self.cast(layernorm_output, self.compute_dtype)
 
         # MLP.
         mlp_output = self.mlp(layernorm_output)
 
+        value_update = None
+        key_update = None
+        if self.use_past:
+            # current key and value
+            key_present, value_present = layer_present
+            # update key and value calculated this step
+            self.assign(self.key_past, key_present)
+            key_update = self.key_past
+            self.assign(self.value_past, value_present)
+            value_update = self.value_past
+            # add dependency for desired execution order
+            key_update = F.depend(key_update, key_reset)
+            value_update = F.depend(value_update, value_reset)
+
+        # add dependency for desired execution order
+        mlp_output = F.depend(mlp_output, value_update)
+        mlp_output = F.depend(mlp_output, key_update)
+
         # Second residual connection.
         # False on default.
         if self.apply_residual_connection_post_layernorm:
             residual = layernorm_output
         else:
             residual = layernorm_input
 
         output = self.dropout(mlp_output)
         output = residual + output
 
         return output
 
 
-def set_parallel_configure_for_layer(layer, layer_id, offset, parallel_config, n_layers, no_recompute_layers=None):
+def set_parallel_configure_for_layer(layer, layer_id, offset, parallel_config, n_layers, select_recompute=False):
     r"""
         Default setting for the pipeline is: `(layer_id + offset) // (layers / pipeline_stage)`.
 
         Args:
             layer(Cell) - Represents the transformer block
             layer_id(int) - Means the layer index for the current module, counts from zero.
             offset(int) - Means the layer_index needs a offset, if there are other modules in the net.
             parallel_config(dict) - Parallel Config
             n_layers(int) - The total layers used for the model.
-            no_recompute_layers(Union[list, None]) - layer not use recompute
     """
+    _ = select_recompute
     pp_dis = max(int((n_layers + 1) / parallel_config.pipeline_stage), 1)
     if isinstance(offset, list):
         if len(offset) != parallel_config.pipeline_stage:
             raise ValueError(f"The length of `offset` {len(offset)} do not match "
                              f"`pipeline stage` {parallel_config.pipeline_stage}.")
         i = min(layer_id // pp_dis, parallel_config.pipeline_stage - 1)
         offset_layer = offset[i]
@@ -485,29 +647,20 @@
     dis = max(int((n_layers + 1) / parallel_config.gradient_aggregation_group), 1)
     if parallel_config.pipeline_stage > 1:
         # we give the fusion in pipeline mode a fixed value, otherwise the performance may become worse.
         layer.set_comm_fusion(2)
     else:
         layer.set_comm_fusion(int((layer_id + offset) / dis) + 1)
     # Used for enabling recomputation of the block
-    if not parallel_config.recompute.select_recompute:
-        if isinstance(parallel_config.recompute, bool):
-            if parallel_config.recompute:
-                layer.recompute()
-        else:
-            if parallel_config.recompute.recompute:
-                layer.recompute(recompute_slice_activation=parallel_config.recompute.recompute_slice_activation)
+    if isinstance(parallel_config.recompute, bool):
+        if parallel_config.recompute:
+            layer.recompute()
     else:
-        if not no_recompute_layers:
-            layer.set_select_recompute()
-        elif layer_id not in no_recompute_layers:
-            if parallel_config.recompute.recompute:
-                layer.recompute()
-            else:
-                layer.recompute(recompute_slice_activation=parallel_config.recompute.recompute_slice_activation)
+        if parallel_config.recompute.recompute:
+            layer.recompute(recompute_slice_activation=parallel_config.recompute.recompute_slice_activation)
 
 
 class ChatGLM2Transformer(nn.Cell):
     """Transformer class."""
 
     def __init__(self, config: ChatGLM2Config):
         super(ChatGLM2Transformer, self).__init__()
@@ -516,42 +669,58 @@
         self.compute_dtype = config.compute_dtype
 
         # Number of layers.
         self.num_layers = config.num_layers
 
         self.pre_seq_len = config.pre_seq_len
 
+        # transformer multiple layer, so check fa here to avoid multiple checking
+        config = self.check_flash_attention(config)
+
         # Transformer layers.
         def build_layer(layer_number):
             return ChatGLM2Block(config, layer_number)
 
         self.layers = nn.CellList()
         for i in range(self.num_layers):
             layer = build_layer(i + 1)
             set_parallel_configure_for_layer(layer, layer_id=i, offset=0, n_layers=self.num_layers,
                                              parallel_config=config.parallel_config,
-                                             no_recompute_layers=config.no_recompute_layers)
+                                             select_recompute=config.parallel_config.recompute.select_recompute)
             self.layers.append(layer)
 
         if self.post_layer_norm:
             layer_norm_func = ChatGLM2RMSNorm if config.rmsnorm else LayerNorm
             # Final layer norm before output.
             self.final_layernorm = layer_norm_func(config.hidden_size, eps=config.layernorm_epsilon,
                                                    param_init_type=config.layernorm_compute_type)
             # self.final_layernorm.shard()
             self.final_layernorm.set_comm_fusion(config.parallel_config.gradient_aggregation_group)
 
+    def check_flash_attention(self, config):
+        """check FA/PFA/IFA valid"""
+        if config.use_flash_attention:
+            config.use_flash_attention = check_valid_flash_attention(fa_type="FlashAttention")
+
+        if config.use_prompt_flash_attention:
+            config.use_prompt_flash_attention = check_valid_flash_attention(PROMPTFLASHATTENTION_VALID,
+                                                                            "PromptFlashAttention")
+
+        if config.use_incre_flash_attention:
+            config.use_incre_flash_attention = check_valid_flash_attention(INCREFLASHATTENTION_VALID,
+                                                                           "IncreFlashAttention")
+        return config
+
     def construct(self,
                   hidden_states,
                   attention_mask,
                   rotary_pos_emb,
+                  init_reset=True,
                   batch_valid_length=None,
-                  prefix_key_values=None,
-                  block_tables=None,
-                  slot_mapping=None):
+                  prefix_key_values=None):
         """Forward process of the transformer."""
         # hidden_states (bs, seq_len, hs)
         # attention_mask (bs, 1, seq_len, seq_len)
         # rotary_pos_emb: first: (sen length, kv_channels//2, 2) after:[1, kv_channels // 2, 2]
 
         if batch_valid_length is not None and isinstance(self.pre_seq_len, int):
             batch_valid_length = batch_valid_length + self.pre_seq_len
@@ -562,18 +731,17 @@
                 prefix_key_value = prefix_key_values[i]
             layer = self.layers[i]
 
             hidden_states = layer(
                 hidden_states,
                 attention_mask,
                 rotary_pos_emb,
+                init_reset=init_reset,
                 batch_valid_length=batch_valid_length,
-                prefix_key_value=prefix_key_value,
-                block_tables=block_tables,
-                slot_mapping=slot_mapping
+                prefix_key_value=prefix_key_value
             )
 
         # Final layer norm.
         if self.post_layer_norm:
             hidden_states = self.final_layernorm(hidden_states)
             hidden_states = self.cast(hidden_states, self.compute_dtype)
```

## mindformers/models/glm3/glm3_tokenizer.py

```diff
@@ -211,15 +211,15 @@
             if item["role"] == "system" and "tools" in item:
                 content = content + "\n" + json.dumps(item["tools"], indent=4, ensure_ascii=False)
             input_ids.extend(self.build_single_message(item["role"], item.get("metadata", ""), content))
         input_ids.extend(self.build_single_message(role, "", query))
         input_ids.extend([self.get_command("<|assistant|>")])
         return self.batch_encode_plus([input_ids], return_tensors="np", is_split_into_words=True)
 
-    def build_batch_input(self, queries, histories=None, roles="user", padding=True):
+    def build_batch_input(self, queries, histories=None, roles="user"):
         """build batch input with role."""
         if isinstance(queries, str):
             queries = [queries]
         batch_size = len(queries)
         if isinstance(roles, str):
             roles = [roles] * batch_size
         if isinstance(histories, list) and len(histories) != batch_size:
@@ -241,15 +241,15 @@
                 if item["role"] == "system" and "tools" in item:
                     content = content + "\n" + json.dumps(item["tools"], indent=4, ensure_ascii=False)
                 input_ids.extend(self.build_single_message(item["role"], item.get("metadata", ""), content))
             input_ids.extend(self.build_single_message(role, "", query))
             input_ids.extend([self.get_command("<|assistant|>")])
             batch_inputs.append(input_ids)
 
-        return self.batch_encode_plus(batch_inputs, return_tensors="np", is_split_into_words=True, padding=padding)
+        return self.batch_encode_plus(batch_inputs, return_tensors="np", is_split_into_words=True)
 
 
     def tokenize(self, text, pair=None, add_special_tokens=True, **kwargs):
         """ Returns a tokenized string. """
         return self._tokenize(text)
 
     def _tokenize(self, text, **kwargs):
@@ -401,18 +401,7 @@
             if "attention_mask" in encoded_inputs:
                 encoded_inputs["attention_mask"] = encoded_inputs["attention_mask"] + [0] * difference
             if "position_ids" in encoded_inputs:
                 encoded_inputs["position_ids"] = encoded_inputs["position_ids"] + [0] * difference
             encoded_inputs[self.model_input_names[0]] = required_input + [self.pad_token_id] * difference
 
         return encoded_inputs
-
-
-    # pylint: disable=W0221
-    def apply_chat_template(self, conversation, **tokenizer_kwargs):
-        queries, roles = [], []
-        if isinstance(conversation, list):
-            for item in conversation:
-                if "content" in item and "role" in item:
-                    queries.append(item.get("content"))
-                    roles.append(item.get("role"))
-        return self.build_batch_input(queries=queries, roles=roles, padding=True)["input_ids"]
```

## mindformers/models/gpt2/gpt2.py

```diff
@@ -106,26 +106,35 @@
         self.all_ones_attention_mask = P.Ones()((1, 1, 1), mstype.float32)
 
     def prepare_inputs_for_generation(self, input_ids, **kwargs):
         return {
             "input_ids": Tensor(input_ids, mstype.int32)
         }
 
-    def add_flags_custom(self, is_first_iteration):
-        """Add customized attributes for specific cells in the model."""
-        self.add_flags(is_first_iteration=is_first_iteration)
-        self.backbone.add_flags(is_first_iteration=is_first_iteration)
-        for layer in self.backbone.blocks:
-            layer.add_flags(is_first_iteration=is_first_iteration)
-            layer.attention.add_flags(is_first_iteration=is_first_iteration)
+    def prepare_inputs_for_export(self, full_model=True):
+        """ inputs for model export """
+        seq_length = self.config.seq_length
+        batch_size = self.config.batch_size
+        if full_model:
+            logger.info('\nexporting with batch_size = %s, seq = %s ...', batch_size, seq_length)
+            input_ids = Tensor(np.ones((batch_size, seq_length)), mstype.int32)
+            input_position = Tensor(np.ones((batch_size,)), mstype.int32)
+            init_reset = Tensor([False], mstype.bool_)
+            batch_valid_length = Tensor(np.ones([batch_size, 1]), mstype.int32)
+        else:
+            logger.info('\nexporting with batch_size = %s, seq = 1 ...', batch_size)
+            input_ids = Tensor(np.ones((batch_size, 1)), mstype.int32)
+            input_position = Tensor(np.ones((batch_size,)), mstype.int32)
+            init_reset = Tensor([True], mstype.bool_)
+            batch_valid_length = Tensor(np.ones([batch_size, 1]), mstype.int32)
+        return input_ids, None, None, None, input_position, None, init_reset, batch_valid_length
 
     # pylint: disable=W0613
     def construct(self, input_ids, attention_mask=None, input_embeds=None, labels=None, input_position=None,
-                  position_ids=None, init_reset=True, batch_valid_length=None, batch_index=None, zactivate_len=None,
-                  block_tables=None, slot_mapping=None):
+                  position_ids=None, init_reset=True, batch_valid_length=None):
         r"""
             construct function for Language Modeling
 
             Args:
                 input_ids (Tensor): the indices of input sequence tokens in the vocabulary with data type int64/int32,
                                     Tensor of shape :math:`(batch, seq\_length)`.
                 attention_mask (Tensor): input sentences padding mask, where 0 indicates padding position with
@@ -432,15 +441,16 @@
                 layernorm_compute_type=config.layernorm_compute_type,
                 softmax_compute_type=config.softmax_compute_type,
                 parallel_config=config.parallel_config.dp_mp_config if not self.use_moe else moe_parallel_config,
                 moe_config=moe_config if not (config.moe_config.save_token_distribution or
                                               config.moe_config.enable_cold_hot_expert) else moe_config[i],
                 use_past=config.use_past,
                 use_flash_attention=config.use_flash_attention,
-                use_prompt_flash_attention=config.use_prompt_flash_attention
+                use_prompt_flash_attention=config.use_prompt_flash_attention,
+                use_incre_flash_attention=config.use_incre_flash_attention
             )
             set_parallel_configure_for_layer(
                 block, layer_id=i, layers=config.num_layers,
                 offset=0, parallel_config=config.parallel_config)
             self.blocks.append(block)
 
         self.cast = P.Cast()
@@ -449,18 +459,15 @@
         self.num_layers = config.num_layers
         self.position_ids = Tensor(np.arange(config.seq_length), mstype.int32)
         self.is_first_iteration = True
         self.use_past = config.use_past
         if self.use_past:
             self.ones = P.Ones()
 
-    # pylint: disable=W0613
-    def construct(self, input_ids, attention_mask=None, input_position=None, init_reset=True, batch_valid_length=None,
-                  labels=None, input_embeds=None, batch_index=None, zactivate_len=None, position_ids=None,
-                  block_tables=None, slot_mapping=None):
+    def construct(self, input_ids, attention_mask, input_position=None, init_reset=True, batch_valid_length=None):
         """GPT model"""
         batch_size, seq_length = F.shape(input_ids)
         if self.use_past:
             if not isinstance(init_reset, Tensor):
                 init_reset = Tensor([init_reset], mstype.bool_)
             if not isinstance(batch_valid_length, Tensor):
                 batch_valid_length = self.ones((batch_size, 1), mstype.int32)
```

## mindformers/models/gpt2/gpt2_config.py

```diff
@@ -66,17 +66,17 @@
             The activation of the internal feedforward layer. Supports 'relu',
             'relu6', 'tanh', 'gelu', 'fast_gelu', 'elu', 'sigmoid', 'prelu', 'leakyrelu', 'hswish',
             'hsigmoid', 'logsigmoid' and so on. User can provide custom activition to the argument.
             If user wants to run the net in the parallel mode, the custom activation must also provide
             the `activation_shard` function. Please see the examples of the
             class:`mindformers.modules.transformer.FeedForward`. Default: gelu.
         use_past (`bool`, *optional*, defaults to `False`):
-            Whether the model should use the past last key/values attentions
+            Whether or not the model should use the past last key/values attentions
             (if applicable to the model) to speed up decoding.
-        post_layernorm_residual(bool): Whether to use post layernorm, default False.
+        post_layernorm_residual(bool): Whether use post layernorm, defaylt False.
         offset(int): Offset of transformer layer when set pipeline stage number.
         checkpoint_name_or_path (Optional[str]):
             checkpoint path or name used to load to the network.
         parallel_config(TransformerOpParallelConfig):
             The parallel configure. Default `default_transformer_config`,
             an instance of `TransformerOpParallelConfig` with default args.
         moe_config(MoEConfig):
@@ -90,15 +90,15 @@
             `max_new_tokens`. Its effect is overridden by `max_new_tokens`, if also set.
         top_k (`int`, *optional*, defaults to 5):
             The number of highest probability vocabulary tokens to keep for top-k-filtering.
         top_p (`float`, *optional*, defaults to 1.0):
             If set to float < 1, only the smallest set of most probable tokens with probabilities
             that add up to `top_p` or higher are kept for generation.
         do_sample (`bool`, *optional*, defaults to `False`):
-            Whether to use sampling ; use greedy decoding otherwise.
+            Whether or not to use sampling ; use greedy decoding otherwise.
 
     Returns:
         Class, GPT2Config.
     """
 
     model_type = 'gpt2'
     _support_list = MindFormerBook.get_config_support_list()['gpt2']
@@ -136,17 +136,15 @@
                  repetition_penalty: float = 1.0,
                  max_decode_length: int = 1024,
                  top_k: int = 5,
                  top_p: float = 1.0,
                  do_sample: bool = True,
                  use_flash_attention: bool = False,
                  use_prompt_flash_attention: bool = False,
-                 is_dynamic=False,
-                 block_size: int = 16,
-                 num_blocks: int = 512,
+                 use_incre_flash_attention: bool = False,
                  **kwargs):
         super(GPT2Config, self).__init__(**kwargs)
         if isinstance(parallel_config, dict):
             parallel_config = TransformerOpParallelConfig(**parallel_config)
         if isinstance(moe_config, dict):
             moe_config = MoEConfig(**moe_config)
         self.batch_size = batch_size
@@ -181,10 +179,8 @@
         self.repetition_penalty = repetition_penalty
         self.max_decode_length = max_decode_length
         self.top_k = top_k
         self.top_p = top_p
         self.do_sample = do_sample
         self.use_flash_attention = use_flash_attention
         self.use_prompt_flash_attention = use_prompt_flash_attention
-        self.is_dynamic = is_dynamic
-        self.block_size = block_size
-        self.num_blocks = num_blocks
+        self.use_incre_flash_attention = use_incre_flash_attention
```

## mindformers/models/gpt2/gpt_modules.py

```diff
@@ -102,15 +102,16 @@
                  softmax_compute_type=mstype.float32,
                  param_init_type=mstype.float32,
                  hidden_act='gelu',
                  use_past=False,
                  moe_config=default_moe_config,
                  parallel_config=default_dpmp_config,
                  use_flash_attention=False,
-                 use_prompt_flash_attention=False):
+                 use_prompt_flash_attention=False,
+                 use_incre_flash_attention=False):
         super(GPTTransformerDecoderLayer, self).__init__(
             batch_size=batch_size,
             hidden_size=hidden_size,
             ffn_hidden_size=ffn_hidden_size,
             num_heads=num_heads,
             seq_length=seq_length,
             attention_dropout_rate=attention_dropout_rate,
@@ -120,15 +121,16 @@
             softmax_compute_type=softmax_compute_type,
             param_init_type=param_init_type,
             hidden_act=hidden_act,
             use_past=use_past,
             moe_config=moe_config,
             parallel_config=parallel_config,
             use_flash_attention=use_flash_attention,
-            use_prompt_flash_attention=use_prompt_flash_attention
+            use_prompt_flash_attention=use_prompt_flash_attention,
+            use_incre_flash_attention=use_incre_flash_attention
         )
 
     def construct(self, x, input_mask=None, init_reset=True, batch_valid_length=None):
         """forward process"""
         self._check_input(x, input_mask, init_reset, batch_valid_length)
         x_shape = F.shape(x)
         x = F.reshape(x, (-1, x_shape[-1]))
```

## mindformers/models/llama/llama.py

```diff
@@ -12,32 +12,58 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ============================================================================
 """LLaMA models' APIs."""
 import copy
 import numpy as np
 
+import mindspore as ms
 import mindspore.common.dtype as mstype
+
+try:
+    from mindspore._checkparam import Validator
+except ImportError:
+    import mindspore._checkparam as Validator
 from mindspore import Tensor, nn
 from mindspore.context import ParallelMode
 from mindspore.ops import operations as P
 from mindspore.parallel._utils import _get_parallel_mode, _is_sharding_propagation
 
+try:
+    # pylint: disable=W0611
+    from mindspore.ops.operations.nn_ops import PromptFlashAttention
+    PFA_VALID = True
+except ImportError:
+    PFA_VALID = False
+try:
+    # pylint: disable=W0611
+    from mindspore.ops.operations.nn_ops import IncreFlashAttention
+    IFA_VALID = True
+except ImportError:
+    IFA_VALID = False
+try:
+    # pylint: disable=W0611
+    from mindformers.modules.flash_attention import FlashAttention
+    FLASHATTENTION_VALID = True
+except ImportError:
+    FLASHATTENTION_VALID = False
+
 from mindformers.core.loss.loss import CrossEntropyLoss
 from mindformers.mindformer_book import MindFormerBook
 from mindformers.models.modeling_utils import PreTrainedModel
-from mindformers.models.utils import set_layer_stage_recompute
 from mindformers.modules.layers import Linear
 from mindformers.modules.transformer import LowerTriangularMaskWithDynamic
 from mindformers.modules.transformer.op_parallel_config import _check_config
 from mindformers.tools.register.register import MindFormerModuleType, MindFormerRegister
+from mindformers.version_control import check_valid_paged_attention, check_valid_flash_attention
 
 from .llama_config import LlamaConfig
 from .llama_layer import LlamaEmbedding, LlamaRMSNorm, FreqsMgr
 from .llama_transformer import LLamaDecodeLayer
+from ...modules import KVCachePreprocess
 from .llama_interleave import LLamaDecodeLayerInterleave
 from ..utils import cell_reuse
 from ...tools.logger import logger
 
 __all__ = ['LlamaModel', 'LlamaForCausalLM']
 
 
@@ -47,14 +73,55 @@
     models.
     """
 
     config_class = LlamaConfig
     base_model_prefix = "llama"
 
 
+def layer_compute_dtype(layer, layer_id, offset, parallel_config, n_layers, select_recompute=False):
+    r"""
+        Default setting for the pipeline is: `(layer_id + offset) // (layers / pipeline_stage)`.
+
+        Args:
+            layer(Cell) - Represents the transformer block
+            parallel_config(dict) - Parallel Config
+            layer_id(int) - Means the layer index for the current module, counts from zero.
+            offset(Union[int, List[int]]) - Means the layer_index needs a offset, if there are other modules in the net.
+            n_layers(int) - The total layers used for the model.
+    """
+    pp_dis = max(int((n_layers + 1) / parallel_config.pipeline_stage), 1)
+    if isinstance(offset, list):
+        if len(offset) != parallel_config.pipeline_stage:
+            raise ValueError(f"The length of `offset` {len(offset)} do not match "
+                             "`pipeline stage` {parallel_config.pipeline_stage}.")
+        i = min(layer_id // pp_dis, parallel_config.pipeline_stage - 1)
+        offset_layer = offset[i]
+    elif isinstance(offset, int):
+        offset_layer = offset
+    else:
+        raise TypeError(f"`offset` must be `int` of list of `int`, but got {type(offset)}.")
+
+    pp_id = min((layer_id + offset_layer) // pp_dis, parallel_config.pipeline_stage - 1)
+    layer.pipeline_stage = pp_id
+
+    # Used for optimizer's fusion tag
+    dis = max(int((n_layers + 1) / parallel_config.gradient_aggregation_group), 1)
+    if parallel_config.pipeline_stage > 1:
+        layer.set_comm_fusion(2)
+    else:
+        layer.set_comm_fusion(int((layer_id + offset_layer) / dis) + 1)
+    if isinstance(parallel_config.recompute, bool):
+        if parallel_config.recompute and not select_recompute:
+            layer.recompute()
+    else:
+        if parallel_config.recompute.recompute and not select_recompute:
+            layer.recompute(
+                recompute_slice_activation=parallel_config.recompute.recompute_slice_activation)
+
+
 class LlamaModel(LlamaPreTrainedModel):
     r"""
     Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]
     Args:
         config(LlamaConfig): the config of network
 
     Returns:
@@ -68,41 +135,61 @@
     """
     _support_list = MindFormerBook.get_model_support_list()['llama']
 
     def __init__(self,
                  config: LlamaConfig = None):
         super().__init__(config, auto_prefix=True)
         _check_config(config.parallel_config)
+        if config.batch_size or config.use_past:
+            Validator.check_positive_int(config.batch_size)
         self.dtype = config.compute_dtype
         self.hidden_size = config.hidden_size
         self.num_layers = config.num_layers
         self.n_head = config.num_heads
         self.head_dim = self.hidden_size // self.n_head
         self.pad_token_id = config.pad_token_id
         self.is_first_iteration = True
         self.use_past = config.use_past
-        self.use_flash_attention = config.use_flash_attention
-        self.cast = P.Cast()
+        self.is_dynamic = config.is_dynamic
+        self.use_kvcache_op = config.use_kvcache_op
+        self.is_flexible_shape = config.is_flexible_shape
+        config.use_flash_attention = config.use_flash_attention and check_valid_flash_attention(
+            FLASHATTENTION_VALID, 'FlashAttention')
+        config.use_paged_attention = config.use_paged_attention and check_valid_paged_attention()
+        config.use_prompt_flash_attention = config.use_prompt_flash_attention and check_valid_flash_attention(
+            PFA_VALID, 'PromptFlashAttention')
+        config.use_incre_flash_attention = config.use_incre_flash_attention and check_valid_flash_attention(
+            IFA_VALID, 'IncreFlashAttention')
+
         self.shape = P.Shape()
         self.reshape = P.Reshape().add_prim_attr("skip_redistribution", True)
+        self.cast = P.Cast()
+        self.tile = P.Tile()
+        self.expand_dims = P.ExpandDims()
+        self.gather = P.Gather()
+        self.slice = P.StridedSlice()
+
         self.freqs_mgr = FreqsMgr(head_dim=self.head_dim,
                                   seq_length=config.seq_length,
                                   max_position_embedding=config.max_position_embedding,
                                   rotary_dtype=config.rotary_dtype,
                                   theta=config.theta,
                                   scaling_factor=config.scaling_factor,
-                                  extend_method=config.extend_method)
+                                  extend_method=config.extend_method,
+                                  is_dynamic=config.is_dynamic)
         self.casual_mask = LowerTriangularMaskWithDynamic(seq_length=config.seq_length,
                                                           compute_type=config.compute_dtype,
                                                           is_dynamic=config.is_dynamic,
                                                           pad_token_id=config.pad_token_id,
-                                                          use_flash_attention=config.use_flash_attention)
+                                                          use_flash_attention=config.use_flash_attention,
+                                                          use_prompt_flash_attention=config.use_prompt_flash_attention,
+                                                          use_incre_flash_attention=config.use_incre_flash_attention)
         self.tok_embeddings = LlamaEmbedding(vocab_table_size=config.vocab_size,
                                              embedding_size=config.hidden_size,
-                                             param_init_type=config.embedding_init_type)
+                                             param_init_type=config.param_init_type)
         self.layers = nn.CellList()
         for layer_id in range(config.num_layers):
             if config.fine_grain_interleave > 1 and config.parallel_config.model_parallel > 1:
                 layer = LLamaDecodeLayerInterleave(config.batch_size,
                                                    config.seq_length,
                                                    layer_id,
                                                    dim=config.hidden_size,
@@ -120,15 +207,17 @@
                                                    softmax_compute_dtype=config.softmax_compute_type,
                                                    rotary_dtype=config.rotary_dtype,
                                                    param_init_type=config.param_init_type,
                                                    use_flash_attention=config.use_flash_attention,
                                                    fine_grain_interleave=config.fine_grain_interleave,
                                                    parallel_config=config.parallel_config)
             else:
-                layer = LLamaDecodeLayer(layer_id,
+                layer = LLamaDecodeLayer(config.batch_size,
+                                         config.seq_length,
+                                         layer_id,
                                          dim=config.hidden_size,
                                          n_heads=config.num_heads,
                                          n_kv_heads=config.n_kv_heads,
                                          intermediate_size=config.intermediate_size,
                                          multiple_of=config.multiple_of,
                                          ffn_dim_multiplier=config.ffn_dim_multiplier,
                                          norm_eps=config.rms_norm_eps,
@@ -137,24 +226,37 @@
                                          compute_dtype=config.compute_dtype,
                                          layernorm_compute_dtype=config.layernorm_compute_type,
                                          softmax_compute_dtype=config.softmax_compute_type,
                                          rotary_dtype=config.rotary_dtype,
                                          param_init_type=config.param_init_type,
                                          use_past=config.use_past,
                                          use_flash_attention=config.use_flash_attention,
+                                         use_paged_attention=config.use_paged_attention,
+                                         use_prompt_flash_attention=config.use_prompt_flash_attention,
+                                         use_incre_flash_attention=config.use_incre_flash_attention,
                                          block_size=config.block_size,
                                          num_blocks=config.num_blocks,
                                          is_dynamic=config.is_dynamic,
+                                         use_kvcache_op=config.use_kvcache_op,
+                                         is_flexible_shape=config.is_flexible_shape,
                                          use_rope_slice=config.use_rope_slice,
                                          moe_config=config.moe_config,
                                          parallel_config=config.parallel_config)
-            set_layer_stage_recompute(layer, layer_id, config.offset, config.parallel_config, config.num_layers)
+            layer_compute_dtype(layer, layer_id, config.offset, config.parallel_config,
+                                config.num_layers, select_recompute=config.parallel_config.recompute.select_recompute)
             self.layers.append(layer)
         self.norm_out = LlamaRMSNorm(config.hidden_size, config.rms_norm_eps,
-                                     compute_type=config.layernorm_compute_type)
+                                     compute_type=config.layernorm_compute_type, is_dynamic=config.is_dynamic)
+        self.kvcache_preprocess = KVCachePreprocess(max_batch_size=config.batch_size,
+                                                    max_seq_length=config.seq_length,
+                                                    is_dynamic=config.is_dynamic,
+                                                    use_kvcache_op=config.use_kvcache_op,
+                                                    is_flexible_shape=config.is_flexible_shape,
+                                                    use_paged_attention=config.use_paged_attention)
+
         dp = config.parallel_config.data_parallel
         if not (_get_parallel_mode() in (ParallelMode.AUTO_PARALLEL,) and _is_sharding_propagation()):
             self.tok_embeddings.pipeline_stage = 0
             if config.parallel_config.pipeline_stage > 1:
                 self.norm_out.pipeline_stage = config.parallel_config.pipeline_stage - 1
                 self.tok_embeddings.set_comm_fusion(2)
                 self.norm_out.set_comm_fusion(2)
@@ -178,38 +280,47 @@
         Args:
             tokens: the tokenized inputs with datatype int32
             input_position(Tensor): current position, used by model.predict.
             init_reset(bool, optional): A bool tensor with shape [1], used to clear the past key parameter and
                 past value parameter used in the incremental prediction. Default True.
             batch_valid_length(Tensor): the past calculated the index with datatype int32, used for incremental
                 prediction. Tensor of shape :math:`(batch_size,)`. Default None.
-            block_tables (Tensor[int64]): Store mapping tables for each sequence.
-            slot_mapping (Tensor[int32]): Store token cache physical slot index.
+
         Returns:
             output: Tensor, the output of llama decoderlayer
         """
         # preprocess
         bs, seq_len = self.shape(tokens)
-        mask = None
-        if self.use_past:
+        if not self.use_past:
+            freqs_cis = self.freqs_mgr()
+            mask = self.casual_mask(tokens)  # mask: [bs, seq, seq]
+            kvcache_inputs = None
+        else:
             if self.is_first_iteration:
-                freqs_cis = self.freqs_mgr.prefill(bs, seq_len)
+                freqs_cis = self.freqs_mgr(seq_len)
+                mask = self.casual_mask(tokens)  # mask: [bs, seq, seq]
             else:
-                freqs_cis = self.freqs_mgr.increment(batch_valid_length)
-        else:
-            freqs_cis = self.freqs_mgr(seq_len)
-            mask = self.casual_mask(tokens)  # mask: [bs, seq, seq]
+                freqs_cis = self.freqs_mgr.increment(batch_valid_length, bs)
+                if self.is_dynamic and self.is_flexible_shape and not self.use_kvcache_op:
+                    mask = self.casual_mask.increment_slice(
+                        self.kvcache_preprocess.range,
+                        self.kvcache_preprocess.max_cache_length // bs, batch_valid_length,
+                        zactivate_len)
+                else:
+                    mask = self.casual_mask.increment(self.kvcache_preprocess.range, batch_valid_length, zactivate_len)
+
+            kvcache_inputs = self.kvcache_preprocess(bs, batch_valid_length, batch_index, zactivate_len,
+                                                     block_tables, slot_mapping)
 
         # tokens: [bs, seq/1]
-        h = self.cast(self.tok_embeddings(tokens), self.dtype)
+        h = self.tok_embeddings(tokens)
         h = self.reshape(h, (bs, seq_len, self.hidden_size))
         # h: [bs, seq/1, hidden_dim]
         for i in range(self.num_layers):
-            h = self.layers[i](h, freqs_cis, mask, batch_valid_length=batch_valid_length, block_tables=block_tables,
-                               slot_mapping=slot_mapping)
+            h = self.layers[i](h, freqs_cis, mask, kvcache_inputs=kvcache_inputs)
         output = self.norm_out(h)
         return output
 
 
 @MindFormerRegister.register(MindFormerModuleType.MODELS)
 class LlamaForCausalLM(LlamaPreTrainedModel):
     r"""
@@ -271,14 +382,15 @@
         loss_parallel_config = copy.deepcopy(config.parallel_config)
         if vocab_size % mp != 0:
             logger.warning("The vocab size of Loss is: %s, it is not divide by model_parallel: %s",
                            vocab_size, mp)
             logger.warning("Now, the model_parallel num of Loss will be changed: mp = 1")
             loss_parallel_config.model_parallel = 1
         self.loss = CrossEntropyLoss(parallel_config=loss_parallel_config)
+        self.seq_length = config.seq_length
 
         dp = config.parallel_config.data_parallel
         mp = config.parallel_config.model_parallel
         if not (_get_parallel_mode() in (ParallelMode.AUTO_PARALLEL,) and _is_sharding_propagation()):
             self.slice.shard(((dp, 1),))
             self.not_equal.shard(((dp, 1), ()))
             self.mul.shard(((dp, 1), (dp, 1)))
@@ -289,50 +401,57 @@
                 self.lm_head.shard(strategy_matmul=((dp, 1), (1, 1)))
             else:
                 self.lm_head.shard(strategy_matmul=((dp, 1), (mp, 1)))
             if config.parallel_config.pipeline_stage > 1:
                 self.lm_head.pipeline_stage = config.parallel_config.pipeline_stage - 1
 
         self.load_checkpoint(config)
-        self.set_model_predict_config()
 
     def prepare_inputs_for_generation(self, input_ids, **kwargs):
-        if self.config.is_dynamic and "origin_inputs" in kwargs:
-            input_ids = kwargs["origin_inputs"]
         return {
             "input_ids": Tensor(input_ids, mstype.int32)
         }
 
-    # pylint: disable=W0613
-    def prepare_inputs_for_predict_layout(self, input_ids, **kwargs):
-        """Get Llama model input tuple for transform ckpt."""
-        input_ids = Tensor(input_ids, mstype.int32)
-        labels = Tensor(kwargs["labels"]) if "labels" in kwargs else None
-        bs = input_ids.shape[0]
-        slot_mapping = Tensor(np.ones(shape=tuple([bs])), mstype.int32)
-        return input_ids, labels, None, None, None, None, None, None, None, None, None, slot_mapping
-
-    def set_dynamic_inputs(self):
-        dynamic_input_ids = Tensor(shape=[None, None], dtype=mstype.int32)
-        dynamic_input_position = Tensor(shape=[None], dtype=mstype.int32)
-        dynamic_init_reset = Tensor([False], mstype.bool_)
-        dynamic_batch_valid_length = Tensor(shape=[None, None], dtype=mstype.int32)
-        dynamic_block_tables = Tensor(shape=[None, None], dtype=mstype.int32)
-        dynamic_slot_mapping = Tensor(shape=[None], dtype=mstype.int32)
-        self.set_inputs(dynamic_input_ids, None, dynamic_input_position, None, None, None, dynamic_init_reset,
-                        dynamic_batch_valid_length, None, None, dynamic_block_tables, dynamic_slot_mapping)
-        logger.info("Set dynamic input for llama.")
-
-    def add_flags_custom(self, is_first_iteration):
-        """Add customized attributes for specific cells in the model."""
-        self.add_flags(is_first_iteration=is_first_iteration)
-        self.model.add_flags(is_first_iteration=is_first_iteration)
-        for layer in self.model.layers:
-            layer.add_flags(is_first_iteration=is_first_iteration)
-            layer.attention.infer_attention.add_flags(is_first_iteration=is_first_iteration)
+    def prepare_inputs_for_export(self, full_model=True):
+        dyn = self.config.is_dynamic
+        use_paged_attention = self.config.use_paged_attention and check_valid_paged_attention()
+        if dyn:
+            logger.info(f"Exporting dynamic MindIR...")
+        if use_paged_attention:
+            logger.info(f"Exporting model with paged attention...")
+        seq_length = self.seq_length
+        bs = None if dyn else self.config.batch_size
+        seq_len = None if dyn else self.seq_length
+
+        max_num_blocks_pre_batch = None if dyn else seq_len // self.config.block_size
+        logger.info(f"max num blocks pre batch: {max_num_blocks_pre_batch}")
+
+        def dummy_tensor(shape, dtype):
+            if None in shape:
+                return Tensor(shape=shape, dtype=dtype)
+            return Tensor(np.ones(shape=tuple(shape)), dtype=dtype)
+
+        batch_valid_length = dummy_tensor(shape=[bs], dtype=ms.int32)
+        batch_index = None if use_paged_attention else dummy_tensor(shape=[bs], dtype=ms.int64)
+        zactivate_len = None if use_paged_attention else dummy_tensor(shape=[seq_len], dtype=ms.int64)
+        pa_input = None if dyn else bs * seq_len
+        if full_model:
+            logger.info('\nexporting with batch_size = %s, seq = %s ...', self.config.batch_size, seq_length)
+            slot_mapping = dummy_tensor(shape=[pa_input], dtype=ms.int32) if use_paged_attention else None
+            input_ids = dummy_tensor(shape=[bs, seq_len], dtype=ms.int32)
+            block_tables = None
+        else:
+            logger.info('\nexporting with batch_size = %s, seq = 1 ...', self.config.batch_size)
+            input_ids = dummy_tensor(shape=[bs, 1], dtype=ms.int32)
+            slot_mapping = dummy_tensor(shape=[bs], dtype=ms.int32) if use_paged_attention else None
+            block_tables = dummy_tensor(shape=[bs, max_num_blocks_pre_batch],
+                                        dtype=ms.int32) if use_paged_attention else None
+
+        return input_ids, None, None, None, None, None, None, batch_valid_length, batch_index, zactivate_len, \
+            block_tables, slot_mapping
 
     # pylint: disable=W0613
     def construct(self, input_ids, labels=None, input_position=None, position_ids=None, attention_mask=None,
                   input_embeds=None, init_reset=True, batch_valid_length=None, batch_index=None, zactivate_len=None,
                   block_tables=None, slot_mapping=None):
         r"""
         LlamaForCausalLM forward.
@@ -344,16 +463,15 @@
             position_ids(Tensor): Reserved param, not used.
             attention_mask(Tensor): Reserved param, not used.
             input_embeds(Tensor): Reserved param, not used.
             init_reset(bool, optional): A bool tensor with shape [1], used to clear the past key parameter and
                 past value parameter used in the incremental prediction. Default True.
             batch_valid_length(Tensor): the past calculated the index with datatype int32, used for incremental
                 prediction. Tensor of shape :math:`(batch_size,)`. Default None.
-            block_tables (Tensor[int64]): Store mapping tables for each sequence.
-            slot_mapping (Tensor[int32]): Store token cache physical slot index.
+
         Returns:
             Tensor: The loss or (logits, tokens, input_mask) of the network.
         """
         bsz, seqlen = self.shape(input_ids)
         if self.use_past:
             if not isinstance(batch_valid_length, Tensor):
                 batch_valid_length = self.ones((bsz,), mstype.int32)
@@ -378,22 +496,21 @@
             if labels.ndim > 1:
                 if self.training:
                     labels = self.slice(labels, (0, 1), (bsz, seqlen), (1, 1))
                 label_mask = self.cast(self.not_equal(labels, self.ignore_token_id), mstype.float32)
                 input_mask = self.mul(input_mask, label_mask)
 
         if not self.training:
+            if not pre_gather:
+                logits = self.reshape(logits, (bsz, seqlen, -1))
             logits = self.cast(logits, mstype.float32)
+            # makes cast effective to avoid allgather issue in Mindspore1.10
+            input_mask = self.add(input_mask, 1)
             return logits, tokens, input_mask
 
         if logits.ndim > 2:
             logits = self.reshape(logits, (-1, logits.shape[-1]))
         logits = self.cast(logits, mstype.float32)
         labels = self.reshape(labels, (-1,))
         input_mask = self.reshape(input_mask, (-1,))
         loss = self.loss(logits, labels, input_mask)
         return loss
-
-    def kvcache(self, layer_idx):
-        key_cache = self.model.layers[layer_idx].attention.infer_attention.paged_attention_mgr.key_cache
-        value_cache = self.model.layers[layer_idx].attention.infer_attention.paged_attention_mgr.value_cache
-        return key_cache, value_cache
```

## mindformers/models/llama/llama_config.py

```diff
@@ -94,14 +94,15 @@
             that add up to `top_p` or higher are kept for generation.
         do_sample (`bool`, *optional*, defaults to `False`):
             Whether or not to use sampling ; use greedy decoding otherwise.
         block_size (`int`, *optional*, defaults to 16):
             The maximum number of tokens in one block can have when using paged attention.
         num_blocks (`int`, *optional*, defaults to 512):
             The maximum number of blocks when using paged attention.
+
         Returns:
             Class, LlamaConfig.
     """
 
     model_type = "llama"
     _support_list = MindFormerBook.get_config_support_list()['llama']
 
@@ -125,15 +126,14 @@
                  ignore_token_id: int = -100,
                  theta: float = 10000.0,
                  compute_dtype: str = "float16",
                  layernorm_compute_type: str = "float32",
                  softmax_compute_type: str = "float32",
                  rotary_dtype: str = "float32",
                  param_init_type: str = "float16",
-                 embedding_init_type=None,
                  qkv_has_bias: bool = False,
                  qkv_concat: bool = False,
                  parallel_config: Union[dict, TransformerOpParallelConfig] = default_transformer_config,
                  moe_config: Union[dict, MoEConfig] = default_moe_config,
                  use_past: bool = False,
                  pretrain_seqlen=None,
                  compute_in_2d=None,
@@ -174,18 +174,14 @@
         self.intermediate_size = intermediate_size
         self.multiple_of = multiple_of
         self.n_kv_heads = n_kv_heads
         self.ffn_dim_multiplier = ffn_dim_multiplier
         self.rms_norm_eps = rms_norm_eps
         self.qkv_concat = qkv_concat
         self.param_init_type = convert_mstype(param_init_type)
-        if embedding_init_type is not None:
-            self.embedding_init_type = convert_mstype(embedding_init_type)
-        else:
-            self.embedding_init_type = self.param_init_type
         self.qkv_has_bias = qkv_has_bias
         self.layernorm_compute_type = convert_mstype(layernorm_compute_type)
         self.softmax_compute_type = convert_mstype(softmax_compute_type)
         self.rotary_dtype = convert_mstype(rotary_dtype)
         self.compute_dtype = convert_mstype(compute_dtype)
         self.parallel_config = parallel_config
         self.moe_config = moe_config
@@ -222,7 +218,11 @@
         self.top_p = top_p
         self.do_sample = do_sample
         self.theta = theta
 
         self.use_paged_attention = use_paged_attention
         self.block_size = block_size
         self.num_blocks = num_blocks
+        if use_paged_attention and (batch_size * seq_length // self.block_size > self.num_blocks):
+            logger.warning(
+                f"Argument `num blocks` is less than the maximum possible block numbers. "
+                f"May cause `block pool is out of memory` error")
```

## mindformers/models/llama/llama_interleave.py

```diff
@@ -507,14 +507,24 @@
 
         if parallel_config.use_seq_parallel and self.is_first_iteration:
             self.add.shard(((dp * mp, 1), (dp * mp, 1)))
             self.attention_norm.shard((dp * mp, 1))
             self.ffn_norm.shard((dp * mp, 1))
             self.feed_forward.w2.shard(((dp, mp), (1, mp)), out_strategy_matmul=((dp * mp, 1),))
 
+        if parallel_config.recompute.select_recompute or (
+                isinstance(parallel_config.recompute, bool) and not parallel_config.recompute
+            ) or not parallel_config.recompute.recompute and self.layer_id < (num_layers // 2):
+            self.feed_forward.mul.recompute()
+            self.feed_forward.w1.activation.silu.recompute()
+
+        if parallel_config.recompute.select_recompute:
+            self.attention_norm.cast.recompute()
+            self.ffn_norm.cast.recompute()
+
         concat_stra1 = []
         concat_stra2 = []
         self.interleave1_inputs = nn.CellList()
         self.interleave1_inputs_ = nn.CellList()
         self.interleave2_inputs = nn.CellList()
         self.interleaved_concat1 = P.Concat(axis=0)
         self.interleaved_concat1.add_prim_attr("fine_grained_interleaved_index", self.layer_id)
@@ -581,15 +591,15 @@
         x = self.add(x, attention_output)
         output_x = self.ffn_norm(x)
         mlp_logit = self.feed_forward(output_x)
         output = self.add(x, mlp_logit)
         return output
 
     # pylint: disable=W0613
-    def construct(self, x, freqs_cis, mask=None, batch_valid_length=None, block_tables=None, slot_mapping=None):
+    def construct(self, x, freqs_cis, mask=None, kvcache_inputs=None):
         """ Forward of transformer block. """
         self._check_input(x, freqs_cis, mask)
         x = self.reshape(x, (-1, x.shape[-1]))
         # ============linear-layer1================
         if self.layer_id == 0:
             query, key, value = self.linear_layer1(x)
         else:
```

## mindformers/models/llama/llama_layer.py

```diff
@@ -15,32 +15,30 @@
 """LLaMA Model Layers' APIs."""
 
 from enum import Enum
 import numpy as np
 
 from mindspore.common.tensor import Tensor
 from mindspore.common.parameter import Parameter
-from mindspore import nn
+from mindspore import nn, ops
 import mindspore.common.dtype as mstype
 from mindspore.ops import operations as P
 from mindspore.ops import functional as F
 from mindspore.nn.cell import Cell
 
 try:
     from mindspore._checkparam import Validator
 except ImportError:
     import mindspore._checkparam as Validator
 from mindspore import log as logger
 from mindspore.common.initializer import initializer
 from mindspore.parallel._utils import _get_parallel_mode
 from mindspore.context import ParallelMode
+from mindformers.modules.layers import Linear, _check_input_dtype, _args_type_validator_check, _valid_value_checks
 from mindformers.version_control import check_valid_big_kernel
-from mindformers.modules.transformer.op_parallel_config import default_dpmp_config
-from mindformers.modules.layers import Linear, _check_input_dtype, _args_type_validator_check, \
-    _valid_value_checks
 from mindformers.tools.logger import _LogActionOnce
 from mindformers.version_control import check_rmsnorm_big_kernel_valid
 
 
 class SeqExtendMethod(Enum):
     """Stores the acceptable string identifiers for seq length extend method"""
     PI = "PI"
@@ -55,18 +53,19 @@
         Inputs:
             - **x** (Tensor) - Tensor.
 
         Outputs:
             Tensor. x = silu(x).
     """
 
+
+    # pylint: disable=W0212
     def __init__(self):
         super().__init__()
         if check_valid_big_kernel():
-            # pylint: disable=W0212
             self.silu = P._inner_ops.SiLU()
             self.self_define = False
         else:
             self.sigmoid = P.Sigmoid()
             self.mul = P.Mul()
             self.silu = self._self_silu
             self.self_define = True
@@ -86,66 +85,107 @@
 
     def activation_shard(self, strategy):
         # activation_shard is the api called by moe [dp_group, expert_dim, capacity, ffn_hidden]
         if hasattr(strategy, "expert_parallel"):
             moe_strategy = ((strategy.data_parallel, strategy.expert_parallel, 1, strategy.model_parallel),)
             self.shard(moe_strategy)
 
+def get_swap_mask(head_dim):
+    """Swap matrix"""
+    zero_block = np.zeros((head_dim // 2, head_dim // 2), dtype=np.float32)
+    id_block = np.identity(head_dim // 2, dtype=np.float32)
+    return np.block([[zero_block, id_block], [-id_block, zero_block]])
+
+
+def precompute_freqs_cis(
+        dim: int,
+        end: int,
+        theta: float = 10000.0,
+        dtype=mstype.float32,
+        pretrain_seqlen=2048,
+        extend_method=SeqExtendMethod.NONE.value):
+    """
+    Precompute of freqs and mask for rotary embedding.
+    """
+    ratio = 1.
+    if extend_method != SeqExtendMethod.NONE.value and end > pretrain_seqlen:
+        ratio = end / pretrain_seqlen
+    if extend_method == SeqExtendMethod.NTK.value:
+        theta *= ratio
+    freqs_base = np.arange(0, dim, 2)[: (dim // 2)].astype(np.float32) # (head_dim // 2, )
+    freqs = 1.0 / (theta ** (freqs_base / dim)) # (head_dim // 2, )
+    if extend_method == SeqExtendMethod.PI.value:
+        t = np.arange(0, end / ratio, 1 / ratio).astype(np.float32)
+    else:
+        t = np.arange(0, end, 1).astype(np.float32)  # type: ignore # (seq_len,)
+    freqs = np.outer(t, freqs)  # type: ignore (seq_len, head_dim // 2)
+    emb = np.concatenate((freqs, freqs), axis=-1)
+    freqs_cos = np.cos(emb) # (seq_len, head_dim)
+    freqs_sin = np.sin(emb) # (seq_len, head_dim)
+    freqs_cos = Tensor(freqs_cos, dtype=dtype)
+    freqs_sin = Tensor(freqs_sin, dtype=dtype)
+
+    swap_mask = get_swap_mask(dim)
+    swap_mask = Tensor(swap_mask, dtype=dtype)
+
+    return freqs_cos, freqs_sin, swap_mask
+
 
 class FreqsMgr(Cell):
     r"""freqs_cis manager."""
-
     def __init__(self,
                  head_dim,
                  seq_length=None,
                  max_position_embedding=4096,
                  rotary_dtype=mstype.float16,
                  theta=10000,
                  scaling_factor=1.0,
-                 extend_method=SeqExtendMethod.NONE.value):
+                 extend_method=SeqExtendMethod.NONE.value,
+                 is_dynamic=False):
         super().__init__()
         if seq_length is not None and seq_length > max_position_embedding:
             max_position_embedding = seq_length
         if extend_method == SeqExtendMethod.NTK.value:
             theta *= scaling_factor
-        freqs_base = np.arange(0, head_dim, 2)[: (head_dim // 2)].astype(np.float32)  # (head_dim // 2, )
-        freqs = 1.0 / (theta ** (freqs_base / head_dim))  # (head_dim // 2, )
+        freqs_base = np.arange(0, head_dim, 2)[: (head_dim // 2)].astype(np.float32) # (head_dim // 2, )
+        freqs = 1.0 / (theta ** (freqs_base / head_dim)) # (head_dim // 2, )
         if extend_method == SeqExtendMethod.PI.value:
             t = np.arange(0, max_position_embedding / scaling_factor, 1 / scaling_factor).astype(np.float32)
         else:
             t = np.arange(0, max_position_embedding, 1).astype(np.float32)
         freqs = np.outer(t, freqs)  # (max_position_embedding, head_dim // 2)
         emb = np.concatenate((freqs, freqs), axis=-1)
-        freqs_cos = np.cos(emb)  # (seq_len, head_dim)
-        freqs_sin = np.sin(emb)  # (seq_len, head_dim)
+        freqs_cos = np.cos(emb) # (seq_len, head_dim)
+        freqs_sin = np.sin(emb) # (seq_len, head_dim)
         swap_mask = FreqsMgr.get_swap_mask(head_dim)
 
         self.head_dim = head_dim
+        self.seq_length = max_position_embedding if seq_length is None else seq_length
+        self.is_dynamic = is_dynamic
         self.freqs_cos = Tensor(freqs_cos, dtype=rotary_dtype)
         self.freqs_sin = Tensor(freqs_sin, dtype=rotary_dtype)
         self.swap_mask = Tensor(swap_mask, dtype=rotary_dtype)
 
+        self.reshape = P.Reshape()
+        if is_dynamic:
+            self.reshape.add_prim_attr("skip_redistribution", True)
         self.slice = P.StridedSlice().shard(((1, 1),))
+        self.sub = P.Sub()
         self.gather = P.Gather().shard(((1, 1), (1,)))
-        self.tile = P.Tile().shard(((1, 1),))
-
-    def construct(self, seq_length):
-        freqs_cos = self.slice(self.freqs_cos, (0, 0), (seq_length, self.head_dim), (1, 1))
-        freqs_sin = self.slice(self.freqs_sin, (0, 0), (seq_length, self.head_dim), (1, 1))
-
-        return freqs_cos, freqs_sin, self.swap_mask
 
-    def prefill(self, batch_size, seq_length):
-        freqs_cos = self.tile(self.slice(self.freqs_cos, (0, 0), (seq_length, self.head_dim), (1, 1)), (batch_size, 1))
-        freqs_sin = self.tile(self.slice(self.freqs_sin, (0, 0), (seq_length, self.head_dim), (1, 1)), (batch_size, 1))
+    def construct(self, seq_length=None):
+        freqs_cos, freqs_sin = self.freqs_cos, self.freqs_sin
+        seqlen = seq_length if self.is_dynamic else self.seq_length
+        freqs_cos = self.slice(freqs_cos, (0, 0), (seqlen, self.head_dim), (1, 1))
+        freqs_sin = self.slice(freqs_sin, (0, 0), (seqlen, self.head_dim), (1, 1))
         return freqs_cos, freqs_sin, self.swap_mask
 
-    def increment(self, batch_valid_length):
-        freqs_cos = self.gather(self.freqs_cos, batch_valid_length, 0)
-        freqs_sin = self.gather(self.freqs_sin, batch_valid_length, 0)
+    def increment(self, batch_valid_length, batch_size):
+        freqs_cos = self.reshape(self.gather(self.freqs_cos, batch_valid_length, 0), (batch_size, 1, 1, self.head_dim))
+        freqs_sin = self.reshape(self.gather(self.freqs_sin, batch_valid_length, 0), (batch_size, 1, 1, self.head_dim))
         return freqs_cos, freqs_sin, self.swap_mask
 
     @staticmethod
     def get_swap_mask(head_dim):
         """Swap matrix"""
         zero_block = np.zeros((head_dim // 2, head_dim // 2), dtype=np.float32)
         id_block = np.identity(head_dim // 2, dtype=np.float32)
@@ -300,21 +340,21 @@
         Inputs:
             - **x** (Tensor) - Tensor of shape :math:`(batch, seq\_length, hidden\_size)`.
 
         Outputs:
             Tensor of shape :math:`(batch, seq_length, hidden_size)`.
     """
 
-    def __init__(self, dim, eps=1e-6, compute_type=mstype.float32):
+    def __init__(self, dim, eps=1e-6, compute_type=mstype.float32, is_dynamic=False):
         super(LlamaRMSNorm, self).__init__()
         self.eps = eps
         self.compute_type = compute_type
         self.weight = Parameter(initializer('ones', (dim,), dtype=mstype.float32), parallel_optimizer=False)
 
-        if check_rmsnorm_big_kernel_valid():
+        if check_rmsnorm_big_kernel_valid(is_dynamic):
             self.norm = P.RmsNorm(eps)
             self.rms_norm = self._rms_norm
             self.self_define = False
             self.cast = P.Cast()
             self.rcast = P.Cast()
             self.cast.recompute()
         else:
@@ -336,15 +376,15 @@
         norm_factor = self.rsqrt(norm_factor)
         output = self.mul(x, self.cast(norm_factor, original_type))
         output = self.mul2(output, self.cast(self.weight, original_type))
         return output
 
     def _rms_norm(self, x):
         original_type = x.dtype
-        output = self.norm(self.cast(x, self.compute_type), self.cast(self.weight, self.compute_type))[0]
+        output = self.norm(self.cast(x, self.compute_type), self.weight)[0]
         return self.rcast(output, original_type)
 
     def construct(self, x):
         """Forward of RMSNorm."""
         return self.rms_norm(x)
 
     def shard(self, strategy_in):
@@ -394,81 +434,72 @@
                  hidden_dim=None,
                  expert_num=1,
                  multiple_of=256,
                  hidden_act=LlamaSiLU,
                  ffn_dim_multiplier=None,
                  compute_dtype=mstype.float16,
                  param_init_type=mstype.float32,
-                 is_dynamic=False,
-                 parallel_config=default_dpmp_config):
+                 is_dynamic=False):
         super().__init__()
 
         if hidden_act is None or not (isinstance(hidden_act, str) or issubclass(hidden_act, nn.Cell)):
             raise TypeError(f"For FeedForward cell, the hidden_act should str type or nn.Cell type, "
                             f"but got {hidden_act}.")
 
         if intermediate_size is not None:
             hidden_dim = intermediate_size
         else:
             if ffn_dim_multiplier is not None:
                 hidden_dim = int((ffn_dim_multiplier + 0.01) * hidden_dim)
             hidden_dim = int(2 * hidden_dim / 3)
             hidden_dim = multiple_of * \
-                         ((hidden_dim + multiple_of - 1) // multiple_of)
+                ((hidden_dim + multiple_of - 1) // multiple_of)
 
-        if expert_num > 1:
-            ep = parallel_config.expert_parallel
-            dp_moe = parallel_config.data_parallel // ep
-        else:
-            dp_moe = 1
         self.dtype = compute_dtype
         self.hidden_act = hidden_act
         self.dim = dim
         self.hidden_dim = hidden_dim
         self.expert_num = expert_num
 
         self.mul = P.Mul()
         self.cast = P.Cast()
         self.w1 = Linear(in_channels=dim,
                          out_channels=hidden_dim,
                          expert_num=expert_num,
-                         outer_batch=dp_moe,
                          activation=hidden_act,
                          has_bias=False,
                          compute_dtype=compute_dtype,
                          param_init_type=param_init_type,
                          skip_redistribution=is_dynamic)
 
         self.w2 = Linear(in_channels=hidden_dim,
                          out_channels=dim,
                          expert_num=expert_num,
-                         outer_batch=dp_moe,
                          has_bias=False,
                          compute_dtype=compute_dtype,
                          param_init_type=param_init_type,
                          skip_redistribution=is_dynamic)
 
         self.w3 = Linear(in_channels=dim,
                          out_channels=hidden_dim,
                          expert_num=expert_num,
-                         outer_batch=dp_moe,
                          has_bias=False,
                          compute_dtype=compute_dtype,
                          param_init_type=param_init_type,
                          skip_redistribution=is_dynamic)
 
     def construct(self, x):
         """Forward process of the FeedForward"""
         _check_input_dtype(F.dtype(x), "x", [mstype.float32, mstype.float16, mstype.bfloat16], self.cls_name)
         x = self.cast(x, self.dtype)
         # [bs, seq, hidden_dim] or [bs * seq, hidden_dim]
-        gate = self.w1(x)  # dp,1 -> dp, mp
-        hidden = self.w3(x)  # dp,1 -> dp, mp
-        hidden = self.mul(hidden, gate)  # dp,mp -> dp, mp
-        output = self.w2(hidden)  # dp,mp -> dp, 1
+        gate = self.w1(x) # dp,1 -> dp, mp
+        hidden = self.w3(x) # dp,1 -> dp, mp
+        hidden = self.mul(hidden, gate) # dp,mp -> dp, mp
+        output = self.w2(hidden) # dp,mp -> dp, 1
         return output
 
     def shard(self, parallel_config):
         """sharding for feedforward"""
         dp = parallel_config.data_parallel
         mp = parallel_config.model_parallel
         if self.hidden_dim % mp != 0:
@@ -490,7 +521,107 @@
             ep = parallel_config.expert_parallel
             dp = parallel_config.data_parallel // ep
             self.w1.shard(strategy_matmul=((dp, ep, 1, 1), (ep, mp, 1)),
                           strategy_activation=((dp, ep, mp, 1),))
             self.w2.shard(strategy_matmul=((dp, ep, 1, mp), (ep, 1, mp)))
             self.w3.shard(strategy_matmul=((dp, ep, 1, 1), (ep, mp, 1)))
             self.mul.shard(((dp * ep, mp), (dp * ep, mp)))
+
+
+class CausalMask(nn.Cell):
+    r""" Get the Lower triangular matrix from the input_ids. """
+    @_LogActionOnce(m_logger=logger, key='AttentionMask',
+                    no_warning=_get_parallel_mode() in (ParallelMode.STAND_ALONE,))
+    def __init__(self, seq_length, compute_type=mstype.float16,
+                 is_dynamic=False, pad_token_id=0, use_flash_attention=False,
+                 use_prompt_flash_attention=False, use_incre_flash_attention=False):
+        super().__init__()
+        self.dtype = compute_type
+        self.is_dynamic = is_dynamic
+        self.pad_token_id = pad_token_id
+        self.use_flash_attention = use_flash_attention
+        self.use_prompt_flash_attention = use_prompt_flash_attention
+        self.use_incre_flash_attention = use_incre_flash_attention
+        self.is_first_iteration = True
+        self.multiply_data = Tensor([-10000.0], dtype=compute_type)
+        self.one = Tensor([1.0], dtype=compute_type)
+        self.lower_triangle_mask = ops.cast(Tensor(np.tril(np.ones(shape=(seq_length, seq_length))), mstype.float32),
+                                            compute_type)
+
+        self.shape = P.Shape()
+        self.cast = P.Cast()
+        self.reshape = P.Reshape()
+        self.not_equal = P.NotEqual()
+        self.less_equal = P.LessEqual()
+        self.bmm = P.BatchMatMul()
+        self.expand_dim = P.ExpandDims()
+        self.slice = P.StridedSlice()
+        self.mul = P.Mul()
+        self.sub = P.Sub()
+        self.mul_post = P.Mul()
+        self.expand_dim_post = P.ExpandDims()
+
+    def construct(self, tokens=None, masks=None):
+        """Forward process of the CausalMask"""
+        if tokens is not None:
+            bs = self.shape(tokens)[0]
+            seq_len = self.shape(tokens)[1]
+            input_mask = self.cast(self.not_equal(tokens, self.pad_token_id), self.dtype)
+        else:
+            bs = self.shape(masks)[0]
+            seq_len = self.shape(masks)[1]
+            input_mask = self.cast(masks, self.dtype)
+
+        shape_right = (bs, 1, seq_len)
+        # Mask the padded inputs
+        attention_mask = self.reshape(input_mask, shape_right)
+        if not self.is_dynamic:
+            lower_traiangle = self.expand_dim(self.lower_triangle_mask, 0)
+        else:
+            lower_triangle_mask = self.slice(self.lower_triangle_mask, (0, 0), (seq_len, seq_len), (1, 1))
+            lower_traiangle = self.expand_dim(lower_triangle_mask, 0)
+        # the returned shape is [bs, seq_length, seq_length]
+        attention_mask = self.mul(attention_mask, lower_traiangle)
+        attention_mask = self.sub(self.one, attention_mask)
+        attention_mask = self.expand_dim_post(attention_mask, 1)
+        if not self.use_flash_attention and not self.use_prompt_flash_attention:
+            attention_mask = self.mul_post(attention_mask, self.multiply_data)
+        elif self.use_flash_attention or self.use_prompt_flash_attention:
+            attention_mask = self.cast(attention_mask, mstype.uint8)
+        return attention_mask
+
+    def increment(self, seq_range, batch_valid_length, zactivate_len=None):
+        "Get mask for incremental inference."
+        if zactivate_len is not None:
+            seq_range = self.slice(seq_range, (0, 0, 0), (1, 1, self.shape(zactivate_len)[0]), (1, 1, 1))
+        mask = self.less_equal(self.reshape(seq_range, (1, 1, -1)), self.reshape(batch_valid_length, (-1, 1, 1)))
+        mask = self.cast(mask, self.dtype)
+        mask = self.sub(self.one, mask)
+        mask = self.expand_dim_post(mask, 1)
+        if not self.use_incre_flash_attention:
+            mask = self.mul_post(mask, self.multiply_data)
+        return mask
+
+    def increment_slice(self, seq_range, seq_length, batch_valid_length, zactivate_len=None):
+        "Get mask for incremental inference and apply slice."
+        if zactivate_len is not None:
+            seq_range_mask = self.slice(seq_range, (0, 0, 0), (1, 1, self.shape(zactivate_len)[0]), (1, 1, 1))
+        else:
+            seq_range_mask = self.slice(seq_range, (0, 0, 0), (1, 1, seq_length), (1, 1, 1))
+        mask = self.less_equal(self.reshape(seq_range_mask, (1, 1, -1)), self.reshape(batch_valid_length, (-1, 1, 1)))
+        mask = self.cast(mask, self.dtype)
+        mask = self.sub(self.one, mask)
+        mask = self.expand_dim_post(mask, 1)
+        if not self.use_incre_flash_attention:
+            mask = self.mul_post(mask, self.multiply_data)
+        return mask
+
+    def shard(self, parallel_config):
+        dp = parallel_config.data_parallel
+        self.not_equal.shard(((dp, 1), ()))
+        self.bmm.shard(((dp, 1, 1), (dp, 1, 1)))
+        self.expand_dim.shard(((1, 1),))
+        self.mul.shard(((dp, 1, 1), (1, 1, 1)))
+        self.less_equal.shard(((1, 1, 1), (1, 1, 1)))
+        self.sub.shard(((1,), (dp, 1, 1)))
+        self.mul_post.shard(((dp, 1, 1, 1), (1,)))
+        self.expand_dim_post.shard(((dp, 1, 1),))
```

## mindformers/models/llama/llama_tokenizer_fast.py

```diff
@@ -87,17 +87,15 @@
             Whether or not the default system prompt for Llama should be used.
     """
     vocab_files_names = VOCAB_FILES_NAMES
     model_input_names = ["input_ids", "attention_mask"]
     FILE_LIST = ['tokenizer_config.json']
     _support_list = MindFormerBook.get_tokenizer_support_list()['llama']
     slow_tokenizer_class = LlamaTokenizer
-
-    # Currently, the llama_tokenizer_fast process supports only the 'right' padding mode.
-    padding_side = "right"
+    padding_side = "left"
 
     def __init__(
             self,
             vocab_file=None,
             tokenizer_file=None,
             clean_up_tokenization_spaces=False,
             unk_token="<unk>",
```

## mindformers/models/llama/llama_transformer.py

```diff
@@ -12,35 +12,46 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ============================================================================
 """LLaMA transformer Layer's APIs."""
 import math
 from typing import Tuple, Optional
 
-from mindspore import nn
+try:
+    from mindspore._checkparam import Validator
+except ImportError:
+    import mindspore._checkparam as Validator
+
+from mindspore import nn, ops
 import mindspore.common.dtype as mstype
 from mindspore.common.tensor import Tensor
 from mindspore.context import ParallelMode
 from mindspore.ops import operations as P
 from mindspore.parallel._utils import _get_parallel_mode, _is_sharding_propagation
 
 from mindformers.models.llama.llama_layer import LlamaFeedForward, LlamaRMSNorm, LlamaRotaryEmbedding
 from mindformers.modules.layers import _check_input_dtype, Linear
 from mindformers.modules.transformer import TransformerOpParallelConfig
+from mindformers.modules import KVCacheMgr, PagedAttentionMgr
 from mindformers.modules.flash_attention import FlashAttention
-from mindformers.modules.infer_attention import InferAttention
 from mindformers.modules.transformer.moe import MoEV2
 from mindformers.tools.logger import logger
 
 
 class LLamaAttention(nn.Cell):
     r"""
     This is an implementation of multihead attention in LLaMA.
 
     Args:
+            - **batch_size** (int): The batch size of the input tensor when do increnmental prediction. Should be a
+                positive value.
+                When do training or prediction, the argument will not work and the user can just pass None to the
+                argument.
+            - **src_seq_length** (int): The sequence length of the query vector.
+            - **tgt_seq_length** (int): The sequence length of the key and value vector.
             - **dim** (int): The hidden size of the input.
             - **head_dim** (int): The dim of head.
             - **n_heads** (int): The number of the heads.
             - **compute_dtype** (dtype.Number): The computation type of dense. Default mstype.float16.
                 Should be mstype.float32 or mstype.float16.
             - **softmax_compute_type** (dtype.Number): The type of softmax computation module. Default mstype.float32.
                 Should be mstype.float32 or mstype.float16.
@@ -62,87 +73,115 @@
             - **x** (Tensor) - The input tokens with shape (batch_size, src_seq_length, hidden_size) or
                 (batch_size * src_seq_length, hidden_size), if the use_past is False or is_first_iteration=True.
                 Otherwise, must be (batch_size, 1, hidden_size)
             - **freqs_cis** (Tuple) - The precompute freqs and mask for rotary position embedding used in attention.
             - **attention_mask** (Tensor) - If the use_past is False or is_first_iteration=True, the attention mask
                 matrix should ba (batch_size, src_seq_length, tgt_seq_length), or None. None means there will be no mask
                 in softmax computation. Otherwise, the mask must be (batch_size, 1, tgt_seq_length)
+            - **key_past** (Tensor) - Float16 tensor with shape (batch_size, num_heads, head_dim, tgt_seq_length).
+                The past calculated key vector. Used for incremental prediction when the use_past is True.
+                Default None.
+            - **value_past** (Tensor) - Float16 tensor with shape (batch_size, num_heads, tgt_seq_length,
+                head_dim).
+                The past calculated value vector. Used for incremental prediction when the use_past is True.
+                Default None.
             - **batch_valid_length** (Tensor) - Int32 tensor with shape (batch_size,) the past calculated the index.
                 Used for incremental prediction when the use_past is True. Default None.
-            - **block_tables** (Tensor[int64]) - Store mapping tables for each sequence.
-            - **slot_mapping** (Tensor[int32]) - Store token cache physical slot index.
+
     Outputs:
             Tuple, a tuple contains(`output`, `layer_present`)
 
             - **output** (Tensor) - Tensor, the float tensor of the output of the layer with
                 shape (batch_size, src_seq_length, hidden_size) or (batch_size * src_seq_length, hidden_size),
                 if the use_past is False or is_first_iteration=True. Otherwise, it will be (batch_size, 1, hidden_size).
 
             - **layer_present** (Tuple) - A tuple of the Tensor of the projected key and value vector with
                 ((batch_size, num_heads, head_dim, tgt_seq_length),
                 (batch_size, num_heads, tgt_seq_length, head_dim)).
     """
 
     def __init__(self,
+                 batch_size,
+                 seq_length,
                  dim: int = 512,
                  n_heads: int = 8,
                  n_kv_heads: Optional[int] = None,
                  qkv_concat=False,
                  compute_dtype=mstype.float16,
                  softmax_compute_dtype=mstype.float32,
                  rotary_dtype=mstype.float32,
                  param_init_type=mstype.float32,
                  qkv_has_bias=False,
                  use_past=False,
                  is_dynamic=False,
+                 use_kvcache_op=False,
+                 is_flexible_shape=False,
                  use_rope_slice=False,
                  use_flash_attention=False,
+                 use_paged_attention=False,
+                 use_prompt_flash_attention=False,
+                 use_incre_flash_attention=False,
                  block_size: Optional[int] = None,
                  num_blocks: Optional[int] = None,
                  parallel_config=TransformerOpParallelConfig()):
         super().__init__()
+        self.seq_length = seq_length
         self.hidden_size = dim
         self.n_head = n_heads
         self.head_dim = dim // n_heads
         self.n_kv_head = n_heads if n_kv_heads is None else n_kv_heads
         self.n_rep = self.n_head // self.n_kv_head
         self.kv_dim = self.n_kv_head * self.head_dim
         self.block_size = block_size
         self.num_blocks = num_blocks
 
         self.dtype = compute_dtype
         self.softmax_dtype = softmax_compute_dtype
         self.is_first_iteration = True
         self.use_past = use_past
         self.use_flash_attention = use_flash_attention
+        self.use_paged_attention = use_paged_attention
+        self.use_prompt_flash_attention = use_prompt_flash_attention
+        self.use_incre_flash_attention = use_incre_flash_attention
         self.qkv_concat = qkv_concat
 
         if self.hidden_size % self.n_head != 0:
             raise ValueError("For 'MultiHeadAttention', the class variable 'hidden_size' must be a multiple "
                              "of 'n_head', but got the hidden_size is {} and the n_head is {}."
                              .format(self.hidden_size, self.n_head))
         if self.n_kv_head % parallel_config.model_parallel != 0:
             raise ValueError("For 'MultiHeadAttention', the class variable 'n_kv_head' must be a multiple of "
                              "'parallel_config.model_parallel', but got the n_kv_head is {} "
                              "and the parallel_config.model_parallel  is {}."
                              .format(self.n_kv_head, parallel_config.model_parallel))
-        dp = parallel_config.data_parallel
-        mp = parallel_config.model_parallel
+
+        self.inv_norm_factor = Tensor(1.0 / math.sqrt(self.head_dim), dtype=compute_dtype)
+
         self.shape = P.Shape()
+        self.reshape = P.Reshape().add_prim_attr("skip_redistribution", True)
+        self.transpose = P.Transpose()
+        self.merger_head_transpose = P.Transpose()
+        self.batch_matmul = P.BatchMatMul()
+        self.batch_matmul_q_k = P.BatchMatMul(transpose_b=True)
+        self.mul = P.Mul()
+        self.add = P.Add()
+        self.softmax = P.Softmax()
         self.cast = P.Cast()
+        self.cast_attn = P.Cast()
+        self.tile_kv = P.Tile()
+        self.slice_qkv = P.StridedSlice()
 
+        self.apply_rotary_emb = LlamaRotaryEmbedding(self.head_dim, rotary_dtype, use_rope_slice=use_rope_slice)
         if self.qkv_concat:
             self.w = Linear(in_channels=self.hidden_size,
                             out_channels=self.hidden_size + self.kv_dim * 2,
                             has_bias=qkv_has_bias,
                             compute_dtype=compute_dtype,
                             param_init_type=param_init_type,
                             skip_redistribution=is_dynamic)
-            self.w.shard(((dp, 1), (mp, 1)))
-
         else:
             self.wq = Linear(self.hidden_size,
                              self.hidden_size,
                              has_bias=qkv_has_bias,
                              compute_dtype=compute_dtype,
                              param_init_type=param_init_type,
                              skip_redistribution=is_dynamic)
@@ -154,136 +193,171 @@
                              skip_redistribution=is_dynamic)
             self.wv = Linear(self.hidden_size,
                              self.kv_dim,
                              has_bias=qkv_has_bias,
                              compute_dtype=compute_dtype,
                              param_init_type=param_init_type,
                              skip_redistribution=is_dynamic)
-            if qkv_has_bias:
-                self.wq.shard(((dp, 1), (mp, 1)), ((dp, mp), (mp,)))
-                self.wk.shard(((dp, 1), (mp, 1)), ((dp, mp), (mp,)))
-                self.wv.shard(((dp, 1), (mp, 1)), ((dp, mp), (mp,)))
-            else:
-                self.wq.shard(((dp, 1), (mp, 1)))
-                self.wk.shard(((dp, 1), (mp, 1)))
-                self.wv.shard(((dp, 1), (mp, 1)))
         self.wo = Linear(in_channels=self.hidden_size,
                          out_channels=self.hidden_size,
                          has_bias=False,
                          compute_dtype=compute_dtype,
                          param_init_type=param_init_type,
                          skip_redistribution=is_dynamic)
-        self.wo.shard(((dp, mp), (1, mp)))
-
-        if self.use_past:
-            self.infer_attention = InferAttention(self.n_head,
-                                                  self.head_dim,
-                                                  self.n_kv_head,
-                                                  scale_value=1. / math.sqrt(self.head_dim),
+        if self.use_flash_attention:
+            self.flash_attention = FlashAttention(head_num=self.n_head,
                                                   pre_tokens=65536,
                                                   next_tokens=0,
-                                                  block_size=self.block_size,
-                                                  num_blocks=self.num_blocks,
-                                                  rotary_cos_format=2,
-                                                  parallel_config=parallel_config)
-        else:
-            self.inv_norm_factor = Tensor(1.0 / math.sqrt(self.head_dim), dtype=compute_dtype)
+                                                  keep_prob=1.,
+                                                  scale_value=1. / math.sqrt(self.head_dim),
+                                                  input_layout="BNSD",
+                                                  sparse_mode=0,
+                                                  use_attention_mask=True,
+                                                  dp=parallel_config.data_parallel,
+                                                  mp=parallel_config.model_parallel)
+        if self.use_prompt_flash_attention:
+            self.prompt_flash_attention = P.nn_ops.PromptFlashAttention(num_heads=self.n_head,
+                                                                        num_key_value_heads=self.n_kv_head,
+                                                                        pre_tokens=65536,
+                                                                        next_tokens=0,
+                                                                        scale_value=1. / math.sqrt(self.head_dim),
+                                                                        input_layout='BNSD')
+        if self.use_incre_flash_attention and self.use_past:
+            self.incre_flash_attention = P.nn_ops.IncreFlashAttention(num_heads=self.n_head,
+                                                                      num_key_value_heads=self.n_kv_head,
+                                                                      scale_value=1. / math.sqrt(self.head_dim),
+                                                                      input_layout='BNSD')
+        dp = parallel_config.data_parallel
+        mp = parallel_config.model_parallel
+        if not (_get_parallel_mode() in (ParallelMode.AUTO_PARALLEL,) and _is_sharding_propagation()):
+            self.transpose.shard(((dp, 1, mp, 1),))
+            self.merger_head_transpose.shard(((dp, mp, 1, 1),))
+            self.batch_matmul_q_k.shard(((dp, mp, 1, 1), (dp, mp, 1, 1)))
+            self.batch_matmul.shard(((dp, mp, 1, 1), (dp, mp, 1, 1)))
+            self.mul.shard(((dp, mp, 1, 1), ()))
+            self.add.shard(((dp, 1, 1, 1), (dp, mp, 1, 1)))
+            self.softmax.shard(((dp, mp, 1, 1),))
+            self.tile_kv.shard(((dp, mp, 1, 1),))
+            self.slice_qkv.shard(((dp, mp),))
+
+            self.apply_rotary_emb.shard((dp, mp, 1, 1))
+
+            if self.qkv_concat:
+                self.w.shard(((dp, 1), (mp, 1)))
+            elif qkv_has_bias:
+                self.wq.shard(((dp, 1), (mp, 1)), ((dp, mp), (mp,)))
+                self.wk.shard(((dp, 1), (mp, 1)), ((dp, mp), (mp,)))
+                self.wv.shard(((dp, 1), (mp, 1)), ((dp, mp), (mp,)))
+            else:
+                self.wq.shard(((dp, 1), (mp, 1)))
+                self.wk.shard(((dp, 1), (mp, 1)))
+                self.wv.shard(((dp, 1), (mp, 1)))
+            self.wo.shard(((dp, mp), (1, mp)))
+            if parallel_config.use_seq_parallel and self.is_first_iteration:
+                self.wo.shard(((dp, mp), (1, mp)), out_strategy_matmul=((dp * mp, 1),))
+            if self.use_prompt_flash_attention:
+                self.prompt_flash_attention.shard(((dp, mp, 1, 1), (dp, mp, 1, 1), (dp, mp, 1, 1), (dp, 1, 1, 1)))
+            if self.use_incre_flash_attention and self.use_past:
+                self.incre_flash_attention.shard(((dp, mp, 1, 1), (dp, mp, 1, 1), (dp, mp, 1, 1), (dp, 1, 1, 1), (1,)))
+            if parallel_config.recompute.select_recompute and not self.use_flash_attention:
+                self.apply_rotary_emb.recompute()
+                self.tile_kv.recompute()
+                self.batch_matmul_q_k.recompute()
+                self.mul.recompute()
+                self.add.recompute()
+                self.cast_attn.recompute()
+                self.softmax.recompute()
+                self.batch_matmul.recompute()
 
-            self.reshape = P.Reshape().add_prim_attr("skip_redistribution", True)
-            self.transpose = P.Transpose()
-            self.merger_head_transpose = P.Transpose()
-            self.batch_matmul = P.BatchMatMul()
-            self.batch_matmul_q_k = P.BatchMatMul(transpose_b=True)
-            self.mul = P.Mul()
-            self.add = P.Add()
-            self.softmax = P.Softmax()
-            self.cast_attn = P.Cast()
-            self.tile_kv = P.Tile()
-            self.slice_qkv = P.StridedSlice()
-
-            self.apply_rotary_emb = LlamaRotaryEmbedding(self.head_dim, rotary_dtype, use_rope_slice=use_rope_slice)
-
-            if not (_get_parallel_mode() in (ParallelMode.AUTO_PARALLEL,) and _is_sharding_propagation()):
-                self.transpose.shard(((dp, 1, mp, 1),))
-                self.merger_head_transpose.shard(((dp, mp, 1, 1),))
-                self.batch_matmul_q_k.shard(((dp, mp, 1, 1), (dp, mp, 1, 1)))
-                self.batch_matmul.shard(((dp, mp, 1, 1), (dp, mp, 1, 1)))
-                self.mul.shard(((dp, mp, 1, 1), ()))
-                self.add.shard(((dp, 1, 1, 1), (dp, mp, 1, 1)))
-                self.softmax.shard(((dp, mp, 1, 1),))
-                self.tile_kv.shard(((dp, mp, 1, 1),))
-                self.slice_qkv.shard(((dp, mp),))
-
-                self.apply_rotary_emb.shard((dp, mp, 1, 1))
-
-                if parallel_config.use_seq_parallel and self.is_first_iteration:
-                    self.wo.shard(((dp, mp), (1, mp)), out_strategy_matmul=((dp * mp, 1),))
-                if parallel_config.recompute.select_recompute and not self.use_flash_attention:
-                    self.apply_rotary_emb.recompute()
-                    self.tile_kv.recompute()
-                    self.batch_matmul_q_k.recompute()
-                    self.mul.recompute()
-                    self.add.recompute()
-                    self.cast_attn.recompute()
-                    self.softmax.recompute()
-                    self.batch_matmul.recompute()
-
-            if self.use_flash_attention:
-                self.flash_attention = FlashAttention(head_num=self.n_head,
-                                                      pre_tokens=65536,
-                                                      next_tokens=0,
-                                                      input_layout="BNSD",
-                                                      keep_prob=1.,
-                                                      scale_value=1. / math.sqrt(self.head_dim),
-                                                      sparse_mode=0,
-                                                      use_attention_mask=True,
-                                                      dp=parallel_config.data_parallel,
-                                                      mp=parallel_config.model_parallel)
+        if self.use_past:
+            if self.use_paged_attention:
+                self.kvcache_mgr = PagedAttentionMgr(self.n_head,
+                                                     self.head_dim,
+                                                     self.hidden_size,
+                                                     n_kv_heads=self.n_kv_head,
+                                                     block_size=self.block_size,
+                                                     num_blocks=self.num_blocks,
+                                                     compute_dtype=compute_dtype)
+            else:
+                self.kvcache_mgr = KVCacheMgr(self.n_kv_head, self.head_dim,
+                                              max_batch_size=batch_size,
+                                              max_seq_length=seq_length,
+                                              compute_dtype=compute_dtype,
+                                              is_dynamic=is_dynamic,
+                                              use_kvcache_op=use_kvcache_op,
+                                              is_flexible_shape=is_flexible_shape)
+            self.kvcache_mgr.shard(parallel_config)
 
-    def construct(self, x: Tensor, freqs_cis: Tuple[Tensor, Tensor], mask=None, batch_valid_length=None,
-                  block_tables=None, slot_mapping=None):
+    def construct(self, x: Tensor, freqs_cis: Tuple[Tensor, Tensor], mask=None, kvcache_inputs=None):
         """Forward process of the MultiHeadAttention"""
         ori_dtype = x.dtype
         # [bs, seq/1, hidden_dim]
         bs, seq_len, _ = self.shape(x)
+        # [bs * seq/1, hidden_dim]
         if self.qkv_concat:
             x = self.reshape(x, (-1, x.shape[-1]))
             bs_seq = x.shape[0]
             qkv = self.cast(self.w(x), self.dtype)
             query = self.slice_qkv(qkv, (0, 0), (bs_seq, self.hidden_size), (1, 1))
             key = self.slice_qkv(qkv, (0, self.hidden_size),
                                  (bs_seq, self.hidden_size + self.kv_dim), (1, 1))
             value = self.slice_qkv(qkv, (0, self.hidden_size + self.kv_dim),
                                    (bs_seq, self.hidden_size + self.kv_dim * 2), (1, 1))
         else:
             query = self.cast(self.wq(x), self.dtype)  # dp, 1 -> dp, mp
             key = self.cast(self.wk(x), self.dtype)  # dp, 1 -> dp, mp
             value = self.cast(self.wv(x), self.dtype)  # dp, 1 -> dp, mp
 
-        # key and value for current token(s)
-        if self.use_past:
-            freqs_cos, freqs_sin, _ = freqs_cis
-            context_layer = self.infer_attention(query, key, value, batch_valid_length, block_tables, slot_mapping,
-                                                 freqs_cos, freqs_sin, mask)
+        if self.use_past and not self.is_first_iteration:
+            query = self.reshape(query, (bs, self.n_head, 1, self.head_dim))
+            key = self.reshape(key, (bs, self.n_kv_head, 1, self.head_dim))
+            value = self.reshape(value, (bs, self.n_kv_head, 1, self.head_dim))
         else:
-            query = self.transpose(self.reshape(query, (bs, seq_len, self.n_head, self.head_dim)), (0, 2, 1, 3))
-            key = self.transpose(self.reshape(key, (bs, seq_len, self.n_kv_head, self.head_dim)), (0, 2, 1, 3))
-            value = self.transpose(self.reshape(value, (bs, seq_len, self.n_kv_head, self.head_dim)), (0, 2, 1, 3))
-            query, key = self.apply_rotary_emb(query, key, freqs_cis)  # dp, mp, 1, 1
-            if self.use_flash_attention:
-                context_layer = self.flash_attention(query, key, value, mask)
-                context_layer = self._merge_heads(context_layer)
+            query = self.reshape(query, (bs, seq_len, self.n_head, self.head_dim))
+            key = self.reshape(key, (bs, seq_len, self.n_kv_head, self.head_dim))
+            value = self.reshape(value, (bs, seq_len, self.n_kv_head, self.head_dim))
+            # [bs, seq/1, n_head/n_kv_head, head_dim]
+            query = self.transpose(query, (0, 2, 1, 3))
+            key = self.transpose(key, (0, 2, 1, 3))
+            value = self.transpose(value, (0, 2, 1, 3))
+        # [bs, n_head/n_kv_head, seq/1, head_dim]
+        query, key = self.apply_rotary_emb(query, key, freqs_cis)  # dp, mp, 1, 1
+        # kv cache: [bs, n_kv_head, 1, head_dim] -> [bs, n_kv_head, seq, head_dim]
+        if self.use_past:
+            if self.use_paged_attention:
+                _, _, slot_mapping = kvcache_inputs
+                key_out = self.kvcache_mgr(key, value, slot_mapping)
+                query = ops.depend(query, key_out)
             else:
-                key = self._repeat_kv(key, self.n_rep)
-                value = self._repeat_kv(value, self.n_rep)
-                context_layer = self._attn(query, key, value, mask)
-
-        # [bs, seq/1, hidden_dim] or [bs * seq/1, hidden_dim]
-        output = self.wo(context_layer)  # dp, mp -> dp, 1 / dp * mp, 1
+                key, value = self.kvcache_mgr(key, value, kvcache_inputs)
+        # q, k, v: [bs, n_head, seq/1, head_dim], [bs, n_head, seq, head_dim], [bs, n_head, seq, head_dim]
+        if self.is_first_iteration and self.use_prompt_flash_attention:  # PFA
+            attention = self.prompt_flash_attention(query, key, value, mask,
+                                                    None, None, None, None, None, None, None, None)
+            attention = self._merge_heads(attention)
+        elif self.is_first_iteration and self.use_flash_attention:  # FA
+            mask = self.cast(mask, mstype.uint8)
+            attention = self.flash_attention(query, key, value, mask)
+            attention = self._merge_heads(attention)
+        elif not self.is_first_iteration and self.use_paged_attention:  # PA
+            batch_valid_length, block_tables, _ = kvcache_inputs
+            attention = self.kvcache_mgr.paged_attn(query, batch_valid_length, block_tables)
+        elif not self.is_first_iteration and self.use_incre_flash_attention:  # IFA
+            attention = self.incre_flash_attention(query, key, value, mask,
+                                                   None, None, None, None, None, None, None, None)
+            attention = self._merge_heads(attention)
+        else:  # SA
+            # kv share: [bs, n_kv_head, seq, head_dim] -> [bs, n_head, seq, head_dim]
+            key = self._repeat_kv(key, self.n_rep)
+            value = self._repeat_kv(value, self.n_rep)
+            attention = self._attn(query, key, value, mask)
+        # [bs, seq/1, hidden_dim]
+        output = self.wo(attention)  # dp, mp -> dp, 1 / dp * mp, 1
         output = self.cast(output, ori_dtype)
+
         return output
 
     def _repeat_kv(self, x, rep):
         if rep == 1:
             return x
         bs, n_kv_head, seqlen, head_dim = self.shape(x)
         x = self.reshape(x, (bs, n_kv_head, 1, seqlen * head_dim))
@@ -340,14 +414,18 @@
 
 class LLamaDecodeLayer(nn.Cell):
     r"""
         Transformer Layer. This is an implementation of the single layer of the transformer
         encoder layer, including multihead attention and feedward layer.
 
         Args:
+            batch_size(int): The batch size of the input tensor when do increnmental prediction. Should be a positive
+                value. When do training or prediction, the argument will not work and the user can just pass None to
+                the argument.
+            seq_length(int): The input sequence length.
             layer_id(int): The layer id of current transformer block layer.
             dim(int): The hidden size of the input.
             num_heads(int): The number of the heads.
             multiple_of(int): The SwiGLU hidden layer size multiple of large power of 2.
             norm_eps (float): The epsilon value of the denominator. Default 1e-5.
             compute_dtype(dtype.Number): The computation type of the layer.
                 Should be mstype.float32 or mstype.float16. Default mstype.float32.
@@ -377,30 +455,31 @@
             - **input_mask** (Tensor) - Float Tensor, If the use_past is False or is_first_iteration=True,
               the attention mask matrix should ba [batch_size, seq_length, seq_length], or None. None means there will
               be no mask in softmax computation. Otherwise, should be [batch_size, 1, hidden_size]
             - **init_reset** (Tensor) - A bool tensor with shape [1], used to clear the past key parameter and
               past value parameter used in the incremental prediction. Only valid when use_past is True. Default True.
             - **batch_valid_length** (Tensor) - Int32 tensor with shape [batch_size] the past calculated the index.
               Used for incremental prediction when the use_past is True. Default None.
-            - **block_tables** (Tensor[int64]) - Store mapping tables for each sequence.
-            - **slot_mapping** (Tensor[int32]) - Store token cache physical slot index.
+
         Outputs:
             Tuple, a tuple contains(`output`, `layer_present`).
 
             - **output** (Tensor) - The float tensor of the output of the layer with
               shape (batch_size, seq_length, hidden_size) or (batch_size * seq_length, hidden_size), if the use_past is
               False or is_first_iteration=True. Otherwise, it will be (batch_size, 1, hidden_size)
 
             - **layer_present** (Tuple) - A tuple of the Tensor of the projected key and value vector with
               ((batch_size, num_heads, head_dim, seq_length),
               (batch_size, num_heads, seq_length, head_dim)).
 
     """
 
     def __init__(self,
+                 batch_size,
+                 seq_length,
                  layer_id,
                  dim: int = 512,
                  n_heads: int = 8,
                  n_kv_heads: Optional[int] = None,
                  intermediate_size: Optional[int] = None,
                  multiple_of: int = 256,
                  ffn_dim_multiplier: Optional[int] = None,
@@ -410,106 +489,124 @@
                  layernorm_compute_dtype=mstype.float32,
                  softmax_compute_dtype=mstype.float32,
                  rotary_dtype=mstype.float32,
                  param_init_type=mstype.float32,
                  qkv_has_bias=False,
                  use_past=False,
                  is_dynamic=False,
+                 use_kvcache_op=False,
+                 is_flexible_shape=False,
                  use_rope_slice=False,
                  moe_config=None,
                  use_flash_attention=False,
+                 use_paged_attention=False,
+                 use_prompt_flash_attention=False,
+                 use_incre_flash_attention=False,
                  block_size: Optional[int] = None,
                  num_blocks: Optional[int] = None,
                  parallel_config=TransformerOpParallelConfig()):
         super().__init__()
+        if batch_size or use_past:
+            Validator.check_positive_int(batch_size)
+        self.batch_size = batch_size
+
+        self.seq_length = seq_length
         self.layer_id = layer_id
         self.hidden_size = dim
         self.n_head = n_heads
         self.head_dim = self.hidden_size // self.n_head
         self.n_kv_head = n_heads if n_kv_heads is None else n_kv_heads
         self.dtype = compute_dtype
         self.is_first_iteration = True
         self.use_past = use_past
 
         self.shape = P.Shape()
         self.reshape = P.Reshape().add_prim_attr("skip_redistribution", True)
         self.add = P.Add()
-        self.batch_valid_length_add = P.Add()
-        self.ffn_norm = LlamaRMSNorm(self.hidden_size, norm_eps, compute_type=layernorm_compute_dtype)
-        self.attention_norm = LlamaRMSNorm(self.hidden_size, norm_eps, compute_type=layernorm_compute_dtype)
-        self.attention = LLamaAttention(dim=dim,
+        self.attention_norm = LlamaRMSNorm(self.hidden_size, norm_eps, compute_type=layernorm_compute_dtype,
+                                           is_dynamic=is_dynamic)
+        self.ffn_norm = LlamaRMSNorm(self.hidden_size, norm_eps, compute_type=layernorm_compute_dtype,
+                                     is_dynamic=is_dynamic)
+        self.attention = LLamaAttention(batch_size=batch_size,
+                                        seq_length=seq_length,
+                                        dim=dim,
                                         n_heads=n_heads,
                                         n_kv_heads=n_kv_heads,
                                         qkv_concat=qkv_concat,
                                         compute_dtype=compute_dtype,
                                         softmax_compute_dtype=softmax_compute_dtype,
                                         rotary_dtype=rotary_dtype,
                                         param_init_type=param_init_type,
                                         qkv_has_bias=qkv_has_bias,
                                         use_past=use_past,
                                         is_dynamic=is_dynamic,
+                                        use_kvcache_op=use_kvcache_op,
+                                        is_flexible_shape=is_flexible_shape,
                                         use_rope_slice=use_rope_slice,
                                         use_flash_attention=use_flash_attention,
+                                        use_paged_attention=use_paged_attention,
+                                        use_prompt_flash_attention=use_prompt_flash_attention,
+                                        use_incre_flash_attention=use_incre_flash_attention,
                                         block_size=block_size,
                                         num_blocks=num_blocks,
                                         parallel_config=parallel_config)
-
         self.expert_num = 1 if moe_config is None else moe_config.expert_num
         ffn = LlamaFeedForward(dim=self.hidden_size,
                                intermediate_size=intermediate_size,
                                hidden_dim=4 * self.hidden_size,
                                multiple_of=multiple_of,
                                expert_num=self.expert_num,
                                ffn_dim_multiplier=ffn_dim_multiplier,
                                compute_dtype=compute_dtype,
                                param_init_type=param_init_type,
-                               is_dynamic=is_dynamic,
-                               parallel_config=parallel_config)
+                               is_dynamic=is_dynamic)
         if self.expert_num == 1:
-            logger.info("MoE config is None, use normal FFN")
+            logger.warning("MoE config is None, use normal FFN")
             self.feed_forward = ffn
         else:
-            logger.info("MoE config is provided, use MoE FFN")
+            logger.warning("MoE config is provided, use MoE FFN")
             self.feed_forward = MoEV2(
                 ffn=ffn,
                 dim=self.hidden_size,
                 moe_config=moe_config,
                 parallel_config=parallel_config)
 
         dp = parallel_config.data_parallel
         mp = parallel_config.model_parallel
         if not (_get_parallel_mode() in (ParallelMode.AUTO_PARALLEL,) and _is_sharding_propagation()):
             if self.expert_num == 1:
                 self.feed_forward.shard(parallel_config)
             else:
                 self.feed_forward.ffn.shard(parallel_config)
             self.add.shard(((dp, 1, 1), (dp, 1, 1)))
-            self.batch_valid_length_add.shard(((dp,), ()))
             self.attention_norm.shard((dp, 1, 1))
             self.ffn_norm.shard((dp, 1, 1))
             if moe_config is None or not moe_config.expert_num > 1:
                 self.feed_forward.mul.shard(((dp, 1, mp), (dp, 1, mp)))
 
         if parallel_config.use_seq_parallel and self.is_first_iteration:
             self.add.shard(((dp, mp, 1), (dp, mp, 1)))
             self.attention_norm.shard((dp, mp, 1))
             self.ffn_norm.shard((dp, mp, 1))
             if moe_config is None or not moe_config.expert_num > 1:
                 self.feed_forward.w2.shard(((dp, mp), (1, mp)), out_strategy_matmul=((dp * mp, 1),))
 
-    def construct(self, x, freqs_cis, mask=None, batch_valid_length=None, block_tables=None, slot_mapping=None):
+        if parallel_config.recompute.select_recompute:
+            self.feed_forward.mul.recompute()
+            self.feed_forward.w1.activation.silu.recompute()
+            self.attention_norm.cast.recompute()
+            self.ffn_norm.cast.recompute()
+
+    def construct(self, x, freqs_cis, mask=None, kvcache_inputs=None):
         """ Forward of transformer block. """
-        if not self.use_past:
-            self._check_input(x, freqs_cis, mask)
-        if batch_valid_length is not None:
-            batch_valid_length = self.batch_valid_length_add(batch_valid_length, 1)
+        self._check_input(x, freqs_cis, mask)
         # [bs, seq/1, hidden_dim]
         input_x = self.attention_norm(x)
         # [bs, seq/1, hidden_dim]
-        h = self.attention(input_x, freqs_cis, mask, batch_valid_length, block_tables, slot_mapping)
+        h = self.attention(input_x, freqs_cis, mask, kvcache_inputs)
         h = self.add(x, h)
         ffn_norm = self.ffn_norm(h)
         # [bs, seq/1, hidden_dim]
         ffn_out = self.feed_forward(ffn_norm)
         # [bs, seq/1, hidden_dim] or [bs * seq/1, hidden_dim]
         out = self.add(h, ffn_out)
         return out
@@ -524,10 +621,9 @@
         _check_input_dtype(freqs_sin.dtype, "freqs_sin",
                            [mstype.float32, mstype.float16, mstype.bfloat16], self.cls_name)
         if swap_mask is not None:
             _check_input_dtype(swap_mask.dtype, "swap_mask",
                                [mstype.float32, mstype.float16, mstype.bfloat16], self.cls_name)
         if mask is not None:
             _check_input_dtype(mask.dtype, "input_mask",
-                               [mstype.float32, mstype.float16, mstype.bfloat16, mstype.uint8, mstype.bool_],
-                               self.cls_name)
+                               [mstype.float32, mstype.float16, mstype.bfloat16, mstype.uint8], self.cls_name)
         return True
```

## mindformers/models/sam/sam.py

```diff
@@ -14,15 +14,15 @@
 # ============================================================================
 """SAM Model"""
 from typing import Tuple
 
 import mindspore as ms
 import mindspore.ops as ops
 
-from mindformers.models.build_model import build_network
+from mindformers.models.build_model import build_model
 from mindformers.tools.register import MindFormerRegister, MindFormerModuleType
 from mindformers.mindformer_book import MindFormerBook
 from mindformers.models.modeling_utils import PreTrainedModel
 from .sam_config import SamConfig
 
 
 class SamPreTrainedModel(PreTrainedModel):
@@ -43,17 +43,17 @@
     """
     mask_threshold: float = 0.0
     image_format: str = "RGB"
 
     _support_list = MindFormerBook.get_model_support_list()['sam']
     def __init__(self, config) -> None:
         super().__init__(config)
-        self.image_encoder = build_network(config.image_encoder)
-        self.prompt_encoder = build_network(config.prompt_config)
-        self.mask_decoder = build_network(config.decoder_config)
+        self.image_encoder = build_model(config.image_encoder)
+        self.prompt_encoder = build_model(config.prompt_config)
+        self.mask_decoder = build_model(config.decoder_config)
 
         self.load_checkpoint(config)
 
     def construct(self,
                   image=None,
                   features=None,
                   input_size=None,
```

## mindformers/modules/kvcache_mgr.py

```diff
@@ -196,19 +196,22 @@
 class KVCachePreprocess(nn.Cell):
     """KVCache Manager."""
     def __init__(self,
                  max_batch_size=8,
                  max_seq_length=4096,
                  is_dynamic=False,
                  use_kvcache_op=False,
-                 is_flexible_shape=False):
+                 is_flexible_shape=False,
+                 use_paged_attention=False
+                 ):
         super().__init__()
         self.is_dynamic = is_dynamic
         self.use_kvcache_op = use_kvcache_op
         self.is_flexible_shape = is_flexible_shape
+        self.use_paged_attention = use_paged_attention
 
         self.max_cache_length = max_batch_size * max_seq_length
         range_len = self.max_cache_length if self.is_flexible_shape else max_seq_length
         self.range = Tensor(np.arange(range_len).reshape((1, 1, -1)), mstype.int32)
         self.cache_length_tensor = Tensor([max_batch_size * max_seq_length], dtype=mstype.int32)
         self.cache_pad_tensor = Tensor([3], dtype=mstype.int64)
         self.seq_length_tensor = Tensor([max_seq_length], dtype=mstype.int32)
@@ -219,16 +222,22 @@
         self.reshape = P.Reshape().add_prim_attr("skip_redistribution", True)
         self.equal = P.Equal().shard(((1, 1, 1), (1, 1, 1)))
         self.less = P.Less().shard(((1, 1, 1), (1, 1, 1)))
         self.expand_dims = P.ExpandDims().shard(((1, 1, 1),))
         self.div = P.Div()
         self.concat = P.Concat(axis=0)
 
-    def construct(self, batch_size, batch_valid_length=None, batch_index=None, zactivate_len=None):
+    def construct(self, batch_size, batch_valid_length=None, batch_index=None, zactivate_len=None,
+                  block_tables=None, slot_mapping=None):
         """precompute kvcache inputs"""
+        if self.use_paged_attention:
+            cur_pos = batch_valid_length + 1
+            kvcache_inputs = (cur_pos, block_tables, slot_mapping)
+            return kvcache_inputs
+
         seq_range = self.range
         if self.is_dynamic and self.is_flexible_shape and not self.use_kvcache_op:
             seq_range = self.slice(seq_range, (0, 0, 0), (1, 1, self.max_cache_length // batch_size), (1, 1, 1))
 
         if self.use_kvcache_op:
             if batch_index is None:
                 batch_index = ops.arange(0, batch_size, 1)
```

## mindformers/modules/layers.py

```diff
@@ -28,15 +28,14 @@
 from mindspore.common.initializer import initializer, Tensor
 import mindspore.common.dtype as mstype
 from mindspore._extends import cell_attr_register
 from mindspore.nn.cell import Cell
 from mindspore.ops import functional as F
 from mindspore.ops import operations as P
 from mindspore.ops.primitive import constexpr
-
 # MindSpore 2.0 has changed the APIs of _checkparam, the following try except is for compatibility
 try:
     from mindspore._checkparam import Validator
 except ImportError:
     import mindspore._checkparam as Validator
 from mindspore.parallel._utils import _get_parallel_mode, _is_sharding_propagation
 from mindspore.context import ParallelMode
@@ -46,15 +45,14 @@
 from mindformers.modules.transformer.op_parallel_config import default_dpmp_config, OpParallelConfig, MoEParallelConfig
 
 __all__ = [
     "FixedSparseAttention",
     "Dropout",
     "LayerNorm",
     "Linear",
-    "AlibiTensor",
     "AlibiTensorV2"
 ]
 
 
 def _args_type_validator_check(*type_args, **type_kwargs):
     """Check whether input data type is correct."""
 
@@ -788,15 +786,14 @@
             self.reshape(
                 v,
                 (-1, 16, self.num_heads * self.size_per_head // 16, 16)),
             (0, 2, 3, 1))
 
         return q, k, v
 
-
 class AlibiTensor(nn.Cell):
     """
     Link to paper: https://arxiv.org/abs/2108.12409 Alibi tensor is not causal as the original paper mentions, it
     relies on a translation invariance of softmax for quick implementation: with l being a tensor, and a fixed value
     `softmax(l+a) = softmax(l)`. Based on
     https://github.com/ofirpress/attention_with_linear_biases/blob/a35aaca144e0eb6b789dfcb46784c4b8e31b7983/fairseq/models/transformer.py#L742
 
@@ -837,27 +834,27 @@
             extra_base = np.array(
                 2 ** (-(2 ** -(math.log2(2 * closest_power_of_2) - 3))), dtype=np.float32
             )
             num_remaining_heads = min(closest_power_of_2, num_heads - closest_power_of_2)
             extra_powers = np.arange(1, 1 + 2 * num_remaining_heads, 2, dtype=np.int32)
             slopes = np.concatenate([slopes, np.power(extra_base, extra_powers)], axis=0)
 
-        self.slopes = Tensor(slopes[:, None], mstype.float32)  # (num_heads, 1)
+        self.slopes = Tensor(slopes[:, None], mstype.float32) # (num_heads, 1)
 
     def construct(self, attention_mask, dtype):
         """
         Note: alibi will added to the attention bias that will be applied to the query, key product of attention
         therefore alibi will have to be of shape (batch_size, num_heads, query_length, key_length)
         """
-        arange_tensor = self.cumsum(attention_mask, -1)  # (batch_size, seq_len)
-        arange_tensor = self.add(arange_tensor, self.minus_one)  # (batch_size, seq_len)
-        arange_tensor = self.mul(arange_tensor, attention_mask)  # (batch_size, seq_len)
-        arange_tensor = self.expand_2d(arange_tensor, 1)  # (batch_size, 1, seq_len)
-        alibi = self.mul_slope(self.slopes, arange_tensor)  # (batch_size, num_heads, seq_len)
-        alibi = self.expand_3d(alibi, 2).astype(dtype)  # (batch_size, num_heads, 1, seq_len)
+        arange_tensor = self.cumsum(attention_mask, -1) # (batch_size, seq_len)
+        arange_tensor = self.add(arange_tensor, self.minus_one) # (batch_size, seq_len)
+        arange_tensor = self.mul(arange_tensor, attention_mask) # (batch_size, seq_len)
+        arange_tensor = self.expand_2d(arange_tensor, 1) # (batch_size, 1, seq_len)
+        alibi = self.mul_slope(self.slopes, arange_tensor) # (batch_size, num_heads, seq_len)
+        alibi = self.expand_3d(alibi, 2).astype(dtype) # (batch_size, num_heads, 1, seq_len)
         return alibi
 
 
 class AlibiTensorV2(nn.Cell):
     """
     Link to paper: https://arxiv.org/abs/2108.12409 Alibi tensor is not causal as the original paper mentions, it
     relies on a translation invariance of softmax for quick implementation: with l being a tensor, and a fixed value
@@ -872,17 +869,20 @@
         attention_mask(Tensor) - Token-wise attention mask, this should be of shape (batch_size, max_seq_len).
         dtype(mstype) - dtype of the output tensor
 
     Returns:
         alibi(Tensor), ailibi tensor shaped (batch_size, num_heads, 1, max_seq_len)
     """
 
-    def __init__(self, num_heads):
+    def __init__(self, seq_length, num_heads):
         super(AlibiTensorV2, self).__init__()
+
+        self.seq_length = seq_length
         self.num_heads = num_heads
+        self.minus_one = Tensor(-np.ones(seq_length), mstype.float32)
 
         self.expand_2d = P.ExpandDims()
         self.expand_3d = P.ExpandDims()
         self.cumsum = P.CumSum()
         self.add_2d = P.Add()
         self.add_3d = P.Add()
         self.mul = P.Mul()
@@ -901,32 +901,32 @@
             extra_base = np.array(
                 2 ** (-(2 ** -(math.log2(2 * closest_power_of_2) - 3))), dtype=np.float32
             )
             num_remaining_heads = min(closest_power_of_2, num_heads - closest_power_of_2)
             extra_powers = np.arange(1, 1 + 2 * num_remaining_heads, 2, dtype=np.int32)
             slopes = np.concatenate([slopes, np.power(extra_base, extra_powers)], axis=0)
 
-        self.slopes = Tensor(slopes[None, :, None, None], mstype.float32)  # (num_heads, 1)
+        self.slopes = Tensor(slopes[None, :, None, None], mstype.float32) # (num_heads, 1)
 
     def construct(self, attention_mask, dtype=mstype.float32):
         """
         Note: alibi will added to the attention bias that will be applied to the query, key product of attention
         therefore alibi will have to be of shape (batch_size, num_heads, query_length, key_length)
         """
         bs, seqlen = attention_mask.shape
-        arange_tensor = self.cumsum(attention_mask, -1)  # (batch_size, seq_len)
-        max_pos = -arange_tensor[:, -1:]  # (batch_size, 1)
-        arange_tensor = self.add_2d(arange_tensor, max_pos)  # (batch_size, seq_len)
-        arange_tensor = self.expand_2d(arange_tensor, 1)  # (batch_size, 1, seq_len)
-        diag = -self.transpose(arange_tensor, (0, 2, 1))  # (batch_size, seq_len, 1)
-        arange_tensor = self.add_3d(arange_tensor, diag)  # (batch_size, seq_len, seq_len)
-        arange_tensor = self.expand_3d(arange_tensor, 1)  # (batch_size, 1, seq_len, seq_len)
-        alibi = self.mul_slope(self.slopes, arange_tensor)  # (batch_size, num_heads, seq_len, seq_len)
-        alibi_mask = self.mul_mask(alibi, self.reshape(attention_mask, (bs, 1, seqlen, 1)))  # (batch_size, num_heads, seq_len, seq_len)
-        alibi_mask = self.mul_mask(alibi_mask, self.reshape(attention_mask, (bs, 1, 1, seqlen)))  # (batch_size, num_heads, seq_len, seq_len)
+        arange_tensor = self.cumsum(attention_mask, -1) # (batch_size, seq_len)
+        max_pos = -arange_tensor[:, -1:] # (batch_size, 1)
+        arange_tensor = self.add_2d(arange_tensor, max_pos) # (batch_size, seq_len)
+        arange_tensor = self.expand_2d(arange_tensor, 1) # (batch_size, 1, seq_len)
+        diag = -self.transpose(arange_tensor, (0, 2, 1)) # (batch_size, seq_len, 1)
+        arange_tensor = self.add_3d(arange_tensor, diag)    # (batch_size, seq_len, seq_len)
+        arange_tensor = self.expand_3d(arange_tensor, 1) # (batch_size, 1, seq_len, seq_len)
+        alibi = self.mul_slope(self.slopes, arange_tensor) # (batch_size, num_heads, seq_len, seq_len)
+        alibi_mask = self.mul_mask(alibi, self.reshape(attention_mask, (bs, 1, seqlen, 1))) # (batch_size, num_heads, seq_len, seq_len)
+        alibi_mask = self.mul_mask(alibi_mask, self.reshape(attention_mask, (bs, 1, 1, seqlen))) # (batch_size, num_heads, seq_len, seq_len)
         return alibi_mask.astype(dtype)
 
     def shard(self, parallel_config):
         """Parallel strategy configuratiuon interface."""
         dp = parallel_config.data_parallel
         mp = parallel_config.model_parallel
 
@@ -939,32 +939,31 @@
         self.mul_slope.shard(((1, 1, 1, 1), (dp, 1, 1, 1)))
         self.transpose.shard(((dp, 1, 1),))
         self.mul_mask.shard(((dp, mp, 1, 1), (dp, 1, 1, 1)))
 
 
 def _get_interleave(n):
     """calculate slopes of alibi tensor"""
-
     def _get_interleave_power_of_2(n):
         start = (2 ** (-2 ** -(math.log2(n) - 3)))
         ratio = start
         return [start * ratio ** i for i in range(n)]
 
     if math.log2(n).is_integer():
         return _get_interleave_power_of_2(n)
 
     closest_power_of_2 = 2 ** math.floor(math.log2(n))
     return _get_interleave_power_of_2(closest_power_of_2) + \
-        _get_interleave(2 * closest_power_of_2)[0::2][:n - closest_power_of_2]
+           _get_interleave(2 * closest_power_of_2)[0::2][:n - closest_power_of_2]
 
 
 def build_alibi_tensor_v2(seq_len, num_heads, return_tensors='ms', dtype=mstype.float32):
     """build alibi tensor"""
-    assert return_tensors in ['np', 'ms'], \
-        f"return tensors must be 'np' or 'ms', {return_tensors} not support."
+    assert return_tensors in ['np', 'ms'],\
+           f"return tensors must be 'np' or 'ms', {return_tensors} not support."
     slopes = _get_interleave(num_heads)
     slopes = np.expand_dims(np.expand_dims(slopes, 1), 1)
     position_point = np.arange(seq_len) - seq_len + 1
     position_point = np.expand_dims(np.expand_dims(position_point, 0), 0)
     position_point = np.tile(position_point, (num_heads, seq_len, 1))
     diag = np.diag(position_point[0])
     diag = np.expand_dims(np.expand_dims(diag, 0), 0)
```

## mindformers/modules/paged_attention_mgr.py

```diff
@@ -12,67 +12,95 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ============================================================================
 
 """Paged Attention Manager for inference."""
 from typing import Optional
 import math
+import numpy as np
 
-import mindspore.common.dtype as mstype
 from mindspore import nn, Parameter
 from mindspore.common.tensor import Tensor
+import mindspore.common.dtype as mstype
 from mindspore import ops as P
-from mindspore.common.initializer import Zero
-
+from mindformers.version_control import is_paged_attention_v2
 
 class PagedAttentionMgr(nn.Cell):
     """Paged Attention Manager."""
 
     def __init__(self,
                  n_heads,
                  head_dim,
+                 hidden_size,
                  n_kv_heads: Optional[int] = None,
                  block_size=16,
                  num_blocks=256,
-                 compute_dtype=mstype.float16):
+                 compute_dtype=mstype.float16,
+                 ):
         super().__init__()
-        self.n_heads = n_heads
+        self.is_first_iteration = True
+        self.n_head = n_heads
         self.head_dim = head_dim
+        self.hidden_size = hidden_size
         self.n_kv_heads = n_kv_heads
+        self.num_blocks = num_blocks
         self.block_size = block_size
         self.num_blocks = num_blocks
+        self.dtype = compute_dtype
 
         self.scale_value = 1 / math.sqrt(self.head_dim)
 
-        kv_shape = (self.num_blocks, self.block_size, self.n_kv_heads, self.head_dim)
-        self.key_cache = Parameter(Tensor(shape=kv_shape, dtype=compute_dtype, init=Zero()), name="key_cache",
-                                   requires_grad=False)
-        self.value_cache = Parameter(Tensor(shape=kv_shape, dtype=compute_dtype, init=Zero()), name="value_cache",
-                                     requires_grad=False)
-
-        self.reshape_and_cache = P.auto_generate.ReshapeAndCache()
-        self.paged_attention = P.auto_generate.PagedAttention(self.n_heads, self.scale_value,
-                                                              self.n_kv_heads)
-        self.paged_attention_with_alibi = P.auto_generate.PagedAttentionMask(self.n_heads,
-                                                                             self.scale_value,
-                                                                             self.n_kv_heads)
+        self.reshape = P.Reshape().add_prim_attr("skip_redistribution", True)
+        self.transpose = P.Transpose()
+
+        kv_shape = (num_blocks, self.block_size, self.n_kv_heads, self.head_dim)
+        self.key_cache = Parameter(Tensor(np.zeros(kv_shape), compute_dtype), name="key_cache", requires_grad=False)
+        self.value_cache = Parameter(Tensor(np.zeros(kv_shape), compute_dtype), name="value_cache", requires_grad=False)
+        if is_paged_attention_v2():
+            self.reshape_and_cache = P.auto_generate.ReshapeAndCache()
+            self.paged_attention = P.auto_generate.PagedAttention(self.n_head, self.scale_value,
+                                                                  self.n_kv_heads)
+            self.paged_attention_with_alibi = P.auto_generate.PagedAttentionMask(self.n_head,
+                                                                                 self.scale_value,
+                                                                                 self.n_kv_heads)
+        else:
+            self.reshape_and_cache = P.operations.nn_ops.ReshapeAndCache()
+            self.paged_attention = P.operations.PagedAttention(self.n_head, self.scale_value, self.n_kv_heads)
+            self.paged_attention_with_alibi = P.operations.PagedAttentionMask(self.n_head, self.scale_value,
+                                                                              self.n_kv_heads)
 
     def construct(self, key, value, slot_mapping):
         """The forward compute of KVCache for Paged Attention."""
-        return self.reshape_and_cache(key, value, self.key_cache, self.value_cache, slot_mapping)
+        _, _, n_kv_head, head_dim = self.key_cache.shape
+        tmp_key = self.transpose(key, (0, 2, 1, 3))  # [b, n, s/1, d] -> [b, s/1, n, d]
+        tmp_key = self.reshape(tmp_key, (-1, n_kv_head, head_dim))
+
+        tmp_value = self.transpose(value, (0, 2, 1, 3))  # [b, n, s/1, d] -> [b, s/1, n, d]
+        tmp_value = self.reshape(tmp_value, (-1, n_kv_head, head_dim))
+
+        key_out = self.reshape_and_cache(tmp_key, tmp_value, self.key_cache, self.value_cache, slot_mapping)
+
+        return key_out
 
     def paged_attn(self, query, batch_valid_length, block_tables):
         """The forward compute of Paged Attention."""
-        return self.paged_attention(query, self.key_cache, self.value_cache, block_tables, batch_valid_length)
+        query_pa = self.reshape(self.transpose(query, (0, 2, 1, 3)), (-1, self.n_head, self.head_dim))
+        pa_out = self.paged_attention(query_pa, self.key_cache, self.value_cache, block_tables, batch_valid_length)
+        attention = self.reshape(pa_out, (-1, 1, self.hidden_size))
+        return attention
 
     def paged_attn_with_alibi(self, query, batch_valid_length, block_tables, alibi_tensor):
         """The forward compute of KVCache for Paged Attention with alibi tensor."""
-        return self.paged_attention_with_alibi(query, self.key_cache, self.value_cache,
-                                               block_tables, batch_valid_length, alibi_tensor)
+        query_pa = self.reshape(self.transpose(query, (0, 2, 1, 3)), (-1, self.n_head, self.head_dim))
+        pa_out = self.paged_attention_with_alibi(query_pa, self.key_cache, self.value_cache,
+                                                 block_tables, batch_valid_length, alibi_tensor)
+        attention = self.reshape(pa_out, (-1, 1, self.hidden_size))
+        return attention
 
     def shard(self, parallel_config):
         """The shard strategy."""
-        dp = 1 if parallel_config is None else parallel_config.data_parallel
-        mp = 1 if parallel_config is None else parallel_config.model_parallel
-        self.reshape_and_cache.shard(((dp, 1, mp), (dp, 1, mp), (1, 1, mp, 1), (1, 1, mp, 1), (1,)))
-        self.paged_attention.shard(((dp, 1, mp), (1, 1, mp, 1), (1, 1, mp, 1), (dp, 1), (dp,)))
-        self.paged_attention_with_alibi.shard(((dp, 1, mp), (1, 1, mp, 1), (1, 1, mp, 1), (dp, 1), (dp,)))
+        dp = parallel_config.data_parallel
+        mp = parallel_config.model_parallel
+        self.transpose.shard(((dp, mp, 1, 1),))
+        self.reshape_and_cache.shard(((dp, mp, 1), (dp, mp, 1), (1, 1, mp, 1), (1, 1, mp, 1), (1,)))
+        self.paged_attention.shard(((dp, mp, 1), (1, 1, mp, 1), (1, 1, mp, 1), (dp, 1), (dp,)))
+        self.paged_attention_with_alibi.shard(((dp, mp, 1), (1, 1, mp, 1), (1, 1, mp, 1), (dp, 1), (dp,)))
```

## mindformers/modules/transformer/moe.py

```diff
@@ -50,4490 +50,3927 @@
 00000310: 0a22 2222 0a66 726f 6d20 5f5f 6675 7475  .""".from __futu
 00000320: 7265 5f5f 2069 6d70 6f72 7420 6162 736f  re__ import abso
 00000330: 6c75 7465 5f69 6d70 6f72 740a 6672 6f6d  lute_import.from
 00000340: 205f 5f66 7574 7572 655f 5f20 696d 706f   __future__ impo
 00000350: 7274 2064 6976 6973 696f 6e0a 0a69 6d70  rt division..imp
 00000360: 6f72 7420 6d61 7468 0a69 6d70 6f72 7420  ort math.import 
 00000370: 636f 7079 0a69 6d70 6f72 7420 6e75 6d70  copy.import nump
-00000380: 7920 6173 206e 700a 696d 706f 7274 206d  y as np.import m
-00000390: 696e 6473 706f 7265 2e6f 7073 2061 7320  indspore.ops as 
-000003a0: 6f70 730a 0a66 726f 6d20 6d69 6e64 7370  ops..from mindsp
-000003b0: 6f72 652e 636f 6d6d 6f6e 2e74 656e 736f  ore.common.tenso
-000003c0: 7220 696d 706f 7274 2054 656e 736f 720a  r import Tensor.
-000003d0: 6672 6f6d 206d 696e 6473 706f 7265 2e63  from mindspore.c
-000003e0: 6f6d 6d6f 6e2e 7061 7261 6d65 7465 7220  ommon.parameter 
-000003f0: 696d 706f 7274 2050 6172 616d 6574 6572  import Parameter
-00000400: 0a66 726f 6d20 6d69 6e64 7370 6f72 652e  .from mindspore.
-00000410: 636f 6d6d 6f6e 2e69 6e69 7469 616c 697a  common.initializ
-00000420: 6572 2069 6d70 6f72 7420 696e 6974 6961  er import initia
-00000430: 6c69 7a65 720a 696d 706f 7274 206d 696e  lizer.import min
-00000440: 6473 706f 7265 2e63 6f6d 6d6f 6e2e 6474  dspore.common.dt
-00000450: 7970 6520 6173 206d 7374 7970 650a 696d  ype as mstype.im
-00000460: 706f 7274 206d 696e 6473 706f 7265 2e63  port mindspore.c
-00000470: 6f6d 6d75 6e69 6361 7469 6f6e 2e6d 616e  ommunication.man
-00000480: 6167 656d 656e 7420 6173 2044 0a0a 2320  agement as D..# 
-00000490: 4d69 6e64 5370 6f72 6520 322e 3020 6861  MindSpore 2.0 ha
-000004a0: 7320 6368 616e 6765 6420 7468 6520 4150  s changed the AP
-000004b0: 4973 206f 6620 5f63 6865 636b 7061 7261  Is of _checkpara
-000004c0: 6d2c 2074 6865 2066 6f6c 6c6f 7769 6e67  m, the following
-000004d0: 2074 7279 2065 7863 6570 7420 6973 2066   try except is f
-000004e0: 6f72 2063 6f6d 7061 7469 6269 6c69 7479  or compatibility
-000004f0: 0a74 7279 3a0a 2020 2020 6672 6f6d 206d  .try:.    from m
-00000500: 696e 6473 706f 7265 2e5f 6368 6563 6b70  indspore._checkp
-00000510: 6172 616d 2069 6d70 6f72 7420 5661 6c69  aram import Vali
-00000520: 6461 746f 720a 6578 6365 7074 2049 6d70  dator.except Imp
-00000530: 6f72 7445 7272 6f72 3a0a 2020 2020 696d  ortError:.    im
-00000540: 706f 7274 206d 696e 6473 706f 7265 2e5f  port mindspore._
-00000550: 6368 6563 6b70 6172 616d 2061 7320 5661  checkparam as Va
-00000560: 6c69 6461 746f 720a 6672 6f6d 206d 696e  lidator.from min
-00000570: 6473 706f 7265 2e6f 7073 2069 6d70 6f72  dspore.ops impor
-00000580: 7420 6f70 6572 6174 696f 6e73 2061 7320  t operations as 
-00000590: 500a 6672 6f6d 206d 696e 6473 706f 7265  P.from mindspore
-000005a0: 2e6f 7073 2069 6d70 6f72 7420 6675 6e63  .ops import func
-000005b0: 7469 6f6e 616c 2061 7320 460a 6672 6f6d  tional as F.from
-000005c0: 206d 696e 6473 706f 7265 2e6f 7073 2e70   mindspore.ops.p
-000005d0: 7269 6d69 7469 7665 2069 6d70 6f72 7420  rimitive import 
-000005e0: 636f 6e73 7465 7870 720a 6672 6f6d 206d  constexpr.from m
-000005f0: 696e 6473 706f 7265 2e6e 6e2e 6365 6c6c  indspore.nn.cell
-00000600: 2069 6d70 6f72 7420 4365 6c6c 0a66 726f   import Cell.fro
-00000610: 6d20 6d69 6e64 7370 6f72 652e 6e6e 2e6c  m mindspore.nn.l
-00000620: 6179 6572 2069 6d70 6f72 7420 4465 6e73  ayer import Dens
-00000630: 650a 6672 6f6d 206d 696e 6473 706f 7265  e.from mindspore
-00000640: 2e63 6f6e 7465 7874 2069 6d70 6f72 7420  .context import 
-00000650: 5061 7261 6c6c 656c 4d6f 6465 0a66 726f  ParallelMode.fro
-00000660: 6d20 6d69 6e64 7370 6f72 652e 7061 7261  m mindspore.para
-00000670: 6c6c 656c 2e5f 7574 696c 7320 696d 706f  llel._utils impo
-00000680: 7274 205f 6765 745f 7061 7261 6c6c 656c  rt _get_parallel
-00000690: 5f6d 6f64 652c 205f 6973 5f73 6861 7264  _mode, _is_shard
-000006a0: 696e 675f 7072 6f70 6167 6174 696f 6e0a  ing_propagation.
-000006b0: 6672 6f6d 206d 696e 6466 6f72 6d65 7273  from mindformers
-000006c0: 2e6d 6f64 756c 6573 2e74 7261 6e73 666f  .modules.transfo
-000006d0: 726d 6572 2e6f 705f 7061 7261 6c6c 656c  rmer.op_parallel
-000006e0: 5f63 6f6e 6669 6720 696d 706f 7274 2064  _config import d
-000006f0: 6566 6175 6c74 5f6d 6f65 7061 7261 6c6c  efault_moeparall
-00000700: 656c 5f63 6f6e 6669 672c 204d 6f45 5061  el_config, MoEPa
-00000710: 7261 6c6c 656c 436f 6e66 6967 0a0a 5f5f  rallelConfig..__
-00000720: 616c 6c5f 5f20 3d20 5b0a 2020 2020 224d  all__ = [.    "M
-00000730: 6f45 436f 6e66 6967 225d 0a0a 6474 7970  oEConfig"]..dtyp
-00000740: 655f 6d61 7020 3d20 7b0a 2020 2020 2766  e_map = {.    'f
-00000750: 6c6f 6174 3136 273a 206d 7374 7970 652e  loat16': mstype.
-00000760: 666c 6f61 7433 322c 0a20 2020 2027 666c  float32,.    'fl
-00000770: 6f61 7433 3227 3a20 6d73 7479 7065 2e66  oat32': mstype.f
-00000780: 6c6f 6174 3332 2c0a 2020 2020 2762 666c  loat32,.    'bfl
-00000790: 6f61 7431 3627 3a20 6d73 7479 7065 2e62  oat16': mstype.b
-000007a0: 666c 6f61 7431 360a 7d0a 0a0a 636c 6173  float16.}...clas
-000007b0: 7320 4d6f 4543 6f6e 6669 673a 0a20 2020  s MoEConfig:.   
-000007c0: 2072 2222 220a 2020 2020 2020 2020 5468   r""".        Th
-000007d0: 6520 636f 6e66 6967 7572 6174 696f 6e20  e configuration 
-000007e0: 6f66 204d 6f45 2028 4d69 7874 7572 6520  of MoE (Mixture 
-000007f0: 6f66 2045 7870 6572 7429 2e0a 0a20 2020  of Expert)...   
-00000800: 2020 2020 2041 7267 733a 0a20 2020 2020       Args:.     
-00000810: 2020 2020 2020 2065 7870 6572 745f 6e75         expert_nu
-00000820: 6d20 2869 6e74 293a 2054 6865 206e 756d  m (int): The num
-00000830: 6265 7220 6f66 2065 7870 6572 7473 2065  ber of experts e
-00000840: 6d70 6c6f 7965 642e 2044 6566 6175 6c74  mployed. Default
-00000850: 3a20 310a 2020 2020 2020 2020 2020 2020  : 1.            
-00000860: 6361 7061 6369 7479 5f66 6163 746f 7220  capacity_factor 
-00000870: 2866 6c6f 6174 293a 2054 6865 2066 6163  (float): The fac
-00000880: 746f 7220 6973 2075 7365 6420 746f 2069  tor is used to i
-00000890: 6e64 6963 6174 6520 686f 7720 6d75 6368  ndicate how much
-000008a0: 2074 6f20 6578 7061 6e64 2065 7870 6572   to expand exper
-000008b0: 7420 6361 7061 6369 7479 2c0a 2020 2020  t capacity,.    
-000008c0: 2020 2020 2020 2020 2020 2020 7768 6963              whic
-000008d0: 6820 6973 203e 3d31 2e30 2e20 4465 6661  h is >=1.0. Defa
-000008e0: 756c 743a 2031 2e31 2e0a 2020 2020 2020  ult: 1.1..      
-000008f0: 2020 2020 2020 6175 785f 6c6f 7373 5f66        aux_loss_f
-00000900: 6163 746f 7220 2866 6c6f 6174 293a 2054  actor (float): T
-00000910: 6865 2066 6163 746f 7220 6973 2075 7365  he factor is use
-00000920: 6420 746f 2069 6e64 6963 6174 6520 686f  d to indicate ho
-00000930: 7720 6d75 6368 2074 6865 206c 6f61 6420  w much the load 
-00000940: 6261 6c61 6e63 6520 6c6f 7373 2028 7072  balance loss (pr
-00000950: 6f64 7563 6564 2062 7920 7468 650a 2020  oduced by the.  
-00000960: 2020 2020 2020 2020 2020 2020 2020 726f                ro
-00000970: 7574 6572 2920 746f 2062 6520 6164 6465  uter) to be adde
-00000980: 6420 746f 2074 6865 2065 6e74 6972 6520  d to the entire 
-00000990: 6d6f 6465 6c20 6c6f 7373 2c20 7768 6963  model loss, whic
-000009a0: 6820 6973 203c 2031 2e30 2e20 4465 6661  h is < 1.0. Defa
-000009b0: 756c 743a 2030 2e30 352e 0a20 2020 2020  ult: 0.05..     
-000009c0: 2020 2020 2020 206e 756d 5f65 7870 6572         num_exper
-000009d0: 7473 5f63 686f 7365 6e20 2869 6e74 293a  ts_chosen (int):
-000009e0: 2054 6865 206e 756d 6265 7220 6f66 2065   The number of e
-000009f0: 7870 6572 7473 2069 7320 6368 6f73 656e  xperts is chosen
-00000a00: 2062 7920 6561 6368 2074 6f6b 656e 2061   by each token a
-00000a10: 6e64 2069 7420 7368 6f75 6c64 206e 6f74  nd it should not
-00000a20: 2062 6520 6c61 7267 6572 0a20 2020 2020   be larger.     
-00000a30: 2020 2020 2020 2020 2020 2074 6861 6e20             than 
-00000a40: 6578 7065 7274 5f6e 756d 2e20 4465 6661  expert_num. Defa
-00000a50: 756c 743a 2031 2e0a 2020 2020 2020 2020  ult: 1..        
-00000a60: 2020 2020 6578 7065 7274 5f67 726f 7570      expert_group
-00000a70: 5f73 697a 6520 2869 6e74 293a 2054 6865  _size (int): The
-00000a80: 206e 756d 6265 7220 6f66 2074 6f6b 656e   number of token
-00000a90: 7320 696e 2065 6163 6820 6461 7461 2070  s in each data p
-00000aa0: 6172 616c 6c65 6c20 6772 6f75 702e 2044  arallel group. D
-00000ab0: 6566 6175 6c74 3a20 4e6f 6e65 2e20 5468  efault: None. Th
-00000ac0: 6973 2070 6172 616d 6574 6572 2069 730a  is parameter is.
-00000ad0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000ae0: 6566 6665 6374 6976 6520 6f6e 6c79 2077  effective only w
-00000af0: 6865 6e20 696e 2041 5554 4f5f 5041 5241  hen in AUTO_PARA
-00000b00: 4c4c 454c 206d 6f64 652c 2061 6e64 204e  LLEL mode, and N
-00000b10: 4f54 2053 4841 5244 494e 475f 5052 4f50  OT SHARDING_PROP
-00000b20: 4147 4154 494f 4e2e 0a20 2020 2020 2020  AGATION..       
-00000b30: 2020 2020 2067 726f 7570 5f77 6973 655f       group_wise_
-00000b40: 6132 6120 2862 6f6f 6c29 3a20 5768 6574  a2a (bool): Whet
-00000b50: 6865 7220 746f 2065 6e61 626c 6520 6772  her to enable gr
-00000b60: 6f75 702d 7769 7365 2061 6c6c 746f 616c  oup-wise alltoal
-00000b70: 6c20 636f 6d6d 756e 6963 6174 696f 6e2c  l communication,
-00000b80: 2077 6869 6368 2063 616e 2072 6564 7563   which can reduc
-00000b90: 6520 636f 6d6d 756e 6963 6174 696f 6e0a  e communication.
-00000ba0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000bb0: 7469 6d65 2062 7920 636f 6e76 6572 7469  time by converti
-00000bc0: 6e67 2070 6172 7420 6f66 2069 6e74 6572  ng part of inter
-00000bd0: 2063 6f6d 6d75 6e69 6361 7469 6f6e 2069   communication i
-00000be0: 6e74 6f20 696e 7472 6120 636f 6d6d 756e  nto intra commun
-00000bf0: 6963 6174 696f 6e2e 2044 6566 6175 6c74  ication. Default
-00000c00: 3a20 4661 6c73 652e 2054 6869 7320 7061  : False. This pa
-00000c10: 7261 6d65 7465 720a 2020 2020 2020 2020  rameter.        
-00000c20: 2020 2020 2020 2020 6973 2065 6666 6563          is effec
-00000c30: 7469 7665 206f 6e6c 7920 7768 656e 206d  tive only when m
-00000c40: 6f64 656c 2070 6172 616c 6c65 6c20 3e20  odel parallel > 
-00000c50: 3120 616e 6420 6461 7461 5f70 6172 616c  1 and data_paral
-00000c60: 6c65 6c20 6571 7561 6c20 746f 2065 7870  lel equal to exp
-00000c70: 6572 7420 7061 7261 6c6c 656c 2e0a 2020  ert parallel..  
-00000c80: 2020 2020 2020 2020 2020 636f 6d70 5f63            comp_c
-00000c90: 6f6d 6d5f 7061 7261 6c6c 656c 2028 626f  omm_parallel (bo
-00000ca0: 6f6c 293a 2057 6865 7468 6572 2074 6f20  ol): Whether to 
-00000cb0: 656e 6162 6c65 2066 666e 2063 6f6d 7075  enable ffn compu
-00000cc0: 7465 2061 6e64 2063 6f6d 6d75 6e69 6361  te and communica
-00000cd0: 7469 6f6e 2070 6172 616c 6c65 6c2c 2077  tion parallel, w
-00000ce0: 6869 6368 2063 616e 2072 6564 7563 6520  hich can reduce 
-00000cf0: 7075 7265 0a20 2020 2020 2020 2020 2020  pure.           
-00000d00: 2020 2020 2063 6f6d 6d75 6e69 6361 7474       communicatt
-00000d10: 696f 6e20 7469 6d65 2062 7920 7370 6c69  ion time by spli
-00000d20: 7474 696e 6720 616e 6420 6f76 6572 6c61  tting and overla
-00000d30: 7070 696e 6720 636f 6d70 7574 6520 616e  pping compute an
-00000d40: 6420 636f 6d6d 756e 6963 6174 696f 6e2e  d communication.
-00000d50: 2044 6566 6175 6c74 3a20 4661 6c73 652e   Default: False.
-00000d60: 0a20 2020 2020 2020 2020 2020 2063 6f6d  .            com
-00000d70: 705f 636f 6d6d 5f70 6172 616c 6c65 6c5f  p_comm_parallel_
-00000d80: 6465 6772 6565 2028 696e 7429 3a20 5468  degree (int): Th
-00000d90: 6520 7370 6c69 7420 6e75 6d62 6572 206f  e split number o
-00000da0: 6620 636f 6d70 7574 6520 616e 6420 636f  f compute and co
-00000db0: 6d6d 756e 6963 6174 696f 6e2e 2054 6865  mmunication. The
-00000dc0: 206c 6172 6765 7220 7468 6520 6e75 6d62   larger the numb
-00000dd0: 6572 732c 0a20 2020 2020 2020 2020 2020  ers,.           
-00000de0: 2020 2020 2074 6865 206d 6f72 6520 6f76       the more ov
-00000df0: 6572 6c61 7020 7468 6572 6520 7769 6c6c  erlap there will
-00000e00: 2062 6520 6275 7420 7769 6c6c 2063 6f6e   be but will con
-00000e10: 7375 6d65 206d 6f72 6520 6d65 6d6f 7279  sume more memory
-00000e20: 2e20 4465 6661 756c 743a 2032 2e20 5468  . Default: 2. Th
-00000e30: 6973 2070 6172 616d 6574 6572 2069 7320  is parameter is 
-00000e40: 6566 6665 6374 6976 650a 2020 2020 2020  effective.      
-00000e50: 2020 2020 2020 2020 2020 6f6e 6c79 2077            only w
-00000e60: 6865 6e20 636f 6d70 5f63 6f6d 6d5f 7061  hen comp_comm_pa
-00000e70: 7261 6c6c 656c 2065 6e61 626c 652e 0a20  rallel enable.. 
-00000e80: 2020 2020 2020 2020 2020 2072 6f75 7469             routi
-00000e90: 6e67 5f70 6f6c 6963 7920 2873 7472 293a  ng_policy (str):
-00000ea0: 2054 6865 2072 6f75 7469 6e67 2070 6f6c   The routing pol
-00000eb0: 6963 7920 746f 2075 7365 2069 6e20 4d6f  icy to use in Mo
-00000ec0: 4520 6c61 7965 722e 2044 6566 6175 6c74  E layer. Default
-00000ed0: 3a20 546f 706b 526f 7574 6572 5631 2e0a  : TopkRouterV1..
-00000ee0: 0a20 2020 2020 2020 2053 7570 706f 7274  .        Support
-00000ef0: 6564 2050 6c61 7466 6f72 6d73 3a0a 2020  ed Platforms:.  
-00000f00: 2020 2020 2020 2020 2020 6060 4173 6365            ``Asce
-00000f10: 6e64 6060 2060 6047 5055 6060 0a0a 2020  nd`` ``GPU``..  
-00000f20: 2020 2020 2020 4578 616d 706c 6573 3a0a        Examples:.
-00000f30: 2020 2020 2020 2020 2020 2020 3e3e 3e20              >>> 
-00000f40: 6672 6f6d 206d 696e 6466 6f72 6d65 7273  from mindformers
-00000f50: 2e6d 6f64 756c 6573 2e74 7261 6e73 666f  .modules.transfo
-00000f60: 726d 6572 2069 6d70 6f72 7420 4d6f 4543  rmer import MoEC
-00000f70: 6f6e 6669 670a 2020 2020 2020 2020 2020  onfig.          
-00000f80: 2020 3e3e 3e20 6d6f 655f 636f 6e66 6967    >>> moe_config
-00000f90: 203d 204d 6f45 436f 6e66 6967 2865 7870   = MoEConfig(exp
-00000fa0: 6572 745f 6e75 6d3d 342c 2063 6170 6163  ert_num=4, capac
-00000fb0: 6974 795f 6661 6374 6f72 3d35 2e30 2c20  ity_factor=5.0, 
-00000fc0: 6175 785f 6c6f 7373 5f66 6163 746f 723d  aux_loss_factor=
-00000fd0: 302e 3035 2c20 6e75 6d5f 6578 7065 7274  0.05, num_expert
-00000fe0: 735f 6368 6f73 656e 3d31 2c0a 2020 2020  s_chosen=1,.    
-00000ff0: 2020 2020 2020 2020 2e2e 2e20 2020 2020          ...     
-00001000: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001010: 2020 2065 7870 6572 745f 6772 6f75 705f     expert_group_
-00001020: 7369 7a65 3d36 342c 2067 726f 7570 5f77  size=64, group_w
-00001030: 6973 655f 6132 613d 5472 7565 2c20 636f  ise_a2a=True, co
-00001040: 6d70 5f63 6f6d 6d5f 7061 7261 6c6c 656c  mp_comm_parallel
-00001050: 3d46 616c 7365 2c0a 2020 2020 2020 2020  =False,.        
-00001060: 2020 2020 2e2e 2e20 2020 2020 2020 2020      ...         
-00001070: 2020 2020 2020 2020 2020 2020 2020 2063                 c
-00001080: 6f6d 705f 636f 6d6d 5f70 6172 616c 6c65  omp_comm_paralle
-00001090: 6c5f 6465 6772 6565 3d32 2c20 726f 7574  l_degree=2, rout
-000010a0: 696e 675f 706f 6c69 6379 3d22 546f 706b  ing_policy="Topk
-000010b0: 526f 7574 6572 5632 2229 0a20 2020 2022  RouterV2").    "
-000010c0: 2222 0a0a 2020 2020 6465 6620 5f5f 696e  ""..    def __in
-000010d0: 6974 5f5f 2873 656c 662c 2065 7870 6572  it__(self, exper
-000010e0: 745f 6e75 6d3d 312c 2063 6170 6163 6974  t_num=1, capacit
-000010f0: 795f 6661 6374 6f72 3d31 2e31 2c20 6175  y_factor=1.1, au
-00001100: 785f 6c6f 7373 5f66 6163 746f 723d 302e  x_loss_factor=0.
-00001110: 3035 2c20 6e75 6d5f 6578 7065 7274 735f  05, num_experts_
-00001120: 6368 6f73 656e 3d31 2c0a 2020 2020 2020  chosen=1,.      
-00001130: 2020 2020 2020 2020 2020 2065 7870 6572             exper
-00001140: 745f 6772 6f75 705f 7369 7a65 3d4e 6f6e  t_group_size=Non
-00001150: 652c 2067 726f 7570 5f77 6973 655f 6132  e, group_wise_a2
-00001160: 613d 4661 6c73 652c 2063 6f6d 705f 636f  a=False, comp_co
-00001170: 6d6d 5f70 6172 616c 6c65 6c3d 4661 6c73  mm_parallel=Fals
-00001180: 652c 2063 6f6d 705f 636f 6d6d 5f70 6172  e, comp_comm_par
-00001190: 616c 6c65 6c5f 6465 6772 6565 3d32 2c0a  allel_degree=2,.
-000011a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000011b0: 2073 6176 655f 746f 6b65 6e5f 6469 7374   save_token_dist
-000011c0: 7269 6275 7469 6f6e 3d46 616c 7365 2c20  ribution=False, 
-000011d0: 6375 725f 6c61 7965 723d 302c 2065 6e61  cur_layer=0, ena
-000011e0: 626c 655f 636f 6c64 5f68 6f74 5f65 7870  ble_cold_hot_exp
-000011f0: 6572 743d 4661 6c73 652c 2075 7064 6174  ert=False, updat
-00001200: 655f 7374 6570 3d31 3030 3030 2c0a 2020  e_step=10000,.  
-00001210: 2020 2020 2020 2020 2020 2020 2020 2068                 h
-00001220: 6f74 5f65 7870 6572 745f 6e75 6d3d 302c  ot_expert_num=0,
-00001230: 2063 6f6c 645f 746f 6b65 6e5f 7065 7263   cold_token_perc
-00001240: 656e 743d 312e 302c 206d 6f65 5f6d 6f64  ent=1.0, moe_mod
-00001250: 756c 655f 6e61 6d65 3d22 222c 2072 6f75  ule_name="", rou
-00001260: 7469 6e67 5f70 6f6c 6963 793d 2254 6f70  ting_policy="Top
-00001270: 6b52 6f75 7465 7256 3122 2c0a 2020 2020  kRouterV1",.    
-00001280: 2020 2020 2020 2020 2020 2020 2065 6e61               ena
-00001290: 626c 655f 7364 726f 703d 4661 6c73 652c  ble_sdrop=False,
-000012a0: 2072 6f75 7465 725f 6465 6e73 655f 7479   router_dense_ty
-000012b0: 7065 3d22 666c 6f61 7433 3222 293a 0a20  pe="float32"):. 
-000012c0: 2020 2020 2020 2056 616c 6964 6174 6f72         Validator
-000012d0: 2e63 6865 636b 5f70 6f73 6974 6976 655f  .check_positive_
-000012e0: 696e 7428 6578 7065 7274 5f6e 756d 2c20  int(expert_num, 
-000012f0: 2265 7870 6572 745f 6e75 6d22 290a 2020  "expert_num").  
-00001300: 2020 2020 2020 5661 6c69 6461 746f 722e        Validator.
-00001310: 6368 6563 6b5f 706f 7369 7469 7665 5f66  check_positive_f
-00001320: 6c6f 6174 2863 6170 6163 6974 795f 6661  loat(capacity_fa
-00001330: 6374 6f72 2c20 2263 6170 6163 6974 795f  ctor, "capacity_
-00001340: 6661 6374 6f72 2229 0a20 2020 2020 2020  factor").       
-00001350: 2056 616c 6964 6174 6f72 2e63 6865 636b   Validator.check
-00001360: 5f70 6f73 6974 6976 655f 666c 6f61 7428  _positive_float(
-00001370: 6175 785f 6c6f 7373 5f66 6163 746f 722c  aux_loss_factor,
-00001380: 2022 6175 785f 6c6f 7373 5f66 6163 746f   "aux_loss_facto
-00001390: 7222 290a 2020 2020 2020 2020 5661 6c69  r").        Vali
-000013a0: 6461 746f 722e 6368 6563 6b5f 706f 7369  dator.check_posi
-000013b0: 7469 7665 5f69 6e74 286e 756d 5f65 7870  tive_int(num_exp
-000013c0: 6572 7473 5f63 686f 7365 6e2c 2022 6e75  erts_chosen, "nu
-000013d0: 6d5f 6578 7065 7274 735f 6368 6f73 656e  m_experts_chosen
-000013e0: 2229 0a20 2020 2020 2020 2056 616c 6964  ").        Valid
-000013f0: 6174 6f72 2e63 6865 636b 5f62 6f6f 6c28  ator.check_bool(
-00001400: 6772 6f75 705f 7769 7365 5f61 3261 2c20  group_wise_a2a, 
-00001410: 2267 726f 7570 5f77 6973 655f 6132 6122  "group_wise_a2a"
-00001420: 290a 2020 2020 2020 2020 5661 6c69 6461  ).        Valida
-00001430: 746f 722e 6368 6563 6b5f 626f 6f6c 2863  tor.check_bool(c
-00001440: 6f6d 705f 636f 6d6d 5f70 6172 616c 6c65  omp_comm_paralle
-00001450: 6c2c 2022 636f 6d70 5f63 6f6d 6d5f 7061  l, "comp_comm_pa
-00001460: 7261 6c6c 656c 2229 0a20 2020 2020 2020  rallel").       
-00001470: 2056 616c 6964 6174 6f72 2e63 6865 636b   Validator.check
-00001480: 5f70 6f73 6974 6976 655f 696e 7428 636f  _positive_int(co
-00001490: 6d70 5f63 6f6d 6d5f 7061 7261 6c6c 656c  mp_comm_parallel
-000014a0: 5f64 6567 7265 652c 2022 636f 6d70 5f63  _degree, "comp_c
-000014b0: 6f6d 6d5f 7061 7261 6c6c 656c 5f64 6567  omm_parallel_deg
-000014c0: 7265 6522 290a 2020 2020 2020 2020 5661  ree").        Va
-000014d0: 6c69 6461 746f 722e 6368 6563 6b5f 626f  lidator.check_bo
-000014e0: 6f6c 2873 6176 655f 746f 6b65 6e5f 6469  ol(save_token_di
-000014f0: 7374 7269 6275 7469 6f6e 2c20 2273 6176  stribution, "sav
-00001500: 655f 746f 6b65 6e5f 6469 7374 7269 6275  e_token_distribu
-00001510: 7469 6f6e 2229 0a20 2020 2020 2020 2056  tion").        V
-00001520: 616c 6964 6174 6f72 2e63 6865 636b 5f6e  alidator.check_n
-00001530: 6f6e 5f6e 6567 6174 6976 655f 696e 7428  on_negative_int(
-00001540: 6375 725f 6c61 7965 722c 2022 6375 725f  cur_layer, "cur_
-00001550: 6c61 7965 7222 290a 2020 2020 2020 2020  layer").        
-00001560: 5661 6c69 6461 746f 722e 6368 6563 6b5f  Validator.check_
-00001570: 626f 6f6c 2865 6e61 626c 655f 636f 6c64  bool(enable_cold
-00001580: 5f68 6f74 5f65 7870 6572 742c 2022 656e  _hot_expert, "en
-00001590: 6162 6c65 5f63 6f6c 645f 686f 745f 6578  able_cold_hot_ex
-000015a0: 7065 7274 2229 0a20 2020 2020 2020 2056  pert").        V
-000015b0: 616c 6964 6174 6f72 2e63 6865 636b 5f70  alidator.check_p
-000015c0: 6f73 6974 6976 655f 696e 7428 7570 6461  ositive_int(upda
-000015d0: 7465 5f73 7465 702c 2022 7570 6461 7465  te_step, "update
-000015e0: 5f73 7465 7022 290a 2020 2020 2020 2020  _step").        
-000015f0: 5661 6c69 6461 746f 722e 6368 6563 6b5f  Validator.check_
-00001600: 6e6f 6e5f 6e65 6761 7469 7665 5f69 6e74  non_negative_int
-00001610: 2868 6f74 5f65 7870 6572 745f 6e75 6d2c  (hot_expert_num,
-00001620: 2022 686f 745f 6578 7065 7274 5f6e 756d   "hot_expert_num
-00001630: 2229 0a20 2020 2020 2020 2056 616c 6964  ").        Valid
-00001640: 6174 6f72 2e63 6865 636b 5f6e 6f6e 5f6e  ator.check_non_n
-00001650: 6567 6174 6976 655f 666c 6f61 7428 636f  egative_float(co
-00001660: 6c64 5f74 6f6b 656e 5f70 6572 6365 6e74  ld_token_percent
-00001670: 2c20 2263 6f6c 645f 746f 6b65 6e5f 7065  , "cold_token_pe
-00001680: 7263 656e 7422 290a 2020 2020 2020 2020  rcent").        
-00001690: 5661 6c69 6461 746f 722e 6368 6563 6b5f  Validator.check_
-000016a0: 7374 7269 6e67 2872 6f75 7465 725f 6465  string(router_de
-000016b0: 6e73 655f 7479 7065 2c20 5b22 666c 6f61  nse_type, ["floa
-000016c0: 7431 3622 2c20 2266 6c6f 6174 3332 222c  t16", "float32",
-000016d0: 2022 6266 6c6f 6174 3136 225d 2c20 2272   "bfloat16"], "r
-000016e0: 6f75 7465 725f 6465 6e73 655f 7479 7065  outer_dense_type
-000016f0: 2229 0a20 2020 2020 2020 2069 6620 6578  ").        if ex
-00001700: 7065 7274 5f67 726f 7570 5f73 697a 6520  pert_group_size 
-00001710: 6973 206e 6f74 204e 6f6e 653a 0a20 2020  is not None:.   
-00001720: 2020 2020 2020 2020 2056 616c 6964 6174           Validat
-00001730: 6f72 2e63 6865 636b 5f70 6f73 6974 6976  or.check_positiv
-00001740: 655f 696e 7428 6578 7065 7274 5f67 726f  e_int(expert_gro
-00001750: 7570 5f73 697a 652c 2022 6578 7065 7274  up_size, "expert
-00001760: 5f67 726f 7570 5f73 697a 6522 290a 2020  _group_size").  
-00001770: 2020 2020 2020 6966 2063 6170 6163 6974        if capacit
-00001780: 795f 6661 6374 6f72 203c 2031 2e30 3a0a  y_factor < 1.0:.
-00001790: 2020 2020 2020 2020 2020 2020 7261 6973              rais
-000017a0: 6520 5661 6c75 6545 7272 6f72 2866 2227  e ValueError(f"'
-000017b0: 6361 7061 6369 7479 5f66 6163 746f 7227  capacity_factor'
-000017c0: 206d 7573 7420 6265 2065 7175 616c 2074   must be equal t
-000017d0: 6f20 6f72 2067 7265 6174 6572 2074 6861  o or greater tha
-000017e0: 6e20 312e 302c 2022 0a20 2020 2020 2020  n 1.0, ".       
-000017f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001800: 2020 2020 2020 6622 6275 7420 676f 7420        f"but got 
-00001810: 7b63 6170 6163 6974 795f 6661 6374 6f72  {capacity_factor
-00001820: 7d2e 2229 0a20 2020 2020 2020 2069 6620  }.").        if 
-00001830: 6175 785f 6c6f 7373 5f66 6163 746f 7220  aux_loss_factor 
-00001840: 3e3d 2031 2e30 3a0a 2020 2020 2020 2020  >= 1.0:.        
-00001850: 2020 2020 7261 6973 6520 5661 6c75 6545      raise ValueE
-00001860: 7272 6f72 2866 2227 6175 785f 6c6f 7373  rror(f"'aux_loss
-00001870: 5f66 6163 746f 7227 206d 7573 7420 6265  _factor' must be
-00001880: 206c 6573 7320 7468 616e 2031 2e30 2c20   less than 1.0, 
-00001890: 220a 2020 2020 2020 2020 2020 2020 2020  ".              
-000018a0: 2020 2020 2020 2020 2020 2020 2020 2066                 f
-000018b0: 2262 7574 2067 6f74 207b 6175 785f 6c6f  "but got {aux_lo
-000018c0: 7373 5f66 6163 746f 727d 2e22 290a 2020  ss_factor}.").  
-000018d0: 2020 2020 2020 6966 206e 756d 5f65 7870        if num_exp
-000018e0: 6572 7473 5f63 686f 7365 6e20 3e20 6578  erts_chosen > ex
-000018f0: 7065 7274 5f6e 756d 3a0a 2020 2020 2020  pert_num:.      
-00001900: 2020 2020 2020 7261 6973 6520 5661 6c75        raise Valu
-00001910: 6545 7272 6f72 2866 2227 6e75 6d5f 6578  eError(f"'num_ex
-00001920: 7065 7274 735f 6368 6f73 656e 2720 6d75  perts_chosen' mu
-00001930: 7374 206e 6f74 2062 6520 6c61 7267 6572  st not be larger
-00001940: 2074 6861 6e20 2765 7870 6572 745f 6e75   than 'expert_nu
-00001950: 6d27 2c20 220a 2020 2020 2020 2020 2020  m', ".          
-00001960: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001970: 2020 2066 2262 7574 2067 6f74 207b 6e75     f"but got {nu
-00001980: 6d5f 6578 7065 7274 735f 6368 6f73 656e  m_experts_chosen
-00001990: 7d2e 2229 0a20 2020 2020 2020 2069 6620  }.").        if 
-000019a0: 686f 745f 6578 7065 7274 5f6e 756d 203e  hot_expert_num >
-000019b0: 2065 7870 6572 745f 6e75 6d3a 0a20 2020   expert_num:.   
-000019c0: 2020 2020 2020 2020 2072 6169 7365 2056           raise V
-000019d0: 616c 7565 4572 726f 7228 6622 2768 6f74  alueError(f"'hot
-000019e0: 5f65 7870 6572 745f 6e75 6d27 206d 7573  _expert_num' mus
-000019f0: 7420 6e6f 7420 6265 206c 6172 6765 7220  t not be larger 
-00001a00: 7468 616e 2027 6578 7065 7274 5f6e 756d  than 'expert_num
-00001a10: 272c 2022 0a20 2020 2020 2020 2020 2020  ', ".           
-00001a20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001a30: 2020 6622 6275 7420 676f 7420 7b68 6f74    f"but got {hot
-00001a40: 5f65 7870 6572 745f 6e75 6d7d 2e22 290a  _expert_num}.").
-00001a50: 2020 2020 2020 2020 6966 2063 6f6c 645f          if cold_
-00001a60: 746f 6b65 6e5f 7065 7263 656e 7420 3e20  token_percent > 
-00001a70: 312e 3020 6f72 2063 6f6c 645f 746f 6b65  1.0 or cold_toke
-00001a80: 6e5f 7065 7263 656e 7420 3c3d 2030 2e30  n_percent <= 0.0
-00001a90: 3a0a 2020 2020 2020 2020 2020 2020 7261  :.            ra
-00001aa0: 6973 6520 5661 6c75 6545 7272 6f72 2866  ise ValueError(f
-00001ab0: 2227 636f 6c64 5f74 6f6b 656e 5f70 6572  "'cold_token_per
-00001ac0: 6365 6e74 2720 6d75 7374 2062 6520 696e  cent' must be in
-00001ad0: 2074 6865 2072 616e 6765 2028 302e 302c   the range (0.0,
-00001ae0: 2031 2e30 5d2c 2022 0a20 2020 2020 2020   1.0], ".       
-00001af0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001b00: 2020 2020 2020 6622 6275 7420 676f 7420        f"but got 
-00001b10: 7b63 6f6c 645f 746f 6b65 6e5f 7065 7263  {cold_token_perc
-00001b20: 656e 747d 2e22 290a 0a20 2020 2020 2020  ent}.")..       
-00001b30: 2073 656c 662e 6578 7065 7274 5f6e 756d   self.expert_num
-00001b40: 203d 2065 7870 6572 745f 6e75 6d0a 2020   = expert_num.  
-00001b50: 2020 2020 2020 7365 6c66 2e63 6170 6163        self.capac
-00001b60: 6974 795f 6661 6374 6f72 203d 2063 6170  ity_factor = cap
-00001b70: 6163 6974 795f 6661 6374 6f72 0a20 2020  acity_factor.   
-00001b80: 2020 2020 2073 656c 662e 6175 785f 6c6f       self.aux_lo
-00001b90: 7373 5f66 6163 746f 7220 3d20 6175 785f  ss_factor = aux_
-00001ba0: 6c6f 7373 5f66 6163 746f 720a 2020 2020  loss_factor.    
-00001bb0: 2020 2020 7365 6c66 2e6e 756d 5f65 7870      self.num_exp
-00001bc0: 6572 7473 5f63 686f 7365 6e20 3d20 6e75  erts_chosen = nu
-00001bd0: 6d5f 6578 7065 7274 735f 6368 6f73 656e  m_experts_chosen
-00001be0: 0a20 2020 2020 2020 2073 656c 662e 6578  .        self.ex
-00001bf0: 7065 7274 5f67 726f 7570 5f73 697a 6520  pert_group_size 
-00001c00: 3d20 6578 7065 7274 5f67 726f 7570 5f73  = expert_group_s
-00001c10: 697a 650a 2020 2020 2020 2020 7365 6c66  ize.        self
-00001c20: 2e67 726f 7570 5f77 6973 655f 6132 6120  .group_wise_a2a 
-00001c30: 3d20 6772 6f75 705f 7769 7365 5f61 3261  = group_wise_a2a
-00001c40: 0a20 2020 2020 2020 2073 656c 662e 636f  .        self.co
-00001c50: 6d70 5f63 6f6d 6d5f 7061 7261 6c6c 656c  mp_comm_parallel
-00001c60: 203d 2063 6f6d 705f 636f 6d6d 5f70 6172   = comp_comm_par
-00001c70: 616c 6c65 6c0a 2020 2020 2020 2020 7365  allel.        se
-00001c80: 6c66 2e63 6f6d 705f 636f 6d6d 5f70 6172  lf.comp_comm_par
-00001c90: 616c 6c65 6c5f 6465 6772 6565 203d 2063  allel_degree = c
-00001ca0: 6f6d 705f 636f 6d6d 5f70 6172 616c 6c65  omp_comm_paralle
-00001cb0: 6c5f 6465 6772 6565 0a20 2020 2020 2020  l_degree.       
-00001cc0: 2073 656c 662e 7361 7665 5f74 6f6b 656e   self.save_token
-00001cd0: 5f64 6973 7472 6962 7574 696f 6e20 3d20  _distribution = 
-00001ce0: 7361 7665 5f74 6f6b 656e 5f64 6973 7472  save_token_distr
-00001cf0: 6962 7574 696f 6e0a 2020 2020 2020 2020  ibution.        
-00001d00: 7365 6c66 2e63 7572 5f6c 6179 6572 203d  self.cur_layer =
-00001d10: 2063 7572 5f6c 6179 6572 0a20 2020 2020   cur_layer.     
-00001d20: 2020 2073 656c 662e 656e 6162 6c65 5f63     self.enable_c
-00001d30: 6f6c 645f 686f 745f 6578 7065 7274 203d  old_hot_expert =
-00001d40: 2065 6e61 626c 655f 636f 6c64 5f68 6f74   enable_cold_hot
-00001d50: 5f65 7870 6572 740a 2020 2020 2020 2020  _expert.        
-00001d60: 7365 6c66 2e75 7064 6174 655f 7374 6570  self.update_step
-00001d70: 203d 2075 7064 6174 655f 7374 6570 0a20   = update_step. 
-00001d80: 2020 2020 2020 2073 656c 662e 686f 745f         self.hot_
-00001d90: 6578 7065 7274 5f6e 756d 203d 2068 6f74  expert_num = hot
-00001da0: 5f65 7870 6572 745f 6e75 6d0a 2020 2020  _expert_num.    
-00001db0: 2020 2020 7365 6c66 2e63 6f6c 645f 746f      self.cold_to
-00001dc0: 6b65 6e5f 7065 7263 656e 7420 3d20 636f  ken_percent = co
-00001dd0: 6c64 5f74 6f6b 656e 5f70 6572 6365 6e74  ld_token_percent
-00001de0: 0a20 2020 2020 2020 2073 656c 662e 6d6f  .        self.mo
-00001df0: 655f 6d6f 6475 6c65 5f6e 616d 6520 3d20  e_module_name = 
-00001e00: 6d6f 655f 6d6f 6475 6c65 5f6e 616d 650a  moe_module_name.
-00001e10: 2020 2020 2020 2020 7365 6c66 2e72 6f75          self.rou
-00001e20: 7469 6e67 5f70 6f6c 6963 7920 3d20 726f  ting_policy = ro
-00001e30: 7574 696e 675f 706f 6c69 6379 0a20 2020  uting_policy.   
-00001e40: 2020 2020 2073 656c 662e 656e 6162 6c65       self.enable
-00001e50: 5f73 6472 6f70 203d 2065 6e61 626c 655f  _sdrop = enable_
-00001e60: 7364 726f 700a 2020 2020 2020 2020 7365  sdrop.        se
-00001e70: 6c66 2e72 6f75 7465 725f 6465 6e73 655f  lf.router_dense_
-00001e80: 7479 7065 203d 2064 7479 7065 5f6d 6170  type = dtype_map
-00001e90: 2e67 6574 2872 6f75 7465 725f 6465 6e73  .get(router_dens
-00001ea0: 655f 7479 7065 290a 0a20 2020 2064 6566  e_type)..    def
-00001eb0: 205f 5f65 715f 5f28 7365 6c66 2c20 6f74   __eq__(self, ot
-00001ec0: 6865 7229 202d 3e20 626f 6f6c 3a0a 2020  her) -> bool:.  
-00001ed0: 2020 2020 2020 7265 7475 726e 2069 7369        return isi
-00001ee0: 6e73 7461 6e63 6528 6f74 6865 722c 204d  nstance(other, M
-00001ef0: 6f45 436f 6e66 6967 2920 616e 6420 2873  oEConfig) and (s
-00001f00: 656c 662e 746f 5f64 6963 7428 2920 3d3d  elf.to_dict() ==
-00001f10: 206f 7468 6572 2e74 6f5f 6469 6374 2829   other.to_dict()
-00001f20: 290a 0a20 2020 2064 6566 2074 6f5f 6469  )..    def to_di
-00001f30: 6666 5f64 6963 7428 7365 6c66 293a 0a20  ff_dict(self):. 
-00001f40: 2020 2020 2020 2063 6f6e 6669 675f 6469         config_di
-00001f50: 6374 203d 2073 656c 662e 746f 5f64 6963  ct = self.to_dic
-00001f60: 7428 290a 2020 2020 2020 2020 6465 6661  t().        defa
-00001f70: 756c 745f 6469 6374 203d 204d 6f45 436f  ult_dict = MoECo
-00001f80: 6e66 6967 2829 2e74 6f5f 6469 6374 2829  nfig().to_dict()
-00001f90: 0a20 2020 2020 2020 2072 6573 5f64 6963  .        res_dic
-00001fa0: 7420 3d20 7b7d 0a20 2020 2020 2020 2066  t = {}.        f
-00001fb0: 6f72 206b 2c20 7620 696e 2063 6f6e 6669  or k, v in confi
-00001fc0: 675f 6469 6374 2e69 7465 6d73 2829 3a0a  g_dict.items():.
-00001fd0: 2020 2020 2020 2020 2020 2020 6966 2076              if v
-00001fe0: 2021 3d20 6465 6661 756c 745f 6469 6374   != default_dict
-00001ff0: 5b6b 5d3a 0a20 2020 2020 2020 2020 2020  [k]:.           
-00002000: 2020 2020 2072 6573 5f64 6963 745b 6b5d       res_dict[k]
-00002010: 203d 2076 0a20 2020 2020 2020 2072 6574   = v.        ret
-00002020: 7572 6e20 7265 735f 6469 6374 0a0a 2020  urn res_dict..  
-00002030: 2020 6465 6620 746f 5f64 6963 7428 7365    def to_dict(se
-00002040: 6c66 293a 0a20 2020 2020 2020 2072 6574  lf):.        ret
-00002050: 7572 6e20 636f 7079 2e64 6565 7063 6f70  urn copy.deepcop
-00002060: 7928 7365 6c66 2e5f 5f64 6963 745f 5f29  y(self.__dict__)
-00002070: 0a0a 0a64 6566 6175 6c74 5f6d 6f65 5f63  ...default_moe_c
-00002080: 6f6e 6669 6720 3d20 4d6f 4543 6f6e 6669  onfig = MoEConfi
-00002090: 6728 290a 0a0a 6465 6620 5f63 6865 636b  g()...def _check
-000020a0: 5f6d 6f65 5f63 6f6e 6669 6728 6d6f 655f  _moe_config(moe_
-000020b0: 636f 6e66 6967 3d4e 6f6e 652c 2070 6172  config=None, par
-000020c0: 616c 6c65 6c5f 636f 6e66 6967 3d4e 6f6e  allel_config=Non
-000020d0: 6529 3a0a 2020 2020 2222 220a 2020 2020  e):.    """.    
-000020e0: 2020 2020 6368 6563 6b20 6966 204d 6f45      check if MoE
-000020f0: 2077 6974 6820 7269 6768 7420 636f 6e66   with right conf
-00002100: 6967 7572 6174 696f 6e2e 0a20 2020 2022  iguration..    "
-00002110: 2222 0a20 2020 2069 6620 6e6f 7420 6973  "".    if not is
-00002120: 696e 7374 616e 6365 286d 6f65 5f63 6f6e  instance(moe_con
-00002130: 6669 672c 204d 6f45 436f 6e66 6967 293a  fig, MoEConfig):
-00002140: 0a20 2020 2020 2020 2072 6169 7365 2054  .        raise T
-00002150: 7970 6545 7272 6f72 2866 2227 6d6f 655f  ypeError(f"'moe_
-00002160: 636f 6e66 6967 2720 6d75 7374 2062 6520  config' must be 
-00002170: 616e 2069 6e73 7461 6e63 6520 6f66 204d  an instance of M
-00002180: 6f45 436f 6e66 6967 2c20 6275 7420 676f  oEConfig, but go
-00002190: 7420 7b74 7970 6528 6d6f 655f 636f 6e66  t {type(moe_conf
-000021a0: 6967 292e 5f5f 6e61 6d65 5f5f 7d2e 2229  ig).__name__}.")
-000021b0: 0a20 2020 2075 7365 5f6d 6f65 203d 2028  .    use_moe = (
-000021c0: 6d6f 655f 636f 6e66 6967 2e65 7870 6572  moe_config.exper
-000021d0: 745f 6e75 6d20 3e20 3129 0a20 2020 2069  t_num > 1).    i
-000021e0: 6620 7573 655f 6d6f 6520 6973 2046 616c  f use_moe is Fal
-000021f0: 7365 3a0a 2020 2020 2020 2020 7265 7475  se:.        retu
-00002200: 726e 0a20 2020 2069 6620 6d6f 655f 636f  rn.    if moe_co
-00002210: 6e66 6967 2e65 7870 6572 745f 6e75 6d20  nfig.expert_num 
-00002220: 2520 7061 7261 6c6c 656c 5f63 6f6e 6669  % parallel_confi
-00002230: 672e 6578 7065 7274 5f70 6172 616c 6c65  g.expert_paralle
-00002240: 6c20 213d 2030 3a0a 2020 2020 2020 2020  l != 0:.        
-00002250: 7261 6973 6520 5661 6c75 6545 7272 6f72  raise ValueError
-00002260: 2866 2257 6865 6e20 7573 696e 6720 4d6f  (f"When using Mo
-00002270: 452c 2074 6865 2027 6578 7065 7274 5f6e  E, the 'expert_n
-00002280: 756d 2720 696e 207b 7479 7065 286d 6f65  um' in {type(moe
-00002290: 5f63 6f6e 6669 6729 2e5f 5f6e 616d 655f  _config).__name_
-000022a0: 5f7d 206d 7573 7420 6265 2061 206d 756c  _} must be a mul
-000022b0: 7469 706c 6520 220a 2020 2020 2020 2020  tiple ".        
-000022c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000022d0: 2066 226f 6620 2765 7870 6572 745f 7061   f"of 'expert_pa
-000022e0: 7261 6c6c 656c 2720 7661 6c75 6520 696e  rallel' value in
-000022f0: 207b 7479 7065 2870 6172 616c 6c65 6c5f   {type(parallel_
-00002300: 636f 6e66 6967 292e 5f5f 6e61 6d65 5f5f  config).__name__
-00002310: 7d2c 2062 7574 2067 6f74 2022 0a20 2020  }, but got ".   
-00002320: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00002330: 2020 2020 2020 6622 7b6d 6f65 5f63 6f6e        f"{moe_con
-00002340: 6669 672e 6578 7065 7274 5f6e 756d 7d20  fig.expert_num} 
-00002350: 666f 7220 2765 7870 6572 745f 6e75 6d27  for 'expert_num'
-00002360: 2061 6e64 207b 7061 7261 6c6c 656c 5f63   and {parallel_c
-00002370: 6f6e 6669 672e 6578 7065 7274 5f70 6172  onfig.expert_par
-00002380: 616c 6c65 6c7d 2066 6f72 2022 0a20 2020  allel} for ".   
-00002390: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000023a0: 2020 2020 2020 6622 2765 7870 6572 745f        f"'expert_
-000023b0: 7061 7261 6c6c 656c 272e 2229 0a0a 2020  parallel'.")..  
-000023c0: 2020 6465 7669 6365 5f6e 756d 203d 2044    device_num = D
-000023d0: 2e67 6574 5f67 726f 7570 5f73 697a 6528  .get_group_size(
-000023e0: 290a 2020 2020 6966 2064 6576 6963 655f  ).    if device_
-000023f0: 6e75 6d20 2520 7061 7261 6c6c 656c 5f63  num % parallel_c
-00002400: 6f6e 6669 672e 6578 7065 7274 5f70 6172  onfig.expert_par
-00002410: 616c 6c65 6c20 213d 2030 3a0a 2020 2020  allel != 0:.    
-00002420: 2020 2020 7261 6973 6520 5661 6c75 6545      raise ValueE
-00002430: 7272 6f72 2866 2264 6576 6963 655f 6e75  rror(f"device_nu
-00002440: 6d3a 207b 6465 7669 6365 5f6e 756d 7d20  m: {device_num} 
-00002450: 6d75 7374 2062 6520 6120 6d75 6c74 6970  must be a multip
-00002460: 6c65 206f 6620 6578 7065 7274 5f70 6172  le of expert_par
-00002470: 616c 6c65 6c3a 2022 0a20 2020 2020 2020  allel: ".       
-00002480: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00002490: 2020 6622 7b70 6172 616c 6c65 6c5f 636f    f"{parallel_co
-000024a0: 6e66 6967 2e65 7870 6572 745f 7061 7261  nfig.expert_para
-000024b0: 6c6c 656c 7d2e 2229 0a20 2020 2069 6620  llel}.").    if 
-000024c0: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
-000024d0: 6461 7461 5f70 6172 616c 6c65 6c20 2520  data_parallel % 
-000024e0: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
-000024f0: 6578 7065 7274 5f70 6172 616c 6c65 6c20  expert_parallel 
-00002500: 213d 2030 3a0a 2020 2020 2020 2020 7261  != 0:.        ra
-00002510: 6973 6520 5661 6c75 6545 7272 6f72 2866  ise ValueError(f
-00002520: 2264 6174 6120 7061 7261 6c6c 656c 3a20  "data parallel: 
-00002530: 7b70 6172 616c 6c65 6c5f 636f 6e66 6967  {parallel_config
-00002540: 2e64 6174 615f 7061 7261 6c6c 656c 7d20  .data_parallel} 
-00002550: 6d75 7374 2062 6520 6120 6d75 6c74 6970  must be a multip
-00002560: 6c65 206f 6620 220a 2020 2020 2020 2020  le of ".        
-00002570: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00002580: 2066 2265 7870 6572 745f 7061 7261 6c6c   f"expert_parall
-00002590: 656c 3a20 7b70 6172 616c 6c65 6c5f 636f  el: {parallel_co
-000025a0: 6e66 6967 2e65 7870 6572 745f 7061 7261  nfig.expert_para
-000025b0: 6c6c 656c 7d20 7768 656e 2075 7369 6e67  llel} when using
-000025c0: 204d 6f45 2e22 290a 2020 2020 6966 2070   MoE.").    if p
-000025d0: 6172 616c 6c65 6c5f 636f 6e66 6967 2e64  arallel_config.d
-000025e0: 6174 615f 7061 7261 6c6c 656c 202a 2070  ata_parallel * p
-000025f0: 6172 616c 6c65 6c5f 636f 6e66 6967 2e6d  arallel_config.m
-00002600: 6f64 656c 5f70 6172 616c 6c65 6c20 3e20  odel_parallel > 
-00002610: 6465 7669 6365 5f6e 756d 3a0a 2020 2020  device_num:.    
-00002620: 2020 2020 7261 6973 6520 5661 6c75 6545      raise ValueE
-00002630: 7272 6f72 2866 2254 6865 2070 726f 6475  rror(f"The produ
-00002640: 6374 206f 6620 7468 6520 6461 7461 2070  ct of the data p
-00002650: 6172 616c 6c65 6c3a 207b 7061 7261 6c6c  arallel: {parall
-00002660: 656c 5f63 6f6e 6669 672e 6461 7461 5f70  el_config.data_p
-00002670: 6172 616c 6c65 6c7d 2061 6e64 2022 0a20  arallel} and ". 
-00002680: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00002690: 2020 2020 2020 2020 6622 6d6f 6465 6c20          f"model 
-000026a0: 7061 7261 6c6c 656c 3a20 7b70 6172 616c  parallel: {paral
-000026b0: 6c65 6c5f 636f 6e66 6967 2e6d 6f64 656c  lel_config.model
-000026c0: 5f70 6172 616c 6c65 6c7d 2022 0a20 2020  _parallel} ".   
-000026d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000026e0: 2020 2020 2020 6622 7368 6f75 6c64 2062        f"should b
-000026f0: 6520 6c65 7373 2074 6861 6e20 6465 7669  e less than devi
-00002700: 6365 5f6e 756d 3a20 7b64 6576 6963 655f  ce_num: {device_
-00002710: 6e75 6d7d 2e22 290a 0a0a 4063 6f6e 7374  num}.")...@const
-00002720: 6578 7072 0a64 6566 2063 616c 6375 6c61  expr.def calcula
-00002730: 7465 5f65 7870 6572 745f 6361 7061 6369  te_expert_capaci
-00002740: 7479 286b 2c20 746f 6b65 6e73 5f70 6572  ty(k, tokens_per
-00002750: 5f67 726f 7570 2c20 6361 7061 6369 7479  _group, capacity
-00002760: 5f66 6163 746f 722c 2065 7870 6572 745f  _factor, expert_
-00002770: 6469 6d29 3a0a 2020 2020 7265 7475 726e  dim):.    return
-00002780: 206d 6174 682e 6365 696c 286b 202a 2074   math.ceil(k * t
-00002790: 6f6b 656e 735f 7065 725f 6772 6f75 7020  okens_per_group 
-000027a0: 2a20 6361 7061 6369 7479 5f66 6163 746f  * capacity_facto
-000027b0: 7220 2f20 6578 7065 7274 5f64 696d 290a  r / expert_dim).
-000027c0: 0a40 636f 6e73 7465 7870 720a 6465 6620  .@constexpr.def 
-000027d0: 6361 6c63 756c 6174 655f 6578 7065 7274  calculate_expert
-000027e0: 5f63 6170 6163 6974 795f 7632 286b 2c20  _capacity_v2(k, 
-000027f0: 746f 6b65 6e73 5f70 6572 5f67 726f 7570  tokens_per_group
-00002800: 2c20 6361 7061 6369 7479 5f66 6163 746f  , capacity_facto
-00002810: 722c 2065 7870 6572 745f 6469 6d2c 206d  r, expert_dim, m
-00002820: 7029 3a0a 2020 2020 7261 775f 6361 7061  p):.    raw_capa
-00002830: 6369 7479 203d 206d 6174 682e 6365 696c  city = math.ceil
-00002840: 286b 202a 2074 6f6b 656e 735f 7065 725f  (k * tokens_per_
-00002850: 6772 6f75 7020 2a20 6361 7061 6369 7479  group * capacity
-00002860: 5f66 6163 746f 7220 2f20 6578 7065 7274  _factor / expert
-00002870: 5f64 696d 290a 2020 2020 7265 7475 726e  _dim).    return
-00002880: 2072 6177 5f63 6170 6163 6974 7920 2b20   raw_capacity + 
-00002890: 6d70 202d 2028 7261 775f 6361 7061 6369  mp - (raw_capaci
-000028a0: 7479 2025 206d 7029 0a0a 0a63 6c61 7373  ty % mp)...class
-000028b0: 204d 6f45 2843 656c 6c29 3a0a 2020 2020   MoE(Cell):.    
-000028c0: 2222 220a 2020 2020 5468 6520 6d69 7874  """.    The mixt
-000028d0: 7572 6520 6f66 2065 7870 6572 7473 2028  ure of experts (
-000028e0: 4d6f 4529 2069 6d70 6c65 6d65 6e74 6174  MoE) implementat
-000028f0: 696f 6e2e 2054 6865 2069 6d70 6c65 6d65  ion. The impleme
-00002900: 6e74 6174 696f 6e20 696e 636c 7564 6573  ntation includes
-00002910: 2061 2072 6f75 7465 7220 616e 6420 6120   a router and a 
-00002920: 4665 6564 466f 7277 6172 6420 6c61 7965  FeedForward laye
-00002930: 722e 0a20 2020 2054 6865 2072 6f75 7465  r..    The route
-00002940: 7220 6469 7370 6174 6368 6573 2074 6f6b  r dispatches tok
-00002950: 656e 7320 746f 2065 7870 6572 7473 2069  ens to experts i
-00002960: 6e20 4665 6564 466f 7277 6172 642c 2074  n FeedForward, t
-00002970: 6865 6e20 4665 6564 466f 7277 6172 6420  hen FeedForward 
-00002980: 646f 6573 2063 6f6d 7075 7461 7469 6f6e  does computation
-00002990: 2c20 616e 6420 7468 6520 6669 6e61 6c20  , and the final 
-000029a0: 6f75 7470 7574 2069 730a 2020 2020 6f62  output is.    ob
-000029b0: 7461 696e 6564 2062 7920 6d75 6c74 6970  tained by multip
-000029c0: 6c79 696e 6720 4665 6564 466f 7277 6172  lying FeedForwar
-000029d0: 6427 7320 6f75 7470 7574 2061 6e64 2072  d's output and r
-000029e0: 6f75 7465 7227 7320 636f 6d62 696e 6520  outer's combine 
-000029f0: 7765 6967 6874 2e0a 0a20 2020 2041 7267  weight...    Arg
-00002a00: 733a 0a20 2020 2020 2020 2068 6964 6465  s:.        hidde
-00002a10: 6e5f 7369 7a65 2028 696e 7429 3a20 5468  n_size (int): Th
-00002a20: 6520 6469 6d65 6e73 696f 6e20 6f66 2074  e dimension of t
-00002a30: 6865 2069 6e70 7574 732e 0a20 2020 2020  he inputs..     
-00002a40: 2020 2066 666e 5f68 6964 6465 6e5f 7369     ffn_hidden_si
-00002a50: 7a65 2028 696e 7429 3a20 5468 6520 696e  ze (int): The in
-00002a60: 7465 726d 6564 6961 7465 2068 6964 6465  termediate hidde
-00002a70: 6e20 7369 7a65 2e0a 2020 2020 2020 2020  n size..        
-00002a80: 6472 6f70 6f75 745f 7261 7465 2028 666c  dropout_rate (fl
-00002a90: 6f61 7429 3a20 5468 6520 6472 6f70 6f75  oat): The dropou
-00002aa0: 7420 7261 7465 2066 6f72 2074 6865 2073  t rate for the s
-00002ab0: 6563 6f6e 6420 6c69 6e65 6172 2773 206f  econd linear's o
-00002ac0: 7574 7075 742e 0a20 2020 2020 2020 2068  utput..        h
-00002ad0: 6964 6465 6e5f 6163 7420 2873 7472 293a  idden_act (str):
-00002ae0: 2054 6865 2061 6374 6976 6174 696f 6e20   The activation 
-00002af0: 6f66 2074 6865 2069 6e74 6572 6e61 6c20  of the internal 
-00002b00: 6665 6564 666f 7277 6172 6420 6c61 7965  feedforward laye
-00002b10: 722e 2053 7570 706f 7274 7320 2772 656c  r. Supports 'rel
-00002b20: 7527 2c0a 2020 2020 2020 2020 2020 2020  u',.            
-00002b30: 2020 2020 2020 2020 2020 2020 2027 7265               're
-00002b40: 6c75 3627 2c20 2774 616e 6827 2c20 2767  lu6', 'tanh', 'g
-00002b50: 656c 7527 2c20 2766 6173 745f 6765 6c75  elu', 'fast_gelu
-00002b60: 272c 2027 656c 7527 2c20 2773 6967 6d6f  ', 'elu', 'sigmo
-00002b70: 6964 272c 2027 7072 656c 7527 2c20 276c  id', 'prelu', 'l
-00002b80: 6561 6b79 7265 6c75 272c 2027 6873 7769  eakyrelu', 'hswi
-00002b90: 7368 272c 0a20 2020 2020 2020 2020 2020  sh',.           
-00002ba0: 2020 2020 2020 2020 2020 2020 2020 2768                'h
-00002bb0: 7369 676d 6f69 6427 2c20 276c 6f67 7369  sigmoid', 'logsi
-00002bc0: 676d 6f69 6427 2061 6e64 2073 6f20 6f6e  gmoid' and so on
-00002bd0: 2e20 4465 6661 756c 743a 2067 656c 752e  . Default: gelu.
-00002be0: 0a20 2020 2020 2020 2070 6172 616d 5f69  .        param_i
-00002bf0: 6e69 745f 7479 7065 2028 6474 7970 652e  nit_type (dtype.
-00002c00: 4e75 6d62 6572 293a 2054 6865 2070 6172  Number): The par
-00002c10: 616d 6574 6572 2069 6e69 7469 616c 697a  ameter initializ
-00002c20: 6174 696f 6e20 7479 7065 2e20 4361 6e20  ation type. Can 
-00002c30: 6265 2064 7479 7065 2e66 6c6f 6174 3332  be dtype.float32
-00002c40: 206f 7220 6474 7970 652e 666c 6f61 7431   or dtype.float1
-00002c50: 362e 0a20 2020 2020 2020 206d 6f65 5f63  6..        moe_c
-00002c60: 6f6e 6669 6728 4d6f 4543 6f6e 6669 6729  onfig(MoEConfig)
-00002c70: 3a20 5468 6520 636f 6e66 6967 7572 6174  : The configurat
-00002c80: 696f 6e20 6f66 204d 6f45 2028 4d69 7874  ion of MoE (Mixt
-00002c90: 7572 6520 6f66 2045 7870 6572 7429 2e20  ure of Expert). 
-00002ca0: 4465 6661 756c 7420 6973 2061 6e20 696e  Default is an in
-00002cb0: 7374 616e 6365 206f 6620 4d6f 4543 6f6e  stance of MoECon
-00002cc0: 6669 6720 7769 7468 0a20 2020 2020 2020  fig with.       
-00002cd0: 2020 2020 2064 6566 6175 6c74 2076 616c       default val
-00002ce0: 7565 732e 2050 6c65 6173 6520 7365 6520  ues. Please see 
-00002cf0: 604d 6f45 436f 6e66 6967 602e 0a20 2020  `MoEConfig`..   
-00002d00: 2020 2020 2070 6172 616c 6c65 6c5f 636f       parallel_co
-00002d10: 6e66 6967 284d 6f45 5061 7261 6c6c 656c  nfig(MoEParallel
-00002d20: 436f 6e66 6967 293a 2054 6865 2070 6172  Config): The par
-00002d30: 616c 6c65 6c20 636f 6e66 6967 2066 6f72  allel config for
-00002d40: 204d 6f45 2c20 7365 6520 604d 6f45 5061   MoE, see `MoEPa
-00002d50: 7261 6c6c 656c 436f 6e66 6967 602e 0a20  rallelConfig`.. 
-00002d60: 2020 2020 2020 2020 2020 2044 6566 6175             Defau
-00002d70: 6c74 2060 6465 6661 756c 745f 6d6f 6570  lt `default_moep
-00002d80: 6172 616c 6c65 6c5f 636f 6e66 6967 602c  arallel_config`,
-00002d90: 2061 6e20 696e 7374 616e 6365 206f 6620   an instance of 
-00002da0: 604d 6f45 5061 7261 6c6c 656c 436f 6e66  `MoEParallelConf
-00002db0: 6967 6020 7769 7468 2064 6566 6175 6c74  ig` with default
-00002dc0: 2061 7267 732e 0a0a 2020 2020 496e 7075   args...    Inpu
-00002dd0: 7473 3a0a 2020 2020 2020 2020 2d20 2a2a  ts:.        - **
-00002de0: 782a 2a20 2854 656e 736f 7229 202d 2073  x** (Tensor) - s
-00002df0: 686f 756c 6420 6265 2060 5b62 6174 6368  hould be `[batch
-00002e00: 2c20 7365 715f 6c65 6e67 7468 2c20 6869  , seq_length, hi
-00002e10: 6464 656e 5f73 697a 655d 602e 2046 6c6f  dden_size]`. Flo
-00002e20: 6174 2074 656e 736f 722e 0a0a 2020 2020  at tensor...    
-00002e30: 4f75 7470 7574 733a 0a20 2020 2020 2020  Outputs:.       
-00002e40: 2054 656e 736f 722c 2074 6865 206f 7574   Tensor, the out
-00002e50: 7075 7420 6f66 2074 6869 7320 6c61 7965  put of this laye
-00002e60: 7220 6166 7465 7220 6d61 7070 696e 672e  r after mapping.
-00002e70: 2054 6865 2073 6861 7065 2069 7320 605b   The shape is `[
-00002e80: 6261 7463 682c 2073 6571 5f6c 656e 6774  batch, seq_lengt
-00002e90: 682c 2068 6964 6465 6e5f 7369 7a65 5d60  h, hidden_size]`
-00002ea0: 2e0a 2020 2020 2222 220a 0a20 2020 2064  ..    """..    d
-00002eb0: 6566 205f 5f69 6e69 745f 5f28 7365 6c66  ef __init__(self
-00002ec0: 2c20 6869 6464 656e 5f73 697a 652c 0a20  , hidden_size,. 
-00002ed0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00002ee0: 6666 6e5f 6869 6464 656e 5f73 697a 652c  ffn_hidden_size,
-00002ef0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00002f00: 2020 6472 6f70 6f75 745f 7261 7465 2c0a    dropout_rate,.
-00002f10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00002f20: 2068 6964 6465 6e5f 6163 743d 2767 656c   hidden_act='gel
-00002f30: 7527 2c0a 2020 2020 2020 2020 2020 2020  u',.            
-00002f40: 2020 2020 2070 6172 616d 5f69 6e69 745f       param_init_
-00002f50: 7479 7065 3d6d 7374 7970 652e 666c 6f61  type=mstype.floa
-00002f60: 7433 322c 0a20 2020 2020 2020 2020 2020  t32,.           
-00002f70: 2020 2020 2020 6d6f 655f 636f 6e66 6967        moe_config
-00002f80: 3d64 6566 6175 6c74 5f6d 6f65 5f63 6f6e  =default_moe_con
-00002f90: 6669 672c 0a20 2020 2020 2020 2020 2020  fig,.           
-00002fa0: 2020 2020 2020 7061 7261 6c6c 656c 5f63        parallel_c
-00002fb0: 6f6e 6669 673d 6465 6661 756c 745f 6d6f  onfig=default_mo
-00002fc0: 6570 6172 616c 6c65 6c5f 636f 6e66 6967  eparallel_config
-00002fd0: 293a 0a20 2020 2020 2020 2073 7570 6572  ):.        super
-00002fe0: 284d 6f45 2c20 7365 6c66 292e 5f5f 696e  (MoE, self).__in
-00002ff0: 6974 5f5f 2829 0a20 2020 2020 2020 2069  it__().        i
-00003000: 6620 5f67 6574 5f70 6172 616c 6c65 6c5f  f _get_parallel_
-00003010: 6d6f 6465 2829 2069 6e20 2850 6172 616c  mode() in (Paral
-00003020: 6c65 6c4d 6f64 652e 4155 544f 5f50 4152  lelMode.AUTO_PAR
-00003030: 414c 4c45 4c2c 2920 616e 6420 5f69 735f  ALLEL,) and _is_
-00003040: 7368 6172 6469 6e67 5f70 726f 7061 6761  sharding_propaga
-00003050: 7469 6f6e 2829 3a0a 2020 2020 2020 2020  tion():.        
-00003060: 2020 2020 7365 6c66 2e68 6964 6465 6e5f      self.hidden_
-00003070: 7369 7a65 203d 2068 6964 6465 6e5f 7369  size = hidden_si
-00003080: 7a65 0a20 2020 2020 2020 2020 2020 2073  ze.            s
-00003090: 656c 662e 6578 7065 7274 5f64 696d 203d  elf.expert_dim =
-000030a0: 206d 6f65 5f63 6f6e 6669 672e 6578 7065   moe_config.expe
-000030b0: 7274 5f6e 756d 0a20 2020 2020 2020 2020  rt_num.         
-000030c0: 2020 2073 656c 662e 6361 7061 6369 7479     self.capacity
-000030d0: 5f66 6163 746f 7220 3d20 6d6f 655f 636f  _factor = moe_co
-000030e0: 6e66 6967 2e63 6170 6163 6974 795f 6661  nfig.capacity_fa
-000030f0: 6374 6f72 0a20 2020 2020 2020 2020 2020  ctor.           
-00003100: 2073 656c 662e 6175 785f 6c6f 7373 5f66   self.aux_loss_f
-00003110: 6163 746f 7220 3d20 6d6f 655f 636f 6e66  actor = moe_conf
-00003120: 6967 2e61 7578 5f6c 6f73 735f 6661 6374  ig.aux_loss_fact
-00003130: 6f72 0a20 2020 2020 2020 2020 2020 2073  or.            s
-00003140: 656c 662e 6e75 6d5f 6578 7065 7274 735f  elf.num_experts_
-00003150: 6368 6f73 656e 203d 206d 6f65 5f63 6f6e  chosen = moe_con
-00003160: 6669 672e 6e75 6d5f 6578 7065 7274 735f  fig.num_experts_
-00003170: 6368 6f73 656e 0a20 2020 2020 2020 2020  chosen.         
-00003180: 2020 2073 656c 662e 6578 7065 7274 5f67     self.expert_g
-00003190: 726f 7570 5f73 697a 6520 3d20 6d6f 655f  roup_size = moe_
-000031a0: 636f 6e66 6967 2e65 7870 6572 745f 6772  config.expert_gr
-000031b0: 6f75 705f 7369 7a65 0a20 2020 2020 2020  oup_size.       
-000031c0: 2020 2020 2073 656c 662e 6470 5f67 726f       self.dp_gro
-000031d0: 7570 203d 2070 6172 616c 6c65 6c5f 636f  up = parallel_co
-000031e0: 6e66 6967 2e64 6174 615f 7061 7261 6c6c  nfig.data_parall
-000031f0: 656c 0a20 2020 2020 2020 2020 2020 2073  el.            s
-00003200: 656c 662e 6470 203d 2070 6172 616c 6c65  elf.dp = paralle
-00003210: 6c5f 636f 6e66 6967 2e64 6174 615f 7061  l_config.data_pa
-00003220: 7261 6c6c 656c 0a20 2020 2020 2020 2020  rallel.         
-00003230: 2020 2073 656c 662e 6570 203d 2070 6172     self.ep = par
-00003240: 616c 6c65 6c5f 636f 6e66 6967 2e65 7870  allel_config.exp
-00003250: 6572 745f 7061 7261 6c6c 656c 0a20 2020  ert_parallel.   
-00003260: 2020 2020 2020 2020 2073 656c 662e 6d70           self.mp
-00003270: 203d 2070 6172 616c 6c65 6c5f 636f 6e66   = parallel_conf
-00003280: 6967 2e6d 6f64 656c 5f70 6172 616c 6c65  ig.model_paralle
-00003290: 6c0a 2020 2020 2020 2020 2020 2020 7365  l.            se
-000032a0: 6c66 2e63 6f6d 705f 636f 6d6d 5f70 6172  lf.comp_comm_par
-000032b0: 616c 6c65 6c20 3d20 6d6f 655f 636f 6e66  allel = moe_conf
-000032c0: 6967 2e63 6f6d 705f 636f 6d6d 5f70 6172  ig.comp_comm_par
-000032d0: 616c 6c65 6c0a 2020 2020 2020 2020 2020  allel.          
-000032e0: 2020 7365 6c66 2e63 6f6d 705f 636f 6d6d    self.comp_comm
-000032f0: 5f70 6172 616c 6c65 6c5f 6465 6772 6565  _parallel_degree
-00003300: 203d 206d 6f65 5f63 6f6e 6669 672e 636f   = moe_config.co
-00003310: 6d70 5f63 6f6d 6d5f 7061 7261 6c6c 656c  mp_comm_parallel
-00003320: 5f64 6567 7265 650a 2020 2020 2020 2020  _degree.        
-00003330: 2020 2020 7365 6c66 2e67 726f 7570 5f77      self.group_w
-00003340: 6973 655f 6132 6120 3d20 6d6f 655f 636f  ise_a2a = moe_co
-00003350: 6e66 6967 2e67 726f 7570 5f77 6973 655f  nfig.group_wise_
-00003360: 6132 610a 2020 2020 2020 2020 2020 2020  a2a.            
-00003370: 6966 206e 6f74 2028 7365 6c66 2e6d 7020  if not (self.mp 
-00003380: 3e20 3120 616e 6420 7365 6c66 2e64 7020  > 1 and self.dp 
-00003390: 3d3d 2073 656c 662e 6570 293a 0a20 2020  == self.ep):.   
-000033a0: 2020 2020 2020 2020 2020 2020 2073 656c               sel
-000033b0: 662e 6772 6f75 705f 7769 7365 5f61 3261  f.group_wise_a2a
-000033c0: 203d 2046 616c 7365 0a20 2020 2020 2020   = False.       
-000033d0: 2020 2020 2066 726f 6d20 6d69 6e64 666f       from mindfo
-000033e0: 726d 6572 732e 6d6f 6475 6c65 732e 7472  rmers.modules.tr
-000033f0: 616e 7366 6f72 6d65 7220 696d 706f 7274  ansformer import
-00003400: 2046 6565 6446 6f72 7761 7264 0a0a 2020   FeedForward..  
-00003410: 2020 2020 2020 2020 2020 7365 6c66 2e66            self.f
-00003420: 666e 203d 2046 6565 6446 6f72 7761 7264  fn = FeedForward
-00003430: 2868 6964 6465 6e5f 7369 7a65 3d68 6964  (hidden_size=hid
-00003440: 6465 6e5f 7369 7a65 2c0a 2020 2020 2020  den_size,.      
-00003450: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00003460: 2020 2020 2020 2020 2020 2020 2066 666e               ffn
-00003470: 5f68 6964 6465 6e5f 7369 7a65 3d66 666e  _hidden_size=ffn
-00003480: 5f68 6964 6465 6e5f 7369 7a65 2c0a 2020  _hidden_size,.  
-00003490: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000034a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000034b0: 2064 726f 706f 7574 5f72 6174 653d 6472   dropout_rate=dr
-000034c0: 6f70 6f75 745f 7261 7465 2c0a 2020 2020  opout_rate,.    
-000034d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000034e0: 2020 2020 2020 2020 2020 2020 2020 2068                 h
-000034f0: 6964 6465 6e5f 6163 743d 6869 6464 656e  idden_act=hidden
-00003500: 5f61 6374 2c0a 2020 2020 2020 2020 2020  _act,.          
-00003510: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00003520: 2020 2020 2020 2020 2065 7870 6572 745f           expert_
-00003530: 6e75 6d3d 7365 6c66 2e65 7870 6572 745f  num=self.expert_
-00003540: 6469 6d2c 0a20 2020 2020 2020 2020 2020  dim,.           
-00003550: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00003560: 2020 2020 2020 2020 6578 7065 7274 5f67          expert_g
-00003570: 726f 7570 5f73 697a 653d 7365 6c66 2e65  roup_size=self.e
-00003580: 7870 6572 745f 6772 6f75 705f 7369 7a65  xpert_group_size
-00003590: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-000035a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000035b0: 2020 2020 2070 6172 616d 5f69 6e69 745f       param_init_
-000035c0: 7479 7065 3d70 6172 616d 5f69 6e69 745f  type=param_init_
-000035d0: 7479 7065 2c0a 2020 2020 2020 2020 2020  type,.          
-000035e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000035f0: 2020 2020 2020 2020 2070 6172 616c 6c65           paralle
-00003600: 6c5f 636f 6e66 6967 3d70 6172 616c 6c65  l_config=paralle
-00003610: 6c5f 636f 6e66 6967 290a 2020 2020 2020  l_config).      
-00003620: 2020 2020 2020 7365 6c66 2e72 6573 6861        self.resha
-00003630: 7065 203d 2050 2e52 6573 6861 7065 2829  pe = P.Reshape()
-00003640: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-00003650: 662e 7368 6170 6520 3d20 502e 5368 6170  f.shape = P.Shap
-00003660: 6528 290a 2020 2020 2020 2020 2020 2020  e().            
-00003670: 7365 6c66 2e74 7261 6e73 706f 7365 5f32  self.transpose_2
-00003680: 6469 6d20 3d20 502e 5472 616e 7370 6f73  dim = P.Transpos
-00003690: 6528 292e 7368 6172 6428 2828 7365 6c66  e().shard(((self
-000036a0: 2e64 702c 2031 292c 2929 0a20 2020 2020  .dp, 1),)).     
-000036b0: 2020 2020 2020 2073 656c 662e 7472 616e         self.tran
-000036c0: 7370 6f73 655f 3364 696d 203d 2050 2e54  spose_3dim = P.T
-000036d0: 7261 6e73 706f 7365 2829 2e73 6861 7264  ranspose().shard
-000036e0: 2828 2873 656c 662e 6470 2c20 312c 2031  (((self.dp, 1, 1
-000036f0: 292c 2929 0a20 2020 2020 2020 2020 2020  ),)).           
-00003700: 2073 656c 662e 7472 616e 7370 6f73 655f   self.transpose_
-00003710: 3464 696d 203d 2050 2e54 7261 6e73 706f  4dim = P.Transpo
-00003720: 7365 2829 2e73 6861 7264 2828 2831 2c20  se().shard(((1, 
-00003730: 7365 6c66 2e64 702c 2031 2c20 3129 2c29  self.dp, 1, 1),)
-00003740: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
-00003750: 6c66 2e74 7261 6e73 706f 7365 5f34 6469  lf.transpose_4di
-00003760: 6d5f 6470 203d 2050 2e54 7261 6e73 706f  m_dp = P.Transpo
-00003770: 7365 2829 2e73 6861 7264 2828 2831 2c20  se().shard(((1, 
-00003780: 312c 2073 656c 662e 6470 2c20 3129 2c29  1, self.dp, 1),)
-00003790: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
-000037a0: 6c66 2e62 6174 6368 5f6d 6d20 3d20 502e  lf.batch_mm = P.
-000037b0: 4261 7463 684d 6174 4d75 6c28 292e 7368  BatchMatMul().sh
-000037c0: 6172 6428 2828 7365 6c66 2e64 702c 2031  ard(((self.dp, 1
-000037d0: 2c20 3129 2c20 2873 656c 662e 6470 2c20  , 1), (self.dp, 
-000037e0: 312c 2031 2929 290a 2020 2020 2020 2020  1, 1))).        
-000037f0: 2020 2020 7365 6c66 2e62 6174 6368 5f6d      self.batch_m
-00003800: 6d32 203d 2050 2e42 6174 6368 4d61 744d  m2 = P.BatchMatM
-00003810: 756c 2829 2e73 6861 7264 2828 2873 656c  ul().shard(((sel
-00003820: 662e 6470 2c20 312c 2031 292c 2028 7365  f.dp, 1, 1), (se
-00003830: 6c66 2e64 702c 2031 2c20 3129 2929 0a20  lf.dp, 1, 1))). 
-00003840: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-00003850: 6d75 6c20 3d20 502e 4d75 6c28 290a 2020  mul = P.Mul().  
-00003860: 2020 2020 2020 2020 2020 7365 6c66 2e72            self.r
-00003870: 6f75 7465 7220 3d20 526f 7574 6572 2864  outer = Router(d
-00003880: 5f6d 6f64 656c 3d68 6964 6465 6e5f 7369  _model=hidden_si
-00003890: 7a65 2c20 6d6f 655f 636f 6e66 6967 3d6d  ze, moe_config=m
-000038a0: 6f65 5f63 6f6e 6669 672c 2072 6f75 7469  oe_config, routi
-000038b0: 6e67 5f70 6f6c 6963 793d 4e6f 6e65 2c0a  ng_policy=None,.
-000038c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000038d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000038e0: 2074 7261 696e 696e 673d 5472 7565 2c20   training=True, 
-000038f0: 7061 7261 6c6c 656c 5f63 6f6e 6669 673d  parallel_config=
-00003900: 7061 7261 6c6c 656c 5f63 6f6e 6669 6729  parallel_config)
-00003910: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-00003920: 662e 6361 7374 203d 2050 2e43 6173 7428  f.cast = P.Cast(
-00003930: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
-00003940: 6c66 2e63 6f6e 6361 7420 3d20 502e 436f  lf.concat = P.Co
-00003950: 6e63 6174 2833 292e 7368 6172 6428 7475  ncat(3).shard(tu
-00003960: 706c 6528 2873 656c 662e 6470 2c20 312c  ple((self.dp, 1,
-00003970: 2031 2c20 3129 2066 6f72 205f 2069 6e20   1, 1) for _ in 
-00003980: 7261 6e67 6528 7365 6c66 2e63 6f6d 705f  range(self.comp_
-00003990: 636f 6d6d 5f70 6172 616c 6c65 6c5f 6465  comm_parallel_de
-000039a0: 6772 6565 2929 290a 2020 2020 2020 2020  gree))).        
-000039b0: 2020 2020 7365 6c66 2e63 6f6e 6361 745f      self.concat_
-000039c0: 6470 203d 2050 2e43 6f6e 6361 7428 3229  dp = P.Concat(2)
-000039d0: 2e73 6861 7264 2828 2831 2c20 7365 6c66  .shard(((1, self
-000039e0: 2e64 702c 2031 2c20 3129 2c20 2831 2c20  .dp, 1, 1), (1, 
-000039f0: 7365 6c66 2e64 702c 2031 2c20 3129 2929  self.dp, 1, 1)))
-00003a00: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-00003a10: 662e 7370 6c69 7420 3d20 502e 5370 6c69  f.split = P.Spli
-00003a20: 7428 6178 6973 3d32 2c20 6f75 7470 7574  t(axis=2, output
-00003a30: 5f6e 756d 3d73 656c 662e 636f 6d70 5f63  _num=self.comp_c
-00003a40: 6f6d 6d5f 7061 7261 6c6c 656c 5f64 6567  omm_parallel_deg
-00003a50: 7265 6529 2e73 6861 7264 2828 2831 2c20  ree).shard(((1, 
-00003a60: 7365 6c66 2e64 702c 2031 2c20 3129 2c29  self.dp, 1, 1),)
-00003a70: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
-00003a80: 6c66 2e73 7472 6964 655f 736c 6963 6520  lf.stride_slice 
-00003a90: 3d20 502e 5374 7269 6465 6453 6c69 6365  = P.StridedSlice
-00003aa0: 2829 2e73 6861 7264 2828 2873 656c 662e  ().shard(((self.
-00003ab0: 6470 2c20 312c 2031 2c20 3129 2c29 290a  dp, 1, 1, 1),)).
-00003ac0: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-00003ad0: 2e73 7472 6964 655f 736c 6963 655f 6470  .stride_slice_dp
-00003ae0: 203d 2050 2e53 7472 6964 6564 536c 6963   = P.StridedSlic
-00003af0: 6528 292e 7368 6172 6428 2828 312c 2073  e().shard(((1, s
-00003b00: 656c 662e 6470 2c20 312c 2031 292c 2929  elf.dp, 1, 1),))
-00003b10: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-00003b20: 662e 7374 7269 6465 5f73 6c69 6365 5f65  f.stride_slice_e
-00003b30: 7020 3d20 502e 5374 7269 6465 6453 6c69  p = P.StridedSli
-00003b40: 6365 2829 2e73 6861 7264 2828 2873 656c  ce().shard(((sel
-00003b50: 662e 6570 2c20 312c 2031 2c20 3129 2c29  f.ep, 1, 1, 1),)
-00003b60: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
-00003b70: 6c66 2e73 7472 6964 655f 736c 6963 655f  lf.stride_slice_
-00003b80: 6470 5f6d 7020 3d20 502e 5374 7269 6465  dp_mp = P.Stride
-00003b90: 6453 6c69 6365 2829 2e73 6861 7264 2828  dSlice().shard((
-00003ba0: 2831 2c20 7365 6c66 2e64 702c 2073 656c  (1, self.dp, sel
-00003bb0: 662e 6d70 2c20 3129 2c29 290a 2020 2020  f.mp, 1),)).    
-00003bc0: 2020 2020 2020 2020 7365 6c66 2e73 7472          self.str
-00003bd0: 6964 655f 736c 6963 655f 6570 5f6d 7020  ide_slice_ep_mp 
-00003be0: 3d20 502e 5374 7269 6465 6453 6c69 6365  = P.StridedSlice
-00003bf0: 2829 2e73 6861 7264 2828 2873 656c 662e  ().shard(((self.
-00003c00: 6570 2c20 312c 2073 656c 662e 6d70 2c20  ep, 1, self.mp, 
-00003c10: 3129 2c29 290a 2020 2020 2020 2020 656c  1),)).        el
-00003c20: 7365 3a0a 2020 2020 2020 2020 2020 2020  se:.            
-00003c30: 7365 6c66 2e68 6964 6465 6e5f 7369 7a65  self.hidden_size
-00003c40: 203d 2068 6964 6465 6e5f 7369 7a65 0a20   = hidden_size. 
-00003c50: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-00003c60: 6578 7065 7274 5f64 696d 203d 206d 6f65  expert_dim = moe
-00003c70: 5f63 6f6e 6669 672e 6578 7065 7274 5f6e  _config.expert_n
-00003c80: 756d 0a20 2020 2020 2020 2020 2020 2073  um.            s
-00003c90: 656c 662e 6361 7061 6369 7479 5f66 6163  elf.capacity_fac
-00003ca0: 746f 7220 3d20 6d6f 655f 636f 6e66 6967  tor = moe_config
-00003cb0: 2e63 6170 6163 6974 795f 6661 6374 6f72  .capacity_factor
-00003cc0: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-00003cd0: 662e 6175 785f 6c6f 7373 5f66 6163 746f  f.aux_loss_facto
-00003ce0: 7220 3d20 6d6f 655f 636f 6e66 6967 2e61  r = moe_config.a
-00003cf0: 7578 5f6c 6f73 735f 6661 6374 6f72 0a20  ux_loss_factor. 
-00003d00: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-00003d10: 6e75 6d5f 6578 7065 7274 735f 6368 6f73  num_experts_chos
-00003d20: 656e 203d 206d 6f65 5f63 6f6e 6669 672e  en = moe_config.
-00003d30: 6e75 6d5f 6578 7065 7274 735f 6368 6f73  num_experts_chos
-00003d40: 656e 0a20 2020 2020 2020 2020 2020 2073  en.            s
-00003d50: 656c 662e 6470 5f67 726f 7570 203d 2070  elf.dp_group = p
-00003d60: 6172 616c 6c65 6c5f 636f 6e66 6967 2e64  arallel_config.d
-00003d70: 6174 615f 7061 7261 6c6c 656c 0a20 2020  ata_parallel.   
-00003d80: 2020 2020 2020 2020 2073 656c 662e 6470           self.dp
-00003d90: 203d 2070 6172 616c 6c65 6c5f 636f 6e66   = parallel_conf
-00003da0: 6967 2e64 6174 615f 7061 7261 6c6c 656c  ig.data_parallel
-00003db0: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-00003dc0: 662e 6570 203d 2070 6172 616c 6c65 6c5f  f.ep = parallel_
-00003dd0: 636f 6e66 6967 2e65 7870 6572 745f 7061  config.expert_pa
-00003de0: 7261 6c6c 656c 0a20 2020 2020 2020 2020  rallel.         
-00003df0: 2020 2073 656c 662e 6d70 203d 2070 6172     self.mp = par
-00003e00: 616c 6c65 6c5f 636f 6e66 6967 2e6d 6f64  allel_config.mod
-00003e10: 656c 5f70 6172 616c 6c65 6c0a 2020 2020  el_parallel.    
-00003e20: 2020 2020 2020 2020 7365 6c66 2e63 6f6d          self.com
-00003e30: 705f 636f 6d6d 5f70 6172 616c 6c65 6c20  p_comm_parallel 
-00003e40: 3d20 6d6f 655f 636f 6e66 6967 2e63 6f6d  = moe_config.com
-00003e50: 705f 636f 6d6d 5f70 6172 616c 6c65 6c0a  p_comm_parallel.
-00003e60: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-00003e70: 2e63 6f6d 705f 636f 6d6d 5f70 6172 616c  .comp_comm_paral
-00003e80: 6c65 6c5f 6465 6772 6565 203d 206d 6f65  lel_degree = moe
-00003e90: 5f63 6f6e 6669 672e 636f 6d70 5f63 6f6d  _config.comp_com
-00003ea0: 6d5f 7061 7261 6c6c 656c 5f64 6567 7265  m_parallel_degre
-00003eb0: 650a 2020 2020 2020 2020 2020 2020 7365  e.            se
-00003ec0: 6c66 2e67 726f 7570 5f77 6973 655f 6132  lf.group_wise_a2
-00003ed0: 6120 3d20 6d6f 655f 636f 6e66 6967 2e67  a = moe_config.g
-00003ee0: 726f 7570 5f77 6973 655f 6132 610a 2020  roup_wise_a2a.  
-00003ef0: 2020 2020 2020 2020 2020 6966 206e 6f74            if not
-00003f00: 2028 7365 6c66 2e6d 7020 3e20 3120 616e   (self.mp > 1 an
-00003f10: 6420 7365 6c66 2e64 7020 3d3d 2073 656c  d self.dp == sel
-00003f20: 662e 6570 293a 0a20 2020 2020 2020 2020  f.ep):.         
-00003f30: 2020 2020 2020 2073 656c 662e 6772 6f75         self.grou
-00003f40: 705f 7769 7365 5f61 3261 203d 2046 616c  p_wise_a2a = Fal
-00003f50: 7365 0a20 2020 2020 2020 2020 2020 2066  se.            f
-00003f60: 726f 6d20 6d69 6e64 666f 726d 6572 732e  rom mindformers.
-00003f70: 6d6f 6475 6c65 732e 7472 616e 7366 6f72  modules.transfor
-00003f80: 6d65 7220 696d 706f 7274 2046 6565 6446  mer import FeedF
-00003f90: 6f72 7761 7264 0a0a 2020 2020 2020 2020  orward..        
-00003fa0: 2020 2020 7365 6c66 2e66 666e 203d 2046      self.ffn = F
-00003fb0: 6565 6446 6f72 7761 7264 2868 6964 6465  eedForward(hidde
-00003fc0: 6e5f 7369 7a65 3d68 6964 6465 6e5f 7369  n_size=hidden_si
-00003fd0: 7a65 2c0a 2020 2020 2020 2020 2020 2020  ze,.            
-00003fe0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00003ff0: 2020 2020 2020 2066 666e 5f68 6964 6465         ffn_hidde
-00004000: 6e5f 7369 7a65 3d66 666e 5f68 6964 6465  n_size=ffn_hidde
-00004010: 6e5f 7369 7a65 2c0a 2020 2020 2020 2020  n_size,.        
-00004020: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00004030: 2020 2020 2020 2020 2020 2064 726f 706f             dropo
-00004040: 7574 5f72 6174 653d 6472 6f70 6f75 745f  ut_rate=dropout_
-00004050: 7261 7465 2c0a 2020 2020 2020 2020 2020  rate,.          
-00004060: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00004070: 2020 2020 2020 2020 2068 6964 6465 6e5f           hidden_
-00004080: 6163 743d 6869 6464 656e 5f61 6374 2c0a  act=hidden_act,.
-00004090: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000040a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000040b0: 2020 2065 7870 6572 745f 6e75 6d3d 7365     expert_num=se
-000040c0: 6c66 2e65 7870 6572 745f 6469 6d2c 0a20  lf.expert_dim,. 
-000040d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000040e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000040f0: 2020 7061 7261 6d5f 696e 6974 5f74 7970    param_init_typ
-00004100: 653d 7061 7261 6d5f 696e 6974 5f74 7970  e=param_init_typ
-00004110: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
-00004120: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00004130: 2020 2020 2020 7061 7261 6c6c 656c 5f63        parallel_c
-00004140: 6f6e 6669 673d 7061 7261 6c6c 656c 5f63  onfig=parallel_c
-00004150: 6f6e 6669 6729 0a20 2020 2020 2020 2020  onfig).         
-00004160: 2020 2073 656c 662e 7265 7368 6170 6520     self.reshape 
-00004170: 3d20 502e 5265 7368 6170 6528 290a 2020  = P.Reshape().  
-00004180: 2020 2020 2020 2020 2020 7365 6c66 2e73            self.s
-00004190: 6861 7065 203d 2050 2e53 6861 7065 2829  hape = P.Shape()
-000041a0: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-000041b0: 662e 7472 616e 7370 6f73 655f 3264 696d  f.transpose_2dim
-000041c0: 203d 2050 2e54 7261 6e73 706f 7365 2829   = P.Transpose()
-000041d0: 2e73 6861 7264 2828 2873 656c 662e 6470  .shard(((self.dp
-000041e0: 2c20 3129 2c29 290a 2020 2020 2020 2020  , 1),)).        
-000041f0: 2020 2020 7365 6c66 2e74 7261 6e73 706f      self.transpo
-00004200: 7365 5f33 6469 6d20 3d20 502e 5472 616e  se_3dim = P.Tran
-00004210: 7370 6f73 6528 292e 7368 6172 6428 2828  spose().shard(((
-00004220: 7365 6c66 2e64 702c 2031 2c20 3129 2c29  self.dp, 1, 1),)
-00004230: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
-00004240: 6c66 2e74 7261 6e73 706f 7365 5f34 6469  lf.transpose_4di
-00004250: 6d20 3d20 502e 5472 616e 7370 6f73 6528  m = P.Transpose(
-00004260: 292e 7368 6172 6428 2828 312c 2073 656c  ).shard(((1, sel
-00004270: 662e 6470 2c20 312c 2031 292c 2929 0a20  f.dp, 1, 1),)). 
-00004280: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-00004290: 7472 616e 7370 6f73 655f 3464 696d 5f64  transpose_4dim_d
-000042a0: 7020 3d20 502e 5472 616e 7370 6f73 6528  p = P.Transpose(
-000042b0: 292e 7368 6172 6428 2828 312c 2031 2c20  ).shard(((1, 1, 
-000042c0: 7365 6c66 2e64 702c 2031 292c 2929 0a20  self.dp, 1),)). 
-000042d0: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-000042e0: 6261 7463 685f 6d6d 203d 2050 2e42 6174  batch_mm = P.Bat
-000042f0: 6368 4d61 744d 756c 2829 2e73 6861 7264  chMatMul().shard
-00004300: 2828 2873 656c 662e 6470 2c20 312c 2031  (((self.dp, 1, 1
-00004310: 292c 2028 7365 6c66 2e64 702c 2031 2c20  ), (self.dp, 1, 
-00004320: 3129 2929 0a20 2020 2020 2020 2020 2020  1))).           
-00004330: 2073 656c 662e 6261 7463 685f 6d6d 3220   self.batch_mm2 
-00004340: 3d20 502e 4261 7463 684d 6174 4d75 6c28  = P.BatchMatMul(
-00004350: 292e 7368 6172 6428 2828 7365 6c66 2e64  ).shard(((self.d
-00004360: 702c 2031 2c20 3129 2c20 2873 656c 662e  p, 1, 1), (self.
-00004370: 6470 2c20 312c 2031 2929 290a 2020 2020  dp, 1, 1))).    
-00004380: 2020 2020 2020 2020 7365 6c66 2e6d 756c          self.mul
-00004390: 203d 2050 2e4d 756c 2829 2e73 6861 7264   = P.Mul().shard
-000043a0: 2828 2829 2c20 2829 2929 0a20 2020 2020  (((), ())).     
-000043b0: 2020 2020 2020 2073 656c 662e 726f 7574         self.rout
-000043c0: 6572 203d 2052 6f75 7465 7228 645f 6d6f  er = Router(d_mo
-000043d0: 6465 6c3d 6869 6464 656e 5f73 697a 652c  del=hidden_size,
-000043e0: 206d 6f65 5f63 6f6e 6669 673d 6d6f 655f   moe_config=moe_
-000043f0: 636f 6e66 6967 2c20 726f 7574 696e 675f  config, routing_
-00004400: 706f 6c69 6379 3d22 546f 706b 526f 7574  policy="TopkRout
-00004410: 6572 5631 222c 0a20 2020 2020 2020 2020  erV1",.         
-00004420: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00004430: 2020 2020 2020 2020 7472 6169 6e69 6e67          training
-00004440: 3d54 7275 652c 2070 6172 616c 6c65 6c5f  =True, parallel_
-00004450: 636f 6e66 6967 3d70 6172 616c 6c65 6c5f  config=parallel_
-00004460: 636f 6e66 6967 290a 2020 2020 2020 2020  config).        
-00004470: 2020 2020 7365 6c66 2e63 6173 7420 3d20      self.cast = 
-00004480: 502e 4361 7374 2829 0a20 2020 2020 2020  P.Cast().       
-00004490: 2020 2020 2073 656c 662e 636f 6e63 6174       self.concat
-000044a0: 203d 2050 2e43 6f6e 6361 7428 3329 2e73   = P.Concat(3).s
-000044b0: 6861 7264 2874 7570 6c65 2828 7365 6c66  hard(tuple((self
-000044c0: 2e64 702c 2031 2c20 312c 2031 2920 666f  .dp, 1, 1, 1) fo
-000044d0: 7220 5f20 696e 2072 616e 6765 2873 656c  r _ in range(sel
-000044e0: 662e 636f 6d70 5f63 6f6d 6d5f 7061 7261  f.comp_comm_para
-000044f0: 6c6c 656c 5f64 6567 7265 6529 2929 0a20  llel_degree))). 
-00004500: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-00004510: 636f 6e63 6174 5f64 7020 3d20 502e 436f  concat_dp = P.Co
-00004520: 6e63 6174 2832 292e 7368 6172 6428 2828  ncat(2).shard(((
-00004530: 312c 2073 656c 662e 6470 2c20 312c 2031  1, self.dp, 1, 1
-00004540: 292c 2028 312c 2073 656c 662e 6470 2c20  ), (1, self.dp, 
-00004550: 312c 2031 2929 290a 2020 2020 2020 2020  1, 1))).        
-00004560: 2020 2020 7365 6c66 2e73 706c 6974 203d      self.split =
-00004570: 2050 2e53 706c 6974 2861 7869 733d 322c   P.Split(axis=2,
-00004580: 206f 7574 7075 745f 6e75 6d3d 7365 6c66   output_num=self
-00004590: 2e63 6f6d 705f 636f 6d6d 5f70 6172 616c  .comp_comm_paral
-000045a0: 6c65 6c5f 6465 6772 6565 292e 7368 6172  lel_degree).shar
-000045b0: 6428 2828 312c 2073 656c 662e 6470 2c20  d(((1, self.dp, 
-000045c0: 312c 2031 292c 2929 0a20 2020 2020 2020  1, 1),)).       
-000045d0: 2020 2020 2073 656c 662e 7374 7269 6465       self.stride
-000045e0: 5f73 6c69 6365 203d 2050 2e53 7472 6964  _slice = P.Strid
-000045f0: 6564 536c 6963 6528 292e 7368 6172 6428  edSlice().shard(
-00004600: 2828 7365 6c66 2e64 702c 2031 2c20 312c  ((self.dp, 1, 1,
-00004610: 2031 292c 2929 0a20 2020 2020 2020 2020   1),)).         
-00004620: 2020 2073 656c 662e 7374 7269 6465 5f73     self.stride_s
-00004630: 6c69 6365 5f64 7020 3d20 502e 5374 7269  lice_dp = P.Stri
-00004640: 6465 6453 6c69 6365 2829 2e73 6861 7264  dedSlice().shard
-00004650: 2828 2831 2c20 7365 6c66 2e64 702c 2031  (((1, self.dp, 1
-00004660: 2c20 3129 2c29 290a 2020 2020 2020 2020  , 1),)).        
-00004670: 2020 2020 7365 6c66 2e73 7472 6964 655f      self.stride_
-00004680: 736c 6963 655f 6570 203d 2050 2e53 7472  slice_ep = P.Str
-00004690: 6964 6564 536c 6963 6528 292e 7368 6172  idedSlice().shar
-000046a0: 6428 2828 7365 6c66 2e65 702c 2031 2c20  d(((self.ep, 1, 
-000046b0: 312c 2031 292c 2929 0a20 2020 2020 2020  1, 1),)).       
-000046c0: 2020 2020 2073 656c 662e 7374 7269 6465       self.stride
-000046d0: 5f73 6c69 6365 5f64 705f 6d70 203d 2050  _slice_dp_mp = P
-000046e0: 2e53 7472 6964 6564 536c 6963 6528 292e  .StridedSlice().
-000046f0: 7368 6172 6428 2828 312c 2073 656c 662e  shard(((1, self.
-00004700: 6470 2c20 7365 6c66 2e6d 702c 2031 292c  dp, self.mp, 1),
-00004710: 2929 0a20 2020 2020 2020 2020 2020 2073  )).            s
-00004720: 656c 662e 7374 7269 6465 5f73 6c69 6365  elf.stride_slice
-00004730: 5f65 705f 6d70 203d 2050 2e53 7472 6964  _ep_mp = P.Strid
-00004740: 6564 536c 6963 6528 292e 7368 6172 6428  edSlice().shard(
-00004750: 2828 7365 6c66 2e65 702c 2031 2c20 7365  ((self.ep, 1, se
-00004760: 6c66 2e6d 702c 2031 292c 2929 0a20 2020  lf.mp, 1),)).   
-00004770: 2020 2020 2020 2020 2073 656c 662e 656e           self.en
-00004780: 6162 6c65 5f63 6f6c 645f 686f 745f 6578  able_cold_hot_ex
-00004790: 7065 7274 203d 206d 6f65 5f63 6f6e 6669  pert = moe_confi
-000047a0: 672e 656e 6162 6c65 5f63 6f6c 645f 686f  g.enable_cold_ho
-000047b0: 745f 6578 7065 7274 0a20 2020 2020 2020  t_expert.       
-000047c0: 2020 2020 2069 6620 7365 6c66 2e65 6e61       if self.ena
-000047d0: 626c 655f 636f 6c64 5f68 6f74 5f65 7870  ble_cold_hot_exp
-000047e0: 6572 743a 0a20 2020 2020 2020 2020 2020  ert:.           
-000047f0: 2020 2020 2073 656c 662e 6375 725f 6c61       self.cur_la
-00004800: 7965 7220 3d20 6d6f 655f 636f 6e66 6967  yer = moe_config
-00004810: 2e63 7572 5f6c 6179 6572 0a20 2020 2020  .cur_layer.     
-00004820: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-00004830: 686f 745f 6578 7065 7274 5f6e 756d 203d  hot_expert_num =
-00004840: 206d 6f65 5f63 6f6e 6669 672e 686f 745f   moe_config.hot_
-00004850: 6578 7065 7274 5f6e 756d 0a20 2020 2020  expert_num.     
-00004860: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-00004870: 7570 6461 7465 5f73 7465 7020 3d20 6d6f  update_step = mo
-00004880: 655f 636f 6e66 6967 2e75 7064 6174 655f  e_config.update_
-00004890: 7374 6570 0a20 2020 2020 2020 2020 2020  step.           
-000048a0: 2020 2020 2073 656c 662e 636f 6c64 5f74       self.cold_t
-000048b0: 6f6b 656e 5f70 6572 6365 6e74 203d 206d  oken_percent = m
-000048c0: 6f65 5f63 6f6e 6669 672e 636f 6c64 5f74  oe_config.cold_t
-000048d0: 6f6b 656e 5f70 6572 6365 6e74 0a20 2020  oken_percent.   
-000048e0: 2020 2020 2020 2020 2020 2020 2073 656c               sel
-000048f0: 662e 686f 745f 6578 7065 7274 5f69 6e64  f.hot_expert_ind
-00004900: 6578 203d 2050 6172 616d 6574 6572 280a  ex = Parameter(.
-00004910: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00004920: 2020 2020 696e 6974 6961 6c69 7a65 7228      initializer(
-00004930: 5465 6e73 6f72 285b 5b69 2066 6f72 2069  Tensor([[i for i
-00004940: 2069 6e20 7261 6e67 6528 7365 6c66 2e68   in range(self.h
-00004950: 6f74 5f65 7870 6572 745f 6e75 6d29 5d5d  ot_expert_num)]]
-00004960: 2c20 6d73 7479 7065 2e69 6e74 3332 292c  , mstype.int32),
-00004970: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00004980: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00004990: 2028 312c 2073 656c 662e 686f 745f 6578   (1, self.hot_ex
-000049a0: 7065 7274 5f6e 756d 2c29 2c20 6d73 7479  pert_num,), msty
-000049b0: 7065 2e69 6e74 3332 292c 0a20 2020 2020  pe.int32),.     
-000049c0: 2020 2020 2020 2020 2020 2020 2020 206e                 n
-000049d0: 616d 653d 2268 6f74 5f65 7870 6572 745f  ame="hot_expert_
-000049e0: 696e 6465 7822 2b73 7472 2873 656c 662e  index"+str(self.
-000049f0: 6375 725f 6c61 7965 7229 2c0a 2020 2020  cur_layer),.    
+00000380: 7920 6173 206e 700a 0a66 726f 6d20 6d69  y as np..from mi
+00000390: 6e64 7370 6f72 652e 636f 6d6d 6f6e 2e74  ndspore.common.t
+000003a0: 656e 736f 7220 696d 706f 7274 2054 656e  ensor import Ten
+000003b0: 736f 720a 6672 6f6d 206d 696e 6473 706f  sor.from mindspo
+000003c0: 7265 2e63 6f6d 6d6f 6e2e 7061 7261 6d65  re.common.parame
+000003d0: 7465 7220 696d 706f 7274 2050 6172 616d  ter import Param
+000003e0: 6574 6572 0a66 726f 6d20 6d69 6e64 7370  eter.from mindsp
+000003f0: 6f72 652e 636f 6d6d 6f6e 2e69 6e69 7469  ore.common.initi
+00000400: 616c 697a 6572 2069 6d70 6f72 7420 696e  alizer import in
+00000410: 6974 6961 6c69 7a65 720a 696d 706f 7274  itializer.import
+00000420: 206d 696e 6473 706f 7265 2e63 6f6d 6d6f   mindspore.commo
+00000430: 6e2e 6474 7970 6520 6173 206d 7374 7970  n.dtype as mstyp
+00000440: 650a 696d 706f 7274 206d 696e 6473 706f  e.import mindspo
+00000450: 7265 2e63 6f6d 6d75 6e69 6361 7469 6f6e  re.communication
+00000460: 2e6d 616e 6167 656d 656e 7420 6173 2044  .management as D
+00000470: 0a0a 2320 4d69 6e64 5370 6f72 6520 322e  ..# MindSpore 2.
+00000480: 3020 6861 7320 6368 616e 6765 6420 7468  0 has changed th
+00000490: 6520 4150 4973 206f 6620 5f63 6865 636b  e APIs of _check
+000004a0: 7061 7261 6d2c 2074 6865 2066 6f6c 6c6f  param, the follo
+000004b0: 7769 6e67 2074 7279 2065 7863 6570 7420  wing try except 
+000004c0: 6973 2066 6f72 2063 6f6d 7061 7469 6269  is for compatibi
+000004d0: 6c69 7479 0a74 7279 3a0a 2020 2020 6672  lity.try:.    fr
+000004e0: 6f6d 206d 696e 6473 706f 7265 2e5f 6368  om mindspore._ch
+000004f0: 6563 6b70 6172 616d 2069 6d70 6f72 7420  eckparam import 
+00000500: 5661 6c69 6461 746f 720a 6578 6365 7074  Validator.except
+00000510: 2049 6d70 6f72 7445 7272 6f72 3a0a 2020   ImportError:.  
+00000520: 2020 696d 706f 7274 206d 696e 6473 706f    import mindspo
+00000530: 7265 2e5f 6368 6563 6b70 6172 616d 2061  re._checkparam a
+00000540: 7320 5661 6c69 6461 746f 720a 6672 6f6d  s Validator.from
+00000550: 206d 696e 6473 706f 7265 2e6f 7073 2069   mindspore.ops i
+00000560: 6d70 6f72 7420 6f70 6572 6174 696f 6e73  mport operations
+00000570: 2061 7320 500a 6672 6f6d 206d 696e 6473   as P.from minds
+00000580: 706f 7265 2e6f 7073 2069 6d70 6f72 7420  pore.ops import 
+00000590: 6675 6e63 7469 6f6e 616c 2061 7320 460a  functional as F.
+000005a0: 6672 6f6d 206d 696e 6473 706f 7265 2e6f  from mindspore.o
+000005b0: 7073 2e70 7269 6d69 7469 7665 2069 6d70  ps.primitive imp
+000005c0: 6f72 7420 636f 6e73 7465 7870 720a 6672  ort constexpr.fr
+000005d0: 6f6d 206d 696e 6473 706f 7265 2e6e 6e2e  om mindspore.nn.
+000005e0: 6365 6c6c 2069 6d70 6f72 7420 4365 6c6c  cell import Cell
+000005f0: 0a66 726f 6d20 6d69 6e64 7370 6f72 652e  .from mindspore.
+00000600: 6e6e 2e6c 6179 6572 2069 6d70 6f72 7420  nn.layer import 
+00000610: 4465 6e73 650a 6672 6f6d 206d 696e 6473  Dense.from minds
+00000620: 706f 7265 2e63 6f6e 7465 7874 2069 6d70  pore.context imp
+00000630: 6f72 7420 5061 7261 6c6c 656c 4d6f 6465  ort ParallelMode
+00000640: 0a66 726f 6d20 6d69 6e64 7370 6f72 652e  .from mindspore.
+00000650: 7061 7261 6c6c 656c 2e5f 7574 696c 7320  parallel._utils 
+00000660: 696d 706f 7274 205f 6765 745f 7061 7261  import _get_para
+00000670: 6c6c 656c 5f6d 6f64 652c 205f 6973 5f73  llel_mode, _is_s
+00000680: 6861 7264 696e 675f 7072 6f70 6167 6174  harding_propagat
+00000690: 696f 6e0a 6672 6f6d 206d 696e 6466 6f72  ion.from mindfor
+000006a0: 6d65 7273 2e6d 6f64 756c 6573 2e74 7261  mers.modules.tra
+000006b0: 6e73 666f 726d 6572 2e6f 705f 7061 7261  nsformer.op_para
+000006c0: 6c6c 656c 5f63 6f6e 6669 6720 696d 706f  llel_config impo
+000006d0: 7274 2064 6566 6175 6c74 5f6d 6f65 7061  rt default_moepa
+000006e0: 7261 6c6c 656c 5f63 6f6e 6669 672c 204d  rallel_config, M
+000006f0: 6f45 5061 7261 6c6c 656c 436f 6e66 6967  oEParallelConfig
+00000700: 0a0a 5f5f 616c 6c5f 5f20 3d20 5b0a 2020  ..__all__ = [.  
+00000710: 2020 224d 6f45 436f 6e66 6967 225d 0a0a    "MoEConfig"]..
+00000720: 0a63 6c61 7373 204d 6f45 436f 6e66 6967  .class MoEConfig
+00000730: 3a0a 2020 2020 7222 2222 0a20 2020 2020  :.    r""".     
+00000740: 2020 2054 6865 2063 6f6e 6669 6775 7261     The configura
+00000750: 7469 6f6e 206f 6620 4d6f 4520 284d 6978  tion of MoE (Mix
+00000760: 7475 7265 206f 6620 4578 7065 7274 292e  ture of Expert).
+00000770: 0a0a 2020 2020 2020 2020 4172 6773 3a0a  ..        Args:.
+00000780: 2020 2020 2020 2020 2020 2020 6578 7065              expe
+00000790: 7274 5f6e 756d 2028 696e 7429 3a20 5468  rt_num (int): Th
+000007a0: 6520 6e75 6d62 6572 206f 6620 6578 7065  e number of expe
+000007b0: 7274 7320 656d 706c 6f79 6564 2e20 4465  rts employed. De
+000007c0: 6661 756c 743a 2031 0a20 2020 2020 2020  fault: 1.       
+000007d0: 2020 2020 2063 6170 6163 6974 795f 6661       capacity_fa
+000007e0: 6374 6f72 2028 666c 6f61 7429 3a20 5468  ctor (float): Th
+000007f0: 6520 6661 6374 6f72 2069 7320 7573 6564  e factor is used
+00000800: 2074 6f20 696e 6469 6361 7465 2068 6f77   to indicate how
+00000810: 206d 7563 6820 746f 2065 7870 616e 6420   much to expand 
+00000820: 6578 7065 7274 2063 6170 6163 6974 792c  expert capacity,
+00000830: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00000840: 2077 6869 6368 2069 7320 3e3d 312e 302e   which is >=1.0.
+00000850: 2044 6566 6175 6c74 3a20 312e 312e 0a20   Default: 1.1.. 
+00000860: 2020 2020 2020 2020 2020 2061 7578 5f6c             aux_l
+00000870: 6f73 735f 6661 6374 6f72 2028 666c 6f61  oss_factor (floa
+00000880: 7429 3a20 5468 6520 6661 6374 6f72 2069  t): The factor i
+00000890: 7320 7573 6564 2074 6f20 696e 6469 6361  s used to indica
+000008a0: 7465 2068 6f77 206d 7563 6820 7468 6520  te how much the 
+000008b0: 6c6f 6164 2062 616c 616e 6365 206c 6f73  load balance los
+000008c0: 7320 2870 726f 6475 6365 6420 6279 2074  s (produced by t
+000008d0: 6865 0a20 2020 2020 2020 2020 2020 2020  he.             
+000008e0: 2020 2072 6f75 7465 7229 2074 6f20 6265     router) to be
+000008f0: 2061 6464 6564 2074 6f20 7468 6520 656e   added to the en
+00000900: 7469 7265 206d 6f64 656c 206c 6f73 732c  tire model loss,
+00000910: 2077 6869 6368 2069 7320 3c20 312e 302e   which is < 1.0.
+00000920: 2044 6566 6175 6c74 3a20 302e 3035 2e0a   Default: 0.05..
+00000930: 2020 2020 2020 2020 2020 2020 6e75 6d5f              num_
+00000940: 6578 7065 7274 735f 6368 6f73 656e 2028  experts_chosen (
+00000950: 696e 7429 3a20 5468 6520 6e75 6d62 6572  int): The number
+00000960: 206f 6620 6578 7065 7274 7320 6973 2063   of experts is c
+00000970: 686f 7365 6e20 6279 2065 6163 6820 746f  hosen by each to
+00000980: 6b65 6e20 616e 6420 6974 2073 686f 756c  ken and it shoul
+00000990: 6420 6e6f 7420 6265 206c 6172 6765 720a  d not be larger.
+000009a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000009b0: 7468 616e 2065 7870 6572 745f 6e75 6d2e  than expert_num.
+000009c0: 2044 6566 6175 6c74 3a20 312e 0a20 2020   Default: 1..   
+000009d0: 2020 2020 2020 2020 2065 7870 6572 745f           expert_
+000009e0: 6772 6f75 705f 7369 7a65 2028 696e 7429  group_size (int)
+000009f0: 3a20 5468 6520 6e75 6d62 6572 206f 6620  : The number of 
+00000a00: 746f 6b65 6e73 2069 6e20 6561 6368 2064  tokens in each d
+00000a10: 6174 6120 7061 7261 6c6c 656c 2067 726f  ata parallel gro
+00000a20: 7570 2e20 4465 6661 756c 743a 204e 6f6e  up. Default: Non
+00000a30: 652e 2054 6869 7320 7061 7261 6d65 7465  e. This paramete
+00000a40: 7220 6973 0a20 2020 2020 2020 2020 2020  r is.           
+00000a50: 2020 2020 2065 6666 6563 7469 7665 206f       effective o
+00000a60: 6e6c 7920 7768 656e 2069 6e20 4155 544f  nly when in AUTO
+00000a70: 5f50 4152 414c 4c45 4c20 6d6f 6465 2c20  _PARALLEL mode, 
+00000a80: 616e 6420 4e4f 5420 5348 4152 4449 4e47  and NOT SHARDING
+00000a90: 5f50 524f 5041 4741 5449 4f4e 2e0a 2020  _PROPAGATION..  
+00000aa0: 2020 2020 2020 2020 2020 6772 6f75 705f            group_
+00000ab0: 7769 7365 5f61 3261 2028 626f 6f6c 293a  wise_a2a (bool):
+00000ac0: 2057 6865 7468 6572 2074 6f20 656e 6162   Whether to enab
+00000ad0: 6c65 2067 726f 7570 2d77 6973 6520 616c  le group-wise al
+00000ae0: 6c74 6f61 6c6c 2063 6f6d 6d75 6e69 6361  ltoall communica
+00000af0: 7469 6f6e 2c20 7768 6963 6820 6361 6e20  tion, which can 
+00000b00: 7265 6475 6365 2063 6f6d 6d75 6e69 6361  reduce communica
+00000b10: 7469 6f6e 0a20 2020 2020 2020 2020 2020  tion.           
+00000b20: 2020 2020 2074 696d 6520 6279 2063 6f6e       time by con
+00000b30: 7665 7274 696e 6720 7061 7274 206f 6620  verting part of 
+00000b40: 696e 7465 7220 636f 6d6d 756e 6963 6174  inter communicat
+00000b50: 696f 6e20 696e 746f 2069 6e74 7261 2063  ion into intra c
+00000b60: 6f6d 6d75 6e69 6361 7469 6f6e 2e20 4465  ommunication. De
+00000b70: 6661 756c 743a 2046 616c 7365 2e20 5468  fault: False. Th
+00000b80: 6973 2070 6172 616d 6574 6572 0a20 2020  is parameter.   
+00000b90: 2020 2020 2020 2020 2020 2020 2069 7320               is 
+00000ba0: 6566 6665 6374 6976 6520 6f6e 6c79 2077  effective only w
+00000bb0: 6865 6e20 6d6f 6465 6c20 7061 7261 6c6c  hen model parall
+00000bc0: 656c 203e 2031 2061 6e64 2064 6174 615f  el > 1 and data_
+00000bd0: 7061 7261 6c6c 656c 2065 7175 616c 2074  parallel equal t
+00000be0: 6f20 6578 7065 7274 2070 6172 616c 6c65  o expert paralle
+00000bf0: 6c2e 0a20 2020 2020 2020 2020 2020 2063  l..            c
+00000c00: 6f6d 705f 636f 6d6d 5f70 6172 616c 6c65  omp_comm_paralle
+00000c10: 6c20 2862 6f6f 6c29 3a20 5768 6574 6865  l (bool): Whethe
+00000c20: 7220 746f 2065 6e61 626c 6520 6666 6e20  r to enable ffn 
+00000c30: 636f 6d70 7574 6520 616e 6420 636f 6d6d  compute and comm
+00000c40: 756e 6963 6174 696f 6e20 7061 7261 6c6c  unication parall
+00000c50: 656c 2c20 7768 6963 6820 6361 6e20 7265  el, which can re
+00000c60: 6475 6365 2070 7572 650a 2020 2020 2020  duce pure.      
+00000c70: 2020 2020 2020 2020 2020 636f 6d6d 756e            commun
+00000c80: 6963 6174 7469 6f6e 2074 696d 6520 6279  icattion time by
+00000c90: 2073 706c 6974 7469 6e67 2061 6e64 206f   splitting and o
+00000ca0: 7665 726c 6170 7069 6e67 2063 6f6d 7075  verlapping compu
+00000cb0: 7465 2061 6e64 2063 6f6d 6d75 6e69 6361  te and communica
+00000cc0: 7469 6f6e 2e20 4465 6661 756c 743a 2046  tion. Default: F
+00000cd0: 616c 7365 2e0a 2020 2020 2020 2020 2020  alse..          
+00000ce0: 2020 636f 6d70 5f63 6f6d 6d5f 7061 7261    comp_comm_para
+00000cf0: 6c6c 656c 5f64 6567 7265 6520 2869 6e74  llel_degree (int
+00000d00: 293a 2054 6865 2073 706c 6974 206e 756d  ): The split num
+00000d10: 6265 7220 6f66 2063 6f6d 7075 7465 2061  ber of compute a
+00000d20: 6e64 2063 6f6d 6d75 6e69 6361 7469 6f6e  nd communication
+00000d30: 2e20 5468 6520 6c61 7267 6572 2074 6865  . The larger the
+00000d40: 206e 756d 6265 7273 2c0a 2020 2020 2020   numbers,.      
+00000d50: 2020 2020 2020 2020 2020 7468 6520 6d6f            the mo
+00000d60: 7265 206f 7665 726c 6170 2074 6865 7265  re overlap there
+00000d70: 2077 696c 6c20 6265 2062 7574 2077 696c   will be but wil
+00000d80: 6c20 636f 6e73 756d 6520 6d6f 7265 206d  l consume more m
+00000d90: 656d 6f72 792e 2044 6566 6175 6c74 3a20  emory. Default: 
+00000da0: 322e 2054 6869 7320 7061 7261 6d65 7465  2. This paramete
+00000db0: 7220 6973 2065 6666 6563 7469 7665 0a20  r is effective. 
+00000dc0: 2020 2020 2020 2020 2020 2020 2020 206f                 o
+00000dd0: 6e6c 7920 7768 656e 2063 6f6d 705f 636f  nly when comp_co
+00000de0: 6d6d 5f70 6172 616c 6c65 6c20 656e 6162  mm_parallel enab
+00000df0: 6c65 2e0a 2020 2020 2020 2020 2020 2020  le..            
+00000e00: 726f 7574 696e 675f 706f 6c69 6379 2028  routing_policy (
+00000e10: 7374 7229 3a20 5468 6520 726f 7574 696e  str): The routin
+00000e20: 6720 706f 6c69 6379 2074 6f20 7573 6520  g policy to use 
+00000e30: 696e 204d 6f45 206c 6179 6572 2e20 4465  in MoE layer. De
+00000e40: 6661 756c 743a 2054 6f70 6b52 6f75 7465  fault: TopkRoute
+00000e50: 7256 312e 0a0a 2020 2020 2020 2020 5375  rV1...        Su
+00000e60: 7070 6f72 7465 6420 506c 6174 666f 726d  pported Platform
+00000e70: 733a 0a20 2020 2020 2020 2020 2020 2060  s:.            `
+00000e80: 6041 7363 656e 6460 6020 6060 4750 5560  `Ascend`` ``GPU`
+00000e90: 600a 0a20 2020 2020 2020 2045 7861 6d70  `..        Examp
+00000ea0: 6c65 733a 0a20 2020 2020 2020 2020 2020  les:.           
+00000eb0: 203e 3e3e 2066 726f 6d20 6d69 6e64 666f   >>> from mindfo
+00000ec0: 726d 6572 732e 6d6f 6475 6c65 732e 7472  rmers.modules.tr
+00000ed0: 616e 7366 6f72 6d65 7220 696d 706f 7274  ansformer import
+00000ee0: 204d 6f45 436f 6e66 6967 0a20 2020 2020   MoEConfig.     
+00000ef0: 2020 2020 2020 203e 3e3e 206d 6f65 5f63         >>> moe_c
+00000f00: 6f6e 6669 6720 3d20 4d6f 4543 6f6e 6669  onfig = MoEConfi
+00000f10: 6728 6578 7065 7274 5f6e 756d 3d34 2c20  g(expert_num=4, 
+00000f20: 6361 7061 6369 7479 5f66 6163 746f 723d  capacity_factor=
+00000f30: 352e 302c 2061 7578 5f6c 6f73 735f 6661  5.0, aux_loss_fa
+00000f40: 6374 6f72 3d30 2e30 352c 206e 756d 5f65  ctor=0.05, num_e
+00000f50: 7870 6572 7473 5f63 686f 7365 6e3d 312c  xperts_chosen=1,
+00000f60: 0a20 2020 2020 2020 2020 2020 202e 2e2e  .            ...
+00000f70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000f80: 2020 2020 2020 2020 6578 7065 7274 5f67          expert_g
+00000f90: 726f 7570 5f73 697a 653d 3634 2c20 6772  roup_size=64, gr
+00000fa0: 6f75 705f 7769 7365 5f61 3261 3d54 7275  oup_wise_a2a=Tru
+00000fb0: 652c 2063 6f6d 705f 636f 6d6d 5f70 6172  e, comp_comm_par
+00000fc0: 616c 6c65 6c3d 4661 6c73 652c 0a20 2020  allel=False,.   
+00000fd0: 2020 2020 2020 2020 202e 2e2e 2020 2020           ...    
+00000fe0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000ff0: 2020 2020 636f 6d70 5f63 6f6d 6d5f 7061      comp_comm_pa
+00001000: 7261 6c6c 656c 5f64 6567 7265 653d 322c  rallel_degree=2,
+00001010: 2072 6f75 7469 6e67 5f70 6f6c 6963 793d   routing_policy=
+00001020: 2254 6f70 6b52 6f75 7465 7256 3222 290a  "TopkRouterV2").
+00001030: 2020 2020 2222 220a 0a20 2020 2064 6566      """..    def
+00001040: 205f 5f69 6e69 745f 5f28 7365 6c66 2c20   __init__(self, 
+00001050: 6578 7065 7274 5f6e 756d 3d31 2c20 6361  expert_num=1, ca
+00001060: 7061 6369 7479 5f66 6163 746f 723d 312e  pacity_factor=1.
+00001070: 312c 2061 7578 5f6c 6f73 735f 6661 6374  1, aux_loss_fact
+00001080: 6f72 3d30 2e30 352c 206e 756d 5f65 7870  or=0.05, num_exp
+00001090: 6572 7473 5f63 686f 7365 6e3d 312c 0a20  erts_chosen=1,. 
+000010a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000010b0: 6578 7065 7274 5f67 726f 7570 5f73 697a  expert_group_siz
+000010c0: 653d 4e6f 6e65 2c20 6772 6f75 705f 7769  e=None, group_wi
+000010d0: 7365 5f61 3261 3d46 616c 7365 2c20 636f  se_a2a=False, co
+000010e0: 6d70 5f63 6f6d 6d5f 7061 7261 6c6c 656c  mp_comm_parallel
+000010f0: 3d46 616c 7365 2c20 636f 6d70 5f63 6f6d  =False, comp_com
+00001100: 6d5f 7061 7261 6c6c 656c 5f64 6567 7265  m_parallel_degre
+00001110: 653d 322c 0a20 2020 2020 2020 2020 2020  e=2,.           
+00001120: 2020 2020 2020 7361 7665 5f74 6f6b 656e        save_token
+00001130: 5f64 6973 7472 6962 7574 696f 6e3d 4661  _distribution=Fa
+00001140: 6c73 652c 2063 7572 5f6c 6179 6572 3d30  lse, cur_layer=0
+00001150: 2c20 656e 6162 6c65 5f63 6f6c 645f 686f  , enable_cold_ho
+00001160: 745f 6578 7065 7274 3d46 616c 7365 2c20  t_expert=False, 
+00001170: 7570 6461 7465 5f73 7465 703d 3130 3030  update_step=1000
+00001180: 302c 0a20 2020 2020 2020 2020 2020 2020  0,.             
+00001190: 2020 2020 686f 745f 6578 7065 7274 5f6e      hot_expert_n
+000011a0: 756d 3d30 2c20 636f 6c64 5f74 6f6b 656e  um=0, cold_token
+000011b0: 5f70 6572 6365 6e74 3d31 2e30 2c20 6d6f  _percent=1.0, mo
+000011c0: 655f 6d6f 6475 6c65 5f6e 616d 653d 2222  e_module_name=""
+000011d0: 2c20 726f 7574 696e 675f 706f 6c69 6379  , routing_policy
+000011e0: 3d22 546f 706b 526f 7574 6572 5631 2229  ="TopkRouterV1")
+000011f0: 3a0a 2020 2020 2020 2020 5661 6c69 6461  :.        Valida
+00001200: 746f 722e 6368 6563 6b5f 706f 7369 7469  tor.check_positi
+00001210: 7665 5f69 6e74 2865 7870 6572 745f 6e75  ve_int(expert_nu
+00001220: 6d2c 2022 6578 7065 7274 5f6e 756d 2229  m, "expert_num")
+00001230: 0a20 2020 2020 2020 2056 616c 6964 6174  .        Validat
+00001240: 6f72 2e63 6865 636b 5f70 6f73 6974 6976  or.check_positiv
+00001250: 655f 666c 6f61 7428 6361 7061 6369 7479  e_float(capacity
+00001260: 5f66 6163 746f 722c 2022 6361 7061 6369  _factor, "capaci
+00001270: 7479 5f66 6163 746f 7222 290a 2020 2020  ty_factor").    
+00001280: 2020 2020 5661 6c69 6461 746f 722e 6368      Validator.ch
+00001290: 6563 6b5f 706f 7369 7469 7665 5f66 6c6f  eck_positive_flo
+000012a0: 6174 2861 7578 5f6c 6f73 735f 6661 6374  at(aux_loss_fact
+000012b0: 6f72 2c20 2261 7578 5f6c 6f73 735f 6661  or, "aux_loss_fa
+000012c0: 6374 6f72 2229 0a20 2020 2020 2020 2056  ctor").        V
+000012d0: 616c 6964 6174 6f72 2e63 6865 636b 5f70  alidator.check_p
+000012e0: 6f73 6974 6976 655f 696e 7428 6e75 6d5f  ositive_int(num_
+000012f0: 6578 7065 7274 735f 6368 6f73 656e 2c20  experts_chosen, 
+00001300: 226e 756d 5f65 7870 6572 7473 5f63 686f  "num_experts_cho
+00001310: 7365 6e22 290a 2020 2020 2020 2020 5661  sen").        Va
+00001320: 6c69 6461 746f 722e 6368 6563 6b5f 626f  lidator.check_bo
+00001330: 6f6c 2867 726f 7570 5f77 6973 655f 6132  ol(group_wise_a2
+00001340: 612c 2022 6772 6f75 705f 7769 7365 5f61  a, "group_wise_a
+00001350: 3261 2229 0a20 2020 2020 2020 2056 616c  2a").        Val
+00001360: 6964 6174 6f72 2e63 6865 636b 5f62 6f6f  idator.check_boo
+00001370: 6c28 636f 6d70 5f63 6f6d 6d5f 7061 7261  l(comp_comm_para
+00001380: 6c6c 656c 2c20 2263 6f6d 705f 636f 6d6d  llel, "comp_comm
+00001390: 5f70 6172 616c 6c65 6c22 290a 2020 2020  _parallel").    
+000013a0: 2020 2020 5661 6c69 6461 746f 722e 6368      Validator.ch
+000013b0: 6563 6b5f 706f 7369 7469 7665 5f69 6e74  eck_positive_int
+000013c0: 2863 6f6d 705f 636f 6d6d 5f70 6172 616c  (comp_comm_paral
+000013d0: 6c65 6c5f 6465 6772 6565 2c20 2263 6f6d  lel_degree, "com
+000013e0: 705f 636f 6d6d 5f70 6172 616c 6c65 6c5f  p_comm_parallel_
+000013f0: 6465 6772 6565 2229 0a20 2020 2020 2020  degree").       
+00001400: 2056 616c 6964 6174 6f72 2e63 6865 636b   Validator.check
+00001410: 5f62 6f6f 6c28 7361 7665 5f74 6f6b 656e  _bool(save_token
+00001420: 5f64 6973 7472 6962 7574 696f 6e2c 2022  _distribution, "
+00001430: 7361 7665 5f74 6f6b 656e 5f64 6973 7472  save_token_distr
+00001440: 6962 7574 696f 6e22 290a 2020 2020 2020  ibution").      
+00001450: 2020 5661 6c69 6461 746f 722e 6368 6563    Validator.chec
+00001460: 6b5f 6e6f 6e5f 6e65 6761 7469 7665 5f69  k_non_negative_i
+00001470: 6e74 2863 7572 5f6c 6179 6572 2c20 2263  nt(cur_layer, "c
+00001480: 7572 5f6c 6179 6572 2229 0a20 2020 2020  ur_layer").     
+00001490: 2020 2056 616c 6964 6174 6f72 2e63 6865     Validator.che
+000014a0: 636b 5f62 6f6f 6c28 656e 6162 6c65 5f63  ck_bool(enable_c
+000014b0: 6f6c 645f 686f 745f 6578 7065 7274 2c20  old_hot_expert, 
+000014c0: 2265 6e61 626c 655f 636f 6c64 5f68 6f74  "enable_cold_hot
+000014d0: 5f65 7870 6572 7422 290a 2020 2020 2020  _expert").      
+000014e0: 2020 5661 6c69 6461 746f 722e 6368 6563    Validator.chec
+000014f0: 6b5f 706f 7369 7469 7665 5f69 6e74 2875  k_positive_int(u
+00001500: 7064 6174 655f 7374 6570 2c20 2275 7064  pdate_step, "upd
+00001510: 6174 655f 7374 6570 2229 0a20 2020 2020  ate_step").     
+00001520: 2020 2056 616c 6964 6174 6f72 2e63 6865     Validator.che
+00001530: 636b 5f6e 6f6e 5f6e 6567 6174 6976 655f  ck_non_negative_
+00001540: 696e 7428 686f 745f 6578 7065 7274 5f6e  int(hot_expert_n
+00001550: 756d 2c20 2268 6f74 5f65 7870 6572 745f  um, "hot_expert_
+00001560: 6e75 6d22 290a 2020 2020 2020 2020 5661  num").        Va
+00001570: 6c69 6461 746f 722e 6368 6563 6b5f 6e6f  lidator.check_no
+00001580: 6e5f 6e65 6761 7469 7665 5f66 6c6f 6174  n_negative_float
+00001590: 2863 6f6c 645f 746f 6b65 6e5f 7065 7263  (cold_token_perc
+000015a0: 656e 742c 2022 636f 6c64 5f74 6f6b 656e  ent, "cold_token
+000015b0: 5f70 6572 6365 6e74 2229 0a20 2020 2020  _percent").     
+000015c0: 2020 2069 6620 6578 7065 7274 5f67 726f     if expert_gro
+000015d0: 7570 5f73 697a 6520 6973 206e 6f74 204e  up_size is not N
+000015e0: 6f6e 653a 0a20 2020 2020 2020 2020 2020  one:.           
+000015f0: 2056 616c 6964 6174 6f72 2e63 6865 636b   Validator.check
+00001600: 5f70 6f73 6974 6976 655f 696e 7428 6578  _positive_int(ex
+00001610: 7065 7274 5f67 726f 7570 5f73 697a 652c  pert_group_size,
+00001620: 2022 6578 7065 7274 5f67 726f 7570 5f73   "expert_group_s
+00001630: 697a 6522 290a 2020 2020 2020 2020 6966  ize").        if
+00001640: 2063 6170 6163 6974 795f 6661 6374 6f72   capacity_factor
+00001650: 203c 2031 2e30 3a0a 2020 2020 2020 2020   < 1.0:.        
+00001660: 2020 2020 7261 6973 6520 5661 6c75 6545      raise ValueE
+00001670: 7272 6f72 2866 2227 6361 7061 6369 7479  rror(f"'capacity
+00001680: 5f66 6163 746f 7227 206d 7573 7420 6265  _factor' must be
+00001690: 2065 7175 616c 2074 6f20 6f72 2067 7265   equal to or gre
+000016a0: 6174 6572 2074 6861 6e20 312e 302c 2022  ater than 1.0, "
+000016b0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+000016c0: 2020 2020 2020 2020 2020 2020 2020 6622                f"
+000016d0: 6275 7420 676f 7420 7b63 6170 6163 6974  but got {capacit
+000016e0: 795f 6661 6374 6f72 7d2e 2229 0a20 2020  y_factor}.").   
+000016f0: 2020 2020 2069 6620 6175 785f 6c6f 7373       if aux_loss
+00001700: 5f66 6163 746f 7220 3e3d 2031 2e30 3a0a  _factor >= 1.0:.
+00001710: 2020 2020 2020 2020 2020 2020 7261 6973              rais
+00001720: 6520 5661 6c75 6545 7272 6f72 2866 2227  e ValueError(f"'
+00001730: 6175 785f 6c6f 7373 5f66 6163 746f 7227  aux_loss_factor'
+00001740: 206d 7573 7420 6265 206c 6573 7320 7468   must be less th
+00001750: 616e 2031 2e30 2c20 220a 2020 2020 2020  an 1.0, ".      
+00001760: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001770: 2020 2020 2020 2066 2262 7574 2067 6f74         f"but got
+00001780: 207b 6175 785f 6c6f 7373 5f66 6163 746f   {aux_loss_facto
+00001790: 727d 2e22 290a 2020 2020 2020 2020 6966  r}.").        if
+000017a0: 206e 756d 5f65 7870 6572 7473 5f63 686f   num_experts_cho
+000017b0: 7365 6e20 3e20 6578 7065 7274 5f6e 756d  sen > expert_num
+000017c0: 3a0a 2020 2020 2020 2020 2020 2020 7261  :.            ra
+000017d0: 6973 6520 5661 6c75 6545 7272 6f72 2866  ise ValueError(f
+000017e0: 2227 6e75 6d5f 6578 7065 7274 735f 6368  "'num_experts_ch
+000017f0: 6f73 656e 2720 6d75 7374 206e 6f74 2062  osen' must not b
+00001800: 6520 6c61 7267 6572 2074 6861 6e20 2765  e larger than 'e
+00001810: 7870 6572 745f 6e75 6d27 2c20 220a 2020  xpert_num', ".  
+00001820: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001830: 2020 2020 2020 2020 2020 2066 2262 7574             f"but
+00001840: 2067 6f74 207b 6e75 6d5f 6578 7065 7274   got {num_expert
+00001850: 735f 6368 6f73 656e 7d2e 2229 0a20 2020  s_chosen}.").   
+00001860: 2020 2020 2069 6620 686f 745f 6578 7065       if hot_expe
+00001870: 7274 5f6e 756d 203e 2065 7870 6572 745f  rt_num > expert_
+00001880: 6e75 6d3a 0a20 2020 2020 2020 2020 2020  num:.           
+00001890: 2072 6169 7365 2056 616c 7565 4572 726f   raise ValueErro
+000018a0: 7228 6622 2768 6f74 5f65 7870 6572 745f  r(f"'hot_expert_
+000018b0: 6e75 6d27 206d 7573 7420 6e6f 7420 6265  num' must not be
+000018c0: 206c 6172 6765 7220 7468 616e 2027 6578   larger than 'ex
+000018d0: 7065 7274 5f6e 756d 272c 2022 0a20 2020  pert_num', ".   
+000018e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000018f0: 2020 2020 2020 2020 2020 6622 6275 7420            f"but 
+00001900: 676f 7420 7b68 6f74 5f65 7870 6572 745f  got {hot_expert_
+00001910: 6e75 6d7d 2e22 290a 2020 2020 2020 2020  num}.").        
+00001920: 6966 2063 6f6c 645f 746f 6b65 6e5f 7065  if cold_token_pe
+00001930: 7263 656e 7420 3e20 312e 3020 6f72 2063  rcent > 1.0 or c
+00001940: 6f6c 645f 746f 6b65 6e5f 7065 7263 656e  old_token_percen
+00001950: 7420 3c3d 2030 2e30 3a0a 2020 2020 2020  t <= 0.0:.      
+00001960: 2020 2020 2020 7261 6973 6520 5661 6c75        raise Valu
+00001970: 6545 7272 6f72 2866 2227 636f 6c64 5f74  eError(f"'cold_t
+00001980: 6f6b 656e 5f70 6572 6365 6e74 2720 6d75  oken_percent' mu
+00001990: 7374 2062 6520 696e 2074 6865 2072 616e  st be in the ran
+000019a0: 6765 2028 302e 302c 2031 2e30 5d2c 2022  ge (0.0, 1.0], "
+000019b0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+000019c0: 2020 2020 2020 2020 2020 2020 2020 6622                f"
+000019d0: 6275 7420 676f 7420 7b63 6f6c 645f 746f  but got {cold_to
+000019e0: 6b65 6e5f 7065 7263 656e 747d 2e22 290a  ken_percent}.").
+000019f0: 0a20 2020 2020 2020 2073 656c 662e 6578  .        self.ex
+00001a00: 7065 7274 5f6e 756d 203d 2065 7870 6572  pert_num = exper
+00001a10: 745f 6e75 6d0a 2020 2020 2020 2020 7365  t_num.        se
+00001a20: 6c66 2e63 6170 6163 6974 795f 6661 6374  lf.capacity_fact
+00001a30: 6f72 203d 2063 6170 6163 6974 795f 6661  or = capacity_fa
+00001a40: 6374 6f72 0a20 2020 2020 2020 2073 656c  ctor.        sel
+00001a50: 662e 6175 785f 6c6f 7373 5f66 6163 746f  f.aux_loss_facto
+00001a60: 7220 3d20 6175 785f 6c6f 7373 5f66 6163  r = aux_loss_fac
+00001a70: 746f 720a 2020 2020 2020 2020 7365 6c66  tor.        self
+00001a80: 2e6e 756d 5f65 7870 6572 7473 5f63 686f  .num_experts_cho
+00001a90: 7365 6e20 3d20 6e75 6d5f 6578 7065 7274  sen = num_expert
+00001aa0: 735f 6368 6f73 656e 0a20 2020 2020 2020  s_chosen.       
+00001ab0: 2073 656c 662e 6578 7065 7274 5f67 726f   self.expert_gro
+00001ac0: 7570 5f73 697a 6520 3d20 6578 7065 7274  up_size = expert
+00001ad0: 5f67 726f 7570 5f73 697a 650a 2020 2020  _group_size.    
+00001ae0: 2020 2020 7365 6c66 2e67 726f 7570 5f77      self.group_w
+00001af0: 6973 655f 6132 6120 3d20 6772 6f75 705f  ise_a2a = group_
+00001b00: 7769 7365 5f61 3261 0a20 2020 2020 2020  wise_a2a.       
+00001b10: 2073 656c 662e 636f 6d70 5f63 6f6d 6d5f   self.comp_comm_
+00001b20: 7061 7261 6c6c 656c 203d 2063 6f6d 705f  parallel = comp_
+00001b30: 636f 6d6d 5f70 6172 616c 6c65 6c0a 2020  comm_parallel.  
+00001b40: 2020 2020 2020 7365 6c66 2e63 6f6d 705f        self.comp_
+00001b50: 636f 6d6d 5f70 6172 616c 6c65 6c5f 6465  comm_parallel_de
+00001b60: 6772 6565 203d 2063 6f6d 705f 636f 6d6d  gree = comp_comm
+00001b70: 5f70 6172 616c 6c65 6c5f 6465 6772 6565  _parallel_degree
+00001b80: 0a20 2020 2020 2020 2073 656c 662e 7361  .        self.sa
+00001b90: 7665 5f74 6f6b 656e 5f64 6973 7472 6962  ve_token_distrib
+00001ba0: 7574 696f 6e20 3d20 7361 7665 5f74 6f6b  ution = save_tok
+00001bb0: 656e 5f64 6973 7472 6962 7574 696f 6e0a  en_distribution.
+00001bc0: 2020 2020 2020 2020 7365 6c66 2e63 7572          self.cur
+00001bd0: 5f6c 6179 6572 203d 2063 7572 5f6c 6179  _layer = cur_lay
+00001be0: 6572 0a20 2020 2020 2020 2073 656c 662e  er.        self.
+00001bf0: 656e 6162 6c65 5f63 6f6c 645f 686f 745f  enable_cold_hot_
+00001c00: 6578 7065 7274 203d 2065 6e61 626c 655f  expert = enable_
+00001c10: 636f 6c64 5f68 6f74 5f65 7870 6572 740a  cold_hot_expert.
+00001c20: 2020 2020 2020 2020 7365 6c66 2e75 7064          self.upd
+00001c30: 6174 655f 7374 6570 203d 2075 7064 6174  ate_step = updat
+00001c40: 655f 7374 6570 0a20 2020 2020 2020 2073  e_step.        s
+00001c50: 656c 662e 686f 745f 6578 7065 7274 5f6e  elf.hot_expert_n
+00001c60: 756d 203d 2068 6f74 5f65 7870 6572 745f  um = hot_expert_
+00001c70: 6e75 6d0a 2020 2020 2020 2020 7365 6c66  num.        self
+00001c80: 2e63 6f6c 645f 746f 6b65 6e5f 7065 7263  .cold_token_perc
+00001c90: 656e 7420 3d20 636f 6c64 5f74 6f6b 656e  ent = cold_token
+00001ca0: 5f70 6572 6365 6e74 0a20 2020 2020 2020  _percent.       
+00001cb0: 2073 656c 662e 6d6f 655f 6d6f 6475 6c65   self.moe_module
+00001cc0: 5f6e 616d 6520 3d20 6d6f 655f 6d6f 6475  _name = moe_modu
+00001cd0: 6c65 5f6e 616d 650a 2020 2020 2020 2020  le_name.        
+00001ce0: 7365 6c66 2e72 6f75 7469 6e67 5f70 6f6c  self.routing_pol
+00001cf0: 6963 7920 3d20 726f 7574 696e 675f 706f  icy = routing_po
+00001d00: 6c69 6379 0a0a 2020 2020 6465 6620 5f5f  licy..    def __
+00001d10: 6571 5f5f 2873 656c 662c 206f 7468 6572  eq__(self, other
+00001d20: 2920 2d3e 2062 6f6f 6c3a 0a20 2020 2020  ) -> bool:.     
+00001d30: 2020 2072 6574 7572 6e20 6973 696e 7374     return isinst
+00001d40: 616e 6365 286f 7468 6572 2c20 4d6f 4543  ance(other, MoEC
+00001d50: 6f6e 6669 6729 2061 6e64 2028 7365 6c66  onfig) and (self
+00001d60: 2e74 6f5f 6469 6374 2829 203d 3d20 6f74  .to_dict() == ot
+00001d70: 6865 722e 746f 5f64 6963 7428 2929 0a0a  her.to_dict())..
+00001d80: 2020 2020 6465 6620 746f 5f64 6966 665f      def to_diff_
+00001d90: 6469 6374 2873 656c 6629 3a0a 2020 2020  dict(self):.    
+00001da0: 2020 2020 636f 6e66 6967 5f64 6963 7420      config_dict 
+00001db0: 3d20 7365 6c66 2e74 6f5f 6469 6374 2829  = self.to_dict()
+00001dc0: 0a20 2020 2020 2020 2064 6566 6175 6c74  .        default
+00001dd0: 5f64 6963 7420 3d20 4d6f 4543 6f6e 6669  _dict = MoEConfi
+00001de0: 6728 292e 746f 5f64 6963 7428 290a 2020  g().to_dict().  
+00001df0: 2020 2020 2020 7265 735f 6469 6374 203d        res_dict =
+00001e00: 207b 7d0a 2020 2020 2020 2020 666f 7220   {}.        for 
+00001e10: 6b2c 2076 2069 6e20 636f 6e66 6967 5f64  k, v in config_d
+00001e20: 6963 742e 6974 656d 7328 293a 0a20 2020  ict.items():.   
+00001e30: 2020 2020 2020 2020 2069 6620 7620 213d           if v !=
+00001e40: 2064 6566 6175 6c74 5f64 6963 745b 6b5d   default_dict[k]
+00001e50: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
+00001e60: 2020 7265 735f 6469 6374 5b6b 5d20 3d20    res_dict[k] = 
+00001e70: 760a 2020 2020 2020 2020 7265 7475 726e  v.        return
+00001e80: 2072 6573 5f64 6963 740a 0a20 2020 2064   res_dict..    d
+00001e90: 6566 2074 6f5f 6469 6374 2873 656c 6629  ef to_dict(self)
+00001ea0: 3a0a 2020 2020 2020 2020 7265 7475 726e  :.        return
+00001eb0: 2063 6f70 792e 6465 6570 636f 7079 2873   copy.deepcopy(s
+00001ec0: 656c 662e 5f5f 6469 6374 5f5f 290a 0a0a  elf.__dict__)...
+00001ed0: 6465 6661 756c 745f 6d6f 655f 636f 6e66  default_moe_conf
+00001ee0: 6967 203d 204d 6f45 436f 6e66 6967 2829  ig = MoEConfig()
+00001ef0: 0a0a 0a64 6566 205f 6368 6563 6b5f 6d6f  ...def _check_mo
+00001f00: 655f 636f 6e66 6967 286d 6f65 5f63 6f6e  e_config(moe_con
+00001f10: 6669 673d 4e6f 6e65 2c20 7061 7261 6c6c  fig=None, parall
+00001f20: 656c 5f63 6f6e 6669 673d 4e6f 6e65 293a  el_config=None):
+00001f30: 0a20 2020 2022 2222 0a20 2020 2020 2020  .    """.       
+00001f40: 2063 6865 636b 2069 6620 4d6f 4520 7769   check if MoE wi
+00001f50: 7468 2072 6967 6874 2063 6f6e 6669 6775  th right configu
+00001f60: 7261 7469 6f6e 2e0a 2020 2020 2222 220a  ration..    """.
+00001f70: 2020 2020 6966 206e 6f74 2069 7369 6e73      if not isins
+00001f80: 7461 6e63 6528 6d6f 655f 636f 6e66 6967  tance(moe_config
+00001f90: 2c20 4d6f 4543 6f6e 6669 6729 3a0a 2020  , MoEConfig):.  
+00001fa0: 2020 2020 2020 7261 6973 6520 5479 7065        raise Type
+00001fb0: 4572 726f 7228 6622 276d 6f65 5f63 6f6e  Error(f"'moe_con
+00001fc0: 6669 6727 206d 7573 7420 6265 2061 6e20  fig' must be an 
+00001fd0: 696e 7374 616e 6365 206f 6620 4d6f 4543  instance of MoEC
+00001fe0: 6f6e 6669 672c 2062 7574 2067 6f74 207b  onfig, but got {
+00001ff0: 7479 7065 286d 6f65 5f63 6f6e 6669 6729  type(moe_config)
+00002000: 2e5f 5f6e 616d 655f 5f7d 2e22 290a 2020  .__name__}.").  
+00002010: 2020 7573 655f 6d6f 6520 3d20 286d 6f65    use_moe = (moe
+00002020: 5f63 6f6e 6669 672e 6578 7065 7274 5f6e  _config.expert_n
+00002030: 756d 203e 2031 290a 2020 2020 6966 2075  um > 1).    if u
+00002040: 7365 5f6d 6f65 2069 7320 4661 6c73 653a  se_moe is False:
+00002050: 0a20 2020 2020 2020 2072 6574 7572 6e0a  .        return.
+00002060: 2020 2020 6966 206d 6f65 5f63 6f6e 6669      if moe_confi
+00002070: 672e 6578 7065 7274 5f6e 756d 2025 2070  g.expert_num % p
+00002080: 6172 616c 6c65 6c5f 636f 6e66 6967 2e65  arallel_config.e
+00002090: 7870 6572 745f 7061 7261 6c6c 656c 2021  xpert_parallel !
+000020a0: 3d20 303a 0a20 2020 2020 2020 2072 6169  = 0:.        rai
+000020b0: 7365 2056 616c 7565 4572 726f 7228 6622  se ValueError(f"
+000020c0: 5768 656e 2075 7369 6e67 204d 6f45 2c20  When using MoE, 
+000020d0: 7468 6520 2765 7870 6572 745f 6e75 6d27  the 'expert_num'
+000020e0: 2069 6e20 7b74 7970 6528 6d6f 655f 636f   in {type(moe_co
+000020f0: 6e66 6967 292e 5f5f 6e61 6d65 5f5f 7d20  nfig).__name__} 
+00002100: 6d75 7374 2062 6520 6120 6d75 6c74 6970  must be a multip
+00002110: 6c65 2022 0a20 2020 2020 2020 2020 2020  le ".           
+00002120: 2020 2020 2020 2020 2020 2020 2020 6622                f"
+00002130: 6f66 2027 6578 7065 7274 5f70 6172 616c  of 'expert_paral
+00002140: 6c65 6c27 2076 616c 7565 2069 6e20 7b74  lel' value in {t
+00002150: 7970 6528 7061 7261 6c6c 656c 5f63 6f6e  ype(parallel_con
+00002160: 6669 6729 2e5f 5f6e 616d 655f 5f7d 2c20  fig).__name__}, 
+00002170: 6275 7420 676f 7420 220a 2020 2020 2020  but got ".      
+00002180: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002190: 2020 2066 227b 6d6f 655f 636f 6e66 6967     f"{moe_config
+000021a0: 2e65 7870 6572 745f 6e75 6d7d 2066 6f72  .expert_num} for
+000021b0: 2027 6578 7065 7274 5f6e 756d 2720 616e   'expert_num' an
+000021c0: 6420 7b70 6172 616c 6c65 6c5f 636f 6e66  d {parallel_conf
+000021d0: 6967 2e65 7870 6572 745f 7061 7261 6c6c  ig.expert_parall
+000021e0: 656c 7d20 666f 7220 220a 2020 2020 2020  el} for ".      
+000021f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002200: 2020 2066 2227 6578 7065 7274 5f70 6172     f"'expert_par
+00002210: 616c 6c65 6c27 2e22 290a 0a20 2020 2064  allel'.")..    d
+00002220: 6576 6963 655f 6e75 6d20 3d20 442e 6765  evice_num = D.ge
+00002230: 745f 6772 6f75 705f 7369 7a65 2829 0a20  t_group_size(). 
+00002240: 2020 2069 6620 6465 7669 6365 5f6e 756d     if device_num
+00002250: 2025 2070 6172 616c 6c65 6c5f 636f 6e66   % parallel_conf
+00002260: 6967 2e65 7870 6572 745f 7061 7261 6c6c  ig.expert_parall
+00002270: 656c 2021 3d20 303a 0a20 2020 2020 2020  el != 0:.       
+00002280: 2072 6169 7365 2056 616c 7565 4572 726f   raise ValueErro
+00002290: 7228 6622 6465 7669 6365 5f6e 756d 3a20  r(f"device_num: 
+000022a0: 7b64 6576 6963 655f 6e75 6d7d 206d 7573  {device_num} mus
+000022b0: 7420 6265 2061 206d 756c 7469 706c 6520  t be a multiple 
+000022c0: 6f66 2065 7870 6572 745f 7061 7261 6c6c  of expert_parall
+000022d0: 656c 3a20 220a 2020 2020 2020 2020 2020  el: ".          
+000022e0: 2020 2020 2020 2020 2020 2020 2020 2066                 f
+000022f0: 227b 7061 7261 6c6c 656c 5f63 6f6e 6669  "{parallel_confi
+00002300: 672e 6578 7065 7274 5f70 6172 616c 6c65  g.expert_paralle
+00002310: 6c7d 2e22 290a 2020 2020 6966 2070 6172  l}.").    if par
+00002320: 616c 6c65 6c5f 636f 6e66 6967 2e64 6174  allel_config.dat
+00002330: 615f 7061 7261 6c6c 656c 2025 2070 6172  a_parallel % par
+00002340: 616c 6c65 6c5f 636f 6e66 6967 2e65 7870  allel_config.exp
+00002350: 6572 745f 7061 7261 6c6c 656c 2021 3d20  ert_parallel != 
+00002360: 303a 0a20 2020 2020 2020 2072 6169 7365  0:.        raise
+00002370: 2056 616c 7565 4572 726f 7228 6622 6461   ValueError(f"da
+00002380: 7461 2070 6172 616c 6c65 6c3a 207b 7061  ta parallel: {pa
+00002390: 7261 6c6c 656c 5f63 6f6e 6669 672e 6461  rallel_config.da
+000023a0: 7461 5f70 6172 616c 6c65 6c7d 206d 7573  ta_parallel} mus
+000023b0: 7420 6265 2061 206d 756c 7469 706c 6520  t be a multiple 
+000023c0: 6f66 2022 0a20 2020 2020 2020 2020 2020  of ".           
+000023d0: 2020 2020 2020 2020 2020 2020 2020 6622                f"
+000023e0: 6578 7065 7274 5f70 6172 616c 6c65 6c3a  expert_parallel:
+000023f0: 207b 7061 7261 6c6c 656c 5f63 6f6e 6669   {parallel_confi
+00002400: 672e 6578 7065 7274 5f70 6172 616c 6c65  g.expert_paralle
+00002410: 6c7d 2077 6865 6e20 7573 696e 6720 4d6f  l} when using Mo
+00002420: 452e 2229 0a20 2020 2069 6620 7061 7261  E.").    if para
+00002430: 6c6c 656c 5f63 6f6e 6669 672e 6461 7461  llel_config.data
+00002440: 5f70 6172 616c 6c65 6c20 2a20 7061 7261  _parallel * para
+00002450: 6c6c 656c 5f63 6f6e 6669 672e 6d6f 6465  llel_config.mode
+00002460: 6c5f 7061 7261 6c6c 656c 203e 2064 6576  l_parallel > dev
+00002470: 6963 655f 6e75 6d3a 0a20 2020 2020 2020  ice_num:.       
+00002480: 2072 6169 7365 2056 616c 7565 4572 726f   raise ValueErro
+00002490: 7228 6622 5468 6520 7072 6f64 7563 7420  r(f"The product 
+000024a0: 6f66 2074 6865 2064 6174 6120 7061 7261  of the data para
+000024b0: 6c6c 656c 3a20 7b70 6172 616c 6c65 6c5f  llel: {parallel_
+000024c0: 636f 6e66 6967 2e64 6174 615f 7061 7261  config.data_para
+000024d0: 6c6c 656c 7d20 616e 6420 220a 2020 2020  llel} and ".    
+000024e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000024f0: 2020 2020 2066 226d 6f64 656c 2070 6172       f"model par
+00002500: 616c 6c65 6c3a 207b 7061 7261 6c6c 656c  allel: {parallel
+00002510: 5f63 6f6e 6669 672e 6d6f 6465 6c5f 7061  _config.model_pa
+00002520: 7261 6c6c 656c 7d20 220a 2020 2020 2020  rallel} ".      
+00002530: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002540: 2020 2066 2273 686f 756c 6420 6265 206c     f"should be l
+00002550: 6573 7320 7468 616e 2064 6576 6963 655f  ess than device_
+00002560: 6e75 6d3a 207b 6465 7669 6365 5f6e 756d  num: {device_num
+00002570: 7d2e 2229 0a0a 0a40 636f 6e73 7465 7870  }.")...@constexp
+00002580: 720a 6465 6620 6361 6c63 756c 6174 655f  r.def calculate_
+00002590: 6578 7065 7274 5f63 6170 6163 6974 7928  expert_capacity(
+000025a0: 6b2c 2074 6f6b 656e 735f 7065 725f 6772  k, tokens_per_gr
+000025b0: 6f75 702c 2063 6170 6163 6974 795f 6661  oup, capacity_fa
+000025c0: 6374 6f72 2c20 6578 7065 7274 5f64 696d  ctor, expert_dim
+000025d0: 293a 0a20 2020 2072 6574 7572 6e20 6d61  ):.    return ma
+000025e0: 7468 2e63 6569 6c28 6b20 2a20 746f 6b65  th.ceil(k * toke
+000025f0: 6e73 5f70 6572 5f67 726f 7570 202a 2063  ns_per_group * c
+00002600: 6170 6163 6974 795f 6661 6374 6f72 202f  apacity_factor /
+00002610: 2065 7870 6572 745f 6469 6d29 0a0a 0a63   expert_dim)...c
+00002620: 6c61 7373 204d 6f45 2843 656c 6c29 3a0a  lass MoE(Cell):.
+00002630: 2020 2020 2222 220a 2020 2020 5468 6520      """.    The 
+00002640: 6d69 7874 7572 6520 6f66 2065 7870 6572  mixture of exper
+00002650: 7473 2028 4d6f 4529 2069 6d70 6c65 6d65  ts (MoE) impleme
+00002660: 6e74 6174 696f 6e2e 2054 6865 2069 6d70  ntation. The imp
+00002670: 6c65 6d65 6e74 6174 696f 6e20 696e 636c  lementation incl
+00002680: 7564 6573 2061 2072 6f75 7465 7220 616e  udes a router an
+00002690: 6420 6120 4665 6564 466f 7277 6172 6420  d a FeedForward 
+000026a0: 6c61 7965 722e 0a20 2020 2054 6865 2072  layer..    The r
+000026b0: 6f75 7465 7220 6469 7370 6174 6368 6573  outer dispatches
+000026c0: 2074 6f6b 656e 7320 746f 2065 7870 6572   tokens to exper
+000026d0: 7473 2069 6e20 4665 6564 466f 7277 6172  ts in FeedForwar
+000026e0: 642c 2074 6865 6e20 4665 6564 466f 7277  d, then FeedForw
+000026f0: 6172 6420 646f 6573 2063 6f6d 7075 7461  ard does computa
+00002700: 7469 6f6e 2c20 616e 6420 7468 6520 6669  tion, and the fi
+00002710: 6e61 6c20 6f75 7470 7574 2069 730a 2020  nal output is.  
+00002720: 2020 6f62 7461 696e 6564 2062 7920 6d75    obtained by mu
+00002730: 6c74 6970 6c79 696e 6720 4665 6564 466f  ltiplying FeedFo
+00002740: 7277 6172 6427 7320 6f75 7470 7574 2061  rward's output a
+00002750: 6e64 2072 6f75 7465 7227 7320 636f 6d62  nd router's comb
+00002760: 696e 6520 7765 6967 6874 2e0a 0a20 2020  ine weight...   
+00002770: 2041 7267 733a 0a20 2020 2020 2020 2068   Args:.        h
+00002780: 6964 6465 6e5f 7369 7a65 2028 696e 7429  idden_size (int)
+00002790: 3a20 5468 6520 6469 6d65 6e73 696f 6e20  : The dimension 
+000027a0: 6f66 2074 6865 2069 6e70 7574 732e 0a20  of the inputs.. 
+000027b0: 2020 2020 2020 2066 666e 5f68 6964 6465         ffn_hidde
+000027c0: 6e5f 7369 7a65 2028 696e 7429 3a20 5468  n_size (int): Th
+000027d0: 6520 696e 7465 726d 6564 6961 7465 2068  e intermediate h
+000027e0: 6964 6465 6e20 7369 7a65 2e0a 2020 2020  idden size..    
+000027f0: 2020 2020 6472 6f70 6f75 745f 7261 7465      dropout_rate
+00002800: 2028 666c 6f61 7429 3a20 5468 6520 6472   (float): The dr
+00002810: 6f70 6f75 7420 7261 7465 2066 6f72 2074  opout rate for t
+00002820: 6865 2073 6563 6f6e 6420 6c69 6e65 6172  he second linear
+00002830: 2773 206f 7574 7075 742e 0a20 2020 2020  's output..     
+00002840: 2020 2068 6964 6465 6e5f 6163 7420 2873     hidden_act (s
+00002850: 7472 293a 2054 6865 2061 6374 6976 6174  tr): The activat
+00002860: 696f 6e20 6f66 2074 6865 2069 6e74 6572  ion of the inter
+00002870: 6e61 6c20 6665 6564 666f 7277 6172 6420  nal feedforward 
+00002880: 6c61 7965 722e 2053 7570 706f 7274 7320  layer. Supports 
+00002890: 2772 656c 7527 2c0a 2020 2020 2020 2020  'relu',.        
+000028a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000028b0: 2027 7265 6c75 3627 2c20 2774 616e 6827   'relu6', 'tanh'
+000028c0: 2c20 2767 656c 7527 2c20 2766 6173 745f  , 'gelu', 'fast_
+000028d0: 6765 6c75 272c 2027 656c 7527 2c20 2773  gelu', 'elu', 's
+000028e0: 6967 6d6f 6964 272c 2027 7072 656c 7527  igmoid', 'prelu'
+000028f0: 2c20 276c 6561 6b79 7265 6c75 272c 2027  , 'leakyrelu', '
+00002900: 6873 7769 7368 272c 0a20 2020 2020 2020  hswish',.       
+00002910: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002920: 2020 2768 7369 676d 6f69 6427 2c20 276c    'hsigmoid', 'l
+00002930: 6f67 7369 676d 6f69 6427 2061 6e64 2073  ogsigmoid' and s
+00002940: 6f20 6f6e 2e20 4465 6661 756c 743a 2067  o on. Default: g
+00002950: 656c 752e 0a20 2020 2020 2020 2070 6172  elu..        par
+00002960: 616d 5f69 6e69 745f 7479 7065 2028 6474  am_init_type (dt
+00002970: 7970 652e 4e75 6d62 6572 293a 2054 6865  ype.Number): The
+00002980: 2070 6172 616d 6574 6572 2069 6e69 7469   parameter initi
+00002990: 616c 697a 6174 696f 6e20 7479 7065 2e20  alization type. 
+000029a0: 4361 6e20 6265 2064 7479 7065 2e66 6c6f  Can be dtype.flo
+000029b0: 6174 3332 206f 7220 6474 7970 652e 666c  at32 or dtype.fl
+000029c0: 6f61 7431 362e 0a20 2020 2020 2020 206d  oat16..        m
+000029d0: 6f65 5f63 6f6e 6669 6728 4d6f 4543 6f6e  oe_config(MoECon
+000029e0: 6669 6729 3a20 5468 6520 636f 6e66 6967  fig): The config
+000029f0: 7572 6174 696f 6e20 6f66 204d 6f45 2028  uration of MoE (
+00002a00: 4d69 7874 7572 6520 6f66 2045 7870 6572  Mixture of Exper
+00002a10: 7429 2e20 4465 6661 756c 7420 6973 2061  t). Default is a
+00002a20: 6e20 696e 7374 616e 6365 206f 6620 4d6f  n instance of Mo
+00002a30: 4543 6f6e 6669 6720 7769 7468 0a20 2020  EConfig with.   
+00002a40: 2020 2020 2020 2020 2064 6566 6175 6c74           default
+00002a50: 2076 616c 7565 732e 2050 6c65 6173 6520   values. Please 
+00002a60: 7365 6520 604d 6f45 436f 6e66 6967 602e  see `MoEConfig`.
+00002a70: 0a20 2020 2020 2020 2070 6172 616c 6c65  .        paralle
+00002a80: 6c5f 636f 6e66 6967 284d 6f45 5061 7261  l_config(MoEPara
+00002a90: 6c6c 656c 436f 6e66 6967 293a 2054 6865  llelConfig): The
+00002aa0: 2070 6172 616c 6c65 6c20 636f 6e66 6967   parallel config
+00002ab0: 2066 6f72 204d 6f45 2c20 7365 6520 604d   for MoE, see `M
+00002ac0: 6f45 5061 7261 6c6c 656c 436f 6e66 6967  oEParallelConfig
+00002ad0: 602e 0a20 2020 2020 2020 2020 2020 2044  `..            D
+00002ae0: 6566 6175 6c74 2060 6465 6661 756c 745f  efault `default_
+00002af0: 6d6f 6570 6172 616c 6c65 6c5f 636f 6e66  moeparallel_conf
+00002b00: 6967 602c 2061 6e20 696e 7374 616e 6365  ig`, an instance
+00002b10: 206f 6620 604d 6f45 5061 7261 6c6c 656c   of `MoEParallel
+00002b20: 436f 6e66 6967 6020 7769 7468 2064 6566  Config` with def
+00002b30: 6175 6c74 2061 7267 732e 0a0a 2020 2020  ault args...    
+00002b40: 496e 7075 7473 3a0a 2020 2020 2020 2020  Inputs:.        
+00002b50: 2d20 2a2a 782a 2a20 2854 656e 736f 7229  - **x** (Tensor)
+00002b60: 202d 2073 686f 756c 6420 6265 2060 5b62   - should be `[b
+00002b70: 6174 6368 2c20 7365 715f 6c65 6e67 7468  atch, seq_length
+00002b80: 2c20 6869 6464 656e 5f73 697a 655d 602e  , hidden_size]`.
+00002b90: 2046 6c6f 6174 2074 656e 736f 722e 0a0a   Float tensor...
+00002ba0: 2020 2020 4f75 7470 7574 733a 0a20 2020      Outputs:.   
+00002bb0: 2020 2020 2054 656e 736f 722c 2074 6865       Tensor, the
+00002bc0: 206f 7574 7075 7420 6f66 2074 6869 7320   output of this 
+00002bd0: 6c61 7965 7220 6166 7465 7220 6d61 7070  layer after mapp
+00002be0: 696e 672e 2054 6865 2073 6861 7065 2069  ing. The shape i
+00002bf0: 7320 605b 6261 7463 682c 2073 6571 5f6c  s `[batch, seq_l
+00002c00: 656e 6774 682c 2068 6964 6465 6e5f 7369  ength, hidden_si
+00002c10: 7a65 5d60 2e0a 2020 2020 2222 220a 0a20  ze]`..    """.. 
+00002c20: 2020 2064 6566 205f 5f69 6e69 745f 5f28     def __init__(
+00002c30: 7365 6c66 2c20 6869 6464 656e 5f73 697a  self, hidden_siz
+00002c40: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
+00002c50: 2020 2020 6666 6e5f 6869 6464 656e 5f73      ffn_hidden_s
+00002c60: 697a 652c 0a20 2020 2020 2020 2020 2020  ize,.           
+00002c70: 2020 2020 2020 6472 6f70 6f75 745f 7261        dropout_ra
+00002c80: 7465 2c0a 2020 2020 2020 2020 2020 2020  te,.            
+00002c90: 2020 2020 2068 6964 6465 6e5f 6163 743d       hidden_act=
+00002ca0: 2767 656c 7527 2c0a 2020 2020 2020 2020  'gelu',.        
+00002cb0: 2020 2020 2020 2020 2070 6172 616d 5f69           param_i
+00002cc0: 6e69 745f 7479 7065 3d6d 7374 7970 652e  nit_type=mstype.
+00002cd0: 666c 6f61 7433 322c 0a20 2020 2020 2020  float32,.       
+00002ce0: 2020 2020 2020 2020 2020 6d6f 655f 636f            moe_co
+00002cf0: 6e66 6967 3d64 6566 6175 6c74 5f6d 6f65  nfig=default_moe
+00002d00: 5f63 6f6e 6669 672c 0a20 2020 2020 2020  _config,.       
+00002d10: 2020 2020 2020 2020 2020 7061 7261 6c6c            parall
+00002d20: 656c 5f63 6f6e 6669 673d 6465 6661 756c  el_config=defaul
+00002d30: 745f 6d6f 6570 6172 616c 6c65 6c5f 636f  t_moeparallel_co
+00002d40: 6e66 6967 293a 0a20 2020 2020 2020 2073  nfig):.        s
+00002d50: 7570 6572 284d 6f45 2c20 7365 6c66 292e  uper(MoE, self).
+00002d60: 5f5f 696e 6974 5f5f 2829 0a20 2020 2020  __init__().     
+00002d70: 2020 2069 6620 5f67 6574 5f70 6172 616c     if _get_paral
+00002d80: 6c65 6c5f 6d6f 6465 2829 2069 6e20 2850  lel_mode() in (P
+00002d90: 6172 616c 6c65 6c4d 6f64 652e 4155 544f  arallelMode.AUTO
+00002da0: 5f50 4152 414c 4c45 4c2c 2920 616e 6420  _PARALLEL,) and 
+00002db0: 5f69 735f 7368 6172 6469 6e67 5f70 726f  _is_sharding_pro
+00002dc0: 7061 6761 7469 6f6e 2829 3a0a 2020 2020  pagation():.    
+00002dd0: 2020 2020 2020 2020 7365 6c66 2e68 6964          self.hid
+00002de0: 6465 6e5f 7369 7a65 203d 2068 6964 6465  den_size = hidde
+00002df0: 6e5f 7369 7a65 0a20 2020 2020 2020 2020  n_size.         
+00002e00: 2020 2073 656c 662e 6578 7065 7274 5f64     self.expert_d
+00002e10: 696d 203d 206d 6f65 5f63 6f6e 6669 672e  im = moe_config.
+00002e20: 6578 7065 7274 5f6e 756d 0a20 2020 2020  expert_num.     
+00002e30: 2020 2020 2020 2073 656c 662e 6361 7061         self.capa
+00002e40: 6369 7479 5f66 6163 746f 7220 3d20 6d6f  city_factor = mo
+00002e50: 655f 636f 6e66 6967 2e63 6170 6163 6974  e_config.capacit
+00002e60: 795f 6661 6374 6f72 0a20 2020 2020 2020  y_factor.       
+00002e70: 2020 2020 2073 656c 662e 6175 785f 6c6f       self.aux_lo
+00002e80: 7373 5f66 6163 746f 7220 3d20 6d6f 655f  ss_factor = moe_
+00002e90: 636f 6e66 6967 2e61 7578 5f6c 6f73 735f  config.aux_loss_
+00002ea0: 6661 6374 6f72 0a20 2020 2020 2020 2020  factor.         
+00002eb0: 2020 2073 656c 662e 6e75 6d5f 6578 7065     self.num_expe
+00002ec0: 7274 735f 6368 6f73 656e 203d 206d 6f65  rts_chosen = moe
+00002ed0: 5f63 6f6e 6669 672e 6e75 6d5f 6578 7065  _config.num_expe
+00002ee0: 7274 735f 6368 6f73 656e 0a20 2020 2020  rts_chosen.     
+00002ef0: 2020 2020 2020 2073 656c 662e 6578 7065         self.expe
+00002f00: 7274 5f67 726f 7570 5f73 697a 6520 3d20  rt_group_size = 
+00002f10: 6d6f 655f 636f 6e66 6967 2e65 7870 6572  moe_config.exper
+00002f20: 745f 6772 6f75 705f 7369 7a65 0a20 2020  t_group_size.   
+00002f30: 2020 2020 2020 2020 2073 656c 662e 6470           self.dp
+00002f40: 5f67 726f 7570 203d 2070 6172 616c 6c65  _group = paralle
+00002f50: 6c5f 636f 6e66 6967 2e64 6174 615f 7061  l_config.data_pa
+00002f60: 7261 6c6c 656c 0a20 2020 2020 2020 2020  rallel.         
+00002f70: 2020 2073 656c 662e 6470 203d 2070 6172     self.dp = par
+00002f80: 616c 6c65 6c5f 636f 6e66 6967 2e64 6174  allel_config.dat
+00002f90: 615f 7061 7261 6c6c 656c 0a20 2020 2020  a_parallel.     
+00002fa0: 2020 2020 2020 2073 656c 662e 6570 203d         self.ep =
+00002fb0: 2070 6172 616c 6c65 6c5f 636f 6e66 6967   parallel_config
+00002fc0: 2e65 7870 6572 745f 7061 7261 6c6c 656c  .expert_parallel
+00002fd0: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
+00002fe0: 662e 6d70 203d 2070 6172 616c 6c65 6c5f  f.mp = parallel_
+00002ff0: 636f 6e66 6967 2e6d 6f64 656c 5f70 6172  config.model_par
+00003000: 616c 6c65 6c0a 2020 2020 2020 2020 2020  allel.          
+00003010: 2020 7365 6c66 2e63 6f6d 705f 636f 6d6d    self.comp_comm
+00003020: 5f70 6172 616c 6c65 6c20 3d20 6d6f 655f  _parallel = moe_
+00003030: 636f 6e66 6967 2e63 6f6d 705f 636f 6d6d  config.comp_comm
+00003040: 5f70 6172 616c 6c65 6c0a 2020 2020 2020  _parallel.      
+00003050: 2020 2020 2020 7365 6c66 2e63 6f6d 705f        self.comp_
+00003060: 636f 6d6d 5f70 6172 616c 6c65 6c5f 6465  comm_parallel_de
+00003070: 6772 6565 203d 206d 6f65 5f63 6f6e 6669  gree = moe_confi
+00003080: 672e 636f 6d70 5f63 6f6d 6d5f 7061 7261  g.comp_comm_para
+00003090: 6c6c 656c 5f64 6567 7265 650a 2020 2020  llel_degree.    
+000030a0: 2020 2020 2020 2020 7365 6c66 2e67 726f          self.gro
+000030b0: 7570 5f77 6973 655f 6132 6120 3d20 6d6f  up_wise_a2a = mo
+000030c0: 655f 636f 6e66 6967 2e67 726f 7570 5f77  e_config.group_w
+000030d0: 6973 655f 6132 610a 2020 2020 2020 2020  ise_a2a.        
+000030e0: 2020 2020 6966 206e 6f74 2028 7365 6c66      if not (self
+000030f0: 2e6d 7020 3e20 3120 616e 6420 7365 6c66  .mp > 1 and self
+00003100: 2e64 7020 3d3d 2073 656c 662e 6570 293a  .dp == self.ep):
+00003110: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00003120: 2073 656c 662e 6772 6f75 705f 7769 7365   self.group_wise
+00003130: 5f61 3261 203d 2046 616c 7365 0a20 2020  _a2a = False.   
+00003140: 2020 2020 2020 2020 2066 726f 6d20 6d69           from mi
+00003150: 6e64 666f 726d 6572 732e 6d6f 6475 6c65  ndformers.module
+00003160: 732e 7472 616e 7366 6f72 6d65 7220 696d  s.transformer im
+00003170: 706f 7274 2046 6565 6446 6f72 7761 7264  port FeedForward
+00003180: 0a0a 2020 2020 2020 2020 2020 2020 7365  ..            se
+00003190: 6c66 2e66 666e 203d 2046 6565 6446 6f72  lf.ffn = FeedFor
+000031a0: 7761 7264 2868 6964 6465 6e5f 7369 7a65  ward(hidden_size
+000031b0: 3d68 6964 6465 6e5f 7369 7a65 2c0a 2020  =hidden_size,.  
+000031c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000031d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000031e0: 2066 666e 5f68 6964 6465 6e5f 7369 7a65   ffn_hidden_size
+000031f0: 3d66 666e 5f68 6964 6465 6e5f 7369 7a65  =ffn_hidden_size
+00003200: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00003210: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00003220: 2020 2020 2064 726f 706f 7574 5f72 6174       dropout_rat
+00003230: 653d 6472 6f70 6f75 745f 7261 7465 2c0a  e=dropout_rate,.
+00003240: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00003250: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00003260: 2020 2068 6964 6465 6e5f 6163 743d 6869     hidden_act=hi
+00003270: 6464 656e 5f61 6374 2c0a 2020 2020 2020  dden_act,.      
+00003280: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00003290: 2020 2020 2020 2020 2020 2020 2065 7870               exp
+000032a0: 6572 745f 6e75 6d3d 7365 6c66 2e65 7870  ert_num=self.exp
+000032b0: 6572 745f 6469 6d2c 0a20 2020 2020 2020  ert_dim,.       
+000032c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000032d0: 2020 2020 2020 2020 2020 2020 6578 7065              expe
+000032e0: 7274 5f67 726f 7570 5f73 697a 653d 7365  rt_group_size=se
+000032f0: 6c66 2e65 7870 6572 745f 6772 6f75 705f  lf.expert_group_
+00003300: 7369 7a65 2c0a 2020 2020 2020 2020 2020  size,.          
+00003310: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00003320: 2020 2020 2020 2020 2070 6172 616d 5f69           param_i
+00003330: 6e69 745f 7479 7065 3d70 6172 616d 5f69  nit_type=param_i
+00003340: 6e69 745f 7479 7065 2c0a 2020 2020 2020  nit_type,.      
+00003350: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00003360: 2020 2020 2020 2020 2020 2020 2070 6172               par
+00003370: 616c 6c65 6c5f 636f 6e66 6967 3d70 6172  allel_config=par
+00003380: 616c 6c65 6c5f 636f 6e66 6967 290a 2020  allel_config).  
+00003390: 2020 2020 2020 2020 2020 7365 6c66 2e72            self.r
+000033a0: 6573 6861 7065 203d 2050 2e52 6573 6861  eshape = P.Resha
+000033b0: 7065 2829 0a20 2020 2020 2020 2020 2020  pe().           
+000033c0: 2073 656c 662e 7368 6170 6520 3d20 502e   self.shape = P.
+000033d0: 5368 6170 6528 290a 2020 2020 2020 2020  Shape().        
+000033e0: 2020 2020 7365 6c66 2e74 7261 6e73 706f      self.transpo
+000033f0: 7365 5f32 6469 6d20 3d20 502e 5472 616e  se_2dim = P.Tran
+00003400: 7370 6f73 6528 292e 7368 6172 6428 2828  spose().shard(((
+00003410: 7365 6c66 2e64 702c 2031 292c 2929 0a20  self.dp, 1),)). 
+00003420: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+00003430: 7472 616e 7370 6f73 655f 3364 696d 203d  transpose_3dim =
+00003440: 2050 2e54 7261 6e73 706f 7365 2829 2e73   P.Transpose().s
+00003450: 6861 7264 2828 2873 656c 662e 6470 2c20  hard(((self.dp, 
+00003460: 312c 2031 292c 2929 0a20 2020 2020 2020  1, 1),)).       
+00003470: 2020 2020 2073 656c 662e 7472 616e 7370       self.transp
+00003480: 6f73 655f 3464 696d 203d 2050 2e54 7261  ose_4dim = P.Tra
+00003490: 6e73 706f 7365 2829 2e73 6861 7264 2828  nspose().shard((
+000034a0: 2831 2c20 7365 6c66 2e64 702c 2031 2c20  (1, self.dp, 1, 
+000034b0: 3129 2c29 290a 2020 2020 2020 2020 2020  1),)).          
+000034c0: 2020 7365 6c66 2e74 7261 6e73 706f 7365    self.transpose
+000034d0: 5f34 6469 6d5f 6470 203d 2050 2e54 7261  _4dim_dp = P.Tra
+000034e0: 6e73 706f 7365 2829 2e73 6861 7264 2828  nspose().shard((
+000034f0: 2831 2c20 312c 2073 656c 662e 6470 2c20  (1, 1, self.dp, 
+00003500: 3129 2c29 290a 2020 2020 2020 2020 2020  1),)).          
+00003510: 2020 7365 6c66 2e62 6174 6368 5f6d 6d20    self.batch_mm 
+00003520: 3d20 502e 4261 7463 684d 6174 4d75 6c28  = P.BatchMatMul(
+00003530: 292e 7368 6172 6428 2828 7365 6c66 2e64  ).shard(((self.d
+00003540: 702c 2031 2c20 3129 2c20 2873 656c 662e  p, 1, 1), (self.
+00003550: 6470 2c20 312c 2031 2929 290a 2020 2020  dp, 1, 1))).    
+00003560: 2020 2020 2020 2020 7365 6c66 2e62 6174          self.bat
+00003570: 6368 5f6d 6d32 203d 2050 2e42 6174 6368  ch_mm2 = P.Batch
+00003580: 4d61 744d 756c 2829 2e73 6861 7264 2828  MatMul().shard((
+00003590: 2873 656c 662e 6470 2c20 312c 2031 292c  (self.dp, 1, 1),
+000035a0: 2028 7365 6c66 2e64 702c 2031 2c20 3129   (self.dp, 1, 1)
+000035b0: 2929 0a20 2020 2020 2020 2020 2020 2073  )).            s
+000035c0: 656c 662e 6d75 6c20 3d20 502e 4d75 6c28  elf.mul = P.Mul(
+000035d0: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
+000035e0: 6c66 2e72 6f75 7465 7220 3d20 526f 7574  lf.router = Rout
+000035f0: 6572 2864 5f6d 6f64 656c 3d68 6964 6465  er(d_model=hidde
+00003600: 6e5f 7369 7a65 2c20 6d6f 655f 636f 6e66  n_size, moe_conf
+00003610: 6967 3d6d 6f65 5f63 6f6e 6669 672c 2072  ig=moe_config, r
+00003620: 6f75 7469 6e67 5f70 6f6c 6963 793d 4e6f  outing_policy=No
+00003630: 6e65 2c0a 2020 2020 2020 2020 2020 2020  ne,.            
+00003640: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00003650: 2020 2020 2074 7261 696e 696e 673d 5472       training=Tr
+00003660: 7565 2c20 7061 7261 6c6c 656c 5f63 6f6e  ue, parallel_con
+00003670: 6669 673d 7061 7261 6c6c 656c 5f63 6f6e  fig=parallel_con
+00003680: 6669 6729 0a20 2020 2020 2020 2020 2020  fig).           
+00003690: 2073 656c 662e 6361 7374 203d 2050 2e43   self.cast = P.C
+000036a0: 6173 7428 290a 2020 2020 2020 2020 2020  ast().          
+000036b0: 2020 7365 6c66 2e63 6f6e 6361 7420 3d20    self.concat = 
+000036c0: 502e 436f 6e63 6174 2833 292e 7368 6172  P.Concat(3).shar
+000036d0: 6428 7475 706c 6528 2873 656c 662e 6470  d(tuple((self.dp
+000036e0: 2c20 312c 2031 2c20 3129 2066 6f72 205f  , 1, 1, 1) for _
+000036f0: 2069 6e20 7261 6e67 6528 7365 6c66 2e63   in range(self.c
+00003700: 6f6d 705f 636f 6d6d 5f70 6172 616c 6c65  omp_comm_paralle
+00003710: 6c5f 6465 6772 6565 2929 290a 2020 2020  l_degree))).    
+00003720: 2020 2020 2020 2020 7365 6c66 2e63 6f6e          self.con
+00003730: 6361 745f 6470 203d 2050 2e43 6f6e 6361  cat_dp = P.Conca
+00003740: 7428 3229 2e73 6861 7264 2828 2831 2c20  t(2).shard(((1, 
+00003750: 7365 6c66 2e64 702c 2031 2c20 3129 2c20  self.dp, 1, 1), 
+00003760: 2831 2c20 7365 6c66 2e64 702c 2031 2c20  (1, self.dp, 1, 
+00003770: 3129 2929 0a20 2020 2020 2020 2020 2020  1))).           
+00003780: 2073 656c 662e 7370 6c69 7420 3d20 502e   self.split = P.
+00003790: 5370 6c69 7428 6178 6973 3d32 2c20 6f75  Split(axis=2, ou
+000037a0: 7470 7574 5f6e 756d 3d73 656c 662e 636f  tput_num=self.co
+000037b0: 6d70 5f63 6f6d 6d5f 7061 7261 6c6c 656c  mp_comm_parallel
+000037c0: 5f64 6567 7265 6529 2e73 6861 7264 2828  _degree).shard((
+000037d0: 2831 2c20 7365 6c66 2e64 702c 2031 2c20  (1, self.dp, 1, 
+000037e0: 3129 2c29 290a 2020 2020 2020 2020 2020  1),)).          
+000037f0: 2020 7365 6c66 2e73 7472 6964 655f 736c    self.stride_sl
+00003800: 6963 6520 3d20 502e 5374 7269 6465 6453  ice = P.StridedS
+00003810: 6c69 6365 2829 2e73 6861 7264 2828 2873  lice().shard(((s
+00003820: 656c 662e 6470 2c20 312c 2031 2c20 3129  elf.dp, 1, 1, 1)
+00003830: 2c29 290a 2020 2020 2020 2020 2020 2020  ,)).            
+00003840: 7365 6c66 2e73 7472 6964 655f 736c 6963  self.stride_slic
+00003850: 655f 6470 203d 2050 2e53 7472 6964 6564  e_dp = P.Strided
+00003860: 536c 6963 6528 292e 7368 6172 6428 2828  Slice().shard(((
+00003870: 312c 2073 656c 662e 6470 2c20 312c 2031  1, self.dp, 1, 1
+00003880: 292c 2929 0a20 2020 2020 2020 2020 2020  ),)).           
+00003890: 2073 656c 662e 7374 7269 6465 5f73 6c69   self.stride_sli
+000038a0: 6365 5f65 7020 3d20 502e 5374 7269 6465  ce_ep = P.Stride
+000038b0: 6453 6c69 6365 2829 2e73 6861 7264 2828  dSlice().shard((
+000038c0: 2873 656c 662e 6570 2c20 312c 2031 2c20  (self.ep, 1, 1, 
+000038d0: 3129 2c29 290a 2020 2020 2020 2020 2020  1),)).          
+000038e0: 2020 7365 6c66 2e73 7472 6964 655f 736c    self.stride_sl
+000038f0: 6963 655f 6470 5f6d 7020 3d20 502e 5374  ice_dp_mp = P.St
+00003900: 7269 6465 6453 6c69 6365 2829 2e73 6861  ridedSlice().sha
+00003910: 7264 2828 2831 2c20 7365 6c66 2e64 702c  rd(((1, self.dp,
+00003920: 2073 656c 662e 6d70 2c20 3129 2c29 290a   self.mp, 1),)).
+00003930: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+00003940: 2e73 7472 6964 655f 736c 6963 655f 6570  .stride_slice_ep
+00003950: 5f6d 7020 3d20 502e 5374 7269 6465 6453  _mp = P.StridedS
+00003960: 6c69 6365 2829 2e73 6861 7264 2828 2873  lice().shard(((s
+00003970: 656c 662e 6570 2c20 312c 2073 656c 662e  elf.ep, 1, self.
+00003980: 6d70 2c20 3129 2c29 290a 2020 2020 2020  mp, 1),)).      
+00003990: 2020 656c 7365 3a0a 2020 2020 2020 2020    else:.        
+000039a0: 2020 2020 7365 6c66 2e68 6964 6465 6e5f      self.hidden_
+000039b0: 7369 7a65 203d 2068 6964 6465 6e5f 7369  size = hidden_si
+000039c0: 7a65 0a20 2020 2020 2020 2020 2020 2073  ze.            s
+000039d0: 656c 662e 6578 7065 7274 5f64 696d 203d  elf.expert_dim =
+000039e0: 206d 6f65 5f63 6f6e 6669 672e 6578 7065   moe_config.expe
+000039f0: 7274 5f6e 756d 0a20 2020 2020 2020 2020  rt_num.         
+00003a00: 2020 2073 656c 662e 6361 7061 6369 7479     self.capacity
+00003a10: 5f66 6163 746f 7220 3d20 6d6f 655f 636f  _factor = moe_co
+00003a20: 6e66 6967 2e63 6170 6163 6974 795f 6661  nfig.capacity_fa
+00003a30: 6374 6f72 0a20 2020 2020 2020 2020 2020  ctor.           
+00003a40: 2073 656c 662e 6175 785f 6c6f 7373 5f66   self.aux_loss_f
+00003a50: 6163 746f 7220 3d20 6d6f 655f 636f 6e66  actor = moe_conf
+00003a60: 6967 2e61 7578 5f6c 6f73 735f 6661 6374  ig.aux_loss_fact
+00003a70: 6f72 0a20 2020 2020 2020 2020 2020 2073  or.            s
+00003a80: 656c 662e 6e75 6d5f 6578 7065 7274 735f  elf.num_experts_
+00003a90: 6368 6f73 656e 203d 206d 6f65 5f63 6f6e  chosen = moe_con
+00003aa0: 6669 672e 6e75 6d5f 6578 7065 7274 735f  fig.num_experts_
+00003ab0: 6368 6f73 656e 0a20 2020 2020 2020 2020  chosen.         
+00003ac0: 2020 2073 656c 662e 6470 5f67 726f 7570     self.dp_group
+00003ad0: 203d 2070 6172 616c 6c65 6c5f 636f 6e66   = parallel_conf
+00003ae0: 6967 2e64 6174 615f 7061 7261 6c6c 656c  ig.data_parallel
+00003af0: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
+00003b00: 662e 6470 203d 2070 6172 616c 6c65 6c5f  f.dp = parallel_
+00003b10: 636f 6e66 6967 2e64 6174 615f 7061 7261  config.data_para
+00003b20: 6c6c 656c 0a20 2020 2020 2020 2020 2020  llel.           
+00003b30: 2073 656c 662e 6570 203d 2070 6172 616c   self.ep = paral
+00003b40: 6c65 6c5f 636f 6e66 6967 2e65 7870 6572  lel_config.exper
+00003b50: 745f 7061 7261 6c6c 656c 0a20 2020 2020  t_parallel.     
+00003b60: 2020 2020 2020 2073 656c 662e 6d70 203d         self.mp =
+00003b70: 2070 6172 616c 6c65 6c5f 636f 6e66 6967   parallel_config
+00003b80: 2e6d 6f64 656c 5f70 6172 616c 6c65 6c0a  .model_parallel.
+00003b90: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+00003ba0: 2e63 6f6d 705f 636f 6d6d 5f70 6172 616c  .comp_comm_paral
+00003bb0: 6c65 6c20 3d20 6d6f 655f 636f 6e66 6967  lel = moe_config
+00003bc0: 2e63 6f6d 705f 636f 6d6d 5f70 6172 616c  .comp_comm_paral
+00003bd0: 6c65 6c0a 2020 2020 2020 2020 2020 2020  lel.            
+00003be0: 7365 6c66 2e63 6f6d 705f 636f 6d6d 5f70  self.comp_comm_p
+00003bf0: 6172 616c 6c65 6c5f 6465 6772 6565 203d  arallel_degree =
+00003c00: 206d 6f65 5f63 6f6e 6669 672e 636f 6d70   moe_config.comp
+00003c10: 5f63 6f6d 6d5f 7061 7261 6c6c 656c 5f64  _comm_parallel_d
+00003c20: 6567 7265 650a 2020 2020 2020 2020 2020  egree.          
+00003c30: 2020 7365 6c66 2e67 726f 7570 5f77 6973    self.group_wis
+00003c40: 655f 6132 6120 3d20 6d6f 655f 636f 6e66  e_a2a = moe_conf
+00003c50: 6967 2e67 726f 7570 5f77 6973 655f 6132  ig.group_wise_a2
+00003c60: 610a 2020 2020 2020 2020 2020 2020 6966  a.            if
+00003c70: 206e 6f74 2028 7365 6c66 2e6d 7020 3e20   not (self.mp > 
+00003c80: 3120 616e 6420 7365 6c66 2e64 7020 3d3d  1 and self.dp ==
+00003c90: 2073 656c 662e 6570 293a 0a20 2020 2020   self.ep):.     
+00003ca0: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+00003cb0: 6772 6f75 705f 7769 7365 5f61 3261 203d  group_wise_a2a =
+00003cc0: 2046 616c 7365 0a20 2020 2020 2020 2020   False.         
+00003cd0: 2020 2066 726f 6d20 6d69 6e64 666f 726d     from mindform
+00003ce0: 6572 732e 6d6f 6475 6c65 732e 7472 616e  ers.modules.tran
+00003cf0: 7366 6f72 6d65 7220 696d 706f 7274 2046  sformer import F
+00003d00: 6565 6446 6f72 7761 7264 0a0a 2020 2020  eedForward..    
+00003d10: 2020 2020 2020 2020 7365 6c66 2e66 666e          self.ffn
+00003d20: 203d 2046 6565 6446 6f72 7761 7264 2868   = FeedForward(h
+00003d30: 6964 6465 6e5f 7369 7a65 3d68 6964 6465  idden_size=hidde
+00003d40: 6e5f 7369 7a65 2c0a 2020 2020 2020 2020  n_size,.        
+00003d50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00003d60: 2020 2020 2020 2020 2020 2066 666e 5f68             ffn_h
+00003d70: 6964 6465 6e5f 7369 7a65 3d66 666e 5f68  idden_size=ffn_h
+00003d80: 6964 6465 6e5f 7369 7a65 2c0a 2020 2020  idden_size,.    
+00003d90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00003da0: 2020 2020 2020 2020 2020 2020 2020 2064                 d
+00003db0: 726f 706f 7574 5f72 6174 653d 6472 6f70  ropout_rate=drop
+00003dc0: 6f75 745f 7261 7465 2c0a 2020 2020 2020  out_rate,.      
+00003dd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00003de0: 2020 2020 2020 2020 2020 2020 2068 6964               hid
+00003df0: 6465 6e5f 6163 743d 6869 6464 656e 5f61  den_act=hidden_a
+00003e00: 6374 2c0a 2020 2020 2020 2020 2020 2020  ct,.            
+00003e10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00003e20: 2020 2020 2020 2065 7870 6572 745f 6e75         expert_nu
+00003e30: 6d3d 7365 6c66 2e65 7870 6572 745f 6469  m=self.expert_di
+00003e40: 6d2c 0a20 2020 2020 2020 2020 2020 2020  m,.             
+00003e50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00003e60: 2020 2020 2020 7061 7261 6d5f 696e 6974        param_init
+00003e70: 5f74 7970 653d 7061 7261 6d5f 696e 6974  _type=param_init
+00003e80: 5f74 7970 652c 0a20 2020 2020 2020 2020  _type,.         
+00003e90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00003ea0: 2020 2020 2020 2020 2020 7061 7261 6c6c            parall
+00003eb0: 656c 5f63 6f6e 6669 673d 7061 7261 6c6c  el_config=parall
+00003ec0: 656c 5f63 6f6e 6669 6729 0a20 2020 2020  el_config).     
+00003ed0: 2020 2020 2020 2073 656c 662e 7265 7368         self.resh
+00003ee0: 6170 6520 3d20 502e 5265 7368 6170 6528  ape = P.Reshape(
+00003ef0: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
+00003f00: 6c66 2e73 6861 7065 203d 2050 2e53 6861  lf.shape = P.Sha
+00003f10: 7065 2829 0a20 2020 2020 2020 2020 2020  pe().           
+00003f20: 2073 656c 662e 7472 616e 7370 6f73 655f   self.transpose_
+00003f30: 3264 696d 203d 2050 2e54 7261 6e73 706f  2dim = P.Transpo
+00003f40: 7365 2829 2e73 6861 7264 2828 2873 656c  se().shard(((sel
+00003f50: 662e 6470 2c20 3129 2c29 290a 2020 2020  f.dp, 1),)).    
+00003f60: 2020 2020 2020 2020 7365 6c66 2e74 7261          self.tra
+00003f70: 6e73 706f 7365 5f33 6469 6d20 3d20 502e  nspose_3dim = P.
+00003f80: 5472 616e 7370 6f73 6528 292e 7368 6172  Transpose().shar
+00003f90: 6428 2828 7365 6c66 2e64 702c 2031 2c20  d(((self.dp, 1, 
+00003fa0: 3129 2c29 290a 2020 2020 2020 2020 2020  1),)).          
+00003fb0: 2020 7365 6c66 2e74 7261 6e73 706f 7365    self.transpose
+00003fc0: 5f34 6469 6d20 3d20 502e 5472 616e 7370  _4dim = P.Transp
+00003fd0: 6f73 6528 292e 7368 6172 6428 2828 312c  ose().shard(((1,
+00003fe0: 2073 656c 662e 6470 2c20 312c 2031 292c   self.dp, 1, 1),
+00003ff0: 2929 0a20 2020 2020 2020 2020 2020 2073  )).            s
+00004000: 656c 662e 7472 616e 7370 6f73 655f 3464  elf.transpose_4d
+00004010: 696d 5f64 7020 3d20 502e 5472 616e 7370  im_dp = P.Transp
+00004020: 6f73 6528 292e 7368 6172 6428 2828 312c  ose().shard(((1,
+00004030: 2031 2c20 7365 6c66 2e64 702c 2031 292c   1, self.dp, 1),
+00004040: 2929 0a20 2020 2020 2020 2020 2020 2073  )).            s
+00004050: 656c 662e 6261 7463 685f 6d6d 203d 2050  elf.batch_mm = P
+00004060: 2e42 6174 6368 4d61 744d 756c 2829 2e73  .BatchMatMul().s
+00004070: 6861 7264 2828 2873 656c 662e 6470 2c20  hard(((self.dp, 
+00004080: 312c 2031 292c 2028 7365 6c66 2e64 702c  1, 1), (self.dp,
+00004090: 2031 2c20 3129 2929 0a20 2020 2020 2020   1, 1))).       
+000040a0: 2020 2020 2073 656c 662e 6261 7463 685f       self.batch_
+000040b0: 6d6d 3220 3d20 502e 4261 7463 684d 6174  mm2 = P.BatchMat
+000040c0: 4d75 6c28 292e 7368 6172 6428 2828 7365  Mul().shard(((se
+000040d0: 6c66 2e64 702c 2031 2c20 3129 2c20 2873  lf.dp, 1, 1), (s
+000040e0: 656c 662e 6470 2c20 312c 2031 2929 290a  elf.dp, 1, 1))).
+000040f0: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+00004100: 2e6d 756c 203d 2050 2e4d 756c 2829 2e73  .mul = P.Mul().s
+00004110: 6861 7264 2828 2829 2c20 2829 2929 0a20  hard(((), ())). 
+00004120: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+00004130: 726f 7574 6572 203d 2052 6f75 7465 7228  router = Router(
+00004140: 645f 6d6f 6465 6c3d 6869 6464 656e 5f73  d_model=hidden_s
+00004150: 697a 652c 206d 6f65 5f63 6f6e 6669 673d  ize, moe_config=
+00004160: 6d6f 655f 636f 6e66 6967 2c20 726f 7574  moe_config, rout
+00004170: 696e 675f 706f 6c69 6379 3d22 546f 706b  ing_policy="Topk
+00004180: 526f 7574 6572 5631 222c 0a20 2020 2020  RouterV1",.     
+00004190: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000041a0: 2020 2020 2020 2020 2020 2020 7472 6169              trai
+000041b0: 6e69 6e67 3d54 7275 652c 2070 6172 616c  ning=True, paral
+000041c0: 6c65 6c5f 636f 6e66 6967 3d70 6172 616c  lel_config=paral
+000041d0: 6c65 6c5f 636f 6e66 6967 290a 2020 2020  lel_config).    
+000041e0: 2020 2020 2020 2020 7365 6c66 2e63 6173          self.cas
+000041f0: 7420 3d20 502e 4361 7374 2829 0a20 2020  t = P.Cast().   
+00004200: 2020 2020 2020 2020 2073 656c 662e 636f           self.co
+00004210: 6e63 6174 203d 2050 2e43 6f6e 6361 7428  ncat = P.Concat(
+00004220: 3329 2e73 6861 7264 2874 7570 6c65 2828  3).shard(tuple((
+00004230: 7365 6c66 2e64 702c 2031 2c20 312c 2031  self.dp, 1, 1, 1
+00004240: 2920 666f 7220 5f20 696e 2072 616e 6765  ) for _ in range
+00004250: 2873 656c 662e 636f 6d70 5f63 6f6d 6d5f  (self.comp_comm_
+00004260: 7061 7261 6c6c 656c 5f64 6567 7265 6529  parallel_degree)
+00004270: 2929 0a20 2020 2020 2020 2020 2020 2073  )).            s
+00004280: 656c 662e 636f 6e63 6174 5f64 7020 3d20  elf.concat_dp = 
+00004290: 502e 436f 6e63 6174 2832 292e 7368 6172  P.Concat(2).shar
+000042a0: 6428 2828 312c 2073 656c 662e 6470 2c20  d(((1, self.dp, 
+000042b0: 312c 2031 292c 2028 312c 2073 656c 662e  1, 1), (1, self.
+000042c0: 6470 2c20 312c 2031 2929 290a 2020 2020  dp, 1, 1))).    
+000042d0: 2020 2020 2020 2020 7365 6c66 2e73 706c          self.spl
+000042e0: 6974 203d 2050 2e53 706c 6974 2861 7869  it = P.Split(axi
+000042f0: 733d 322c 206f 7574 7075 745f 6e75 6d3d  s=2, output_num=
+00004300: 7365 6c66 2e63 6f6d 705f 636f 6d6d 5f70  self.comp_comm_p
+00004310: 6172 616c 6c65 6c5f 6465 6772 6565 292e  arallel_degree).
+00004320: 7368 6172 6428 2828 312c 2073 656c 662e  shard(((1, self.
+00004330: 6470 2c20 312c 2031 292c 2929 0a20 2020  dp, 1, 1),)).   
+00004340: 2020 2020 2020 2020 2073 656c 662e 7374           self.st
+00004350: 7269 6465 5f73 6c69 6365 203d 2050 2e53  ride_slice = P.S
+00004360: 7472 6964 6564 536c 6963 6528 292e 7368  tridedSlice().sh
+00004370: 6172 6428 2828 7365 6c66 2e64 702c 2031  ard(((self.dp, 1
+00004380: 2c20 312c 2031 292c 2929 0a20 2020 2020  , 1, 1),)).     
+00004390: 2020 2020 2020 2073 656c 662e 7374 7269         self.stri
+000043a0: 6465 5f73 6c69 6365 5f64 7020 3d20 502e  de_slice_dp = P.
+000043b0: 5374 7269 6465 6453 6c69 6365 2829 2e73  StridedSlice().s
+000043c0: 6861 7264 2828 2831 2c20 7365 6c66 2e64  hard(((1, self.d
+000043d0: 702c 2031 2c20 3129 2c29 290a 2020 2020  p, 1, 1),)).    
+000043e0: 2020 2020 2020 2020 7365 6c66 2e73 7472          self.str
+000043f0: 6964 655f 736c 6963 655f 6570 203d 2050  ide_slice_ep = P
+00004400: 2e53 7472 6964 6564 536c 6963 6528 292e  .StridedSlice().
+00004410: 7368 6172 6428 2828 7365 6c66 2e65 702c  shard(((self.ep,
+00004420: 2031 2c20 312c 2031 292c 2929 0a20 2020   1, 1, 1),)).   
+00004430: 2020 2020 2020 2020 2073 656c 662e 7374           self.st
+00004440: 7269 6465 5f73 6c69 6365 5f64 705f 6d70  ride_slice_dp_mp
+00004450: 203d 2050 2e53 7472 6964 6564 536c 6963   = P.StridedSlic
+00004460: 6528 292e 7368 6172 6428 2828 312c 2073  e().shard(((1, s
+00004470: 656c 662e 6470 2c20 7365 6c66 2e6d 702c  elf.dp, self.mp,
+00004480: 2031 292c 2929 0a20 2020 2020 2020 2020   1),)).         
+00004490: 2020 2073 656c 662e 7374 7269 6465 5f73     self.stride_s
+000044a0: 6c69 6365 5f65 705f 6d70 203d 2050 2e53  lice_ep_mp = P.S
+000044b0: 7472 6964 6564 536c 6963 6528 292e 7368  tridedSlice().sh
+000044c0: 6172 6428 2828 7365 6c66 2e65 702c 2031  ard(((self.ep, 1
+000044d0: 2c20 7365 6c66 2e6d 702c 2031 292c 2929  , self.mp, 1),))
+000044e0: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
+000044f0: 662e 656e 6162 6c65 5f63 6f6c 645f 686f  f.enable_cold_ho
+00004500: 745f 6578 7065 7274 203d 206d 6f65 5f63  t_expert = moe_c
+00004510: 6f6e 6669 672e 656e 6162 6c65 5f63 6f6c  onfig.enable_col
+00004520: 645f 686f 745f 6578 7065 7274 0a20 2020  d_hot_expert.   
+00004530: 2020 2020 2020 2020 2069 6620 7365 6c66           if self
+00004540: 2e65 6e61 626c 655f 636f 6c64 5f68 6f74  .enable_cold_hot
+00004550: 5f65 7870 6572 743a 0a20 2020 2020 2020  _expert:.       
+00004560: 2020 2020 2020 2020 2073 656c 662e 6375           self.cu
+00004570: 725f 6c61 7965 7220 3d20 6d6f 655f 636f  r_layer = moe_co
+00004580: 6e66 6967 2e63 7572 5f6c 6179 6572 0a20  nfig.cur_layer. 
+00004590: 2020 2020 2020 2020 2020 2020 2020 2073                 s
+000045a0: 656c 662e 686f 745f 6578 7065 7274 5f6e  elf.hot_expert_n
+000045b0: 756d 203d 206d 6f65 5f63 6f6e 6669 672e  um = moe_config.
+000045c0: 686f 745f 6578 7065 7274 5f6e 756d 0a20  hot_expert_num. 
+000045d0: 2020 2020 2020 2020 2020 2020 2020 2073                 s
+000045e0: 656c 662e 7570 6461 7465 5f73 7465 7020  elf.update_step 
+000045f0: 3d20 6d6f 655f 636f 6e66 6967 2e75 7064  = moe_config.upd
+00004600: 6174 655f 7374 6570 0a20 2020 2020 2020  ate_step.       
+00004610: 2020 2020 2020 2020 2073 656c 662e 636f           self.co
+00004620: 6c64 5f74 6f6b 656e 5f70 6572 6365 6e74  ld_token_percent
+00004630: 203d 206d 6f65 5f63 6f6e 6669 672e 636f   = moe_config.co
+00004640: 6c64 5f74 6f6b 656e 5f70 6572 6365 6e74  ld_token_percent
+00004650: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00004660: 2073 656c 662e 686f 745f 6578 7065 7274   self.hot_expert
+00004670: 5f69 6e64 6578 203d 2050 6172 616d 6574  _index = Paramet
+00004680: 6572 280a 2020 2020 2020 2020 2020 2020  er(.            
+00004690: 2020 2020 2020 2020 696e 6974 6961 6c69          initiali
+000046a0: 7a65 7228 5465 6e73 6f72 285b 5b69 2066  zer(Tensor([[i f
+000046b0: 6f72 2069 2069 6e20 7261 6e67 6528 7365  or i in range(se
+000046c0: 6c66 2e68 6f74 5f65 7870 6572 745f 6e75  lf.hot_expert_nu
+000046d0: 6d29 5d5d 2c20 6d73 7479 7065 2e69 6e74  m)]], mstype.int
+000046e0: 3332 292c 0a20 2020 2020 2020 2020 2020  32),.           
+000046f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00004700: 2020 2020 2028 312c 2073 656c 662e 686f       (1, self.ho
+00004710: 745f 6578 7065 7274 5f6e 756d 2c29 2c20  t_expert_num,), 
+00004720: 6d73 7479 7065 2e69 6e74 3332 292c 0a20  mstype.int32),. 
+00004730: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00004740: 2020 206e 616d 653d 2268 6f74 5f65 7870     name="hot_exp
+00004750: 6572 745f 696e 6465 7822 2b73 7472 2873  ert_index"+str(s
+00004760: 656c 662e 6375 725f 6c61 7965 7229 2c0a  elf.cur_layer),.
+00004770: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00004780: 2020 2020 7265 7175 6972 6573 5f67 7261      requires_gra
+00004790: 643d 4661 6c73 652c 2070 6172 616c 6c65  d=False, paralle
+000047a0: 6c5f 6f70 7469 6d69 7a65 723d 4661 6c73  l_optimizer=Fals
+000047b0: 6529 0a20 2020 2020 2020 2020 2020 2020  e).             
+000047c0: 2020 2073 656c 662e 636f 6c64 5f65 7870     self.cold_exp
+000047d0: 6572 745f 696e 6465 7820 3d20 5061 7261  ert_index = Para
+000047e0: 6d65 7465 7228 0a20 2020 2020 2020 2020  meter(.         
+000047f0: 2020 2020 2020 2020 2020 2069 6e69 7469             initi
+00004800: 616c 697a 6572 2854 656e 736f 7228 5b5b  alizer(Tensor([[
+00004810: 6920 666f 7220 6920 696e 2072 616e 6765  i for i in range
+00004820: 2873 656c 662e 686f 745f 6578 7065 7274  (self.hot_expert
+00004830: 5f6e 756d 2c20 7365 6c66 2e65 7870 6572  _num, self.exper
+00004840: 745f 6469 6d29 5d5d 2c20 6d73 7479 7065  t_dim)]], mstype
+00004850: 2e69 6e74 3332 292c 0a20 2020 2020 2020  .int32),.       
+00004860: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00004870: 2020 2020 2020 2020 2028 312c 2073 656c           (1, sel
+00004880: 662e 6578 7065 7274 5f64 696d 202d 2073  f.expert_dim - s
+00004890: 656c 662e 686f 745f 6578 7065 7274 5f6e  elf.hot_expert_n
+000048a0: 756d 2c29 2c20 6d73 7479 7065 2e69 6e74  um,), mstype.int
+000048b0: 3332 292c 0a20 2020 2020 2020 2020 2020  32),.           
+000048c0: 2020 2020 2020 2020 206e 616d 653d 2263           name="c
+000048d0: 6f6c 645f 6578 7065 7274 5f69 6e64 6578  old_expert_index
+000048e0: 222b 7374 7228 7365 6c66 2e63 7572 5f6c  "+str(self.cur_l
+000048f0: 6179 6572 292c 0a20 2020 2020 2020 2020  ayer),.         
+00004900: 2020 2020 2020 2020 2020 2072 6571 7569             requi
+00004910: 7265 735f 6772 6164 3d46 616c 7365 2c20  res_grad=False, 
+00004920: 7061 7261 6c6c 656c 5f6f 7074 696d 697a  parallel_optimiz
+00004930: 6572 3d46 616c 7365 290a 2020 2020 2020  er=False).      
+00004940: 2020 2020 2020 2020 2020 6d6c 705f 7061            mlp_pa
+00004950: 7261 6c6c 656c 5f63 6f6e 6669 6720 3d20  rallel_config = 
+00004960: 4d6f 4550 6172 616c 6c65 6c43 6f6e 6669  MoEParallelConfi
+00004970: 6728 6461 7461 5f70 6172 616c 6c65 6c3d  g(data_parallel=
+00004980: 7365 6c66 2e64 702c 0a20 2020 2020 2020  self.dp,.       
+00004990: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000049a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000049b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000049c0: 206d 6f64 656c 5f70 6172 616c 6c65 6c3d   model_parallel=
+000049d0: 7365 6c66 2e6d 702c 0a20 2020 2020 2020  self.mp,.       
+000049e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000049f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00004a00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00004a10: 7265 7175 6972 6573 5f67 7261 643d 4661  requires_grad=Fa
-00004a20: 6c73 652c 2070 6172 616c 6c65 6c5f 6f70  lse, parallel_op
-00004a30: 7469 6d69 7a65 723d 4661 6c73 6529 0a20  timizer=False). 
-00004a40: 2020 2020 2020 2020 2020 2020 2020 2073                 s
-00004a50: 656c 662e 636f 6c64 5f65 7870 6572 745f  elf.cold_expert_
-00004a60: 696e 6465 7820 3d20 5061 7261 6d65 7465  index = Paramete
-00004a70: 7228 0a20 2020 2020 2020 2020 2020 2020  r(.             
-00004a80: 2020 2020 2020 2069 6e69 7469 616c 697a         initializ
-00004a90: 6572 2854 656e 736f 7228 5b5b 6920 666f  er(Tensor([[i fo
-00004aa0: 7220 6920 696e 2072 616e 6765 2873 656c  r i in range(sel
-00004ab0: 662e 686f 745f 6578 7065 7274 5f6e 756d  f.hot_expert_num
-00004ac0: 2c20 7365 6c66 2e65 7870 6572 745f 6469  , self.expert_di
-00004ad0: 6d29 5d5d 2c20 6d73 7479 7065 2e69 6e74  m)]], mstype.int
-00004ae0: 3332 292c 0a20 2020 2020 2020 2020 2020  32),.           
+00004a10: 2065 7870 6572 745f 7061 7261 6c6c 656c   expert_parallel
+00004a20: 3d31 290a 2020 2020 2020 2020 2020 2020  =1).            
+00004a30: 2020 2020 7365 6c66 2e6d 6c70 203d 2046      self.mlp = F
+00004a40: 6565 6446 6f72 7761 7264 2868 6964 6465  eedForward(hidde
+00004a50: 6e5f 7369 7a65 3d68 6964 6465 6e5f 7369  n_size=hidden_si
+00004a60: 7a65 2c0a 2020 2020 2020 2020 2020 2020  ze,.            
+00004a70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00004a80: 2020 2020 2020 2020 2020 2066 666e 5f68             ffn_h
+00004a90: 6964 6465 6e5f 7369 7a65 3d66 666e 5f68  idden_size=ffn_h
+00004aa0: 6964 6465 6e5f 7369 7a65 2c0a 2020 2020  idden_size,.    
+00004ab0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00004ac0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00004ad0: 2020 2064 726f 706f 7574 5f72 6174 653d     dropout_rate=
+00004ae0: 6472 6f70 6f75 745f 7261 7465 2c0a 2020  dropout_rate,.  
 00004af0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00004b00: 2020 2020 2028 312c 2073 656c 662e 6578       (1, self.ex
-00004b10: 7065 7274 5f64 696d 202d 2073 656c 662e  pert_dim - self.
-00004b20: 686f 745f 6578 7065 7274 5f6e 756d 2c29  hot_expert_num,)
-00004b30: 2c20 6d73 7479 7065 2e69 6e74 3332 292c  , mstype.int32),
-00004b40: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00004b50: 2020 2020 206e 616d 653d 2263 6f6c 645f       name="cold_
-00004b60: 6578 7065 7274 5f69 6e64 6578 222b 7374  expert_index"+st
-00004b70: 7228 7365 6c66 2e63 7572 5f6c 6179 6572  r(self.cur_layer
-00004b80: 292c 0a20 2020 2020 2020 2020 2020 2020  ),.             
-00004b90: 2020 2020 2020 2072 6571 7569 7265 735f         requires_
-00004ba0: 6772 6164 3d46 616c 7365 2c20 7061 7261  grad=False, para
-00004bb0: 6c6c 656c 5f6f 7074 696d 697a 6572 3d46  llel_optimizer=F
-00004bc0: 616c 7365 290a 2020 2020 2020 2020 2020  alse).          
-00004bd0: 2020 2020 2020 6d6c 705f 7061 7261 6c6c        mlp_parall
-00004be0: 656c 5f63 6f6e 6669 6720 3d20 4d6f 4550  el_config = MoEP
-00004bf0: 6172 616c 6c65 6c43 6f6e 6669 6728 6461  arallelConfig(da
-00004c00: 7461 5f70 6172 616c 6c65 6c3d 7365 6c66  ta_parallel=self
-00004c10: 2e64 702c 0a20 2020 2020 2020 2020 2020  .dp,.           
-00004c20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00004c30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00004c40: 2020 2020 2020 2020 2020 2020 206d 6f64               mod
-00004c50: 656c 5f70 6172 616c 6c65 6c3d 7365 6c66  el_parallel=self
-00004c60: 2e6d 702c 0a20 2020 2020 2020 2020 2020  .mp,.           
-00004c70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00004c80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00004c90: 2020 2020 2020 2020 2020 2020 2065 7870               exp
-00004ca0: 6572 745f 7061 7261 6c6c 656c 3d31 290a  ert_parallel=1).
-00004cb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00004cc0: 7365 6c66 2e6d 6c70 203d 2046 6565 6446  self.mlp = FeedF
-00004cd0: 6f72 7761 7264 2868 6964 6465 6e5f 7369  orward(hidden_si
-00004ce0: 7a65 3d68 6964 6465 6e5f 7369 7a65 2c0a  ze=hidden_size,.
-00004cf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00004d00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00004d10: 2020 2020 2020 2066 666e 5f68 6964 6465         ffn_hidde
-00004d20: 6e5f 7369 7a65 3d66 666e 5f68 6964 6465  n_size=ffn_hidde
-00004d30: 6e5f 7369 7a65 2c0a 2020 2020 2020 2020  n_size,.        
-00004d40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00004d50: 2020 2020 2020 2020 2020 2020 2020 2064                 d
-00004d60: 726f 706f 7574 5f72 6174 653d 6472 6f70  ropout_rate=drop
-00004d70: 6f75 745f 7261 7465 2c0a 2020 2020 2020  out_rate,.      
-00004d80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00004d90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00004da0: 2068 6964 6465 6e5f 6163 743d 6869 6464   hidden_act=hidd
-00004db0: 656e 5f61 6374 2c0a 2020 2020 2020 2020  en_act,.        
-00004dc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00004dd0: 2020 2020 2020 2020 2020 2020 2020 2065                 e
-00004de0: 7870 6572 745f 6e75 6d3d 7365 6c66 2e68  xpert_num=self.h
-00004df0: 6f74 5f65 7870 6572 745f 6e75 6d2c 0a20  ot_expert_num,. 
-00004e00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00004b00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00004b10: 2020 2020 2068 6964 6465 6e5f 6163 743d       hidden_act=
+00004b20: 6869 6464 656e 5f61 6374 2c0a 2020 2020  hidden_act,.    
+00004b30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00004b40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00004b50: 2020 2065 7870 6572 745f 6e75 6d3d 7365     expert_num=se
+00004b60: 6c66 2e68 6f74 5f65 7870 6572 745f 6e75  lf.hot_expert_nu
+00004b70: 6d2c 0a20 2020 2020 2020 2020 2020 2020  m,.             
+00004b80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00004b90: 2020 2020 2020 2020 2020 7061 7261 6d5f            param_
+00004ba0: 696e 6974 5f74 7970 653d 7061 7261 6d5f  init_type=param_
+00004bb0: 696e 6974 5f74 7970 652c 0a20 2020 2020  init_type,.     
+00004bc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00004bd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00004be0: 2020 7061 7261 6c6c 656c 5f63 6f6e 6669    parallel_confi
+00004bf0: 673d 6d6c 705f 7061 7261 6c6c 656c 5f63  g=mlp_parallel_c
+00004c00: 6f6e 6669 6729 0a20 2020 2020 2020 2020  onfig).         
+00004c10: 2020 2020 2020 2073 656c 662e 6761 7468         self.gath
+00004c20: 6572 203d 2050 2e47 6174 6865 7228 3029  er = P.Gather(0)
+00004c30: 2e73 6861 7264 2828 2831 2c20 312c 2073  .shard(((1, 1, s
+00004c40: 656c 662e 6470 2c20 3129 2c20 2831 2c29  elf.dp, 1), (1,)
+00004c50: 2929 0a20 2020 2020 2020 2020 2020 2020  )).             
+00004c60: 2020 2073 656c 662e 6761 7468 6572 3220     self.gather2 
+00004c70: 3d20 502e 4761 7468 6572 2830 292e 7368  = P.Gather(0).sh
+00004c80: 6172 6428 2828 7365 6c66 2e64 702c 2031  ard(((self.dp, 1
+00004c90: 2c20 312c 2031 292c 2028 312c 2929 290a  , 1, 1), (1,))).
+00004ca0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00004cb0: 7365 6c66 2e63 6f6e 6361 7430 203d 2050  self.concat0 = P
+00004cc0: 2e43 6f6e 6361 7428 3029 2e73 6861 7264  .Concat(0).shard
+00004cd0: 2828 2831 2c29 2c20 2831 2c29 2929 0a20  (((1,), (1,))). 
+00004ce0: 2020 2020 2020 2020 2020 2020 2020 2073                 s
+00004cf0: 656c 662e 636f 6e63 6174 3120 3d20 502e  elf.concat1 = P.
+00004d00: 436f 6e63 6174 2831 292e 7368 6172 6428  Concat(1).shard(
+00004d10: 2828 7365 6c66 2e64 702c 2031 2c20 312c  ((self.dp, 1, 1,
+00004d20: 2031 292c 2028 7365 6c66 2e64 702c 2031   1), (self.dp, 1
+00004d30: 2c20 312c 2031 2929 290a 2020 2020 2020  , 1, 1))).      
+00004d40: 2020 2020 2020 2020 2020 7365 6c66 2e63            self.c
+00004d50: 6f6e 6361 7432 203d 2050 2e43 6f6e 6361  oncat2 = P.Conca
+00004d60: 7428 3229 2e73 6861 7264 2828 2873 656c  t(2).shard(((sel
+00004d70: 662e 6470 2c20 312c 2031 2c20 3129 2c20  f.dp, 1, 1, 1), 
+00004d80: 2873 656c 662e 6470 2c20 312c 2031 2c20  (self.dp, 1, 1, 
+00004d90: 3129 2929 0a20 2020 2020 2020 2020 2020  1))).           
+00004da0: 2020 2020 2073 656c 662e 7a65 726f 7320       self.zeros 
+00004db0: 3d20 502e 5a65 726f 7328 290a 2020 2020  = P.Zeros().    
+00004dc0: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+00004dd0: 2e74 7261 6e73 706f 7365 5f31 6469 6d5f  .transpose_1dim_
+00004de0: 6470 203d 2050 2e54 7261 6e73 706f 7365  dp = P.Transpose
+00004df0: 2829 2e73 6861 7264 2828 2873 656c 662e  ().shard(((self.
+00004e00: 6470 2c20 312c 2031 2c20 3129 2c29 290a  dp, 1, 1, 1),)).
 00004e10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00004e20: 2020 2020 2020 7061 7261 6d5f 696e 6974        param_init
-00004e30: 5f74 7970 653d 7061 7261 6d5f 696e 6974  _type=param_init
-00004e40: 5f74 7970 652c 0a20 2020 2020 2020 2020  _type,.         
-00004e50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00004e60: 2020 2020 2020 2020 2020 2020 2020 7061                pa
-00004e70: 7261 6c6c 656c 5f63 6f6e 6669 673d 6d6c  rallel_config=ml
-00004e80: 705f 7061 7261 6c6c 656c 5f63 6f6e 6669  p_parallel_confi
-00004e90: 6729 0a20 2020 2020 2020 2020 2020 2020  g).             
-00004ea0: 2020 2073 656c 662e 6761 7468 6572 203d     self.gather =
-00004eb0: 2050 2e47 6174 6865 7228 3029 2e73 6861   P.Gather(0).sha
-00004ec0: 7264 2828 2831 2c20 312c 2073 656c 662e  rd(((1, 1, self.
-00004ed0: 6470 2c20 3129 2c20 2831 2c29 2929 0a20  dp, 1), (1,))). 
-00004ee0: 2020 2020 2020 2020 2020 2020 2020 2073                 s
-00004ef0: 656c 662e 6761 7468 6572 3220 3d20 502e  elf.gather2 = P.
-00004f00: 4761 7468 6572 2830 292e 7368 6172 6428  Gather(0).shard(
-00004f10: 2828 7365 6c66 2e64 702c 2031 2c20 312c  ((self.dp, 1, 1,
-00004f20: 2031 292c 2028 312c 2929 290a 2020 2020   1), (1,))).    
-00004f30: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-00004f40: 2e63 6f6e 6361 7430 203d 2050 2e43 6f6e  .concat0 = P.Con
-00004f50: 6361 7428 3029 2e73 6861 7264 2828 2831  cat(0).shard(((1
-00004f60: 2c29 2c20 2831 2c29 2929 0a20 2020 2020  ,), (1,))).     
-00004f70: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-00004f80: 636f 6e63 6174 3120 3d20 502e 436f 6e63  concat1 = P.Conc
-00004f90: 6174 2831 292e 7368 6172 6428 2828 7365  at(1).shard(((se
-00004fa0: 6c66 2e64 702c 2031 2c20 312c 2031 292c  lf.dp, 1, 1, 1),
-00004fb0: 2028 7365 6c66 2e64 702c 2031 2c20 312c   (self.dp, 1, 1,
-00004fc0: 2031 2929 290a 2020 2020 2020 2020 2020   1))).          
-00004fd0: 2020 2020 2020 7365 6c66 2e63 6f6e 6361        self.conca
-00004fe0: 7432 203d 2050 2e43 6f6e 6361 7428 3229  t2 = P.Concat(2)
-00004ff0: 2e73 6861 7264 2828 2873 656c 662e 6470  .shard(((self.dp
-00005000: 2c20 312c 2031 2c20 3129 2c20 2873 656c  , 1, 1, 1), (sel
-00005010: 662e 6470 2c20 312c 2031 2c20 3129 2929  f.dp, 1, 1, 1)))
-00005020: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00005030: 2073 656c 662e 7a65 726f 7320 3d20 502e   self.zeros = P.
-00005040: 5a65 726f 7328 290a 2020 2020 2020 2020  Zeros().        
-00005050: 2020 2020 2020 2020 7365 6c66 2e74 7261          self.tra
-00005060: 6e73 706f 7365 5f31 6469 6d5f 6470 203d  nspose_1dim_dp =
-00005070: 2050 2e54 7261 6e73 706f 7365 2829 2e73   P.Transpose().s
-00005080: 6861 7264 2828 2873 656c 662e 6470 2c20  hard(((self.dp, 
-00005090: 312c 2031 2c20 3129 2c29 290a 2020 2020  1, 1, 1),)).    
-000050a0: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-000050b0: 2e65 7175 616c 203d 2050 2e45 7175 616c  .equal = P.Equal
-000050c0: 2829 2e73 6861 7264 2828 2831 2c20 3129  ().shard(((1, 1)
-000050d0: 2c20 2831 2c20 3129 2929 0a20 2020 2020  , (1, 1))).     
-000050e0: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-000050f0: 6571 7561 6c32 203d 2050 2e45 7175 616c  equal2 = P.Equal
-00005100: 2829 2e73 6861 7264 2828 2831 2c29 2c20  ().shard(((1,), 
-00005110: 2829 2929 0a20 2020 2020 2020 2020 2020  ())).           
-00005120: 2020 2020 2073 656c 662e 7265 6475 6365       self.reduce
-00005130: 5f61 6e79 203d 2050 2e52 6564 7563 6541  _any = P.ReduceA
-00005140: 6e79 2829 2e73 6861 7264 2828 2831 2c20  ny().shard(((1, 
-00005150: 3129 2c29 290a 2020 2020 2020 2020 2020  1),)).          
-00005160: 2020 2020 2020 7365 6c66 2e74 6f70 6b20        self.topk 
-00005170: 3d20 502e 546f 704b 2829 2e73 6861 7264  = P.TopK().shard
-00005180: 2828 2831 2c29 2c29 290a 0a20 2020 2064  (((1,),))..    d
-00005190: 6566 2066 666e 5f69 6e66 6572 2873 656c  ef ffn_infer(sel
-000051a0: 662c 2065 7870 6572 745f 696e 7075 742c  f, expert_input,
-000051b0: 2063 6170 6163 6974 7929 3a0a 2020 2020   capacity):.    
-000051c0: 2020 2020 2222 220a 2020 2020 2020 2020      """.        
-000051d0: 436f 6d70 7574 696e 6720 7468 6520 4646  Computing the FF
-000051e0: 4e2e 0a20 2020 2020 2020 2022 2222 0a20  N..        """. 
-000051f0: 2020 2020 2020 2070 6164 5f73 697a 6520         pad_size 
-00005200: 3d20 300a 2020 2020 2020 2020 6966 2073  = 0.        if s
-00005210: 656c 662e 6772 6f75 705f 7769 7365 5f61  elf.group_wise_a
-00005220: 3261 3a0a 2020 2020 2020 2020 2020 2020  2a:.            
-00005230: 2320 4966 2063 6170 6163 6974 7920 6361  # If capacity ca
-00005240: 6e27 7420 6469 7620 6279 206d 702c 2070  n't div by mp, p
-00005250: 6164 2066 6f72 206d 7020 7368 6172 642e  ad for mp shard.
-00005260: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if 
-00005270: 6361 7061 6369 7479 2025 2073 656c 662e  capacity % self.
-00005280: 6d70 2021 3d20 303a 0a20 2020 2020 2020  mp != 0:.       
-00005290: 2020 2020 2020 2020 2070 6164 5f73 697a           pad_siz
-000052a0: 6520 3d20 7365 6c66 2e6d 7020 2d20 2863  e = self.mp - (c
-000052b0: 6170 6163 6974 7920 2520 7365 6c66 2e6d  apacity % self.m
-000052c0: 7029 0a20 2020 2020 2020 2020 2020 2069  p).            i
-000052d0: 6620 7061 645f 7369 7a65 2021 3d20 303a  f pad_size != 0:
-000052e0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-000052f0: 2063 6170 6163 6974 7920 2b3d 2070 6164   capacity += pad
-00005300: 5f73 697a 650a 2020 2020 2020 2020 2020  _size.          
-00005310: 2020 2020 2020 7061 645f 7465 6e73 6f72        pad_tensor
+00004e20: 7365 6c66 2e65 7175 616c 203d 2050 2e45  self.equal = P.E
+00004e30: 7175 616c 2829 2e73 6861 7264 2828 2831  qual().shard(((1
+00004e40: 2c20 3129 2c20 2831 2c20 3129 2929 0a20  , 1), (1, 1))). 
+00004e50: 2020 2020 2020 2020 2020 2020 2020 2073                 s
+00004e60: 656c 662e 6571 7561 6c32 203d 2050 2e45  elf.equal2 = P.E
+00004e70: 7175 616c 2829 2e73 6861 7264 2828 2831  qual().shard(((1
+00004e80: 2c29 2c20 2829 2929 0a20 2020 2020 2020  ,), ())).       
+00004e90: 2020 2020 2020 2020 2073 656c 662e 7265           self.re
+00004ea0: 6475 6365 5f61 6e79 203d 2050 2e52 6564  duce_any = P.Red
+00004eb0: 7563 6541 6e79 2829 2e73 6861 7264 2828  uceAny().shard((
+00004ec0: 2831 2c20 3129 2c29 290a 2020 2020 2020  (1, 1),)).      
+00004ed0: 2020 2020 2020 2020 2020 7365 6c66 2e74            self.t
+00004ee0: 6f70 6b20 3d20 502e 546f 704b 2829 2e73  opk = P.TopK().s
+00004ef0: 6861 7264 2828 2831 2c29 2c29 290a 0a20  hard(((1,),)).. 
+00004f00: 2020 2064 6566 2066 666e 5f69 6e66 6572     def ffn_infer
+00004f10: 2873 656c 662c 2065 7870 6572 745f 696e  (self, expert_in
+00004f20: 7075 742c 2063 6170 6163 6974 7929 3a0a  put, capacity):.
+00004f30: 2020 2020 2020 2020 2222 220a 2020 2020          """.    
+00004f40: 2020 2020 436f 6d70 7574 696e 6720 7468      Computing th
+00004f50: 6520 4646 4e2e 0a20 2020 2020 2020 2022  e FFN..        "
+00004f60: 2222 0a20 2020 2020 2020 2070 6164 5f73  "".        pad_s
+00004f70: 697a 6520 3d20 300a 2020 2020 2020 2020  ize = 0.        
+00004f80: 6966 2073 656c 662e 6772 6f75 705f 7769  if self.group_wi
+00004f90: 7365 5f61 3261 3a0a 2020 2020 2020 2020  se_a2a:.        
+00004fa0: 2020 2020 2320 4966 2063 6170 6163 6974      # If capacit
+00004fb0: 7920 6361 6e27 7420 6469 7620 6279 206d  y can't div by m
+00004fc0: 702c 2070 6164 2066 6f72 206d 7020 7368  p, pad for mp sh
+00004fd0: 6172 642e 0a20 2020 2020 2020 2020 2020  ard..           
+00004fe0: 2069 6620 6361 7061 6369 7479 2025 2073   if capacity % s
+00004ff0: 656c 662e 6d70 2021 3d20 303a 0a20 2020  elf.mp != 0:.   
+00005000: 2020 2020 2020 2020 2020 2020 2070 6164               pad
+00005010: 5f73 697a 6520 3d20 7365 6c66 2e6d 7020  _size = self.mp 
+00005020: 2d20 2863 6170 6163 6974 7920 2520 7365  - (capacity % se
+00005030: 6c66 2e6d 7029 0a20 2020 2020 2020 2020  lf.mp).         
+00005040: 2020 2069 6620 7061 645f 7369 7a65 2021     if pad_size !
+00005050: 3d20 303a 0a20 2020 2020 2020 2020 2020  = 0:.           
+00005060: 2020 2020 2063 6170 6163 6974 7920 2b3d       capacity +=
+00005070: 2070 6164 5f73 697a 650a 2020 2020 2020   pad_size.      
+00005080: 2020 2020 2020 2020 2020 7061 645f 7465            pad_te
+00005090: 6e73 6f72 203d 2073 656c 662e 7374 7269  nsor = self.stri
+000050a0: 6465 5f73 6c69 6365 5f64 7028 6578 7065  de_slice_dp(expe
+000050b0: 7274 5f69 6e70 7574 2c20 2830 2c20 302c  rt_input, (0, 0,
+000050c0: 2030 2c20 3029 2c0a 2020 2020 2020 2020   0, 0),.        
+000050d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000050e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000050f0: 2020 2020 2020 2020 2020 2873 656c 662e            (self.
+00005100: 6578 7065 7274 5f64 696d 2c20 7365 6c66  expert_dim, self
+00005110: 2e64 705f 6772 6f75 702c 2070 6164 5f73  .dp_group, pad_s
+00005120: 697a 652c 2073 656c 662e 6869 6464 656e  ize, self.hidden
+00005130: 5f73 697a 6529 2c0a 2020 2020 2020 2020  _size),.        
+00005140: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00005150: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00005160: 2020 2020 2020 2020 2020 2831 2c20 312c            (1, 1,
+00005170: 2031 2c20 3129 290a 2020 2020 2020 2020   1, 1)).        
+00005180: 2020 2020 2020 2020 6578 7065 7274 5f69          expert_i
+00005190: 6e70 7574 203d 2073 656c 662e 636f 6e63  nput = self.conc
+000051a0: 6174 5f64 7028 2865 7870 6572 745f 696e  at_dp((expert_in
+000051b0: 7075 742c 2070 6164 5f74 656e 736f 7229  put, pad_tensor)
+000051c0: 290a 2020 2020 2020 2020 2020 2020 2320  ).            # 
+000051d0: 6361 7061 6369 7479 2073 6861 7264 2062  capacity shard b
+000051e0: 7920 6d70 0a20 2020 2020 2020 2020 2020  y mp.           
+000051f0: 2065 7870 6572 745f 696e 7075 7420 3d20   expert_input = 
+00005200: 7365 6c66 2e73 7472 6964 655f 736c 6963  self.stride_slic
+00005210: 655f 6470 5f6d 7028 6578 7065 7274 5f69  e_dp_mp(expert_i
+00005220: 6e70 7574 2c20 2830 2c20 302c 2030 2c20  nput, (0, 0, 0, 
+00005230: 3029 2c0a 2020 2020 2020 2020 2020 2020  0),.            
+00005240: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00005250: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00005260: 2020 2020 2020 2028 7365 6c66 2e65 7870         (self.exp
+00005270: 6572 745f 6469 6d2c 2073 656c 662e 6470  ert_dim, self.dp
+00005280: 5f67 726f 7570 2c20 6361 7061 6369 7479  _group, capacity
+00005290: 2c20 7365 6c66 2e68 6964 6465 6e5f 7369  , self.hidden_si
+000052a0: 7a65 292c 0a20 2020 2020 2020 2020 2020  ze),.           
+000052b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000052c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000052d0: 2020 2020 2020 2020 2831 2c20 312c 2031          (1, 1, 1
+000052e0: 2c20 3129 290a 2020 2020 2020 2020 2020  , 1)).          
+000052f0: 2020 2320 6772 6f75 702d 7769 7365 2061    # group-wise a
+00005300: 6c6c 746f 616c 6c0a 2020 2020 2020 2020  lltoall.        
+00005310: 2020 2020 6578 7065 7274 5f69 6e70 7574      expert_input
 00005320: 203d 2073 656c 662e 7374 7269 6465 5f73   = self.stride_s
-00005330: 6c69 6365 5f64 7028 6578 7065 7274 5f69  lice_dp(expert_i
-00005340: 6e70 7574 2c20 2830 2c20 302c 2030 2c20  nput, (0, 0, 0, 
-00005350: 3029 2c0a 2020 2020 2020 2020 2020 2020  0),.            
+00005330: 6c69 6365 5f65 705f 6d70 2865 7870 6572  lice_ep_mp(exper
+00005340: 745f 696e 7075 742c 2028 302c 2030 2c20  t_input, (0, 0, 
+00005350: 302c 2030 292c 0a20 2020 2020 2020 2020  0, 0),.         
 00005360: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00005370: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00005380: 2020 2020 2020 2873 656c 662e 6578 7065        (self.expe
-00005390: 7274 5f64 696d 2c20 7365 6c66 2e64 705f  rt_dim, self.dp_
-000053a0: 6772 6f75 702c 2070 6164 5f73 697a 652c  group, pad_size,
-000053b0: 2073 656c 662e 6869 6464 656e 5f73 697a   self.hidden_siz
-000053c0: 6529 2c0a 2020 2020 2020 2020 2020 2020  e),.            
+00005380: 2020 2020 2020 2020 2020 2873 656c 662e            (self.
+00005390: 6578 7065 7274 5f64 696d 2c20 7365 6c66  expert_dim, self
+000053a0: 2e64 705f 6772 6f75 702c 2063 6170 6163  .dp_group, capac
+000053b0: 6974 792c 2073 656c 662e 6869 6464 656e  ity, self.hidden
+000053c0: 5f73 697a 6529 2c0a 2020 2020 2020 2020  _size),.        
 000053d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 000053e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000053f0: 2020 2020 2020 2831 2c20 312c 2031 2c20        (1, 1, 1, 
-00005400: 3129 290a 2020 2020 2020 2020 2020 2020  1)).            
-00005410: 2020 2020 6578 7065 7274 5f69 6e70 7574      expert_input
-00005420: 203d 2073 656c 662e 636f 6e63 6174 5f64   = self.concat_d
-00005430: 7028 2865 7870 6572 745f 696e 7075 742c  p((expert_input,
-00005440: 2070 6164 5f74 656e 736f 7229 290a 2020   pad_tensor)).  
-00005450: 2020 2020 2020 2020 2020 2320 6361 7061            # capa
-00005460: 6369 7479 2073 6861 7264 2062 7920 6d70  city shard by mp
-00005470: 0a20 2020 2020 2020 2020 2020 2065 7870  .            exp
-00005480: 6572 745f 696e 7075 7420 3d20 7365 6c66  ert_input = self
-00005490: 2e73 7472 6964 655f 736c 6963 655f 6470  .stride_slice_dp
-000054a0: 5f6d 7028 6578 7065 7274 5f69 6e70 7574  _mp(expert_input
-000054b0: 2c20 2830 2c20 302c 2030 2c20 3029 2c0a  , (0, 0, 0, 0),.
-000054c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000054d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000053f0: 2020 2020 2020 2020 2020 2028 312c 2031             (1, 1
+00005400: 2c20 312c 2031 2929 0a20 2020 2020 2020  , 1, 1)).       
+00005410: 2020 2020 2023 2061 6c6c 6761 7468 6572       # allgather
+00005420: 0a20 2020 2020 2020 2020 2020 2065 7870  .            exp
+00005430: 6572 745f 696e 7075 7420 3d20 7365 6c66  ert_input = self
+00005440: 2e73 7472 6964 655f 736c 6963 655f 6570  .stride_slice_ep
+00005450: 2865 7870 6572 745f 696e 7075 742c 2028  (expert_input, (
+00005460: 302c 2030 2c20 302c 2030 292c 0a20 2020  0, 0, 0, 0),.   
+00005470: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00005480: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00005490: 2020 2020 2020 2020 2020 2020 2028 7365               (se
+000054a0: 6c66 2e65 7870 6572 745f 6469 6d2c 2073  lf.expert_dim, s
+000054b0: 656c 662e 6470 5f67 726f 7570 2c20 6361  elf.dp_group, ca
+000054c0: 7061 6369 7479 2c20 7365 6c66 2e68 6964  pacity, self.hid
+000054d0: 6465 6e5f 7369 7a65 292c 0a20 2020 2020  den_size),.     
 000054e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000054f0: 2020 2028 7365 6c66 2e65 7870 6572 745f     (self.expert_
-00005500: 6469 6d2c 2073 656c 662e 6470 5f67 726f  dim, self.dp_gro
-00005510: 7570 2c20 6361 7061 6369 7479 2c20 7365  up, capacity, se
-00005520: 6c66 2e68 6964 6465 6e5f 7369 7a65 292c  lf.hidden_size),
-00005530: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00005540: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00005550: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00005560: 2020 2020 2831 2c20 312c 2031 2c20 3129      (1, 1, 1, 1)
-00005570: 290a 2020 2020 2020 2020 2020 2020 2320  ).            # 
-00005580: 6772 6f75 702d 7769 7365 2061 6c6c 746f  group-wise allto
-00005590: 616c 6c0a 2020 2020 2020 2020 2020 2020  all.            
-000055a0: 6578 7065 7274 5f69 6e70 7574 203d 2073  expert_input = s
-000055b0: 656c 662e 7374 7269 6465 5f73 6c69 6365  elf.stride_slice
-000055c0: 5f65 705f 6d70 2865 7870 6572 745f 696e  _ep_mp(expert_in
-000055d0: 7075 742c 2028 302c 2030 2c20 302c 2030  put, (0, 0, 0, 0
-000055e0: 292c 0a20 2020 2020 2020 2020 2020 2020  ),.             
-000055f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00005600: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00005610: 2020 2020 2020 2873 656c 662e 6578 7065        (self.expe
-00005620: 7274 5f64 696d 2c20 7365 6c66 2e64 705f  rt_dim, self.dp_
-00005630: 6772 6f75 702c 2063 6170 6163 6974 792c  group, capacity,
-00005640: 2073 656c 662e 6869 6464 656e 5f73 697a   self.hidden_siz
-00005650: 6529 2c0a 2020 2020 2020 2020 2020 2020  e),.            
-00005660: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00005670: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00005680: 2020 2020 2020 2028 312c 2031 2c20 312c         (1, 1, 1,
-00005690: 2031 2929 0a20 2020 2020 2020 2020 2020   1)).           
-000056a0: 2023 2061 6c6c 6761 7468 6572 0a20 2020   # allgather.   
-000056b0: 2020 2020 2020 2020 2065 7870 6572 745f           expert_
-000056c0: 696e 7075 7420 3d20 7365 6c66 2e73 7472  input = self.str
-000056d0: 6964 655f 736c 6963 655f 6570 2865 7870  ide_slice_ep(exp
-000056e0: 6572 745f 696e 7075 742c 2028 302c 2030  ert_input, (0, 0
-000056f0: 2c20 302c 2030 292c 0a20 2020 2020 2020  , 0, 0),.       
-00005700: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00005710: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00005720: 2020 2020 2020 2020 2028 7365 6c66 2e65           (self.e
-00005730: 7870 6572 745f 6469 6d2c 2073 656c 662e  xpert_dim, self.
-00005740: 6470 5f67 726f 7570 2c20 6361 7061 6369  dp_group, capaci
-00005750: 7479 2c20 7365 6c66 2e68 6964 6465 6e5f  ty, self.hidden_
-00005760: 7369 7a65 292c 0a20 2020 2020 2020 2020  size),.         
-00005770: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00005780: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00005790: 2020 2020 2020 2028 312c 2031 2c20 312c         (1, 1, 1,
-000057a0: 2031 2929 0a0a 2020 2020 2020 2020 6578   1))..        ex
-000057b0: 7065 7274 5f69 6e70 7574 203d 2073 656c  pert_input = sel
-000057c0: 662e 7265 7368 6170 6528 6578 7065 7274  f.reshape(expert
-000057d0: 5f69 6e70 7574 2c20 2873 656c 662e 6578  _input, (self.ex
-000057e0: 7065 7274 5f64 696d 202a 2073 656c 662e  pert_dim * self.
-000057f0: 6470 5f67 726f 7570 202a 2063 6170 6163  dp_group * capac
-00005800: 6974 792c 0a20 2020 2020 2020 2020 2020  ity,.           
+000054f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00005500: 2020 2020 2020 2020 2020 2028 312c 2031             (1, 1
+00005510: 2c20 312c 2031 2929 0a0a 2020 2020 2020  , 1, 1))..      
+00005520: 2020 6578 7065 7274 5f69 6e70 7574 203d    expert_input =
+00005530: 2073 656c 662e 7265 7368 6170 6528 6578   self.reshape(ex
+00005540: 7065 7274 5f69 6e70 7574 2c20 2873 656c  pert_input, (sel
+00005550: 662e 6578 7065 7274 5f64 696d 202a 2073  f.expert_dim * s
+00005560: 656c 662e 6470 5f67 726f 7570 202a 2063  elf.dp_group * c
+00005570: 6170 6163 6974 792c 0a20 2020 2020 2020  apacity,.       
+00005580: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00005590: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000055a0: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+000055b0: 2e68 6964 6465 6e5f 7369 7a65 2929 0a20  .hidden_size)). 
+000055c0: 2020 2020 2020 2023 2065 7870 6572 745f         # expert_
+000055d0: 6f75 7470 7574 2773 2073 6861 7065 3a20  output's shape: 
+000055e0: 2873 656c 662e 6578 7065 7274 5f64 696d  (self.expert_dim
+000055f0: 2c20 7365 6c66 2e64 705f 6772 6f75 702a  , self.dp_group*
+00005600: 6578 7065 7274 5f63 6170 6163 6974 792c  expert_capacity,
+00005610: 2073 656c 662e 6869 6464 656e 5f73 697a   self.hidden_siz
+00005620: 6529 0a20 2020 2020 2020 2065 7870 6572  e).        exper
+00005630: 745f 6f75 7470 7574 203d 2073 656c 662e  t_output = self.
+00005640: 6666 6e28 6578 7065 7274 5f69 6e70 7574  ffn(expert_input
+00005650: 290a 2020 2020 2020 2020 6578 7065 7274  ).        expert
+00005660: 5f6f 7574 7075 7420 3d20 7365 6c66 2e72  _output = self.r
+00005670: 6573 6861 7065 2865 7870 6572 745f 6f75  eshape(expert_ou
+00005680: 7470 7574 2c20 2873 656c 662e 6578 7065  tput, (self.expe
+00005690: 7274 5f64 696d 2c20 7365 6c66 2e64 705f  rt_dim, self.dp_
+000056a0: 6772 6f75 702c 0a20 2020 2020 2020 2020  group,.         
+000056b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000056c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000056d0: 2020 2020 2020 2020 2020 2020 6361 7061              capa
+000056e0: 6369 7479 2c20 7365 6c66 2e68 6964 6465  city, self.hidde
+000056f0: 6e5f 7369 7a65 2929 0a0a 2020 2020 2020  n_size))..      
+00005700: 2020 6966 2073 656c 662e 6772 6f75 705f    if self.group_
+00005710: 7769 7365 5f61 3261 3a0a 2020 2020 2020  wise_a2a:.      
+00005720: 2020 2020 2020 2320 6361 7061 6369 7479        # capacity
+00005730: 2073 6861 7264 2062 7920 6d70 0a20 2020   shard by mp.   
+00005740: 2020 2020 2020 2020 2065 7870 6572 745f           expert_
+00005750: 6f75 7470 7574 203d 2073 656c 662e 7374  output = self.st
+00005760: 7269 6465 5f73 6c69 6365 5f65 705f 6d70  ride_slice_ep_mp
+00005770: 2865 7870 6572 745f 6f75 7470 7574 2c20  (expert_output, 
+00005780: 2830 2c20 302c 2030 2c20 3029 2c0a 2020  (0, 0, 0, 0),.  
+00005790: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000057a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000057b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000057c0: 2020 2873 656c 662e 6578 7065 7274 5f64    (self.expert_d
+000057d0: 696d 2c20 7365 6c66 2e64 705f 6772 6f75  im, self.dp_grou
+000057e0: 702c 2063 6170 6163 6974 792c 2073 656c  p, capacity, sel
+000057f0: 662e 6869 6464 656e 5f73 697a 6529 2c0a  f.hidden_size),.
+00005800: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00005810: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00005820: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00005830: 2020 2020 2020 2020 7365 6c66 2e68 6964          self.hid
-00005840: 6465 6e5f 7369 7a65 2929 0a20 2020 2020  den_size)).     
-00005850: 2020 2023 2065 7870 6572 745f 6f75 7470     # expert_outp
-00005860: 7574 2773 2073 6861 7065 3a20 2873 656c  ut's shape: (sel
-00005870: 662e 6578 7065 7274 5f64 696d 2c20 7365  f.expert_dim, se
-00005880: 6c66 2e64 705f 6772 6f75 702a 6578 7065  lf.dp_group*expe
-00005890: 7274 5f63 6170 6163 6974 792c 2073 656c  rt_capacity, sel
-000058a0: 662e 6869 6464 656e 5f73 697a 6529 0a20  f.hidden_size). 
-000058b0: 2020 2020 2020 2065 7870 6572 745f 6f75         expert_ou
-000058c0: 7470 7574 203d 2073 656c 662e 6666 6e28  tput = self.ffn(
-000058d0: 6578 7065 7274 5f69 6e70 7574 290a 2020  expert_input).  
-000058e0: 2020 2020 2020 6578 7065 7274 5f6f 7574        expert_out
-000058f0: 7075 7420 3d20 7365 6c66 2e72 6573 6861  put = self.resha
-00005900: 7065 2865 7870 6572 745f 6f75 7470 7574  pe(expert_output
-00005910: 2c20 2873 656c 662e 6578 7065 7274 5f64  , (self.expert_d
-00005920: 696d 2c20 7365 6c66 2e64 705f 6772 6f75  im, self.dp_grou
-00005930: 702c 0a20 2020 2020 2020 2020 2020 2020  p,.             
+00005830: 2020 2020 2831 2c20 312c 2031 2c20 3129      (1, 1, 1, 1)
+00005840: 290a 2020 2020 2020 2020 2020 2020 2320  ).            # 
+00005850: 6772 6f75 702d 7769 7365 2061 6c6c 746f  group-wise allto
+00005860: 616c 6c0a 2020 2020 2020 2020 2020 2020  all.            
+00005870: 6578 7065 7274 5f6f 7574 7075 7420 3d20  expert_output = 
+00005880: 7365 6c66 2e73 7472 6964 655f 736c 6963  self.stride_slic
+00005890: 655f 6470 5f6d 7028 6578 7065 7274 5f6f  e_dp_mp(expert_o
+000058a0: 7574 7075 742c 2028 302c 2030 2c20 302c  utput, (0, 0, 0,
+000058b0: 2030 292c 0a20 2020 2020 2020 2020 2020   0),.           
+000058c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000058d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000058e0: 2020 2020 2020 2020 2028 7365 6c66 2e65           (self.e
+000058f0: 7870 6572 745f 6469 6d2c 2073 656c 662e  xpert_dim, self.
+00005900: 6470 5f67 726f 7570 2c20 6361 7061 6369  dp_group, capaci
+00005910: 7479 2c20 7365 6c66 2e68 6964 6465 6e5f  ty, self.hidden_
+00005920: 7369 7a65 292c 0a20 2020 2020 2020 2020  size),.         
+00005930: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00005940: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00005950: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00005960: 2020 2020 2020 2020 6361 7061 6369 7479          capacity
-00005970: 2c20 7365 6c66 2e68 6964 6465 6e5f 7369  , self.hidden_si
-00005980: 7a65 2929 0a0a 2020 2020 2020 2020 6966  ze))..        if
-00005990: 2073 656c 662e 6772 6f75 705f 7769 7365   self.group_wise
-000059a0: 5f61 3261 3a0a 2020 2020 2020 2020 2020  _a2a:.          
-000059b0: 2020 2320 6361 7061 6369 7479 2073 6861    # capacity sha
-000059c0: 7264 2062 7920 6d70 0a20 2020 2020 2020  rd by mp.       
-000059d0: 2020 2020 2065 7870 6572 745f 6f75 7470       expert_outp
-000059e0: 7574 203d 2073 656c 662e 7374 7269 6465  ut = self.stride
-000059f0: 5f73 6c69 6365 5f65 705f 6d70 2865 7870  _slice_ep_mp(exp
-00005a00: 6572 745f 6f75 7470 7574 2c20 2830 2c20  ert_output, (0, 
-00005a10: 302c 2030 2c20 3029 2c0a 2020 2020 2020  0, 0, 0),.      
-00005a20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00005a30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00005a40: 2020 2020 2020 2020 2020 2020 2020 2873                (s
-00005a50: 656c 662e 6578 7065 7274 5f64 696d 2c20  elf.expert_dim, 
-00005a60: 7365 6c66 2e64 705f 6772 6f75 702c 2063  self.dp_group, c
-00005a70: 6170 6163 6974 792c 2073 656c 662e 6869  apacity, self.hi
-00005a80: 6464 656e 5f73 697a 6529 2c0a 2020 2020  dden_size),.    
-00005a90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00005aa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00005ab0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00005ac0: 2831 2c20 312c 2031 2c20 3129 290a 2020  (1, 1, 1, 1)).  
-00005ad0: 2020 2020 2020 2020 2020 2320 6772 6f75            # grou
-00005ae0: 702d 7769 7365 2061 6c6c 746f 616c 6c0a  p-wise alltoall.
-00005af0: 2020 2020 2020 2020 2020 2020 6578 7065              expe
-00005b00: 7274 5f6f 7574 7075 7420 3d20 7365 6c66  rt_output = self
-00005b10: 2e73 7472 6964 655f 736c 6963 655f 6470  .stride_slice_dp
-00005b20: 5f6d 7028 6578 7065 7274 5f6f 7574 7075  _mp(expert_outpu
-00005b30: 742c 2028 302c 2030 2c20 302c 2030 292c  t, (0, 0, 0, 0),
-00005b40: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00005950: 2020 2020 2020 2020 2020 2028 312c 2031             (1, 1
+00005960: 2c20 312c 2031 2929 0a20 2020 2020 2020  , 1, 1)).       
+00005970: 2020 2020 2023 2061 6c6c 6761 7468 6572       # allgather
+00005980: 0a20 2020 2020 2020 2020 2020 2065 7870  .            exp
+00005990: 6572 745f 6f75 7470 7574 203d 2073 656c  ert_output = sel
+000059a0: 662e 7374 7269 6465 5f73 6c69 6365 5f64  f.stride_slice_d
+000059b0: 7028 6578 7065 7274 5f6f 7574 7075 742c  p(expert_output,
+000059c0: 2028 302c 2030 2c20 302c 2030 292c 0a20   (0, 0, 0, 0),. 
+000059d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000059e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000059f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00005a00: 2873 656c 662e 6578 7065 7274 5f64 696d  (self.expert_dim
+00005a10: 2c20 7365 6c66 2e64 705f 6772 6f75 702c  , self.dp_group,
+00005a20: 2063 6170 6163 6974 792c 2073 656c 662e   capacity, self.
+00005a30: 6869 6464 656e 5f73 697a 6529 2c0a 2020  hidden_size),.  
+00005a40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00005a50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00005a60: 2020 2020 2020 2020 2020 2020 2020 2028                 (
+00005a70: 312c 2031 2c20 312c 2031 2929 0a20 2020  1, 1, 1, 1)).   
+00005a80: 2020 2020 2020 2020 2023 2053 6c69 6365           # Slice
+00005a90: 2063 6170 6163 6974 7920 6261 636b 2074   capacity back t
+00005aa0: 6f20 6f72 6720 7368 6170 652e 0a20 2020  o org shape..   
+00005ab0: 2020 2020 2020 2020 2069 6620 7061 645f           if pad_
+00005ac0: 7369 7a65 2021 3d20 303a 0a20 2020 2020  size != 0:.     
+00005ad0: 2020 2020 2020 2020 2020 2063 6170 6163             capac
+00005ae0: 6974 7920 2d3d 2070 6164 5f73 697a 650a  ity -= pad_size.
+00005af0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00005b00: 6578 7065 7274 5f6f 7574 7075 7420 3d20  expert_output = 
+00005b10: 7365 6c66 2e73 7472 6964 655f 736c 6963  self.stride_slic
+00005b20: 655f 6470 2865 7870 6572 745f 6f75 7470  e_dp(expert_outp
+00005b30: 7574 2c20 2830 2c20 302c 2030 2c20 3029  ut, (0, 0, 0, 0)
+00005b40: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
 00005b50: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00005b60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00005b70: 2020 2020 2028 7365 6c66 2e65 7870 6572       (self.exper
-00005b80: 745f 6469 6d2c 2073 656c 662e 6470 5f67  t_dim, self.dp_g
-00005b90: 726f 7570 2c20 6361 7061 6369 7479 2c20  roup, capacity, 
-00005ba0: 7365 6c66 2e68 6964 6465 6e5f 7369 7a65  self.hidden_size
-00005bb0: 292c 0a20 2020 2020 2020 2020 2020 2020  ),.             
+00005b70: 2020 2020 2020 2028 7365 6c66 2e65 7870         (self.exp
+00005b80: 6572 745f 6469 6d2c 2073 656c 662e 6470  ert_dim, self.dp
+00005b90: 5f67 726f 7570 2c20 6361 7061 6369 7479  _group, capacity
+00005ba0: 2c20 7365 6c66 2e68 6964 6465 6e5f 7369  , self.hidden_si
+00005bb0: 7a65 292c 0a20 2020 2020 2020 2020 2020  ze),.           
 00005bc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00005bd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00005be0: 2020 2020 2020 2028 312c 2031 2c20 312c         (1, 1, 1,
-00005bf0: 2031 2929 0a20 2020 2020 2020 2020 2020   1)).           
-00005c00: 2023 2061 6c6c 6761 7468 6572 0a20 2020   # allgather.   
-00005c10: 2020 2020 2020 2020 2065 7870 6572 745f           expert_
-00005c20: 6f75 7470 7574 203d 2073 656c 662e 7374  output = self.st
-00005c30: 7269 6465 5f73 6c69 6365 5f64 7028 6578  ride_slice_dp(ex
-00005c40: 7065 7274 5f6f 7574 7075 742c 2028 302c  pert_output, (0,
-00005c50: 2030 2c20 302c 2030 292c 0a20 2020 2020   0, 0, 0),.     
-00005c60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00005c70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00005c80: 2020 2020 2020 2020 2020 2020 2873 656c              (sel
-00005c90: 662e 6578 7065 7274 5f64 696d 2c20 7365  f.expert_dim, se
-00005ca0: 6c66 2e64 705f 6772 6f75 702c 2063 6170  lf.dp_group, cap
-00005cb0: 6163 6974 792c 2073 656c 662e 6869 6464  acity, self.hidd
-00005cc0: 656e 5f73 697a 6529 2c0a 2020 2020 2020  en_size),.      
-00005cd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00005ce0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00005cf0: 2020 2020 2020 2020 2020 2028 312c 2031             (1, 1
-00005d00: 2c20 312c 2031 2929 0a20 2020 2020 2020  , 1, 1)).       
-00005d10: 2020 2020 2023 2053 6c69 6365 2063 6170       # Slice cap
-00005d20: 6163 6974 7920 6261 636b 2074 6f20 6f72  acity back to or
-00005d30: 6720 7368 6170 652e 0a20 2020 2020 2020  g shape..       
-00005d40: 2020 2020 2069 6620 7061 645f 7369 7a65       if pad_size
-00005d50: 2021 3d20 303a 0a20 2020 2020 2020 2020   != 0:.         
-00005d60: 2020 2020 2020 2063 6170 6163 6974 7920         capacity 
-00005d70: 2d3d 2070 6164 5f73 697a 650a 2020 2020  -= pad_size.    
-00005d80: 2020 2020 2020 2020 2020 2020 6578 7065              expe
-00005d90: 7274 5f6f 7574 7075 7420 3d20 7365 6c66  rt_output = self
-00005da0: 2e73 7472 6964 655f 736c 6963 655f 6470  .stride_slice_dp
-00005db0: 2865 7870 6572 745f 6f75 7470 7574 2c20  (expert_output, 
-00005dc0: 2830 2c20 302c 2030 2c20 3029 2c0a 2020  (0, 0, 0, 0),.  
-00005dd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00005de0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00005df0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00005e00: 2020 2028 7365 6c66 2e65 7870 6572 745f     (self.expert_
-00005e10: 6469 6d2c 2073 656c 662e 6470 5f67 726f  dim, self.dp_gro
-00005e20: 7570 2c20 6361 7061 6369 7479 2c20 7365  up, capacity, se
-00005e30: 6c66 2e68 6964 6465 6e5f 7369 7a65 292c  lf.hidden_size),
-00005e40: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00005e50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00005e60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00005e70: 2020 2020 2020 2831 2c20 312c 2031 2c20        (1, 1, 1, 
-00005e80: 3129 290a 2020 2020 2020 2020 6966 2073  1)).        if s
-00005e90: 656c 662e 656e 6162 6c65 5f63 6f6c 645f  elf.enable_cold_
-00005ea0: 686f 745f 6578 7065 7274 3a0a 2020 2020  hot_expert:.    
-00005eb0: 2020 2020 2020 2020 2320 6578 7065 7274          # expert
-00005ec0: 5f6f 7574 7075 7427 7320 7368 6170 653a  _output's shape:
-00005ed0: 2028 7365 6c66 2e64 705f 6772 6f75 702c   (self.dp_group,
-00005ee0: 2073 656c 662e 6578 7065 7274 5f64 696d   self.expert_dim
-00005ef0: 2c20 6578 7065 7274 5f63 6170 6163 6974  , expert_capacit
-00005f00: 792c 2073 656c 662e 6869 6464 656e 5f73  y, self.hidden_s
-00005f10: 697a 6529 0a20 2020 2020 2020 2020 2020  ize).           
-00005f20: 2065 7870 6572 745f 6f75 7470 7574 203d   expert_output =
-00005f30: 2073 656c 662e 7472 616e 7370 6f73 655f   self.transpose_
-00005f40: 3464 696d 2865 7870 6572 745f 6f75 7470  4dim(expert_outp
-00005f50: 7574 2c20 2831 2c20 302c 2032 2c20 3329  ut, (1, 0, 2, 3)
-00005f60: 290a 2020 2020 2020 2020 656c 7365 3a0a  ).        else:.
-00005f70: 2020 2020 2020 2020 2020 2020 2320 6578              # ex
-00005f80: 7065 7274 5f6f 7574 7075 7427 7320 7368  pert_output's sh
-00005f90: 6170 653a 2028 7365 6c66 2e64 705f 6772  ape: (self.dp_gr
-00005fa0: 6f75 702c 2073 656c 662e 6869 6464 656e  oup, self.hidden
-00005fb0: 5f73 697a 652c 2073 656c 662e 6578 7065  _size, self.expe
-00005fc0: 7274 5f64 696d 2c20 6578 7065 7274 5f63  rt_dim, expert_c
-00005fd0: 6170 6163 6974 7929 0a20 2020 2020 2020  apacity).       
-00005fe0: 2020 2020 2065 7870 6572 745f 6f75 7470       expert_outp
-00005ff0: 7574 203d 2073 656c 662e 7472 616e 7370  ut = self.transp
-00006000: 6f73 655f 3464 696d 2865 7870 6572 745f  ose_4dim(expert_
-00006010: 6f75 7470 7574 2c20 2831 2c20 332c 2030  output, (1, 3, 0
-00006020: 2c20 3229 290a 2020 2020 2020 2020 7265  , 2)).        re
-00006030: 7475 726e 2065 7870 6572 745f 6f75 7470  turn expert_outp
-00006040: 7574 0a0a 2020 2020 6465 6620 6666 6e5f  ut..    def ffn_
-00006050: 7061 7261 6c6c 656c 5f69 6e66 6572 2873  parallel_infer(s
-00006060: 656c 662c 2065 7870 6572 745f 696e 7075  elf, expert_inpu
-00006070: 742c 2063 6170 6163 6974 7929 3a0a 2020  t, capacity):.  
-00006080: 2020 2020 2020 2222 220a 2020 2020 2020        """.      
-00006090: 2020 5370 6c69 7420 616e 6420 6f76 6572    Split and over
-000060a0: 6c61 7020 4646 4e20 636f 6d70 7574 6520  lap FFN compute 
-000060b0: 616e 6420 636f 6d6d 756e 6963 6174 696f  and communicatio
-000060c0: 6e2e 0a20 2020 2020 2020 2022 2222 0a20  n..        """. 
-000060d0: 2020 2020 2020 2023 2050 6164 2063 6170         # Pad cap
-000060e0: 6163 6974 7920 666f 7220 636f 6d70 5f63  acity for comp_c
-000060f0: 6f6d 6d5f 7061 7261 6c6c 656c 5f64 6567  omm_parallel_deg
-00006100: 7265 6520 7370 6c69 742e 0a20 2020 2020  ree split..     
-00006110: 2020 2070 6164 5f73 697a 6520 3d20 300a     pad_size = 0.
-00006120: 2020 2020 2020 2020 6966 2063 6170 6163          if capac
-00006130: 6974 7920 2520 7365 6c66 2e63 6f6d 705f  ity % self.comp_
-00006140: 636f 6d6d 5f70 6172 616c 6c65 6c5f 6465  comm_parallel_de
-00006150: 6772 6565 2021 3d20 303a 0a20 2020 2020  gree != 0:.     
-00006160: 2020 2020 2020 2070 6164 5f73 697a 6520         pad_size 
-00006170: 3d20 7365 6c66 2e63 6f6d 705f 636f 6d6d  = self.comp_comm
-00006180: 5f70 6172 616c 6c65 6c5f 6465 6772 6565  _parallel_degree
-00006190: 202d 2028 6361 7061 6369 7479 2025 2073   - (capacity % s
-000061a0: 656c 662e 636f 6d70 5f63 6f6d 6d5f 7061  elf.comp_comm_pa
-000061b0: 7261 6c6c 656c 5f64 6567 7265 6529 0a20  rallel_degree). 
-000061c0: 2020 2020 2020 2020 2020 2063 6170 6163             capac
-000061d0: 6974 7920 2b3d 2070 6164 5f73 697a 650a  ity += pad_size.
-000061e0: 2020 2020 2020 2020 2020 2020 7061 645f              pad_
-000061f0: 7465 6e73 6f72 203d 2073 656c 662e 7374  tensor = self.st
-00006200: 7269 6465 5f73 6c69 6365 5f64 7028 6578  ride_slice_dp(ex
-00006210: 7065 7274 5f69 6e70 7574 2c20 2830 2c20  pert_input, (0, 
-00006220: 302c 2030 2c20 3029 2c0a 2020 2020 2020  0, 0, 0),.      
-00006230: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00006240: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00006250: 2020 2020 2020 2020 2873 656c 662e 6578          (self.ex
-00006260: 7065 7274 5f64 696d 2c20 7365 6c66 2e64  pert_dim, self.d
-00006270: 705f 6772 6f75 702c 2070 6164 5f73 697a  p_group, pad_siz
-00006280: 652c 2073 656c 662e 6869 6464 656e 5f73  e, self.hidden_s
-00006290: 697a 6529 2c0a 2020 2020 2020 2020 2020  ize),.          
+00005be0: 2020 2020 2020 2020 2020 2831 2c20 312c            (1, 1,
+00005bf0: 2031 2c20 3129 290a 2020 2020 2020 2020   1, 1)).        
+00005c00: 6966 2073 656c 662e 656e 6162 6c65 5f63  if self.enable_c
+00005c10: 6f6c 645f 686f 745f 6578 7065 7274 3a0a  old_hot_expert:.
+00005c20: 2020 2020 2020 2020 2020 2020 2320 6578              # ex
+00005c30: 7065 7274 5f6f 7574 7075 7427 7320 7368  pert_output's sh
+00005c40: 6170 653a 2028 7365 6c66 2e64 705f 6772  ape: (self.dp_gr
+00005c50: 6f75 702c 2073 656c 662e 6578 7065 7274  oup, self.expert
+00005c60: 5f64 696d 2c20 6578 7065 7274 5f63 6170  _dim, expert_cap
+00005c70: 6163 6974 792c 2073 656c 662e 6869 6464  acity, self.hidd
+00005c80: 656e 5f73 697a 6529 0a20 2020 2020 2020  en_size).       
+00005c90: 2020 2020 2065 7870 6572 745f 6f75 7470       expert_outp
+00005ca0: 7574 203d 2073 656c 662e 7472 616e 7370  ut = self.transp
+00005cb0: 6f73 655f 3464 696d 2865 7870 6572 745f  ose_4dim(expert_
+00005cc0: 6f75 7470 7574 2c20 2831 2c20 302c 2032  output, (1, 0, 2
+00005cd0: 2c20 3329 290a 2020 2020 2020 2020 656c  , 3)).        el
+00005ce0: 7365 3a0a 2020 2020 2020 2020 2020 2020  se:.            
+00005cf0: 2320 6578 7065 7274 5f6f 7574 7075 7427  # expert_output'
+00005d00: 7320 7368 6170 653a 2028 7365 6c66 2e64  s shape: (self.d
+00005d10: 705f 6772 6f75 702c 2073 656c 662e 6869  p_group, self.hi
+00005d20: 6464 656e 5f73 697a 652c 2073 656c 662e  dden_size, self.
+00005d30: 6578 7065 7274 5f64 696d 2c20 6578 7065  expert_dim, expe
+00005d40: 7274 5f63 6170 6163 6974 7929 0a20 2020  rt_capacity).   
+00005d50: 2020 2020 2020 2020 2065 7870 6572 745f           expert_
+00005d60: 6f75 7470 7574 203d 2073 656c 662e 7472  output = self.tr
+00005d70: 616e 7370 6f73 655f 3464 696d 2865 7870  anspose_4dim(exp
+00005d80: 6572 745f 6f75 7470 7574 2c20 2831 2c20  ert_output, (1, 
+00005d90: 332c 2030 2c20 3229 290a 2020 2020 2020  3, 0, 2)).      
+00005da0: 2020 7265 7475 726e 2065 7870 6572 745f    return expert_
+00005db0: 6f75 7470 7574 0a0a 2020 2020 6465 6620  output..    def 
+00005dc0: 6666 6e5f 7061 7261 6c6c 656c 5f69 6e66  ffn_parallel_inf
+00005dd0: 6572 2873 656c 662c 2065 7870 6572 745f  er(self, expert_
+00005de0: 696e 7075 742c 2063 6170 6163 6974 7929  input, capacity)
+00005df0: 3a0a 2020 2020 2020 2020 2222 220a 2020  :.        """.  
+00005e00: 2020 2020 2020 5370 6c69 7420 616e 6420        Split and 
+00005e10: 6f76 6572 6c61 7020 4646 4e20 636f 6d70  overlap FFN comp
+00005e20: 7574 6520 616e 6420 636f 6d6d 756e 6963  ute and communic
+00005e30: 6174 696f 6e2e 0a20 2020 2020 2020 2022  ation..        "
+00005e40: 2222 0a20 2020 2020 2020 2023 2050 6164  "".        # Pad
+00005e50: 2063 6170 6163 6974 7920 666f 7220 636f   capacity for co
+00005e60: 6d70 5f63 6f6d 6d5f 7061 7261 6c6c 656c  mp_comm_parallel
+00005e70: 5f64 6567 7265 6520 7370 6c69 742e 0a20  _degree split.. 
+00005e80: 2020 2020 2020 2070 6164 5f73 697a 6520         pad_size 
+00005e90: 3d20 300a 2020 2020 2020 2020 6966 2063  = 0.        if c
+00005ea0: 6170 6163 6974 7920 2520 7365 6c66 2e63  apacity % self.c
+00005eb0: 6f6d 705f 636f 6d6d 5f70 6172 616c 6c65  omp_comm_paralle
+00005ec0: 6c5f 6465 6772 6565 2021 3d20 303a 0a20  l_degree != 0:. 
+00005ed0: 2020 2020 2020 2020 2020 2070 6164 5f73             pad_s
+00005ee0: 697a 6520 3d20 7365 6c66 2e63 6f6d 705f  ize = self.comp_
+00005ef0: 636f 6d6d 5f70 6172 616c 6c65 6c5f 6465  comm_parallel_de
+00005f00: 6772 6565 202d 2028 6361 7061 6369 7479  gree - (capacity
+00005f10: 2025 2073 656c 662e 636f 6d70 5f63 6f6d   % self.comp_com
+00005f20: 6d5f 7061 7261 6c6c 656c 5f64 6567 7265  m_parallel_degre
+00005f30: 6529 0a20 2020 2020 2020 2020 2020 2063  e).            c
+00005f40: 6170 6163 6974 7920 2b3d 2070 6164 5f73  apacity += pad_s
+00005f50: 697a 650a 2020 2020 2020 2020 2020 2020  ize.            
+00005f60: 7061 645f 7465 6e73 6f72 203d 2073 656c  pad_tensor = sel
+00005f70: 662e 7374 7269 6465 5f73 6c69 6365 5f64  f.stride_slice_d
+00005f80: 7028 6578 7065 7274 5f69 6e70 7574 2c20  p(expert_input, 
+00005f90: 2830 2c20 302c 2030 2c20 3029 2c0a 2020  (0, 0, 0, 0),.  
+00005fa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00005fb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00005fc0: 2020 2020 2020 2020 2020 2020 2873 656c              (sel
+00005fd0: 662e 6578 7065 7274 5f64 696d 2c20 7365  f.expert_dim, se
+00005fe0: 6c66 2e64 705f 6772 6f75 702c 2070 6164  lf.dp_group, pad
+00005ff0: 5f73 697a 652c 2073 656c 662e 6869 6464  _size, self.hidd
+00006000: 656e 5f73 697a 6529 2c0a 2020 2020 2020  en_size),.      
+00006010: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00006020: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00006030: 2020 2020 2020 2020 2831 2c20 312c 2031          (1, 1, 1
+00006040: 2c20 3129 290a 2020 2020 2020 2020 2020  , 1)).          
+00006050: 2020 6578 7065 7274 5f69 6e70 7574 203d    expert_input =
+00006060: 2073 656c 662e 636f 6e63 6174 5f64 7028   self.concat_dp(
+00006070: 2865 7870 6572 745f 696e 7075 742c 2070  (expert_input, p
+00006080: 6164 5f74 656e 736f 7229 290a 0a20 2020  ad_tensor))..   
+00006090: 2020 2020 2073 7562 5f63 6170 6163 6974       sub_capacit
+000060a0: 7920 3d20 6361 7061 6369 7479 202f 2f20  y = capacity // 
+000060b0: 7365 6c66 2e63 6f6d 705f 636f 6d6d 5f70  self.comp_comm_p
+000060c0: 6172 616c 6c65 6c5f 6465 6772 6565 0a20  arallel_degree. 
+000060d0: 2020 2020 2020 206f 7574 7075 745f 6c69         output_li
+000060e0: 7374 203d 205b 5d0a 2020 2020 2020 2020  st = [].        
+000060f0: 666f 7220 7375 625f 6578 7065 7274 5f69  for sub_expert_i
+00006100: 6e70 7574 2069 6e20 7365 6c66 2e73 706c  nput in self.spl
+00006110: 6974 2865 7870 6572 745f 696e 7075 7429  it(expert_input)
+00006120: 3a0a 2020 2020 2020 2020 2020 2020 7375  :.            su
+00006130: 625f 6578 7065 7274 5f6f 7574 7075 7420  b_expert_output 
+00006140: 3d20 7365 6c66 2e66 666e 5f69 6e66 6572  = self.ffn_infer
+00006150: 2873 7562 5f65 7870 6572 745f 696e 7075  (sub_expert_inpu
+00006160: 742c 2073 7562 5f63 6170 6163 6974 7929  t, sub_capacity)
+00006170: 0a20 2020 2020 2020 2020 2020 206f 7574  .            out
+00006180: 7075 745f 6c69 7374 2e61 7070 656e 6428  put_list.append(
+00006190: 7375 625f 6578 7065 7274 5f6f 7574 7075  sub_expert_outpu
+000061a0: 7429 0a20 2020 2020 2020 2065 7870 6572  t).        exper
+000061b0: 745f 6f75 7470 7574 203d 2073 656c 662e  t_output = self.
+000061c0: 636f 6e63 6174 286f 7574 7075 745f 6c69  concat(output_li
+000061d0: 7374 290a 0a20 2020 2020 2020 2023 2053  st)..        # S
+000061e0: 6c69 6365 2063 6170 6163 6974 7920 6261  lice capacity ba
+000061f0: 636b 2074 6f20 6f72 6720 7368 6170 652e  ck to org shape.
+00006200: 0a20 2020 2020 2020 2069 6620 7061 645f  .        if pad_
+00006210: 7369 7a65 2021 3d20 303a 0a20 2020 2020  size != 0:.     
+00006220: 2020 2020 2020 2063 6170 6163 6974 7920         capacity 
+00006230: 2d3d 2070 6164 5f73 697a 650a 2020 2020  -= pad_size.    
+00006240: 2020 2020 2020 2020 6578 7065 7274 5f6f          expert_o
+00006250: 7574 7075 7420 3d20 7365 6c66 2e73 7472  utput = self.str
+00006260: 6964 655f 736c 6963 6528 6578 7065 7274  ide_slice(expert
+00006270: 5f6f 7574 7075 742c 2028 302c 2030 2c20  _output, (0, 0, 
+00006280: 302c 2030 292c 0a20 2020 2020 2020 2020  0, 0),.         
+00006290: 2020 2020 2020 2020 2020 2020 2020 2020                  
 000062a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000062b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000062c0: 2020 2020 2831 2c20 312c 2031 2c20 3129      (1, 1, 1, 1)
-000062d0: 290a 2020 2020 2020 2020 2020 2020 6578  ).            ex
-000062e0: 7065 7274 5f69 6e70 7574 203d 2073 656c  pert_input = sel
-000062f0: 662e 636f 6e63 6174 5f64 7028 2865 7870  f.concat_dp((exp
-00006300: 6572 745f 696e 7075 742c 2070 6164 5f74  ert_input, pad_t
-00006310: 656e 736f 7229 290a 0a20 2020 2020 2020  ensor))..       
-00006320: 2073 7562 5f63 6170 6163 6974 7920 3d20   sub_capacity = 
-00006330: 6361 7061 6369 7479 202f 2f20 7365 6c66  capacity // self
-00006340: 2e63 6f6d 705f 636f 6d6d 5f70 6172 616c  .comp_comm_paral
-00006350: 6c65 6c5f 6465 6772 6565 0a20 2020 2020  lel_degree.     
-00006360: 2020 206f 7574 7075 745f 6c69 7374 203d     output_list =
-00006370: 205b 5d0a 2020 2020 2020 2020 666f 7220   [].        for 
-00006380: 7375 625f 6578 7065 7274 5f69 6e70 7574  sub_expert_input
-00006390: 2069 6e20 7365 6c66 2e73 706c 6974 2865   in self.split(e
-000063a0: 7870 6572 745f 696e 7075 7429 3a0a 2020  xpert_input):.  
-000063b0: 2020 2020 2020 2020 2020 7375 625f 6578            sub_ex
-000063c0: 7065 7274 5f6f 7574 7075 7420 3d20 7365  pert_output = se
-000063d0: 6c66 2e66 666e 5f69 6e66 6572 2873 7562  lf.ffn_infer(sub
-000063e0: 5f65 7870 6572 745f 696e 7075 742c 2073  _expert_input, s
-000063f0: 7562 5f63 6170 6163 6974 7929 0a20 2020  ub_capacity).   
-00006400: 2020 2020 2020 2020 206f 7574 7075 745f           output_
-00006410: 6c69 7374 2e61 7070 656e 6428 7375 625f  list.append(sub_
-00006420: 6578 7065 7274 5f6f 7574 7075 7429 0a20  expert_output). 
-00006430: 2020 2020 2020 2065 7870 6572 745f 6f75         expert_ou
-00006440: 7470 7574 203d 2073 656c 662e 636f 6e63  tput = self.conc
-00006450: 6174 286f 7574 7075 745f 6c69 7374 290a  at(output_list).
-00006460: 0a20 2020 2020 2020 2023 2053 6c69 6365  .        # Slice
-00006470: 2063 6170 6163 6974 7920 6261 636b 2074   capacity back t
-00006480: 6f20 6f72 6720 7368 6170 652e 0a20 2020  o org shape..   
-00006490: 2020 2020 2069 6620 7061 645f 7369 7a65       if pad_size
-000064a0: 2021 3d20 303a 0a20 2020 2020 2020 2020   != 0:.         
-000064b0: 2020 2063 6170 6163 6974 7920 2d3d 2070     capacity -= p
-000064c0: 6164 5f73 697a 650a 2020 2020 2020 2020  ad_size.        
-000064d0: 2020 2020 6578 7065 7274 5f6f 7574 7075      expert_outpu
-000064e0: 7420 3d20 7365 6c66 2e73 7472 6964 655f  t = self.stride_
-000064f0: 736c 6963 6528 6578 7065 7274 5f6f 7574  slice(expert_out
-00006500: 7075 742c 2028 302c 2030 2c20 302c 2030  put, (0, 0, 0, 0
-00006510: 292c 0a20 2020 2020 2020 2020 2020 2020  ),.             
-00006520: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00006530: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00006540: 2028 7365 6c66 2e64 705f 6772 6f75 702c   (self.dp_group,
-00006550: 2073 656c 662e 6869 6464 656e 5f73 697a   self.hidden_siz
-00006560: 652c 2073 656c 662e 6578 7065 7274 5f64  e, self.expert_d
-00006570: 696d 2c20 6361 7061 6369 7479 292c 0a20  im, capacity),. 
-00006580: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00006590: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000065a0: 2020 2020 2020 2020 2020 2020 2028 312c               (1,
-000065b0: 2031 2c20 312c 2031 2929 0a20 2020 2020   1, 1, 1)).     
-000065c0: 2020 2072 6574 7572 6e20 6578 7065 7274     return expert
-000065d0: 5f6f 7574 7075 740a 0a20 2020 2064 6566  _output..    def
-000065e0: 2063 6f6e 7374 7275 6374 2873 656c 662c   construct(self,
-000065f0: 2069 6e70 7574 5f74 656e 736f 7229 3a0a   input_tensor):.
-00006600: 2020 2020 2020 2020 2222 2266 6f72 7761          """forwa
-00006610: 7264 2070 726f 6365 7373 2222 220a 2020  rd process""".  
-00006620: 2020 2020 2020 696e 7075 745f 7368 6170        input_shap
-00006630: 6520 3d20 462e 7368 6170 6528 696e 7075  e = F.shape(inpu
-00006640: 745f 7465 6e73 6f72 290a 2020 2020 2020  t_tensor).      
-00006650: 2020 696e 7075 745f 7465 6e73 6f72 203d    input_tensor =
-00006660: 2073 656c 662e 7265 7368 6170 6528 696e   self.reshape(in
-00006670: 7075 745f 7465 6e73 6f72 2c20 282d 312c  put_tensor, (-1,
-00006680: 2073 656c 662e 6869 6464 656e 5f73 697a   self.hidden_siz
-00006690: 6529 290a 2020 2020 2020 2020 6273 5f61  e)).        bs_a
-000066a0: 6e64 5f64 6d6f 6465 6c20 3d20 7365 6c66  nd_dmodel = self
-000066b0: 2e73 6861 7065 2869 6e70 7574 5f74 656e  .shape(input_ten
-000066c0: 736f 7229 0a20 2020 2020 2020 2074 6f6b  sor).        tok
-000066d0: 656e 735f 7065 725f 6772 6f75 7020 3d20  ens_per_group = 
-000066e0: 6273 5f61 6e64 5f64 6d6f 6465 6c5b 305d  bs_and_dmodel[0]
-000066f0: 202f 2f20 7365 6c66 2e64 705f 6772 6f75   // self.dp_grou
-00006700: 700a 2020 2020 2020 2020 696e 7075 745f  p.        input_
-00006710: 7465 6e73 6f72 203d 2073 656c 662e 7265  tensor = self.re
-00006720: 7368 6170 6528 696e 7075 745f 7465 6e73  shape(input_tens
-00006730: 6f72 2c20 2873 656c 662e 6470 5f67 726f  or, (self.dp_gro
-00006740: 7570 2c20 746f 6b65 6e73 5f70 6572 5f67  up, tokens_per_g
-00006750: 726f 7570 2c20 7365 6c66 2e68 6964 6465  roup, self.hidde
-00006760: 6e5f 7369 7a65 2929 0a0a 2020 2020 2020  n_size))..      
-00006770: 2020 6578 7065 7274 5f63 6170 6163 6974    expert_capacit
-00006780: 7920 3d20 6361 6c63 756c 6174 655f 6578  y = calculate_ex
-00006790: 7065 7274 5f63 6170 6163 6974 7928 7365  pert_capacity(se
-000067a0: 6c66 2e6e 756d 5f65 7870 6572 7473 5f63  lf.num_experts_c
-000067b0: 686f 7365 6e2c 2074 6f6b 656e 735f 7065  hosen, tokens_pe
-000067c0: 725f 6772 6f75 702c 0a20 2020 2020 2020  r_group,.       
+000062b0: 2020 2020 2028 7365 6c66 2e64 705f 6772       (self.dp_gr
+000062c0: 6f75 702c 2073 656c 662e 6869 6464 656e  oup, self.hidden
+000062d0: 5f73 697a 652c 2073 656c 662e 6578 7065  _size, self.expe
+000062e0: 7274 5f64 696d 2c20 6361 7061 6369 7479  rt_dim, capacity
+000062f0: 292c 0a20 2020 2020 2020 2020 2020 2020  ),.             
+00006300: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00006310: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00006320: 2028 312c 2031 2c20 312c 2031 2929 0a20   (1, 1, 1, 1)). 
+00006330: 2020 2020 2020 2072 6574 7572 6e20 6578         return ex
+00006340: 7065 7274 5f6f 7574 7075 740a 0a20 2020  pert_output..   
+00006350: 2064 6566 2063 6f6e 7374 7275 6374 2873   def construct(s
+00006360: 656c 662c 2069 6e70 7574 5f74 656e 736f  elf, input_tenso
+00006370: 7229 3a0a 2020 2020 2020 2020 2222 2266  r):.        """f
+00006380: 6f72 7761 7264 2070 726f 6365 7373 2222  orward process""
+00006390: 220a 2020 2020 2020 2020 696e 7075 745f  ".        input_
+000063a0: 7368 6170 6520 3d20 462e 7368 6170 6528  shape = F.shape(
+000063b0: 696e 7075 745f 7465 6e73 6f72 290a 2020  input_tensor).  
+000063c0: 2020 2020 2020 696e 7075 745f 7465 6e73        input_tens
+000063d0: 6f72 203d 2073 656c 662e 7265 7368 6170  or = self.reshap
+000063e0: 6528 696e 7075 745f 7465 6e73 6f72 2c20  e(input_tensor, 
+000063f0: 282d 312c 2073 656c 662e 6869 6464 656e  (-1, self.hidden
+00006400: 5f73 697a 6529 290a 2020 2020 2020 2020  _size)).        
+00006410: 6273 5f61 6e64 5f64 6d6f 6465 6c20 3d20  bs_and_dmodel = 
+00006420: 7365 6c66 2e73 6861 7065 2869 6e70 7574  self.shape(input
+00006430: 5f74 656e 736f 7229 0a20 2020 2020 2020  _tensor).       
+00006440: 2074 6f6b 656e 735f 7065 725f 6772 6f75   tokens_per_grou
+00006450: 7020 3d20 6273 5f61 6e64 5f64 6d6f 6465  p = bs_and_dmode
+00006460: 6c5b 305d 202f 2f20 7365 6c66 2e64 705f  l[0] // self.dp_
+00006470: 6772 6f75 700a 2020 2020 2020 2020 696e  group.        in
+00006480: 7075 745f 7465 6e73 6f72 203d 2073 656c  put_tensor = sel
+00006490: 662e 7265 7368 6170 6528 696e 7075 745f  f.reshape(input_
+000064a0: 7465 6e73 6f72 2c20 2873 656c 662e 6470  tensor, (self.dp
+000064b0: 5f67 726f 7570 2c20 746f 6b65 6e73 5f70  _group, tokens_p
+000064c0: 6572 5f67 726f 7570 2c20 7365 6c66 2e68  er_group, self.h
+000064d0: 6964 6465 6e5f 7369 7a65 2929 0a0a 2020  idden_size))..  
+000064e0: 2020 2020 2020 6578 7065 7274 5f63 6170        expert_cap
+000064f0: 6163 6974 7920 3d20 6361 6c63 756c 6174  acity = calculat
+00006500: 655f 6578 7065 7274 5f63 6170 6163 6974  e_expert_capacit
+00006510: 7928 7365 6c66 2e6e 756d 5f65 7870 6572  y(self.num_exper
+00006520: 7473 5f63 686f 7365 6e2c 2074 6f6b 656e  ts_chosen, token
+00006530: 735f 7065 725f 6772 6f75 702c 0a20 2020  s_per_group,.   
+00006540: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00006550: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00006560: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00006570: 2073 656c 662e 6361 7061 6369 7479 5f66   self.capacity_f
+00006580: 6163 746f 722c 2073 656c 662e 6578 7065  actor, self.expe
+00006590: 7274 5f64 696d 290a 2020 2020 2020 2020  rt_dim).        
+000065a0: 2320 6469 7370 6174 6368 5f74 656e 736f  # dispatch_tenso
+000065b0: 7227 7320 7368 6170 653a 2028 7365 6c66  r's shape: (self
+000065c0: 2e64 705f 6772 6f75 702c 2074 6f6b 656e  .dp_group, token
+000065d0: 735f 7065 725f 6772 6f75 702c 2073 656c  s_per_group, sel
+000065e0: 662e 6578 7065 7274 5f64 696d 2c20 6578  f.expert_dim, ex
+000065f0: 7065 7274 5f63 6170 6163 6974 7929 0a20  pert_capacity). 
+00006600: 2020 2020 2020 2023 2063 6f6d 6269 6e65         # combine
+00006610: 5f74 656e 736f 7227 7320 7368 6170 653a  _tensor's shape:
+00006620: 2028 7365 6c66 2e64 705f 6772 6f75 702c   (self.dp_group,
+00006630: 2074 6f6b 656e 735f 7065 725f 6772 6f75   tokens_per_grou
+00006640: 702c 2073 656c 662e 6578 7065 7274 5f64  p, self.expert_d
+00006650: 696d 2c20 6578 7065 7274 5f63 6170 6163  im, expert_capac
+00006660: 6974 7929 0a20 2020 2020 2020 2064 6973  ity).        dis
+00006670: 7061 7463 685f 7465 6e73 6f72 2c20 636f  patch_tensor, co
+00006680: 6d62 696e 655f 7465 6e73 6f72 2c20 6175  mbine_tensor, au
+00006690: 785f 6c6f 7373 203d 2073 656c 662e 726f  x_loss = self.ro
+000066a0: 7574 6572 2869 6e70 7574 5f74 656e 736f  uter(input_tenso
+000066b0: 7229 0a0a 2020 2020 2020 2020 2320 6166  r)..        # af
+000066c0: 7465 7220 7472 616e 7370 6f73 652c 2069  ter transpose, i
+000066d0: 6e70 7574 5f74 656e 736f 7227 7320 7368  nput_tensor's sh
+000066e0: 6170 653a 2028 7365 6c66 2e64 705f 6772  ape: (self.dp_gr
+000066f0: 6f75 702c 2073 656c 662e 6869 6464 656e  oup, self.hidden
+00006700: 5f73 697a 652c 2074 6f6b 656e 735f 7065  _size, tokens_pe
+00006710: 725f 6772 6f75 7029 0a20 2020 2020 2020  r_group).       
+00006720: 2069 6e70 7574 5f74 656e 736f 7220 3d20   input_tensor = 
+00006730: 7365 6c66 2e74 7261 6e73 706f 7365 5f33  self.transpose_3
+00006740: 6469 6d28 696e 7075 745f 7465 6e73 6f72  dim(input_tensor
+00006750: 2c20 2830 2c20 322c 2031 2929 0a20 2020  , (0, 2, 1)).   
+00006760: 2020 2020 2064 6973 7061 7463 685f 7465       dispatch_te
+00006770: 6e73 6f72 203d 2073 656c 662e 7265 7368  nsor = self.resh
+00006780: 6170 6528 6469 7370 6174 6368 5f74 656e  ape(dispatch_ten
+00006790: 736f 722c 2028 7365 6c66 2e64 705f 6772  sor, (self.dp_gr
+000067a0: 6f75 702c 2074 6f6b 656e 735f 7065 725f  oup, tokens_per_
+000067b0: 6772 6f75 702c 0a20 2020 2020 2020 2020  group,.         
+000067c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 000067d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 000067e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000067f0: 2020 2020 2020 2020 2020 2020 2073 656c               sel
-00006800: 662e 6361 7061 6369 7479 5f66 6163 746f  f.capacity_facto
-00006810: 722c 2073 656c 662e 6578 7065 7274 5f64  r, self.expert_d
-00006820: 696d 290a 2020 2020 2020 2020 2320 6469  im).        # di
-00006830: 7370 6174 6368 5f74 656e 736f 7227 7320  spatch_tensor's 
-00006840: 7368 6170 653a 2028 7365 6c66 2e64 705f  shape: (self.dp_
-00006850: 6772 6f75 702c 2074 6f6b 656e 735f 7065  group, tokens_pe
-00006860: 725f 6772 6f75 702c 2073 656c 662e 6578  r_group, self.ex
-00006870: 7065 7274 5f64 696d 2c20 6578 7065 7274  pert_dim, expert
-00006880: 5f63 6170 6163 6974 7929 0a20 2020 2020  _capacity).     
-00006890: 2020 2023 2063 6f6d 6269 6e65 5f74 656e     # combine_ten
-000068a0: 736f 7227 7320 7368 6170 653a 2028 7365  sor's shape: (se
-000068b0: 6c66 2e64 705f 6772 6f75 702c 2074 6f6b  lf.dp_group, tok
-000068c0: 656e 735f 7065 725f 6772 6f75 702c 2073  ens_per_group, s
-000068d0: 656c 662e 6578 7065 7274 5f64 696d 2c20  elf.expert_dim, 
-000068e0: 6578 7065 7274 5f63 6170 6163 6974 7929  expert_capacity)
-000068f0: 0a20 2020 2020 2020 2064 6973 7061 7463  .        dispatc
-00006900: 685f 7465 6e73 6f72 2c20 636f 6d62 696e  h_tensor, combin
-00006910: 655f 7465 6e73 6f72 2c20 6175 785f 6c6f  e_tensor, aux_lo
-00006920: 7373 203d 2073 656c 662e 726f 7574 6572  ss = self.router
-00006930: 2869 6e70 7574 5f74 656e 736f 7229 0a0a  (input_tensor)..
-00006940: 2020 2020 2020 2020 2320 6166 7465 7220          # after 
-00006950: 7472 616e 7370 6f73 652c 2069 6e70 7574  transpose, input
-00006960: 5f74 656e 736f 7227 7320 7368 6170 653a  _tensor's shape:
-00006970: 2028 7365 6c66 2e64 705f 6772 6f75 702c   (self.dp_group,
-00006980: 2073 656c 662e 6869 6464 656e 5f73 697a   self.hidden_siz
-00006990: 652c 2074 6f6b 656e 735f 7065 725f 6772  e, tokens_per_gr
-000069a0: 6f75 7029 0a20 2020 2020 2020 2069 6e70  oup).        inp
-000069b0: 7574 5f74 656e 736f 7220 3d20 7365 6c66  ut_tensor = self
-000069c0: 2e74 7261 6e73 706f 7365 5f33 6469 6d28  .transpose_3dim(
-000069d0: 696e 7075 745f 7465 6e73 6f72 2c20 2830  input_tensor, (0
-000069e0: 2c20 322c 2031 2929 0a20 2020 2020 2020  , 2, 1)).       
-000069f0: 2064 6973 7061 7463 685f 7465 6e73 6f72   dispatch_tensor
-00006a00: 203d 2073 656c 662e 7265 7368 6170 6528   = self.reshape(
-00006a10: 6469 7370 6174 6368 5f74 656e 736f 722c  dispatch_tensor,
-00006a20: 2028 7365 6c66 2e64 705f 6772 6f75 702c   (self.dp_group,
-00006a30: 2074 6f6b 656e 735f 7065 725f 6772 6f75   tokens_per_grou
-00006a40: 702c 0a20 2020 2020 2020 2020 2020 2020  p,.             
-00006a50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00006a60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00006a70: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-00006a80: 2e65 7870 6572 745f 6469 6d20 2a20 6578  .expert_dim * ex
-00006a90: 7065 7274 5f63 6170 6163 6974 7929 290a  pert_capacity)).
-00006aa0: 2020 2020 2020 2020 6469 7370 6174 6368          dispatch
-00006ab0: 5f74 656e 736f 7220 3d20 7365 6c66 2e63  _tensor = self.c
-00006ac0: 6173 7428 6469 7370 6174 6368 5f74 656e  ast(dispatch_ten
-00006ad0: 736f 722c 2046 2e64 7479 7065 2869 6e70  sor, F.dtype(inp
-00006ae0: 7574 5f74 656e 736f 7229 290a 2020 2020  ut_tensor)).    
-00006af0: 2020 2020 2320 6578 7065 7274 5f69 6e70      # expert_inp
-00006b00: 7574 2773 2073 6861 7065 3a20 2873 656c  ut's shape: (sel
-00006b10: 662e 6470 5f67 726f 7570 2c20 7365 6c66  f.dp_group, self
-00006b20: 2e68 6964 6465 6e5f 7369 7a65 2c20 7365  .hidden_size, se
-00006b30: 6c66 2e65 7870 6572 745f 6469 6d20 2a20  lf.expert_dim * 
-00006b40: 6578 7065 7274 5f63 6170 6163 6974 7929  expert_capacity)
-00006b50: 0a20 2020 2020 2020 2065 7870 6572 745f  .        expert_
-00006b60: 696e 7075 7420 3d20 7365 6c66 2e62 6174  input = self.bat
-00006b70: 6368 5f6d 6d28 696e 7075 745f 7465 6e73  ch_mm(input_tens
-00006b80: 6f72 2c20 6469 7370 6174 6368 5f74 656e  or, dispatch_ten
-00006b90: 736f 7229 0a20 2020 2020 2020 2065 7870  sor).        exp
-00006ba0: 6572 745f 696e 7075 7420 3d20 7365 6c66  ert_input = self
-00006bb0: 2e72 6573 6861 7065 2865 7870 6572 745f  .reshape(expert_
-00006bc0: 696e 7075 742c 2028 7365 6c66 2e64 705f  input, (self.dp_
-00006bd0: 6772 6f75 702c 2073 656c 662e 6869 6464  group, self.hidd
-00006be0: 656e 5f73 697a 652c 2073 656c 662e 6578  en_size, self.ex
-00006bf0: 7065 7274 5f64 696d 2c0a 2020 2020 2020  pert_dim,.      
-00006c00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00006c10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00006c20: 2020 2020 2020 2020 2020 2020 2065 7870               exp
-00006c30: 6572 745f 6361 7061 6369 7479 2929 0a20  ert_capacity)). 
-00006c40: 2020 2020 2020 2023 2054 6865 2066 6f6c         # The fol
-00006c50: 6c6f 7769 6e67 2066 6f75 7220 6f70 7320  lowing four ops 
-00006c60: 6172 6520 746f 2069 6d70 6c65 6d65 6e74  are to implement
-00006c70: 2074 7261 6e73 706f 7365 2865 7870 6572   transpose(exper
-00006c80: 745f 696e 7075 742c 2028 322c 2030 2c20  t_input, (2, 0, 
-00006c90: 332c 2031 2929 2c20 666f 7220 7468 6174  3, 1)), for that
-00006ca0: 2061 2073 696e 676c 6520 7472 616e 7370   a single transp
-00006cb0: 6f73 650a 2020 2020 2020 2020 2320 6861  ose.        # ha
-00006cc0: 7320 6261 6420 7065 7266 6f72 6d61 6e63  s bad performanc
-00006cd0: 650a 2020 2020 2020 2020 6578 7065 7274  e.        expert
-00006ce0: 5f69 6e70 7574 203d 2073 656c 662e 7265  _input = self.re
-00006cf0: 7368 6170 6528 6578 7065 7274 5f69 6e70  shape(expert_inp
-00006d00: 7574 2c20 2873 656c 662e 6470 5f67 726f  ut, (self.dp_gro
-00006d10: 7570 202a 2073 656c 662e 6869 6464 656e  up * self.hidden
-00006d20: 5f73 697a 652c 0a20 2020 2020 2020 2020  _size,.         
-00006d30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00006d40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00006d50: 2020 2020 2020 2020 2020 7365 6c66 2e65            self.e
-00006d60: 7870 6572 745f 6469 6d20 2a20 6578 7065  xpert_dim * expe
-00006d70: 7274 5f63 6170 6163 6974 7929 290a 2020  rt_capacity)).  
-00006d80: 2020 2020 2020 6578 7065 7274 5f69 6e70        expert_inp
-00006d90: 7574 203d 2073 656c 662e 7472 616e 7370  ut = self.transp
-00006da0: 6f73 655f 3264 696d 2865 7870 6572 745f  ose_2dim(expert_
-00006db0: 696e 7075 742c 2028 312c 2030 2929 0a20  input, (1, 0)). 
-00006dc0: 2020 2020 2020 2065 7870 6572 745f 696e         expert_in
-00006dd0: 7075 7420 3d20 7365 6c66 2e72 6573 6861  put = self.resha
-00006de0: 7065 2865 7870 6572 745f 696e 7075 742c  pe(expert_input,
-00006df0: 2028 7365 6c66 2e65 7870 6572 745f 6469   (self.expert_di
-00006e00: 6d2c 2065 7870 6572 745f 6361 7061 6369  m, expert_capaci
-00006e10: 7479 2c20 7365 6c66 2e64 705f 6772 6f75  ty, self.dp_grou
-00006e20: 702c 0a20 2020 2020 2020 2020 2020 2020  p,.             
-00006e30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00006e40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00006e50: 2020 2020 2020 7365 6c66 2e68 6964 6465        self.hidde
-00006e60: 6e5f 7369 7a65 2929 0a20 2020 2020 2020  n_size)).       
-00006e70: 2069 6620 7365 6c66 2e65 6e61 626c 655f   if self.enable_
-00006e80: 636f 6c64 5f68 6f74 5f65 7870 6572 743a  cold_hot_expert:
-00006e90: 0a20 2020 2020 2020 2020 2020 2068 6f74  .            hot
-00006ea0: 5f65 7870 6572 745f 696e 6465 7820 3d20  _expert_index = 
-00006eb0: 7365 6c66 2e68 6f74 5f65 7870 6572 745f  self.hot_expert_
-00006ec0: 696e 6465 782e 7661 6c75 6528 292e 636f  index.value().co
-00006ed0: 7079 2829 5b30 5d0a 2020 2020 2020 2020  py()[0].        
-00006ee0: 2020 2020 636f 6c64 5f65 7870 6572 745f      cold_expert_
-00006ef0: 696e 6465 7820 3d20 7365 6c66 2e63 6f6c  index = self.col
-00006f00: 645f 6578 7065 7274 5f69 6e64 6578 2e76  d_expert_index.v
-00006f10: 616c 7565 2829 2e63 6f70 7928 295b 305d  alue().copy()[0]
-00006f20: 0a0a 2020 2020 2020 2020 2020 2020 686f  ..            ho
-00006f30: 745f 6578 7065 7274 5f69 6e70 7574 203d  t_expert_input =
-00006f40: 2073 656c 662e 6761 7468 6572 2865 7870   self.gather(exp
-00006f50: 6572 745f 696e 7075 742c 2068 6f74 5f65  ert_input, hot_e
-00006f60: 7870 6572 745f 696e 6465 782c 2030 290a  xpert_index, 0).
-00006f70: 2020 2020 2020 2020 2020 2020 636f 6c64              cold
-00006f80: 5f65 7870 6572 745f 696e 7075 7420 3d20  _expert_input = 
-00006f90: 6578 7065 7274 5f69 6e70 7574 0a20 2020  expert_input.   
-00006fa0: 2020 2020 2020 2020 2063 6f6c 645f 6578           cold_ex
-00006fb0: 7065 7274 5f63 6170 6163 6974 7920 3d20  pert_capacity = 
-00006fc0: 696e 7428 6578 7065 7274 5f63 6170 6163  int(expert_capac
-00006fd0: 6974 7920 2a20 7365 6c66 2e63 6f6c 645f  ity * self.cold_
-00006fe0: 746f 6b65 6e5f 7065 7263 656e 7429 0a20  token_percent). 
-00006ff0: 2020 2020 2020 2020 2020 2068 6f74 5f65             hot_e
-00007000: 7870 6572 745f 696e 7075 7420 3d20 7365  xpert_input = se
-00007010: 6c66 2e74 7261 6e73 706f 7365 5f34 6469  lf.transpose_4di
-00007020: 6d5f 6470 2868 6f74 5f65 7870 6572 745f  m_dp(hot_expert_
-00007030: 696e 7075 742c 2028 322c 2030 2c20 312c  input, (2, 0, 1,
-00007040: 2033 2929 0a20 2020 2020 2020 2020 2020   3)).           
-00007050: 2063 6f6c 645f 6578 7065 7274 5f69 6e70   cold_expert_inp
-00007060: 7574 203d 2073 656c 662e 7472 616e 7370  ut = self.transp
-00007070: 6f73 655f 3464 696d 5f64 7028 636f 6c64  ose_4dim_dp(cold
-00007080: 5f65 7870 6572 745f 696e 7075 742c 2028  _expert_input, (
-00007090: 302c 2032 2c20 312c 2033 2929 0a20 2020  0, 2, 1, 3)).   
-000070a0: 2020 2020 2020 2020 2063 6f6c 645f 6578           cold_ex
-000070b0: 7065 7274 5f69 6e70 7574 203d 2073 656c  pert_input = sel
-000070c0: 662e 7374 7269 6465 5f73 6c69 6365 5f64  f.stride_slice_d
-000070d0: 7028 0a20 2020 2020 2020 2020 2020 2020  p(.             
-000070e0: 2020 2063 6f6c 645f 6578 7065 7274 5f69     cold_expert_i
-000070f0: 6e70 7574 2c20 2830 2c20 302c 2030 2c20  nput, (0, 0, 0, 
-00007100: 3029 2c0a 2020 2020 2020 2020 2020 2020  0),.            
-00007110: 2020 2020 2873 656c 662e 6578 7065 7274      (self.expert
-00007120: 5f64 696d 2c20 7365 6c66 2e64 705f 6772  _dim, self.dp_gr
-00007130: 6f75 702c 2063 6f6c 645f 6578 7065 7274  oup, cold_expert
-00007140: 5f63 6170 6163 6974 792c 2073 656c 662e  _capacity, self.
-00007150: 6869 6464 656e 5f73 697a 6529 2c0a 2020  hidden_size),.  
-00007160: 2020 2020 2020 2020 2020 2020 2020 2831                (1
-00007170: 2c20 312c 2031 2c20 3129 290a 2020 2020  , 1, 1, 1)).    
-00007180: 2020 2020 2020 2020 2320 6578 7065 7274          # expert
-00007190: 5f6f 7574 7075 7427 7320 7368 6170 653a  _output's shape:
-000071a0: 2028 7365 6c66 2e64 705f 6772 6f75 702c   (self.dp_group,
-000071b0: 2073 656c 662e 6869 6464 656e 5f73 697a   self.hidden_siz
-000071c0: 652c 2073 656c 662e 6578 7065 7274 5f64  e, self.expert_d
-000071d0: 696d 2c20 6578 7065 7274 5f63 6170 6163  im, expert_capac
-000071e0: 6974 7929 0a20 2020 2020 2020 2020 2020  ity).           
-000071f0: 2069 6620 7365 6c66 2e63 6f6d 705f 636f   if self.comp_co
-00007200: 6d6d 5f70 6172 616c 6c65 6c3a 0a20 2020  mm_parallel:.   
-00007210: 2020 2020 2020 2020 2020 2020 2063 6f6c               col
-00007220: 645f 6578 7065 7274 5f6f 7574 7075 7420  d_expert_output 
-00007230: 3d20 7365 6c66 2e66 666e 5f70 6172 616c  = self.ffn_paral
-00007240: 6c65 6c5f 696e 6665 7228 636f 6c64 5f65  lel_infer(cold_e
-00007250: 7870 6572 745f 696e 7075 742c 2063 6f6c  xpert_input, col
-00007260: 645f 6578 7065 7274 5f63 6170 6163 6974  d_expert_capacit
-00007270: 7929 0a20 2020 2020 2020 2020 2020 2065  y).            e
-00007280: 6c73 653a 0a20 2020 2020 2020 2020 2020  lse:.           
-00007290: 2020 2020 2063 6f6c 645f 6578 7065 7274       cold_expert
-000072a0: 5f6f 7574 7075 7420 3d20 7365 6c66 2e66  _output = self.f
-000072b0: 666e 5f69 6e66 6572 2863 6f6c 645f 6578  fn_infer(cold_ex
-000072c0: 7065 7274 5f69 6e70 7574 2c20 636f 6c64  pert_input, cold
-000072d0: 5f65 7870 6572 745f 6361 7061 6369 7479  _expert_capacity
-000072e0: 290a 0a20 2020 2020 2020 2020 2020 2068  )..            h
-000072f0: 6f74 5f65 7870 6572 745f 696e 7075 7420  ot_expert_input 
-00007300: 3d20 7365 6c66 2e72 6573 6861 7065 2868  = self.reshape(h
-00007310: 6f74 5f65 7870 6572 745f 696e 7075 742c  ot_expert_input,
-00007320: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00007330: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00007340: 2020 2020 2020 2020 2020 2020 2028 7365               (se
-00007350: 6c66 2e68 6f74 5f65 7870 6572 745f 6e75  lf.hot_expert_nu
-00007360: 6d20 2a20 7365 6c66 2e64 705f 6772 6f75  m * self.dp_grou
-00007370: 7020 2a20 6578 7065 7274 5f63 6170 6163  p * expert_capac
-00007380: 6974 792c 2073 656c 662e 6869 6464 656e  ity, self.hidden
-00007390: 5f73 697a 6529 290a 2020 2020 2020 2020  _size)).        
-000073a0: 2020 2020 686f 745f 6578 7065 7274 5f6f      hot_expert_o
-000073b0: 7574 7075 7420 3d20 7365 6c66 2e6d 6c70  utput = self.mlp
-000073c0: 2868 6f74 5f65 7870 6572 745f 696e 7075  (hot_expert_inpu
-000073d0: 7429 0a0a 2020 2020 2020 2020 2020 2020  t)..            
-000073e0: 686f 745f 6578 7065 7274 5f6f 7574 7075  hot_expert_outpu
-000073f0: 7420 3d20 7365 6c66 2e72 6573 6861 7065  t = self.reshape
-00007400: 2868 6f74 5f65 7870 6572 745f 6f75 7470  (hot_expert_outp
-00007410: 7574 2c0a 2020 2020 2020 2020 2020 2020  ut,.            
-00007420: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00007430: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00007440: 2028 7365 6c66 2e64 705f 6772 6f75 702c   (self.dp_group,
-00007450: 2073 656c 662e 686f 745f 6578 7065 7274   self.hot_expert
-00007460: 5f6e 756d 2c20 6578 7065 7274 5f63 6170  _num, expert_cap
-00007470: 6163 6974 792c 2073 656c 662e 6869 6464  acity, self.hidd
-00007480: 656e 5f73 697a 6529 290a 0a20 2020 2020  en_size))..     
-00007490: 2020 2020 2020 2063 6f6c 645f 6578 7065         cold_expe
-000074a0: 7274 5f6f 7574 7075 7420 3d20 7365 6c66  rt_output = self
-000074b0: 2e67 6174 6865 7232 2863 6f6c 645f 6578  .gather2(cold_ex
-000074c0: 7065 7274 5f6f 7574 7075 742c 2063 6f6c  pert_output, col
-000074d0: 645f 6578 7065 7274 5f69 6e64 6578 2c20  d_expert_index, 
-000074e0: 3129 0a20 2020 2020 2020 2020 2020 2069  1).            i
-000074f0: 6620 7365 6c66 2e63 6f6c 645f 746f 6b65  f self.cold_toke
-00007500: 6e5f 7065 7263 656e 7420 3c20 312e 303a  n_percent < 1.0:
-00007510: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00007520: 207a 6572 6f73 203d 2073 656c 662e 7a65   zeros = self.ze
-00007530: 726f 7328 2873 656c 662e 6470 5f67 726f  ros((self.dp_gro
-00007540: 7570 2c20 7365 6c66 2e65 7870 6572 745f  up, self.expert_
-00007550: 6469 6d20 2d20 7365 6c66 2e68 6f74 5f65  dim - self.hot_e
-00007560: 7870 6572 745f 6e75 6d2c 0a20 2020 2020  xpert_num,.     
-00007570: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00007580: 2020 2020 2020 2020 2020 2020 2020 2065                 e
-00007590: 7870 6572 745f 6361 7061 6369 7479 202d  xpert_capacity -
-000075a0: 2063 6f6c 645f 6578 7065 7274 5f63 6170   cold_expert_cap
-000075b0: 6163 6974 792c 2073 656c 662e 6869 6464  acity, self.hidd
-000075c0: 656e 5f73 697a 6529 2c20 6d73 7479 7065  en_size), mstype
-000075d0: 2e66 6c6f 6174 3136 290a 2020 2020 2020  .float16).      
-000075e0: 2020 2020 2020 2020 2020 636f 6c64 5f65            cold_e
-000075f0: 7870 6572 745f 6f75 7470 7574 203d 2073  xpert_output = s
-00007600: 656c 662e 636f 6e63 6174 3228 2863 6f6c  elf.concat2((col
-00007610: 645f 6578 7065 7274 5f6f 7574 7075 742c  d_expert_output,
-00007620: 207a 6572 6f73 2929 0a0a 2020 2020 2020   zeros))..      
-00007630: 2020 2020 2020 6578 7065 7274 5f6f 7574        expert_out
-00007640: 7075 7420 3d20 7365 6c66 2e63 6f6e 6361  put = self.conca
-00007650: 7431 2828 686f 745f 6578 7065 7274 5f6f  t1((hot_expert_o
-00007660: 7574 7075 742c 2063 6f6c 645f 6578 7065  utput, cold_expe
-00007670: 7274 5f6f 7574 7075 7429 290a 2020 2020  rt_output)).    
-00007680: 2020 2020 2020 2020 6578 7065 7274 5f69          expert_i
-00007690: 6e64 6578 203d 2073 656c 662e 636f 6e63  ndex = self.conc
-000076a0: 6174 3028 2868 6f74 5f65 7870 6572 745f  at0((hot_expert_
-000076b0: 696e 6465 782c 2063 6f6c 645f 6578 7065  index, cold_expe
-000076c0: 7274 5f69 6e64 6578 2929 0a20 2020 2020  rt_index)).     
-000076d0: 2020 2020 2020 205f 2c20 6578 7065 7274         _, expert
-000076e0: 5f67 6174 6865 725f 696e 6465 7820 3d20  _gather_index = 
-000076f0: 7365 6c66 2e72 6573 6861 7065 2865 7870  self.reshape(exp
-00007700: 6572 745f 696e 6465 782c 2028 312c 202d  ert_index, (1, -
-00007710: 3129 292e 746f 706b 2873 656c 662e 6578  1)).topk(self.ex
-00007720: 7065 7274 5f64 696d 2c20 6c61 7267 6573  pert_dim, larges
-00007730: 743d 4661 6c73 6529 0a20 2020 2020 2020  t=False).       
-00007740: 2020 2020 2065 7870 6572 745f 6761 7468       expert_gath
-00007750: 6572 5f69 6e64 6578 203d 2073 656c 662e  er_index = self.
-00007760: 7265 7368 6170 6528 6578 7065 7274 5f67  reshape(expert_g
-00007770: 6174 6865 725f 696e 6465 782c 2028 2d31  ather_index, (-1
-00007780: 2c29 290a 2020 2020 2020 2020 2020 2020  ,)).            
-00007790: 6578 7065 7274 5f6f 7574 7075 7420 3d20  expert_output = 
-000077a0: 7365 6c66 2e67 6174 6865 7232 2865 7870  self.gather2(exp
-000077b0: 6572 745f 6f75 7470 7574 2c20 6578 7065  ert_output, expe
-000077c0: 7274 5f67 6174 6865 725f 696e 6465 782c  rt_gather_index,
-000077d0: 2031 290a 2020 2020 2020 2020 2020 2020   1).            
-000077e0: 2320 6578 7065 7274 5f6f 7574 7075 7427  # expert_output'
-000077f0: 7320 7368 6170 653a 2028 7365 6c66 2e64  s shape: (self.d
-00007800: 705f 6772 6f75 702c 2073 656c 662e 6869  p_group, self.hi
-00007810: 6464 656e 5f73 697a 652c 2073 656c 662e  dden_size, self.
-00007820: 6578 7065 7274 5f64 696d 2c20 6578 7065  expert_dim, expe
-00007830: 7274 5f63 6170 6163 6974 7929 0a20 2020  rt_capacity).   
-00007840: 2020 2020 2020 2020 2065 7870 6572 745f           expert_
-00007850: 6f75 7470 7574 203d 2073 656c 662e 7472  output = self.tr
-00007860: 616e 7370 6f73 655f 3164 696d 5f64 7028  anspose_1dim_dp(
-00007870: 6578 7065 7274 5f6f 7574 7075 742c 2028  expert_output, (
-00007880: 302c 2033 2c20 312c 2032 2929 0a20 2020  0, 3, 1, 2)).   
-00007890: 2020 2020 2065 6c73 653a 0a20 2020 2020       else:.     
-000078a0: 2020 2020 2020 2023 2065 7870 6572 745f         # expert_
-000078b0: 696e 7075 7427 7320 7368 6170 653a 2028  input's shape: (
-000078c0: 7365 6c66 2e65 7870 6572 745f 6469 6d2c  self.expert_dim,
-000078d0: 2073 656c 662e 6470 5f67 726f 7570 2c20   self.dp_group, 
-000078e0: 6578 7065 7274 5f63 6170 6163 6974 792c  expert_capacity,
-000078f0: 2073 656c 662e 6869 6464 656e 5f73 697a   self.hidden_siz
-00007900: 6529 0a20 2020 2020 2020 2020 2020 2065  e).            e
-00007910: 7870 6572 745f 696e 7075 7420 3d20 7365  xpert_input = se
-00007920: 6c66 2e74 7261 6e73 706f 7365 5f34 6469  lf.transpose_4di
-00007930: 6d5f 6470 2865 7870 6572 745f 696e 7075  m_dp(expert_inpu
-00007940: 742c 2028 302c 2032 2c20 312c 2033 2929  t, (0, 2, 1, 3))
-00007950: 0a20 2020 2020 2020 2020 2020 2023 2065  .            # e
-00007960: 7870 6572 745f 6f75 7470 7574 2773 2073  xpert_output's s
-00007970: 6861 7065 3a20 2873 656c 662e 6470 5f67  hape: (self.dp_g
-00007980: 726f 7570 2c20 7365 6c66 2e68 6964 6465  roup, self.hidde
-00007990: 6e5f 7369 7a65 2c20 7365 6c66 2e65 7870  n_size, self.exp
-000079a0: 6572 745f 6469 6d2c 2065 7870 6572 745f  ert_dim, expert_
-000079b0: 6361 7061 6369 7479 290a 2020 2020 2020  capacity).      
-000079c0: 2020 2020 2020 6966 2073 656c 662e 636f        if self.co
-000079d0: 6d70 5f63 6f6d 6d5f 7061 7261 6c6c 656c  mp_comm_parallel
-000079e0: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
-000079f0: 2020 6578 7065 7274 5f6f 7574 7075 7420    expert_output 
-00007a00: 3d20 7365 6c66 2e66 666e 5f70 6172 616c  = self.ffn_paral
-00007a10: 6c65 6c5f 696e 6665 7228 6578 7065 7274  lel_infer(expert
-00007a20: 5f69 6e70 7574 2c20 6578 7065 7274 5f63  _input, expert_c
-00007a30: 6170 6163 6974 7929 0a20 2020 2020 2020  apacity).       
-00007a40: 2020 2020 2065 6c73 653a 0a20 2020 2020       else:.     
-00007a50: 2020 2020 2020 2020 2020 2065 7870 6572             exper
-00007a60: 745f 6f75 7470 7574 203d 2073 656c 662e  t_output = self.
-00007a70: 6666 6e5f 696e 6665 7228 6578 7065 7274  ffn_infer(expert
-00007a80: 5f69 6e70 7574 2c20 6578 7065 7274 5f63  _input, expert_c
-00007a90: 6170 6163 6974 7929 0a0a 2020 2020 2020  apacity)..      
-00007aa0: 2020 6578 7065 7274 5f6f 7574 7075 7420    expert_output 
-00007ab0: 3d20 7365 6c66 2e72 6573 6861 7065 2865  = self.reshape(e
-00007ac0: 7870 6572 745f 6f75 7470 7574 2c20 2873  xpert_output, (s
-00007ad0: 656c 662e 6470 5f67 726f 7570 2c20 7365  elf.dp_group, se
-00007ae0: 6c66 2e68 6964 6465 6e5f 7369 7a65 2c0a  lf.hidden_size,.
-00007af0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00007b00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00007b10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00007b20: 2020 2020 2073 656c 662e 6578 7065 7274       self.expert
-00007b30: 5f64 696d 202a 2065 7870 6572 745f 6361  _dim * expert_ca
-00007b40: 7061 6369 7479 2929 0a20 2020 2020 2020  pacity)).       
-00007b50: 2063 6f6d 6269 6e65 5f74 656e 736f 7220   combine_tensor 
-00007b60: 3d20 7365 6c66 2e72 6573 6861 7065 2863  = self.reshape(c
-00007b70: 6f6d 6269 6e65 5f74 656e 736f 722c 2028  ombine_tensor, (
-00007b80: 7365 6c66 2e64 705f 6772 6f75 702c 2074  self.dp_group, t
-00007b90: 6f6b 656e 735f 7065 725f 6772 6f75 702c  okens_per_group,
-00007ba0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00007bb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00007bc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00007bd0: 2020 2020 2020 2020 7365 6c66 2e65 7870          self.exp
-00007be0: 6572 745f 6469 6d20 2a20 6578 7065 7274  ert_dim * expert
-00007bf0: 5f63 6170 6163 6974 7929 290a 2020 2020  _capacity)).    
-00007c00: 2020 2020 2320 636f 6d62 696e 655f 7465      # combine_te
-00007c10: 6e73 6f72 2773 2073 6861 7065 3a20 2873  nsor's shape: (s
-00007c20: 656c 662e 6470 5f67 726f 7570 2c20 7365  elf.dp_group, se
-00007c30: 6c66 2e65 7870 6572 745f 6469 6d20 2a20  lf.expert_dim * 
-00007c40: 6578 7065 7274 5f63 6170 6163 6974 792c  expert_capacity,
-00007c50: 2074 6f6b 656e 735f 7065 725f 6772 6f75   tokens_per_grou
-00007c60: 7029 0a20 2020 2020 2020 2063 6f6d 6269  p).        combi
-00007c70: 6e65 5f74 656e 736f 7220 3d20 7365 6c66  ne_tensor = self
-00007c80: 2e74 7261 6e73 706f 7365 5f33 6469 6d28  .transpose_3dim(
-00007c90: 636f 6d62 696e 655f 7465 6e73 6f72 2c20  combine_tensor, 
-00007ca0: 2830 2c20 322c 2031 2929 0a20 2020 2020  (0, 2, 1)).     
-00007cb0: 2020 2063 6f6d 6269 6e65 5f74 656e 736f     combine_tenso
-00007cc0: 7220 3d20 7365 6c66 2e63 6173 7428 636f  r = self.cast(co
-00007cd0: 6d62 696e 655f 7465 6e73 6f72 2c20 462e  mbine_tensor, F.
-00007ce0: 6474 7970 6528 6578 7065 7274 5f6f 7574  dtype(expert_out
-00007cf0: 7075 7429 290a 0a20 2020 2020 2020 2023  put))..        #
-00007d00: 2063 6f6d 6269 6e65 645f 6f75 7470 7574   combined_output
-00007d10: 2773 2073 6861 7065 3a20 2873 656c 662e  's shape: (self.
-00007d20: 6470 5f67 726f 7570 2c20 7365 6c66 2e68  dp_group, self.h
-00007d30: 6964 6465 6e5f 7369 7a65 2c20 746f 6b65  idden_size, toke
-00007d40: 6e73 5f70 6572 5f67 726f 7570 290a 2020  ns_per_group).  
-00007d50: 2020 2020 2020 636f 6d62 696e 6564 5f6f        combined_o
-00007d60: 7574 7075 7420 3d20 7365 6c66 2e62 6174  utput = self.bat
-00007d70: 6368 5f6d 6d32 2865 7870 6572 745f 6f75  ch_mm2(expert_ou
-00007d80: 7470 7574 2c20 636f 6d62 696e 655f 7465  tput, combine_te
-00007d90: 6e73 6f72 290a 2020 2020 2020 2020 2320  nsor).        # 
-00007da0: 636f 6d62 696e 6564 5f6f 7574 7075 7427  combined_output'
-00007db0: 7320 7368 6170 653a 2028 7365 6c66 2e64  s shape: (self.d
-00007dc0: 705f 6772 6f75 702c 2074 6f6b 656e 735f  p_group, tokens_
-00007dd0: 7065 725f 6772 6f75 702c 2073 656c 662e  per_group, self.
-00007de0: 6869 6464 656e 5f73 697a 6529 0a20 2020  hidden_size).   
-00007df0: 2020 2020 2063 6f6d 6269 6e65 645f 6f75       combined_ou
-00007e00: 7470 7574 203d 2073 656c 662e 7472 616e  tput = self.tran
-00007e10: 7370 6f73 655f 3364 696d 2863 6f6d 6269  spose_3dim(combi
-00007e20: 6e65 645f 6f75 7470 7574 2c20 2830 2c20  ned_output, (0, 
-00007e30: 322c 2031 2929 0a20 2020 2020 2020 2063  2, 1)).        c
-00007e40: 6f6d 6269 6e65 645f 6f75 7470 7574 203d  ombined_output =
-00007e50: 2073 656c 662e 7265 7368 6170 6528 636f   self.reshape(co
-00007e60: 6d62 696e 6564 5f6f 7574 7075 742c 2028  mbined_output, (
-00007e70: 6273 5f61 6e64 5f64 6d6f 6465 6c5b 305d  bs_and_dmodel[0]
-00007e80: 2c20 6273 5f61 6e64 5f64 6d6f 6465 6c5b  , bs_and_dmodel[
-00007e90: 315d 2929 0a20 2020 2020 2020 2063 6f6d  1])).        com
-00007ea0: 6269 6e65 645f 6f75 7470 7574 203d 2073  bined_output = s
-00007eb0: 656c 662e 7265 7368 6170 6528 636f 6d62  elf.reshape(comb
-00007ec0: 696e 6564 5f6f 7574 7075 742c 2069 6e70  ined_output, inp
-00007ed0: 7574 5f73 6861 7065 290a 0a20 2020 2020  ut_shape)..     
-00007ee0: 2020 2061 7578 5f6c 6f73 7320 3d20 7365     aux_loss = se
-00007ef0: 6c66 2e6d 756c 2873 656c 662e 6175 785f  lf.mul(self.aux_
-00007f00: 6c6f 7373 5f66 6163 746f 722c 2061 7578  loss_factor, aux
-00007f10: 5f6c 6f73 7329 0a20 2020 2020 2020 2072  _loss).        r
-00007f20: 6574 7572 6e20 636f 6d62 696e 6564 5f6f  eturn combined_o
-00007f30: 7574 7075 742c 2061 7578 5f6c 6f73 730a  utput, aux_loss.
-00007f40: 0a0a 636c 6173 7320 4d6f 4556 3228 4365  ..class MoEV2(Ce
-00007f50: 6c6c 293a 0a20 2020 2022 2222 0a20 2020  ll):.    """.   
-00007f60: 2054 6865 206d 6978 7475 7265 206f 6620   The mixture of 
-00007f70: 6578 7065 7274 7320 284d 6f45 2920 696d  experts (MoE) im
-00007f80: 706c 656d 656e 7461 7469 6f6e 2e20 5468  plementation. Th
-00007f90: 6520 696d 706c 656d 656e 7461 7469 6f6e  e implementation
-00007fa0: 2069 6e63 6c75 6465 7320 6120 726f 7574   includes a rout
-00007fb0: 6572 2061 6e64 2061 2046 6565 6446 6f72  er and a FeedFor
-00007fc0: 7761 7264 206c 6179 6572 2e0a 2020 2020  ward layer..    
-00007fd0: 5468 6520 726f 7574 6572 2064 6973 7061  The router dispa
-00007fe0: 7463 6865 7320 746f 6b65 6e73 2074 6f20  tches tokens to 
-00007ff0: 6578 7065 7274 7320 696e 2046 6565 6446  experts in FeedF
-00008000: 6f72 7761 7264 2c20 7468 656e 2046 6565  orward, then Fee
-00008010: 6446 6f72 7761 7264 2064 6f65 7320 636f  dForward does co
-00008020: 6d70 7574 6174 696f 6e2c 2061 6e64 2074  mputation, and t
-00008030: 6865 2066 696e 616c 206f 7574 7075 7420  he final output 
-00008040: 6973 0a20 2020 206f 6274 6169 6e65 6420  is.    obtained 
-00008050: 6279 206d 756c 7469 706c 7969 6e67 2046  by multiplying F
-00008060: 6565 6446 6f72 7761 7264 2773 206f 7574  eedForward's out
-00008070: 7075 7420 616e 6420 726f 7574 6572 2773  put and router's
-00008080: 2063 6f6d 6269 6e65 2077 6569 6768 742e   combine weight.
-00008090: 0a20 2020 2054 6869 7320 6973 2061 2063  .    This is a c
-000080a0: 6f6d 6d6f 6e20 696e 7465 7266 6163 652c  ommon interface,
-000080b0: 2077 6869 6368 2061 6c6c 6f77 7320 616e   which allows an
-000080c0: 7920 6666 6e20 636c 6173 7320 616e 6420  y ffn class and 
-000080d0: 616e 7920 526f 7574 6572 2061 6c67 6f72  any Router algor
-000080e0: 6974 686d 2869 6d70 6c65 6d65 6e74 6564  ithm(implemented
-000080f0: 2069 6e20 5632 2066 6f72 6d29 2e0a 0a20   in V2 form)... 
-00008100: 2020 2041 7267 733a 0a20 2020 2020 2020     Args:.       
-00008110: 2068 6964 6465 6e5f 7369 7a65 2028 696e   hidden_size (in
-00008120: 7429 3a20 5468 6520 6469 6d65 6e73 696f  t): The dimensio
-00008130: 6e20 6f66 2074 6865 2069 6e70 7574 732e  n of the inputs.
-00008140: 0a20 2020 2020 2020 2066 666e 5f68 6964  .        ffn_hid
-00008150: 6465 6e5f 7369 7a65 2028 696e 7429 3a20  den_size (int): 
-00008160: 5468 6520 696e 7465 726d 6564 6961 7465  The intermediate
-00008170: 2068 6964 6465 6e20 7369 7a65 2e0a 2020   hidden size..  
-00008180: 2020 2020 2020 6472 6f70 6f75 745f 7261        dropout_ra
-00008190: 7465 2028 666c 6f61 7429 3a20 5468 6520  te (float): The 
-000081a0: 6472 6f70 6f75 7420 7261 7465 2066 6f72  dropout rate for
-000081b0: 2074 6865 2073 6563 6f6e 6420 6c69 6e65   the second line
-000081c0: 6172 2773 206f 7574 7075 742e 0a20 2020  ar's output..   
-000081d0: 2020 2020 2068 6964 6465 6e5f 6163 7420       hidden_act 
-000081e0: 2873 7472 293a 2054 6865 2061 6374 6976  (str): The activ
-000081f0: 6174 696f 6e20 6f66 2074 6865 2069 6e74  ation of the int
-00008200: 6572 6e61 6c20 6665 6564 666f 7277 6172  ernal feedforwar
-00008210: 6420 6c61 7965 722e 2053 7570 706f 7274  d layer. Support
-00008220: 7320 2772 656c 7527 2c0a 2020 2020 2020  s 'relu',.      
-00008230: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00008240: 2020 2027 7265 6c75 3627 2c20 2774 616e     'relu6', 'tan
-00008250: 6827 2c20 2767 656c 7527 2c20 2766 6173  h', 'gelu', 'fas
-00008260: 745f 6765 6c75 272c 2027 656c 7527 2c20  t_gelu', 'elu', 
-00008270: 2773 6967 6d6f 6964 272c 2027 7072 656c  'sigmoid', 'prel
-00008280: 7527 2c20 276c 6561 6b79 7265 6c75 272c  u', 'leakyrelu',
-00008290: 2027 6873 7769 7368 272c 0a20 2020 2020   'hswish',.     
-000082a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000082b0: 2020 2020 2768 7369 676d 6f69 6427 2c20      'hsigmoid', 
-000082c0: 276c 6f67 7369 676d 6f69 6427 2061 6e64  'logsigmoid' and
-000082d0: 2073 6f20 6f6e 2e20 4465 6661 756c 743a   so on. Default:
-000082e0: 2067 656c 752e 0a20 2020 2020 2020 2070   gelu..        p
-000082f0: 6172 616d 5f69 6e69 745f 7479 7065 2028  aram_init_type (
-00008300: 6474 7970 652e 4e75 6d62 6572 293a 2054  dtype.Number): T
-00008310: 6865 2070 6172 616d 6574 6572 2069 6e69  he parameter ini
-00008320: 7469 616c 697a 6174 696f 6e20 7479 7065  tialization type
-00008330: 2e20 4361 6e20 6265 2064 7479 7065 2e66  . Can be dtype.f
-00008340: 6c6f 6174 3332 206f 7220 6474 7970 652e  loat32 or dtype.
-00008350: 666c 6f61 7431 362e 0a20 2020 2020 2020  float16..       
-00008360: 206d 6f65 5f63 6f6e 6669 6728 4d6f 4543   moe_config(MoEC
-00008370: 6f6e 6669 6729 3a20 5468 6520 636f 6e66  onfig): The conf
-00008380: 6967 7572 6174 696f 6e20 6f66 204d 6f45  iguration of MoE
-00008390: 2028 4d69 7874 7572 6520 6f66 2045 7870   (Mixture of Exp
-000083a0: 6572 7429 2e20 4465 6661 756c 7420 6973  ert). Default is
-000083b0: 2061 6e20 696e 7374 616e 6365 206f 6620   an instance of 
-000083c0: 4d6f 4543 6f6e 6669 6720 7769 7468 0a20  MoEConfig with. 
-000083d0: 2020 2020 2020 2020 2020 2064 6566 6175             defau
-000083e0: 6c74 2076 616c 7565 732e 2050 6c65 6173  lt values. Pleas
-000083f0: 6520 7365 6520 604d 6f45 436f 6e66 6967  e see `MoEConfig
-00008400: 602e 0a20 2020 2020 2020 2070 6172 616c  `..        paral
-00008410: 6c65 6c5f 636f 6e66 6967 284d 6f45 5061  lel_config(MoEPa
-00008420: 7261 6c6c 656c 436f 6e66 6967 293a 2054  rallelConfig): T
-00008430: 6865 2070 6172 616c 6c65 6c20 636f 6e66  he parallel conf
-00008440: 6967 2066 6f72 204d 6f45 2c20 7365 6520  ig for MoE, see 
-00008450: 604d 6f45 5061 7261 6c6c 656c 436f 6e66  `MoEParallelConf
-00008460: 6967 602e 0a20 2020 2020 2020 2020 2020  ig`..           
-00008470: 2044 6566 6175 6c74 2060 6465 6661 756c   Default `defaul
-00008480: 745f 6d6f 6570 6172 616c 6c65 6c5f 636f  t_moeparallel_co
-00008490: 6e66 6967 602c 2061 6e20 696e 7374 616e  nfig`, an instan
-000084a0: 6365 206f 6620 604d 6f45 5061 7261 6c6c  ce of `MoEParall
-000084b0: 656c 436f 6e66 6967 6020 7769 7468 2064  elConfig` with d
-000084c0: 6566 6175 6c74 2061 7267 732e 0a0a 2020  efault args...  
-000084d0: 2020 496e 7075 7473 3a0a 2020 2020 2020    Inputs:.      
-000084e0: 2020 2d20 2a2a 782a 2a20 2854 656e 736f    - **x** (Tenso
-000084f0: 7229 202d 2073 686f 756c 6420 6265 2060  r) - should be `
-00008500: 5b62 6174 6368 2c20 7365 715f 6c65 6e67  [batch, seq_leng
-00008510: 7468 2c20 6869 6464 656e 5f73 697a 655d  th, hidden_size]
-00008520: 602e 2046 6c6f 6174 2074 656e 736f 722e  `. Float tensor.
-00008530: 0a0a 2020 2020 4f75 7470 7574 733a 0a20  ..    Outputs:. 
-00008540: 2020 2020 2020 2054 656e 736f 722c 2074         Tensor, t
-00008550: 6865 206f 7574 7075 7420 6f66 2074 6869  he output of thi
-00008560: 7320 6c61 7965 7220 6166 7465 7220 6d61  s layer after ma
-00008570: 7070 696e 672e 2054 6865 2073 6861 7065  pping. The shape
-00008580: 2069 7320 605b 6261 7463 682c 2073 6571   is `[batch, seq
-00008590: 5f6c 656e 6774 682c 2068 6964 6465 6e5f  _length, hidden_
-000085a0: 7369 7a65 5d60 2e0a 2020 2020 2222 220a  size]`..    """.
-000085b0: 2020 2020 6465 6620 5f5f 696e 6974 5f5f      def __init__
-000085c0: 2873 656c 662c 0a20 2020 2020 2020 2020  (self,.         
-000085d0: 2020 2020 2020 2020 6666 6e2c 0a20 2020          ffn,.   
-000085e0: 2020 2020 2020 2020 2020 2020 2020 6469                di
-000085f0: 6d2c 0a20 2020 2020 2020 2020 2020 2020  m,.             
-00008600: 2020 2020 6d6f 655f 636f 6e66 6967 3d64      moe_config=d
-00008610: 6566 6175 6c74 5f6d 6f65 5f63 6f6e 6669  efault_moe_confi
-00008620: 672c 0a20 2020 2020 2020 2020 2020 2020  g,.             
-00008630: 2020 2020 7061 7261 6c6c 656c 5f63 6f6e      parallel_con
-00008640: 6669 673d 6465 6661 756c 745f 6d6f 6570  fig=default_moep
-00008650: 6172 616c 6c65 6c5f 636f 6e66 6967 293a  arallel_config):
-00008660: 0a20 2020 2020 2020 2073 7570 6572 284d  .        super(M
-00008670: 6f45 5632 2c20 7365 6c66 292e 5f5f 696e  oEV2, self).__in
-00008680: 6974 5f5f 2829 0a20 2020 2020 2020 2073  it__().        s
-00008690: 656c 662e 6869 6464 656e 5f73 697a 6520  elf.hidden_size 
-000086a0: 3d20 6469 6d0a 2020 2020 2020 2020 7365  = dim.        se
-000086b0: 6c66 2e65 7870 6572 745f 6469 6d20 3d20  lf.expert_dim = 
-000086c0: 6d6f 655f 636f 6e66 6967 2e65 7870 6572  moe_config.exper
-000086d0: 745f 6e75 6d0a 2020 2020 2020 2020 7365  t_num.        se
-000086e0: 6c66 2e63 6170 6163 6974 795f 6661 6374  lf.capacity_fact
-000086f0: 6f72 203d 206d 6f65 5f63 6f6e 6669 672e  or = moe_config.
-00008700: 6361 7061 6369 7479 5f66 6163 746f 720a  capacity_factor.
-00008710: 2020 2020 2020 2020 7365 6c66 2e6e 756d          self.num
-00008720: 5f65 7870 6572 7473 5f63 686f 7365 6e20  _experts_chosen 
-00008730: 3d20 6d6f 655f 636f 6e66 6967 2e6e 756d  = moe_config.num
-00008740: 5f65 7870 6572 7473 5f63 686f 7365 6e0a  _experts_chosen.
-00008750: 2020 2020 2020 2020 7365 6c66 2e64 705f          self.dp_
-00008760: 6772 6f75 7020 3d20 7061 7261 6c6c 656c  group = parallel
-00008770: 5f63 6f6e 6669 672e 6461 7461 5f70 6172  _config.data_par
-00008780: 616c 6c65 6c0a 2020 2020 2020 2020 7365  allel.        se
-00008790: 6c66 2e64 7020 3d20 7061 7261 6c6c 656c  lf.dp = parallel
-000087a0: 5f63 6f6e 6669 672e 6461 7461 5f70 6172  _config.data_par
-000087b0: 616c 6c65 6c0a 2020 2020 2020 2020 7365  allel.        se
-000087c0: 6c66 2e65 7020 3d20 7061 7261 6c6c 656c  lf.ep = parallel
-000087d0: 5f63 6f6e 6669 672e 6578 7065 7274 5f70  _config.expert_p
-000087e0: 6172 616c 6c65 6c0a 2020 2020 2020 2020  arallel.        
-000087f0: 7365 6c66 2e6d 7020 3d20 7061 7261 6c6c  self.mp = parall
-00008800: 656c 5f63 6f6e 6669 672e 6d6f 6465 6c5f  el_config.model_
-00008810: 7061 7261 6c6c 656c 0a20 2020 2020 2020  parallel.       
-00008820: 2073 656c 662e 6772 6f75 705f 7769 7365   self.group_wise
-00008830: 5f61 3261 203d 206d 6f65 5f63 6f6e 6669  _a2a = moe_confi
-00008840: 672e 6772 6f75 705f 7769 7365 5f61 3261  g.group_wise_a2a
-00008850: 0a20 2020 2020 2020 2073 656c 662e 6470  .        self.dp
-00008860: 5f6d 6f65 203d 2073 656c 662e 6470 202f  _moe = self.dp /
-00008870: 2f20 7365 6c66 2e65 700a 2020 2020 2020  / self.ep.      
-00008880: 2020 7365 6c66 2e64 705f 7261 6e67 6520    self.dp_range 
-00008890: 3d20 5465 6e73 6f72 286e 702e 6172 616e  = Tensor(np.aran
-000088a0: 6765 2873 656c 662e 6470 5f67 726f 7570  ge(self.dp_group
-000088b0: 292e 7265 7368 6170 6528 2d31 2c20 3129  ).reshape(-1, 1)
-000088c0: 2c20 6d73 7479 7065 2e69 6e74 3332 2920  , mstype.int32) 
-000088d0: 2320 2864 702c 2031 2920 3d20 5b5b 305d  # (dp, 1) = [[0]
-000088e0: 2c5b 315d 2c5b 325d 2e2e 2e5b 6470 5d5d  ,[1],[2]...[dp]]
-000088f0: 0a0a 2020 2020 2020 2020 7365 6c66 2e66  ..        self.f
-00008900: 666e 203d 2066 666e 0a20 2020 2020 2020  fn = ffn.       
-00008910: 2056 616c 6964 6174 6f72 2e63 6865 636b   Validator.check
-00008920: 5f73 7472 696e 6728 6d6f 655f 636f 6e66  _string(moe_conf
-00008930: 6967 2e72 6f75 7469 6e67 5f70 6f6c 6963  ig.routing_polic
-00008940: 792c 205b 2254 6f70 6b52 6f75 7465 7256  y, ["TopkRouterV
-00008950: 3222 5d2c 2022 726f 7574 696e 675f 706f  2"], "routing_po
-00008960: 6c69 6379 2229 0a20 2020 2020 2020 2073  licy").        s
-00008970: 656c 662e 726f 7574 6572 203d 2052 6f75  elf.router = Rou
-00008980: 7465 7228 645f 6d6f 6465 6c3d 7365 6c66  ter(d_model=self
-00008990: 2e68 6964 6465 6e5f 7369 7a65 2c0a 2020  .hidden_size,.  
-000089a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000089b0: 2020 2020 2020 2020 2020 206d 6f65 5f63             moe_c
-000089c0: 6f6e 6669 673d 6d6f 655f 636f 6e66 6967  onfig=moe_config
-000089d0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-000089e0: 2020 2020 2020 2020 2020 2020 2020 2072                 r
-000089f0: 6f75 7469 6e67 5f70 6f6c 6963 793d 6d6f  outing_policy=mo
-00008a00: 655f 636f 6e66 6967 2e72 6f75 7469 6e67  e_config.routing
-00008a10: 5f70 6f6c 6963 792c 0a20 2020 2020 2020  _policy,.       
-00008a20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00008a30: 2020 2020 2020 7472 6169 6e69 6e67 3d54        training=T
-00008a40: 7275 652c 0a20 2020 2020 2020 2020 2020  rue,.           
-00008a50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00008a60: 2020 7061 7261 6c6c 656c 5f63 6f6e 6669    parallel_confi
-00008a70: 673d 7061 7261 6c6c 656c 5f63 6f6e 6669  g=parallel_confi
-00008a80: 6729 0a0a 2020 2020 2020 2020 7365 6c66  g)..        self
-00008a90: 2e72 6573 6861 7065 203d 2050 2e52 6573  .reshape = P.Res
-00008aa0: 6861 7065 2829 0a20 2020 2020 2020 2073  hape().        s
-00008ab0: 656c 662e 7368 6170 6520 3d20 502e 5368  elf.shape = P.Sh
-00008ac0: 6170 6528 290a 2020 2020 2020 2020 7365  ape().        se
-00008ad0: 6c66 2e63 6173 7420 3d20 502e 4361 7374  lf.cast = P.Cast
-00008ae0: 2829 0a20 2020 2020 2020 2073 656c 662e  ().        self.
-00008af0: 7472 616e 7370 6f73 655f 3464 696d 5f64  transpose_4dim_d
-00008b00: 7031 203d 2050 2e54 7261 6e73 706f 7365  p1 = P.Transpose
-00008b10: 2829 2e73 6861 7264 2828 2831 2c20 7365  ().shard(((1, se
-00008b20: 6c66 2e64 702c 2031 2c20 3129 2c29 290a  lf.dp, 1, 1),)).
-00008b30: 2020 2020 2020 2020 7365 6c66 2e74 7261          self.tra
-00008b40: 6e73 706f 7365 5f34 6469 6d5f 6470 3020  nspose_4dim_dp0 
-00008b50: 3d20 502e 5472 616e 7370 6f73 6528 292e  = P.Transpose().
-00008b60: 7368 6172 6428 2828 7365 6c66 2e64 702c  shard(((self.dp,
-00008b70: 2031 2c20 312c 2031 292c 2929 0a20 2020   1, 1, 1),)).   
-00008b80: 2020 2020 2073 656c 662e 7472 616e 7370       self.transp
-00008b90: 6f73 655f 3564 696d 5f65 7032 203d 2050  ose_5dim_ep2 = P
-00008ba0: 2e54 7261 6e73 706f 7365 2829 2e73 6861  .Transpose().sha
-00008bb0: 7264 2828 2873 656c 662e 6470 5f6d 6f65  rd(((self.dp_moe
-00008bc0: 2c20 312c 2073 656c 662e 6570 2c20 312c  , 1, self.ep, 1,
-00008bd0: 2031 292c 2929 0a20 2020 2020 2020 2073   1),)).        s
-00008be0: 656c 662e 636f 6e63 6174 5f64 7020 3d20  elf.concat_dp = 
-00008bf0: 502e 436f 6e63 6174 2832 292e 7368 6172  P.Concat(2).shar
-00008c00: 6428 2828 312c 2073 656c 662e 6470 2c20  d(((1, self.dp, 
-00008c10: 312c 2031 292c 2028 312c 2073 656c 662e  1, 1), (1, self.
-00008c20: 6470 2c20 312c 2031 2929 290a 2020 2020  dp, 1, 1))).    
-00008c30: 2020 2020 7365 6c66 2e73 7472 6964 655f      self.stride_
-00008c40: 736c 6963 6520 3d20 502e 5374 7269 6465  slice = P.Stride
-00008c50: 6453 6c69 6365 2829 2e73 6861 7264 2828  dSlice().shard((
-00008c60: 2873 656c 662e 6470 2c20 312c 2031 2c20  (self.dp, 1, 1, 
-00008c70: 3129 2c29 290a 2020 2020 2020 2020 7365  1),)).        se
-00008c80: 6c66 2e73 7472 6964 655f 736c 6963 655f  lf.stride_slice_
-00008c90: 6470 203d 2050 2e53 7472 6964 6564 536c  dp = P.StridedSl
-00008ca0: 6963 6528 292e 7368 6172 6428 2828 312c  ice().shard(((1,
-00008cb0: 2073 656c 662e 6470 2c20 312c 2031 292c   self.dp, 1, 1),
-00008cc0: 2929 0a20 2020 2020 2020 2073 656c 662e  )).        self.
-00008cd0: 7374 7269 6465 5f73 6c69 6365 5f65 7020  stride_slice_ep 
-00008ce0: 3d20 502e 5374 7269 6465 6453 6c69 6365  = P.StridedSlice
-00008cf0: 2829 2e73 6861 7264 2828 2873 656c 662e  ().shard(((self.
-00008d00: 6570 2c20 312c 2031 2c20 3129 2c29 290a  ep, 1, 1, 1),)).
-00008d10: 2020 2020 2020 2020 7365 6c66 2e73 7472          self.str
-00008d20: 6964 655f 736c 6963 655f 6470 5f6d 7020  ide_slice_dp_mp 
-00008d30: 3d20 502e 5374 7269 6465 6453 6c69 6365  = P.StridedSlice
-00008d40: 2829 2e73 6861 7264 2828 2831 2c20 7365  ().shard(((1, se
-00008d50: 6c66 2e64 702c 2073 656c 662e 6d70 2c20  lf.dp, self.mp, 
-00008d60: 3129 2c29 290a 2020 2020 2020 2020 7365  1),)).        se
-00008d70: 6c66 2e73 7472 6964 655f 736c 6963 655f  lf.stride_slice_
-00008d80: 6570 5f6d 7020 3d20 502e 5374 7269 6465  ep_mp = P.Stride
-00008d90: 6453 6c69 6365 2829 2e73 6861 7264 2828  dSlice().shard((
-00008da0: 2873 656c 662e 6570 2c20 312c 2073 656c  (self.ep, 1, sel
-00008db0: 662e 6d70 2c20 3129 2c29 290a 0a20 2020  f.mp, 1),))..   
-00008dc0: 2064 6566 2066 666e 5f69 6e66 6572 2873   def ffn_infer(s
-00008dd0: 656c 662c 2065 7870 6572 745f 696e 7075  elf, expert_inpu
-00008de0: 742c 2063 6170 6163 6974 7929 3a0a 2020  t, capacity):.  
-00008df0: 2020 2020 2020 2222 220a 2020 2020 2020        """.      
-00008e00: 2020 436f 6d70 7574 696e 6720 7468 6520    Computing the 
-00008e10: 4646 4e2e 0a20 2020 2020 2020 2022 2222  FFN..        """
-00008e20: 0a20 2020 2020 2020 2069 6620 7365 6c66  .        if self
-00008e30: 2e64 705f 6d6f 6520 3d3d 2031 3a0a 2020  .dp_moe == 1:.  
-00008e40: 2020 2020 2020 2020 2020 6966 2073 656c            if sel
-00008e50: 662e 6772 6f75 705f 7769 7365 5f61 3261  f.group_wise_a2a
-00008e60: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
-00008e70: 2020 6578 7065 7274 5f69 6e70 7574 203d    expert_input =
-00008e80: 2073 656c 662e 7472 616e 7370 6f73 655f   self.transpose_
-00008e90: 3464 696d 5f64 7030 2865 7870 6572 745f  4dim_dp0(expert_
-00008ea0: 696e 7075 742c 2028 312c 2030 2c20 322c  input, (1, 0, 2,
-00008eb0: 2033 2929 2023 2028 452c 2064 702c 206e   3)) # (E, dp, n
-00008ec0: 202c 6829 203c 2d2d 2028 6470 2c20 452c   ,h) <-- (dp, E,
-00008ed0: 206e 2c20 6829 0a20 2020 2020 2020 2020   n, h).         
-00008ee0: 2020 2020 2020 2023 2063 6170 6163 6974         # capacit
-00008ef0: 7920 7368 6172 6420 6279 206d 700a 2020  y shard by mp.  
-00008f00: 2020 2020 2020 2020 2020 2020 2020 6578                ex
-00008f10: 7065 7274 5f69 6e70 7574 203d 2073 656c  pert_input = sel
-00008f20: 662e 7374 7269 6465 5f73 6c69 6365 5f64  f.stride_slice_d
-00008f30: 705f 6d70 2865 7870 6572 745f 696e 7075  p_mp(expert_inpu
-00008f40: 742c 2028 302c 2030 2c20 302c 2030 292c  t, (0, 0, 0, 0),
-00008f50: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00008f60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00008f70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00008f80: 2020 2020 2020 2020 2873 656c 662e 6578          (self.ex
-00008f90: 7065 7274 5f64 696d 2c20 7365 6c66 2e64  pert_dim, self.d
-00008fa0: 705f 6772 6f75 702c 2063 6170 6163 6974  p_group, capacit
-00008fb0: 792c 2073 656c 662e 6869 6464 656e 5f73  y, self.hidden_s
-00008fc0: 697a 6529 2c0a 2020 2020 2020 2020 2020  ize),.          
-00008fd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00008fe0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00008ff0: 2020 2020 2020 2020 2020 2020 2028 312c               (1,
-00009000: 2031 2c20 312c 2031 2929 0a20 2020 2020   1, 1, 1)).     
-00009010: 2020 2020 2020 2020 2020 2023 2067 726f             # gro
-00009020: 7570 2d77 6973 6520 616c 6c74 6f61 6c6c  up-wise alltoall
-00009030: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00009040: 2065 7870 6572 745f 696e 7075 7420 3d20   expert_input = 
-00009050: 7365 6c66 2e73 7472 6964 655f 736c 6963  self.stride_slic
-00009060: 655f 6570 5f6d 7028 6578 7065 7274 5f69  e_ep_mp(expert_i
-00009070: 6e70 7574 2c20 2830 2c20 302c 2030 2c20  nput, (0, 0, 0, 
-00009080: 3029 2c0a 2020 2020 2020 2020 2020 2020  0),.            
-00009090: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000090a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000090b0: 2020 2020 2020 2020 2020 2028 7365 6c66             (self
-000090c0: 2e65 7870 6572 745f 6469 6d2c 2073 656c  .expert_dim, sel
-000090d0: 662e 6470 5f67 726f 7570 2c20 6361 7061  f.dp_group, capa
-000090e0: 6369 7479 2c20 7365 6c66 2e68 6964 6465  city, self.hidde
-000090f0: 6e5f 7369 7a65 292c 0a20 2020 2020 2020  n_size),.       
-00009100: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00009110: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00009120: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00009130: 2831 2c20 312c 2031 2c20 3129 290a 2020  (1, 1, 1, 1)).  
-00009140: 2020 2020 2020 2020 2020 2020 2020 2320                # 
-00009150: 616c 6c67 6174 6865 720a 2020 2020 2020  allgather.      
-00009160: 2020 2020 2020 2020 2020 6578 7065 7274            expert
-00009170: 5f69 6e70 7574 203d 2073 656c 662e 7374  _input = self.st
-00009180: 7269 6465 5f73 6c69 6365 5f65 7028 6578  ride_slice_ep(ex
-00009190: 7065 7274 5f69 6e70 7574 2c20 2830 2c20  pert_input, (0, 
-000091a0: 302c 2030 2c20 3029 2c0a 2020 2020 2020  0, 0, 0),.      
-000091b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000091c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000091d0: 2020 2020 2020 2020 2020 2020 2020 2873                (s
-000091e0: 656c 662e 6578 7065 7274 5f64 696d 2c20  elf.expert_dim, 
-000091f0: 7365 6c66 2e64 705f 6772 6f75 702c 2063  self.dp_group, c
-00009200: 6170 6163 6974 792c 2073 656c 662e 6869  apacity, self.hi
-00009210: 6464 656e 5f73 697a 6529 2c0a 2020 2020  dden_size),.    
-00009220: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00009230: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00009240: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00009250: 2831 2c20 312c 2031 2c20 3129 290a 2020  (1, 1, 1, 1)).  
-00009260: 2020 2020 2020 2020 2020 656c 7365 3a0a            else:.
-00009270: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00009280: 2320 2845 2c20 6470 2c20 6e2c 2068 2920  # (E, dp, n, h) 
-00009290: 3c2d 2d20 2864 702c 2045 2c20 6e2c 2068  <-- (dp, E, n, h
-000092a0: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
-000092b0: 2020 6578 7065 7274 5f69 6e70 7574 203d    expert_input =
-000092c0: 2073 656c 662e 7472 616e 7370 6f73 655f   self.transpose_
-000092d0: 3464 696d 5f64 7031 2865 7870 6572 745f  4dim_dp1(expert_
-000092e0: 696e 7075 742c 2028 312c 2030 2c20 322c  input, (1, 0, 2,
-000092f0: 2033 2929 0a20 2020 2020 2020 2065 6c73   3)).        els
-00009300: 653a 0a20 2020 2020 2020 2020 2020 2023  e:.            #
-00009310: 7265 7368 6170 6520 666f 7220 6578 7065  reshape for expe
-00009320: 7274 5f69 6e70 7574 2c28 6470 5f6d 6f65  rt_input,(dp_moe
-00009330: 2c20 452c 2065 702a 6e2c 2068 2920 3c2d  , E, ep*n, h) <-
-00009340: 2d20 2864 702c 2045 2c20 6e2c 2068 290a  - (dp, E, n, h).
-00009350: 2020 2020 2020 2020 2020 2020 2320 2864              # (d
-00009360: 705f 6d6f 652c 2065 702c 2045 2c20 6e2c  p_moe, ep, E, n,
-00009370: 2068 2920 3c2d 2d20 2864 702c 2045 2c20   h) <-- (dp, E, 
-00009380: 6e2c 2068 290a 2020 2020 2020 2020 2020  n, h).          
-00009390: 2020 6578 7065 7274 5f69 6e70 7574 203d    expert_input =
-000093a0: 2073 656c 662e 7265 7368 6170 6528 6578   self.reshape(ex
-000093b0: 7065 7274 5f69 6e70 7574 2c20 2873 656c  pert_input, (sel
-000093c0: 662e 6470 5f6d 6f65 2c20 7365 6c66 2e65  f.dp_moe, self.e
-000093d0: 702c 2073 656c 662e 6578 7065 7274 5f64  p, self.expert_d
-000093e0: 696d 2c20 2d31 2c20 7365 6c66 2e68 6964  im, -1, self.hid
-000093f0: 6465 6e5f 7369 7a65 2929 0a20 2020 2020  den_size)).     
-00009400: 2020 2020 2020 2023 2028 6470 5f6d 6f65         # (dp_moe
-00009410: 2c20 452c 2065 702c 206e 2c20 6829 203c  , E, ep, n, h) <
-00009420: 2d2d 2028 6470 5f6d 6f65 2c20 6570 2c20  -- (dp_moe, ep, 
-00009430: 452c 206e 2c20 6829 0a20 2020 2020 2020  E, n, h).       
-00009440: 2020 2020 2065 7870 6572 745f 696e 7075       expert_inpu
-00009450: 7420 3d20 7365 6c66 2e74 7261 6e73 706f  t = self.transpo
-00009460: 7365 5f35 6469 6d5f 6570 3228 6578 7065  se_5dim_ep2(expe
-00009470: 7274 5f69 6e70 7574 2c20 2830 2c20 322c  rt_input, (0, 2,
-00009480: 2031 2c20 332c 2034 2929 0a20 2020 2020   1, 3, 4)).     
-00009490: 2020 2020 2020 2023 2028 6470 5f6d 6f65         # (dp_moe
-000094a0: 2c20 452c 2065 702a 6e2c 2068 2920 3c2d  , E, ep*n, h) <-
-000094b0: 2d20 2864 705f 6d6f 652c 2045 2c20 6570  - (dp_moe, E, ep
-000094c0: 2c20 6e2c 2068 290a 2020 2020 2020 2020  , n, h).        
-000094d0: 2020 2020 6578 7065 7274 5f69 6e70 7574      expert_input
-000094e0: 203d 2073 656c 662e 7265 7368 6170 6528   = self.reshape(
-000094f0: 6578 7065 7274 5f69 6e70 7574 2c20 2873  expert_input, (s
-00009500: 656c 662e 6470 5f6d 6f65 2c20 7365 6c66  elf.dp_moe, self
-00009510: 2e65 7870 6572 745f 6469 6d2c 202d 312c  .expert_dim, -1,
-00009520: 2073 656c 662e 6869 6464 656e 5f73 697a   self.hidden_siz
-00009530: 6529 290a 0a20 2020 2020 2020 2065 7870  e))..        exp
-00009540: 6572 745f 696e 7075 7420 3d20 7365 6c66  ert_input = self
-00009550: 2e72 6573 6861 7065 2865 7870 6572 745f  .reshape(expert_
-00009560: 696e 7075 742c 2028 2d31 2c20 7365 6c66  input, (-1, self
-00009570: 2e68 6964 6465 6e5f 7369 7a65 2929 0a20  .hidden_size)). 
-00009580: 2020 2020 2020 2065 7870 6572 745f 6f75         expert_ou
-00009590: 7470 7574 203d 2073 656c 662e 6666 6e28  tput = self.ffn(
-000095a0: 6578 7065 7274 5f69 6e70 7574 290a 0a20  expert_input).. 
-000095b0: 2020 2020 2020 2069 6620 7365 6c66 2e64         if self.d
-000095c0: 705f 6d6f 6520 3d3d 2031 3a0a 2020 2020  p_moe == 1:.    
-000095d0: 2020 2020 2020 2020 2320 2845 2c20 6470          # (E, dp
-000095e0: 2c20 6e2c 2068 2920 3c2d 2d20 2845 2c20  , n, h) <-- (E, 
-000095f0: 6470 2a6e 2c20 6829 0a20 2020 2020 2020  dp*n, h).       
-00009600: 2020 2020 2065 7870 6572 745f 6f75 7470       expert_outp
-00009610: 7574 203d 2073 656c 662e 7265 7368 6170  ut = self.reshap
-00009620: 6528 6578 7065 7274 5f6f 7574 7075 742c  e(expert_output,
-00009630: 2028 7365 6c66 2e65 7870 6572 745f 6469   (self.expert_di
-00009640: 6d2c 2073 656c 662e 6470 5f67 726f 7570  m, self.dp_group
-00009650: 2c20 2d31 2c20 7365 6c66 2e68 6964 6465  , -1, self.hidde
-00009660: 6e5f 7369 7a65 2929 0a20 2020 2020 2020  n_size)).       
-00009670: 2020 2020 2069 6620 7365 6c66 2e67 726f       if self.gro
-00009680: 7570 5f77 6973 655f 6132 613a 0a20 2020  up_wise_a2a:.   
-00009690: 2020 2020 2020 2020 2020 2020 2023 2063               # c
-000096a0: 6170 6163 6974 7920 7368 6172 6420 6279  apacity shard by
-000096b0: 206d 700a 2020 2020 2020 2020 2020 2020   mp.            
-000096c0: 2020 2020 6578 7065 7274 5f6f 7574 7075      expert_outpu
-000096d0: 7420 3d20 7365 6c66 2e73 7472 6964 655f  t = self.stride_
-000096e0: 736c 6963 655f 6570 5f6d 7028 6578 7065  slice_ep_mp(expe
-000096f0: 7274 5f6f 7574 7075 742c 2028 302c 2030  rt_output, (0, 0
-00009700: 2c20 302c 2030 292c 0a20 2020 2020 2020  , 0, 0),.       
-00009710: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00009720: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00009730: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00009740: 2028 7365 6c66 2e65 7870 6572 745f 6469   (self.expert_di
-00009750: 6d2c 2073 656c 662e 6470 5f67 726f 7570  m, self.dp_group
-00009760: 2c20 6361 7061 6369 7479 2c20 7365 6c66  , capacity, self
-00009770: 2e68 6964 6465 6e5f 7369 7a65 292c 0a20  .hidden_size),. 
-00009780: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00009790: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000097a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000097b0: 2020 2020 2020 2028 312c 2031 2c20 312c         (1, 1, 1,
-000097c0: 2031 2929 0a20 2020 2020 2020 2020 2020   1)).           
-000097d0: 2020 2020 2023 2067 726f 7570 2d77 6973       # group-wis
-000097e0: 6520 616c 6c74 6f61 6c6c 0a20 2020 2020  e alltoall.     
-000097f0: 2020 2020 2020 2020 2020 2065 7870 6572             exper
-00009800: 745f 6f75 7470 7574 203d 2073 656c 662e  t_output = self.
-00009810: 7374 7269 6465 5f73 6c69 6365 5f64 705f  stride_slice_dp_
-00009820: 6d70 2865 7870 6572 745f 6f75 7470 7574  mp(expert_output
-00009830: 2c20 2830 2c20 302c 2030 2c20 3029 2c0a  , (0, 0, 0, 0),.
+000067f0: 7365 6c66 2e65 7870 6572 745f 6469 6d20  self.expert_dim 
+00006800: 2a20 6578 7065 7274 5f63 6170 6163 6974  * expert_capacit
+00006810: 7929 290a 2020 2020 2020 2020 6469 7370  y)).        disp
+00006820: 6174 6368 5f74 656e 736f 7220 3d20 7365  atch_tensor = se
+00006830: 6c66 2e63 6173 7428 6469 7370 6174 6368  lf.cast(dispatch
+00006840: 5f74 656e 736f 722c 2046 2e64 7479 7065  _tensor, F.dtype
+00006850: 2869 6e70 7574 5f74 656e 736f 7229 290a  (input_tensor)).
+00006860: 2020 2020 2020 2020 2320 6578 7065 7274          # expert
+00006870: 5f69 6e70 7574 2773 2073 6861 7065 3a20  _input's shape: 
+00006880: 2873 656c 662e 6470 5f67 726f 7570 2c20  (self.dp_group, 
+00006890: 7365 6c66 2e68 6964 6465 6e5f 7369 7a65  self.hidden_size
+000068a0: 2c20 7365 6c66 2e65 7870 6572 745f 6469  , self.expert_di
+000068b0: 6d20 2a20 6578 7065 7274 5f63 6170 6163  m * expert_capac
+000068c0: 6974 7929 0a20 2020 2020 2020 2065 7870  ity).        exp
+000068d0: 6572 745f 696e 7075 7420 3d20 7365 6c66  ert_input = self
+000068e0: 2e62 6174 6368 5f6d 6d28 696e 7075 745f  .batch_mm(input_
+000068f0: 7465 6e73 6f72 2c20 6469 7370 6174 6368  tensor, dispatch
+00006900: 5f74 656e 736f 7229 0a20 2020 2020 2020  _tensor).       
+00006910: 2065 7870 6572 745f 696e 7075 7420 3d20   expert_input = 
+00006920: 7365 6c66 2e72 6573 6861 7065 2865 7870  self.reshape(exp
+00006930: 6572 745f 696e 7075 742c 2028 7365 6c66  ert_input, (self
+00006940: 2e64 705f 6772 6f75 702c 2073 656c 662e  .dp_group, self.
+00006950: 6869 6464 656e 5f73 697a 652c 2073 656c  hidden_size, sel
+00006960: 662e 6578 7065 7274 5f64 696d 2c0a 2020  f.expert_dim,.  
+00006970: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00006980: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00006990: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000069a0: 2065 7870 6572 745f 6361 7061 6369 7479   expert_capacity
+000069b0: 2929 0a20 2020 2020 2020 2023 2054 6865  )).        # The
+000069c0: 2066 6f6c 6c6f 7769 6e67 2066 6f75 7220   following four 
+000069d0: 6f70 7320 6172 6520 746f 2069 6d70 6c65  ops are to imple
+000069e0: 6d65 6e74 2074 7261 6e73 706f 7365 2865  ment transpose(e
+000069f0: 7870 6572 745f 696e 7075 742c 2028 322c  xpert_input, (2,
+00006a00: 2030 2c20 332c 2031 2929 2c20 666f 7220   0, 3, 1)), for 
+00006a10: 7468 6174 2061 2073 696e 676c 6520 7472  that a single tr
+00006a20: 616e 7370 6f73 650a 2020 2020 2020 2020  anspose.        
+00006a30: 2320 6861 7320 6261 6420 7065 7266 6f72  # has bad perfor
+00006a40: 6d61 6e63 650a 2020 2020 2020 2020 6578  mance.        ex
+00006a50: 7065 7274 5f69 6e70 7574 203d 2073 656c  pert_input = sel
+00006a60: 662e 7265 7368 6170 6528 6578 7065 7274  f.reshape(expert
+00006a70: 5f69 6e70 7574 2c20 2873 656c 662e 6470  _input, (self.dp
+00006a80: 5f67 726f 7570 202a 2073 656c 662e 6869  _group * self.hi
+00006a90: 6464 656e 5f73 697a 652c 0a20 2020 2020  dden_size,.     
+00006aa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00006ab0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00006ac0: 2020 2020 2020 2020 2020 2020 2020 7365                se
+00006ad0: 6c66 2e65 7870 6572 745f 6469 6d20 2a20  lf.expert_dim * 
+00006ae0: 6578 7065 7274 5f63 6170 6163 6974 7929  expert_capacity)
+00006af0: 290a 2020 2020 2020 2020 6578 7065 7274  ).        expert
+00006b00: 5f69 6e70 7574 203d 2073 656c 662e 7472  _input = self.tr
+00006b10: 616e 7370 6f73 655f 3264 696d 2865 7870  anspose_2dim(exp
+00006b20: 6572 745f 696e 7075 742c 2028 312c 2030  ert_input, (1, 0
+00006b30: 2929 0a20 2020 2020 2020 2065 7870 6572  )).        exper
+00006b40: 745f 696e 7075 7420 3d20 7365 6c66 2e72  t_input = self.r
+00006b50: 6573 6861 7065 2865 7870 6572 745f 696e  eshape(expert_in
+00006b60: 7075 742c 2028 7365 6c66 2e65 7870 6572  put, (self.exper
+00006b70: 745f 6469 6d2c 2065 7870 6572 745f 6361  t_dim, expert_ca
+00006b80: 7061 6369 7479 2c20 7365 6c66 2e64 705f  pacity, self.dp_
+00006b90: 6772 6f75 702c 0a20 2020 2020 2020 2020  group,.         
+00006ba0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00006bb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00006bc0: 2020 2020 2020 2020 2020 7365 6c66 2e68            self.h
+00006bd0: 6964 6465 6e5f 7369 7a65 2929 0a20 2020  idden_size)).   
+00006be0: 2020 2020 2069 6620 7365 6c66 2e65 6e61       if self.ena
+00006bf0: 626c 655f 636f 6c64 5f68 6f74 5f65 7870  ble_cold_hot_exp
+00006c00: 6572 743a 0a20 2020 2020 2020 2020 2020  ert:.           
+00006c10: 2068 6f74 5f65 7870 6572 745f 696e 6465   hot_expert_inde
+00006c20: 7820 3d20 7365 6c66 2e68 6f74 5f65 7870  x = self.hot_exp
+00006c30: 6572 745f 696e 6465 782e 7661 6c75 6528  ert_index.value(
+00006c40: 292e 636f 7079 2829 5b30 5d0a 2020 2020  ).copy()[0].    
+00006c50: 2020 2020 2020 2020 636f 6c64 5f65 7870          cold_exp
+00006c60: 6572 745f 696e 6465 7820 3d20 7365 6c66  ert_index = self
+00006c70: 2e63 6f6c 645f 6578 7065 7274 5f69 6e64  .cold_expert_ind
+00006c80: 6578 2e76 616c 7565 2829 2e63 6f70 7928  ex.value().copy(
+00006c90: 295b 305d 0a0a 2020 2020 2020 2020 2020  )[0]..          
+00006ca0: 2020 686f 745f 6578 7065 7274 5f69 6e70    hot_expert_inp
+00006cb0: 7574 203d 2073 656c 662e 6761 7468 6572  ut = self.gather
+00006cc0: 2865 7870 6572 745f 696e 7075 742c 2068  (expert_input, h
+00006cd0: 6f74 5f65 7870 6572 745f 696e 6465 782c  ot_expert_index,
+00006ce0: 2030 290a 2020 2020 2020 2020 2020 2020   0).            
+00006cf0: 636f 6c64 5f65 7870 6572 745f 696e 7075  cold_expert_inpu
+00006d00: 7420 3d20 6578 7065 7274 5f69 6e70 7574  t = expert_input
+00006d10: 0a20 2020 2020 2020 2020 2020 2063 6f6c  .            col
+00006d20: 645f 6578 7065 7274 5f63 6170 6163 6974  d_expert_capacit
+00006d30: 7920 3d20 696e 7428 6578 7065 7274 5f63  y = int(expert_c
+00006d40: 6170 6163 6974 7920 2a20 7365 6c66 2e63  apacity * self.c
+00006d50: 6f6c 645f 746f 6b65 6e5f 7065 7263 656e  old_token_percen
+00006d60: 7429 0a20 2020 2020 2020 2020 2020 2068  t).            h
+00006d70: 6f74 5f65 7870 6572 745f 696e 7075 7420  ot_expert_input 
+00006d80: 3d20 7365 6c66 2e74 7261 6e73 706f 7365  = self.transpose
+00006d90: 5f34 6469 6d5f 6470 2868 6f74 5f65 7870  _4dim_dp(hot_exp
+00006da0: 6572 745f 696e 7075 742c 2028 322c 2030  ert_input, (2, 0
+00006db0: 2c20 312c 2033 2929 0a20 2020 2020 2020  , 1, 3)).       
+00006dc0: 2020 2020 2063 6f6c 645f 6578 7065 7274       cold_expert
+00006dd0: 5f69 6e70 7574 203d 2073 656c 662e 7472  _input = self.tr
+00006de0: 616e 7370 6f73 655f 3464 696d 5f64 7028  anspose_4dim_dp(
+00006df0: 636f 6c64 5f65 7870 6572 745f 696e 7075  cold_expert_inpu
+00006e00: 742c 2028 302c 2032 2c20 312c 2033 2929  t, (0, 2, 1, 3))
+00006e10: 0a20 2020 2020 2020 2020 2020 2063 6f6c  .            col
+00006e20: 645f 6578 7065 7274 5f69 6e70 7574 203d  d_expert_input =
+00006e30: 2073 656c 662e 7374 7269 6465 5f73 6c69   self.stride_sli
+00006e40: 6365 5f64 7028 0a20 2020 2020 2020 2020  ce_dp(.         
+00006e50: 2020 2020 2020 2063 6f6c 645f 6578 7065         cold_expe
+00006e60: 7274 5f69 6e70 7574 2c20 2830 2c20 302c  rt_input, (0, 0,
+00006e70: 2030 2c20 3029 2c0a 2020 2020 2020 2020   0, 0),.        
+00006e80: 2020 2020 2020 2020 2873 656c 662e 6578          (self.ex
+00006e90: 7065 7274 5f64 696d 2c20 7365 6c66 2e64  pert_dim, self.d
+00006ea0: 705f 6772 6f75 702c 2063 6f6c 645f 6578  p_group, cold_ex
+00006eb0: 7065 7274 5f63 6170 6163 6974 792c 2073  pert_capacity, s
+00006ec0: 656c 662e 6869 6464 656e 5f73 697a 6529  elf.hidden_size)
+00006ed0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00006ee0: 2020 2831 2c20 312c 2031 2c20 3129 290a    (1, 1, 1, 1)).
+00006ef0: 2020 2020 2020 2020 2020 2020 2320 6578              # ex
+00006f00: 7065 7274 5f6f 7574 7075 7427 7320 7368  pert_output's sh
+00006f10: 6170 653a 2028 7365 6c66 2e64 705f 6772  ape: (self.dp_gr
+00006f20: 6f75 702c 2073 656c 662e 6869 6464 656e  oup, self.hidden
+00006f30: 5f73 697a 652c 2073 656c 662e 6578 7065  _size, self.expe
+00006f40: 7274 5f64 696d 2c20 6578 7065 7274 5f63  rt_dim, expert_c
+00006f50: 6170 6163 6974 7929 0a20 2020 2020 2020  apacity).       
+00006f60: 2020 2020 2069 6620 7365 6c66 2e63 6f6d       if self.com
+00006f70: 705f 636f 6d6d 5f70 6172 616c 6c65 6c3a  p_comm_parallel:
+00006f80: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00006f90: 2063 6f6c 645f 6578 7065 7274 5f6f 7574   cold_expert_out
+00006fa0: 7075 7420 3d20 7365 6c66 2e66 666e 5f70  put = self.ffn_p
+00006fb0: 6172 616c 6c65 6c5f 696e 6665 7228 636f  arallel_infer(co
+00006fc0: 6c64 5f65 7870 6572 745f 696e 7075 742c  ld_expert_input,
+00006fd0: 2063 6f6c 645f 6578 7065 7274 5f63 6170   cold_expert_cap
+00006fe0: 6163 6974 7929 0a20 2020 2020 2020 2020  acity).         
+00006ff0: 2020 2065 6c73 653a 0a20 2020 2020 2020     else:.       
+00007000: 2020 2020 2020 2020 2063 6f6c 645f 6578           cold_ex
+00007010: 7065 7274 5f6f 7574 7075 7420 3d20 7365  pert_output = se
+00007020: 6c66 2e66 666e 5f69 6e66 6572 2863 6f6c  lf.ffn_infer(col
+00007030: 645f 6578 7065 7274 5f69 6e70 7574 2c20  d_expert_input, 
+00007040: 636f 6c64 5f65 7870 6572 745f 6361 7061  cold_expert_capa
+00007050: 6369 7479 290a 0a20 2020 2020 2020 2020  city)..         
+00007060: 2020 2068 6f74 5f65 7870 6572 745f 696e     hot_expert_in
+00007070: 7075 7420 3d20 7365 6c66 2e72 6573 6861  put = self.resha
+00007080: 7065 2868 6f74 5f65 7870 6572 745f 696e  pe(hot_expert_in
+00007090: 7075 742c 0a20 2020 2020 2020 2020 2020  put,.           
+000070a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000070b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000070c0: 2028 7365 6c66 2e68 6f74 5f65 7870 6572   (self.hot_exper
+000070d0: 745f 6e75 6d20 2a20 7365 6c66 2e64 705f  t_num * self.dp_
+000070e0: 6772 6f75 7020 2a20 6578 7065 7274 5f63  group * expert_c
+000070f0: 6170 6163 6974 792c 2073 656c 662e 6869  apacity, self.hi
+00007100: 6464 656e 5f73 697a 6529 290a 2020 2020  dden_size)).    
+00007110: 2020 2020 2020 2020 686f 745f 6578 7065          hot_expe
+00007120: 7274 5f6f 7574 7075 7420 3d20 7365 6c66  rt_output = self
+00007130: 2e6d 6c70 2868 6f74 5f65 7870 6572 745f  .mlp(hot_expert_
+00007140: 696e 7075 7429 0a0a 2020 2020 2020 2020  input)..        
+00007150: 2020 2020 686f 745f 6578 7065 7274 5f6f      hot_expert_o
+00007160: 7574 7075 7420 3d20 7365 6c66 2e72 6573  utput = self.res
+00007170: 6861 7065 2868 6f74 5f65 7870 6572 745f  hape(hot_expert_
+00007180: 6f75 7470 7574 2c0a 2020 2020 2020 2020  output,.        
+00007190: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000071a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000071b0: 2020 2020 2028 7365 6c66 2e64 705f 6772       (self.dp_gr
+000071c0: 6f75 702c 2073 656c 662e 686f 745f 6578  oup, self.hot_ex
+000071d0: 7065 7274 5f6e 756d 2c20 6578 7065 7274  pert_num, expert
+000071e0: 5f63 6170 6163 6974 792c 2073 656c 662e  _capacity, self.
+000071f0: 6869 6464 656e 5f73 697a 6529 290a 0a20  hidden_size)).. 
+00007200: 2020 2020 2020 2020 2020 2063 6f6c 645f             cold_
+00007210: 6578 7065 7274 5f6f 7574 7075 7420 3d20  expert_output = 
+00007220: 7365 6c66 2e67 6174 6865 7232 2863 6f6c  self.gather2(col
+00007230: 645f 6578 7065 7274 5f6f 7574 7075 742c  d_expert_output,
+00007240: 2063 6f6c 645f 6578 7065 7274 5f69 6e64   cold_expert_ind
+00007250: 6578 2c20 3129 0a20 2020 2020 2020 2020  ex, 1).         
+00007260: 2020 2069 6620 7365 6c66 2e63 6f6c 645f     if self.cold_
+00007270: 746f 6b65 6e5f 7065 7263 656e 7420 3c20  token_percent < 
+00007280: 312e 303a 0a20 2020 2020 2020 2020 2020  1.0:.           
+00007290: 2020 2020 207a 6572 6f73 203d 2073 656c       zeros = sel
+000072a0: 662e 7a65 726f 7328 2873 656c 662e 6470  f.zeros((self.dp
+000072b0: 5f67 726f 7570 2c20 7365 6c66 2e65 7870  _group, self.exp
+000072c0: 6572 745f 6469 6d20 2d20 7365 6c66 2e68  ert_dim - self.h
+000072d0: 6f74 5f65 7870 6572 745f 6e75 6d2c 0a20  ot_expert_num,. 
+000072e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000072f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00007300: 2020 2065 7870 6572 745f 6361 7061 6369     expert_capaci
+00007310: 7479 202d 2063 6f6c 645f 6578 7065 7274  ty - cold_expert
+00007320: 5f63 6170 6163 6974 792c 2073 656c 662e  _capacity, self.
+00007330: 6869 6464 656e 5f73 697a 6529 2c20 6d73  hidden_size), ms
+00007340: 7479 7065 2e66 6c6f 6174 3136 290a 2020  type.float16).  
+00007350: 2020 2020 2020 2020 2020 2020 2020 636f                co
+00007360: 6c64 5f65 7870 6572 745f 6f75 7470 7574  ld_expert_output
+00007370: 203d 2073 656c 662e 636f 6e63 6174 3228   = self.concat2(
+00007380: 2863 6f6c 645f 6578 7065 7274 5f6f 7574  (cold_expert_out
+00007390: 7075 742c 207a 6572 6f73 2929 0a0a 2020  put, zeros))..  
+000073a0: 2020 2020 2020 2020 2020 6578 7065 7274            expert
+000073b0: 5f6f 7574 7075 7420 3d20 7365 6c66 2e63  _output = self.c
+000073c0: 6f6e 6361 7431 2828 686f 745f 6578 7065  oncat1((hot_expe
+000073d0: 7274 5f6f 7574 7075 742c 2063 6f6c 645f  rt_output, cold_
+000073e0: 6578 7065 7274 5f6f 7574 7075 7429 290a  expert_output)).
+000073f0: 2020 2020 2020 2020 2020 2020 6578 7065              expe
+00007400: 7274 5f69 6e64 6578 203d 2073 656c 662e  rt_index = self.
+00007410: 636f 6e63 6174 3028 2868 6f74 5f65 7870  concat0((hot_exp
+00007420: 6572 745f 696e 6465 782c 2063 6f6c 645f  ert_index, cold_
+00007430: 6578 7065 7274 5f69 6e64 6578 2929 0a20  expert_index)). 
+00007440: 2020 2020 2020 2020 2020 205f 2c20 6578             _, ex
+00007450: 7065 7274 5f67 6174 6865 725f 696e 6465  pert_gather_inde
+00007460: 7820 3d20 7365 6c66 2e72 6573 6861 7065  x = self.reshape
+00007470: 2865 7870 6572 745f 696e 6465 782c 2028  (expert_index, (
+00007480: 312c 202d 3129 292e 746f 706b 2873 656c  1, -1)).topk(sel
+00007490: 662e 6578 7065 7274 5f64 696d 2c20 6c61  f.expert_dim, la
+000074a0: 7267 6573 743d 4661 6c73 6529 0a20 2020  rgest=False).   
+000074b0: 2020 2020 2020 2020 2065 7870 6572 745f           expert_
+000074c0: 6761 7468 6572 5f69 6e64 6578 203d 2073  gather_index = s
+000074d0: 656c 662e 7265 7368 6170 6528 6578 7065  elf.reshape(expe
+000074e0: 7274 5f67 6174 6865 725f 696e 6465 782c  rt_gather_index,
+000074f0: 2028 2d31 2c29 290a 2020 2020 2020 2020   (-1,)).        
+00007500: 2020 2020 6578 7065 7274 5f6f 7574 7075      expert_outpu
+00007510: 7420 3d20 7365 6c66 2e67 6174 6865 7232  t = self.gather2
+00007520: 2865 7870 6572 745f 6f75 7470 7574 2c20  (expert_output, 
+00007530: 6578 7065 7274 5f67 6174 6865 725f 696e  expert_gather_in
+00007540: 6465 782c 2031 290a 2020 2020 2020 2020  dex, 1).        
+00007550: 2020 2020 2320 6578 7065 7274 5f6f 7574      # expert_out
+00007560: 7075 7427 7320 7368 6170 653a 2028 7365  put's shape: (se
+00007570: 6c66 2e64 705f 6772 6f75 702c 2073 656c  lf.dp_group, sel
+00007580: 662e 6869 6464 656e 5f73 697a 652c 2073  f.hidden_size, s
+00007590: 656c 662e 6578 7065 7274 5f64 696d 2c20  elf.expert_dim, 
+000075a0: 6578 7065 7274 5f63 6170 6163 6974 7929  expert_capacity)
+000075b0: 0a20 2020 2020 2020 2020 2020 2065 7870  .            exp
+000075c0: 6572 745f 6f75 7470 7574 203d 2073 656c  ert_output = sel
+000075d0: 662e 7472 616e 7370 6f73 655f 3164 696d  f.transpose_1dim
+000075e0: 5f64 7028 6578 7065 7274 5f6f 7574 7075  _dp(expert_outpu
+000075f0: 742c 2028 302c 2033 2c20 312c 2032 2929  t, (0, 3, 1, 2))
+00007600: 0a20 2020 2020 2020 2065 6c73 653a 0a20  .        else:. 
+00007610: 2020 2020 2020 2020 2020 2023 2065 7870             # exp
+00007620: 6572 745f 696e 7075 7427 7320 7368 6170  ert_input's shap
+00007630: 653a 2028 7365 6c66 2e65 7870 6572 745f  e: (self.expert_
+00007640: 6469 6d2c 2073 656c 662e 6470 5f67 726f  dim, self.dp_gro
+00007650: 7570 2c20 6578 7065 7274 5f63 6170 6163  up, expert_capac
+00007660: 6974 792c 2073 656c 662e 6869 6464 656e  ity, self.hidden
+00007670: 5f73 697a 6529 0a20 2020 2020 2020 2020  _size).         
+00007680: 2020 2065 7870 6572 745f 696e 7075 7420     expert_input 
+00007690: 3d20 7365 6c66 2e74 7261 6e73 706f 7365  = self.transpose
+000076a0: 5f34 6469 6d5f 6470 2865 7870 6572 745f  _4dim_dp(expert_
+000076b0: 696e 7075 742c 2028 302c 2032 2c20 312c  input, (0, 2, 1,
+000076c0: 2033 2929 0a20 2020 2020 2020 2020 2020   3)).           
+000076d0: 2023 2065 7870 6572 745f 6f75 7470 7574   # expert_output
+000076e0: 2773 2073 6861 7065 3a20 2873 656c 662e  's shape: (self.
+000076f0: 6470 5f67 726f 7570 2c20 7365 6c66 2e68  dp_group, self.h
+00007700: 6964 6465 6e5f 7369 7a65 2c20 7365 6c66  idden_size, self
+00007710: 2e65 7870 6572 745f 6469 6d2c 2065 7870  .expert_dim, exp
+00007720: 6572 745f 6361 7061 6369 7479 290a 2020  ert_capacity).  
+00007730: 2020 2020 2020 2020 2020 6966 2073 656c            if sel
+00007740: 662e 636f 6d70 5f63 6f6d 6d5f 7061 7261  f.comp_comm_para
+00007750: 6c6c 656c 3a0a 2020 2020 2020 2020 2020  llel:.          
+00007760: 2020 2020 2020 6578 7065 7274 5f6f 7574        expert_out
+00007770: 7075 7420 3d20 7365 6c66 2e66 666e 5f70  put = self.ffn_p
+00007780: 6172 616c 6c65 6c5f 696e 6665 7228 6578  arallel_infer(ex
+00007790: 7065 7274 5f69 6e70 7574 2c20 6578 7065  pert_input, expe
+000077a0: 7274 5f63 6170 6163 6974 7929 0a20 2020  rt_capacity).   
+000077b0: 2020 2020 2020 2020 2065 6c73 653a 0a20           else:. 
+000077c0: 2020 2020 2020 2020 2020 2020 2020 2065                 e
+000077d0: 7870 6572 745f 6f75 7470 7574 203d 2073  xpert_output = s
+000077e0: 656c 662e 6666 6e5f 696e 6665 7228 6578  elf.ffn_infer(ex
+000077f0: 7065 7274 5f69 6e70 7574 2c20 6578 7065  pert_input, expe
+00007800: 7274 5f63 6170 6163 6974 7929 0a0a 2020  rt_capacity)..  
+00007810: 2020 2020 2020 6578 7065 7274 5f6f 7574        expert_out
+00007820: 7075 7420 3d20 7365 6c66 2e72 6573 6861  put = self.resha
+00007830: 7065 2865 7870 6572 745f 6f75 7470 7574  pe(expert_output
+00007840: 2c20 2873 656c 662e 6470 5f67 726f 7570  , (self.dp_group
+00007850: 2c20 7365 6c66 2e68 6964 6465 6e5f 7369  , self.hidden_si
+00007860: 7a65 2c0a 2020 2020 2020 2020 2020 2020  ze,.            
+00007870: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00007880: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00007890: 2020 2020 2020 2020 2073 656c 662e 6578           self.ex
+000078a0: 7065 7274 5f64 696d 202a 2065 7870 6572  pert_dim * exper
+000078b0: 745f 6361 7061 6369 7479 2929 0a20 2020  t_capacity)).   
+000078c0: 2020 2020 2063 6f6d 6269 6e65 5f74 656e       combine_ten
+000078d0: 736f 7220 3d20 7365 6c66 2e72 6573 6861  sor = self.resha
+000078e0: 7065 2863 6f6d 6269 6e65 5f74 656e 736f  pe(combine_tenso
+000078f0: 722c 2028 7365 6c66 2e64 705f 6772 6f75  r, (self.dp_grou
+00007900: 702c 2074 6f6b 656e 735f 7065 725f 6772  p, tokens_per_gr
+00007910: 6f75 702c 0a20 2020 2020 2020 2020 2020  oup,.           
+00007920: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00007930: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00007940: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+00007950: 2e65 7870 6572 745f 6469 6d20 2a20 6578  .expert_dim * ex
+00007960: 7065 7274 5f63 6170 6163 6974 7929 290a  pert_capacity)).
+00007970: 2020 2020 2020 2020 2320 636f 6d62 696e          # combin
+00007980: 655f 7465 6e73 6f72 2773 2073 6861 7065  e_tensor's shape
+00007990: 3a20 2873 656c 662e 6470 5f67 726f 7570  : (self.dp_group
+000079a0: 2c20 7365 6c66 2e65 7870 6572 745f 6469  , self.expert_di
+000079b0: 6d20 2a20 6578 7065 7274 5f63 6170 6163  m * expert_capac
+000079c0: 6974 792c 2074 6f6b 656e 735f 7065 725f  ity, tokens_per_
+000079d0: 6772 6f75 7029 0a20 2020 2020 2020 2063  group).        c
+000079e0: 6f6d 6269 6e65 5f74 656e 736f 7220 3d20  ombine_tensor = 
+000079f0: 7365 6c66 2e74 7261 6e73 706f 7365 5f33  self.transpose_3
+00007a00: 6469 6d28 636f 6d62 696e 655f 7465 6e73  dim(combine_tens
+00007a10: 6f72 2c20 2830 2c20 322c 2031 2929 0a20  or, (0, 2, 1)). 
+00007a20: 2020 2020 2020 2063 6f6d 6269 6e65 5f74         combine_t
+00007a30: 656e 736f 7220 3d20 7365 6c66 2e63 6173  ensor = self.cas
+00007a40: 7428 636f 6d62 696e 655f 7465 6e73 6f72  t(combine_tensor
+00007a50: 2c20 462e 6474 7970 6528 6578 7065 7274  , F.dtype(expert
+00007a60: 5f6f 7574 7075 7429 290a 0a20 2020 2020  _output))..     
+00007a70: 2020 2023 2063 6f6d 6269 6e65 645f 6f75     # combined_ou
+00007a80: 7470 7574 2773 2073 6861 7065 3a20 2873  tput's shape: (s
+00007a90: 656c 662e 6470 5f67 726f 7570 2c20 7365  elf.dp_group, se
+00007aa0: 6c66 2e68 6964 6465 6e5f 7369 7a65 2c20  lf.hidden_size, 
+00007ab0: 746f 6b65 6e73 5f70 6572 5f67 726f 7570  tokens_per_group
+00007ac0: 290a 2020 2020 2020 2020 636f 6d62 696e  ).        combin
+00007ad0: 6564 5f6f 7574 7075 7420 3d20 7365 6c66  ed_output = self
+00007ae0: 2e62 6174 6368 5f6d 6d32 2865 7870 6572  .batch_mm2(exper
+00007af0: 745f 6f75 7470 7574 2c20 636f 6d62 696e  t_output, combin
+00007b00: 655f 7465 6e73 6f72 290a 2020 2020 2020  e_tensor).      
+00007b10: 2020 2320 636f 6d62 696e 6564 5f6f 7574    # combined_out
+00007b20: 7075 7427 7320 7368 6170 653a 2028 7365  put's shape: (se
+00007b30: 6c66 2e64 705f 6772 6f75 702c 2074 6f6b  lf.dp_group, tok
+00007b40: 656e 735f 7065 725f 6772 6f75 702c 2073  ens_per_group, s
+00007b50: 656c 662e 6869 6464 656e 5f73 697a 6529  elf.hidden_size)
+00007b60: 0a20 2020 2020 2020 2063 6f6d 6269 6e65  .        combine
+00007b70: 645f 6f75 7470 7574 203d 2073 656c 662e  d_output = self.
+00007b80: 7472 616e 7370 6f73 655f 3364 696d 2863  transpose_3dim(c
+00007b90: 6f6d 6269 6e65 645f 6f75 7470 7574 2c20  ombined_output, 
+00007ba0: 2830 2c20 322c 2031 2929 0a20 2020 2020  (0, 2, 1)).     
+00007bb0: 2020 2063 6f6d 6269 6e65 645f 6f75 7470     combined_outp
+00007bc0: 7574 203d 2073 656c 662e 7265 7368 6170  ut = self.reshap
+00007bd0: 6528 636f 6d62 696e 6564 5f6f 7574 7075  e(combined_outpu
+00007be0: 742c 2028 6273 5f61 6e64 5f64 6d6f 6465  t, (bs_and_dmode
+00007bf0: 6c5b 305d 2c20 6273 5f61 6e64 5f64 6d6f  l[0], bs_and_dmo
+00007c00: 6465 6c5b 315d 2929 0a20 2020 2020 2020  del[1])).       
+00007c10: 2063 6f6d 6269 6e65 645f 6f75 7470 7574   combined_output
+00007c20: 203d 2073 656c 662e 7265 7368 6170 6528   = self.reshape(
+00007c30: 636f 6d62 696e 6564 5f6f 7574 7075 742c  combined_output,
+00007c40: 2069 6e70 7574 5f73 6861 7065 290a 0a20   input_shape).. 
+00007c50: 2020 2020 2020 2061 7578 5f6c 6f73 7320         aux_loss 
+00007c60: 3d20 7365 6c66 2e6d 756c 2873 656c 662e  = self.mul(self.
+00007c70: 6175 785f 6c6f 7373 5f66 6163 746f 722c  aux_loss_factor,
+00007c80: 2061 7578 5f6c 6f73 7329 0a20 2020 2020   aux_loss).     
+00007c90: 2020 2072 6574 7572 6e20 636f 6d62 696e     return combin
+00007ca0: 6564 5f6f 7574 7075 742c 2061 7578 5f6c  ed_output, aux_l
+00007cb0: 6f73 730a 0a0a 636c 6173 7320 4d6f 4556  oss...class MoEV
+00007cc0: 3228 4365 6c6c 293a 0a20 2020 2022 2222  2(Cell):.    """
+00007cd0: 0a20 2020 2054 6865 206d 6978 7475 7265  .    The mixture
+00007ce0: 206f 6620 6578 7065 7274 7320 284d 6f45   of experts (MoE
+00007cf0: 2920 696d 706c 656d 656e 7461 7469 6f6e  ) implementation
+00007d00: 2e20 5468 6520 696d 706c 656d 656e 7461  . The implementa
+00007d10: 7469 6f6e 2069 6e63 6c75 6465 7320 6120  tion includes a 
+00007d20: 726f 7574 6572 2061 6e64 2061 2046 6565  router and a Fee
+00007d30: 6446 6f72 7761 7264 206c 6179 6572 2e0a  dForward layer..
+00007d40: 2020 2020 5468 6520 726f 7574 6572 2064      The router d
+00007d50: 6973 7061 7463 6865 7320 746f 6b65 6e73  ispatches tokens
+00007d60: 2074 6f20 6578 7065 7274 7320 696e 2046   to experts in F
+00007d70: 6565 6446 6f72 7761 7264 2c20 7468 656e  eedForward, then
+00007d80: 2046 6565 6446 6f72 7761 7264 2064 6f65   FeedForward doe
+00007d90: 7320 636f 6d70 7574 6174 696f 6e2c 2061  s computation, a
+00007da0: 6e64 2074 6865 2066 696e 616c 206f 7574  nd the final out
+00007db0: 7075 7420 6973 0a20 2020 206f 6274 6169  put is.    obtai
+00007dc0: 6e65 6420 6279 206d 756c 7469 706c 7969  ned by multiplyi
+00007dd0: 6e67 2046 6565 6446 6f72 7761 7264 2773  ng FeedForward's
+00007de0: 206f 7574 7075 7420 616e 6420 726f 7574   output and rout
+00007df0: 6572 2773 2063 6f6d 6269 6e65 2077 6569  er's combine wei
+00007e00: 6768 742e 0a20 2020 2054 6869 7320 6973  ght..    This is
+00007e10: 2061 2063 6f6d 6d6f 6e20 696e 7465 7266   a common interf
+00007e20: 6163 652c 2077 6869 6368 2061 6c6c 6f77  ace, which allow
+00007e30: 7320 616e 7920 6666 6e20 636c 6173 7320  s any ffn class 
+00007e40: 616e 6420 616e 7920 526f 7574 6572 2061  and any Router a
+00007e50: 6c67 6f72 6974 686d 2869 6d70 6c65 6d65  lgorithm(impleme
+00007e60: 6e74 6564 2069 6e20 5632 2066 6f72 6d29  nted in V2 form)
+00007e70: 2e0a 0a20 2020 2041 7267 733a 0a20 2020  ...    Args:.   
+00007e80: 2020 2020 2068 6964 6465 6e5f 7369 7a65       hidden_size
+00007e90: 2028 696e 7429 3a20 5468 6520 6469 6d65   (int): The dime
+00007ea0: 6e73 696f 6e20 6f66 2074 6865 2069 6e70  nsion of the inp
+00007eb0: 7574 732e 0a20 2020 2020 2020 2066 666e  uts..        ffn
+00007ec0: 5f68 6964 6465 6e5f 7369 7a65 2028 696e  _hidden_size (in
+00007ed0: 7429 3a20 5468 6520 696e 7465 726d 6564  t): The intermed
+00007ee0: 6961 7465 2068 6964 6465 6e20 7369 7a65  iate hidden size
+00007ef0: 2e0a 2020 2020 2020 2020 6472 6f70 6f75  ..        dropou
+00007f00: 745f 7261 7465 2028 666c 6f61 7429 3a20  t_rate (float): 
+00007f10: 5468 6520 6472 6f70 6f75 7420 7261 7465  The dropout rate
+00007f20: 2066 6f72 2074 6865 2073 6563 6f6e 6420   for the second 
+00007f30: 6c69 6e65 6172 2773 206f 7574 7075 742e  linear's output.
+00007f40: 0a20 2020 2020 2020 2068 6964 6465 6e5f  .        hidden_
+00007f50: 6163 7420 2873 7472 293a 2054 6865 2061  act (str): The a
+00007f60: 6374 6976 6174 696f 6e20 6f66 2074 6865  ctivation of the
+00007f70: 2069 6e74 6572 6e61 6c20 6665 6564 666f   internal feedfo
+00007f80: 7277 6172 6420 6c61 7965 722e 2053 7570  rward layer. Sup
+00007f90: 706f 7274 7320 2772 656c 7527 2c0a 2020  ports 'relu',.  
+00007fa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00007fb0: 2020 2020 2020 2027 7265 6c75 3627 2c20         'relu6', 
+00007fc0: 2774 616e 6827 2c20 2767 656c 7527 2c20  'tanh', 'gelu', 
+00007fd0: 2766 6173 745f 6765 6c75 272c 2027 656c  'fast_gelu', 'el
+00007fe0: 7527 2c20 2773 6967 6d6f 6964 272c 2027  u', 'sigmoid', '
+00007ff0: 7072 656c 7527 2c20 276c 6561 6b79 7265  prelu', 'leakyre
+00008000: 6c75 272c 2027 6873 7769 7368 272c 0a20  lu', 'hswish',. 
+00008010: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00008020: 2020 2020 2020 2020 2768 7369 676d 6f69          'hsigmoi
+00008030: 6427 2c20 276c 6f67 7369 676d 6f69 6427  d', 'logsigmoid'
+00008040: 2061 6e64 2073 6f20 6f6e 2e20 4465 6661   and so on. Defa
+00008050: 756c 743a 2067 656c 752e 0a20 2020 2020  ult: gelu..     
+00008060: 2020 2070 6172 616d 5f69 6e69 745f 7479     param_init_ty
+00008070: 7065 2028 6474 7970 652e 4e75 6d62 6572  pe (dtype.Number
+00008080: 293a 2054 6865 2070 6172 616d 6574 6572  ): The parameter
+00008090: 2069 6e69 7469 616c 697a 6174 696f 6e20   initialization 
+000080a0: 7479 7065 2e20 4361 6e20 6265 2064 7479  type. Can be dty
+000080b0: 7065 2e66 6c6f 6174 3332 206f 7220 6474  pe.float32 or dt
+000080c0: 7970 652e 666c 6f61 7431 362e 0a20 2020  ype.float16..   
+000080d0: 2020 2020 206d 6f65 5f63 6f6e 6669 6728       moe_config(
+000080e0: 4d6f 4543 6f6e 6669 6729 3a20 5468 6520  MoEConfig): The 
+000080f0: 636f 6e66 6967 7572 6174 696f 6e20 6f66  configuration of
+00008100: 204d 6f45 2028 4d69 7874 7572 6520 6f66   MoE (Mixture of
+00008110: 2045 7870 6572 7429 2e20 4465 6661 756c   Expert). Defaul
+00008120: 7420 6973 2061 6e20 696e 7374 616e 6365  t is an instance
+00008130: 206f 6620 4d6f 4543 6f6e 6669 6720 7769   of MoEConfig wi
+00008140: 7468 0a20 2020 2020 2020 2020 2020 2064  th.            d
+00008150: 6566 6175 6c74 2076 616c 7565 732e 2050  efault values. P
+00008160: 6c65 6173 6520 7365 6520 604d 6f45 436f  lease see `MoECo
+00008170: 6e66 6967 602e 0a20 2020 2020 2020 2070  nfig`..        p
+00008180: 6172 616c 6c65 6c5f 636f 6e66 6967 284d  arallel_config(M
+00008190: 6f45 5061 7261 6c6c 656c 436f 6e66 6967  oEParallelConfig
+000081a0: 293a 2054 6865 2070 6172 616c 6c65 6c20  ): The parallel 
+000081b0: 636f 6e66 6967 2066 6f72 204d 6f45 2c20  config for MoE, 
+000081c0: 7365 6520 604d 6f45 5061 7261 6c6c 656c  see `MoEParallel
+000081d0: 436f 6e66 6967 602e 0a20 2020 2020 2020  Config`..       
+000081e0: 2020 2020 2044 6566 6175 6c74 2060 6465       Default `de
+000081f0: 6661 756c 745f 6d6f 6570 6172 616c 6c65  fault_moeparalle
+00008200: 6c5f 636f 6e66 6967 602c 2061 6e20 696e  l_config`, an in
+00008210: 7374 616e 6365 206f 6620 604d 6f45 5061  stance of `MoEPa
+00008220: 7261 6c6c 656c 436f 6e66 6967 6020 7769  rallelConfig` wi
+00008230: 7468 2064 6566 6175 6c74 2061 7267 732e  th default args.
+00008240: 0a0a 2020 2020 496e 7075 7473 3a0a 2020  ..    Inputs:.  
+00008250: 2020 2020 2020 2d20 2a2a 782a 2a20 2854        - **x** (T
+00008260: 656e 736f 7229 202d 2073 686f 756c 6420  ensor) - should 
+00008270: 6265 2060 5b62 6174 6368 2c20 7365 715f  be `[batch, seq_
+00008280: 6c65 6e67 7468 2c20 6869 6464 656e 5f73  length, hidden_s
+00008290: 697a 655d 602e 2046 6c6f 6174 2074 656e  ize]`. Float ten
+000082a0: 736f 722e 0a0a 2020 2020 4f75 7470 7574  sor...    Output
+000082b0: 733a 0a20 2020 2020 2020 2054 656e 736f  s:.        Tenso
+000082c0: 722c 2074 6865 206f 7574 7075 7420 6f66  r, the output of
+000082d0: 2074 6869 7320 6c61 7965 7220 6166 7465   this layer afte
+000082e0: 7220 6d61 7070 696e 672e 2054 6865 2073  r mapping. The s
+000082f0: 6861 7065 2069 7320 605b 6261 7463 682c  hape is `[batch,
+00008300: 2073 6571 5f6c 656e 6774 682c 2068 6964   seq_length, hid
+00008310: 6465 6e5f 7369 7a65 5d60 2e0a 2020 2020  den_size]`..    
+00008320: 2222 220a 2020 2020 6465 6620 5f5f 696e  """.    def __in
+00008330: 6974 5f5f 2873 656c 662c 0a20 2020 2020  it__(self,.     
+00008340: 2020 2020 2020 2020 2020 2020 6666 6e2c              ffn,
+00008350: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00008360: 2020 6469 6d2c 0a20 2020 2020 2020 2020    dim,.         
+00008370: 2020 2020 2020 2020 6d6f 655f 636f 6e66          moe_conf
+00008380: 6967 3d64 6566 6175 6c74 5f6d 6f65 5f63  ig=default_moe_c
+00008390: 6f6e 6669 672c 0a20 2020 2020 2020 2020  onfig,.         
+000083a0: 2020 2020 2020 2020 7061 7261 6c6c 656c          parallel
+000083b0: 5f63 6f6e 6669 673d 6465 6661 756c 745f  _config=default_
+000083c0: 6d6f 6570 6172 616c 6c65 6c5f 636f 6e66  moeparallel_conf
+000083d0: 6967 293a 0a20 2020 2020 2020 2073 7570  ig):.        sup
+000083e0: 6572 284d 6f45 5632 2c20 7365 6c66 292e  er(MoEV2, self).
+000083f0: 5f5f 696e 6974 5f5f 2829 0a20 2020 2020  __init__().     
+00008400: 2020 2073 656c 662e 6869 6464 656e 5f73     self.hidden_s
+00008410: 697a 6520 3d20 6469 6d0a 2020 2020 2020  ize = dim.      
+00008420: 2020 7365 6c66 2e65 7870 6572 745f 6469    self.expert_di
+00008430: 6d20 3d20 6d6f 655f 636f 6e66 6967 2e65  m = moe_config.e
+00008440: 7870 6572 745f 6e75 6d0a 2020 2020 2020  xpert_num.      
+00008450: 2020 7365 6c66 2e63 6170 6163 6974 795f    self.capacity_
+00008460: 6661 6374 6f72 203d 206d 6f65 5f63 6f6e  factor = moe_con
+00008470: 6669 672e 6361 7061 6369 7479 5f66 6163  fig.capacity_fac
+00008480: 746f 720a 2020 2020 2020 2020 7365 6c66  tor.        self
+00008490: 2e6e 756d 5f65 7870 6572 7473 5f63 686f  .num_experts_cho
+000084a0: 7365 6e20 3d20 6d6f 655f 636f 6e66 6967  sen = moe_config
+000084b0: 2e6e 756d 5f65 7870 6572 7473 5f63 686f  .num_experts_cho
+000084c0: 7365 6e0a 2020 2020 2020 2020 7365 6c66  sen.        self
+000084d0: 2e64 705f 6772 6f75 7020 3d20 7061 7261  .dp_group = para
+000084e0: 6c6c 656c 5f63 6f6e 6669 672e 6461 7461  llel_config.data
+000084f0: 5f70 6172 616c 6c65 6c0a 2020 2020 2020  _parallel.      
+00008500: 2020 7365 6c66 2e64 7020 3d20 7061 7261    self.dp = para
+00008510: 6c6c 656c 5f63 6f6e 6669 672e 6461 7461  llel_config.data
+00008520: 5f70 6172 616c 6c65 6c0a 2020 2020 2020  _parallel.      
+00008530: 2020 7365 6c66 2e65 7020 3d20 7061 7261    self.ep = para
+00008540: 6c6c 656c 5f63 6f6e 6669 672e 6578 7065  llel_config.expe
+00008550: 7274 5f70 6172 616c 6c65 6c0a 2020 2020  rt_parallel.    
+00008560: 2020 2020 7365 6c66 2e6d 7020 3d20 7061      self.mp = pa
+00008570: 7261 6c6c 656c 5f63 6f6e 6669 672e 6d6f  rallel_config.mo
+00008580: 6465 6c5f 7061 7261 6c6c 656c 0a20 2020  del_parallel.   
+00008590: 2020 2020 2073 656c 662e 6470 5f72 616e       self.dp_ran
+000085a0: 6765 203d 2054 656e 736f 7228 6e70 2e61  ge = Tensor(np.a
+000085b0: 7261 6e67 6528 7365 6c66 2e64 705f 6772  range(self.dp_gr
+000085c0: 6f75 7029 2e72 6573 6861 7065 282d 312c  oup).reshape(-1,
+000085d0: 2031 292c 206d 7374 7970 652e 696e 7433   1), mstype.int3
+000085e0: 3229 2023 2028 6470 2c20 3129 203d 205b  2) # (dp, 1) = [
+000085f0: 5b30 5d2c 5b31 5d2c 5b32 5d2e 2e2e 5b64  [0],[1],[2]...[d
+00008600: 705d 5d0a 0a20 2020 2020 2020 2073 656c  p]]..        sel
+00008610: 662e 6666 6e20 3d20 6666 6e0a 2020 2020  f.ffn = ffn.    
+00008620: 2020 2020 5661 6c69 6461 746f 722e 6368      Validator.ch
+00008630: 6563 6b5f 7374 7269 6e67 286d 6f65 5f63  eck_string(moe_c
+00008640: 6f6e 6669 672e 726f 7574 696e 675f 706f  onfig.routing_po
+00008650: 6c69 6379 2c20 5b22 546f 706b 526f 7574  licy, ["TopkRout
+00008660: 6572 5632 225d 2c20 2272 6f75 7469 6e67  erV2"], "routing
+00008670: 5f70 6f6c 6963 7922 290a 2020 2020 2020  _policy").      
+00008680: 2020 7365 6c66 2e72 6f75 7465 7220 3d20    self.router = 
+00008690: 526f 7574 6572 2864 5f6d 6f64 656c 3d73  Router(d_model=s
+000086a0: 656c 662e 6869 6464 656e 5f73 697a 652c  elf.hidden_size,
+000086b0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+000086c0: 2020 2020 2020 2020 2020 2020 2020 6d6f                mo
+000086d0: 655f 636f 6e66 6967 3d6d 6f65 5f63 6f6e  e_config=moe_con
+000086e0: 6669 672c 0a20 2020 2020 2020 2020 2020  fig,.           
+000086f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00008700: 2020 726f 7574 696e 675f 706f 6c69 6379    routing_policy
+00008710: 3d6d 6f65 5f63 6f6e 6669 672e 726f 7574  =moe_config.rout
+00008720: 696e 675f 706f 6c69 6379 2c0a 2020 2020  ing_policy,.    
+00008730: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00008740: 2020 2020 2020 2020 2074 7261 696e 696e           trainin
+00008750: 673d 5472 7565 2c0a 2020 2020 2020 2020  g=True,.        
+00008760: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00008770: 2020 2020 2070 6172 616c 6c65 6c5f 636f       parallel_co
+00008780: 6e66 6967 3d70 6172 616c 6c65 6c5f 636f  nfig=parallel_co
+00008790: 6e66 6967 290a 0a20 2020 2020 2020 2073  nfig)..        s
+000087a0: 656c 662e 7265 7368 6170 6520 3d20 502e  elf.reshape = P.
+000087b0: 5265 7368 6170 6528 290a 2020 2020 2020  Reshape().      
+000087c0: 2020 7365 6c66 2e73 6861 7065 203d 2050    self.shape = P
+000087d0: 2e53 6861 7065 2829 0a20 2020 2020 2020  .Shape().       
+000087e0: 2073 656c 662e 6361 7374 203d 2050 2e43   self.cast = P.C
+000087f0: 6173 7428 290a 2020 2020 2020 2020 7365  ast().        se
+00008800: 6c66 2e74 7261 6e73 706f 7365 5f34 6469  lf.transpose_4di
+00008810: 6d5f 6470 3120 3d20 502e 5472 616e 7370  m_dp1 = P.Transp
+00008820: 6f73 6528 292e 7368 6172 6428 2828 312c  ose().shard(((1,
+00008830: 2073 656c 662e 6470 2c20 312c 2031 292c   self.dp, 1, 1),
+00008840: 2929 0a0a 2020 2020 6465 6620 6666 6e5f  ))..    def ffn_
+00008850: 696e 6665 7228 7365 6c66 2c20 6578 7065  infer(self, expe
+00008860: 7274 5f69 6e70 7574 293a 0a20 2020 2020  rt_input):.     
+00008870: 2020 2022 2222 0a20 2020 2020 2020 2043     """.        C
+00008880: 6f6d 7075 7469 6e67 2074 6865 2046 464e  omputing the FFN
+00008890: 2e0a 2020 2020 2020 2020 2222 220a 2020  ..        """.  
+000088a0: 2020 2020 2020 6578 7065 7274 5f69 6e70        expert_inp
+000088b0: 7574 203d 2073 656c 662e 7265 7368 6170  ut = self.reshap
+000088c0: 6528 6578 7065 7274 5f69 6e70 7574 2c20  e(expert_input, 
+000088d0: 282d 312c 2073 656c 662e 6869 6464 656e  (-1, self.hidden
+000088e0: 5f73 697a 6529 2920 2320 2845 2a64 702a  _size)) # (E*dp*
+000088f0: 6e2c 2068 2920 3c2d 2d20 2845 2c20 6470  n, h) <-- (E, dp
+00008900: 2c20 6e2c 2068 2920 233c 3c3c 3c3c 3c3c  , n, h) #<<<<<<<
+00008910: 3c3c 0a20 2020 2020 2020 2065 7870 6572  <<.        exper
+00008920: 745f 6f75 7470 7574 203d 2073 656c 662e  t_output = self.
+00008930: 6666 6e28 6578 7065 7274 5f69 6e70 7574  ffn(expert_input
+00008940: 2920 2320 2845 2c20 6470 2a6e 2c20 6829  ) # (E, dp*n, h)
+00008950: 203c 2d2d 2028 452a 6470 2a6e 2c20 6829   <-- (E*dp*n, h)
+00008960: 0a20 2020 2020 2020 2065 7870 6572 745f  .        expert_
+00008970: 6f75 7470 7574 203d 2073 656c 662e 7265  output = self.re
+00008980: 7368 6170 6528 6578 7065 7274 5f6f 7574  shape(expert_out
+00008990: 7075 742c 2028 7365 6c66 2e65 7870 6572  put, (self.exper
+000089a0: 745f 6469 6d2c 2073 656c 662e 6470 5f67  t_dim, self.dp_g
+000089b0: 726f 7570 2c20 2d31 2c20 7365 6c66 2e68  roup, -1, self.h
+000089c0: 6964 6465 6e5f 7369 7a65 2929 2023 2028  idden_size)) # (
+000089d0: 452c 2064 702c 206e 2c20 6829 203c 2d2d  E, dp, n, h) <--
+000089e0: 2028 452c 2064 702a 6e2c 2068 290a 2020   (E, dp*n, h).  
+000089f0: 2020 2020 2020 7265 7475 726e 2065 7870        return exp
+00008a00: 6572 745f 6f75 7470 7574 0a0a 0a20 2020  ert_output...   
+00008a10: 2064 6566 2063 6f6e 7374 7275 6374 2873   def construct(s
+00008a20: 656c 662c 2069 6e70 7574 5f74 656e 736f  elf, input_tenso
+00008a30: 7229 3a0a 2020 2020 2020 2020 2222 2266  r):.        """f
+00008a40: 6f72 7761 7264 2070 726f 6365 7373 2222  orward process""
+00008a50: 220a 2020 2020 2020 2020 696e 7075 745f  ".        input_
+00008a60: 7465 6e73 6f72 5f73 6861 7065 203d 2073  tensor_shape = s
+00008a70: 656c 662e 7368 6170 6528 696e 7075 745f  elf.shape(input_
+00008a80: 7465 6e73 6f72 290a 2020 2020 2020 2020  tensor).        
+00008a90: 696e 7075 745f 7465 6e73 6f72 203d 2073  input_tensor = s
+00008aa0: 656c 662e 6361 7374 2869 6e70 7574 5f74  elf.cast(input_t
+00008ab0: 656e 736f 722c 206d 7374 7970 652e 666c  ensor, mstype.fl
+00008ac0: 6f61 7431 3629 0a20 2020 2020 2020 2069  oat16).        i
+00008ad0: 6e70 7574 5f74 656e 736f 7220 3d20 7365  nput_tensor = se
+00008ae0: 6c66 2e72 6573 6861 7065 2869 6e70 7574  lf.reshape(input
+00008af0: 5f74 656e 736f 722c 2028 7365 6c66 2e64  _tensor, (self.d
+00008b00: 705f 6772 6f75 702c 202d 312c 2073 656c  p_group, -1, sel
+00008b10: 662e 6869 6464 656e 5f73 697a 6529 2920  f.hidden_size)) 
+00008b20: 2320 2864 702c 204e 2c20 6829 203c 2d2d  # (dp, N, h) <--
+00008b30: 2028 422a 532c 2068 290a 0a20 2020 2020   (B*S, h)..     
+00008b40: 2020 2023 2063 616c 6375 6c61 7465 2072     # calculate r
+00008b50: 6f75 7465 720a 2020 2020 2020 2020 6469  outer.        di
+00008b60: 7370 6174 6368 5f70 6f6c 6963 792c 2063  spatch_policy, c
+00008b70: 6f6d 6269 6e65 5f70 6f6c 6963 792c 2072  ombine_policy, r
+00008b80: 6f75 7465 725f 636f 6566 6620 3d20 7365  outer_coeff = se
+00008b90: 6c66 2e72 6f75 7465 7228 696e 7075 745f  lf.router(input_
+00008ba0: 7465 6e73 6f72 2920 2320 2864 702c 2045  tensor) # (dp, E
+00008bb0: 2c20 6e29 696e 7433 322c 2028 6470 2c20  , n)int32, (dp, 
+00008bc0: 4e2c 206b 2969 6e74 3332 2c20 2864 702c  N, k)int32, (dp,
+00008bd0: 204e 2c20 6b29 6670 3136 203c 2d2d 2028   N, k)fp16 <-- (
+00008be0: 6470 2c20 4e2c 2068 292c 2077 6865 7265  dp, N, h), where
+00008bf0: 2030 3c3d 2064 6973 7061 7463 685f 696e   0<= dispatch_in
+00008c00: 6465 7820 3c20 312b 4e2c 2030 3c3d 2063  dex < 1+N, 0<= c
+00008c10: 6f6d 6269 6e65 5f69 6e64 6578 203c 452a  ombine_index <E*
+00008c20: 2831 2b6e 290a 0a20 2020 2020 2020 2023  (1+n)..        #
+00008c30: 2064 6973 7061 7463 680a 2020 2020 2020   dispatch.      
+00008c40: 2020 6578 7065 7274 5f69 6e70 7574 203d    expert_input =
+00008c50: 2073 656c 662e 726f 7574 6572 2e72 6f75   self.router.rou
+00008c60: 7465 722e 6469 7370 6174 6368 2869 6e70  ter.dispatch(inp
+00008c70: 7574 5f74 656e 736f 722c 2064 6973 7061  ut_tensor, dispa
+00008c80: 7463 685f 706f 6c69 6379 2920 2320 2864  tch_policy) # (d
+00008c90: 702c 2045 2c20 6e2c 2068 2920 3c2d 2d20  p, E, n, h) <-- 
+00008ca0: 2864 702c 204e 2c20 6829 2c20 2864 702c  (dp, N, h), (dp,
+00008cb0: 2045 2c20 6e29 0a0a 2020 2020 2020 2020   E, n)..        
+00008cc0: 2320 6666 6e0a 2020 2020 2020 2020 6578  # ffn.        ex
+00008cd0: 7065 7274 5f69 6e70 7574 203d 2073 656c  pert_input = sel
+00008ce0: 662e 7472 616e 7370 6f73 655f 3464 696d  f.transpose_4dim
+00008cf0: 5f64 7031 2865 7870 6572 745f 696e 7075  _dp1(expert_inpu
+00008d00: 742c 2028 312c 2030 2c20 322c 2033 2929  t, (1, 0, 2, 3))
+00008d10: 2023 2028 452c 2064 702c 206e 2c20 6829   # (E, dp, n, h)
+00008d20: 203c 2d2d 2028 6470 2c20 452c 206e 2c20   <-- (dp, E, n, 
+00008d30: 6829 0a20 2020 2020 2020 2065 7870 6572  h).        exper
+00008d40: 745f 6f75 7470 7574 203d 2073 656c 662e  t_output = self.
+00008d50: 6666 6e5f 696e 6665 7228 6578 7065 7274  ffn_infer(expert
+00008d60: 5f69 6e70 7574 2920 2320 2845 2c20 6470  _input) # (E, dp
+00008d70: 2c20 6e2c 2068 2920 3c2d 2d20 2845 2c20  , n, h) <-- (E, 
+00008d80: 6470 2c20 6e2c 2068 290a 2020 2020 2020  dp, n, h).      
+00008d90: 2020 6578 7065 7274 5f6f 7574 7075 7420    expert_output 
+00008da0: 3d20 7365 6c66 2e74 7261 6e73 706f 7365  = self.transpose
+00008db0: 5f34 6469 6d5f 6470 3128 6578 7065 7274  _4dim_dp1(expert
+00008dc0: 5f6f 7574 7075 742c 2028 312c 2030 2c20  _output, (1, 0, 
+00008dd0: 322c 2033 2929 2023 2028 6470 2c20 452c  2, 3)) # (dp, E,
+00008de0: 206e 2c20 6829 203c 2d2d 2028 452c 2064   n, h) <-- (E, d
+00008df0: 702c 206e 2c20 6829 0a0a 2020 2020 2020  p, n, h)..      
+00008e00: 2020 2320 636f 6d62 696e 650a 2020 2020    # combine.    
+00008e10: 2020 2020 6f75 7470 7574 5f74 656e 736f      output_tenso
+00008e20: 7220 3d20 7365 6c66 2e72 6f75 7465 722e  r = self.router.
+00008e30: 726f 7574 6572 2e63 6f6d 6269 6e65 2865  router.combine(e
+00008e40: 7870 6572 745f 6f75 7470 7574 2c20 636f  xpert_output, co
+00008e50: 6d62 696e 655f 706f 6c69 6379 2c20 726f  mbine_policy, ro
+00008e60: 7574 6572 5f63 6f65 6666 2920 2320 2864  uter_coeff) # (d
+00008e70: 702c 204e 2c20 6b2c 2068 2920 3c2d 2d20  p, N, k, h) <-- 
+00008e80: 2864 702c 2045 2a28 312b 6e29 2c20 6829  (dp, E*(1+n), h)
+00008e90: 2c20 2864 702c 204e 2c20 6b29 0a20 2020  , (dp, N, k).   
+00008ea0: 2020 2020 206f 7574 7075 745f 7465 6e73       output_tens
+00008eb0: 6f72 203d 2073 656c 662e 7265 7368 6170  or = self.reshap
+00008ec0: 6528 6f75 7470 7574 5f74 656e 736f 722c  e(output_tensor,
+00008ed0: 2069 6e70 7574 5f74 656e 736f 725f 7368   input_tensor_sh
+00008ee0: 6170 6529 2023 2028 422a 532c 2068 2920  ape) # (B*S, h) 
+00008ef0: 3c2d 2d20 2864 702c 204e 2c20 6829 0a20  <-- (dp, N, h). 
+00008f00: 2020 2020 2020 2072 6574 7572 6e20 6f75         return ou
+00008f10: 7470 7574 5f74 656e 736f 7220 2320 2864  tput_tensor # (d
+00008f20: 702c 204e 2c20 6829 0a0a 0a63 6c61 7373  p, N, h)...class
+00008f30: 2052 6f75 7465 7228 4365 6c6c 293a 0a20   Router(Cell):. 
+00008f40: 2020 2072 2222 220a 2020 2020 2020 2020     r""".        
+00008f50: 4120 726f 7574 6572 2062 6163 6b62 6f6e  A router backbon
+00008f60: 6520 7573 6564 2074 6f20 6361 6c63 756c  e used to calcul
+00008f70: 6174 6520 6c6f 6769 7473 206f 6620 6561  ate logits of ea
+00008f80: 6368 2074 6f6b 656e 2c20 7768 6963 6820  ch token, which 
+00008f90: 7368 6f75 6c64 2062 6520 6361 7363 6164  should be cascad
+00008fa0: 6564 2062 7920 726f 7574 6572 2069 6d70  ed by router imp
+00008fb0: 6c65 6d65 6e74 6174 696f 6e73 0a20 2020  lementations.   
+00008fc0: 2020 2020 206d 6170 7069 6e67 2074 6f6b       mapping tok
+00008fd0: 656e 7320 746f 2065 7870 6572 7473 2e0a  ens to experts..
+00008fe0: 2020 2020 2020 2020 7768 656e 206d 6f65          when moe
+00008ff0: 5f63 6f6e 6669 672e 6e75 6d5f 6578 7065  _config.num_expe
+00009000: 7274 735f 6368 6f73 656e 203d 2031 2c20  rts_chosen = 1, 
+00009010: 7573 6520 746f 7031 2072 6f75 7469 6e67  use top1 routing
+00009020: 3b0a 2020 2020 2020 2020 7768 656e 206d  ;.        when m
+00009030: 6f65 5f63 6f6e 6669 672e 6e75 6d5f 6578  oe_config.num_ex
+00009040: 7065 7274 735f 6368 6f73 656e 203e 2031  perts_chosen > 1
+00009050: 2c20 7573 6520 746f 706b 2072 6f75 7469  , use topk routi
+00009060: 6e67 0a0a 2020 2020 2020 2020 4172 6773  ng..        Args
+00009070: 3a0a 2020 2020 2020 2020 2020 2020 645f  :.            d_
+00009080: 6d6f 6465 6c20 2869 6e74 293a 2054 6865  model (int): The
+00009090: 2068 6964 6465 6e20 7369 7a65 206f 6620   hidden size of 
+000090a0: 6561 6368 2074 6f6b 656e 2e0a 2020 2020  each token..    
+000090b0: 2020 2020 2020 2020 6d6f 655f 636f 6e66          moe_conf
+000090c0: 6967 284d 6f45 436f 6e66 6967 293a 2054  ig(MoEConfig): T
+000090d0: 6865 2063 6f6e 6669 6775 7261 7469 6f6e  he configuration
+000090e0: 206f 6620 4d6f 4520 284d 6978 7475 7265   of MoE (Mixture
+000090f0: 206f 6620 4578 7065 7274 292e 0a20 2020   of Expert)..   
+00009100: 2020 2020 2020 2020 2072 6f75 7469 6e67           routing
+00009110: 5f70 6f6c 6963 793a 2054 6865 2070 6f6c  _policy: The pol
+00009120: 6963 7920 6f66 206d 6170 7069 6e67 2074  icy of mapping t
+00009130: 6f6b 656e 7320 746f 2065 7870 6572 7473  okens to experts
+00009140: 2e20 4465 6661 756c 743a 2074 6f70 6b52  . Default: topkR
+00009150: 6f75 7465 720a 2020 2020 2020 2020 2020  outer.          
+00009160: 2020 7472 6169 6e69 6e67 2028 626f 6f6c    training (bool
+00009170: 293a 2054 6865 2076 616c 7565 2069 6e64  ): The value ind
+00009180: 6963 6174 696e 6720 7768 6574 6865 7220  icating whether 
+00009190: 6973 2069 6e20 7472 6169 6e69 6e67 2070  is in training p
+000091a0: 6861 7365 2e0a 2020 2020 2020 2020 2020  hase..          
+000091b0: 2020 7061 7261 6c6c 656c 5f63 6f6e 6669    parallel_confi
+000091c0: 673a 2054 6865 2070 6172 616c 6c65 6c2d  g: The parallel-
+000091d0: 7265 6c61 7465 6420 636f 6e66 6967 7572  related configur
+000091e0: 6174 696f 6e2e 0a20 2020 2020 2020 2049  ation..        I
+000091f0: 6e70 7574 733a 0a20 2020 2020 2020 2020  nputs:.         
+00009200: 2020 202d 202a 2a69 6e70 7574 5f74 656e     - **input_ten
+00009210: 736f 722a 2a20 2854 656e 736f 7229 202d  sor** (Tensor) -
+00009220: 2054 656e 736f 7220 6f66 2073 6861 7065   Tensor of shape
+00009230: 203a 6d61 7468 3a60 2865 7870 6572 745c   :math:`(expert\
+00009240: 5f70 6172 616c 6c65 6c2c 2074 6f6b 656e  _parallel, token
+00009250: 735c 5f70 6572 5c5f 6465 7669 6365 2c0a  s\_per\_device,.
+00009260: 2020 2020 2020 2020 2020 2020 6869 6464              hidd
+00009270: 656e 5c5f 7369 7a65 2960 2e0a 0a20 2020  en\_size)`...   
+00009280: 2020 2020 204f 7574 7075 7473 3a0a 2020       Outputs:.  
+00009290: 2020 2020 2020 2020 2020 5465 6e73 6f72            Tensor
+000092a0: 206f 6620 7368 6170 6520 3a6d 6174 683a   of shape :math:
+000092b0: 6028 6578 7065 7274 5c5f 7061 7261 6c6c  `(expert\_parall
+000092c0: 656c 2c20 746f 6b65 6e73 5c5f 7065 725c  el, tokens\_per\
+000092d0: 5f64 6576 6963 652c 2065 7870 6572 745c  _device, expert\
+000092e0: 5f64 696d 2960 2e0a 2020 2020 2222 220a  _dim)`..    """.
+000092f0: 0a20 2020 2064 6566 205f 5f69 6e69 745f  .    def __init_
+00009300: 5f28 7365 6c66 2c0a 2020 2020 2020 2020  _(self,.        
+00009310: 2020 2020 2020 2020 2064 5f6d 6f64 656c           d_model
+00009320: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00009330: 2020 206d 6f65 5f63 6f6e 6669 672c 0a20     moe_config,. 
+00009340: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00009350: 726f 7574 696e 675f 706f 6c69 6379 3d4e  routing_policy=N
+00009360: 6f6e 652c 0a20 2020 2020 2020 2020 2020  one,.           
+00009370: 2020 2020 2020 7472 6169 6e69 6e67 3d54        training=T
+00009380: 7275 652c 0a20 2020 2020 2020 2020 2020  rue,.           
+00009390: 2020 2020 2020 7061 7261 6c6c 656c 5f63        parallel_c
+000093a0: 6f6e 6669 673d 4e6f 6e65 293a 0a20 2020  onfig=None):.   
+000093b0: 2020 2020 2073 7570 6572 2852 6f75 7465       super(Route
+000093c0: 722c 2073 656c 6629 2e5f 5f69 6e69 745f  r, self).__init_
+000093d0: 5f28 290a 2020 2020 2020 2020 6470 203d  _().        dp =
+000093e0: 2070 6172 616c 6c65 6c5f 636f 6e66 6967   parallel_config
+000093f0: 2e64 6174 615f 7061 7261 6c6c 656c 0a20  .data_parallel. 
+00009400: 2020 2020 2020 2073 656c 662e 645f 6d6f         self.d_mo
+00009410: 6465 6c20 3d20 645f 6d6f 6465 6c0a 2020  del = d_model.  
+00009420: 2020 2020 2020 7365 6c66 2e65 7870 6572        self.exper
+00009430: 745f 6469 6d20 3d20 6d6f 655f 636f 6e66  t_dim = moe_conf
+00009440: 6967 2e65 7870 6572 745f 6e75 6d0a 2020  ig.expert_num.  
+00009450: 2020 2020 2020 7365 6c66 2e63 6170 6163        self.capac
+00009460: 6974 795f 6661 6374 6f72 203d 206d 6f65  ity_factor = moe
+00009470: 5f63 6f6e 6669 672e 6361 7061 6369 7479  _config.capacity
+00009480: 5f66 6163 746f 720a 2020 2020 2020 2020  _factor.        
+00009490: 7365 6c66 2e6e 756d 5f65 7870 6572 7473  self.num_experts
+000094a0: 5f63 686f 7365 6e20 3d20 6d6f 655f 636f  _chosen = moe_co
+000094b0: 6e66 6967 2e6e 756d 5f65 7870 6572 7473  nfig.num_experts
+000094c0: 5f63 686f 7365 6e0a 2020 2020 2020 2020  _chosen.        
+000094d0: 7365 6c66 2e74 7261 696e 696e 6720 3d20  self.training = 
+000094e0: 7472 6169 6e69 6e67 0a20 2020 2020 2020  training.       
+000094f0: 2073 656c 662e 726f 7574 696e 675f 706f   self.routing_po
+00009500: 6c69 6379 203d 2072 6f75 7469 6e67 5f70  licy = routing_p
+00009510: 6f6c 6963 790a 2020 2020 2020 2020 7365  olicy.        se
+00009520: 6c66 2e6e 6f69 7379 5f70 6f6c 6963 7920  lf.noisy_policy 
+00009530: 3d20 4e6f 6e65 2020 2320 6361 6e64 6964  = None  # candid
+00009540: 6174 653a 205b 226a 6974 7465 7222 2c20  ate: ["jitter", 
+00009550: 2272 7361 6d70 6c65 222c 2022 4e6f 6e65  "rsample", "None
+00009560: 225d 0a20 2020 2020 2020 2073 656c 662e  "].        self.
+00009570: 6e6f 6973 795f 6570 7369 6c6f 6e20 3d20  noisy_epsilon = 
+00009580: 3165 2d32 0a20 2020 2020 2020 2073 656c  1e-2.        sel
+00009590: 662e 6e6f 6973 6520 3d20 5465 6e73 6f72  f.noise = Tensor
+000095a0: 286e 702e 7261 6e64 6f6d 2e75 6e69 666f  (np.random.unifo
+000095b0: 726d 2831 202d 2073 656c 662e 6e6f 6973  rm(1 - self.nois
+000095c0: 795f 6570 7369 6c6f 6e2c 2031 202b 2073  y_epsilon, 1 + s
+000095d0: 656c 662e 6e6f 6973 795f 6570 7369 6c6f  elf.noisy_epsilo
+000095e0: 6e2c 2028 645f 6d6f 6465 6c2c 2929 290a  n, (d_model,))).
+000095f0: 0a20 2020 2020 2020 2073 656c 662e 6465  .        self.de
+00009600: 6e73 6520 3d20 4465 6e73 6528 696e 5f63  nse = Dense(in_c
+00009610: 6861 6e6e 656c 733d 7365 6c66 2e64 5f6d  hannels=self.d_m
+00009620: 6f64 656c 2c20 6f75 745f 6368 616e 6e65  odel, out_channe
+00009630: 6c73 3d73 656c 662e 6578 7065 7274 5f64  ls=self.expert_d
+00009640: 696d 2c20 6861 735f 6269 6173 3d46 616c  im, has_bias=Fal
+00009650: 7365 290a 2020 2020 2020 2020 7365 6c66  se).        self
+00009660: 2e64 656e 7365 2e6d 6174 6d75 6c2e 7368  .dense.matmul.sh
+00009670: 6172 6428 2828 6470 2c20 3129 2c20 2831  ard(((dp, 1), (1
+00009680: 2c20 3129 2929 0a20 2020 2020 2020 2073  , 1))).        s
+00009690: 656c 662e 6d75 6c20 3d20 502e 4d75 6c28  elf.mul = P.Mul(
+000096a0: 290a 2020 2020 2020 2020 7365 6c66 2e63  ).        self.c
+000096b0: 6173 7420 3d20 502e 4361 7374 2829 0a0a  ast = P.Cast()..
+000096c0: 2020 2020 2020 2020 6966 2073 656c 662e          if self.
+000096d0: 726f 7574 696e 675f 706f 6c69 6379 203d  routing_policy =
+000096e0: 3d20 2254 6f70 6b52 6f75 7465 7256 3122  = "TopkRouterV1"
+000096f0: 3a0a 2020 2020 2020 2020 2020 2020 7365  :.            se
+00009700: 6c66 2e72 6f75 7465 7220 3d20 546f 706b  lf.router = Topk
+00009710: 526f 7574 6572 2864 5f6d 6f64 656c 3d64  Router(d_model=d
+00009720: 5f6d 6f64 656c 2c20 6d6f 655f 636f 6e66  _model, moe_conf
+00009730: 6967 3d6d 6f65 5f63 6f6e 6669 672c 2074  ig=moe_config, t
+00009740: 7261 696e 696e 673d 7472 6169 6e69 6e67  raining=training
+00009750: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00009760: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00009770: 2020 2020 2020 2070 6172 616c 6c65 6c5f         parallel_
+00009780: 636f 6e66 6967 3d70 6172 616c 6c65 6c5f  config=parallel_
+00009790: 636f 6e66 6967 290a 2020 2020 2020 2020  config).        
+000097a0: 656c 6966 2073 656c 662e 726f 7574 696e  elif self.routin
+000097b0: 675f 706f 6c69 6379 203d 3d20 2254 6f70  g_policy == "Top
+000097c0: 6b52 6f75 7465 7256 3222 3a0a 2020 2020  kRouterV2":.    
+000097d0: 2020 2020 2020 2020 7365 6c66 2e72 6f75          self.rou
+000097e0: 7465 7220 3d20 546f 706b 526f 7574 6572  ter = TopkRouter
+000097f0: 5632 2864 5f6d 6f64 656c 3d64 5f6d 6f64  V2(d_model=d_mod
+00009800: 656c 2c20 6d6f 655f 636f 6e66 6967 3d6d  el, moe_config=m
+00009810: 6f65 5f63 6f6e 6669 672c 2074 7261 696e  oe_config, train
+00009820: 696e 673d 7472 6169 6e69 6e67 2c0a 2020  ing=training,.  
+00009830: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00009840: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00009850: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00009860: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00009870: 2020 2020 2020 2020 2873 656c 662e 6578          (self.ex
-00009880: 7065 7274 5f64 696d 2c20 7365 6c66 2e64  pert_dim, self.d
-00009890: 705f 6772 6f75 702c 2063 6170 6163 6974  p_group, capacit
-000098a0: 792c 2073 656c 662e 6869 6464 656e 5f73  y, self.hidden_s
-000098b0: 697a 6529 2c0a 2020 2020 2020 2020 2020  ize),.          
-000098c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000098d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000098e0: 2020 2020 2020 2020 2020 2020 2020 2831                (1
-000098f0: 2c20 312c 2031 2c20 3129 290a 2020 2020  , 1, 1, 1)).    
-00009900: 2020 2020 2020 2020 2020 2020 2320 616c              # al
-00009910: 6c67 6174 6865 720a 2020 2020 2020 2020  lgather.        
-00009920: 2020 2020 2020 2020 6578 7065 7274 5f6f          expert_o
-00009930: 7574 7075 7420 3d20 7365 6c66 2e73 7472  utput = self.str
-00009940: 6964 655f 736c 6963 655f 6470 2865 7870  ide_slice_dp(exp
-00009950: 6572 745f 6f75 7470 7574 2c20 2830 2c20  ert_output, (0, 
-00009960: 302c 2030 2c20 3029 2c0a 2020 2020 2020  0, 0, 0),.      
-00009970: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00009980: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00009990: 2020 2020 2020 2020 2020 2020 2020 2028                 (
-000099a0: 7365 6c66 2e65 7870 6572 745f 6469 6d2c  self.expert_dim,
-000099b0: 2073 656c 662e 6470 5f67 726f 7570 2c20   self.dp_group, 
-000099c0: 6361 7061 6369 7479 2c20 7365 6c66 2e68  capacity, self.h
-000099d0: 6964 6465 6e5f 7369 7a65 292c 0a20 2020  idden_size),.   
-000099e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000099f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00009a00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00009a10: 2020 2831 2c20 312c 2031 2c20 3129 290a    (1, 1, 1, 1)).
-00009a20: 2020 2020 2020 2020 2020 2020 656c 7365              else
-00009a30: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
-00009a40: 2020 2320 2864 702c 2045 2c20 6e2c 2068    # (dp, E, n, h
-00009a50: 2920 3c2d 2d20 2845 2c20 6470 2c20 6e2c  ) <-- (E, dp, n,
-00009a60: 2068 290a 2020 2020 2020 2020 2020 2020   h).            
-00009a70: 2020 2020 6578 7065 7274 5f6f 7574 7075      expert_outpu
-00009a80: 7420 3d20 7365 6c66 2e74 7261 6e73 706f  t = self.transpo
-00009a90: 7365 5f34 6469 6d5f 6470 3128 6578 7065  se_4dim_dp1(expe
-00009aa0: 7274 5f6f 7574 7075 742c 2028 312c 2030  rt_output, (1, 0
-00009ab0: 2c20 322c 2033 2929 0a20 2020 2020 2020  , 2, 3)).       
-00009ac0: 2065 6c73 653a 0a20 2020 2020 2020 2020   else:.         
-00009ad0: 2020 2023 7265 7368 6170 6520 666f 7220     #reshape for 
-00009ae0: 6578 7065 7274 5f6f 7574 7075 742c 2028  expert_output, (
-00009af0: 452c 2064 702c 206e 2c20 6829 203c 2d2d  E, dp, n, h) <--
-00009b00: 2028 6470 5f6d 6f65 2c20 452c 2065 702a   (dp_moe, E, ep*
-00009b10: 6e2c 2068 290a 2020 2020 2020 2020 2020  n, h).          
-00009b20: 2020 2320 2864 705f 6d6f 652c 2045 2c20    # (dp_moe, E, 
-00009b30: 6570 2c20 6e2c 2068 2920 3c2d 2d20 2864  ep, n, h) <-- (d
-00009b40: 705f 6d6f 652c 2045 2c20 6570 2a6e 2c20  p_moe, E, ep*n, 
-00009b50: 6829 0a20 2020 2020 2020 2020 2020 2065  h).            e
-00009b60: 7870 6572 745f 6f75 7470 7574 203d 2073  xpert_output = s
-00009b70: 656c 662e 7265 7368 6170 6528 6578 7065  elf.reshape(expe
-00009b80: 7274 5f6f 7574 7075 742c 2028 7365 6c66  rt_output, (self
-00009b90: 2e64 705f 6d6f 652c 2073 656c 662e 6578  .dp_moe, self.ex
-00009ba0: 7065 7274 5f64 696d 2c20 7365 6c66 2e65  pert_dim, self.e
-00009bb0: 702c 202d 312c 2073 656c 662e 6869 6464  p, -1, self.hidd
-00009bc0: 656e 5f73 697a 6529 290a 2020 2020 2020  en_size)).      
-00009bd0: 2020 2020 2020 2320 2864 705f 6d6f 652c        # (dp_moe,
-00009be0: 2065 702c 2045 2c20 6e2c 2068 2920 3c2d   ep, E, n, h) <-
-00009bf0: 2d20 2864 705f 6d6f 652c 2045 2c20 6570  - (dp_moe, E, ep
-00009c00: 2c20 6e2c 2068 290a 2020 2020 2020 2020  , n, h).        
-00009c10: 2020 2020 6578 7065 7274 5f6f 7574 7075      expert_outpu
-00009c20: 7420 3d20 7365 6c66 2e74 7261 6e73 706f  t = self.transpo
-00009c30: 7365 5f35 6469 6d5f 6570 3228 6578 7065  se_5dim_ep2(expe
-00009c40: 7274 5f6f 7574 7075 742c 2028 302c 2032  rt_output, (0, 2
-00009c50: 2c20 312c 2033 2c20 3429 290a 2020 2020  , 1, 3, 4)).    
-00009c60: 2020 2020 2020 2020 2320 2864 702c 2045          # (dp, E
-00009c70: 2c20 6e2c 2068 2920 3c2d 2d20 2864 705f  , n, h) <-- (dp_
-00009c80: 6d6f 652c 2065 702c 2045 2c20 6e2c 2068  moe, ep, E, n, h
-00009c90: 290a 2020 2020 2020 2020 2020 2020 6578  ).            ex
-00009ca0: 7065 7274 5f6f 7574 7075 7420 3d20 7365  pert_output = se
-00009cb0: 6c66 2e72 6573 6861 7065 2865 7870 6572  lf.reshape(exper
-00009cc0: 745f 6f75 7470 7574 2c20 2873 656c 662e  t_output, (self.
-00009cd0: 6470 2c20 7365 6c66 2e65 7870 6572 745f  dp, self.expert_
-00009ce0: 6469 6d2c 202d 312c 2073 656c 662e 6869  dim, -1, self.hi
-00009cf0: 6464 656e 5f73 697a 6529 290a 2020 2020  dden_size)).    
-00009d00: 2020 2020 7265 7475 726e 2065 7870 6572      return exper
-00009d10: 745f 6f75 7470 7574 0a0a 0a20 2020 2064  t_output...    d
-00009d20: 6566 2063 6f6e 7374 7275 6374 2873 656c  ef construct(sel
-00009d30: 662c 2069 6e70 7574 5f74 656e 736f 7229  f, input_tensor)
-00009d40: 3a0a 2020 2020 2020 2020 2222 2266 6f72  :.        """for
-00009d50: 7761 7264 2070 726f 6365 7373 2222 220a  ward process""".
-00009d60: 2020 2020 2020 2020 696e 7075 745f 7465          input_te
-00009d70: 6e73 6f72 5f73 6861 7065 203d 2073 656c  nsor_shape = sel
-00009d80: 662e 7368 6170 6528 696e 7075 745f 7465  f.shape(input_te
-00009d90: 6e73 6f72 290a 0a20 2020 2020 2020 2069  nsor)..        i
-00009da0: 6e70 7574 5f74 656e 736f 7220 3d20 7365  nput_tensor = se
-00009db0: 6c66 2e72 6573 6861 7065 2869 6e70 7574  lf.reshape(input
-00009dc0: 5f74 656e 736f 722c 2028 2d31 2c20 7365  _tensor, (-1, se
-00009dd0: 6c66 2e68 6964 6465 6e5f 7369 7a65 2929  lf.hidden_size))
-00009de0: 0a20 2020 2020 2020 2062 735f 616e 645f  .        bs_and_
-00009df0: 646d 6f64 656c 203d 2073 656c 662e 7368  dmodel = self.sh
-00009e00: 6170 6528 696e 7075 745f 7465 6e73 6f72  ape(input_tensor
-00009e10: 290a 2020 2020 2020 2020 746f 6b65 6e73  ).        tokens
-00009e20: 5f70 6572 5f67 726f 7570 203d 2062 735f  _per_group = bs_
-00009e30: 616e 645f 646d 6f64 656c 5b30 5d20 2f2f  and_dmodel[0] //
-00009e40: 2073 656c 662e 6470 5f67 726f 7570 0a20   self.dp_group. 
-00009e50: 2020 2020 2020 2065 7870 6572 745f 6361         expert_ca
-00009e60: 7061 6369 7479 203d 2063 616c 6375 6c61  pacity = calcula
-00009e70: 7465 5f65 7870 6572 745f 6361 7061 6369  te_expert_capaci
-00009e80: 7479 5f76 3228 7365 6c66 2e6e 756d 5f65  ty_v2(self.num_e
-00009e90: 7870 6572 7473 5f63 686f 7365 6e2c 2074  xperts_chosen, t
-00009ea0: 6f6b 656e 735f 7065 725f 6772 6f75 702c  okens_per_group,
-00009eb0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00009ec0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00009ed0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00009ee0: 2020 2020 2020 2020 7365 6c66 2e63 6170          self.cap
-00009ef0: 6163 6974 795f 6661 6374 6f72 2c20 7365  acity_factor, se
-00009f00: 6c66 2e65 7870 6572 745f 6469 6d2c 2073  lf.expert_dim, s
-00009f10: 656c 662e 6d70 290a 0a20 2020 2020 2020  elf.mp)..       
-00009f20: 2069 6e70 7574 5f74 656e 736f 7220 3d20   input_tensor = 
-00009f30: 7365 6c66 2e72 6573 6861 7065 2869 6e70  self.reshape(inp
-00009f40: 7574 5f74 656e 736f 722c 2028 7365 6c66  ut_tensor, (self
-00009f50: 2e64 705f 6772 6f75 702c 202d 312c 2073  .dp_group, -1, s
-00009f60: 656c 662e 6869 6464 656e 5f73 697a 6529  elf.hidden_size)
-00009f70: 2920 2320 2864 702c 204e 2c20 6829 203c  ) # (dp, N, h) <
-00009f80: 2d2d 2028 422a 532c 2068 290a 0a20 2020  -- (B*S, h)..   
-00009f90: 2020 2020 2023 2063 616c 6375 6c61 7465       # calculate
-00009fa0: 2072 6f75 7465 720a 2020 2020 2020 2020   router.        
-00009fb0: 6469 7370 6174 6368 5f70 6f6c 6963 792c  dispatch_policy,
-00009fc0: 2063 6f6d 6269 6e65 5f70 6f6c 6963 792c   combine_policy,
-00009fd0: 2072 6f75 7465 725f 636f 6566 6620 3d20   router_coeff = 
-00009fe0: 7365 6c66 2e72 6f75 7465 7228 696e 7075  self.router(inpu
-00009ff0: 745f 7465 6e73 6f72 2920 2320 2864 702c  t_tensor) # (dp,
-0000a000: 2045 2c20 6e29 696e 7433 322c 2028 6470   E, n)int32, (dp
-0000a010: 2c20 4e2c 206b 2969 6e74 3332 2c20 2864  , N, k)int32, (d
-0000a020: 702c 204e 2c20 6b29 6670 3136 203c 2d2d  p, N, k)fp16 <--
-0000a030: 2028 6470 2c20 4e2c 2068 292c 2077 6865   (dp, N, h), whe
-0000a040: 7265 2030 3c3d 2064 6973 7061 7463 685f  re 0<= dispatch_
-0000a050: 696e 6465 7820 3c20 312b 4e2c 2030 3c3d  index < 1+N, 0<=
-0000a060: 2063 6f6d 6269 6e65 5f69 6e64 6578 203c   combine_index <
-0000a070: 452a 2831 2b6e 290a 0a20 2020 2020 2020  E*(1+n)..       
-0000a080: 2023 2064 6973 7061 7463 680a 2020 2020   # dispatch.    
-0000a090: 2020 2020 6578 7065 7274 5f69 6e70 7574      expert_input
-0000a0a0: 203d 2073 656c 662e 726f 7574 6572 2e72   = self.router.r
-0000a0b0: 6f75 7465 722e 6469 7370 6174 6368 2869  outer.dispatch(i
-0000a0c0: 6e70 7574 5f74 656e 736f 722c 2064 6973  nput_tensor, dis
-0000a0d0: 7061 7463 685f 706f 6c69 6379 2920 2320  patch_policy) # 
-0000a0e0: 2864 702c 2045 2c20 6e2c 2068 2920 3c2d  (dp, E, n, h) <-
-0000a0f0: 2d20 2864 702c 204e 2c20 6829 2c20 2864  - (dp, N, h), (d
-0000a100: 702c 2045 2c20 6e29 0a0a 2020 2020 2020  p, E, n)..      
-0000a110: 2020 2320 6666 6e0a 2020 2020 2020 2020    # ffn.        
-0000a120: 6578 7065 7274 5f6f 7574 7075 7420 3d20  expert_output = 
-0000a130: 7365 6c66 2e66 666e 5f69 6e66 6572 2865  self.ffn_infer(e
-0000a140: 7870 6572 745f 696e 7075 742c 2065 7870  xpert_input, exp
-0000a150: 6572 745f 6361 7061 6369 7479 2920 2320  ert_capacity) # 
-0000a160: 2845 2c20 6470 2c20 6e2c 2068 2920 3c2d  (E, dp, n, h) <-
-0000a170: 2d20 2845 2c20 6470 2c20 6e2c 2068 290a  - (E, dp, n, h).
-0000a180: 0a20 2020 2020 2020 2023 2063 6f6d 6269  .        # combi
-0000a190: 6e65 0a20 2020 2020 2020 206f 7574 7075  ne.        outpu
-0000a1a0: 745f 7465 6e73 6f72 203d 2073 656c 662e  t_tensor = self.
-0000a1b0: 726f 7574 6572 2e72 6f75 7465 722e 636f  router.router.co
-0000a1c0: 6d62 696e 6528 6578 7065 7274 5f6f 7574  mbine(expert_out
-0000a1d0: 7075 742c 2063 6f6d 6269 6e65 5f70 6f6c  put, combine_pol
-0000a1e0: 6963 792c 2072 6f75 7465 725f 636f 6566  icy, router_coef
-0000a1f0: 6629 2023 2028 6470 2c20 4e2c 206b 2c20  f) # (dp, N, k, 
-0000a200: 6829 203c 2d2d 2028 6470 2c20 452a 2831  h) <-- (dp, E*(1
-0000a210: 2b6e 292c 2068 292c 2028 6470 2c20 4e2c  +n), h), (dp, N,
-0000a220: 206b 290a 2020 2020 2020 2020 6f75 7470   k).        outp
-0000a230: 7574 5f74 656e 736f 7220 3d20 7365 6c66  ut_tensor = self
-0000a240: 2e72 6573 6861 7065 286f 7574 7075 745f  .reshape(output_
-0000a250: 7465 6e73 6f72 2c20 696e 7075 745f 7465  tensor, input_te
-0000a260: 6e73 6f72 5f73 6861 7065 2920 2320 2842  nsor_shape) # (B
-0000a270: 2a53 2c20 6829 203c 2d2d 2028 6470 2c20  *S, h) <-- (dp, 
-0000a280: 4e2c 2068 290a 2020 2020 2020 2020 7265  N, h).        re
-0000a290: 7475 726e 206f 7574 7075 745f 7465 6e73  turn output_tens
-0000a2a0: 6f72 2023 2028 6470 2c20 4e2c 2068 290a  or # (dp, N, h).
-0000a2b0: 0a0a 636c 6173 7320 526f 7574 6572 2843  ..class Router(C
-0000a2c0: 656c 6c29 3a0a 2020 2020 7222 2222 0a20  ell):.    r""". 
-0000a2d0: 2020 2020 2020 2041 2072 6f75 7465 7220         A router 
-0000a2e0: 6261 636b 626f 6e65 2075 7365 6420 746f  backbone used to
-0000a2f0: 2063 616c 6375 6c61 7465 206c 6f67 6974   calculate logit
-0000a300: 7320 6f66 2065 6163 6820 746f 6b65 6e2c  s of each token,
-0000a310: 2077 6869 6368 2073 686f 756c 6420 6265   which should be
-0000a320: 2063 6173 6361 6465 6420 6279 2072 6f75   cascaded by rou
-0000a330: 7465 7220 696d 706c 656d 656e 7461 7469  ter implementati
-0000a340: 6f6e 730a 2020 2020 2020 2020 6d61 7070  ons.        mapp
-0000a350: 696e 6720 746f 6b65 6e73 2074 6f20 6578  ing tokens to ex
-0000a360: 7065 7274 732e 0a20 2020 2020 2020 2077  perts..        w
-0000a370: 6865 6e20 6d6f 655f 636f 6e66 6967 2e6e  hen moe_config.n
-0000a380: 756d 5f65 7870 6572 7473 5f63 686f 7365  um_experts_chose
-0000a390: 6e20 3d20 312c 2075 7365 2074 6f70 3120  n = 1, use top1 
-0000a3a0: 726f 7574 696e 673b 0a20 2020 2020 2020  routing;.       
-0000a3b0: 2077 6865 6e20 6d6f 655f 636f 6e66 6967   when moe_config
-0000a3c0: 2e6e 756d 5f65 7870 6572 7473 5f63 686f  .num_experts_cho
-0000a3d0: 7365 6e20 3e20 312c 2075 7365 2074 6f70  sen > 1, use top
-0000a3e0: 6b20 726f 7574 696e 670a 0a20 2020 2020  k routing..     
-0000a3f0: 2020 2041 7267 733a 0a20 2020 2020 2020     Args:.       
-0000a400: 2020 2020 2064 5f6d 6f64 656c 2028 696e       d_model (in
-0000a410: 7429 3a20 5468 6520 6869 6464 656e 2073  t): The hidden s
-0000a420: 697a 6520 6f66 2065 6163 6820 746f 6b65  ize of each toke
-0000a430: 6e2e 0a20 2020 2020 2020 2020 2020 206d  n..            m
-0000a440: 6f65 5f63 6f6e 6669 6728 4d6f 4543 6f6e  oe_config(MoECon
-0000a450: 6669 6729 3a20 5468 6520 636f 6e66 6967  fig): The config
-0000a460: 7572 6174 696f 6e20 6f66 204d 6f45 2028  uration of MoE (
-0000a470: 4d69 7874 7572 6520 6f66 2045 7870 6572  Mixture of Exper
-0000a480: 7429 2e0a 2020 2020 2020 2020 2020 2020  t)..            
-0000a490: 726f 7574 696e 675f 706f 6c69 6379 3a20  routing_policy: 
-0000a4a0: 5468 6520 706f 6c69 6379 206f 6620 6d61  The policy of ma
-0000a4b0: 7070 696e 6720 746f 6b65 6e73 2074 6f20  pping tokens to 
-0000a4c0: 6578 7065 7274 732e 2044 6566 6175 6c74  experts. Default
-0000a4d0: 3a20 746f 706b 526f 7574 6572 0a20 2020  : topkRouter.   
-0000a4e0: 2020 2020 2020 2020 2074 7261 696e 696e           trainin
-0000a4f0: 6720 2862 6f6f 6c29 3a20 5468 6520 7661  g (bool): The va
-0000a500: 6c75 6520 696e 6469 6361 7469 6e67 2077  lue indicating w
-0000a510: 6865 7468 6572 2069 7320 696e 2074 7261  hether is in tra
-0000a520: 696e 696e 6720 7068 6173 652e 0a20 2020  ining phase..   
-0000a530: 2020 2020 2020 2020 2070 6172 616c 6c65           paralle
-0000a540: 6c5f 636f 6e66 6967 3a20 5468 6520 7061  l_config: The pa
-0000a550: 7261 6c6c 656c 2d72 656c 6174 6564 2063  rallel-related c
-0000a560: 6f6e 6669 6775 7261 7469 6f6e 2e0a 2020  onfiguration..  
-0000a570: 2020 2020 2020 496e 7075 7473 3a0a 2020        Inputs:.  
-0000a580: 2020 2020 2020 2020 2020 2d20 2a2a 696e            - **in
-0000a590: 7075 745f 7465 6e73 6f72 2a2a 2028 5465  put_tensor** (Te
-0000a5a0: 6e73 6f72 2920 2d20 5465 6e73 6f72 206f  nsor) - Tensor o
-0000a5b0: 6620 7368 6170 6520 3a6d 6174 683a 6028  f shape :math:`(
-0000a5c0: 6578 7065 7274 5c5f 7061 7261 6c6c 656c  expert\_parallel
-0000a5d0: 2c20 746f 6b65 6e73 5c5f 7065 725c 5f64  , tokens\_per\_d
-0000a5e0: 6576 6963 652c 0a20 2020 2020 2020 2020  evice,.         
-0000a5f0: 2020 2068 6964 6465 6e5c 5f73 697a 6529     hidden\_size)
-0000a600: 602e 0a0a 2020 2020 2020 2020 4f75 7470  `...        Outp
-0000a610: 7574 733a 0a20 2020 2020 2020 2020 2020  uts:.           
-0000a620: 2054 656e 736f 7220 6f66 2073 6861 7065   Tensor of shape
-0000a630: 203a 6d61 7468 3a60 2865 7870 6572 745c   :math:`(expert\
-0000a640: 5f70 6172 616c 6c65 6c2c 2074 6f6b 656e  _parallel, token
-0000a650: 735c 5f70 6572 5c5f 6465 7669 6365 2c20  s\_per\_device, 
-0000a660: 6578 7065 7274 5c5f 6469 6d29 602e 0a20  expert\_dim)`.. 
-0000a670: 2020 2022 2222 0a0a 2020 2020 6465 6620     """..    def 
-0000a680: 5f5f 696e 6974 5f5f 2873 656c 662c 0a20  __init__(self,. 
-0000a690: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000a6a0: 645f 6d6f 6465 6c2c 0a20 2020 2020 2020  d_model,.       
-0000a6b0: 2020 2020 2020 2020 2020 6d6f 655f 636f            moe_co
-0000a6c0: 6e66 6967 2c0a 2020 2020 2020 2020 2020  nfig,.          
-0000a6d0: 2020 2020 2020 2072 6f75 7469 6e67 5f70         routing_p
-0000a6e0: 6f6c 6963 793d 4e6f 6e65 2c0a 2020 2020  olicy=None,.    
-0000a6f0: 2020 2020 2020 2020 2020 2020 2074 7261               tra
-0000a700: 696e 696e 673d 5472 7565 2c0a 2020 2020  ining=True,.    
-0000a710: 2020 2020 2020 2020 2020 2020 2070 6172               par
-0000a720: 616c 6c65 6c5f 636f 6e66 6967 3d4e 6f6e  allel_config=Non
-0000a730: 6529 3a0a 2020 2020 2020 2020 7375 7065  e):.        supe
-0000a740: 7228 526f 7574 6572 2c20 7365 6c66 292e  r(Router, self).
-0000a750: 5f5f 696e 6974 5f5f 2829 0a20 2020 2020  __init__().     
-0000a760: 2020 2064 7020 3d20 7061 7261 6c6c 656c     dp = parallel
-0000a770: 5f63 6f6e 6669 672e 6461 7461 5f70 6172  _config.data_par
-0000a780: 616c 6c65 6c0a 2020 2020 2020 2020 7365  allel.        se
-0000a790: 6c66 2e64 5f6d 6f64 656c 203d 2064 5f6d  lf.d_model = d_m
-0000a7a0: 6f64 656c 0a20 2020 2020 2020 2073 656c  odel.        sel
-0000a7b0: 662e 6d6f 655f 636f 6e66 6967 203d 206d  f.moe_config = m
-0000a7c0: 6f65 5f63 6f6e 6669 670a 2020 2020 2020  oe_config.      
-0000a7d0: 2020 7365 6c66 2e65 7870 6572 745f 6469    self.expert_di
-0000a7e0: 6d20 3d20 6d6f 655f 636f 6e66 6967 2e65  m = moe_config.e
-0000a7f0: 7870 6572 745f 6e75 6d0a 2020 2020 2020  xpert_num.      
-0000a800: 2020 7365 6c66 2e63 6170 6163 6974 795f    self.capacity_
-0000a810: 6661 6374 6f72 203d 206d 6f65 5f63 6f6e  factor = moe_con
-0000a820: 6669 672e 6361 7061 6369 7479 5f66 6163  fig.capacity_fac
-0000a830: 746f 720a 2020 2020 2020 2020 7365 6c66  tor.        self
-0000a840: 2e6e 756d 5f65 7870 6572 7473 5f63 686f  .num_experts_cho
-0000a850: 7365 6e20 3d20 6d6f 655f 636f 6e66 6967  sen = moe_config
-0000a860: 2e6e 756d 5f65 7870 6572 7473 5f63 686f  .num_experts_cho
-0000a870: 7365 6e0a 2020 2020 2020 2020 7365 6c66  sen.        self
-0000a880: 2e74 7261 696e 696e 6720 3d20 7472 6169  .training = trai
-0000a890: 6e69 6e67 0a20 2020 2020 2020 2073 656c  ning.        sel
-0000a8a0: 662e 726f 7574 696e 675f 706f 6c69 6379  f.routing_policy
-0000a8b0: 203d 2072 6f75 7469 6e67 5f70 6f6c 6963   = routing_polic
-0000a8c0: 790a 2020 2020 2020 2020 7365 6c66 2e6e  y.        self.n
-0000a8d0: 6f69 7379 5f70 6f6c 6963 7920 3d20 4e6f  oisy_policy = No
-0000a8e0: 6e65 2020 2320 6361 6e64 6964 6174 653a  ne  # candidate:
-0000a8f0: 205b 226a 6974 7465 7222 2c20 2272 7361   ["jitter", "rsa
-0000a900: 6d70 6c65 222c 2022 4e6f 6e65 225d 0a20  mple", "None"]. 
-0000a910: 2020 2020 2020 2073 656c 662e 6e6f 6973         self.nois
-0000a920: 795f 6570 7369 6c6f 6e20 3d20 3165 2d32  y_epsilon = 1e-2
-0000a930: 0a20 2020 2020 2020 2073 656c 662e 6e6f  .        self.no
-0000a940: 6973 6520 3d20 5465 6e73 6f72 286e 702e  ise = Tensor(np.
-0000a950: 7261 6e64 6f6d 2e75 6e69 666f 726d 2831  random.uniform(1
-0000a960: 202d 2073 656c 662e 6e6f 6973 795f 6570   - self.noisy_ep
-0000a970: 7369 6c6f 6e2c 2031 202b 2073 656c 662e  silon, 1 + self.
-0000a980: 6e6f 6973 795f 6570 7369 6c6f 6e2c 2028  noisy_epsilon, (
-0000a990: 645f 6d6f 6465 6c2c 2929 290a 0a20 2020  d_model,)))..   
-0000a9a0: 2020 2020 2073 656c 662e 6465 6e73 6520       self.dense 
-0000a9b0: 3d20 4465 6e73 6528 696e 5f63 6861 6e6e  = Dense(in_chann
-0000a9c0: 656c 733d 7365 6c66 2e64 5f6d 6f64 656c  els=self.d_model
-0000a9d0: 2c20 6f75 745f 6368 616e 6e65 6c73 3d73  , out_channels=s
-0000a9e0: 656c 662e 6578 7065 7274 5f64 696d 2c0a  elf.expert_dim,.
-0000a9f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000aa00: 2020 2020 2020 2020 2020 2068 6173 5f62             has_b
-0000aa10: 6961 733d 4661 6c73 652c 2064 7479 7065  ias=False, dtype
-0000aa20: 3d6d 6f65 5f63 6f6e 6669 672e 726f 7574  =moe_config.rout
-0000aa30: 6572 5f64 656e 7365 5f74 7970 6529 0a20  er_dense_type). 
-0000aa40: 2020 2020 2020 2073 656c 662e 6465 6e73         self.dens
-0000aa50: 652e 6d61 746d 756c 2e73 6861 7264 2828  e.matmul.shard((
-0000aa60: 2864 702c 2031 292c 2028 312c 2031 2929  (dp, 1), (1, 1))
-0000aa70: 290a 2020 2020 2020 2020 7365 6c66 2e6d  ).        self.m
-0000aa80: 756c 203d 2050 2e4d 756c 2829 0a20 2020  ul = P.Mul().   
-0000aa90: 2020 2020 2073 656c 662e 6361 7374 203d       self.cast =
-0000aaa0: 2050 2e43 6173 7428 290a 0a20 2020 2020   P.Cast()..     
-0000aab0: 2020 2069 6620 7365 6c66 2e72 6f75 7469     if self.routi
-0000aac0: 6e67 5f70 6f6c 6963 7920 3d3d 2022 546f  ng_policy == "To
-0000aad0: 706b 526f 7574 6572 5631 223a 0a20 2020  pkRouterV1":.   
-0000aae0: 2020 2020 2020 2020 2073 656c 662e 726f           self.ro
-0000aaf0: 7574 6572 203d 2054 6f70 6b52 6f75 7465  uter = TopkRoute
-0000ab00: 7228 645f 6d6f 6465 6c3d 645f 6d6f 6465  r(d_model=d_mode
-0000ab10: 6c2c 206d 6f65 5f63 6f6e 6669 673d 6d6f  l, moe_config=mo
-0000ab20: 655f 636f 6e66 6967 2c20 7472 6169 6e69  e_config, traini
-0000ab30: 6e67 3d74 7261 696e 696e 672c 0a20 2020  ng=training,.   
-0000ab40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000ab50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000ab60: 2020 7061 7261 6c6c 656c 5f63 6f6e 6669    parallel_confi
-0000ab70: 673d 7061 7261 6c6c 656c 5f63 6f6e 6669  g=parallel_confi
-0000ab80: 6729 0a20 2020 2020 2020 2065 6c69 6620  g).        elif 
-0000ab90: 7365 6c66 2e72 6f75 7469 6e67 5f70 6f6c  self.routing_pol
-0000aba0: 6963 7920 3d3d 2022 546f 706b 526f 7574  icy == "TopkRout
-0000abb0: 6572 5632 223a 0a20 2020 2020 2020 2020  erV2":.         
-0000abc0: 2020 2073 656c 662e 726f 7574 6572 203d     self.router =
-0000abd0: 2054 6f70 6b52 6f75 7465 7256 3228 645f   TopkRouterV2(d_
-0000abe0: 6d6f 6465 6c3d 645f 6d6f 6465 6c2c 206d  model=d_model, m
-0000abf0: 6f65 5f63 6f6e 6669 673d 6d6f 655f 636f  oe_config=moe_co
-0000ac00: 6e66 6967 2c20 7472 6169 6e69 6e67 3d74  nfig, training=t
-0000ac10: 7261 696e 696e 672c 0a20 2020 2020 2020  raining,.       
-0000ac20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000ac30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000ac40: 7061 7261 6c6c 656c 5f63 6f6e 6669 673d  parallel_config=
-0000ac50: 7061 7261 6c6c 656c 5f63 6f6e 6669 6729  parallel_config)
-0000ac60: 0a20 2020 2020 2020 2065 6c73 653a 0a20  .        else:. 
-0000ac70: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-0000ac80: 726f 7574 6572 203d 2072 6f75 7469 6e67  router = routing
-0000ac90: 5f70 6f6c 6963 790a 0a20 2020 2020 2020  _policy..       
-0000aca0: 2069 6620 6e6f 7420 285f 6765 745f 7061   if not (_get_pa
-0000acb0: 7261 6c6c 656c 5f6d 6f64 6528 2920 696e  rallel_mode() in
-0000acc0: 2028 5061 7261 6c6c 656c 4d6f 6465 2e41   (ParallelMode.A
-0000acd0: 5554 4f5f 5041 5241 4c4c 454c 2c29 2061  UTO_PARALLEL,) a
-0000ace0: 6e64 205f 6973 5f73 6861 7264 696e 675f  nd _is_sharding_
-0000acf0: 7072 6f70 6167 6174 696f 6e28 2929 3a0a  propagation()):.
-0000ad00: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-0000ad10: 2e6d 756c 2e73 6861 7264 2828 2864 702c  .mul.shard(((dp,
-0000ad20: 2031 2c20 3129 2c20 2864 702c 2929 290a   1, 1), (dp,))).
-0000ad30: 0a20 2020 2064 6566 2063 6f6e 7374 7275  .    def constru
-0000ad40: 6374 2873 656c 662c 2069 6e70 7574 5f74  ct(self, input_t
-0000ad50: 656e 736f 7229 3a0a 2020 2020 2020 2020  ensor):.        
-0000ad60: 696e 7075 745f 7465 6e73 6f72 203d 2073  input_tensor = s
-0000ad70: 656c 662e 6361 7374 2869 6e70 7574 5f74  elf.cast(input_t
-0000ad80: 656e 736f 722c 2073 656c 662e 6d6f 655f  ensor, self.moe_
-0000ad90: 636f 6e66 6967 2e72 6f75 7465 725f 6465  config.router_de
-0000ada0: 6e73 655f 7479 7065 290a 2020 2020 2020  nse_type).      
-0000adb0: 2020 6966 2073 656c 662e 6e6f 6973 795f    if self.noisy_
-0000adc0: 706f 6c69 6379 203d 3d20 226a 6974 7465  policy == "jitte
-0000add0: 7222 2061 6e64 2073 656c 662e 7472 6169  r" and self.trai
-0000ade0: 6e69 6e67 3a0a 2020 2020 2020 2020 2020  ning:.          
-0000adf0: 2020 2320 4865 7265 2c20 7765 2074 656d    # Here, we tem
-0000ae00: 706f 7261 7269 6c79 2069 6d70 6c65 6d65  porarily impleme
-0000ae10: 6e74 2074 6865 206d 756c 7469 706c 6963  nt the multiplic
-0000ae20: 6174 6976 6520 6a69 7474 6572 2074 6869  ative jitter thi
-0000ae30: 7320 7761 792c 0a20 2020 2020 2020 2020  s way,.         
-0000ae40: 2020 2023 2066 6f72 2074 6865 206c 6163     # for the lac
-0000ae50: 6b20 6f66 2055 6e69 666f 7252 6561 6c20  k of UniforReal 
-0000ae60: 7061 7261 6c6c 656c 206f 7065 7261 746f  parallel operato
-0000ae70: 722e 0a20 2020 2020 2020 2020 2020 2069  r..            i
-0000ae80: 6e70 7574 5f74 656e 736f 7220 3d20 7365  nput_tensor = se
-0000ae90: 6c66 2e6d 756c 2869 6e70 7574 5f74 656e  lf.mul(input_ten
-0000aea0: 736f 722c 2073 656c 662e 6e6f 6973 6529  sor, self.noise)
-0000aeb0: 0a0a 2020 2020 2020 2020 726f 7574 6572  ..        router
-0000aec0: 5f6c 6f67 6974 7320 3d20 7365 6c66 2e64  _logits = self.d
-0000aed0: 656e 7365 2869 6e70 7574 5f74 656e 736f  ense(input_tenso
-0000aee0: 7229 0a20 2020 2020 2020 2072 6574 7572  r).        retur
-0000aef0: 6e20 7365 6c66 2e72 6f75 7465 7228 726f  n self.router(ro
-0000af00: 7574 6572 5f6c 6f67 6974 7329 0a0a 0a63  uter_logits)...c
-0000af10: 6c61 7373 2054 6f70 6b52 6f75 7465 7228  lass TopkRouter(
-0000af20: 4365 6c6c 293a 0a20 2020 2072 2222 220a  Cell):.    r""".
-0000af30: 2020 2020 2020 2020 4120 726f 7574 6572          A router
-0000af40: 2069 6d70 6c65 6d65 6e74 6174 696f 6e20   implementation 
-0000af50: 7768 6963 6820 6d61 7073 2065 6163 6820  which maps each 
-0000af60: 746f 6b65 6e73 2074 6f20 7468 6520 746f  tokens to the to
-0000af70: 706b 2065 7870 6572 742e 0a0a 2020 2020  pk expert...    
-0000af80: 2020 2020 4172 6773 3a0a 2020 2020 2020      Args:.      
-0000af90: 2020 2020 2020 645f 6d6f 6465 6c20 2869        d_model (i
-0000afa0: 6e74 293a 2054 6865 2068 6964 6465 6e20  nt): The hidden 
-0000afb0: 7369 7a65 206f 6620 6561 6368 2074 6f6b  size of each tok
-0000afc0: 656e 2e0a 2020 2020 2020 2020 2020 2020  en..            
-0000afd0: 6d6f 655f 636f 6e66 6967 284d 6f45 436f  moe_config(MoECo
-0000afe0: 6e66 6967 293a 2054 6865 2063 6f6e 6669  nfig): The confi
-0000aff0: 6775 7261 7469 6f6e 206f 6620 4d6f 4520  guration of MoE 
-0000b000: 284d 6978 7475 7265 206f 6620 4578 7065  (Mixture of Expe
-0000b010: 7274 292e 0a20 2020 2020 2020 2020 2020  rt)..           
-0000b020: 2074 7261 696e 696e 6720 2862 6f6f 6c29   training (bool)
-0000b030: 3a20 5468 6520 7661 6c75 6520 696e 6469  : The value indi
-0000b040: 6361 7469 6e67 2077 6865 7468 6572 2069  cating whether i
-0000b050: 7320 696e 2074 7261 696e 696e 6720 7068  s in training ph
-0000b060: 6173 652e 0a20 2020 2020 2020 2020 2020  ase..           
-0000b070: 2063 6f6e 6669 673a 2054 6865 2070 6172   config: The par
-0000b080: 616c 6c65 6c2d 7265 6c61 7465 6420 636f  allel-related co
-0000b090: 6e66 6967 7572 6174 696f 6e2e 0a20 2020  nfiguration..   
-0000b0a0: 2020 2020 2049 6e70 7574 733a 0a20 2020       Inputs:.   
-0000b0b0: 2020 2020 2020 2020 202d 202a 2a69 6e70           - **inp
-0000b0c0: 7574 5f74 656e 736f 722a 2a20 2854 656e  ut_tensor** (Ten
-0000b0d0: 736f 7229 202d 2054 656e 736f 7220 6f66  sor) - Tensor of
-0000b0e0: 2073 6861 7065 203a 6d61 7468 3a60 2865   shape :math:`(e
-0000b0f0: 7870 6572 745c 5f70 6172 616c 6c65 6c2c  xpert\_parallel,
-0000b100: 2074 6f6b 656e 735c 5f70 6572 5c5f 6465   tokens\_per\_de
-0000b110: 7669 6365 2c0a 2020 2020 2020 2020 2020  vice,.          
-0000b120: 2020 6869 6464 656e 5c5f 7369 7a65 2960    hidden\_size)`
-0000b130: 2e0a 0a20 2020 2020 2020 204f 7574 7075  ...        Outpu
-0000b140: 7473 3a0a 2020 2020 2020 2020 2020 2020  ts:.            
-0000b150: 5465 6e73 6f72 206f 6620 7368 6170 6520  Tensor of shape 
-0000b160: 3a6d 6174 683a 6028 6578 7065 7274 5c5f  :math:`(expert\_
-0000b170: 7061 7261 6c6c 656c 2c20 746f 6b65 6e73  parallel, tokens
-0000b180: 5c5f 7065 725c 5f64 6576 6963 652c 2065  \_per\_device, e
-0000b190: 7870 6572 745c 5f64 696d 2c20 6578 7065  xpert\_dim, expe
-0000b1a0: 7274 5c5f 6361 7061 6369 7479 2960 2c0a  rt\_capacity)`,.
-0000b1b0: 2020 2020 2020 2020 2020 2020 5465 6e73              Tens
-0000b1c0: 6f72 206f 6620 7368 6170 6520 3a6d 6174  or of shape :mat
-0000b1d0: 683a 6028 6578 7065 7274 5c5f 7061 7261  h:`(expert\_para
-0000b1e0: 6c6c 656c 2c20 746f 6b65 6e73 5c5f 7065  llel, tokens\_pe
-0000b1f0: 725c 5f64 6576 6963 652c 2065 7870 6572  r\_device, exper
-0000b200: 745c 5f64 696d 2c20 6578 7065 7274 5c5f  t\_dim, expert\_
-0000b210: 6361 7061 6369 7479 2960 2c0a 2020 2020  capacity)`,.    
-0000b220: 2020 2020 2020 2020 5465 6e73 6f72 206f          Tensor o
-0000b230: 6620 7368 6170 6520 3a6d 6174 683a 6028  f shape :math:`(
-0000b240: 3129 602e 0a20 2020 2022 2222 0a0a 2020  1)`..    """..  
-0000b250: 2020 6465 6620 5f5f 696e 6974 5f5f 2873    def __init__(s
-0000b260: 656c 662c 0a20 2020 2020 2020 2020 2020  elf,.           
-0000b270: 2020 2020 2020 645f 6d6f 6465 6c2c 0a20        d_model,. 
-0000b280: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000b290: 6d6f 655f 636f 6e66 6967 2c0a 2020 2020  moe_config,.    
-0000b2a0: 2020 2020 2020 2020 2020 2020 2074 7261               tra
-0000b2b0: 696e 696e 673d 5472 7565 2c0a 2020 2020  ining=True,.    
-0000b2c0: 2020 2020 2020 2020 2020 2020 2070 6172               par
-0000b2d0: 616c 6c65 6c5f 636f 6e66 6967 3d4e 6f6e  allel_config=Non
-0000b2e0: 6529 3a0a 2020 2020 2020 2020 7375 7065  e):.        supe
-0000b2f0: 7228 546f 706b 526f 7574 6572 2c20 7365  r(TopkRouter, se
-0000b300: 6c66 292e 5f5f 696e 6974 5f5f 2829 0a20  lf).__init__(). 
-0000b310: 2020 2020 2020 2069 6620 5f67 6574 5f70         if _get_p
-0000b320: 6172 616c 6c65 6c5f 6d6f 6465 2829 2069  arallel_mode() i
-0000b330: 6e20 2850 6172 616c 6c65 6c4d 6f64 652e  n (ParallelMode.
-0000b340: 4155 544f 5f50 4152 414c 4c45 4c2c 2920  AUTO_PARALLEL,) 
-0000b350: 616e 6420 5f69 735f 7368 6172 6469 6e67  and _is_sharding
-0000b360: 5f70 726f 7061 6761 7469 6f6e 2829 3a0a  _propagation():.
-0000b370: 2020 2020 2020 2020 2020 2020 6470 203d              dp =
-0000b380: 2070 6172 616c 6c65 6c5f 636f 6e66 6967   parallel_config
-0000b390: 2e64 6174 615f 7061 7261 6c6c 656c 0a20  .data_parallel. 
-0000b3a0: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-0000b3b0: 645f 6d6f 6465 6c20 3d20 645f 6d6f 6465  d_model = d_mode
-0000b3c0: 6c0a 2020 2020 2020 2020 2020 2020 7365  l.            se
-0000b3d0: 6c66 2e65 7870 6572 745f 6469 6d20 3d20  lf.expert_dim = 
-0000b3e0: 6d6f 655f 636f 6e66 6967 2e65 7870 6572  moe_config.exper
-0000b3f0: 745f 6e75 6d0a 2020 2020 2020 2020 2020  t_num.          
-0000b400: 2020 7365 6c66 2e63 6170 6163 6974 795f    self.capacity_
-0000b410: 6661 6374 6f72 203d 206d 6f65 5f63 6f6e  factor = moe_con
-0000b420: 6669 672e 6361 7061 6369 7479 5f66 6163  fig.capacity_fac
-0000b430: 746f 720a 2020 2020 2020 2020 2020 2020  tor.            
-0000b440: 7365 6c66 2e74 7261 696e 696e 6720 3d20  self.training = 
-0000b450: 7472 6169 6e69 6e67 0a20 2020 2020 2020  training.       
-0000b460: 2020 2020 2073 656c 662e 6470 5f67 726f       self.dp_gro
-0000b470: 7570 203d 2064 700a 2020 2020 2020 2020  up = dp.        
-0000b480: 2020 2020 7365 6c66 2e6e 6f69 7379 5f70      self.noisy_p
-0000b490: 6f6c 6963 7920 3d20 4e6f 6e65 0a20 2020  olicy = None.   
-0000b4a0: 2020 2020 2020 2020 2073 656c 662e 6361           self.ca
-0000b4b0: 7374 203d 2050 2e43 6173 7428 290a 2020  st = P.Cast().  
-0000b4c0: 2020 2020 2020 2020 2020 7365 6c66 2e72            self.r
-0000b4d0: 6573 6861 7065 203d 2050 2e52 6573 6861  eshape = P.Resha
-0000b4e0: 7065 2829 0a20 2020 2020 2020 2020 2020  pe().           
-0000b4f0: 2073 656c 662e 7368 6170 6520 3d20 502e   self.shape = P.
-0000b500: 5368 6170 6528 290a 2020 2020 2020 2020  Shape().        
-0000b510: 2020 2020 7365 6c66 2e73 6f66 746d 6178      self.softmax
-0000b520: 203d 2050 2e53 6f66 746d 6178 2861 7869   = P.Softmax(axi
-0000b530: 733d 2d31 290a 2020 2020 2020 2020 2020  s=-1).          
-0000b540: 2020 7365 6c66 2e61 7267 6d61 7820 3d20    self.argmax = 
-0000b550: 502e 4172 674d 6178 5769 7468 5661 6c75  P.ArgMaxWithValu
-0000b560: 6528 6178 6973 3d2d 312c 206b 6565 705f  e(axis=-1, keep_
-0000b570: 6469 6d73 3d46 616c 7365 290a 2020 2020  dims=False).    
-0000b580: 2020 2020 2020 2020 7365 6c66 2e6e 756d          self.num
-0000b590: 5f65 7870 6572 7473 5f63 686f 7365 6e20  _experts_chosen 
-0000b5a0: 3d20 6d6f 655f 636f 6e66 6967 2e6e 756d  = moe_config.num
-0000b5b0: 5f65 7870 6572 7473 5f63 686f 7365 6e0a  _experts_chosen.
-0000b5c0: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-0000b5d0: 2e6f 6e65 686f 7420 3d20 502e 4f6e 6548  .onehot = P.OneH
-0000b5e0: 6f74 2829 0a20 2020 2020 2020 2020 2020  ot().           
-0000b5f0: 2073 656c 662e 6f6e 6568 6f74 3220 3d20   self.onehot2 = 
-0000b600: 502e 4f6e 6548 6f74 2829 0a20 2020 2020  P.OneHot().     
-0000b610: 2020 2020 2020 2073 656c 662e 6f6e 6568         self.oneh
-0000b620: 6f74 3320 3d20 502e 4f6e 6548 6f74 2829  ot3 = P.OneHot()
-0000b630: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-0000b640: 662e 6f6e 5f76 616c 7565 203d 2054 656e  f.on_value = Ten
-0000b650: 736f 7228 312e 302c 206d 7374 7970 652e  sor(1.0, mstype.
-0000b660: 666c 6f61 7433 3229 0a20 2020 2020 2020  float32).       
-0000b670: 2020 2020 2073 656c 662e 6f66 665f 7661       self.off_va
-0000b680: 6c75 6520 3d20 5465 6e73 6f72 2830 2e30  lue = Tensor(0.0
-0000b690: 2c20 6d73 7479 7065 2e66 6c6f 6174 3332  , mstype.float32
-0000b6a0: 290a 0a20 2020 2020 2020 2020 2020 2073  )..            s
-0000b6b0: 656c 662e 7265 6475 6365 5f6d 6561 6e20  elf.reduce_mean 
-0000b6c0: 3d20 502e 5265 6475 6365 4d65 616e 286b  = P.ReduceMean(k
-0000b6d0: 6565 705f 6469 6d73 3d46 616c 7365 290a  eep_dims=False).
-0000b6e0: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-0000b6f0: 2e72 6564 7563 655f 6d65 616e 3220 3d20  .reduce_mean2 = 
-0000b700: 502e 5265 6475 6365 4d65 616e 286b 6565  P.ReduceMean(kee
-0000b710: 705f 6469 6d73 3d46 616c 7365 290a 2020  p_dims=False).  
-0000b720: 2020 2020 2020 2020 2020 7365 6c66 2e72            self.r
-0000b730: 6564 7563 655f 6d65 616e 3320 3d20 502e  educe_mean3 = P.
-0000b740: 5265 6475 6365 4d65 616e 286b 6565 705f  ReduceMean(keep_
-0000b750: 6469 6d73 3d46 616c 7365 290a 2020 2020  dims=False).    
-0000b760: 2020 2020 2020 2020 7365 6c66 2e6d 756c          self.mul
-0000b770: 203d 2050 2e4d 756c 2829 0a20 2020 2020   = P.Mul().     
-0000b780: 2020 2020 2020 2073 656c 662e 6d75 6c32         self.mul2
-0000b790: 203d 2050 2e4d 756c 2829 0a20 2020 2020   = P.Mul().     
-0000b7a0: 2020 2020 2020 2073 656c 662e 6d75 6c33         self.mul3
-0000b7b0: 203d 2050 2e4d 756c 2829 0a20 2020 2020   = P.Mul().     
-0000b7c0: 2020 2020 2020 2073 656c 662e 6d75 6c34         self.mul4
-0000b7d0: 203d 2050 2e4d 756c 2829 0a20 2020 2020   = P.Mul().     
-0000b7e0: 2020 2020 2020 2073 656c 662e 6d75 6c35         self.mul5
-0000b7f0: 203d 2050 2e4d 756c 2829 0a20 2020 2020   = P.Mul().     
-0000b800: 2020 2020 2020 2073 656c 662e 6d75 6c36         self.mul6
-0000b810: 203d 2050 2e4d 756c 2829 0a20 2020 2020   = P.Mul().     
-0000b820: 2020 2020 2020 2073 656c 662e 6d75 6c37         self.mul7
-0000b830: 203d 2050 2e4d 756c 2829 0a20 2020 2020   = P.Mul().     
-0000b840: 2020 2020 2020 2073 656c 662e 6d75 6c38         self.mul8
-0000b850: 203d 2050 2e4d 756c 2829 2e73 6861 7264   = P.Mul().shard
-0000b860: 2828 2864 702c 2031 2c20 3129 2c20 2864  (((dp, 1, 1), (d
-0000b870: 702c 2031 2c20 3129 2929 0a20 2020 2020  p, 1, 1))).     
-0000b880: 2020 2020 2020 2073 656c 662e 6d75 6c39         self.mul9
-0000b890: 203d 2050 2e4d 756c 2829 2e73 6861 7264   = P.Mul().shard
-0000b8a0: 2828 2864 702c 2031 2c20 312c 2031 292c  (((dp, 1, 1, 1),
-0000b8b0: 2028 6470 2c20 312c 2031 2c20 3129 2929   (dp, 1, 1, 1)))
-0000b8c0: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-0000b8d0: 662e 6e6f 745f 6571 7561 6c20 3d20 502e  f.not_equal = P.
-0000b8e0: 4e6f 7445 7175 616c 2829 0a20 2020 2020  NotEqual().     
-0000b8f0: 2020 2020 2020 2073 656c 662e 6469 7631         self.div1
-0000b900: 203d 2050 2e52 6561 6c44 6976 2829 0a20   = P.RealDiv(). 
-0000b910: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-0000b920: 6469 7632 203d 2050 2e52 6561 6c44 6976  div2 = P.RealDiv
-0000b930: 2829 0a20 2020 2020 2020 2020 2020 2073  ().            s
-0000b940: 656c 662e 6164 6420 3d20 502e 4164 6428  elf.add = P.Add(
-0000b950: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
-0000b960: 6c66 2e61 6464 3120 3d20 502e 4164 6428  lf.add1 = P.Add(
-0000b970: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
-0000b980: 6c66 2e61 6464 3220 3d20 502e 4164 6428  lf.add2 = P.Add(
-0000b990: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
-0000b9a0: 6c66 2e61 6464 3320 3d20 502e 4164 6428  lf.add3 = P.Add(
-0000b9b0: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
-0000b9c0: 6c66 2e61 6464 3420 3d20 502e 4164 6428  lf.add4 = P.Add(
-0000b9d0: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
-0000b9e0: 6c66 2e73 7562 203d 2050 2e53 7562 2829  lf.sub = P.Sub()
-0000b9f0: 0a0a 2020 2020 2020 2020 2020 2020 7365  ..            se
-0000ba00: 6c66 2e63 756d 7375 6d20 3d20 502e 4375  lf.cumsum = P.Cu
-0000ba10: 6d53 756d 2865 7863 6c75 7369 7665 3d54  mSum(exclusive=T
-0000ba20: 7275 6529 0a20 2020 2020 2020 2020 2020  rue).           
-0000ba30: 2073 656c 662e 6c65 7373 203d 2050 2e4c   self.less = P.L
-0000ba40: 6573 7328 290a 2020 2020 2020 2020 2020  ess().          
-0000ba50: 2020 7365 6c66 2e72 6564 7563 655f 7375    self.reduce_su
-0000ba60: 6d20 3d20 502e 5265 6475 6365 5375 6d28  m = P.ReduceSum(
-0000ba70: 6b65 6570 5f64 696d 733d 4661 6c73 6529  keep_dims=False)
-0000ba80: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-0000ba90: 662e 7265 6475 6365 5f73 756d 5f6b 6565  f.reduce_sum_kee
-0000baa0: 7020 3d20 502e 5265 6475 6365 5375 6d28  p = P.ReduceSum(
-0000bab0: 6b65 6570 5f64 696d 733d 5472 7565 290a  keep_dims=True).
-0000bac0: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-0000bad0: 2e72 6564 7563 655f 7375 6d5f 6b65 6570  .reduce_sum_keep
-0000bae0: 3220 3d20 502e 5265 6475 6365 5375 6d28  2 = P.ReduceSum(
-0000baf0: 6b65 6570 5f64 696d 733d 5472 7565 290a  keep_dims=True).
-0000bb00: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-0000bb10: 2e65 7870 616e 6420 3d20 502e 4578 7061  .expand = P.Expa
-0000bb20: 6e64 4469 6d73 2829 0a20 2020 2020 2020  ndDims().       
-0000bb30: 2020 2020 2073 656c 662e 6578 7061 6e64       self.expand
-0000bb40: 3220 3d20 502e 4578 7061 6e64 4469 6d73  2 = P.ExpandDims
-0000bb50: 2829 0a20 2020 2020 2020 2020 2020 2073  ().            s
-0000bb60: 656c 662e 6164 645f 7363 616c 6120 3d20  elf.add_scala = 
-0000bb70: 502e 4164 6428 290a 2020 2020 2020 2020  P.Add().        
-0000bb80: 2020 2020 7365 6c66 2e69 6e69 745f 6c6f      self.init_lo
-0000bb90: 7373 203d 2054 656e 736f 7228 302e 302c  ss = Tensor(0.0,
-0000bba0: 206d 7374 7970 652e 666c 6f61 7433 3229   mstype.float32)
-0000bbb0: 0a20 2020 2020 2020 2065 6c73 653a 0a20  .        else:. 
-0000bbc0: 2020 2020 2020 2020 2020 2064 7020 3d20             dp = 
-0000bbd0: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
-0000bbe0: 6461 7461 5f70 6172 616c 6c65 6c0a 2020  data_parallel.  
-0000bbf0: 2020 2020 2020 2020 2020 7365 6c66 2e64            self.d
-0000bc00: 5f6d 6f64 656c 203d 2064 5f6d 6f64 656c  _model = d_model
-0000bc10: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-0000bc20: 662e 6578 7065 7274 5f64 696d 203d 206d  f.expert_dim = m
-0000bc30: 6f65 5f63 6f6e 6669 672e 6578 7065 7274  oe_config.expert
-0000bc40: 5f6e 756d 0a20 2020 2020 2020 2020 2020  _num.           
-0000bc50: 2073 656c 662e 6361 7061 6369 7479 5f66   self.capacity_f
-0000bc60: 6163 746f 7220 3d20 6d6f 655f 636f 6e66  actor = moe_conf
-0000bc70: 6967 2e63 6170 6163 6974 795f 6661 6374  ig.capacity_fact
-0000bc80: 6f72 0a20 2020 2020 2020 2020 2020 2073  or.            s
-0000bc90: 656c 662e 7361 7665 5f74 6f6b 656e 5f64  elf.save_token_d
-0000bca0: 6973 7472 6962 7574 696f 6e20 3d20 6d6f  istribution = mo
-0000bcb0: 655f 636f 6e66 6967 2e73 6176 655f 746f  e_config.save_to
-0000bcc0: 6b65 6e5f 6469 7374 7269 6275 7469 6f6e  ken_distribution
-0000bcd0: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-0000bce0: 662e 656e 6162 6c65 5f63 6f6c 645f 686f  f.enable_cold_ho
-0000bcf0: 745f 6578 7065 7274 203d 206d 6f65 5f63  t_expert = moe_c
-0000bd00: 6f6e 6669 672e 656e 6162 6c65 5f63 6f6c  onfig.enable_col
-0000bd10: 645f 686f 745f 6578 7065 7274 0a20 2020  d_hot_expert.   
-0000bd20: 2020 2020 2020 2020 2073 656c 662e 7472           self.tr
-0000bd30: 6169 6e69 6e67 203d 2074 7261 696e 696e  aining = trainin
-0000bd40: 670a 2020 2020 2020 2020 2020 2020 7365  g.            se
-0000bd50: 6c66 2e64 705f 6772 6f75 7020 3d20 6470  lf.dp_group = dp
-0000bd60: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-0000bd70: 662e 6e6f 6973 795f 706f 6c69 6379 203d  f.noisy_policy =
-0000bd80: 204e 6f6e 650a 2020 2020 2020 2020 2020   None.          
-0000bd90: 2020 7365 6c66 2e63 6173 7420 3d20 502e    self.cast = P.
-0000bda0: 4361 7374 2829 0a20 2020 2020 2020 2020  Cast().         
-0000bdb0: 2020 2073 656c 662e 7265 7368 6170 6520     self.reshape 
-0000bdc0: 3d20 502e 5265 7368 6170 6528 290a 2020  = P.Reshape().  
-0000bdd0: 2020 2020 2020 2020 2020 7365 6c66 2e73            self.s
-0000bde0: 6861 7065 203d 2050 2e53 6861 7065 2829  hape = P.Shape()
-0000bdf0: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-0000be00: 662e 736f 6674 6d61 7820 3d20 502e 536f  f.softmax = P.So
-0000be10: 6674 6d61 7828 6178 6973 3d2d 3129 2e73  ftmax(axis=-1).s
-0000be20: 6861 7264 2828 2864 702c 2031 2c20 312c  hard(((dp, 1, 1,
-0000be30: 292c 2929 0a20 2020 2020 2020 2020 2020  ),)).           
-0000be40: 2073 656c 662e 6172 676d 6178 203d 2050   self.argmax = P
-0000be50: 2e41 7267 4d61 7857 6974 6856 616c 7565  .ArgMaxWithValue
-0000be60: 2861 7869 733d 2d31 2c20 6b65 6570 5f64  (axis=-1, keep_d
-0000be70: 696d 733d 4661 6c73 6529 2e73 6861 7264  ims=False).shard
-0000be80: 2828 2864 702c 2031 2c20 3129 2c29 290a  (((dp, 1, 1),)).
-0000be90: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-0000bea0: 2e6e 756d 5f65 7870 6572 7473 5f63 686f  .num_experts_cho
-0000beb0: 7365 6e20 3d20 6d6f 655f 636f 6e66 6967  sen = moe_config
-0000bec0: 2e6e 756d 5f65 7870 6572 7473 5f63 686f  .num_experts_cho
-0000bed0: 7365 6e0a 2020 2020 2020 2020 2020 2020  sen.            
-0000bee0: 7365 6c66 2e6f 6e65 686f 7420 3d20 502e  self.onehot = P.
-0000bef0: 4f6e 6548 6f74 2829 2e73 6861 7264 2828  OneHot().shard((
-0000bf00: 2864 702c 2031 2c20 3129 2c20 2829 2c20  (dp, 1, 1), (), 
-0000bf10: 2829 2929 0a20 2020 2020 2020 2020 2020  ())).           
-0000bf20: 2073 656c 662e 6f6e 6568 6f74 3220 3d20   self.onehot2 = 
-0000bf30: 502e 4f6e 6548 6f74 2829 2e73 6861 7264  P.OneHot().shard
-0000bf40: 2828 2864 702c 2031 2c20 3129 2c20 2829  (((dp, 1, 1), ()
-0000bf50: 2c20 2829 2929 0a20 2020 2020 2020 2020  , ())).         
-0000bf60: 2020 2073 656c 662e 6f6e 6568 6f74 3320     self.onehot3 
-0000bf70: 3d20 502e 4f6e 6548 6f74 2829 2e73 6861  = P.OneHot().sha
-0000bf80: 7264 2828 2864 702c 2031 2c20 312c 2031  rd(((dp, 1, 1, 1
-0000bf90: 292c 2028 292c 2028 2929 290a 2020 2020  ), (), ())).    
-0000bfa0: 2020 2020 2020 2020 7365 6c66 2e6f 6e5f          self.on_
-0000bfb0: 7661 6c75 6520 3d20 5465 6e73 6f72 2831  value = Tensor(1
-0000bfc0: 2e30 2c20 6d73 7479 7065 2e66 6c6f 6174  .0, mstype.float
-0000bfd0: 3332 290a 2020 2020 2020 2020 2020 2020  32).            
-0000bfe0: 7365 6c66 2e6f 6666 5f76 616c 7565 203d  self.off_value =
-0000bff0: 2054 656e 736f 7228 302e 302c 206d 7374   Tensor(0.0, mst
-0000c000: 7970 652e 666c 6f61 7433 3229 0a0a 2020  ype.float32)..  
-0000c010: 2020 2020 2020 2020 2020 7365 6c66 2e72            self.r
-0000c020: 6564 7563 655f 6d65 616e 203d 2050 2e52  educe_mean = P.R
-0000c030: 6564 7563 654d 6561 6e28 6b65 6570 5f64  educeMean(keep_d
-0000c040: 696d 733d 4661 6c73 6529 2e73 6861 7264  ims=False).shard
-0000c050: 2828 2864 702c 2031 2c20 3129 2c29 290a  (((dp, 1, 1),)).
-0000c060: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-0000c070: 2e72 6564 7563 655f 6d65 616e 3220 3d20  .reduce_mean2 = 
-0000c080: 502e 5265 6475 6365 4d65 616e 286b 6565  P.ReduceMean(kee
-0000c090: 705f 6469 6d73 3d46 616c 7365 292e 7368  p_dims=False).sh
-0000c0a0: 6172 6428 2828 6470 2c20 312c 2031 292c  ard(((dp, 1, 1),
-0000c0b0: 2929 0a20 2020 2020 2020 2020 2020 2073  )).            s
-0000c0c0: 656c 662e 7265 6475 6365 5f6d 6561 6e33  elf.reduce_mean3
-0000c0d0: 203d 2050 2e52 6564 7563 654d 6561 6e28   = P.ReduceMean(
-0000c0e0: 6b65 6570 5f64 696d 733d 4661 6c73 6529  keep_dims=False)
-0000c0f0: 2e73 6861 7264 2828 2864 702c 2031 292c  .shard(((dp, 1),
-0000c100: 2929 0a20 2020 2020 2020 2020 2020 2073  )).            s
-0000c110: 656c 662e 6d75 6c20 3d20 502e 4d75 6c28  elf.mul = P.Mul(
-0000c120: 292e 7368 6172 6428 2828 6470 2c20 3129  ).shard(((dp, 1)
-0000c130: 2c20 2864 702c 2031 2929 290a 2020 2020  , (dp, 1))).    
-0000c140: 2020 2020 2020 2020 7365 6c66 2e6d 756c          self.mul
-0000c150: 3220 3d20 502e 4d75 6c28 292e 7368 6172  2 = P.Mul().shar
-0000c160: 6428 2828 292c 2028 2929 290a 2020 2020  d(((), ())).    
-0000c170: 2020 2020 2020 2020 7365 6c66 2e6d 756c          self.mul
-0000c180: 3320 3d20 502e 4d75 6c28 292e 7368 6172  3 = P.Mul().shar
-0000c190: 6428 2828 292c 2028 2929 290a 2020 2020  d(((), ())).    
-0000c1a0: 2020 2020 2020 2020 7365 6c66 2e6d 756c          self.mul
-0000c1b0: 3420 3d20 502e 4d75 6c28 292e 7368 6172  4 = P.Mul().shar
-0000c1c0: 6428 2828 6470 2c20 312c 2031 292c 2028  d(((dp, 1, 1), (
-0000c1d0: 6470 2c20 312c 2031 2929 290a 2020 2020  dp, 1, 1))).    
-0000c1e0: 2020 2020 2020 2020 7365 6c66 2e6d 756c          self.mul
-0000c1f0: 3520 3d20 502e 4d75 6c28 292e 7368 6172  5 = P.Mul().shar
-0000c200: 6428 2828 6470 2c20 312c 2031 292c 2028  d(((dp, 1, 1), (
-0000c210: 6470 2c20 312c 2031 2929 290a 2020 2020  dp, 1, 1))).    
-0000c220: 2020 2020 2020 2020 7365 6c66 2e6d 756c          self.mul
-0000c230: 3620 3d20 502e 4d75 6c28 292e 7368 6172  6 = P.Mul().shar
-0000c240: 6428 2828 6470 2c20 3129 2c20 2864 702c  d(((dp, 1), (dp,
-0000c250: 2031 2929 290a 2020 2020 2020 2020 2020   1))).          
-0000c260: 2020 7365 6c66 2e6d 756c 3720 3d20 502e    self.mul7 = P.
-0000c270: 4d75 6c28 292e 7368 6172 6428 2828 6470  Mul().shard(((dp
-0000c280: 2c20 3129 2c20 2864 702c 2031 2929 290a  , 1), (dp, 1))).
-0000c290: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-0000c2a0: 2e6d 756c 3820 3d20 502e 4d75 6c28 292e  .mul8 = P.Mul().
-0000c2b0: 7368 6172 6428 2828 6470 2c20 312c 2031  shard(((dp, 1, 1
-0000c2c0: 292c 2028 6470 2c20 312c 2031 2929 290a  ), (dp, 1, 1))).
-0000c2d0: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-0000c2e0: 2e6d 756c 3920 3d20 502e 4d75 6c28 292e  .mul9 = P.Mul().
-0000c2f0: 7368 6172 6428 2828 6470 2c20 312c 2031  shard(((dp, 1, 1
-0000c300: 2c20 3129 2c20 2864 702c 2031 2c20 312c  , 1), (dp, 1, 1,
-0000c310: 2031 2929 290a 2020 2020 2020 2020 2020   1))).          
-0000c320: 2020 7365 6c66 2e6e 6f74 5f65 7175 616c    self.not_equal
-0000c330: 203d 2050 2e4e 6f74 4571 7561 6c28 292e   = P.NotEqual().
-0000c340: 7368 6172 6428 2828 6470 2c20 312c 2031  shard(((dp, 1, 1
-0000c350: 2c20 3129 2c20 2829 2929 0a20 2020 2020  , 1), ())).     
-0000c360: 2020 2020 2020 2073 656c 662e 6469 7631         self.div1
-0000c370: 203d 2050 2e52 6561 6c44 6976 2829 2e73   = P.RealDiv().s
-0000c380: 6861 7264 2828 2864 702c 2031 2c20 3129  hard(((dp, 1, 1)
-0000c390: 2c20 2864 702c 2031 2c20 3129 2929 0a20  , (dp, 1, 1))). 
-0000c3a0: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-0000c3b0: 6469 7632 203d 2050 2e52 6561 6c44 6976  div2 = P.RealDiv
-0000c3c0: 2829 2e73 6861 7264 2828 2864 702c 2031  ().shard(((dp, 1
-0000c3d0: 2c20 312c 2031 292c 2028 6470 2c20 312c  , 1, 1), (dp, 1,
-0000c3e0: 2031 2c20 3129 2929 0a20 2020 2020 2020   1, 1))).       
-0000c3f0: 2020 2020 2073 656c 662e 6164 6420 3d20       self.add = 
-0000c400: 502e 4164 6428 292e 7368 6172 6428 2828  P.Add().shard(((
-0000c410: 6470 2c20 312c 2031 292c 2028 6470 2c20  dp, 1, 1), (dp, 
-0000c420: 312c 2031 2929 290a 2020 2020 2020 2020  1, 1))).        
-0000c430: 2020 2020 7365 6c66 2e61 6464 3120 3d20      self.add1 = 
-0000c440: 502e 4164 6428 292e 7368 6172 6428 2828  P.Add().shard(((
-0000c450: 6470 2c20 312c 2031 292c 2028 2929 290a  dp, 1, 1), ())).
-0000c460: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-0000c470: 2e61 6464 3220 3d20 502e 4164 6428 292e  .add2 = P.Add().
-0000c480: 7368 6172 6428 2828 6470 2c20 312c 2031  shard(((dp, 1, 1
-0000c490: 2c20 3129 2c20 2864 702c 2031 2c20 312c  , 1), (dp, 1, 1,
-0000c4a0: 2031 2929 290a 2020 2020 2020 2020 2020   1))).          
-0000c4b0: 2020 7365 6c66 2e61 6464 3320 3d20 502e    self.add3 = P.
-0000c4c0: 4164 6428 292e 7368 6172 6428 2828 6470  Add().shard(((dp
-0000c4d0: 2c20 3129 2c20 2864 702c 2031 2929 290a  , 1), (dp, 1))).
-0000c4e0: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-0000c4f0: 2e61 6464 3420 3d20 502e 4164 6428 292e  .add4 = P.Add().
-0000c500: 7368 6172 6428 2828 6470 2c20 312c 2031  shard(((dp, 1, 1
-0000c510: 2c20 3129 2c20 2829 2929 0a20 2020 2020  , 1), ())).     
-0000c520: 2020 2020 2020 2073 656c 662e 7375 6220         self.sub 
-0000c530: 3d20 502e 5375 6228 292e 7368 6172 6428  = P.Sub().shard(
-0000c540: 2828 292c 2028 6470 2c20 312c 2031 2929  ((), (dp, 1, 1))
-0000c550: 290a 0a20 2020 2020 2020 2020 2020 2073  )..            s
-0000c560: 656c 662e 6375 6d73 756d 203d 2050 2e43  elf.cumsum = P.C
-0000c570: 756d 5375 6d28 6578 636c 7573 6976 653d  umSum(exclusive=
-0000c580: 5472 7565 292e 7368 6172 6428 2828 6470  True).shard(((dp
-0000c590: 2c20 312c 2031 292c 2929 0a20 2020 2020  , 1, 1),)).     
-0000c5a0: 2020 2020 2020 2073 656c 662e 6c65 7373         self.less
-0000c5b0: 203d 2050 2e4c 6573 7328 292e 7368 6172   = P.Less().shar
-0000c5c0: 6428 2828 6470 2c20 312c 2031 292c 2028  d(((dp, 1, 1), (
-0000c5d0: 2929 290a 2020 2020 2020 2020 2020 2020  ))).            
-0000c5e0: 7365 6c66 2e72 6564 7563 655f 7375 6d20  self.reduce_sum 
-0000c5f0: 3d20 502e 5265 6475 6365 5375 6d28 6b65  = P.ReduceSum(ke
-0000c600: 6570 5f64 696d 733d 4661 6c73 6529 2e73  ep_dims=False).s
-0000c610: 6861 7264 2828 2864 702c 2031 2c20 3129  hard(((dp, 1, 1)
-0000c620: 2c29 290a 2020 2020 2020 2020 2020 2020  ,)).            
-0000c630: 7365 6c66 2e72 6564 7563 655f 7375 6d5f  self.reduce_sum_
-0000c640: 6b65 6570 203d 2050 2e52 6564 7563 6553  keep = P.ReduceS
-0000c650: 756d 286b 6565 705f 6469 6d73 3d54 7275  um(keep_dims=Tru
-0000c660: 6529 2e73 6861 7264 2828 2864 702c 2031  e).shard(((dp, 1
-0000c670: 2c20 3129 2c29 290a 2020 2020 2020 2020  , 1),)).        
-0000c680: 2020 2020 7365 6c66 2e72 6564 7563 655f      self.reduce_
-0000c690: 7375 6d5f 6b65 6570 3220 3d20 502e 5265  sum_keep2 = P.Re
-0000c6a0: 6475 6365 5375 6d28 6b65 6570 5f64 696d  duceSum(keep_dim
-0000c6b0: 733d 5472 7565 292e 7368 6172 6428 2828  s=True).shard(((
-0000c6c0: 6470 2c20 312c 2031 2c20 3129 2c29 290a  dp, 1, 1, 1),)).
-0000c6d0: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-0000c6e0: 2e65 7870 616e 6420 3d20 502e 4578 7061  .expand = P.Expa
-0000c6f0: 6e64 4469 6d73 2829 2e73 6861 7264 2828  ndDims().shard((
-0000c700: 2864 702c 2031 292c 2929 0a20 2020 2020  (dp, 1),)).     
-0000c710: 2020 2020 2020 2073 656c 662e 6578 7061         self.expa
-0000c720: 6e64 3220 3d20 502e 4578 7061 6e64 4469  nd2 = P.ExpandDi
-0000c730: 6d73 2829 2e73 6861 7264 2828 2864 702c  ms().shard(((dp,
-0000c740: 2031 2c20 3129 2c29 290a 2020 2020 2020   1, 1),)).      
-0000c750: 2020 2020 2020 7365 6c66 2e61 6464 5f73        self.add_s
-0000c760: 6361 6c61 203d 2050 2e41 6464 2829 2e73  cala = P.Add().s
-0000c770: 6861 7264 2828 2829 2c20 2829 2929 0a20  hard(((), ())). 
-0000c780: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-0000c790: 696e 6974 5f6c 6f73 7320 3d20 5465 6e73  init_loss = Tens
-0000c7a0: 6f72 2830 2e30 2c20 6d73 7479 7065 2e66  or(0.0, mstype.f
-0000c7b0: 6c6f 6174 3332 290a 2020 2020 2020 2020  loat32).        
-0000c7c0: 2020 2020 6966 2073 656c 662e 7361 7665      if self.save
-0000c7d0: 5f74 6f6b 656e 5f64 6973 7472 6962 7574  _token_distribut
-0000c7e0: 696f 6e3a 0a20 2020 2020 2020 2020 2020  ion:.           
-0000c7f0: 2020 2020 2073 656c 662e 6375 725f 6c61       self.cur_la
-0000c800: 7965 7220 3d20 6d6f 655f 636f 6e66 6967  yer = moe_config
-0000c810: 2e63 7572 5f6c 6179 6572 0a20 2020 2020  .cur_layer.     
-0000c820: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-0000c830: 7465 6e73 6f72 5f73 756d 6d61 7279 203d  tensor_summary =
-0000c840: 2050 2e54 656e 736f 7253 756d 6d61 7279   P.TensorSummary
-0000c850: 2829 0a20 2020 2020 2020 2020 2020 2069  ().            i
-0000c860: 6620 7365 6c66 2e65 6e61 626c 655f 636f  f self.enable_co
-0000c870: 6c64 5f68 6f74 5f65 7870 6572 743a 0a20  ld_hot_expert:. 
-0000c880: 2020 2020 2020 2020 2020 2020 2020 2073                 s
-0000c890: 656c 662e 6375 725f 6c61 7965 7220 3d20  elf.cur_layer = 
-0000c8a0: 6d6f 655f 636f 6e66 6967 2e63 7572 5f6c  moe_config.cur_l
-0000c8b0: 6179 6572 0a20 2020 2020 2020 2020 2020  ayer.           
-0000c8c0: 2020 2020 2073 656c 662e 6375 6d73 756d       self.cumsum
-0000c8d0: 5f76 616c 7565 203d 2050 6172 616d 6574  _value = Paramet
-0000c8e0: 6572 2869 6e69 7469 616c 697a 6572 2827  er(initializer('
-0000c8f0: 7a65 726f 7327 2c20 2873 656c 662e 6578  zeros', (self.ex
-0000c900: 7065 7274 5f64 696d 2c29 2c20 6d73 7479  pert_dim,), msty
-0000c910: 7065 2e69 6e74 3332 292c 0a20 2020 2020  pe.int32),.     
-0000c920: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000c930: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000c940: 2020 2020 2020 2020 206e 616d 653d 2263           name="c
-0000c950: 756d 7375 6d5f 7661 6c75 6522 2b73 7472  umsum_value"+str
-0000c960: 2873 656c 662e 6375 725f 6c61 7965 7229  (self.cur_layer)
-0000c970: 2c20 7265 7175 6972 6573 5f67 7261 643d  , requires_grad=
-0000c980: 4661 6c73 652c 0a20 2020 2020 2020 2020  False,.         
-0000c990: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000c9a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000c9b0: 2020 2020 2070 6172 616c 6c65 6c5f 6f70       parallel_op
-0000c9c0: 7469 6d69 7a65 723d 4661 6c73 6529 0a20  timizer=False). 
-0000c9d0: 2020 2020 2020 2020 2020 2020 2020 2073                 s
-0000c9e0: 656c 662e 6173 7369 676e 203d 2050 2e41  elf.assign = P.A
-0000c9f0: 7373 6967 6e28 292e 7368 6172 6428 2828  ssign().shard(((
-0000ca00: 312c 292c 2028 312c 2929 290a 0a20 2020  1,), (1,)))..   
-0000ca10: 2064 6566 2063 6f6e 7374 7275 6374 2873   def construct(s
-0000ca20: 656c 662c 2072 6f75 7465 725f 6c6f 6769  elf, router_logi
-0000ca30: 7473 293a 0a20 2020 2020 2020 2022 2222  ts):.        """
-0000ca40: 666f 7277 6172 6420 7072 6f63 6573 7322  forward process"
-0000ca50: 2222 0a20 2020 2020 2020 2072 6f75 7465  "".        route
-0000ca60: 725f 6c6f 6769 7473 5f73 6861 7065 203d  r_logits_shape =
-0000ca70: 2073 656c 662e 7368 6170 6528 726f 7574   self.shape(rout
-0000ca80: 6572 5f6c 6f67 6974 7329 0a20 2020 2020  er_logits).     
-0000ca90: 2020 2072 6f75 7465 725f 6c6f 6769 7473     router_logits
-0000caa0: 203d 2073 656c 662e 7265 7368 6170 6528   = self.reshape(
-0000cab0: 726f 7574 6572 5f6c 6f67 6974 732c 2028  router_logits, (
-0000cac0: 2d31 2c20 726f 7574 6572 5f6c 6f67 6974  -1, router_logit
-0000cad0: 735f 7368 6170 655b 2d31 5d29 290a 2020  s_shape[-1])).  
-0000cae0: 2020 2020 2020 6c6f 6769 7473 5f73 6861        logits_sha
-0000caf0: 7065 203d 2073 656c 662e 7368 6170 6528  pe = self.shape(
-0000cb00: 726f 7574 6572 5f6c 6f67 6974 7329 0a20  router_logits). 
-0000cb10: 2020 2020 2020 2074 6f6b 656e 735f 7065         tokens_pe
-0000cb20: 725f 6772 6f75 7020 3d20 6c6f 6769 7473  r_group = logits
-0000cb30: 5f73 6861 7065 5b30 5d20 2f2f 2073 656c  _shape[0] // sel
-0000cb40: 662e 6470 5f67 726f 7570 0a20 2020 2020  f.dp_group.     
-0000cb50: 2020 2065 7870 6572 745f 6361 7061 6369     expert_capaci
-0000cb60: 7479 203d 2063 616c 6375 6c61 7465 5f65  ty = calculate_e
-0000cb70: 7870 6572 745f 6361 7061 6369 7479 2873  xpert_capacity(s
-0000cb80: 656c 662e 6e75 6d5f 6578 7065 7274 735f  elf.num_experts_
-0000cb90: 6368 6f73 656e 2c20 746f 6b65 6e73 5f70  chosen, tokens_p
-0000cba0: 6572 5f67 726f 7570 2c20 7365 6c66 2e63  er_group, self.c
-0000cbb0: 6170 6163 6974 795f 6661 6374 6f72 2c0a  apacity_factor,.
-0000cbc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000cbd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000cbe0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000cbf0: 2020 2020 7365 6c66 2e65 7870 6572 745f      self.expert_
-0000cc00: 6469 6d29 0a20 2020 2020 2020 2072 6f75  dim).        rou
-0000cc10: 7465 725f 6c6f 6769 7473 203d 2073 656c  ter_logits = sel
-0000cc20: 662e 7265 7368 6170 6528 726f 7574 6572  f.reshape(router
-0000cc30: 5f6c 6f67 6974 732c 2028 7365 6c66 2e64  _logits, (self.d
-0000cc40: 705f 6772 6f75 702c 2074 6f6b 656e 735f  p_group, tokens_
-0000cc50: 7065 725f 6772 6f75 702c 2073 656c 662e  per_group, self.
-0000cc60: 6578 7065 7274 5f64 696d 2929 0a0a 2020  expert_dim))..  
-0000cc70: 2020 2020 2020 6163 6375 6d5f 6578 7065        accum_expe
-0000cc80: 7274 5f6d 6173 6b20 3d20 300a 2020 2020  rt_mask = 0.    
-0000cc90: 2020 2020 6163 6375 6d5f 6578 7065 7274      accum_expert
-0000cca0: 5f67 6174 6520 3d20 300a 2020 2020 2020  _gate = 0.      
-0000ccb0: 2020 6c6f 7373 203d 2073 656c 662e 696e    loss = self.in
-0000ccc0: 6974 5f6c 6f73 730a 2020 2020 2020 2020  it_loss.        
-0000ccd0: 6d61 736b 5f63 6f75 6e74 203d 2030 0a20  mask_count = 0. 
-0000cce0: 2020 2020 2020 2061 6363 756d 5f63 6f6d         accum_com
-0000ccf0: 6269 6e65 5f74 656e 736f 7220 3d20 300a  bine_tensor = 0.
-0000cd00: 2020 2020 2020 2020 2320 5072 6f62 6162          # Probab
-0000cd10: 696c 6974 6965 7320 666f 7220 6561 6368  ilities for each
-0000cd20: 2074 6f6b 656e 206f 6620 7768 6174 2065   token of what e
-0000cd30: 7870 6572 7420 6973 2073 686f 756c 6420  xpert is should 
-0000cd40: 6265 2073 656e 7420 746f 0a20 2020 2020  be sent to.     
-0000cd50: 2020 2072 6f75 7465 725f 7072 6f62 203d     router_prob =
-0000cd60: 2073 656c 662e 736f 6674 6d61 7828 726f   self.softmax(ro
-0000cd70: 7574 6572 5f6c 6f67 6974 7329 0a0a 2020  uter_logits)..  
-0000cd80: 2020 2020 2020 666f 7220 6578 7065 7274        for expert
-0000cd90: 5f63 686f 7365 6e5f 696e 6465 7820 696e  _chosen_index in
-0000cda0: 2072 616e 6765 2873 656c 662e 6e75 6d5f   range(self.num_
-0000cdb0: 6578 7065 7274 735f 6368 6f73 656e 293a  experts_chosen):
-0000cdc0: 0a20 2020 2020 2020 2020 2020 2023 2066  .            # f
-0000cdd0: 6f72 2065 6163 6820 746f 6b65 6e2c 2073  or each token, s
-0000cde0: 6574 2074 6865 2072 6f75 7465 725f 7072  et the router_pr
-0000cdf0: 6f62 206f 6620 7468 6520 7365 6c65 6374  ob of the select
-0000ce00: 6564 2065 7870 6572 7473 2074 6f20 7a65  ed experts to ze
-0000ce10: 726f 0a20 2020 2020 2020 2020 2020 2072  ro.            r
-0000ce20: 6f75 7465 725f 7072 6f62 203d 2073 656c  outer_prob = sel
-0000ce30: 662e 6d75 6c34 2872 6f75 7465 725f 7072  f.mul4(router_pr
-0000ce40: 6f62 2c20 7365 6c66 2e73 7562 2873 656c  ob, self.sub(sel
-0000ce50: 662e 6f6e 5f76 616c 7565 2c20 6163 6375  f.on_value, accu
-0000ce60: 6d5f 6578 7065 7274 5f6d 6173 6b29 290a  m_expert_mask)).
-0000ce70: 2020 2020 2020 2020 2020 2020 2320 7368              # sh
-0000ce80: 6170 6520 6973 203a 2028 6470 5f67 726f  ape is : (dp_gro
-0000ce90: 7570 2c20 746f 6b65 6e73 5f70 6572 5f67  up, tokens_per_g
-0000cea0: 726f 7570 290a 2020 2020 2020 2020 2020  roup).          
-0000ceb0: 2020 6578 7065 7274 5f69 6e64 6578 2c20    expert_index, 
-0000cec0: 6578 7065 7274 5f67 6174 6520 3d20 7365  expert_gate = se
-0000ced0: 6c66 2e61 7267 6d61 7828 726f 7574 6572  lf.argmax(router
-0000cee0: 5f70 726f 6229 0a20 2020 2020 2020 2020  _prob).         
-0000cef0: 2020 2023 2065 7870 6572 745f 6d61 736b     # expert_mask
-0000cf00: 2773 2073 6861 7065 3a20 2864 705f 6772  's shape: (dp_gr
-0000cf10: 6f75 702c 2074 6f6b 656e 735f 7065 725f  oup, tokens_per_
-0000cf20: 6772 6f75 702c 2073 656c 662e 6578 7065  group, self.expe
-0000cf30: 7274 5f64 696d 290a 2020 2020 2020 2020  rt_dim).        
-0000cf40: 2020 2020 6578 7065 7274 5f6d 6173 6b20      expert_mask 
-0000cf50: 3d20 7365 6c66 2e6f 6e65 686f 7428 6578  = self.onehot(ex
-0000cf60: 7065 7274 5f69 6e64 6578 2c20 7365 6c66  pert_index, self
-0000cf70: 2e65 7870 6572 745f 6469 6d2c 2073 656c  .expert_dim, sel
-0000cf80: 662e 6f6e 5f76 616c 7565 2c20 7365 6c66  f.on_value, self
-0000cf90: 2e6f 6666 5f76 616c 7565 290a 2020 2020  .off_value).    
-0000cfa0: 2020 2020 2020 2020 2320 7265 6e6f 726d          # renorm
-0000cfb0: 616c 697a 6520 7468 6520 7265 7374 2070  alize the rest p
-0000cfc0: 726f 6220 746f 2062 6520 6f66 2073 756d  rob to be of sum
-0000cfd0: 2031 0a20 2020 2020 2020 2020 2020 2072   1.            r
-0000cfe0: 6f75 7465 725f 7072 6f62 5f6e 6f72 6d61  outer_prob_norma
-0000cff0: 6c20 3d20 7365 6c66 2e64 6976 3128 726f  l = self.div1(ro
-0000d000: 7574 6572 5f70 726f 622c 2073 656c 662e  uter_prob, self.
-0000d010: 6164 6431 2873 656c 662e 7265 6475 6365  add1(self.reduce
-0000d020: 5f73 756d 5f6b 6565 7028 726f 7574 6572  _sum_keep(router
-0000d030: 5f70 726f 622c 202d 3129 2c20 3165 2d39  _prob, -1), 1e-9
-0000d040: 2929 0a0a 2020 2020 2020 2020 2020 2020  ))..            
-0000d050: 2320 7468 6520 6261 6c61 6e63 6520 6c6f  # the balance lo
-0000d060: 7373 2069 7320 636f 6d70 7574 6564 2061  ss is computed a
-0000d070: 7420 6561 6368 2072 6f75 7469 6e67 2073  t each routing s
-0000d080: 7465 700a 2020 2020 2020 2020 2020 2020  tep.            
-0000d090: 6c6f 7373 203d 2073 656c 662e 6164 645f  loss = self.add_
-0000d0a0: 7363 616c 6128 6c6f 7373 2c20 7365 6c66  scala(loss, self
-0000d0b0: 2e5f 6175 7869 6c69 6172 795f 6c6f 7373  ._auxiliary_loss
-0000d0c0: 2865 7870 6572 745f 6d61 736b 2c20 726f  (expert_mask, ro
-0000d0d0: 7574 6572 5f70 726f 625f 6e6f 726d 616c  uter_prob_normal
-0000d0e0: 2929 0a0a 2020 2020 2020 2020 2020 2020  ))..            
-0000d0f0: 6f75 7470 7574 203d 2073 656c 662e 5f6d  output = self._m
-0000d100: 6173 6b6f 7574 5f6f 7665 7266 6c6f 7765  askout_overflowe
-0000d110: 645f 746f 6b65 6e73 2865 7870 6572 745f  d_tokens(expert_
-0000d120: 6d61 736b 2c20 6578 7065 7274 5f63 6170  mask, expert_cap
-0000d130: 6163 6974 792c 2065 7870 6572 745f 6761  acity, expert_ga
-0000d140: 7465 2c0a 2020 2020 2020 2020 2020 2020  te,.            
-0000d150: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000d160: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000d170: 2020 2020 2020 2020 206d 6173 6b5f 636f           mask_co
-0000d180: 756e 742c 2065 7870 6572 745f 6368 6f73  unt, expert_chos
-0000d190: 656e 5f69 6e64 6578 290a 2020 2020 2020  en_index).      
-0000d1a0: 2020 2020 2020 6578 7065 7274 5f6d 6173        expert_mas
-0000d1b0: 6b2c 2065 7870 6572 745f 6761 7465 2c20  k, expert_gate, 
-0000d1c0: 6578 7065 7274 5f6d 6173 6b5f 666c 6174  expert_mask_flat
-0000d1d0: 2c20 706f 7369 7469 6f6e 5f69 6e5f 6578  , position_in_ex
-0000d1e0: 7065 7274 203d 206f 7574 7075 745b 305d  pert = output[0]
-0000d1f0: 2c20 6f75 7470 7574 5b31 5d2c 206f 7574  , output[1], out
-0000d200: 7075 745b 325d 2c20 6f75 7470 7574 5b33  put[2], output[3
-0000d210: 5d0a 2020 2020 2020 2020 2020 2020 6163  ].            ac
-0000d220: 6375 6d5f 6578 7065 7274 5f6d 6173 6b20  cum_expert_mask 
-0000d230: 3d20 7365 6c66 2e61 6464 2861 6363 756d  = self.add(accum
-0000d240: 5f65 7870 6572 745f 6d61 736b 2c20 6578  _expert_mask, ex
-0000d250: 7065 7274 5f6d 6173 6b29 0a20 2020 2020  pert_mask).     
-0000d260: 2020 2020 2020 2061 6363 756d 5f65 7870         accum_exp
-0000d270: 6572 745f 6761 7465 203d 2073 656c 662e  ert_gate = self.
-0000d280: 6164 6433 2861 6363 756d 5f65 7870 6572  add3(accum_exper
-0000d290: 745f 6761 7465 2c20 6578 7065 7274 5f67  t_gate, expert_g
-0000d2a0: 6174 6529 0a20 2020 2020 2020 2020 2020  ate).           
-0000d2b0: 206d 6173 6b5f 636f 756e 7420 3d20 7365   mask_count = se
-0000d2c0: 6c66 2e61 6464 286d 6173 6b5f 636f 756e  lf.add(mask_coun
-0000d2d0: 742c 2073 656c 662e 7265 6475 6365 5f73  t, self.reduce_s
-0000d2e0: 756d 5f6b 6565 7028 6578 7065 7274 5f6d  um_keep(expert_m
-0000d2f0: 6173 6b2c 2031 2929 0a0a 2020 2020 2020  ask, 1))..      
-0000d300: 2020 2020 2020 2320 636f 6d62 696e 655f        # combine_
-0000d310: 7465 6e73 6f72 2773 2073 6861 7065 3a20  tensor's shape: 
-0000d320: 2864 705f 6772 6f75 702c 2074 6f6b 656e  (dp_group, token
-0000d330: 735f 7065 725f 6772 6f75 7029 0a20 2020  s_per_group).   
-0000d340: 2020 2020 2020 2020 2063 6f6d 6269 6e65           combine
-0000d350: 5f74 656e 736f 7220 3d20 7365 6c66 2e6d  _tensor = self.m
-0000d360: 756c 3728 6578 7065 7274 5f67 6174 652c  ul7(expert_gate,
-0000d370: 2065 7870 6572 745f 6d61 736b 5f66 6c61   expert_mask_fla
-0000d380: 7429 0a20 2020 2020 2020 2020 2020 2023  t).            #
-0000d390: 2063 6f6d 6269 6e65 5f74 656e 736f 7227   combine_tensor'
-0000d3a0: 7320 7368 6170 653a 2028 6470 5f67 726f  s shape: (dp_gro
-0000d3b0: 7570 2c20 746f 6b65 6e73 5f70 6572 5f67  up, tokens_per_g
-0000d3c0: 726f 7570 2c20 7365 6c66 2e65 7870 6572  roup, self.exper
-0000d3d0: 745f 6469 6d29 0a20 2020 2020 2020 2020  t_dim).         
-0000d3e0: 2020 2063 6f6d 6269 6e65 5f74 656e 736f     combine_tenso
-0000d3f0: 7220 3d20 7365 6c66 2e6d 756c 3828 7365  r = self.mul8(se
-0000d400: 6c66 2e65 7870 616e 6428 636f 6d62 696e  lf.expand(combin
-0000d410: 655f 7465 6e73 6f72 2c20 2d31 292c 0a20  e_tensor, -1),. 
-0000d420: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000d430: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000d440: 2020 2020 2020 7365 6c66 2e6f 6e65 686f        self.oneho
-0000d450: 7432 2865 7870 6572 745f 696e 6465 782c  t2(expert_index,
-0000d460: 2073 656c 662e 6578 7065 7274 5f64 696d   self.expert_dim
-0000d470: 2c20 7365 6c66 2e6f 6e5f 7661 6c75 652c  , self.on_value,
-0000d480: 2073 656c 662e 6f66 665f 7661 6c75 6529   self.off_value)
-0000d490: 290a 2020 2020 2020 2020 2020 2020 2320  ).            # 
-0000d4a0: 636f 6d62 696e 655f 7465 6e73 6f72 2773  combine_tensor's
-0000d4b0: 2073 6861 7065 3a20 2864 705f 6772 6f75   shape: (dp_grou
-0000d4c0: 702c 2074 6f6b 656e 735f 7065 725f 6772  p, tokens_per_gr
-0000d4d0: 6f75 702c 2073 656c 662e 6578 7065 7274  oup, self.expert
-0000d4e0: 5f64 696d 2c20 7365 6c66 2e65 7870 6572  _dim, self.exper
-0000d4f0: 745f 6361 7061 6369 7479 290a 2020 2020  t_capacity).    
-0000d500: 2020 2020 2020 2020 636f 6d62 696e 655f          combine_
-0000d510: 7465 6e73 6f72 203d 2073 656c 662e 6d75  tensor = self.mu
-0000d520: 6c39 2873 656c 662e 6578 7061 6e64 3228  l9(self.expand2(
-0000d530: 636f 6d62 696e 655f 7465 6e73 6f72 2c20  combine_tensor, 
-0000d540: 2d31 292c 0a20 2020 2020 2020 2020 2020  -1),.           
-0000d550: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000d560: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-0000d570: 2e6f 6e65 686f 7433 2873 656c 662e 6361  .onehot3(self.ca
-0000d580: 7374 2870 6f73 6974 696f 6e5f 696e 5f65  st(position_in_e
-0000d590: 7870 6572 742c 206d 7374 7970 652e 696e  xpert, mstype.in
-0000d5a0: 7433 3229 2c20 6578 7065 7274 5f63 6170  t32), expert_cap
-0000d5b0: 6163 6974 792c 0a20 2020 2020 2020 2020  acity,.         
-0000d5c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000d5d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000d5e0: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-0000d5f0: 6f6e 5f76 616c 7565 2c20 7365 6c66 2e6f  on_value, self.o
-0000d600: 6666 5f76 616c 7565 2929 0a20 2020 2020  ff_value)).     
-0000d610: 2020 2020 2020 2061 6363 756d 5f63 6f6d         accum_com
-0000d620: 6269 6e65 5f74 656e 736f 7220 3d20 7365  bine_tensor = se
-0000d630: 6c66 2e61 6464 3228 6163 6375 6d5f 636f  lf.add2(accum_co
-0000d640: 6d62 696e 655f 7465 6e73 6f72 2c20 636f  mbine_tensor, co
-0000d650: 6d62 696e 655f 7465 6e73 6f72 290a 0a20  mbine_tensor).. 
-0000d660: 2020 2020 2020 2023 2065 7870 6572 7420         # expert 
-0000d670: 7765 6967 6874 7320 6e6f 726d 616c 697a  weights normaliz
-0000d680: 6174 696f 6e0a 2020 2020 2020 2020 636f  ation.        co
-0000d690: 6d62 696e 655f 7465 6e73 6f72 5f73 756d  mbine_tensor_sum
-0000d6a0: 203d 2073 656c 662e 7265 6475 6365 5f73   = self.reduce_s
-0000d6b0: 756d 5f6b 6565 7032 2873 656c 662e 7265  um_keep2(self.re
-0000d6c0: 6475 6365 5f73 756d 5f6b 6565 7032 2861  duce_sum_keep2(a
-0000d6d0: 6363 756d 5f63 6f6d 6269 6e65 5f74 656e  ccum_combine_ten
-0000d6e0: 736f 722c 202d 3129 2c20 2d32 290a 2020  sor, -1), -2).  
-0000d6f0: 2020 2020 2020 6163 6375 6d5f 636f 6d62        accum_comb
-0000d700: 696e 655f 7465 6e73 6f72 203d 2073 656c  ine_tensor = sel
-0000d710: 662e 6469 7632 2861 6363 756d 5f63 6f6d  f.div2(accum_com
-0000d720: 6269 6e65 5f74 656e 736f 722c 2073 656c  bine_tensor, sel
-0000d730: 662e 6164 6434 2863 6f6d 6269 6e65 5f74  f.add4(combine_t
-0000d740: 656e 736f 725f 7375 6d2c 2031 652d 3929  ensor_sum, 1e-9)
-0000d750: 290a 2020 2020 2020 2020 2320 6469 7370  ).        # disp
-0000d760: 6174 6368 5f74 656e 736f 7220 6973 206f  atch_tensor is o
-0000d770: 6620 626f 6f6c 6561 6e20 7479 7065 2e20  f boolean type. 
-0000d780: 4865 7265 2c20 7573 696e 6720 4e6f 7445  Here, using NotE
-0000d790: 7175 616c 2069 6e73 7465 6164 206f 6620  qual instead of 
-0000d7a0: 4361 7374 2c20 666f 7220 7468 6174 2027  Cast, for that '
-0000d7b0: 4361 7374 2074 6f20 626f 6f6c 2720 6861  Cast to bool' ha
-0000d7c0: 730a 2020 2020 2020 2020 2320 6261 6420  s.        # bad 
-0000d7d0: 7065 7266 6f72 6d61 6e63 650a 2020 2020  performance.    
-0000d7e0: 2020 2020 6469 7370 6174 6368 5f74 656e      dispatch_ten
-0000d7f0: 736f 7220 3d20 7365 6c66 2e6e 6f74 5f65  sor = self.not_e
-0000d800: 7175 616c 2861 6363 756d 5f63 6f6d 6269  qual(accum_combi
-0000d810: 6e65 5f74 656e 736f 722c 2030 2e30 290a  ne_tensor, 0.0).
-0000d820: 2020 2020 2020 2020 7265 7475 726e 2064          return d
-0000d830: 6973 7061 7463 685f 7465 6e73 6f72 2c20  ispatch_tensor, 
-0000d840: 6163 6375 6d5f 636f 6d62 696e 655f 7465  accum_combine_te
-0000d850: 6e73 6f72 2c20 6c6f 7373 0a0a 2020 2020  nsor, loss..    
-0000d860: 6465 6620 5f61 7578 696c 6961 7279 5f6c  def _auxiliary_l
-0000d870: 6f73 7328 7365 6c66 2c20 6578 7065 7274  oss(self, expert
-0000d880: 5f6d 6173 6b2c 2072 6f75 7465 725f 7072  _mask, router_pr
-0000d890: 6f62 293a 0a20 2020 2020 2020 2022 2222  ob):.        """
-0000d8a0: 0a20 2020 2020 2020 2043 6f6d 7075 7469  .        Computi
-0000d8b0: 6e67 2074 6865 206c 6f61 6420 6261 6c61  ng the load bala
-0000d8c0: 6e63 6520 6c6f 7373 2e0a 2020 2020 2020  nce loss..      
-0000d8d0: 2020 2222 220a 2020 2020 2020 2020 2320    """.        # 
-0000d8e0: 6465 6e73 6974 795f 3127 7320 7368 6170  density_1's shap
-0000d8f0: 653a 2028 6470 5f67 726f 7570 2c20 7365  e: (dp_group, se
-0000d900: 6c66 2e65 7870 6572 745f 6469 6d29 0a20  lf.expert_dim). 
-0000d910: 2020 2020 2020 2064 656e 7369 7479 5f31         density_1
-0000d920: 203d 2073 656c 662e 7265 6475 6365 5f6d   = self.reduce_m
-0000d930: 6561 6e28 6578 7065 7274 5f6d 6173 6b2c  ean(expert_mask,
-0000d940: 2031 290a 2020 2020 2020 2020 2320 6465   1).        # de
-0000d950: 6e73 6974 795f 315f 7072 6f78 7927 7320  nsity_1_proxy's 
-0000d960: 7368 6170 653a 2028 6470 5f67 726f 7570  shape: (dp_group
-0000d970: 2c20 7365 6c66 2e65 7870 6572 745f 6469  , self.expert_di
-0000d980: 6d29 0a20 2020 2020 2020 2064 656e 7369  m).        densi
-0000d990: 7479 5f31 5f70 726f 7879 203d 2073 656c  ty_1_proxy = sel
-0000d9a0: 662e 7265 6475 6365 5f6d 6561 6e32 2872  f.reduce_mean2(r
-0000d9b0: 6f75 7465 725f 7072 6f62 2c20 3129 0a20  outer_prob, 1). 
-0000d9c0: 2020 2020 2020 206c 6f73 7320 3d20 7365         loss = se
-0000d9d0: 6c66 2e6d 756c 2864 656e 7369 7479 5f31  lf.mul(density_1
-0000d9e0: 2c20 6465 6e73 6974 795f 315f 7072 6f78  , density_1_prox
-0000d9f0: 7929 0a20 2020 2020 2020 206c 6f73 7320  y).        loss 
-0000da00: 3d20 7365 6c66 2e72 6564 7563 655f 6d65  = self.reduce_me
-0000da10: 616e 3328 6c6f 7373 290a 2020 2020 2020  an3(loss).      
-0000da20: 2020 6c6f 7373 203d 2073 656c 662e 6d75    loss = self.mu
-0000da30: 6c33 2873 656c 662e 6d75 6c32 286c 6f73  l3(self.mul2(los
-0000da40: 732c 2073 656c 662e 6578 7065 7274 5f64  s, self.expert_d
-0000da50: 696d 292c 2073 656c 662e 6578 7065 7274  im), self.expert
-0000da60: 5f64 696d 290a 2020 2020 2020 2020 7265  _dim).        re
-0000da70: 7475 726e 206c 6f73 730a 0a20 2020 2064  turn loss..    d
-0000da80: 6566 205f 6d61 736b 6f75 745f 6f76 6572  ef _maskout_over
-0000da90: 666c 6f77 6564 5f74 6f6b 656e 7328 7365  flowed_tokens(se
-0000daa0: 6c66 2c20 6578 7065 7274 5f6d 6173 6b2c  lf, expert_mask,
-0000dab0: 2065 7870 6572 745f 6361 7061 6369 7479   expert_capacity
-0000dac0: 2c20 6578 7065 7274 5f67 6174 652c 206c  , expert_gate, l
-0000dad0: 6173 745f 6e75 6d2c 2065 7870 6572 745f  ast_num, expert_
-0000dae0: 6368 6f73 656e 5f69 6e64 6578 293a 0a20  chosen_index):. 
-0000daf0: 2020 2020 2020 2022 2222 0a20 2020 2020         """.     
-0000db00: 2020 204b 6565 7069 6e67 206f 6e6c 7920     Keeping only 
-0000db10: 7468 6520 746f 6b65 6e73 2074 6861 7420  the tokens that 
-0000db20: 6669 7420 7769 7468 696e 2065 7870 6572  fit within exper
-0000db30: 745f 6361 7061 6369 7479 2e0a 2020 2020  t_capacity..    
-0000db40: 2020 2020 2222 220a 2020 2020 2020 2020      """.        
-0000db50: 6375 6d73 756d 203d 2073 656c 662e 6375  cumsum = self.cu
-0000db60: 6d73 756d 2865 7870 6572 745f 6d61 736b  msum(expert_mask
-0000db70: 2c20 3129 0a20 2020 2020 2020 2069 6620  , 1).        if 
-0000db80: 6578 7065 7274 5f63 686f 7365 6e5f 696e  expert_chosen_in
-0000db90: 6465 7820 3e20 303a 0a20 2020 2020 2020  dex > 0:.       
-0000dba0: 2020 2020 2063 756d 7375 6d20 3d20 7365       cumsum = se
-0000dbb0: 6c66 2e61 6464 2863 756d 7375 6d2c 206c  lf.add(cumsum, l
-0000dbc0: 6173 745f 6e75 6d29 0a20 2020 2020 2020  ast_num).       
-0000dbd0: 2069 6620 7365 6c66 2e73 6176 655f 746f   if self.save_to
-0000dbe0: 6b65 6e5f 6469 7374 7269 6275 7469 6f6e  ken_distribution
-0000dbf0: 3a0a 2020 2020 2020 2020 2020 2020 7265  :.            re
-0000dc00: 636f 7264 5f6e 616d 6520 3d20 276c 6179  cord_name = 'lay
-0000dc10: 6572 2d27 202b 2073 7472 2873 656c 662e  er-' + str(self.
-0000dc20: 6375 725f 6c61 7965 7229 0a20 2020 2020  cur_layer).     
-0000dc30: 2020 2020 2020 2073 656c 662e 7465 6e73         self.tens
-0000dc40: 6f72 5f73 756d 6d61 7279 2872 6563 6f72  or_summary(recor
-0000dc50: 645f 6e61 6d65 2c20 6375 6d73 756d 5b30  d_name, cumsum[0
-0000dc60: 5d5b 2d31 5d29 0a20 2020 2020 2020 2069  ][-1]).        i
-0000dc70: 6620 7365 6c66 2e65 6e61 626c 655f 636f  f self.enable_co
-0000dc80: 6c64 5f68 6f74 5f65 7870 6572 743a 0a20  ld_hot_expert:. 
-0000dc90: 2020 2020 2020 2020 2020 2063 756d 7375             cumsu
-0000dca0: 6d5f 696e 745f 7661 6c75 6520 3d20 7365  m_int_value = se
-0000dcb0: 6c66 2e63 6173 7428 6375 6d73 756d 5b30  lf.cast(cumsum[0
-0000dcc0: 5d5b 2d31 5d2c 206d 7374 7970 652e 696e  ][-1], mstype.in
-0000dcd0: 7433 3229 0a20 2020 2020 2020 2020 2020  t32).           
-0000dce0: 2073 656c 662e 6173 7369 676e 2873 656c   self.assign(sel
-0000dcf0: 662e 6375 6d73 756d 5f76 616c 7565 2c20  f.cumsum_value, 
-0000dd00: 6375 6d73 756d 5f69 6e74 5f76 616c 7565  cumsum_int_value
-0000dd10: 290a 2020 2020 2020 2020 2320 706f 7369  ).        # posi
-0000dd20: 7469 6f6e 5f69 6e5f 6578 7065 7274 2773  tion_in_expert's
-0000dd30: 2073 6861 7065 3a20 2864 705f 6772 6f75   shape: (dp_grou
-0000dd40: 702c 2074 6f6b 656e 735f 7065 725f 6772  p, tokens_per_gr
-0000dd50: 6f75 702c 2073 656c 662e 6578 7065 7274  oup, self.expert
-0000dd60: 5f64 696d 290a 2020 2020 2020 2020 706f  _dim).        po
-0000dd70: 7369 7469 6f6e 5f69 6e5f 6578 7065 7274  sition_in_expert
-0000dd80: 203d 2073 656c 662e 6d75 6c34 2863 756d   = self.mul4(cum
-0000dd90: 7375 6d2c 2065 7870 6572 745f 6d61 736b  sum, expert_mask
-0000dda0: 290a 2020 2020 2020 2020 6c65 7373 5f72  ).        less_r
-0000ddb0: 6573 756c 7420 3d20 7365 6c66 2e6c 6573  esult = self.les
-0000ddc0: 7328 706f 7369 7469 6f6e 5f69 6e5f 6578  s(position_in_ex
-0000ddd0: 7065 7274 2c20 6578 7065 7274 5f63 6170  pert, expert_cap
-0000dde0: 6163 6974 7929 0a20 2020 2020 2020 2023  acity).        #
-0000ddf0: 2065 7870 6572 745f 6d61 736b 2773 2073   expert_mask's s
-0000de00: 6861 7065 3a20 2864 705f 6772 6f75 702c  hape: (dp_group,
-0000de10: 2074 6f6b 656e 735f 7065 725f 6772 6f75   tokens_per_grou
-0000de20: 702c 2073 656c 662e 6578 7065 7274 5f64  p, self.expert_d
-0000de30: 696d 290a 2020 2020 2020 2020 6578 7065  im).        expe
-0000de40: 7274 5f6d 6173 6b20 3d20 7365 6c66 2e6d  rt_mask = self.m
-0000de50: 756c 3528 6c65 7373 5f72 6573 756c 742c  ul5(less_result,
-0000de60: 2065 7870 6572 745f 6d61 736b 290a 2020   expert_mask).  
-0000de70: 2020 2020 2020 2320 6578 7065 7274 5f6d        # expert_m
-0000de80: 6173 6b5f 666c 6174 2773 2073 6861 7065  ask_flat's shape
-0000de90: 3a20 2864 705f 6772 6f75 702c 2074 6f6b  : (dp_group, tok
-0000dea0: 656e 735f 7065 725f 6772 6f75 7029 0a20  ens_per_group). 
-0000deb0: 2020 2020 2020 2065 7870 6572 745f 6d61         expert_ma
-0000dec0: 736b 5f66 6c61 7420 3d20 7365 6c66 2e72  sk_flat = self.r
-0000ded0: 6564 7563 655f 7375 6d28 6578 7065 7274  educe_sum(expert
-0000dee0: 5f6d 6173 6b2c 202d 3129 0a0a 2020 2020  _mask, -1)..    
-0000def0: 2020 2020 2320 4d61 736b 206f 7574 2074      # Mask out t
-0000df00: 6865 2065 7870 6572 7473 2074 6861 7420  he experts that 
-0000df10: 6861 7665 206f 7665 7266 6c6f 7765 6420  have overflowed 
-0000df20: 7468 6520 6578 7065 7274 5f63 6170 6163  the expert_capac
-0000df30: 6974 792e 0a20 2020 2020 2020 2023 2065  ity..        # e
-0000df40: 7870 6572 745f 6761 7465 2773 2073 6861  xpert_gate's sha
-0000df50: 7065 3a20 2864 705f 6772 6f75 702c 2074  pe: (dp_group, t
-0000df60: 6f6b 656e 735f 7065 725f 6772 6f75 7029  okens_per_group)
-0000df70: 0a20 2020 2020 2020 2065 7870 6572 745f  .        expert_
-0000df80: 6761 7465 203d 2073 656c 662e 6d75 6c36  gate = self.mul6
-0000df90: 2865 7870 6572 745f 6761 7465 2c20 6578  (expert_gate, ex
-0000dfa0: 7065 7274 5f6d 6173 6b5f 666c 6174 290a  pert_mask_flat).
-0000dfb0: 2020 2020 2020 2020 6f75 7470 7574 203d          output =
-0000dfc0: 2028 6578 7065 7274 5f6d 6173 6b2c 2065   (expert_mask, e
-0000dfd0: 7870 6572 745f 6761 7465 2c20 6578 7065  xpert_gate, expe
-0000dfe0: 7274 5f6d 6173 6b5f 666c 6174 2c20 706f  rt_mask_flat, po
-0000dff0: 7369 7469 6f6e 5f69 6e5f 6578 7065 7274  sition_in_expert
-0000e000: 290a 2020 2020 2020 2020 7265 7475 726e  ).        return
-0000e010: 206f 7574 7075 740a 0a0a 636c 6173 7320   output...class 
-0000e020: 546f 706b 526f 7574 6572 5632 2843 656c  TopkRouterV2(Cel
-0000e030: 6c29 3a0a 2020 2020 7222 2222 0a20 2020  l):.    r""".   
-0000e040: 2020 2020 2041 2072 6f75 7465 7220 696d       A router im
-0000e050: 706c 656d 656e 7461 7469 6f6e 2077 6869  plementation whi
-0000e060: 6368 206d 6170 7320 6561 6368 2074 6f6b  ch maps each tok
-0000e070: 656e 7320 746f 2074 6865 2074 6f70 6b20  ens to the topk 
-0000e080: 6578 7065 7274 2e0a 0a20 2020 2020 2020  expert...       
-0000e090: 2041 7267 733a 0a20 2020 2020 2020 2020   Args:.         
-0000e0a0: 2020 2064 5f6d 6f64 656c 2028 696e 7429     d_model (int)
-0000e0b0: 3a20 5468 6520 6869 6464 656e 2073 697a  : The hidden siz
-0000e0c0: 6520 6f66 2065 6163 6820 746f 6b65 6e2e  e of each token.
-0000e0d0: 0a20 2020 2020 2020 2020 2020 206d 6f65  .            moe
-0000e0e0: 5f63 6f6e 6669 6728 4d6f 4543 6f6e 6669  _config(MoEConfi
-0000e0f0: 6729 3a20 5468 6520 636f 6e66 6967 7572  g): The configur
-0000e100: 6174 696f 6e20 6f66 204d 6f45 2028 4d69  ation of MoE (Mi
-0000e110: 7874 7572 6520 6f66 2045 7870 6572 7429  xture of Expert)
-0000e120: 2e0a 2020 2020 2020 2020 2020 2020 7472  ..            tr
-0000e130: 6169 6e69 6e67 2028 626f 6f6c 293a 2054  aining (bool): T
-0000e140: 6865 2076 616c 7565 2069 6e64 6963 6174  he value indicat
-0000e150: 696e 6720 7768 6574 6865 7220 6973 2069  ing whether is i
-0000e160: 6e20 7472 6169 6e69 6e67 2070 6861 7365  n training phase
-0000e170: 2e0a 2020 2020 2020 2020 2020 2020 636f  ..            co
-0000e180: 6e66 6967 3a20 5468 6520 7061 7261 6c6c  nfig: The parall
-0000e190: 656c 2d72 656c 6174 6564 2063 6f6e 6669  el-related confi
-0000e1a0: 6775 7261 7469 6f6e 2e0a 2020 2020 2020  guration..      
-0000e1b0: 2020 496e 7075 7473 3a0a 2020 2020 2020    Inputs:.      
-0000e1c0: 2020 2020 2020 2d20 2a2a 726f 7574 6572        - **router
-0000e1d0: 5f6c 6f67 6974 732a 2a20 2854 656e 736f  _logits** (Tenso
-0000e1e0: 7229 202d 2054 656e 736f 7220 6f66 2073  r) - Tensor of s
-0000e1f0: 6861 7065 203a 6d61 7468 3a60 2864 6174  hape :math:`(dat
-0000e200: 615c 5f70 6172 616c 6c65 6c2c 2074 6f6b  a\_parallel, tok
-0000e210: 656e 735c 5f70 6572 5c5f 6772 6f75 702c  ens\_per\_group,
-0000e220: 0a20 2020 2020 2020 2020 2020 2065 7870  .            exp
-0000e230: 6572 745c 5f64 696d 2960 2e28 6470 2c20  ert\_dim)`.(dp, 
-0000e240: 4e2c 2065 7870 6572 745f 6469 6d29 0a0a  N, expert_dim)..
-0000e250: 2020 2020 2020 2020 4f75 7470 7574 733a          Outputs:
-0000e260: 0a20 2020 2020 2020 2020 2020 202d 202a  .            - *
-0000e270: 2a64 6973 7061 7463 685f 696e 6465 782a  *dispatch_index*
-0000e280: 2a20 2854 656e 736f 7229 202d 2054 656e  * (Tensor) - Ten
-0000e290: 736f 7220 6f66 2073 6861 7065 203a 6d61  sor of shape :ma
-0000e2a0: 7468 3a60 2864 6174 615c 5f70 6172 616c  th:`(data\_paral
-0000e2b0: 6c65 6c2c 2065 7870 6572 745c 5f64 696d  lel, expert\_dim
-0000e2c0: 2c20 6578 7065 7274 5c5f 6361 7061 6369  , expert\_capaci
-0000e2d0: 7479 2960 2c0a 2020 2020 2020 2020 2020  ty)`,.          
-0000e2e0: 2020 2d20 2a2a 636f 6d62 696e 655f 696e    - **combine_in
-0000e2f0: 6465 782a 2a20 2854 656e 736f 7229 202d  dex** (Tensor) -
-0000e300: 2054 656e 736f 7220 6f66 2073 6861 7065   Tensor of shape
-0000e310: 203a 6d61 7468 3a60 2864 6174 615c 5f70   :math:`(data\_p
-0000e320: 6172 616c 6c65 6c2c 2074 6f6b 656e 735c  arallel, tokens\
-0000e330: 5f70 6572 5c5f 6772 6f75 702c 206b 2960  _per\_group, k)`
-0000e340: 2c0a 2020 2020 2020 2020 2020 2020 2d20  ,.            - 
-0000e350: 2a2a 726f 7574 6572 5f63 6f65 6666 2a2a  **router_coeff**
-0000e360: 2028 5465 6e73 6f72 2920 2d20 5465 6e73   (Tensor) - Tens
-0000e370: 6f72 206f 6620 7368 6170 6520 3a6d 6174  or of shape :mat
-0000e380: 683a 6028 6461 7461 5c5f 7061 7261 6c6c  h:`(data\_parall
-0000e390: 656c 2c20 746f 6b65 6e73 5c5f 7065 725c  el, tokens\_per\
-0000e3a0: 5f67 726f 7570 2c20 6b29 602e 0a20 2020  _group, k)`..   
-0000e3b0: 2022 2222 0a0a 2020 2020 6465 6620 5f5f   """..    def __
-0000e3c0: 696e 6974 5f5f 2873 656c 662c 0a20 2020  init__(self,.   
-0000e3d0: 2020 2020 2020 2020 2020 2020 2020 645f                d_
-0000e3e0: 6d6f 6465 6c2c 0a20 2020 2020 2020 2020  model,.         
-0000e3f0: 2020 2020 2020 2020 6d6f 655f 636f 6e66          moe_conf
-0000e400: 6967 2c0a 2020 2020 2020 2020 2020 2020  ig,.            
-0000e410: 2020 2020 2074 7261 696e 696e 673d 5472       training=Tr
-0000e420: 7565 2c0a 2020 2020 2020 2020 2020 2020  ue,.            
-0000e430: 2020 2020 2070 6172 616c 6c65 6c5f 636f       parallel_co
-0000e440: 6e66 6967 3d4e 6f6e 6529 3a0a 2020 2020  nfig=None):.    
-0000e450: 2020 2020 7375 7065 7228 546f 706b 526f      super(TopkRo
-0000e460: 7574 6572 5632 2c20 7365 6c66 292e 5f5f  uterV2, self).__
-0000e470: 696e 6974 5f5f 2829 0a0a 2020 2020 2020  init__()..      
-0000e480: 2020 6470 203d 2070 6172 616c 6c65 6c5f    dp = parallel_
-0000e490: 636f 6e66 6967 2e64 6174 615f 7061 7261  config.data_para
-0000e4a0: 6c6c 656c 0a20 2020 2020 2020 2073 656c  llel.        sel
-0000e4b0: 662e 6d70 203d 2070 6172 616c 6c65 6c5f  f.mp = parallel_
-0000e4c0: 636f 6e66 6967 2e6d 6f64 656c 5f70 6172  config.model_par
-0000e4d0: 616c 6c65 6c0a 2020 2020 2020 2020 7365  allel.        se
-0000e4e0: 6c66 2e64 5f6d 6f64 656c 203d 2064 5f6d  lf.d_model = d_m
-0000e4f0: 6f64 656c 0a20 2020 2020 2020 2073 656c  odel.        sel
-0000e500: 662e 6d6f 655f 636f 6e66 6967 203d 206d  f.moe_config = m
-0000e510: 6f65 5f63 6f6e 6669 670a 2020 2020 2020  oe_config.      
-0000e520: 2020 7365 6c66 2e65 7870 6572 745f 6469    self.expert_di
-0000e530: 6d20 3d20 6d6f 655f 636f 6e66 6967 2e65  m = moe_config.e
-0000e540: 7870 6572 745f 6e75 6d0a 2020 2020 2020  xpert_num.      
-0000e550: 2020 7365 6c66 2e63 6170 6163 6974 795f    self.capacity_
-0000e560: 6661 6374 6f72 203d 206d 6f65 5f63 6f6e  factor = moe_con
-0000e570: 6669 672e 6361 7061 6369 7479 5f66 6163  fig.capacity_fac
-0000e580: 746f 720a 2020 2020 2020 2020 7365 6c66  tor.        self
-0000e590: 2e73 6176 655f 746f 6b65 6e5f 6469 7374  .save_token_dist
-0000e5a0: 7269 6275 7469 6f6e 203d 206d 6f65 5f63  ribution = moe_c
-0000e5b0: 6f6e 6669 672e 7361 7665 5f74 6f6b 656e  onfig.save_token
-0000e5c0: 5f64 6973 7472 6962 7574 696f 6e0a 2020  _distribution.  
-0000e5d0: 2020 2020 2020 7365 6c66 2e74 7261 696e        self.train
-0000e5e0: 696e 6720 3d20 7472 6169 6e69 6e67 0a20  ing = training. 
-0000e5f0: 2020 2020 2020 2073 656c 662e 6470 5f67         self.dp_g
-0000e600: 726f 7570 203d 2064 700a 2020 2020 2020  roup = dp.      
-0000e610: 2020 7365 6c66 2e6e 756d 5f65 7870 6572    self.num_exper
-0000e620: 7473 5f63 686f 7365 6e20 3d20 6d6f 655f  ts_chosen = moe_
-0000e630: 636f 6e66 6967 2e6e 756d 5f65 7870 6572  config.num_exper
-0000e640: 7473 5f63 686f 7365 6e0a 2020 2020 2020  ts_chosen.      
-0000e650: 2020 7365 6c66 2e6f 6e5f 7661 6c75 6520    self.on_value 
-0000e660: 3d20 5465 6e73 6f72 2831 2e30 2c20 6d73  = Tensor(1.0, ms
-0000e670: 7479 7065 2e66 6c6f 6174 3332 290a 2020  type.float32).  
-0000e680: 2020 2020 2020 7365 6c66 2e6f 6666 5f76        self.off_v
-0000e690: 616c 7565 203d 2054 656e 736f 7228 302e  alue = Tensor(0.
-0000e6a0: 302c 206d 7374 7970 652e 666c 6f61 7433  0, mstype.float3
-0000e6b0: 3229 0a20 2020 2020 2020 2073 656c 662e  2).        self.
-0000e6c0: 7261 6e67 6520 3d20 5465 6e73 6f72 286e  range = Tensor(n
-0000e6d0: 702e 7469 6c65 286e 702e 6172 616e 6765  p.tile(np.arange
-0000e6e0: 2831 3331 3037 3229 2b31 2c20 2873 656c  (131072)+1, (sel
-0000e6f0: 662e 6e75 6d5f 6578 7065 7274 735f 6368  f.num_experts_ch
-0000e700: 6f73 656e 2c20 3129 292c 206d 7374 7970  osen, 1)), mstyp
-0000e710: 652e 666c 6f61 7433 3229 0a0a 2020 2020  e.float32)..    
-0000e720: 2020 2020 7365 6c66 2e63 6173 7420 3d20      self.cast = 
-0000e730: 502e 4361 7374 2829 0a20 2020 2020 2020  P.Cast().       
-0000e740: 2073 656c 662e 7265 7368 6170 6520 3d20   self.reshape = 
-0000e750: 502e 5265 7368 6170 6528 290a 2020 2020  P.Reshape().    
-0000e760: 2020 2020 7365 6c66 2e73 6861 7065 203d      self.shape =
-0000e770: 2050 2e53 6861 7065 2829 0a20 2020 2020   P.Shape().     
-0000e780: 2020 2073 656c 662e 736f 6674 6d61 7820     self.softmax 
-0000e790: 3d20 502e 536f 6674 6d61 7828 6178 6973  = P.Softmax(axis
-0000e7a0: 3d2d 3129 2e73 6861 7264 2828 2864 702c  =-1).shard(((dp,
-0000e7b0: 2031 2c20 312c 292c 2929 0a20 2020 2020   1, 1,),)).     
-0000e7c0: 2020 2073 656c 662e 746f 706b 203d 2050     self.topk = P
-0000e7d0: 2e54 6f70 4b28 292e 7368 6172 6428 2828  .TopK().shard(((
-0000e7e0: 6470 2c20 312c 2031 292c 2929 0a20 2020  dp, 1, 1),)).   
-0000e7f0: 2020 2020 2073 656c 662e 6172 676d 6178       self.argmax
-0000e800: 203d 2050 2e41 7267 4d61 7857 6974 6856   = P.ArgMaxWithV
-0000e810: 616c 7565 2861 7869 733d 2d31 2c20 6b65  alue(axis=-1, ke
-0000e820: 6570 5f64 696d 733d 4661 6c73 6529 2e73  ep_dims=False).s
-0000e830: 6861 7264 2828 2864 702c 2031 2c20 3129  hard(((dp, 1, 1)
-0000e840: 2c29 290a 2020 2020 2020 2020 7365 6c66  ,)).        self
-0000e850: 2e6f 6e65 686f 745f 3264 203d 2050 2e4f  .onehot_2d = P.O
-0000e860: 6e65 486f 7428 292e 7368 6172 6428 2828  neHot().shard(((
-0000e870: 6470 2c20 312c 2031 292c 2028 292c 2028  dp, 1, 1), (), (
-0000e880: 2929 290a 2020 2020 2020 2020 7365 6c66  ))).        self
-0000e890: 2e6f 6e65 686f 745f 3364 203d 2050 2e4f  .onehot_3d = P.O
-0000e8a0: 6e65 486f 7428 292e 7368 6172 6428 2828  neHot().shard(((
-0000e8b0: 6470 2c20 312c 2031 2c20 3129 2c20 2829  dp, 1, 1, 1), ()
-0000e8c0: 2c20 2829 2929 0a20 2020 2020 2020 2073  , ())).        s
-0000e8d0: 656c 662e 6375 6d73 756d 203d 2050 2e43  elf.cumsum = P.C
-0000e8e0: 756d 5375 6d28 6578 636c 7573 6976 653d  umSum(exclusive=
-0000e8f0: 4661 6c73 6529 2e73 6861 7264 2828 2864  False).shard(((d
-0000e900: 702c 2031 2c20 3129 2c29 290a 2020 2020  p, 1, 1),)).    
-0000e910: 2020 2020 7365 6c66 2e6d 756c 5f32 645f      self.mul_2d_
-0000e920: 3164 203d 2050 2e4d 756c 2829 2e73 6861  1d = P.Mul().sha
-0000e930: 7264 2828 2864 702c 2031 292c 2028 2929  rd(((dp, 1), ())
-0000e940: 290a 2020 2020 2020 2020 7365 6c66 2e6d  ).        self.m
-0000e950: 756c 5f32 6420 3d20 502e 4d75 6c28 292e  ul_2d = P.Mul().
-0000e960: 7368 6172 6428 2828 6470 2c20 3129 2c20  shard(((dp, 1), 
-0000e970: 2864 702c 2031 2929 290a 2020 2020 2020  (dp, 1))).      
-0000e980: 2020 7365 6c66 2e6d 756c 5f33 6420 3d20    self.mul_3d = 
-0000e990: 502e 4d75 6c28 292e 7368 6172 6428 2828  P.Mul().shard(((
-0000e9a0: 6470 2c20 312c 2031 292c 2028 6470 2c20  dp, 1, 1), (dp, 
-0000e9b0: 312c 2031 2929 290a 2020 2020 2020 2020  1, 1))).        
-0000e9c0: 7365 6c66 2e6d 756c 5f34 6420 3d20 502e  self.mul_4d = P.
-0000e9d0: 4d75 6c28 292e 7368 6172 6428 2828 6470  Mul().shard(((dp
-0000e9e0: 2c20 312c 2031 2c20 3129 2c20 2864 702c  , 1, 1, 1), (dp,
-0000e9f0: 2031 2c20 312c 2031 2929 290a 2020 2020   1, 1, 1))).    
-0000ea00: 2020 2020 7365 6c66 2e61 6464 5f32 6420      self.add_2d 
-0000ea10: 3d20 502e 4164 6428 292e 7368 6172 6428  = P.Add().shard(
-0000ea20: 2828 6470 2c20 3129 2c20 2864 702c 2031  ((dp, 1), (dp, 1
-0000ea30: 2929 290a 2020 2020 2020 2020 7365 6c66  ))).        self
-0000ea40: 2e61 6464 5f33 6420 3d20 502e 4164 6428  .add_3d = P.Add(
-0000ea50: 292e 7368 6172 6428 2828 6470 2c20 312c  ).shard(((dp, 1,
-0000ea60: 2031 292c 2028 6470 2c20 312c 2031 2929   1), (dp, 1, 1))
-0000ea70: 290a 2020 2020 2020 2020 7365 6c66 2e6c  ).        self.l
-0000ea80: 6573 7320 3d20 502e 4c65 7373 2829 2e73  ess = P.Less().s
-0000ea90: 6861 7264 2828 2864 702c 2031 2c20 3129  hard(((dp, 1, 1)
-0000eaa0: 2c20 2829 2929 0a20 2020 2020 2020 2073  , ())).        s
-0000eab0: 656c 662e 6774 203d 2050 2e47 7265 6174  elf.gt = P.Great
-0000eac0: 6572 2829 2e73 6861 7264 2828 2864 702c  er().shard(((dp,
-0000ead0: 2031 292c 2028 2929 290a 2020 2020 2020   1), ())).      
-0000eae0: 2020 7365 6c66 2e72 6564 7563 655f 7375    self.reduce_su
-0000eaf0: 6d20 3d20 502e 5265 6475 6365 5375 6d28  m = P.ReduceSum(
-0000eb00: 6b65 6570 5f64 696d 733d 4661 6c73 6529  keep_dims=False)
-0000eb10: 2e73 6861 7264 2828 2864 702c 2031 2c20  .shard(((dp, 1, 
-0000eb20: 3129 2c29 290a 2020 2020 2020 2020 7365  1),)).        se
-0000eb30: 6c66 2e74 7261 6e73 706f 7365 5f33 6420  lf.transpose_3d 
-0000eb40: 3d20 502e 5472 616e 7370 6f73 6528 292e  = P.Transpose().
-0000eb50: 7368 6172 6428 2828 6470 2c20 312c 2031  shard(((dp, 1, 1
-0000eb60: 292c 2929 0a20 2020 2020 2020 2073 656c  ),)).        sel
-0000eb70: 662e 7472 616e 7370 6f73 6520 3d20 502e  f.transpose = P.
-0000eb80: 5472 616e 7370 6f73 6528 292e 7368 6172  Transpose().shar
-0000eb90: 6428 2828 6470 2c20 312c 2031 2c20 3129  d(((dp, 1, 1, 1)
-0000eba0: 2c29 290a 2020 2020 2020 2020 7365 6c66  ,)).        self
-0000ebb0: 2e73 6c69 6365 203d 2050 2e53 7472 6964  .slice = P.Strid
-0000ebc0: 6564 536c 6963 6528 292e 7368 6172 6428  edSlice().shard(
-0000ebd0: 2828 6470 2c20 312c 2031 292c 2929 0a20  ((dp, 1, 1),)). 
-0000ebe0: 2020 2020 2020 2073 656c 662e 736c 6963         self.slic
-0000ebf0: 655f 7261 6e67 6520 3d20 502e 5374 7269  e_range = P.Stri
-0000ec00: 6465 6453 6c69 6365 2829 2e73 6861 7264  dedSlice().shard
-0000ec10: 2828 2831 2c20 3129 2c29 290a 2020 2020  (((1, 1),)).    
-0000ec20: 2020 2020 7365 6c66 2e62 6d6d 5f72 616e      self.bmm_ran
-0000ec30: 6765 203d 2050 2e42 6174 6368 4d61 744d  ge = P.BatchMatM
-0000ec40: 756c 2829 2e73 6861 7264 2828 2831 2c20  ul().shard(((1, 
-0000ec50: 3129 2c20 2864 702c 2031 2c20 312c 2031  1), (dp, 1, 1, 1
-0000ec60: 2929 290a 2020 2020 2020 2020 7365 6c66  ))).        self
-0000ec70: 2e61 6464 5f65 7073 203d 2050 2e41 6464  .add_eps = P.Add
-0000ec80: 2829 2e73 6861 7264 2828 2864 702c 2031  ().shard(((dp, 1
-0000ec90: 2c20 3129 2c20 2829 2929 0a20 2020 2020  , 1), ())).     
-0000eca0: 2020 2073 656c 662e 7265 6475 6365 5f73     self.reduce_s
-0000ecb0: 756d 5f6b 6565 7020 3d20 502e 5265 6475  um_keep = P.Redu
-0000ecc0: 6365 5375 6d28 6b65 6570 5f64 696d 733d  ceSum(keep_dims=
-0000ecd0: 5472 7565 292e 7368 6172 6428 2828 6470  True).shard(((dp
-0000ece0: 2c20 312c 2031 292c 2929 0a20 2020 2020  , 1, 1),)).     
-0000ecf0: 2020 2073 656c 662e 6469 765f 3364 203d     self.div_3d =
-0000ed00: 2050 2e52 6561 6c44 6976 2829 2e73 6861   P.RealDiv().sha
-0000ed10: 7264 2828 2864 702c 2031 2c20 3129 2c20  rd(((dp, 1, 1), 
-0000ed20: 2864 702c 2031 2c20 3129 2929 0a20 2020  (dp, 1, 1))).   
-0000ed30: 2020 2020 2073 656c 662e 636f 6e63 6174       self.concat
-0000ed40: 5f33 6420 3d20 502e 436f 6e63 6174 2831  _3d = P.Concat(1
-0000ed50: 292e 7368 6172 6428 2828 6470 2c20 312c  ).shard(((dp, 1,
-0000ed60: 2031 292c 2028 6470 2c20 312c 2031 2929   1), (dp, 1, 1))
-0000ed70: 290a 2020 2020 2020 2020 7365 6c66 2e7a  ).        self.z
-0000ed80: 6572 6f73 203d 2054 656e 736f 7228 6e70  eros = Tensor(np
-0000ed90: 2e7a 6572 6f73 2828 6470 2c20 7365 6c66  .zeros((dp, self
-0000eda0: 2e65 7870 6572 745f 6469 6d2c 2031 2c20  .expert_dim, 1, 
-0000edb0: 645f 6d6f 6465 6c29 292c 206d 7374 7970  d_model)), mstyp
-0000edc0: 652e 666c 6f61 7431 3629 0a20 2020 2020  e.float16).     
-0000edd0: 2020 2073 656c 662e 7a65 726f 735f 3364     self.zeros_3d
-0000ede0: 203d 2054 656e 736f 7228 6e70 2e7a 6572   = Tensor(np.zer
-0000edf0: 6f73 2828 6470 2c20 312c 2064 5f6d 6f64  os((dp, 1, d_mod
-0000ee00: 656c 2929 2c20 6d73 7479 7065 2e66 6c6f  el)), mstype.flo
-0000ee10: 6174 3136 290a 2020 2020 2020 2020 7365  at16).        se
-0000ee20: 6c66 2e64 6973 7061 7463 685f 6761 7468  lf.dispatch_gath
-0000ee30: 6572 203d 2050 2e47 6174 6865 7228 6261  er = P.Gather(ba
-0000ee40: 7463 685f 6469 6d73 3d31 292e 7368 6172  tch_dims=1).shar
-0000ee50: 6428 2828 6470 2c20 312c 2031 292c 2028  d(((dp, 1, 1), (
-0000ee60: 6470 2c20 312c 2031 292c 2929 0a20 2020  dp, 1, 1),)).   
-0000ee70: 2020 2020 2073 656c 662e 636f 6e63 6174       self.concat
-0000ee80: 203d 2050 2e43 6f6e 6361 7428 3229 2e73   = P.Concat(2).s
-0000ee90: 6861 7264 2828 2864 702c 2031 2c20 312c  hard(((dp, 1, 1,
-0000eea0: 2031 292c 2028 6470 2c20 312c 2031 2c20   1), (dp, 1, 1, 
-0000eeb0: 3129 2929 0a20 2020 2020 2020 2073 656c  1))).        sel
-0000eec0: 662e 636f 6d62 696e 655f 6761 7468 6572  f.combine_gather
-0000eed0: 203d 2050 2e47 6174 6865 7228 6261 7463   = P.Gather(batc
-0000eee0: 685f 6469 6d73 3d31 292e 7368 6172 6428  h_dims=1).shard(
-0000eef0: 2828 6470 2c20 312c 2031 292c 2028 6470  ((dp, 1, 1), (dp
-0000ef00: 2c20 312c 2031 292c 2929 0a20 2020 2020  , 1, 1),)).     
-0000ef10: 2020 2073 656c 662e 6d75 6c5f 726f 7574     self.mul_rout
-0000ef20: 6572 5f63 6f65 6666 203d 2050 2e4d 756c  er_coeff = P.Mul
-0000ef30: 2829 2e73 6861 7264 2828 2864 702c 2031  ().shard(((dp, 1
-0000ef40: 2c20 312c 2031 292c 2028 6470 2c20 312c  , 1, 1), (dp, 1,
-0000ef50: 2031 2c20 3129 2929 0a20 2020 2020 2020   1, 1))).       
-0000ef60: 2073 656c 662e 7375 6d5f 726f 7574 6572   self.sum_router
-0000ef70: 5f63 6f65 6666 203d 2050 2e52 6564 7563  _coeff = P.Reduc
-0000ef80: 6553 756d 286b 6565 705f 6469 6d73 3d46  eSum(keep_dims=F
-0000ef90: 616c 7365 292e 7368 6172 6428 2828 6470  alse).shard(((dp
-0000efa0: 2c20 312c 2031 2c20 3129 2c29 290a 0a20  , 1, 1, 1),)).. 
-0000efb0: 2020 2020 2020 2023 2073 6f72 7420 696e         # sort in
-0000efc0: 6465 7869 6e67 0a20 2020 2020 2020 2073  dexing.        s
-0000efd0: 656c 662e 7261 6e67 6532 203d 2054 656e  elf.range2 = Ten
-0000efe0: 736f 7228 6e70 2e74 696c 6528 6e70 2e61  sor(np.tile(np.a
-0000eff0: 7261 6e67 6528 3133 3130 3732 292c 2028  range(131072), (
-0000f000: 7365 6c66 2e65 7870 6572 745f 6469 6d2c  self.expert_dim,
-0000f010: 2031 2929 2c20 6d73 7479 7065 2e66 6c6f   1)), mstype.flo
-0000f020: 6174 3332 290a 2020 2020 2020 2020 7365  at32).        se
-0000f030: 6c66 2e61 6464 5f6f 6e65 203d 2050 2e41  lf.add_one = P.A
-0000f040: 6464 2829 2e73 6861 7264 2828 2864 702c  dd().shard(((dp,
-0000f050: 2031 2c20 3129 2c20 2829 2929 0a20 2020   1, 1), ())).   
-0000f060: 2020 2020 2073 656c 662e 6164 645f 7261       self.add_ra
-0000f070: 6e67 6520 3d20 502e 4164 6428 292e 7368  nge = P.Add().sh
-0000f080: 6172 6428 2828 312c 2031 2c20 3129 2c20  ard(((1, 1, 1), 
-0000f090: 2829 2929 0a20 2020 2020 2020 2073 656c  ())).        sel
-0000f0a0: 662e 7375 625f 7261 6e67 6520 3d20 502e  f.sub_range = P.
-0000f0b0: 5375 6228 292e 7368 6172 6428 2828 292c  Sub().shard(((),
-0000f0c0: 2028 6470 2c20 312c 2031 2929 290a 2020   (dp, 1, 1))).  
-0000f0d0: 2020 2020 2020 7365 6c66 2e6d 756c 5f72        self.mul_r
-0000f0e0: 616e 6765 203d 2050 2e4d 756c 2829 2e73  ange = P.Mul().s
-0000f0f0: 6861 7264 2828 2864 702c 2031 2c20 3129  hard(((dp, 1, 1)
-0000f100: 2c20 2831 2c20 312c 2031 2929 290a 2020  , (1, 1, 1))).  
-0000f110: 2020 2020 2020 7365 6c66 2e73 6f72 745f        self.sort_
-0000f120: 7261 6e67 6520 3d20 502e 536f 7274 2829  range = P.Sort()
-0000f130: 2e73 6861 7264 2828 2864 702c 2031 2c20  .shard(((dp, 1, 
-0000f140: 3129 2c29 290a 2020 2020 2020 2020 7365  1),)).        se
-0000f150: 6c66 2e6d 6f64 203d 2050 2e4d 6f64 2829  lf.mod = P.Mod()
-0000f160: 2e73 6861 7264 2828 2864 702c 2031 2c20  .shard(((dp, 1, 
-0000f170: 3129 2c20 2829 2929 0a20 2020 2020 2020  1), ())).       
-0000f180: 2073 656c 662e 7072 696e 7420 3d20 502e   self.print = P.
-0000f190: 5072 696e 7428 290a 0a20 2020 2064 6566  Print()..    def
-0000f1a0: 2064 6973 7061 7463 6828 7365 6c66 2c20   dispatch(self, 
-0000f1b0: 696e 7075 745f 7465 6e73 6f72 2c20 6469  input_tensor, di
-0000f1c0: 7370 6174 6368 5f69 6e64 6578 293a 0a20  spatch_index):. 
-0000f1d0: 2020 2020 2020 2072 2222 220a 2020 2020         r""".    
-0000f1e0: 2020 2020 2020 2020 496d 706c 656d 656e          Implemen
-0000f1f0: 7469 6e67 2064 6973 7061 7463 6820 6f70  ting dispatch op
-0000f200: 6572 6174 696f 6e2e 0a20 2020 2020 2020  eration..       
-0000f210: 2020 2020 2049 6e70 7574 733a 0a20 2020       Inputs:.   
-0000f220: 2020 2020 2020 2020 2020 2020 202d 202a               - *
-0000f230: 2a69 6e70 7574 5f74 656e 736f 722a 2a20  *input_tensor** 
-0000f240: 2854 656e 736f 7229 202d 2054 656e 736f  (Tensor) - Tenso
-0000f250: 7220 6f66 2073 6861 7065 203a 6d61 7468  r of shape :math
-0000f260: 3a60 2864 6174 615c 5f70 6172 616c 6c65  :`(data\_paralle
-0000f270: 6c2c 2074 6f6b 656e 735c 5f70 6572 5c5f  l, tokens\_per\_
-0000f280: 6772 6f75 702c 0a20 2020 2020 2020 2020  group,.         
-0000f290: 2020 2020 2020 2068 6964 6465 6e5c 5f73         hidden\_s
-0000f2a0: 697a 6529 602e 2864 702c 204e 2c20 6829  ize)`.(dp, N, h)
-0000f2b0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-0000f2c0: 2020 2d20 2a2a 6469 7370 6174 6368 5f69    - **dispatch_i
-0000f2d0: 6e64 6578 2a2a 2028 5465 6e73 6f72 2920  ndex** (Tensor) 
-0000f2e0: 2d20 5465 6e73 6f72 206f 6620 7368 6170  - Tensor of shap
-0000f2f0: 6520 3a6d 6174 683a 6028 6461 7461 5c5f  e :math:`(data\_
-0000f300: 7061 7261 6c6c 656c 2c20 6578 7065 7274  parallel, expert
-0000f310: 5c5f 6e75 6d2c 0a20 2020 2020 2020 2020  \_num,.         
-0000f320: 2020 2020 2020 2065 7870 6572 745c 5f63         expert\_c
-0000f330: 6170 6163 6974 7929 602e 2864 702c 2045  apacity)`.(dp, E
-0000f340: 2c20 6e29 2e0a 0a20 2020 2020 2020 2020  , n)...         
-0000f350: 2020 204f 7574 7075 7473 3a0a 2020 2020     Outputs:.    
-0000f360: 2020 2020 2020 2020 2020 2020 2d20 2a2a              - **
-0000f370: 6578 7065 7274 5f69 6e70 7574 2a2a 2028  expert_input** (
-0000f380: 5465 6e73 6f72 2920 2d20 5465 6e73 6f72  Tensor) - Tensor
-0000f390: 206f 6620 7368 6170 6520 3a6d 6174 683a   of shape :math:
-0000f3a0: 6028 6461 7461 5c5f 7061 7261 6c6c 656c  `(data\_parallel
-0000f3b0: 2c20 6578 7065 7274 5c5f 6e75 6d2c 0a20  , expert\_num,. 
-0000f3c0: 2020 2020 2020 2020 2020 2020 2020 2065                 e
-0000f3d0: 7870 6572 745c 5f63 6170 6163 6974 792c  xpert\_capacity,
-0000f3e0: 2068 6964 6465 6e5c 5f73 697a 6529 602e   hidden\_size)`.
-0000f3f0: 2864 702c 2045 2c20 6e2c 2068 292e 0a20  (dp, E, n, h).. 
-0000f400: 2020 2020 2020 2022 2222 0a20 2020 2020         """.     
-0000f410: 2020 2069 6e70 7574 5f74 656e 736f 725f     input_tensor_
-0000f420: 7061 6464 6564 203d 2073 656c 662e 636f  padded = self.co
-0000f430: 6e63 6174 5f33 6428 2873 656c 662e 6361  ncat_3d((self.ca
-0000f440: 7374 2873 656c 662e 7a65 726f 735f 3364  st(self.zeros_3d
-0000f450: 2c20 462e 6474 7970 6528 696e 7075 745f  , F.dtype(input_
-0000f460: 7465 6e73 6f72 2929 2c20 696e 7075 745f  tensor)), input_
-0000f470: 7465 6e73 6f72 2929 2023 2023 2028 6470  tensor)) # # (dp
-0000f480: 2c20 312b 4e2c 2068 2920 3c2d 2d20 2864  , 1+N, h) <-- (d
-0000f490: 702c 204e 2c20 6829 0a20 2020 2020 2020  p, N, h).       
-0000f4a0: 2065 7870 6572 745f 696e 7075 7420 3d20   expert_input = 
-0000f4b0: 7365 6c66 2e64 6973 7061 7463 685f 6761  self.dispatch_ga
-0000f4c0: 7468 6572 2869 6e70 7574 5f74 656e 736f  ther(input_tenso
-0000f4d0: 725f 7061 6464 6564 2c20 6469 7370 6174  r_padded, dispat
-0000f4e0: 6368 5f69 6e64 6578 2c20 3129 2023 2028  ch_index, 1) # (
-0000f4f0: 6470 2c20 452c 206e 2c20 6829 203c 2d2d  dp, E, n, h) <--
-0000f500: 2028 6470 2c20 4e2c 2068 292c 2028 6470   (dp, N, h), (dp
-0000f510: 2c20 452c 206e 290a 2020 2020 2020 2020  , E, n).        
-0000f520: 7265 7475 726e 2065 7870 6572 745f 696e  return expert_in
-0000f530: 7075 740a 0a20 2020 2064 6566 2063 6f6d  put..    def com
-0000f540: 6269 6e65 2873 656c 662c 2065 7870 6572  bine(self, exper
-0000f550: 745f 6f75 7470 7574 2c20 636f 6d62 696e  t_output, combin
-0000f560: 655f 696e 6465 782c 2072 6f75 7465 725f  e_index, router_
-0000f570: 636f 6566 6629 3a0a 2020 2020 2020 2020  coeff):.        
-0000f580: 7222 2222 0a20 2020 2020 2020 2020 2020  r""".           
-0000f590: 2049 6d70 6c65 6d65 6e74 696e 6720 636f   Implementing co
-0000f5a0: 6d62 696e 6520 6f70 6572 6174 696f 6e2e  mbine operation.
-0000f5b0: 0a20 2020 2020 2020 2020 2020 2049 6e70  .            Inp
-0000f5c0: 7574 733a 0a20 2020 2020 2020 2020 2020  uts:.           
-0000f5d0: 2020 2020 202d 202a 2a65 7870 6572 745f       - **expert_
-0000f5e0: 6f75 7470 7574 2a2a 2028 5465 6e73 6f72  output** (Tensor
-0000f5f0: 2920 2d20 5465 6e73 6f72 206f 6620 7368  ) - Tensor of sh
-0000f600: 6170 6520 3a6d 6174 683a 6028 6461 7461  ape :math:`(data
-0000f610: 5c5f 7061 7261 6c6c 656c 2c20 6578 7065  \_parallel, expe
-0000f620: 7274 5c5f 6e75 6d2c 0a20 2020 2020 2020  rt\_num,.       
-0000f630: 2020 2020 2020 2020 2065 7870 6572 745c           expert\
-0000f640: 5f63 6170 6163 6974 792c 2068 6964 6465  _capacity, hidde
-0000f650: 6e5c 5f73 697a 6529 602e 2864 702c 2045  n\_size)`.(dp, E
-0000f660: 2c20 6e2c 2068 292c 0a20 2020 2020 2020  , n, h),.       
-0000f670: 2020 2020 2020 2020 202d 202a 2a63 6f6d           - **com
-0000f680: 6269 6e65 5f69 6e64 6578 2a2a 2028 5465  bine_index** (Te
-0000f690: 6e73 6f72 2920 2d20 5465 6e73 6f72 206f  nsor) - Tensor o
-0000f6a0: 6620 7368 6170 6520 3a6d 6174 683a 6028  f shape :math:`(
-0000f6b0: 6461 7461 5c5f 7061 7261 6c6c 656c 2c20  data\_parallel, 
-0000f6c0: 746f 6b65 6e73 5c5f 7065 725c 5f67 726f  tokens\_per\_gro
-0000f6d0: 7570 2c0a 2020 2020 2020 2020 2020 2020  up,.            
-0000f6e0: 2020 2020 6e75 6d5c 5f65 7870 6572 7473      num\_experts
-0000f6f0: 5c5f 6368 6f73 656e 2960 2e28 6470 2c20  \_chosen)`.(dp, 
-0000f700: 4e2c 206b 292c 0a20 2020 2020 2020 2020  N, k),.         
-0000f710: 2020 2020 2020 202d 202a 2a72 6f75 7465         - **route
-0000f720: 725f 636f 6566 662a 2a20 2854 656e 736f  r_coeff** (Tenso
-0000f730: 7229 202d 2054 656e 736f 7220 6f66 2073  r) - Tensor of s
-0000f740: 6861 7065 203a 6d61 7468 3a60 2864 6174  hape :math:`(dat
-0000f750: 615c 5f70 6172 616c 6c65 6c2c 2074 6f6b  a\_parallel, tok
-0000f760: 656e 735c 5f70 6572 5c5f 6772 6f75 702c  ens\_per\_group,
-0000f770: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0000f780: 206e 756d 5c5f 6578 7065 7274 735c 5f63   num\_experts\_c
-0000f790: 686f 7365 6e29 602e 2864 702c 204e 2c20  hosen)`.(dp, N, 
-0000f7a0: 6b29 2e0a 0a20 2020 2020 2020 2020 2020  k)...           
-0000f7b0: 204f 7574 7075 7473 3a0a 2020 2020 2020   Outputs:.      
-0000f7c0: 2020 2020 2020 2020 2020 2d20 2a2a 6f75            - **ou
-0000f7d0: 7470 7574 5f74 656e 736f 722a 2a20 2854  tput_tensor** (T
-0000f7e0: 656e 736f 7229 202d 2054 656e 736f 7220  ensor) - Tensor 
-0000f7f0: 6f66 2073 6861 7065 203a 6d61 7468 3a60  of shape :math:`
-0000f800: 2864 6174 615c 5f70 6172 616c 6c65 6c2c  (data\_parallel,
-0000f810: 2074 6f6b 656e 735c 5f70 6572 5c5f 6772   tokens\_per\_gr
-0000f820: 6f75 702c 0a20 2020 2020 2020 2020 2020  oup,.           
-0000f830: 2020 2020 2068 6964 6465 6e5c 5f73 697a       hidden\_siz
-0000f840: 6529 602e 2864 702c 204e 2c20 6829 2e0a  e)`.(dp, N, h)..
-0000f850: 2020 2020 2020 2020 2222 220a 2020 2020          """.    
-0000f860: 2020 2020 6578 7065 7274 5f6f 7574 7075      expert_outpu
-0000f870: 7420 3d20 7365 6c66 2e63 6f6e 6361 7428  t = self.concat(
-0000f880: 2873 656c 662e 6361 7374 2873 656c 662e  (self.cast(self.
-0000f890: 7a65 726f 732c 2046 2e64 7479 7065 2865  zeros, F.dtype(e
-0000f8a0: 7870 6572 745f 6f75 7470 7574 2929 2c20  xpert_output)), 
-0000f8b0: 6578 7065 7274 5f6f 7574 7075 7429 2920  expert_output)) 
-0000f8c0: 2320 2864 702c 2045 2c20 312b 6e2c 2068  # (dp, E, 1+n, h
-0000f8d0: 2920 3c2d 2d20 2864 702c 2045 2c20 6e2c  ) <-- (dp, E, n,
-0000f8e0: 2068 290a 2020 2020 2020 2020 6578 7065   h).        expe
-0000f8f0: 7274 5f6f 7574 7075 7420 3d20 7365 6c66  rt_output = self
-0000f900: 2e72 6573 6861 7065 2865 7870 6572 745f  .reshape(expert_
-0000f910: 6f75 7470 7574 2c20 2865 7870 6572 745f  output, (expert_
-0000f920: 6f75 7470 7574 2e73 6861 7065 5b30 5d2c  output.shape[0],
-0000f930: 2065 7870 6572 745f 6f75 7470 7574 2e73   expert_output.s
-0000f940: 6861 7065 5b31 5d2a 6578 7065 7274 5f6f  hape[1]*expert_o
-0000f950: 7574 7075 742e 7368 6170 655b 325d 2c20  utput.shape[2], 
-0000f960: 6578 7065 7274 5f6f 7574 7075 742e 7368  expert_output.sh
-0000f970: 6170 655b 335d 2929 2023 2028 6470 2c20  ape[3])) # (dp, 
-0000f980: 452a 2831 2b6e 292c 2068 2920 3c2d 2d20  E*(1+n), h) <-- 
-0000f990: 2864 702c 2045 2c20 312b 6e2c 2068 290a  (dp, E, 1+n, h).
-0000f9a0: 2020 2020 2020 2020 6f75 7470 7574 5f74          output_t
-0000f9b0: 656e 736f 7220 3d20 7365 6c66 2e63 6f6d  ensor = self.com
-0000f9c0: 6269 6e65 5f67 6174 6865 7228 6578 7065  bine_gather(expe
-0000f9d0: 7274 5f6f 7574 7075 742c 2063 6f6d 6269  rt_output, combi
-0000f9e0: 6e65 5f69 6e64 6578 2c20 3129 2023 2028  ne_index, 1) # (
-0000f9f0: 6470 2c20 4e2c 206b 2c20 6829 203c 2d2d  dp, N, k, h) <--
-0000fa00: 2028 6470 2c20 452a 2831 2b6e 292c 2068   (dp, E*(1+n), h
-0000fa10: 292c 2028 6470 2c20 4e2c 206b 290a 2020  ), (dp, N, k).  
-0000fa20: 2020 2020 2020 726f 7574 6572 5f63 6f65        router_coe
-0000fa30: 6666 203d 2073 656c 662e 6361 7374 2872  ff = self.cast(r
-0000fa40: 6f75 7465 725f 636f 6566 662c 2046 2e64  outer_coeff, F.d
-0000fa50: 7479 7065 2865 7870 6572 745f 6f75 7470  type(expert_outp
-0000fa60: 7574 2929 0a20 2020 2020 2020 206f 7574  ut)).        out
-0000fa70: 7075 745f 7465 6e73 6f72 203d 2073 656c  put_tensor = sel
-0000fa80: 662e 6d75 6c5f 726f 7574 6572 5f63 6f65  f.mul_router_coe
-0000fa90: 6666 286f 7574 7075 745f 7465 6e73 6f72  ff(output_tensor
-0000faa0: 2c20 7365 6c66 2e72 6573 6861 7065 2872  , self.reshape(r
-0000fab0: 6f75 7465 725f 636f 6566 662c 2028 726f  outer_coeff, (ro
-0000fac0: 7574 6572 5f63 6f65 6666 2e73 6861 7065  uter_coeff.shape
-0000fad0: 5b30 5d2c 2072 6f75 7465 725f 636f 6566  [0], router_coef
-0000fae0: 662e 7368 6170 655b 315d 2c20 726f 7574  f.shape[1], rout
-0000faf0: 6572 5f63 6f65 6666 2e73 6861 7065 5b32  er_coeff.shape[2
-0000fb00: 5d2c 2031 2929 2920 2320 2864 702c 204e  ], 1))) # (dp, N
-0000fb10: 2c20 6b2c 2068 2920 3c2d 2d20 2864 702c  , k, h) <-- (dp,
-0000fb20: 204e 2c20 6b2c 2068 2920 2864 702c 204e   N, k, h) (dp, N
-0000fb30: 2c20 6b2c 2031 290a 2020 2020 2020 2020  , k, 1).        
-0000fb40: 6f75 7470 7574 5f74 656e 736f 7220 3d20  output_tensor = 
-0000fb50: 7365 6c66 2e73 756d 5f72 6f75 7465 725f  self.sum_router_
-0000fb60: 636f 6566 6628 6f75 7470 7574 5f74 656e  coeff(output_ten
-0000fb70: 736f 722c 2032 2920 2372 6564 7563 6520  sor, 2) #reduce 
-0000fb80: 7375 6d20 2320 2864 702c 204e 2c20 6829  sum # (dp, N, h)
-0000fb90: 203c 2d2d 2028 6470 2c20 4e2c 206b 2c20   <-- (dp, N, k, 
-0000fba0: 6829 0a20 2020 2020 2020 2072 6574 7572  h).        retur
-0000fbb0: 6e20 6f75 7470 7574 5f74 656e 736f 720a  n output_tensor.
-0000fbc0: 0a20 2020 2064 6566 2063 6f6e 7374 7275  .    def constru
-0000fbd0: 6374 2873 656c 662c 2072 6f75 7465 725f  ct(self, router_
-0000fbe0: 6c6f 6769 7473 293a 0a20 2020 2020 2020  logits):.       
-0000fbf0: 2072 6f75 7465 725f 7072 6f62 203d 2073   router_prob = s
-0000fc00: 656c 662e 736f 6674 6d61 7828 726f 7574  elf.softmax(rout
-0000fc10: 6572 5f6c 6f67 6974 7329 2023 2028 6470  er_logits) # (dp
-0000fc20: 2c20 4e2c 2065 7870 6572 745f 6469 6d29  , N, expert_dim)
-0000fc30: 6670 3332 203c 2d2d 2028 6470 2c20 4e2c  fp32 <-- (dp, N,
-0000fc40: 2065 7870 6572 745f 6469 6d29 6670 3332   expert_dim)fp32
-0000fc50: 0a20 2020 2020 2020 2065 7870 6572 745f  .        expert_
-0000fc60: 6761 7465 2c20 6578 7065 7274 5f69 6e64  gate, expert_ind
-0000fc70: 6578 203d 2073 656c 662e 746f 706b 2872  ex = self.topk(r
-0000fc80: 6f75 7465 725f 7072 6f62 2c20 7365 6c66  outer_prob, self
-0000fc90: 2e6e 756d 5f65 7870 6572 7473 5f63 686f  .num_experts_cho
-0000fca0: 7365 6e29 2023 2028 6470 2c20 4e2c 206b  sen) # (dp, N, k
-0000fcb0: 2969 6e74 3332 2c20 2864 702c 204e 2c20  )int32, (dp, N, 
-0000fcc0: 6b29 6670 3332 203c 2d2d 2028 6470 2c20  k)fp32 <-- (dp, 
-0000fcd0: 4e2c 2065 7870 6572 745f 6469 6d29 6670  N, expert_dim)fp
-0000fce0: 3332 0a20 2020 2020 2020 2069 6620 7365  32.        if se
-0000fcf0: 6c66 2e6d 6f65 5f63 6f6e 6669 672e 656e  lf.moe_config.en
-0000fd00: 6162 6c65 5f73 6472 6f70 3a0a 2020 2020  able_sdrop:.    
-0000fd10: 2020 2020 2020 2020 6469 7370 6174 6368          dispatch
-0000fd20: 5f69 6e64 6578 2c20 636f 6d62 696e 655f  _index, combine_
-0000fd30: 696e 6465 782c 2072 6f75 7465 725f 636f  index, router_co
-0000fd40: 6566 6620 3d20 7365 6c66 2e5f 6d61 736b  eff = self._mask
-0000fd50: 6f75 745f 6f76 6572 666c 6f77 6564 5f74  out_overflowed_t
-0000fd60: 6f6b 656e 735f 736f 7274 5f73 6472 6f70  okens_sort_sdrop
-0000fd70: 2865 7870 6572 745f 696e 6465 782c 2065  (expert_index, e
-0000fd80: 7870 6572 745f 6761 7465 2920 2320 2864  xpert_gate) # (d
-0000fd90: 702c 2045 2c20 6e29 696e 7433 322c 2028  p, E, n)int32, (
-0000fda0: 6470 2c20 4e2c 206b 292c 2028 6470 2c20  dp, N, k), (dp, 
-0000fdb0: 4e2c 206b 2920 3c2d 2d20 2864 702c 204e  N, k) <-- (dp, N
-0000fdc0: 2c20 6b29 2c20 2864 702c 204e 2c20 6b29  , k), (dp, N, k)
-0000fdd0: 0a20 2020 2020 2020 2065 6c73 653a 0a20  .        else:. 
-0000fde0: 2020 2020 2020 2020 2020 2064 6973 7061             dispa
-0000fdf0: 7463 685f 696e 6465 782c 2063 6f6d 6269  tch_index, combi
-0000fe00: 6e65 5f69 6e64 6578 2c20 726f 7574 6572  ne_index, router
-0000fe10: 5f63 6f65 6666 203d 2073 656c 662e 5f6d  _coeff = self._m
-0000fe20: 6173 6b6f 7574 5f6f 7665 7266 6c6f 7765  askout_overflowe
-0000fe30: 645f 746f 6b65 6e73 5f73 6f72 745f 6b64  d_tokens_sort_kd
-0000fe40: 726f 7028 6578 7065 7274 5f69 6e64 6578  rop(expert_index
-0000fe50: 2c20 6578 7065 7274 5f67 6174 6529 2023  , expert_gate) #
-0000fe60: 2028 6470 2c20 452c 206e 2969 6e74 3332   (dp, E, n)int32
-0000fe70: 2c20 2864 702c 204e 2c20 6b29 2c20 2864  , (dp, N, k), (d
-0000fe80: 702c 204e 2c20 6b29 203c 2d2d 2028 6470  p, N, k) <-- (dp
-0000fe90: 2c20 4e2c 206b 292c 2028 6470 2c20 4e2c  , N, k), (dp, N,
-0000fea0: 206b 290a 2020 2020 2020 2020 7265 7475   k).        retu
-0000feb0: 726e 2064 6973 7061 7463 685f 696e 6465  rn dispatch_inde
-0000fec0: 782c 2063 6f6d 6269 6e65 5f69 6e64 6578  x, combine_index
-0000fed0: 2c20 726f 7574 6572 5f63 6f65 6666 2023  , router_coeff #
-0000fee0: 2028 6470 2c20 452c 206e 2969 6e74 3332   (dp, E, n)int32
-0000fef0: 2c20 2864 702c 204e 2c20 6b29 2c20 2864  , (dp, N, k), (d
-0000ff00: 702c 204e 2c20 6b29 0a0a 2020 2020 6465  p, N, k)..    de
-0000ff10: 6620 5f6d 6173 6b6f 7574 5f6f 7665 7266  f _maskout_overf
-0000ff20: 6c6f 7765 645f 746f 6b65 6e73 5f73 6f72  lowed_tokens_sor
-0000ff30: 745f 6b64 726f 7028 7365 6c66 2c20 6578  t_kdrop(self, ex
-0000ff40: 7065 7274 5f69 6e64 6578 2c20 6578 7065  pert_index, expe
-0000ff50: 7274 5f67 6174 6529 3a0a 2020 2020 2020  rt_gate):.      
-0000ff60: 2020 2222 220a 2020 2020 2020 2020 4b65    """.        Ke
-0000ff70: 6570 696e 6720 6f6e 6c79 2074 6865 2074  eping only the t
-0000ff80: 6f6b 656e 7320 7468 6174 2066 6974 2077  okens that fit w
-0000ff90: 6974 6869 6e20 6578 7065 7274 5f63 6170  ithin expert_cap
-0000ffa0: 6163 6974 792e 0a20 2020 2020 2020 2023  acity..        #
-0000ffb0: 2069 6620 746f 6b65 6e73 5f70 6572 5f67   if tokens_per_g
-0000ffc0: 726f 7570 3e31 303a 2073 656c 662e 7072  roup>10: self.pr
-0000ffd0: 696e 7428 2272 616e 6765 5f6b 6e22 2c20  int("range_kn", 
-0000ffe0: 7261 6e67 655f 6b6e 290a 2020 2020 2020  range_kn).      
-0000fff0: 2020 2222 220a 2020 2020 2020 2020 6b20    """.        k 
-00010000: 3d20 7365 6c66 2e6e 756d 5f65 7870 6572  = self.num_exper
-00010010: 7473 5f63 686f 7365 6e0a 2020 2020 2020  ts_chosen.      
-00010020: 2020 746f 6b65 6e73 5f70 6572 5f67 726f    tokens_per_gro
-00010030: 7570 203d 2073 656c 662e 7368 6170 6528  up = self.shape(
-00010040: 6578 7065 7274 5f69 6e64 6578 295b 315d  expert_index)[1]
-00010050: 0a20 2020 2020 2020 206b 6e20 3d20 6b20  .        kn = k 
-00010060: 2a20 746f 6b65 6e73 5f70 6572 5f67 726f  * tokens_per_gro
-00010070: 7570 2023 2074 6869 7320 6e20 7265 6665  up # this n refe
-00010080: 7273 2074 6f20 4e0a 2020 2020 2020 2020  rs to N.        
-00010090: 6578 7065 7274 5f63 6170 6163 6974 7920  expert_capacity 
-000100a0: 3d20 6361 6c63 756c 6174 655f 6578 7065  = calculate_expe
-000100b0: 7274 5f63 6170 6163 6974 795f 7632 2873  rt_capacity_v2(s
-000100c0: 656c 662e 6e75 6d5f 6578 7065 7274 735f  elf.num_experts_
-000100d0: 6368 6f73 656e 2c20 746f 6b65 6e73 5f70  chosen, tokens_p
-000100e0: 6572 5f67 726f 7570 2c0a 2020 2020 2020  er_group,.      
-000100f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010100: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010110: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010120: 2073 656c 662e 6361 7061 6369 7479 5f66   self.capacity_f
-00010130: 6163 746f 722c 2073 656c 662e 6578 7065  actor, self.expe
-00010140: 7274 5f64 696d 2c20 7365 6c66 2e6d 7029  rt_dim, self.mp)
-00010150: 0a0a 2020 2020 2020 2020 2320 6361 6c63  ..        # calc
-00010160: 756c 6174 6520 636f 6d62 696e 655f 696e  ulate combine_in
-00010170: 6465 7820 6672 6f6d 2063 756d 7375 6d0a  dex from cumsum.
-00010180: 2020 2020 2020 2020 234b 2d64 726f 7028          #K-drop(
-00010190: 6b2c 4e29 0a20 2020 2020 2020 2065 7870  k,N).        exp
-000101a0: 6572 745f 696e 6465 7820 3d20 7365 6c66  ert_index = self
-000101b0: 2e72 6573 6861 7065 2873 656c 662e 7472  .reshape(self.tr
-000101c0: 616e 7370 6f73 655f 3364 2865 7870 6572  anspose_3d(exper
-000101d0: 745f 696e 6465 782c 2028 302c 2032 2c20  t_index, (0, 2, 
-000101e0: 3129 292c 2028 6578 7065 7274 5f69 6e64  1)), (expert_ind
-000101f0: 6578 2e73 6861 7065 5b30 5d2c 202d 3129  ex.shape[0], -1)
-00010200: 2920 2320 2864 702c 206b 4e29 203c 2d2d  ) # (dp, kN) <--
-00010210: 2028 6470 2c20 4e2c 206b 2920 6163 636f   (dp, N, k) acco
-00010220: 756e 7420 666f 7220 746f 706b 2070 7269  unt for topk pri
-00010230: 6f72 6974 790a 2020 2020 2020 2020 6578  ority.        ex
-00010240: 7065 7274 5f6d 6173 6b20 3d20 7365 6c66  pert_mask = self
-00010250: 2e6f 6e65 686f 745f 3264 2865 7870 6572  .onehot_2d(exper
-00010260: 745f 696e 6465 782c 2073 656c 662e 6578  t_index, self.ex
-00010270: 7065 7274 5f64 696d 2c20 7365 6c66 2e6f  pert_dim, self.o
-00010280: 6e5f 7661 6c75 652c 2073 656c 662e 6f66  n_value, self.of
-00010290: 665f 7661 6c75 6529 2023 2028 6470 2c20  f_value) # (dp, 
-000102a0: 6b4e 2c20 4529 6670 3332 203c 2d2d 2028  kN, E)fp32 <-- (
-000102b0: 6470 2c20 6b4e 2969 6e74 3332 0a20 2020  dp, kN)int32.   
-000102c0: 2020 2020 2070 6f73 6974 696f 6e5f 696e       position_in
-000102d0: 5f65 7870 6572 7420 3d20 7365 6c66 2e6d  _expert = self.m
-000102e0: 756c 5f33 6428 7365 6c66 2e63 756d 7375  ul_3d(self.cumsu
-000102f0: 6d28 6578 7065 7274 5f6d 6173 6b2c 2031  m(expert_mask, 1
-00010300: 292c 2065 7870 6572 745f 6d61 736b 2920  ), expert_mask) 
-00010310: 2320 2864 702c 206b 4e2c 2045 2966 7031  # (dp, kN, E)fp1
-00010320: 3620 3c2d 2d20 2864 702c 206b 4e2c 2045  6 <-- (dp, kN, E
-00010330: 2966 7033 322c 2028 6470 2c20 6b4e 2c20  )fp32, (dp, kN, 
-00010340: 4529 6670 3332 0a20 2020 2020 2020 2070  E)fp32.        p
-00010350: 6f73 6974 696f 6e5f 696e 5f65 7870 6572  osition_in_exper
-00010360: 7420 3d20 7365 6c66 2e6d 756c 5f33 6428  t = self.mul_3d(
-00010370: 706f 7369 7469 6f6e 5f69 6e5f 6578 7065  position_in_expe
-00010380: 7274 2c20 7365 6c66 2e6c 6573 7328 706f  rt, self.less(po
-00010390: 7369 7469 6f6e 5f69 6e5f 6578 7065 7274  sition_in_expert
-000103a0: 2c20 6578 7065 7274 5f63 6170 6163 6974  , expert_capacit
-000103b0: 792b 3129 2920 2320 2864 702c 206b 4e2c  y+1)) # (dp, kN,
-000103c0: 2045 2966 7033 3220 3c2d 2d20 2864 702c   E)fp32 <-- (dp,
-000103d0: 206b 4e2c 2045 2966 7033 322c 2028 6470   kN, E)fp32, (dp
-000103e0: 2c20 6b4e 2c20 4529 626f 6f6c 2c20 7768  , kN, E)bool, wh
-000103f0: 6572 6520 303c 3d70 6f73 6974 696f 6e5f  ere 0<=position_
-00010400: 696e 5f65 7870 6572 743c 2831 2b6e 290a  in_expert<(1+n).
-00010410: 2020 2020 2020 2020 706f 7369 7469 6f6e          position
-00010420: 5f69 6e5f 6578 7065 7274 5f32 6420 3d20  _in_expert_2d = 
-00010430: 7365 6c66 2e72 6564 7563 655f 7375 6d28  self.reduce_sum(
-00010440: 706f 7369 7469 6f6e 5f69 6e5f 6578 7065  position_in_expe
-00010450: 7274 2c20 2d31 2920 2320 2864 702c 206b  rt, -1) # (dp, k
-00010460: 4e29 6670 3332 203c 2d2d 2028 6470 2c20  N)fp32 <-- (dp, 
-00010470: 6b4e 2c20 4529 6670 3332 0a20 2020 2020  kN, E)fp32.     
-00010480: 2020 2063 6f6d 6269 6e65 5f69 6e64 6578     combine_index
-00010490: 203d 2073 656c 662e 6164 645f 3264 2873   = self.add_2d(s
-000104a0: 656c 662e 6d75 6c5f 3264 5f31 6428 6578  elf.mul_2d_1d(ex
-000104b0: 7065 7274 5f69 6e64 6578 2c20 6578 7065  pert_index, expe
-000104c0: 7274 5f63 6170 6163 6974 7920 2b20 3129  rt_capacity + 1)
-000104d0: 2c20 706f 7369 7469 6f6e 5f69 6e5f 6578  , position_in_ex
-000104e0: 7065 7274 5f32 6429 2023 2028 6470 2c20  pert_2d) # (dp, 
-000104f0: 6b4e 2966 7033 3220 3c2d 2d20 2864 702c  kN)fp32 <-- (dp,
-00010500: 206b 4e29 6670 3332 2c20 2864 702c 206b   kN)fp32, (dp, k
-00010510: 4e29 6670 3332 2077 6865 7265 2030 3c3d  N)fp32 where 0<=
-00010520: 2063 6f6d 6269 6e65 5f69 6e64 6578 203c   combine_index <
-00010530: 452a 2831 2b6e 292c 2063 6f6d 6269 6e65  E*(1+n), combine
-00010540: 5f69 6e64 6578 203d 2065 7870 6572 745f  _index = expert_
-00010550: 6964 202a 2831 2b6e 2920 2b20 706f 7369  id *(1+n) + posi
-00010560: 7469 6f6e 5f69 6e5f 6578 7065 7274 5f32  tion_in_expert_2
-00010570: 640a 2020 2020 2020 2020 636f 6d62 696e  d.        combin
-00010580: 655f 696e 6465 7820 3d20 7365 6c66 2e74  e_index = self.t
-00010590: 7261 6e73 706f 7365 5f33 6428 7365 6c66  ranspose_3d(self
-000105a0: 2e72 6573 6861 7065 2863 6f6d 6269 6e65  .reshape(combine
-000105b0: 5f69 6e64 6578 2c20 2863 6f6d 6269 6e65  _index, (combine
-000105c0: 5f69 6e64 6578 2e73 6861 7065 5b30 5d2c  _index.shape[0],
-000105d0: 206b 2c20 746f 6b65 6e73 5f70 6572 5f67   k, tokens_per_g
-000105e0: 726f 7570 2929 2c20 2830 2c20 322c 2031  roup)), (0, 2, 1
-000105f0: 2929 2023 2028 6470 2c20 4e2c 206b 2920  )) # (dp, N, k) 
-00010600: 3c2d 2d20 2864 702c 206b 4e29 2061 6363  <-- (dp, kN) acc
-00010610: 6f75 6e74 2066 6f72 2074 6f70 6b20 7072  ount for topk pr
-00010620: 696f 7269 7479 0a20 2020 2020 2020 2077  iority.        w
-00010630: 6974 6869 6e5f 6361 7061 6369 7479 203d  ithin_capacity =
-00010640: 2073 656c 662e 6361 7374 2873 656c 662e   self.cast(self.
-00010650: 6774 2870 6f73 6974 696f 6e5f 696e 5f65  gt(position_in_e
-00010660: 7870 6572 745f 3264 2c20 3029 2c20 6d73  xpert_2d, 0), ms
-00010670: 7479 7065 2e66 6c6f 6174 3332 2920 2320  type.float32) # 
-00010680: 2864 702c 206b 4e29 626f 6f6c 0a0a 2020  (dp, kN)bool..  
-00010690: 2020 2020 2020 2320 6361 6c63 756c 6174        # calculat
-000106a0: 6520 6469 7370 6174 6368 5f69 6e64 6578  e dispatch_index
-000106b0: 2066 726f 6d20 706f 7369 7469 6f6e 5f69   from position_i
-000106c0: 6e5f 6578 7065 7274 5f6f 6e65 686f 740a  n_expert_onehot.
-000106d0: 2020 2020 2020 2020 7361 6665 5f6b 6e20          safe_kn 
-000106e0: 3d20 3220 2a20 6b6e 2023 2066 6163 746f  = 2 * kn # facto
-000106f0: 723d 3220 666f 7220 7361 6665 7479 0a20  r=2 for safety. 
-00010700: 2020 2020 2020 2072 616e 6765 5f6b 6e20         range_kn 
-00010710: 3d20 7365 6c66 2e73 6c69 6365 5f72 616e  = self.slice_ran
-00010720: 6765 2873 656c 662e 7261 6e67 6532 2c20  ge(self.range2, 
-00010730: 2830 2c20 3029 2c20 2873 656c 662e 6578  (0, 0), (self.ex
-00010740: 7065 7274 5f64 696d 2c20 6b6e 292c 2028  pert_dim, kn), (
-00010750: 312c 2031 2929 2e72 6573 6861 7065 2831  1, 1)).reshape(1
-00010760: 2c20 7365 6c66 2e65 7870 6572 745f 6469  , self.expert_di
-00010770: 6d2c 206b 6e29 2023 2831 2c20 452c 206b  m, kn) #(1, E, k
-00010780: 4e29 2066 7033 3220 3c2d 2d20 2845 2c20  N) fp32 <-- (E, 
-00010790: 3133 3130 3732 290a 2020 2020 2020 2020  131072).        
-000107a0: 7365 6c65 6374 203d 2073 656c 662e 7472  select = self.tr
-000107b0: 616e 7370 6f73 655f 3364 2865 7870 6572  anspose_3d(exper
-000107c0: 745f 6d61 736b 2c20 2830 2c20 322c 2031  t_mask, (0, 2, 1
-000107d0: 2929 2023 2028 6470 2c20 452c 206b 4e29  )) # (dp, E, kN)
-000107e0: 2066 7033 3220 3c2d 2d20 2864 702c 206b   fp32 <-- (dp, k
-000107f0: 4e2c 2045 2920 6670 3332 0a20 2020 2020  N, E) fp32.     
-00010800: 2020 2064 6973 7061 7463 685f 696e 6465     dispatch_inde
-00010810: 785f 7261 7720 3d20 7365 6c66 2e61 6464  x_raw = self.add
-00010820: 5f33 6428 7365 6c66 2e6d 756c 5f72 616e  _3d(self.mul_ran
-00010830: 6765 2873 656c 6563 742c 2072 616e 6765  ge(select, range
-00010840: 5f6b 6e29 2c20 7365 6c66 2e6d 756c 5f72  _kn), self.mul_r
-00010850: 616e 6765 2873 656c 662e 7375 625f 7261  ange(self.sub_ra
-00010860: 6e67 6528 312c 2073 656c 6563 7429 2c20  nge(1, select), 
-00010870: 7365 6c66 2e61 6464 5f72 616e 6765 2872  self.add_range(r
-00010880: 616e 6765 5f6b 6e2c 2073 6166 655f 6b6e  ange_kn, safe_kn
-00010890: 2929 2920 2320 2864 702c 2045 2c20 6b4e  ))) # (dp, E, kN
-000108a0: 2920 3c2d 2d20 2864 702c 2045 2c20 6b4e  ) <-- (dp, E, kN
-000108b0: 2920 6670 3332 0a20 2020 2020 2020 2064  ) fp32.        d
-000108c0: 6973 7061 7463 685f 696e 6465 782c 205f  ispatch_index, _
-000108d0: 203d 2073 656c 662e 736f 7274 5f72 616e   = self.sort_ran
-000108e0: 6765 2864 6973 7061 7463 685f 696e 6465  ge(dispatch_inde
-000108f0: 785f 7261 7729 2023 2028 6470 2c20 452c  x_raw) # (dp, E,
-00010900: 206b 2920 3c2d 2d20 2864 702c 2045 2c20   k) <-- (dp, E, 
-00010910: 6b4e efbc 890a 2020 2020 2020 2020 6469  kN....        di
-00010920: 7370 6174 6368 5f69 6e64 6578 203d 2073  spatch_index = s
-00010930: 656c 662e 736c 6963 6528 6469 7370 6174  elf.slice(dispat
-00010940: 6368 5f69 6e64 6578 2c20 2830 2c20 302c  ch_index, (0, 0,
-00010950: 2030 292c 2028 6469 7370 6174 6368 5f69   0), (dispatch_i
-00010960: 6e64 6578 2e73 6861 7065 5b30 5d2c 2064  ndex.shape[0], d
-00010970: 6973 7061 7463 685f 696e 6465 782e 7368  ispatch_index.sh
-00010980: 6170 655b 315d 2c20 6578 7065 7274 5f63  ape[1], expert_c
-00010990: 6170 6163 6974 7929 2c20 2831 2c20 312c  apacity), (1, 1,
-000109a0: 2031 2929 2023 2028 6470 2c20 452c 206e   1)) # (dp, E, n
-000109b0: 2920 3c2d 2d20 2864 702c 2045 2c20 6b4e  ) <-- (dp, E, kN
-000109c0: 2920 6670 3332 0a20 2020 2020 2020 2069  ) fp32.        i
-000109d0: 735f 7361 6665 203d 2073 656c 662e 6c65  s_safe = self.le
-000109e0: 7373 2864 6973 7061 7463 685f 696e 6465  ss(dispatch_inde
-000109f0: 782c 2073 6166 655f 6b6e 2920 2320 2864  x, safe_kn) # (d
-00010a00: 702c 2045 2c20 6e29 2062 6f6f 6c0a 2020  p, E, n) bool.  
-00010a10: 2020 2020 2020 6469 7370 6174 6368 5f69        dispatch_i
-00010a20: 6e64 6578 203d 2073 656c 662e 6164 645f  ndex = self.add_
-00010a30: 6f6e 6528 7365 6c66 2e6d 6f64 2864 6973  one(self.mod(dis
-00010a40: 7061 7463 685f 696e 6465 782c 2074 6f6b  patch_index, tok
-00010a50: 656e 735f 7065 725f 6772 6f75 7029 2c20  ens_per_group), 
-00010a60: 3129 2023 2028 6470 2c20 452c 206e 2920  1) # (dp, E, n) 
-00010a70: 6670 3332 0a20 2020 2020 2020 2064 6973  fp32.        dis
-00010a80: 7061 7463 685f 696e 6465 7820 3d20 7365  patch_index = se
-00010a90: 6c66 2e6d 756c 5f33 6428 6469 7370 6174  lf.mul_3d(dispat
-00010aa0: 6368 5f69 6e64 6578 2c20 6973 5f73 6166  ch_index, is_saf
-00010ab0: 6529 2023 2028 6470 2c20 452c 206e 2920  e) # (dp, E, n) 
-00010ac0: 6670 3332 0a0a 2020 2020 2020 2020 2320  fp32..        # 
-00010ad0: 7265 7475 726e 0a20 2020 2020 2020 2064  return.        d
-00010ae0: 6973 7061 7463 685f 696e 6465 7820 3d20  ispatch_index = 
-00010af0: 7365 6c66 2e63 6173 7428 6469 7370 6174  self.cast(dispat
-00010b00: 6368 5f69 6e64 6578 2c20 6d73 7479 7065  ch_index, mstype
-00010b10: 2e69 6e74 3332 290a 2020 2020 2020 2020  .int32).        
-00010b20: 636f 6d62 696e 655f 696e 6465 7820 3d20  combine_index = 
-00010b30: 7365 6c66 2e63 6173 7428 636f 6d62 696e  self.cast(combin
-00010b40: 655f 696e 6465 782c 206d 7374 7970 652e  e_index, mstype.
-00010b50: 696e 7433 3229 0a20 2020 2020 2020 2072  int32).        r
-00010b60: 6f75 7465 725f 636f 6566 665f 7261 7720  outer_coeff_raw 
-00010b70: 3d20 7365 6c66 2e6d 756c 5f33 6428 6578  = self.mul_3d(ex
-00010b80: 7065 7274 5f67 6174 652c 2073 656c 662e  pert_gate, self.
-00010b90: 7472 616e 7370 6f73 655f 3364 2873 656c  transpose_3d(sel
-00010ba0: 662e 7265 7368 6170 6528 7769 7468 696e  f.reshape(within
-00010bb0: 5f63 6170 6163 6974 792c 2028 7769 7468  _capacity, (with
-00010bc0: 696e 5f63 6170 6163 6974 792e 7368 6170  in_capacity.shap
-00010bd0: 655b 305d 2c20 6b2c 2074 6f6b 656e 735f  e[0], k, tokens_
-00010be0: 7065 725f 6772 6f75 7029 292c 2028 302c  per_group)), (0,
-00010bf0: 2032 2c20 3129 2929 2023 2061 7070 6c79   2, 1))) # apply
-00010c00: 2077 6974 6869 6e5f 6361 7061 6369 7479   within_capacity
-00010c10: 2028 6470 2c20 4e2c 206b 2920 3c2d 2d20   (dp, N, k) <-- 
-00010c20: 2864 702c 204e 2c20 6b29 2c20 2864 702c  (dp, N, k), (dp,
-00010c30: 204e 2c20 6b29 203c 2d2d 2020 2864 702c   N, k) <--  (dp,
-00010c40: 206b 4e29 0a20 2020 2020 2020 2072 6f75   kN).        rou
-00010c50: 7465 725f 636f 6566 6620 3d20 7365 6c66  ter_coeff = self
-00010c60: 2e5f 6e6f 726d 616c 697a 6528 726f 7574  ._normalize(rout
-00010c70: 6572 5f63 6f65 6666 5f72 6177 2920 2320  er_coeff_raw) # 
-00010c80: 2864 702c 204e 2c20 6b29 203c 2d2d 2028  (dp, N, k) <-- (
-00010c90: 6470 2c20 4e2c 206b 290a 2020 2020 2020  dp, N, k).      
-00010ca0: 2020 7265 7475 726e 2064 6973 7061 7463    return dispatc
-00010cb0: 685f 696e 6465 782c 2063 6f6d 6269 6e65  h_index, combine
-00010cc0: 5f69 6e64 6578 2c20 726f 7574 6572 5f63  _index, router_c
-00010cd0: 6f65 6666 2023 2028 6470 2c20 452c 206e  oeff # (dp, E, n
-00010ce0: 292c 2028 6470 2c20 4e2c 206b 292c 2028  ), (dp, N, k), (
-00010cf0: 6470 2c20 4e2c 206b 290a 0a20 2020 2064  dp, N, k)..    d
-00010d00: 6566 205f 6d61 736b 6f75 745f 6f76 6572  ef _maskout_over
-00010d10: 666c 6f77 6564 5f74 6f6b 656e 735f 736f  flowed_tokens_so
-00010d20: 7274 5f73 6472 6f70 2873 656c 662c 2065  rt_sdrop(self, e
-00010d30: 7870 6572 745f 696e 6465 782c 2065 7870  xpert_index, exp
-00010d40: 6572 745f 6761 7465 293a 0a20 2020 2020  ert_gate):.     
-00010d50: 2020 2022 2222 0a20 2020 2020 2020 204b     """.        K
-00010d60: 6565 7069 6e67 206f 6e6c 7920 7468 6520  eeping only the 
-00010d70: 746f 6b65 6e73 2074 6861 7420 6669 7420  tokens that fit 
-00010d80: 7769 7468 696e 2065 7870 6572 745f 6361  within expert_ca
-00010d90: 7061 6369 7479 2e0a 2020 2020 2020 2020  pacity..        
-00010da0: 2320 6966 2074 6f6b 656e 735f 7065 725f  # if tokens_per_
-00010db0: 6772 6f75 703e 3130 3a20 7365 6c66 2e70  group>10: self.p
-00010dc0: 7269 6e74 2822 7261 6e67 655f 6b6e 222c  rint("range_kn",
-00010dd0: 2072 616e 6765 5f6b 6e29 0a20 2020 2020   range_kn).     
-00010de0: 2020 2022 2222 0a20 2020 2020 2020 206b     """.        k
-00010df0: 203d 2073 656c 662e 6e75 6d5f 6578 7065   = self.num_expe
-00010e00: 7274 735f 6368 6f73 656e 0a20 2020 2020  rts_chosen.     
-00010e10: 2020 2074 6f6b 656e 735f 7065 725f 6772     tokens_per_gr
-00010e20: 6f75 7020 3d20 7365 6c66 2e73 6861 7065  oup = self.shape
-00010e30: 2865 7870 6572 745f 696e 6465 7829 5b31  (expert_index)[1
-00010e40: 5d0a 2020 2020 2020 2020 6b6e 203d 206b  ].        kn = k
-00010e50: 202a 2074 6f6b 656e 735f 7065 725f 6772   * tokens_per_gr
-00010e60: 6f75 7020 2320 7468 6973 206e 2072 6566  oup # this n ref
-00010e70: 6572 7320 746f 204e 0a20 2020 2020 2020  ers to N.       
-00010e80: 2065 7870 6572 745f 6361 7061 6369 7479   expert_capacity
-00010e90: 203d 2063 616c 6375 6c61 7465 5f65 7870   = calculate_exp
-00010ea0: 6572 745f 6361 7061 6369 7479 5f76 3228  ert_capacity_v2(
-00010eb0: 7365 6c66 2e6e 756d 5f65 7870 6572 7473  self.num_experts
-00010ec0: 5f63 686f 7365 6e2c 2074 6f6b 656e 735f  _chosen, tokens_
-00010ed0: 7065 725f 6772 6f75 702c 0a20 2020 2020  per_group,.     
-00010ee0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010ef0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010f00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010f10: 2020 7365 6c66 2e63 6170 6163 6974 795f    self.capacity_
-00010f20: 6661 6374 6f72 2c20 7365 6c66 2e65 7870  factor, self.exp
-00010f30: 6572 745f 6469 6d2c 2073 656c 662e 6d70  ert_dim, self.mp
-00010f40: 290a 0a20 2020 2020 2020 2023 2063 616c  )..        # cal
-00010f50: 6375 6c61 7465 2063 6f6d 6269 6e65 5f69  culate combine_i
-00010f60: 6e64 6578 2066 726f 6d20 6375 6d73 756d  ndex from cumsum
-00010f70: 0a20 2020 2020 2020 2023 532d 6472 6f70  .        #S-drop
-00010f80: 284e 2c6b 290a 2020 2020 2020 2020 6578  (N,k).        ex
-00010f90: 7065 7274 5f69 6e64 6578 203d 2073 656c  pert_index = sel
-00010fa0: 662e 7265 7368 6170 6528 6578 7065 7274  f.reshape(expert
-00010fb0: 5f69 6e64 6578 2c20 2865 7870 6572 745f  _index, (expert_
-00010fc0: 696e 6465 782e 7368 6170 655b 305d 2c20  index.shape[0], 
-00010fd0: 2d31 2929 2023 2028 6470 2c20 4e6b 2920  -1)) # (dp, Nk) 
-00010fe0: 3c2d 2d20 2864 702c 204e 2c20 6b29 2061  <-- (dp, N, k) a
-00010ff0: 6363 6f75 6e74 2066 6f72 2074 6f70 6b20  ccount for topk 
-00011000: 7072 696f 7269 7479 0a20 2020 2020 2020  priority.       
-00011010: 2065 7870 6572 745f 6d61 736b 203d 2073   expert_mask = s
-00011020: 656c 662e 6f6e 6568 6f74 5f32 6428 6578  elf.onehot_2d(ex
-00011030: 7065 7274 5f69 6e64 6578 2c20 7365 6c66  pert_index, self
-00011040: 2e65 7870 6572 745f 6469 6d2c 2073 656c  .expert_dim, sel
-00011050: 662e 6f6e 5f76 616c 7565 2c20 7365 6c66  f.on_value, self
-00011060: 2e6f 6666 5f76 616c 7565 2920 2320 2864  .off_value) # (d
-00011070: 702c 204e 6b2c 2045 2966 7033 3220 3c2d  p, Nk, E)fp32 <-
-00011080: 2d20 2864 702c 204e 6b29 696e 7433 320a  - (dp, Nk)int32.
-00011090: 2020 2020 2020 2020 706f 7369 7469 6f6e          position
-000110a0: 5f69 6e5f 6578 7065 7274 203d 2073 656c  _in_expert = sel
-000110b0: 662e 6d75 6c5f 3364 2873 656c 662e 6375  f.mul_3d(self.cu
-000110c0: 6d73 756d 2865 7870 6572 745f 6d61 736b  msum(expert_mask
-000110d0: 2c20 3129 2c20 6578 7065 7274 5f6d 6173  , 1), expert_mas
-000110e0: 6b29 2023 2028 6470 2c20 4e6b 2c20 4529  k) # (dp, Nk, E)
-000110f0: 6670 3136 203c 2d2d 2028 6470 2c20 4e6b  fp16 <-- (dp, Nk
-00011100: 2c20 4529 6670 3332 2c20 2864 702c 204e  , E)fp32, (dp, N
-00011110: 6b2c 2045 2966 7033 320a 2020 2020 2020  k, E)fp32.      
-00011120: 2020 706f 7369 7469 6f6e 5f69 6e5f 6578    position_in_ex
-00011130: 7065 7274 203d 2073 656c 662e 6d75 6c5f  pert = self.mul_
-00011140: 3364 2870 6f73 6974 696f 6e5f 696e 5f65  3d(position_in_e
-00011150: 7870 6572 742c 2073 656c 662e 6c65 7373  xpert, self.less
-00011160: 2870 6f73 6974 696f 6e5f 696e 5f65 7870  (position_in_exp
-00011170: 6572 742c 2065 7870 6572 745f 6361 7061  ert, expert_capa
-00011180: 6369 7479 2b31 2929 2023 2028 6470 2c20  city+1)) # (dp, 
-00011190: 4e6b 2c20 4529 6670 3332 203c 2d2d 2028  Nk, E)fp32 <-- (
-000111a0: 6470 2c20 4e6b 2c20 4529 6670 3332 2c20  dp, Nk, E)fp32, 
-000111b0: 2864 702c 204e 6b2c 2045 2962 6f6f 6c2c  (dp, Nk, E)bool,
-000111c0: 2077 6865 7265 2030 3c3d 706f 7369 7469   where 0<=positi
-000111d0: 6f6e 5f69 6e5f 6578 7065 7274 3c28 312b  on_in_expert<(1+
-000111e0: 6e29 0a20 2020 2020 2020 2070 6f73 6974  n).        posit
-000111f0: 696f 6e5f 696e 5f65 7870 6572 745f 3264  ion_in_expert_2d
-00011200: 203d 2073 656c 662e 7265 6475 6365 5f73   = self.reduce_s
-00011210: 756d 2870 6f73 6974 696f 6e5f 696e 5f65  um(position_in_e
-00011220: 7870 6572 742c 202d 3129 2023 2028 6470  xpert, -1) # (dp
-00011230: 2c20 4e6b 2966 7033 3220 3c2d 2d20 2864  , Nk)fp32 <-- (d
-00011240: 702c 204e 6b2c 2045 2966 7033 320a 2020  p, Nk, E)fp32.  
-00011250: 2020 2020 2020 636f 6d62 696e 655f 696e        combine_in
-00011260: 6465 7820 3d20 7365 6c66 2e61 6464 5f32  dex = self.add_2
-00011270: 6428 7365 6c66 2e6d 756c 5f32 645f 3164  d(self.mul_2d_1d
-00011280: 2865 7870 6572 745f 696e 6465 782c 2065  (expert_index, e
-00011290: 7870 6572 745f 6361 7061 6369 7479 202b  xpert_capacity +
-000112a0: 2031 292c 2070 6f73 6974 696f 6e5f 696e   1), position_in
-000112b0: 5f65 7870 6572 745f 3264 2920 2320 2864  _expert_2d) # (d
-000112c0: 702c 204e 6b29 6670 3332 203c 2d2d 2028  p, Nk)fp32 <-- (
-000112d0: 6470 2c20 4e6b 2966 7033 322c 2028 6470  dp, Nk)fp32, (dp
-000112e0: 2c20 4e6b 2966 7033 3220 7768 6572 6520  , Nk)fp32 where 
-000112f0: 303c 3d20 636f 6d62 696e 655f 696e 6465  0<= combine_inde
-00011300: 7820 3c45 2a28 312b 6e29 2c20 636f 6d62  x <E*(1+n), comb
-00011310: 696e 655f 696e 6465 7820 3d20 6578 7065  ine_index = expe
-00011320: 7274 5f69 6420 2a28 312b 6e29 202b 2070  rt_id *(1+n) + p
-00011330: 6f73 6974 696f 6e5f 696e 5f65 7870 6572  osition_in_exper
-00011340: 745f 3264 0a20 2020 2020 2020 2063 6f6d  t_2d.        com
-00011350: 6269 6e65 5f69 6e64 6578 203d 2073 656c  bine_index = sel
-00011360: 662e 7265 7368 6170 6528 636f 6d62 696e  f.reshape(combin
-00011370: 655f 696e 6465 782c 2028 636f 6d62 696e  e_index, (combin
-00011380: 655f 696e 6465 782e 7368 6170 655b 305d  e_index.shape[0]
-00011390: 2c20 746f 6b65 6e73 5f70 6572 5f67 726f  , tokens_per_gro
-000113a0: 7570 2c20 6b29 2920 2320 2864 702c 204e  up, k)) # (dp, N
-000113b0: 2c20 6b29 203c 2d2d 2028 6470 2c20 4e6b  , k) <-- (dp, Nk
-000113c0: 2920 6163 636f 756e 7420 666f 7220 746f  ) account for to
-000113d0: 706b 2070 7269 6f72 6974 790a 2020 2020  pk priority.    
-000113e0: 2020 2020 7769 7468 696e 5f63 6170 6163      within_capac
-000113f0: 6974 7920 3d20 7365 6c66 2e63 6173 7428  ity = self.cast(
-00011400: 7365 6c66 2e67 7428 706f 7369 7469 6f6e  self.gt(position
-00011410: 5f69 6e5f 6578 7065 7274 5f32 642c 2030  _in_expert_2d, 0
-00011420: 292c 206d 7374 7970 652e 666c 6f61 7433  ), mstype.float3
-00011430: 3229 2023 2028 6470 2c20 4e6b 2962 6f6f  2) # (dp, Nk)boo
-00011440: 6c0a 0a20 2020 2020 2020 2023 2063 616c  l..        # cal
-00011450: 6375 6c61 7465 2064 6973 7061 7463 685f  culate dispatch_
-00011460: 696e 6465 7820 6672 6f6d 2070 6f73 6974  index from posit
-00011470: 696f 6e5f 696e 5f65 7870 6572 745f 6f6e  ion_in_expert_on
-00011480: 6568 6f74 0a20 2020 2020 2020 2073 6166  ehot.        saf
-00011490: 655f 6b6e 203d 2032 202a 206b 6e20 2320  e_kn = 2 * kn # 
-000114a0: 6661 6374 6f72 3d32 2066 6f72 2073 6166  factor=2 for saf
-000114b0: 6574 790a 2020 2020 2020 2020 7261 6e67  ety.        rang
-000114c0: 655f 6b6e 203d 2073 656c 662e 736c 6963  e_kn = self.slic
-000114d0: 655f 7261 6e67 6528 7365 6c66 2e72 616e  e_range(self.ran
-000114e0: 6765 322c 2028 302c 2030 292c 2028 7365  ge2, (0, 0), (se
-000114f0: 6c66 2e65 7870 6572 745f 6469 6d2c 206b  lf.expert_dim, k
-00011500: 6e29 2c20 2831 2c20 3129 292e 7265 7368  n), (1, 1)).resh
-00011510: 6170 6528 312c 2073 656c 662e 6578 7065  ape(1, self.expe
-00011520: 7274 5f64 696d 2c20 6b6e 2920 2328 312c  rt_dim, kn) #(1,
-00011530: 2045 2c20 6b4e 2920 6670 3332 203c 2d2d   E, kN) fp32 <--
-00011540: 2028 452c 2031 3331 3037 3229 0a20 2020   (E, 131072).   
-00011550: 2020 2020 2073 656c 6563 7420 3d20 7365       select = se
-00011560: 6c66 2e74 7261 6e73 706f 7365 5f33 6428  lf.transpose_3d(
-00011570: 6578 7065 7274 5f6d 6173 6b2c 2028 302c  expert_mask, (0,
-00011580: 2032 2c20 3129 2920 2320 2864 702c 2045   2, 1)) # (dp, E
-00011590: 2c20 4e6b 2920 6670 3332 203c 2d2d 2028  , Nk) fp32 <-- (
-000115a0: 6470 2c20 4e6b 2c20 4529 2066 7033 320a  dp, Nk, E) fp32.
-000115b0: 2020 2020 2020 2020 6469 7370 6174 6368          dispatch
-000115c0: 5f69 6e64 6578 5f72 6177 203d 2073 656c  _index_raw = sel
-000115d0: 662e 6164 645f 3364 2873 656c 662e 6d75  f.add_3d(self.mu
-000115e0: 6c5f 7261 6e67 6528 7365 6c65 6374 2c20  l_range(select, 
-000115f0: 7261 6e67 655f 6b6e 292c 2073 656c 662e  range_kn), self.
-00011600: 6d75 6c5f 7261 6e67 6528 7365 6c66 2e73  mul_range(self.s
-00011610: 7562 5f72 616e 6765 2831 2c20 7365 6c65  ub_range(1, sele
-00011620: 6374 292c 2073 656c 662e 6164 645f 7261  ct), self.add_ra
-00011630: 6e67 6528 7261 6e67 655f 6b6e 2c20 7361  nge(range_kn, sa
-00011640: 6665 5f6b 6e29 2929 2023 2028 6470 2c20  fe_kn))) # (dp, 
-00011650: 452c 206b 4e29 203c 2d2d 2028 6470 2c20  E, kN) <-- (dp, 
-00011660: 452c 206b 4e29 2066 7033 320a 2020 2020  E, kN) fp32.    
-00011670: 2020 2020 6469 7370 6174 6368 5f69 6e64      dispatch_ind
-00011680: 6578 2c20 5f20 3d20 7365 6c66 2e73 6f72  ex, _ = self.sor
-00011690: 745f 7261 6e67 6528 6469 7370 6174 6368  t_range(dispatch
-000116a0: 5f69 6e64 6578 5f72 6177 2920 2320 2864  _index_raw) # (d
-000116b0: 702c 2045 2c20 6b29 203c 2d2d 2028 6470  p, E, k) <-- (dp
-000116c0: 2c20 452c 206b 4eef bc89 0a20 2020 2020  , E, kN....     
-000116d0: 2020 2064 6973 7061 7463 685f 696e 6465     dispatch_inde
-000116e0: 7820 3d20 7365 6c66 2e73 6c69 6365 2864  x = self.slice(d
-000116f0: 6973 7061 7463 685f 696e 6465 782c 2028  ispatch_index, (
-00011700: 302c 2030 2c20 3029 2c20 2864 6973 7061  0, 0, 0), (dispa
-00011710: 7463 685f 696e 6465 782e 7368 6170 655b  tch_index.shape[
-00011720: 305d 2c20 6469 7370 6174 6368 5f69 6e64  0], dispatch_ind
-00011730: 6578 2e73 6861 7065 5b31 5d2c 2065 7870  ex.shape[1], exp
-00011740: 6572 745f 6361 7061 6369 7479 292c 2028  ert_capacity), (
-00011750: 312c 2031 2c20 3129 2920 2320 2864 702c  1, 1, 1)) # (dp,
-00011760: 2045 2c20 6e29 203c 2d2d 2028 6470 2c20   E, n) <-- (dp, 
-00011770: 452c 206b 4e29 2066 7033 320a 2020 2020  E, kN) fp32.    
-00011780: 2020 2020 6973 5f73 6166 6520 3d20 7365      is_safe = se
-00011790: 6c66 2e6c 6573 7328 6469 7370 6174 6368  lf.less(dispatch
-000117a0: 5f69 6e64 6578 2c20 7361 6665 5f6b 6e29  _index, safe_kn)
-000117b0: 2023 2028 6470 2c20 452c 206e 2920 626f   # (dp, E, n) bo
-000117c0: 6f6c 0a20 2020 2020 2020 2064 6973 7061  ol.        dispa
-000117d0: 7463 685f 696e 6465 7820 3d20 7365 6c66  tch_index = self
-000117e0: 2e61 6464 5f6f 6e65 286f 7073 2e66 6c6f  .add_one(ops.flo
-000117f0: 6f72 5f64 6976 6964 6528 6469 7370 6174  or_divide(dispat
-00011800: 6368 5f69 6e64 6578 2c20 6b29 2c20 3129  ch_index, k), 1)
-00011810: 2023 2028 6470 2c20 452c 206e 2920 6670   # (dp, E, n) fp
-00011820: 3332 0a20 2020 2020 2020 2064 6973 7061  32.        dispa
-00011830: 7463 685f 696e 6465 7820 3d20 7365 6c66  tch_index = self
-00011840: 2e6d 756c 5f33 6428 6469 7370 6174 6368  .mul_3d(dispatch
-00011850: 5f69 6e64 6578 2c20 6973 5f73 6166 6529  _index, is_safe)
-00011860: 2023 2028 6470 2c20 452c 206e 2920 6670   # (dp, E, n) fp
-00011870: 3332 0a0a 2020 2020 2020 2020 2320 7265  32..        # re
-00011880: 7475 726e 0a20 2020 2020 2020 2064 6973  turn.        dis
-00011890: 7061 7463 685f 696e 6465 7820 3d20 7365  patch_index = se
-000118a0: 6c66 2e63 6173 7428 6469 7370 6174 6368  lf.cast(dispatch
-000118b0: 5f69 6e64 6578 2c20 6d73 7479 7065 2e69  _index, mstype.i
-000118c0: 6e74 3332 290a 2020 2020 2020 2020 636f  nt32).        co
-000118d0: 6d62 696e 655f 696e 6465 7820 3d20 7365  mbine_index = se
-000118e0: 6c66 2e63 6173 7428 636f 6d62 696e 655f  lf.cast(combine_
-000118f0: 696e 6465 782c 206d 7374 7970 652e 696e  index, mstype.in
-00011900: 7433 3229 0a20 2020 2020 2020 2077 6974  t32).        wit
-00011910: 6869 6e5f 6361 7061 6369 7479 203d 2073  hin_capacity = s
-00011920: 656c 662e 7265 7368 6170 6528 7769 7468  elf.reshape(with
-00011930: 696e 5f63 6170 6163 6974 792c 2028 7769  in_capacity, (wi
-00011940: 7468 696e 5f63 6170 6163 6974 792e 7368  thin_capacity.sh
-00011950: 6170 655b 305d 2c20 746f 6b65 6e73 5f70  ape[0], tokens_p
-00011960: 6572 5f67 726f 7570 2c20 6b29 290a 2020  er_group, k)).  
-00011970: 2020 2020 2020 726f 7574 6572 5f63 6f65        router_coe
-00011980: 6666 5f72 6177 203d 2073 656c 662e 6d75  ff_raw = self.mu
-00011990: 6c5f 3364 2865 7870 6572 745f 6761 7465  l_3d(expert_gate
-000119a0: 2c20 7769 7468 696e 5f63 6170 6163 6974  , within_capacit
-000119b0: 7929 0a20 2020 2020 2020 2072 6f75 7465  y).        route
-000119c0: 725f 636f 6566 6620 3d20 7365 6c66 2e5f  r_coeff = self._
-000119d0: 6e6f 726d 616c 697a 6528 726f 7574 6572  normalize(router
-000119e0: 5f63 6f65 6666 5f72 6177 2920 2320 2864  _coeff_raw) # (d
-000119f0: 702c 204e 2c20 6b29 203c 2d2d 2028 6470  p, N, k) <-- (dp
-00011a00: 2c20 4e2c 206b 290a 2020 2020 2020 2020  , N, k).        
-00011a10: 7265 7475 726e 2064 6973 7061 7463 685f  return dispatch_
-00011a20: 696e 6465 782c 2063 6f6d 6269 6e65 5f69  index, combine_i
-00011a30: 6e64 6578 2c20 726f 7574 6572 5f63 6f65  ndex, router_coe
-00011a40: 6666 2023 2028 6470 2c20 452c 206e 292c  ff # (dp, E, n),
-00011a50: 2028 6470 2c20 4e2c 206b 292c 2028 6470   (dp, N, k), (dp
-00011a60: 2c20 4e2c 206b 290a 0a20 2020 2064 6566  , N, k)..    def
-00011a70: 205f 6e6f 726d 616c 697a 6528 7365 6c66   _normalize(self
-00011a80: 2c20 726f 7574 6572 5f63 6f65 6666 5f72  , router_coeff_r
-00011a90: 6177 293a 0a20 2020 2020 2020 2072 6f75  aw):.        rou
-00011aa0: 7465 725f 636f 6566 665f 7375 6d20 3d20  ter_coeff_sum = 
-00011ab0: 7365 6c66 2e72 6564 7563 655f 7375 6d5f  self.reduce_sum_
-00011ac0: 6b65 6570 2872 6f75 7465 725f 636f 6566  keep(router_coef
-00011ad0: 665f 7261 772c 2032 2920 2320 2864 702c  f_raw, 2) # (dp,
-00011ae0: 204e 2c20 3129 203c 2d2d 2028 6470 2c20   N, 1) <-- (dp, 
-00011af0: 4e2c 206b 290a 2020 2020 2020 2020 726f  N, k).        ro
-00011b00: 7574 6572 5f63 6f65 6666 203d 2073 656c  uter_coeff = sel
-00011b10: 662e 6469 765f 3364 2872 6f75 7465 725f  f.div_3d(router_
-00011b20: 636f 6566 665f 7261 772c 2073 656c 662e  coeff_raw, self.
-00011b30: 6164 645f 6570 7328 726f 7574 6572 5f63  add_eps(router_c
-00011b40: 6f65 6666 5f73 756d 2c20 3165 2d39 2929  oeff_sum, 1e-9))
-00011b50: 2023 2028 6470 2c20 4e2c 206b 2920 3c2d   # (dp, N, k) <-
-00011b60: 2d20 2864 702c 204e 2c20 6b29 2028 6470  - (dp, N, k) (dp
-00011b70: 2c20 4e2c 2031 290a 2020 2020 2020 2020  , N, 1).        
-00011b80: 7265 7475 726e 2072 6f75 7465 725f 636f  return router_co
-00011b90: 6566 6620 2320 2864 702c 204e 2c20 6b29  eff # (dp, N, k)
-00011ba0: 0a                                       .
+00009850: 2020 2020 2070 6172 616c 6c65 6c5f 636f       parallel_co
+00009860: 6e66 6967 3d70 6172 616c 6c65 6c5f 636f  nfig=parallel_co
+00009870: 6e66 6967 290a 2020 2020 2020 2020 656c  nfig).        el
+00009880: 7365 3a0a 2020 2020 2020 2020 2020 2020  se:.            
+00009890: 7365 6c66 2e72 6f75 7465 7220 3d20 726f  self.router = ro
+000098a0: 7574 696e 675f 706f 6c69 6379 0a0a 2020  uting_policy..  
+000098b0: 2020 2020 2020 6966 206e 6f74 2028 5f67        if not (_g
+000098c0: 6574 5f70 6172 616c 6c65 6c5f 6d6f 6465  et_parallel_mode
+000098d0: 2829 2069 6e20 2850 6172 616c 6c65 6c4d  () in (ParallelM
+000098e0: 6f64 652e 4155 544f 5f50 4152 414c 4c45  ode.AUTO_PARALLE
+000098f0: 4c2c 2920 616e 6420 5f69 735f 7368 6172  L,) and _is_shar
+00009900: 6469 6e67 5f70 726f 7061 6761 7469 6f6e  ding_propagation
+00009910: 2829 293a 0a20 2020 2020 2020 2020 2020  ()):.           
+00009920: 2073 656c 662e 6d75 6c2e 7368 6172 6428   self.mul.shard(
+00009930: 2828 6470 2c20 312c 2031 292c 2028 6470  ((dp, 1, 1), (dp
+00009940: 2c29 2929 0a0a 2020 2020 6465 6620 636f  ,)))..    def co
+00009950: 6e73 7472 7563 7428 7365 6c66 2c20 696e  nstruct(self, in
+00009960: 7075 745f 7465 6e73 6f72 293a 0a20 2020  put_tensor):.   
+00009970: 2020 2020 2069 6e70 7574 5f74 656e 736f       input_tenso
+00009980: 7220 3d20 7365 6c66 2e63 6173 7428 696e  r = self.cast(in
+00009990: 7075 745f 7465 6e73 6f72 2c20 6d73 7479  put_tensor, msty
+000099a0: 7065 2e66 6c6f 6174 3332 290a 2020 2020  pe.float32).    
+000099b0: 2020 2020 6966 2073 656c 662e 6e6f 6973      if self.nois
+000099c0: 795f 706f 6c69 6379 203d 3d20 226a 6974  y_policy == "jit
+000099d0: 7465 7222 2061 6e64 2073 656c 662e 7472  ter" and self.tr
+000099e0: 6169 6e69 6e67 3a0a 2020 2020 2020 2020  aining:.        
+000099f0: 2020 2020 2320 4865 7265 2c20 7765 2074      # Here, we t
+00009a00: 656d 706f 7261 7269 6c79 2069 6d70 6c65  emporarily imple
+00009a10: 6d65 6e74 2074 6865 206d 756c 7469 706c  ment the multipl
+00009a20: 6963 6174 6976 6520 6a69 7474 6572 2074  icative jitter t
+00009a30: 6869 7320 7761 792c 0a20 2020 2020 2020  his way,.       
+00009a40: 2020 2020 2023 2066 6f72 2074 6865 206c       # for the l
+00009a50: 6163 6b20 6f66 2055 6e69 666f 7252 6561  ack of UniforRea
+00009a60: 6c20 7061 7261 6c6c 656c 206f 7065 7261  l parallel opera
+00009a70: 746f 722e 0a20 2020 2020 2020 2020 2020  tor..           
+00009a80: 2069 6e70 7574 5f74 656e 736f 7220 3d20   input_tensor = 
+00009a90: 7365 6c66 2e6d 756c 2869 6e70 7574 5f74  self.mul(input_t
+00009aa0: 656e 736f 722c 2073 656c 662e 6e6f 6973  ensor, self.nois
+00009ab0: 6529 0a0a 2020 2020 2020 2020 726f 7574  e)..        rout
+00009ac0: 6572 5f6c 6f67 6974 7320 3d20 7365 6c66  er_logits = self
+00009ad0: 2e64 656e 7365 2869 6e70 7574 5f74 656e  .dense(input_ten
+00009ae0: 736f 7229 0a20 2020 2020 2020 2072 6574  sor).        ret
+00009af0: 7572 6e20 7365 6c66 2e72 6f75 7465 7228  urn self.router(
+00009b00: 726f 7574 6572 5f6c 6f67 6974 7329 0a0a  router_logits)..
+00009b10: 0a63 6c61 7373 2054 6f70 6b52 6f75 7465  .class TopkRoute
+00009b20: 7228 4365 6c6c 293a 0a20 2020 2072 2222  r(Cell):.    r""
+00009b30: 220a 2020 2020 2020 2020 4120 726f 7574  ".        A rout
+00009b40: 6572 2069 6d70 6c65 6d65 6e74 6174 696f  er implementatio
+00009b50: 6e20 7768 6963 6820 6d61 7073 2065 6163  n which maps eac
+00009b60: 6820 746f 6b65 6e73 2074 6f20 7468 6520  h tokens to the 
+00009b70: 746f 706b 2065 7870 6572 742e 0a0a 2020  topk expert...  
+00009b80: 2020 2020 2020 4172 6773 3a0a 2020 2020        Args:.    
+00009b90: 2020 2020 2020 2020 645f 6d6f 6465 6c20          d_model 
+00009ba0: 2869 6e74 293a 2054 6865 2068 6964 6465  (int): The hidde
+00009bb0: 6e20 7369 7a65 206f 6620 6561 6368 2074  n size of each t
+00009bc0: 6f6b 656e 2e0a 2020 2020 2020 2020 2020  oken..          
+00009bd0: 2020 6d6f 655f 636f 6e66 6967 284d 6f45    moe_config(MoE
+00009be0: 436f 6e66 6967 293a 2054 6865 2063 6f6e  Config): The con
+00009bf0: 6669 6775 7261 7469 6f6e 206f 6620 4d6f  figuration of Mo
+00009c00: 4520 284d 6978 7475 7265 206f 6620 4578  E (Mixture of Ex
+00009c10: 7065 7274 292e 0a20 2020 2020 2020 2020  pert)..         
+00009c20: 2020 2074 7261 696e 696e 6720 2862 6f6f     training (boo
+00009c30: 6c29 3a20 5468 6520 7661 6c75 6520 696e  l): The value in
+00009c40: 6469 6361 7469 6e67 2077 6865 7468 6572  dicating whether
+00009c50: 2069 7320 696e 2074 7261 696e 696e 6720   is in training 
+00009c60: 7068 6173 652e 0a20 2020 2020 2020 2020  phase..         
+00009c70: 2020 2063 6f6e 6669 673a 2054 6865 2070     config: The p
+00009c80: 6172 616c 6c65 6c2d 7265 6c61 7465 6420  arallel-related 
+00009c90: 636f 6e66 6967 7572 6174 696f 6e2e 0a20  configuration.. 
+00009ca0: 2020 2020 2020 2049 6e70 7574 733a 0a20         Inputs:. 
+00009cb0: 2020 2020 2020 2020 2020 202d 202a 2a69             - **i
+00009cc0: 6e70 7574 5f74 656e 736f 722a 2a20 2854  nput_tensor** (T
+00009cd0: 656e 736f 7229 202d 2054 656e 736f 7220  ensor) - Tensor 
+00009ce0: 6f66 2073 6861 7065 203a 6d61 7468 3a60  of shape :math:`
+00009cf0: 2865 7870 6572 745c 5f70 6172 616c 6c65  (expert\_paralle
+00009d00: 6c2c 2074 6f6b 656e 735c 5f70 6572 5c5f  l, tokens\_per\_
+00009d10: 6465 7669 6365 2c0a 2020 2020 2020 2020  device,.        
+00009d20: 2020 2020 6869 6464 656e 5c5f 7369 7a65      hidden\_size
+00009d30: 2960 2e0a 0a20 2020 2020 2020 204f 7574  )`...        Out
+00009d40: 7075 7473 3a0a 2020 2020 2020 2020 2020  puts:.          
+00009d50: 2020 5465 6e73 6f72 206f 6620 7368 6170    Tensor of shap
+00009d60: 6520 3a6d 6174 683a 6028 6578 7065 7274  e :math:`(expert
+00009d70: 5c5f 7061 7261 6c6c 656c 2c20 746f 6b65  \_parallel, toke
+00009d80: 6e73 5c5f 7065 725c 5f64 6576 6963 652c  ns\_per\_device,
+00009d90: 2065 7870 6572 745c 5f64 696d 2c20 6578   expert\_dim, ex
+00009da0: 7065 7274 5c5f 6361 7061 6369 7479 2960  pert\_capacity)`
+00009db0: 2c0a 2020 2020 2020 2020 2020 2020 5465  ,.            Te
+00009dc0: 6e73 6f72 206f 6620 7368 6170 6520 3a6d  nsor of shape :m
+00009dd0: 6174 683a 6028 6578 7065 7274 5c5f 7061  ath:`(expert\_pa
+00009de0: 7261 6c6c 656c 2c20 746f 6b65 6e73 5c5f  rallel, tokens\_
+00009df0: 7065 725c 5f64 6576 6963 652c 2065 7870  per\_device, exp
+00009e00: 6572 745c 5f64 696d 2c20 6578 7065 7274  ert\_dim, expert
+00009e10: 5c5f 6361 7061 6369 7479 2960 2c0a 2020  \_capacity)`,.  
+00009e20: 2020 2020 2020 2020 2020 5465 6e73 6f72            Tensor
+00009e30: 206f 6620 7368 6170 6520 3a6d 6174 683a   of shape :math:
+00009e40: 6028 3129 602e 0a20 2020 2022 2222 0a0a  `(1)`..    """..
+00009e50: 2020 2020 6465 6620 5f5f 696e 6974 5f5f      def __init__
+00009e60: 2873 656c 662c 0a20 2020 2020 2020 2020  (self,.         
+00009e70: 2020 2020 2020 2020 645f 6d6f 6465 6c2c          d_model,
+00009e80: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00009e90: 2020 6d6f 655f 636f 6e66 6967 2c0a 2020    moe_config,.  
+00009ea0: 2020 2020 2020 2020 2020 2020 2020 2074                 t
+00009eb0: 7261 696e 696e 673d 5472 7565 2c0a 2020  raining=True,.  
+00009ec0: 2020 2020 2020 2020 2020 2020 2020 2070                 p
+00009ed0: 6172 616c 6c65 6c5f 636f 6e66 6967 3d4e  arallel_config=N
+00009ee0: 6f6e 6529 3a0a 2020 2020 2020 2020 7375  one):.        su
+00009ef0: 7065 7228 546f 706b 526f 7574 6572 2c20  per(TopkRouter, 
+00009f00: 7365 6c66 292e 5f5f 696e 6974 5f5f 2829  self).__init__()
+00009f10: 0a20 2020 2020 2020 2069 6620 5f67 6574  .        if _get
+00009f20: 5f70 6172 616c 6c65 6c5f 6d6f 6465 2829  _parallel_mode()
+00009f30: 2069 6e20 2850 6172 616c 6c65 6c4d 6f64   in (ParallelMod
+00009f40: 652e 4155 544f 5f50 4152 414c 4c45 4c2c  e.AUTO_PARALLEL,
+00009f50: 2920 616e 6420 5f69 735f 7368 6172 6469  ) and _is_shardi
+00009f60: 6e67 5f70 726f 7061 6761 7469 6f6e 2829  ng_propagation()
+00009f70: 3a0a 2020 2020 2020 2020 2020 2020 6470  :.            dp
+00009f80: 203d 2070 6172 616c 6c65 6c5f 636f 6e66   = parallel_conf
+00009f90: 6967 2e64 6174 615f 7061 7261 6c6c 656c  ig.data_parallel
+00009fa0: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
+00009fb0: 662e 645f 6d6f 6465 6c20 3d20 645f 6d6f  f.d_model = d_mo
+00009fc0: 6465 6c0a 2020 2020 2020 2020 2020 2020  del.            
+00009fd0: 7365 6c66 2e65 7870 6572 745f 6469 6d20  self.expert_dim 
+00009fe0: 3d20 6d6f 655f 636f 6e66 6967 2e65 7870  = moe_config.exp
+00009ff0: 6572 745f 6e75 6d0a 2020 2020 2020 2020  ert_num.        
+0000a000: 2020 2020 7365 6c66 2e63 6170 6163 6974      self.capacit
+0000a010: 795f 6661 6374 6f72 203d 206d 6f65 5f63  y_factor = moe_c
+0000a020: 6f6e 6669 672e 6361 7061 6369 7479 5f66  onfig.capacity_f
+0000a030: 6163 746f 720a 2020 2020 2020 2020 2020  actor.          
+0000a040: 2020 7365 6c66 2e74 7261 696e 696e 6720    self.training 
+0000a050: 3d20 7472 6169 6e69 6e67 0a20 2020 2020  = training.     
+0000a060: 2020 2020 2020 2073 656c 662e 6470 5f67         self.dp_g
+0000a070: 726f 7570 203d 2064 700a 2020 2020 2020  roup = dp.      
+0000a080: 2020 2020 2020 7365 6c66 2e6e 6f69 7379        self.noisy
+0000a090: 5f70 6f6c 6963 7920 3d20 4e6f 6e65 0a20  _policy = None. 
+0000a0a0: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+0000a0b0: 6361 7374 203d 2050 2e43 6173 7428 290a  cast = P.Cast().
+0000a0c0: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+0000a0d0: 2e72 6573 6861 7065 203d 2050 2e52 6573  .reshape = P.Res
+0000a0e0: 6861 7065 2829 0a20 2020 2020 2020 2020  hape().         
+0000a0f0: 2020 2073 656c 662e 7368 6170 6520 3d20     self.shape = 
+0000a100: 502e 5368 6170 6528 290a 2020 2020 2020  P.Shape().      
+0000a110: 2020 2020 2020 7365 6c66 2e73 6f66 746d        self.softm
+0000a120: 6178 203d 2050 2e53 6f66 746d 6178 2861  ax = P.Softmax(a
+0000a130: 7869 733d 2d31 290a 2020 2020 2020 2020  xis=-1).        
+0000a140: 2020 2020 7365 6c66 2e61 7267 6d61 7820      self.argmax 
+0000a150: 3d20 502e 4172 674d 6178 5769 7468 5661  = P.ArgMaxWithVa
+0000a160: 6c75 6528 6178 6973 3d2d 312c 206b 6565  lue(axis=-1, kee
+0000a170: 705f 6469 6d73 3d46 616c 7365 290a 2020  p_dims=False).  
+0000a180: 2020 2020 2020 2020 2020 7365 6c66 2e6e            self.n
+0000a190: 756d 5f65 7870 6572 7473 5f63 686f 7365  um_experts_chose
+0000a1a0: 6e20 3d20 6d6f 655f 636f 6e66 6967 2e6e  n = moe_config.n
+0000a1b0: 756d 5f65 7870 6572 7473 5f63 686f 7365  um_experts_chose
+0000a1c0: 6e0a 2020 2020 2020 2020 2020 2020 7365  n.            se
+0000a1d0: 6c66 2e6f 6e65 686f 7420 3d20 502e 4f6e  lf.onehot = P.On
+0000a1e0: 6548 6f74 2829 0a20 2020 2020 2020 2020  eHot().         
+0000a1f0: 2020 2073 656c 662e 6f6e 6568 6f74 3220     self.onehot2 
+0000a200: 3d20 502e 4f6e 6548 6f74 2829 0a20 2020  = P.OneHot().   
+0000a210: 2020 2020 2020 2020 2073 656c 662e 6f6e           self.on
+0000a220: 6568 6f74 3320 3d20 502e 4f6e 6548 6f74  ehot3 = P.OneHot
+0000a230: 2829 0a20 2020 2020 2020 2020 2020 2073  ().            s
+0000a240: 656c 662e 6f6e 5f76 616c 7565 203d 2054  elf.on_value = T
+0000a250: 656e 736f 7228 312e 302c 206d 7374 7970  ensor(1.0, mstyp
+0000a260: 652e 666c 6f61 7433 3229 0a20 2020 2020  e.float32).     
+0000a270: 2020 2020 2020 2073 656c 662e 6f66 665f         self.off_
+0000a280: 7661 6c75 6520 3d20 5465 6e73 6f72 2830  value = Tensor(0
+0000a290: 2e30 2c20 6d73 7479 7065 2e66 6c6f 6174  .0, mstype.float
+0000a2a0: 3332 290a 0a20 2020 2020 2020 2020 2020  32)..           
+0000a2b0: 2073 656c 662e 7265 6475 6365 5f6d 6561   self.reduce_mea
+0000a2c0: 6e20 3d20 502e 5265 6475 6365 4d65 616e  n = P.ReduceMean
+0000a2d0: 286b 6565 705f 6469 6d73 3d46 616c 7365  (keep_dims=False
+0000a2e0: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
+0000a2f0: 6c66 2e72 6564 7563 655f 6d65 616e 3220  lf.reduce_mean2 
+0000a300: 3d20 502e 5265 6475 6365 4d65 616e 286b  = P.ReduceMean(k
+0000a310: 6565 705f 6469 6d73 3d46 616c 7365 290a  eep_dims=False).
+0000a320: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+0000a330: 2e72 6564 7563 655f 6d65 616e 3320 3d20  .reduce_mean3 = 
+0000a340: 502e 5265 6475 6365 4d65 616e 286b 6565  P.ReduceMean(kee
+0000a350: 705f 6469 6d73 3d46 616c 7365 290a 2020  p_dims=False).  
+0000a360: 2020 2020 2020 2020 2020 7365 6c66 2e6d            self.m
+0000a370: 756c 203d 2050 2e4d 756c 2829 0a20 2020  ul = P.Mul().   
+0000a380: 2020 2020 2020 2020 2073 656c 662e 6d75           self.mu
+0000a390: 6c32 203d 2050 2e4d 756c 2829 0a20 2020  l2 = P.Mul().   
+0000a3a0: 2020 2020 2020 2020 2073 656c 662e 6d75           self.mu
+0000a3b0: 6c33 203d 2050 2e4d 756c 2829 0a20 2020  l3 = P.Mul().   
+0000a3c0: 2020 2020 2020 2020 2073 656c 662e 6d75           self.mu
+0000a3d0: 6c34 203d 2050 2e4d 756c 2829 0a20 2020  l4 = P.Mul().   
+0000a3e0: 2020 2020 2020 2020 2073 656c 662e 6d75           self.mu
+0000a3f0: 6c35 203d 2050 2e4d 756c 2829 0a20 2020  l5 = P.Mul().   
+0000a400: 2020 2020 2020 2020 2073 656c 662e 6d75           self.mu
+0000a410: 6c36 203d 2050 2e4d 756c 2829 0a20 2020  l6 = P.Mul().   
+0000a420: 2020 2020 2020 2020 2073 656c 662e 6d75           self.mu
+0000a430: 6c37 203d 2050 2e4d 756c 2829 0a20 2020  l7 = P.Mul().   
+0000a440: 2020 2020 2020 2020 2073 656c 662e 6d75           self.mu
+0000a450: 6c38 203d 2050 2e4d 756c 2829 2e73 6861  l8 = P.Mul().sha
+0000a460: 7264 2828 2864 702c 2031 2c20 3129 2c20  rd(((dp, 1, 1), 
+0000a470: 2864 702c 2031 2c20 3129 2929 0a20 2020  (dp, 1, 1))).   
+0000a480: 2020 2020 2020 2020 2073 656c 662e 6d75           self.mu
+0000a490: 6c39 203d 2050 2e4d 756c 2829 2e73 6861  l9 = P.Mul().sha
+0000a4a0: 7264 2828 2864 702c 2031 2c20 312c 2031  rd(((dp, 1, 1, 1
+0000a4b0: 292c 2028 6470 2c20 312c 2031 2c20 3129  ), (dp, 1, 1, 1)
+0000a4c0: 2929 0a20 2020 2020 2020 2020 2020 2073  )).            s
+0000a4d0: 656c 662e 6e6f 745f 6571 7561 6c20 3d20  elf.not_equal = 
+0000a4e0: 502e 4e6f 7445 7175 616c 2829 0a20 2020  P.NotEqual().   
+0000a4f0: 2020 2020 2020 2020 2073 656c 662e 6469           self.di
+0000a500: 7631 203d 2050 2e52 6561 6c44 6976 2829  v1 = P.RealDiv()
+0000a510: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
+0000a520: 662e 6469 7632 203d 2050 2e52 6561 6c44  f.div2 = P.RealD
+0000a530: 6976 2829 0a20 2020 2020 2020 2020 2020  iv().           
+0000a540: 2073 656c 662e 6164 6420 3d20 502e 4164   self.add = P.Ad
+0000a550: 6428 290a 2020 2020 2020 2020 2020 2020  d().            
+0000a560: 7365 6c66 2e61 6464 3120 3d20 502e 4164  self.add1 = P.Ad
+0000a570: 6428 290a 2020 2020 2020 2020 2020 2020  d().            
+0000a580: 7365 6c66 2e61 6464 3220 3d20 502e 4164  self.add2 = P.Ad
+0000a590: 6428 290a 2020 2020 2020 2020 2020 2020  d().            
+0000a5a0: 7365 6c66 2e61 6464 3320 3d20 502e 4164  self.add3 = P.Ad
+0000a5b0: 6428 290a 2020 2020 2020 2020 2020 2020  d().            
+0000a5c0: 7365 6c66 2e61 6464 3420 3d20 502e 4164  self.add4 = P.Ad
+0000a5d0: 6428 290a 2020 2020 2020 2020 2020 2020  d().            
+0000a5e0: 7365 6c66 2e73 7562 203d 2050 2e53 7562  self.sub = P.Sub
+0000a5f0: 2829 0a0a 2020 2020 2020 2020 2020 2020  ()..            
+0000a600: 7365 6c66 2e63 756d 7375 6d20 3d20 502e  self.cumsum = P.
+0000a610: 4375 6d53 756d 2865 7863 6c75 7369 7665  CumSum(exclusive
+0000a620: 3d54 7275 6529 0a20 2020 2020 2020 2020  =True).         
+0000a630: 2020 2073 656c 662e 6c65 7373 203d 2050     self.less = P
+0000a640: 2e4c 6573 7328 290a 2020 2020 2020 2020  .Less().        
+0000a650: 2020 2020 7365 6c66 2e72 6564 7563 655f      self.reduce_
+0000a660: 7375 6d20 3d20 502e 5265 6475 6365 5375  sum = P.ReduceSu
+0000a670: 6d28 6b65 6570 5f64 696d 733d 4661 6c73  m(keep_dims=Fals
+0000a680: 6529 0a20 2020 2020 2020 2020 2020 2073  e).            s
+0000a690: 656c 662e 7265 6475 6365 5f73 756d 5f6b  elf.reduce_sum_k
+0000a6a0: 6565 7020 3d20 502e 5265 6475 6365 5375  eep = P.ReduceSu
+0000a6b0: 6d28 6b65 6570 5f64 696d 733d 5472 7565  m(keep_dims=True
+0000a6c0: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
+0000a6d0: 6c66 2e72 6564 7563 655f 7375 6d5f 6b65  lf.reduce_sum_ke
+0000a6e0: 6570 3220 3d20 502e 5265 6475 6365 5375  ep2 = P.ReduceSu
+0000a6f0: 6d28 6b65 6570 5f64 696d 733d 5472 7565  m(keep_dims=True
+0000a700: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
+0000a710: 6c66 2e65 7870 616e 6420 3d20 502e 4578  lf.expand = P.Ex
+0000a720: 7061 6e64 4469 6d73 2829 0a20 2020 2020  pandDims().     
+0000a730: 2020 2020 2020 2073 656c 662e 6578 7061         self.expa
+0000a740: 6e64 3220 3d20 502e 4578 7061 6e64 4469  nd2 = P.ExpandDi
+0000a750: 6d73 2829 0a20 2020 2020 2020 2020 2020  ms().           
+0000a760: 2073 656c 662e 6164 645f 7363 616c 6120   self.add_scala 
+0000a770: 3d20 502e 4164 6428 290a 2020 2020 2020  = P.Add().      
+0000a780: 2020 2020 2020 7365 6c66 2e69 6e69 745f        self.init_
+0000a790: 6c6f 7373 203d 2054 656e 736f 7228 302e  loss = Tensor(0.
+0000a7a0: 302c 206d 7374 7970 652e 666c 6f61 7433  0, mstype.float3
+0000a7b0: 3229 0a20 2020 2020 2020 2065 6c73 653a  2).        else:
+0000a7c0: 0a20 2020 2020 2020 2020 2020 2064 7020  .            dp 
+0000a7d0: 3d20 7061 7261 6c6c 656c 5f63 6f6e 6669  = parallel_confi
+0000a7e0: 672e 6461 7461 5f70 6172 616c 6c65 6c0a  g.data_parallel.
+0000a7f0: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+0000a800: 2e64 5f6d 6f64 656c 203d 2064 5f6d 6f64  .d_model = d_mod
+0000a810: 656c 0a20 2020 2020 2020 2020 2020 2073  el.            s
+0000a820: 656c 662e 6578 7065 7274 5f64 696d 203d  elf.expert_dim =
+0000a830: 206d 6f65 5f63 6f6e 6669 672e 6578 7065   moe_config.expe
+0000a840: 7274 5f6e 756d 0a20 2020 2020 2020 2020  rt_num.         
+0000a850: 2020 2073 656c 662e 6361 7061 6369 7479     self.capacity
+0000a860: 5f66 6163 746f 7220 3d20 6d6f 655f 636f  _factor = moe_co
+0000a870: 6e66 6967 2e63 6170 6163 6974 795f 6661  nfig.capacity_fa
+0000a880: 6374 6f72 0a20 2020 2020 2020 2020 2020  ctor.           
+0000a890: 2073 656c 662e 7361 7665 5f74 6f6b 656e   self.save_token
+0000a8a0: 5f64 6973 7472 6962 7574 696f 6e20 3d20  _distribution = 
+0000a8b0: 6d6f 655f 636f 6e66 6967 2e73 6176 655f  moe_config.save_
+0000a8c0: 746f 6b65 6e5f 6469 7374 7269 6275 7469  token_distributi
+0000a8d0: 6f6e 0a20 2020 2020 2020 2020 2020 2073  on.            s
+0000a8e0: 656c 662e 656e 6162 6c65 5f63 6f6c 645f  elf.enable_cold_
+0000a8f0: 686f 745f 6578 7065 7274 203d 206d 6f65  hot_expert = moe
+0000a900: 5f63 6f6e 6669 672e 656e 6162 6c65 5f63  _config.enable_c
+0000a910: 6f6c 645f 686f 745f 6578 7065 7274 0a20  old_hot_expert. 
+0000a920: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+0000a930: 7472 6169 6e69 6e67 203d 2074 7261 696e  training = train
+0000a940: 696e 670a 2020 2020 2020 2020 2020 2020  ing.            
+0000a950: 7365 6c66 2e64 705f 6772 6f75 7020 3d20  self.dp_group = 
+0000a960: 6470 0a20 2020 2020 2020 2020 2020 2073  dp.            s
+0000a970: 656c 662e 6e6f 6973 795f 706f 6c69 6379  elf.noisy_policy
+0000a980: 203d 204e 6f6e 650a 2020 2020 2020 2020   = None.        
+0000a990: 2020 2020 7365 6c66 2e63 6173 7420 3d20      self.cast = 
+0000a9a0: 502e 4361 7374 2829 0a20 2020 2020 2020  P.Cast().       
+0000a9b0: 2020 2020 2073 656c 662e 7265 7368 6170       self.reshap
+0000a9c0: 6520 3d20 502e 5265 7368 6170 6528 290a  e = P.Reshape().
+0000a9d0: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+0000a9e0: 2e73 6861 7065 203d 2050 2e53 6861 7065  .shape = P.Shape
+0000a9f0: 2829 0a20 2020 2020 2020 2020 2020 2073  ().            s
+0000aa00: 656c 662e 736f 6674 6d61 7820 3d20 502e  elf.softmax = P.
+0000aa10: 536f 6674 6d61 7828 6178 6973 3d2d 3129  Softmax(axis=-1)
+0000aa20: 2e73 6861 7264 2828 2864 702c 2031 2c20  .shard(((dp, 1, 
+0000aa30: 312c 292c 2929 0a20 2020 2020 2020 2020  1,),)).         
+0000aa40: 2020 2073 656c 662e 6172 676d 6178 203d     self.argmax =
+0000aa50: 2050 2e41 7267 4d61 7857 6974 6856 616c   P.ArgMaxWithVal
+0000aa60: 7565 2861 7869 733d 2d31 2c20 6b65 6570  ue(axis=-1, keep
+0000aa70: 5f64 696d 733d 4661 6c73 6529 2e73 6861  _dims=False).sha
+0000aa80: 7264 2828 2864 702c 2031 2c20 3129 2c29  rd(((dp, 1, 1),)
+0000aa90: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
+0000aaa0: 6c66 2e6e 756d 5f65 7870 6572 7473 5f63  lf.num_experts_c
+0000aab0: 686f 7365 6e20 3d20 6d6f 655f 636f 6e66  hosen = moe_conf
+0000aac0: 6967 2e6e 756d 5f65 7870 6572 7473 5f63  ig.num_experts_c
+0000aad0: 686f 7365 6e0a 2020 2020 2020 2020 2020  hosen.          
+0000aae0: 2020 7365 6c66 2e6f 6e65 686f 7420 3d20    self.onehot = 
+0000aaf0: 502e 4f6e 6548 6f74 2829 2e73 6861 7264  P.OneHot().shard
+0000ab00: 2828 2864 702c 2031 2c20 3129 2c20 2829  (((dp, 1, 1), ()
+0000ab10: 2c20 2829 2929 0a20 2020 2020 2020 2020  , ())).         
+0000ab20: 2020 2073 656c 662e 6f6e 6568 6f74 3220     self.onehot2 
+0000ab30: 3d20 502e 4f6e 6548 6f74 2829 2e73 6861  = P.OneHot().sha
+0000ab40: 7264 2828 2864 702c 2031 2c20 3129 2c20  rd(((dp, 1, 1), 
+0000ab50: 2829 2c20 2829 2929 0a20 2020 2020 2020  (), ())).       
+0000ab60: 2020 2020 2073 656c 662e 6f6e 6568 6f74       self.onehot
+0000ab70: 3320 3d20 502e 4f6e 6548 6f74 2829 2e73  3 = P.OneHot().s
+0000ab80: 6861 7264 2828 2864 702c 2031 2c20 312c  hard(((dp, 1, 1,
+0000ab90: 2031 292c 2028 292c 2028 2929 290a 2020   1), (), ())).  
+0000aba0: 2020 2020 2020 2020 2020 7365 6c66 2e6f            self.o
+0000abb0: 6e5f 7661 6c75 6520 3d20 5465 6e73 6f72  n_value = Tensor
+0000abc0: 2831 2e30 2c20 6d73 7479 7065 2e66 6c6f  (1.0, mstype.flo
+0000abd0: 6174 3332 290a 2020 2020 2020 2020 2020  at32).          
+0000abe0: 2020 7365 6c66 2e6f 6666 5f76 616c 7565    self.off_value
+0000abf0: 203d 2054 656e 736f 7228 302e 302c 206d   = Tensor(0.0, m
+0000ac00: 7374 7970 652e 666c 6f61 7433 3229 0a0a  stype.float32)..
+0000ac10: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+0000ac20: 2e72 6564 7563 655f 6d65 616e 203d 2050  .reduce_mean = P
+0000ac30: 2e52 6564 7563 654d 6561 6e28 6b65 6570  .ReduceMean(keep
+0000ac40: 5f64 696d 733d 4661 6c73 6529 2e73 6861  _dims=False).sha
+0000ac50: 7264 2828 2864 702c 2031 2c20 3129 2c29  rd(((dp, 1, 1),)
+0000ac60: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
+0000ac70: 6c66 2e72 6564 7563 655f 6d65 616e 3220  lf.reduce_mean2 
+0000ac80: 3d20 502e 5265 6475 6365 4d65 616e 286b  = P.ReduceMean(k
+0000ac90: 6565 705f 6469 6d73 3d46 616c 7365 292e  eep_dims=False).
+0000aca0: 7368 6172 6428 2828 6470 2c20 312c 2031  shard(((dp, 1, 1
+0000acb0: 292c 2929 0a20 2020 2020 2020 2020 2020  ),)).           
+0000acc0: 2073 656c 662e 7265 6475 6365 5f6d 6561   self.reduce_mea
+0000acd0: 6e33 203d 2050 2e52 6564 7563 654d 6561  n3 = P.ReduceMea
+0000ace0: 6e28 6b65 6570 5f64 696d 733d 4661 6c73  n(keep_dims=Fals
+0000acf0: 6529 2e73 6861 7264 2828 2864 702c 2031  e).shard(((dp, 1
+0000ad00: 292c 2929 0a20 2020 2020 2020 2020 2020  ),)).           
+0000ad10: 2073 656c 662e 6d75 6c20 3d20 502e 4d75   self.mul = P.Mu
+0000ad20: 6c28 292e 7368 6172 6428 2828 6470 2c20  l().shard(((dp, 
+0000ad30: 3129 2c20 2864 702c 2031 2929 290a 2020  1), (dp, 1))).  
+0000ad40: 2020 2020 2020 2020 2020 7365 6c66 2e6d            self.m
+0000ad50: 756c 3220 3d20 502e 4d75 6c28 292e 7368  ul2 = P.Mul().sh
+0000ad60: 6172 6428 2828 292c 2028 2929 290a 2020  ard(((), ())).  
+0000ad70: 2020 2020 2020 2020 2020 7365 6c66 2e6d            self.m
+0000ad80: 756c 3320 3d20 502e 4d75 6c28 292e 7368  ul3 = P.Mul().sh
+0000ad90: 6172 6428 2828 292c 2028 2929 290a 2020  ard(((), ())).  
+0000ada0: 2020 2020 2020 2020 2020 7365 6c66 2e6d            self.m
+0000adb0: 756c 3420 3d20 502e 4d75 6c28 292e 7368  ul4 = P.Mul().sh
+0000adc0: 6172 6428 2828 6470 2c20 312c 2031 292c  ard(((dp, 1, 1),
+0000add0: 2028 6470 2c20 312c 2031 2929 290a 2020   (dp, 1, 1))).  
+0000ade0: 2020 2020 2020 2020 2020 7365 6c66 2e6d            self.m
+0000adf0: 756c 3520 3d20 502e 4d75 6c28 292e 7368  ul5 = P.Mul().sh
+0000ae00: 6172 6428 2828 6470 2c20 312c 2031 292c  ard(((dp, 1, 1),
+0000ae10: 2028 6470 2c20 312c 2031 2929 290a 2020   (dp, 1, 1))).  
+0000ae20: 2020 2020 2020 2020 2020 7365 6c66 2e6d            self.m
+0000ae30: 756c 3620 3d20 502e 4d75 6c28 292e 7368  ul6 = P.Mul().sh
+0000ae40: 6172 6428 2828 6470 2c20 3129 2c20 2864  ard(((dp, 1), (d
+0000ae50: 702c 2031 2929 290a 2020 2020 2020 2020  p, 1))).        
+0000ae60: 2020 2020 7365 6c66 2e6d 756c 3720 3d20      self.mul7 = 
+0000ae70: 502e 4d75 6c28 292e 7368 6172 6428 2828  P.Mul().shard(((
+0000ae80: 6470 2c20 3129 2c20 2864 702c 2031 2929  dp, 1), (dp, 1))
+0000ae90: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
+0000aea0: 6c66 2e6d 756c 3820 3d20 502e 4d75 6c28  lf.mul8 = P.Mul(
+0000aeb0: 292e 7368 6172 6428 2828 6470 2c20 312c  ).shard(((dp, 1,
+0000aec0: 2031 292c 2028 6470 2c20 312c 2031 2929   1), (dp, 1, 1))
+0000aed0: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
+0000aee0: 6c66 2e6d 756c 3920 3d20 502e 4d75 6c28  lf.mul9 = P.Mul(
+0000aef0: 292e 7368 6172 6428 2828 6470 2c20 312c  ).shard(((dp, 1,
+0000af00: 2031 2c20 3129 2c20 2864 702c 2031 2c20   1, 1), (dp, 1, 
+0000af10: 312c 2031 2929 290a 2020 2020 2020 2020  1, 1))).        
+0000af20: 2020 2020 7365 6c66 2e6e 6f74 5f65 7175      self.not_equ
+0000af30: 616c 203d 2050 2e4e 6f74 4571 7561 6c28  al = P.NotEqual(
+0000af40: 292e 7368 6172 6428 2828 6470 2c20 312c  ).shard(((dp, 1,
+0000af50: 2031 2c20 3129 2c20 2829 2929 0a20 2020   1, 1), ())).   
+0000af60: 2020 2020 2020 2020 2073 656c 662e 6469           self.di
+0000af70: 7631 203d 2050 2e52 6561 6c44 6976 2829  v1 = P.RealDiv()
+0000af80: 2e73 6861 7264 2828 2864 702c 2031 2c20  .shard(((dp, 1, 
+0000af90: 3129 2c20 2864 702c 2031 2c20 3129 2929  1), (dp, 1, 1)))
+0000afa0: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
+0000afb0: 662e 6469 7632 203d 2050 2e52 6561 6c44  f.div2 = P.RealD
+0000afc0: 6976 2829 2e73 6861 7264 2828 2864 702c  iv().shard(((dp,
+0000afd0: 2031 2c20 312c 2031 292c 2028 6470 2c20   1, 1, 1), (dp, 
+0000afe0: 312c 2031 2c20 3129 2929 0a20 2020 2020  1, 1, 1))).     
+0000aff0: 2020 2020 2020 2073 656c 662e 6164 6420         self.add 
+0000b000: 3d20 502e 4164 6428 292e 7368 6172 6428  = P.Add().shard(
+0000b010: 2828 6470 2c20 312c 2031 292c 2028 6470  ((dp, 1, 1), (dp
+0000b020: 2c20 312c 2031 2929 290a 2020 2020 2020  , 1, 1))).      
+0000b030: 2020 2020 2020 7365 6c66 2e61 6464 3120        self.add1 
+0000b040: 3d20 502e 4164 6428 292e 7368 6172 6428  = P.Add().shard(
+0000b050: 2828 6470 2c20 312c 2031 292c 2028 2929  ((dp, 1, 1), ())
+0000b060: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
+0000b070: 6c66 2e61 6464 3220 3d20 502e 4164 6428  lf.add2 = P.Add(
+0000b080: 292e 7368 6172 6428 2828 6470 2c20 312c  ).shard(((dp, 1,
+0000b090: 2031 2c20 3129 2c20 2864 702c 2031 2c20   1, 1), (dp, 1, 
+0000b0a0: 312c 2031 2929 290a 2020 2020 2020 2020  1, 1))).        
+0000b0b0: 2020 2020 7365 6c66 2e61 6464 3320 3d20      self.add3 = 
+0000b0c0: 502e 4164 6428 292e 7368 6172 6428 2828  P.Add().shard(((
+0000b0d0: 6470 2c20 3129 2c20 2864 702c 2031 2929  dp, 1), (dp, 1))
+0000b0e0: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
+0000b0f0: 6c66 2e61 6464 3420 3d20 502e 4164 6428  lf.add4 = P.Add(
+0000b100: 292e 7368 6172 6428 2828 6470 2c20 312c  ).shard(((dp, 1,
+0000b110: 2031 2c20 3129 2c20 2829 2929 0a20 2020   1, 1), ())).   
+0000b120: 2020 2020 2020 2020 2073 656c 662e 7375           self.su
+0000b130: 6220 3d20 502e 5375 6228 292e 7368 6172  b = P.Sub().shar
+0000b140: 6428 2828 292c 2028 6470 2c20 312c 2031  d(((), (dp, 1, 1
+0000b150: 2929 290a 0a20 2020 2020 2020 2020 2020  )))..           
+0000b160: 2073 656c 662e 6375 6d73 756d 203d 2050   self.cumsum = P
+0000b170: 2e43 756d 5375 6d28 6578 636c 7573 6976  .CumSum(exclusiv
+0000b180: 653d 5472 7565 292e 7368 6172 6428 2828  e=True).shard(((
+0000b190: 6470 2c20 312c 2031 292c 2929 0a20 2020  dp, 1, 1),)).   
+0000b1a0: 2020 2020 2020 2020 2073 656c 662e 6c65           self.le
+0000b1b0: 7373 203d 2050 2e4c 6573 7328 292e 7368  ss = P.Less().sh
+0000b1c0: 6172 6428 2828 6470 2c20 312c 2031 292c  ard(((dp, 1, 1),
+0000b1d0: 2028 2929 290a 2020 2020 2020 2020 2020   ())).          
+0000b1e0: 2020 7365 6c66 2e72 6564 7563 655f 7375    self.reduce_su
+0000b1f0: 6d20 3d20 502e 5265 6475 6365 5375 6d28  m = P.ReduceSum(
+0000b200: 6b65 6570 5f64 696d 733d 4661 6c73 6529  keep_dims=False)
+0000b210: 2e73 6861 7264 2828 2864 702c 2031 2c20  .shard(((dp, 1, 
+0000b220: 3129 2c29 290a 2020 2020 2020 2020 2020  1),)).          
+0000b230: 2020 7365 6c66 2e72 6564 7563 655f 7375    self.reduce_su
+0000b240: 6d5f 6b65 6570 203d 2050 2e52 6564 7563  m_keep = P.Reduc
+0000b250: 6553 756d 286b 6565 705f 6469 6d73 3d54  eSum(keep_dims=T
+0000b260: 7275 6529 2e73 6861 7264 2828 2864 702c  rue).shard(((dp,
+0000b270: 2031 2c20 3129 2c29 290a 2020 2020 2020   1, 1),)).      
+0000b280: 2020 2020 2020 7365 6c66 2e72 6564 7563        self.reduc
+0000b290: 655f 7375 6d5f 6b65 6570 3220 3d20 502e  e_sum_keep2 = P.
+0000b2a0: 5265 6475 6365 5375 6d28 6b65 6570 5f64  ReduceSum(keep_d
+0000b2b0: 696d 733d 5472 7565 292e 7368 6172 6428  ims=True).shard(
+0000b2c0: 2828 6470 2c20 312c 2031 2c20 3129 2c29  ((dp, 1, 1, 1),)
+0000b2d0: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
+0000b2e0: 6c66 2e65 7870 616e 6420 3d20 502e 4578  lf.expand = P.Ex
+0000b2f0: 7061 6e64 4469 6d73 2829 2e73 6861 7264  pandDims().shard
+0000b300: 2828 2864 702c 2031 292c 2929 0a20 2020  (((dp, 1),)).   
+0000b310: 2020 2020 2020 2020 2073 656c 662e 6578           self.ex
+0000b320: 7061 6e64 3220 3d20 502e 4578 7061 6e64  pand2 = P.Expand
+0000b330: 4469 6d73 2829 2e73 6861 7264 2828 2864  Dims().shard(((d
+0000b340: 702c 2031 2c20 3129 2c29 290a 2020 2020  p, 1, 1),)).    
+0000b350: 2020 2020 2020 2020 7365 6c66 2e61 6464          self.add
+0000b360: 5f73 6361 6c61 203d 2050 2e41 6464 2829  _scala = P.Add()
+0000b370: 2e73 6861 7264 2828 2829 2c20 2829 2929  .shard(((), ()))
+0000b380: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
+0000b390: 662e 696e 6974 5f6c 6f73 7320 3d20 5465  f.init_loss = Te
+0000b3a0: 6e73 6f72 2830 2e30 2c20 6d73 7479 7065  nsor(0.0, mstype
+0000b3b0: 2e66 6c6f 6174 3332 290a 2020 2020 2020  .float32).      
+0000b3c0: 2020 2020 2020 6966 2073 656c 662e 7361        if self.sa
+0000b3d0: 7665 5f74 6f6b 656e 5f64 6973 7472 6962  ve_token_distrib
+0000b3e0: 7574 696f 6e3a 0a20 2020 2020 2020 2020  ution:.         
+0000b3f0: 2020 2020 2020 2073 656c 662e 6375 725f         self.cur_
+0000b400: 6c61 7965 7220 3d20 6d6f 655f 636f 6e66  layer = moe_conf
+0000b410: 6967 2e63 7572 5f6c 6179 6572 0a20 2020  ig.cur_layer.   
+0000b420: 2020 2020 2020 2020 2020 2020 2073 656c               sel
+0000b430: 662e 7465 6e73 6f72 5f73 756d 6d61 7279  f.tensor_summary
+0000b440: 203d 2050 2e54 656e 736f 7253 756d 6d61   = P.TensorSumma
+0000b450: 7279 2829 0a20 2020 2020 2020 2020 2020  ry().           
+0000b460: 2069 6620 7365 6c66 2e65 6e61 626c 655f   if self.enable_
+0000b470: 636f 6c64 5f68 6f74 5f65 7870 6572 743a  cold_hot_expert:
+0000b480: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0000b490: 2073 656c 662e 6375 725f 6c61 7965 7220   self.cur_layer 
+0000b4a0: 3d20 6d6f 655f 636f 6e66 6967 2e63 7572  = moe_config.cur
+0000b4b0: 5f6c 6179 6572 0a20 2020 2020 2020 2020  _layer.         
+0000b4c0: 2020 2020 2020 2073 656c 662e 6375 6d73         self.cums
+0000b4d0: 756d 5f76 616c 7565 203d 2050 6172 616d  um_value = Param
+0000b4e0: 6574 6572 2869 6e69 7469 616c 697a 6572  eter(initializer
+0000b4f0: 2827 7a65 726f 7327 2c20 2873 656c 662e  ('zeros', (self.
+0000b500: 6578 7065 7274 5f64 696d 2c29 2c20 6d73  expert_dim,), ms
+0000b510: 7479 7065 2e69 6e74 3332 292c 0a20 2020  type.int32),.   
+0000b520: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000b530: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000b540: 2020 2020 2020 2020 2020 206e 616d 653d             name=
+0000b550: 2263 756d 7375 6d5f 7661 6c75 6522 2b73  "cumsum_value"+s
+0000b560: 7472 2873 656c 662e 6375 725f 6c61 7965  tr(self.cur_laye
+0000b570: 7229 2c20 7265 7175 6972 6573 5f67 7261  r), requires_gra
+0000b580: 643d 4661 6c73 652c 0a20 2020 2020 2020  d=False,.       
+0000b590: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000b5a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000b5b0: 2020 2020 2020 2070 6172 616c 6c65 6c5f         parallel_
+0000b5c0: 6f70 7469 6d69 7a65 723d 4661 6c73 6529  optimizer=False)
+0000b5d0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0000b5e0: 2073 656c 662e 6173 7369 676e 203d 2050   self.assign = P
+0000b5f0: 2e41 7373 6967 6e28 292e 7368 6172 6428  .Assign().shard(
+0000b600: 2828 312c 292c 2028 312c 2929 290a 0a20  ((1,), (1,))).. 
+0000b610: 2020 2064 6566 2063 6f6e 7374 7275 6374     def construct
+0000b620: 2873 656c 662c 2072 6f75 7465 725f 6c6f  (self, router_lo
+0000b630: 6769 7473 293a 0a20 2020 2020 2020 2022  gits):.        "
+0000b640: 2222 666f 7277 6172 6420 7072 6f63 6573  ""forward proces
+0000b650: 7322 2222 0a20 2020 2020 2020 2072 6f75  s""".        rou
+0000b660: 7465 725f 6c6f 6769 7473 5f73 6861 7065  ter_logits_shape
+0000b670: 203d 2073 656c 662e 7368 6170 6528 726f   = self.shape(ro
+0000b680: 7574 6572 5f6c 6f67 6974 7329 0a20 2020  uter_logits).   
+0000b690: 2020 2020 2072 6f75 7465 725f 6c6f 6769       router_logi
+0000b6a0: 7473 203d 2073 656c 662e 7265 7368 6170  ts = self.reshap
+0000b6b0: 6528 726f 7574 6572 5f6c 6f67 6974 732c  e(router_logits,
+0000b6c0: 2028 2d31 2c20 726f 7574 6572 5f6c 6f67   (-1, router_log
+0000b6d0: 6974 735f 7368 6170 655b 2d31 5d29 290a  its_shape[-1])).
+0000b6e0: 2020 2020 2020 2020 6c6f 6769 7473 5f73          logits_s
+0000b6f0: 6861 7065 203d 2073 656c 662e 7368 6170  hape = self.shap
+0000b700: 6528 726f 7574 6572 5f6c 6f67 6974 7329  e(router_logits)
+0000b710: 0a20 2020 2020 2020 2074 6f6b 656e 735f  .        tokens_
+0000b720: 7065 725f 6772 6f75 7020 3d20 6c6f 6769  per_group = logi
+0000b730: 7473 5f73 6861 7065 5b30 5d20 2f2f 2073  ts_shape[0] // s
+0000b740: 656c 662e 6470 5f67 726f 7570 0a20 2020  elf.dp_group.   
+0000b750: 2020 2020 2065 7870 6572 745f 6361 7061       expert_capa
+0000b760: 6369 7479 203d 2063 616c 6375 6c61 7465  city = calculate
+0000b770: 5f65 7870 6572 745f 6361 7061 6369 7479  _expert_capacity
+0000b780: 2873 656c 662e 6e75 6d5f 6578 7065 7274  (self.num_expert
+0000b790: 735f 6368 6f73 656e 2c20 746f 6b65 6e73  s_chosen, tokens
+0000b7a0: 5f70 6572 5f67 726f 7570 2c20 7365 6c66  _per_group, self
+0000b7b0: 2e63 6170 6163 6974 795f 6661 6374 6f72  .capacity_factor
+0000b7c0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+0000b7d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000b7e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000b7f0: 2020 2020 2020 7365 6c66 2e65 7870 6572        self.exper
+0000b800: 745f 6469 6d29 0a20 2020 2020 2020 2072  t_dim).        r
+0000b810: 6f75 7465 725f 6c6f 6769 7473 203d 2073  outer_logits = s
+0000b820: 656c 662e 7265 7368 6170 6528 726f 7574  elf.reshape(rout
+0000b830: 6572 5f6c 6f67 6974 732c 2028 7365 6c66  er_logits, (self
+0000b840: 2e64 705f 6772 6f75 702c 2074 6f6b 656e  .dp_group, token
+0000b850: 735f 7065 725f 6772 6f75 702c 2073 656c  s_per_group, sel
+0000b860: 662e 6578 7065 7274 5f64 696d 2929 0a0a  f.expert_dim))..
+0000b870: 2020 2020 2020 2020 6163 6375 6d5f 6578          accum_ex
+0000b880: 7065 7274 5f6d 6173 6b20 3d20 300a 2020  pert_mask = 0.  
+0000b890: 2020 2020 2020 6163 6375 6d5f 6578 7065        accum_expe
+0000b8a0: 7274 5f67 6174 6520 3d20 300a 2020 2020  rt_gate = 0.    
+0000b8b0: 2020 2020 6c6f 7373 203d 2073 656c 662e      loss = self.
+0000b8c0: 696e 6974 5f6c 6f73 730a 2020 2020 2020  init_loss.      
+0000b8d0: 2020 6d61 736b 5f63 6f75 6e74 203d 2030    mask_count = 0
+0000b8e0: 0a20 2020 2020 2020 2061 6363 756d 5f63  .        accum_c
+0000b8f0: 6f6d 6269 6e65 5f74 656e 736f 7220 3d20  ombine_tensor = 
+0000b900: 300a 2020 2020 2020 2020 2320 5072 6f62  0.        # Prob
+0000b910: 6162 696c 6974 6965 7320 666f 7220 6561  abilities for ea
+0000b920: 6368 2074 6f6b 656e 206f 6620 7768 6174  ch token of what
+0000b930: 2065 7870 6572 7420 6973 2073 686f 756c   expert is shoul
+0000b940: 6420 6265 2073 656e 7420 746f 0a20 2020  d be sent to.   
+0000b950: 2020 2020 2072 6f75 7465 725f 7072 6f62       router_prob
+0000b960: 203d 2073 656c 662e 736f 6674 6d61 7828   = self.softmax(
+0000b970: 726f 7574 6572 5f6c 6f67 6974 7329 0a0a  router_logits)..
+0000b980: 2020 2020 2020 2020 666f 7220 6578 7065          for expe
+0000b990: 7274 5f63 686f 7365 6e5f 696e 6465 7820  rt_chosen_index 
+0000b9a0: 696e 2072 616e 6765 2873 656c 662e 6e75  in range(self.nu
+0000b9b0: 6d5f 6578 7065 7274 735f 6368 6f73 656e  m_experts_chosen
+0000b9c0: 293a 0a20 2020 2020 2020 2020 2020 2023  ):.            #
+0000b9d0: 2066 6f72 2065 6163 6820 746f 6b65 6e2c   for each token,
+0000b9e0: 2073 6574 2074 6865 2072 6f75 7465 725f   set the router_
+0000b9f0: 7072 6f62 206f 6620 7468 6520 7365 6c65  prob of the sele
+0000ba00: 6374 6564 2065 7870 6572 7473 2074 6f20  cted experts to 
+0000ba10: 7a65 726f 0a20 2020 2020 2020 2020 2020  zero.           
+0000ba20: 2072 6f75 7465 725f 7072 6f62 203d 2073   router_prob = s
+0000ba30: 656c 662e 6d75 6c34 2872 6f75 7465 725f  elf.mul4(router_
+0000ba40: 7072 6f62 2c20 7365 6c66 2e73 7562 2873  prob, self.sub(s
+0000ba50: 656c 662e 6f6e 5f76 616c 7565 2c20 6163  elf.on_value, ac
+0000ba60: 6375 6d5f 6578 7065 7274 5f6d 6173 6b29  cum_expert_mask)
+0000ba70: 290a 2020 2020 2020 2020 2020 2020 2320  ).            # 
+0000ba80: 7368 6170 6520 6973 203a 2028 6470 5f67  shape is : (dp_g
+0000ba90: 726f 7570 2c20 746f 6b65 6e73 5f70 6572  roup, tokens_per
+0000baa0: 5f67 726f 7570 290a 2020 2020 2020 2020  _group).        
+0000bab0: 2020 2020 6578 7065 7274 5f69 6e64 6578      expert_index
+0000bac0: 2c20 6578 7065 7274 5f67 6174 6520 3d20  , expert_gate = 
+0000bad0: 7365 6c66 2e61 7267 6d61 7828 726f 7574  self.argmax(rout
+0000bae0: 6572 5f70 726f 6229 0a20 2020 2020 2020  er_prob).       
+0000baf0: 2020 2020 2023 2065 7870 6572 745f 6d61       # expert_ma
+0000bb00: 736b 2773 2073 6861 7065 3a20 2864 705f  sk's shape: (dp_
+0000bb10: 6772 6f75 702c 2074 6f6b 656e 735f 7065  group, tokens_pe
+0000bb20: 725f 6772 6f75 702c 2073 656c 662e 6578  r_group, self.ex
+0000bb30: 7065 7274 5f64 696d 290a 2020 2020 2020  pert_dim).      
+0000bb40: 2020 2020 2020 6578 7065 7274 5f6d 6173        expert_mas
+0000bb50: 6b20 3d20 7365 6c66 2e6f 6e65 686f 7428  k = self.onehot(
+0000bb60: 6578 7065 7274 5f69 6e64 6578 2c20 7365  expert_index, se
+0000bb70: 6c66 2e65 7870 6572 745f 6469 6d2c 2073  lf.expert_dim, s
+0000bb80: 656c 662e 6f6e 5f76 616c 7565 2c20 7365  elf.on_value, se
+0000bb90: 6c66 2e6f 6666 5f76 616c 7565 290a 2020  lf.off_value).  
+0000bba0: 2020 2020 2020 2020 2020 2320 7265 6e6f            # reno
+0000bbb0: 726d 616c 697a 6520 7468 6520 7265 7374  rmalize the rest
+0000bbc0: 2070 726f 6220 746f 2062 6520 6f66 2073   prob to be of s
+0000bbd0: 756d 2031 0a20 2020 2020 2020 2020 2020  um 1.           
+0000bbe0: 2072 6f75 7465 725f 7072 6f62 5f6e 6f72   router_prob_nor
+0000bbf0: 6d61 6c20 3d20 7365 6c66 2e64 6976 3128  mal = self.div1(
+0000bc00: 726f 7574 6572 5f70 726f 622c 2073 656c  router_prob, sel
+0000bc10: 662e 6164 6431 2873 656c 662e 7265 6475  f.add1(self.redu
+0000bc20: 6365 5f73 756d 5f6b 6565 7028 726f 7574  ce_sum_keep(rout
+0000bc30: 6572 5f70 726f 622c 202d 3129 2c20 3165  er_prob, -1), 1e
+0000bc40: 2d39 2929 0a0a 2020 2020 2020 2020 2020  -9))..          
+0000bc50: 2020 2320 7468 6520 6261 6c61 6e63 6520    # the balance 
+0000bc60: 6c6f 7373 2069 7320 636f 6d70 7574 6564  loss is computed
+0000bc70: 2061 7420 6561 6368 2072 6f75 7469 6e67   at each routing
+0000bc80: 2073 7465 700a 2020 2020 2020 2020 2020   step.          
+0000bc90: 2020 6c6f 7373 203d 2073 656c 662e 6164    loss = self.ad
+0000bca0: 645f 7363 616c 6128 6c6f 7373 2c20 7365  d_scala(loss, se
+0000bcb0: 6c66 2e5f 6175 7869 6c69 6172 795f 6c6f  lf._auxiliary_lo
+0000bcc0: 7373 2865 7870 6572 745f 6d61 736b 2c20  ss(expert_mask, 
+0000bcd0: 726f 7574 6572 5f70 726f 625f 6e6f 726d  router_prob_norm
+0000bce0: 616c 2929 0a0a 2020 2020 2020 2020 2020  al))..          
+0000bcf0: 2020 6f75 7470 7574 203d 2073 656c 662e    output = self.
+0000bd00: 5f6d 6173 6b6f 7574 5f6f 7665 7266 6c6f  _maskout_overflo
+0000bd10: 7765 645f 746f 6b65 6e73 2865 7870 6572  wed_tokens(exper
+0000bd20: 745f 6d61 736b 2c20 6578 7065 7274 5f63  t_mask, expert_c
+0000bd30: 6170 6163 6974 792c 2065 7870 6572 745f  apacity, expert_
+0000bd40: 6761 7465 2c0a 2020 2020 2020 2020 2020  gate,.          
+0000bd50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000bd60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000bd70: 2020 2020 2020 2020 2020 206d 6173 6b5f             mask_
+0000bd80: 636f 756e 742c 2065 7870 6572 745f 6368  count, expert_ch
+0000bd90: 6f73 656e 5f69 6e64 6578 290a 2020 2020  osen_index).    
+0000bda0: 2020 2020 2020 2020 6578 7065 7274 5f6d          expert_m
+0000bdb0: 6173 6b2c 2065 7870 6572 745f 6761 7465  ask, expert_gate
+0000bdc0: 2c20 6578 7065 7274 5f6d 6173 6b5f 666c  , expert_mask_fl
+0000bdd0: 6174 2c20 706f 7369 7469 6f6e 5f69 6e5f  at, position_in_
+0000bde0: 6578 7065 7274 203d 206f 7574 7075 745b  expert = output[
+0000bdf0: 305d 2c20 6f75 7470 7574 5b31 5d2c 206f  0], output[1], o
+0000be00: 7574 7075 745b 325d 2c20 6f75 7470 7574  utput[2], output
+0000be10: 5b33 5d0a 2020 2020 2020 2020 2020 2020  [3].            
+0000be20: 6163 6375 6d5f 6578 7065 7274 5f6d 6173  accum_expert_mas
+0000be30: 6b20 3d20 7365 6c66 2e61 6464 2861 6363  k = self.add(acc
+0000be40: 756d 5f65 7870 6572 745f 6d61 736b 2c20  um_expert_mask, 
+0000be50: 6578 7065 7274 5f6d 6173 6b29 0a20 2020  expert_mask).   
+0000be60: 2020 2020 2020 2020 2061 6363 756d 5f65           accum_e
+0000be70: 7870 6572 745f 6761 7465 203d 2073 656c  xpert_gate = sel
+0000be80: 662e 6164 6433 2861 6363 756d 5f65 7870  f.add3(accum_exp
+0000be90: 6572 745f 6761 7465 2c20 6578 7065 7274  ert_gate, expert
+0000bea0: 5f67 6174 6529 0a20 2020 2020 2020 2020  _gate).         
+0000beb0: 2020 206d 6173 6b5f 636f 756e 7420 3d20     mask_count = 
+0000bec0: 7365 6c66 2e61 6464 286d 6173 6b5f 636f  self.add(mask_co
+0000bed0: 756e 742c 2073 656c 662e 7265 6475 6365  unt, self.reduce
+0000bee0: 5f73 756d 5f6b 6565 7028 6578 7065 7274  _sum_keep(expert
+0000bef0: 5f6d 6173 6b2c 2031 2929 0a0a 2020 2020  _mask, 1))..    
+0000bf00: 2020 2020 2020 2020 2320 636f 6d62 696e          # combin
+0000bf10: 655f 7465 6e73 6f72 2773 2073 6861 7065  e_tensor's shape
+0000bf20: 3a20 2864 705f 6772 6f75 702c 2074 6f6b  : (dp_group, tok
+0000bf30: 656e 735f 7065 725f 6772 6f75 7029 0a20  ens_per_group). 
+0000bf40: 2020 2020 2020 2020 2020 2063 6f6d 6269             combi
+0000bf50: 6e65 5f74 656e 736f 7220 3d20 7365 6c66  ne_tensor = self
+0000bf60: 2e6d 756c 3728 6578 7065 7274 5f67 6174  .mul7(expert_gat
+0000bf70: 652c 2065 7870 6572 745f 6d61 736b 5f66  e, expert_mask_f
+0000bf80: 6c61 7429 0a20 2020 2020 2020 2020 2020  lat).           
+0000bf90: 2023 2063 6f6d 6269 6e65 5f74 656e 736f   # combine_tenso
+0000bfa0: 7227 7320 7368 6170 653a 2028 6470 5f67  r's shape: (dp_g
+0000bfb0: 726f 7570 2c20 746f 6b65 6e73 5f70 6572  roup, tokens_per
+0000bfc0: 5f67 726f 7570 2c20 7365 6c66 2e65 7870  _group, self.exp
+0000bfd0: 6572 745f 6469 6d29 0a20 2020 2020 2020  ert_dim).       
+0000bfe0: 2020 2020 2063 6f6d 6269 6e65 5f74 656e       combine_ten
+0000bff0: 736f 7220 3d20 7365 6c66 2e6d 756c 3828  sor = self.mul8(
+0000c000: 7365 6c66 2e65 7870 616e 6428 636f 6d62  self.expand(comb
+0000c010: 696e 655f 7465 6e73 6f72 2c20 2d31 292c  ine_tensor, -1),
+0000c020: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0000c030: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000c040: 2020 2020 2020 2020 7365 6c66 2e6f 6e65          self.one
+0000c050: 686f 7432 2865 7870 6572 745f 696e 6465  hot2(expert_inde
+0000c060: 782c 2073 656c 662e 6578 7065 7274 5f64  x, self.expert_d
+0000c070: 696d 2c20 7365 6c66 2e6f 6e5f 7661 6c75  im, self.on_valu
+0000c080: 652c 2073 656c 662e 6f66 665f 7661 6c75  e, self.off_valu
+0000c090: 6529 290a 2020 2020 2020 2020 2020 2020  e)).            
+0000c0a0: 2320 636f 6d62 696e 655f 7465 6e73 6f72  # combine_tensor
+0000c0b0: 2773 2073 6861 7065 3a20 2864 705f 6772  's shape: (dp_gr
+0000c0c0: 6f75 702c 2074 6f6b 656e 735f 7065 725f  oup, tokens_per_
+0000c0d0: 6772 6f75 702c 2073 656c 662e 6578 7065  group, self.expe
+0000c0e0: 7274 5f64 696d 2c20 7365 6c66 2e65 7870  rt_dim, self.exp
+0000c0f0: 6572 745f 6361 7061 6369 7479 290a 2020  ert_capacity).  
+0000c100: 2020 2020 2020 2020 2020 636f 6d62 696e            combin
+0000c110: 655f 7465 6e73 6f72 203d 2073 656c 662e  e_tensor = self.
+0000c120: 6d75 6c39 2873 656c 662e 6578 7061 6e64  mul9(self.expand
+0000c130: 3228 636f 6d62 696e 655f 7465 6e73 6f72  2(combine_tensor
+0000c140: 2c20 2d31 292c 0a20 2020 2020 2020 2020  , -1),.         
+0000c150: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000c160: 2020 2020 2020 2020 2020 2020 2020 7365                se
+0000c170: 6c66 2e6f 6e65 686f 7433 2873 656c 662e  lf.onehot3(self.
+0000c180: 6361 7374 2870 6f73 6974 696f 6e5f 696e  cast(position_in
+0000c190: 5f65 7870 6572 742c 206d 7374 7970 652e  _expert, mstype.
+0000c1a0: 696e 7433 3229 2c20 6578 7065 7274 5f63  int32), expert_c
+0000c1b0: 6170 6163 6974 792c 0a20 2020 2020 2020  apacity,.       
+0000c1c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000c1d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000c1e0: 2020 2020 2020 2020 2020 2020 2073 656c               sel
+0000c1f0: 662e 6f6e 5f76 616c 7565 2c20 7365 6c66  f.on_value, self
+0000c200: 2e6f 6666 5f76 616c 7565 2929 0a20 2020  .off_value)).   
+0000c210: 2020 2020 2020 2020 2061 6363 756d 5f63           accum_c
+0000c220: 6f6d 6269 6e65 5f74 656e 736f 7220 3d20  ombine_tensor = 
+0000c230: 7365 6c66 2e61 6464 3228 6163 6375 6d5f  self.add2(accum_
+0000c240: 636f 6d62 696e 655f 7465 6e73 6f72 2c20  combine_tensor, 
+0000c250: 636f 6d62 696e 655f 7465 6e73 6f72 290a  combine_tensor).
+0000c260: 0a20 2020 2020 2020 2023 2065 7870 6572  .        # exper
+0000c270: 7420 7765 6967 6874 7320 6e6f 726d 616c  t weights normal
+0000c280: 697a 6174 696f 6e0a 2020 2020 2020 2020  ization.        
+0000c290: 636f 6d62 696e 655f 7465 6e73 6f72 5f73  combine_tensor_s
+0000c2a0: 756d 203d 2073 656c 662e 7265 6475 6365  um = self.reduce
+0000c2b0: 5f73 756d 5f6b 6565 7032 2873 656c 662e  _sum_keep2(self.
+0000c2c0: 7265 6475 6365 5f73 756d 5f6b 6565 7032  reduce_sum_keep2
+0000c2d0: 2861 6363 756d 5f63 6f6d 6269 6e65 5f74  (accum_combine_t
+0000c2e0: 656e 736f 722c 202d 3129 2c20 2d32 290a  ensor, -1), -2).
+0000c2f0: 2020 2020 2020 2020 6163 6375 6d5f 636f          accum_co
+0000c300: 6d62 696e 655f 7465 6e73 6f72 203d 2073  mbine_tensor = s
+0000c310: 656c 662e 6469 7632 2861 6363 756d 5f63  elf.div2(accum_c
+0000c320: 6f6d 6269 6e65 5f74 656e 736f 722c 2073  ombine_tensor, s
+0000c330: 656c 662e 6164 6434 2863 6f6d 6269 6e65  elf.add4(combine
+0000c340: 5f74 656e 736f 725f 7375 6d2c 2031 652d  _tensor_sum, 1e-
+0000c350: 3929 290a 2020 2020 2020 2020 2320 6469  9)).        # di
+0000c360: 7370 6174 6368 5f74 656e 736f 7220 6973  spatch_tensor is
+0000c370: 206f 6620 626f 6f6c 6561 6e20 7479 7065   of boolean type
+0000c380: 2e20 4865 7265 2c20 7573 696e 6720 4e6f  . Here, using No
+0000c390: 7445 7175 616c 2069 6e73 7465 6164 206f  tEqual instead o
+0000c3a0: 6620 4361 7374 2c20 666f 7220 7468 6174  f Cast, for that
+0000c3b0: 2027 4361 7374 2074 6f20 626f 6f6c 2720   'Cast to bool' 
+0000c3c0: 6861 730a 2020 2020 2020 2020 2320 6261  has.        # ba
+0000c3d0: 6420 7065 7266 6f72 6d61 6e63 650a 2020  d performance.  
+0000c3e0: 2020 2020 2020 6469 7370 6174 6368 5f74        dispatch_t
+0000c3f0: 656e 736f 7220 3d20 7365 6c66 2e6e 6f74  ensor = self.not
+0000c400: 5f65 7175 616c 2861 6363 756d 5f63 6f6d  _equal(accum_com
+0000c410: 6269 6e65 5f74 656e 736f 722c 2030 2e30  bine_tensor, 0.0
+0000c420: 290a 2020 2020 2020 2020 7265 7475 726e  ).        return
+0000c430: 2064 6973 7061 7463 685f 7465 6e73 6f72   dispatch_tensor
+0000c440: 2c20 6163 6375 6d5f 636f 6d62 696e 655f  , accum_combine_
+0000c450: 7465 6e73 6f72 2c20 6c6f 7373 0a0a 2020  tensor, loss..  
+0000c460: 2020 6465 6620 5f61 7578 696c 6961 7279    def _auxiliary
+0000c470: 5f6c 6f73 7328 7365 6c66 2c20 6578 7065  _loss(self, expe
+0000c480: 7274 5f6d 6173 6b2c 2072 6f75 7465 725f  rt_mask, router_
+0000c490: 7072 6f62 293a 0a20 2020 2020 2020 2022  prob):.        "
+0000c4a0: 2222 0a20 2020 2020 2020 2043 6f6d 7075  "".        Compu
+0000c4b0: 7469 6e67 2074 6865 206c 6f61 6420 6261  ting the load ba
+0000c4c0: 6c61 6e63 6520 6c6f 7373 2e0a 2020 2020  lance loss..    
+0000c4d0: 2020 2020 2222 220a 2020 2020 2020 2020      """.        
+0000c4e0: 2320 6465 6e73 6974 795f 3127 7320 7368  # density_1's sh
+0000c4f0: 6170 653a 2028 6470 5f67 726f 7570 2c20  ape: (dp_group, 
+0000c500: 7365 6c66 2e65 7870 6572 745f 6469 6d29  self.expert_dim)
+0000c510: 0a20 2020 2020 2020 2064 656e 7369 7479  .        density
+0000c520: 5f31 203d 2073 656c 662e 7265 6475 6365  _1 = self.reduce
+0000c530: 5f6d 6561 6e28 6578 7065 7274 5f6d 6173  _mean(expert_mas
+0000c540: 6b2c 2031 290a 2020 2020 2020 2020 2320  k, 1).        # 
+0000c550: 6465 6e73 6974 795f 315f 7072 6f78 7927  density_1_proxy'
+0000c560: 7320 7368 6170 653a 2028 6470 5f67 726f  s shape: (dp_gro
+0000c570: 7570 2c20 7365 6c66 2e65 7870 6572 745f  up, self.expert_
+0000c580: 6469 6d29 0a20 2020 2020 2020 2064 656e  dim).        den
+0000c590: 7369 7479 5f31 5f70 726f 7879 203d 2073  sity_1_proxy = s
+0000c5a0: 656c 662e 7265 6475 6365 5f6d 6561 6e32  elf.reduce_mean2
+0000c5b0: 2872 6f75 7465 725f 7072 6f62 2c20 3129  (router_prob, 1)
+0000c5c0: 0a20 2020 2020 2020 206c 6f73 7320 3d20  .        loss = 
+0000c5d0: 7365 6c66 2e6d 756c 2864 656e 7369 7479  self.mul(density
+0000c5e0: 5f31 2c20 6465 6e73 6974 795f 315f 7072  _1, density_1_pr
+0000c5f0: 6f78 7929 0a20 2020 2020 2020 206c 6f73  oxy).        los
+0000c600: 7320 3d20 7365 6c66 2e72 6564 7563 655f  s = self.reduce_
+0000c610: 6d65 616e 3328 6c6f 7373 290a 2020 2020  mean3(loss).    
+0000c620: 2020 2020 6c6f 7373 203d 2073 656c 662e      loss = self.
+0000c630: 6d75 6c33 2873 656c 662e 6d75 6c32 286c  mul3(self.mul2(l
+0000c640: 6f73 732c 2073 656c 662e 6578 7065 7274  oss, self.expert
+0000c650: 5f64 696d 292c 2073 656c 662e 6578 7065  _dim), self.expe
+0000c660: 7274 5f64 696d 290a 2020 2020 2020 2020  rt_dim).        
+0000c670: 7265 7475 726e 206c 6f73 730a 0a20 2020  return loss..   
+0000c680: 2064 6566 205f 6d61 736b 6f75 745f 6f76   def _maskout_ov
+0000c690: 6572 666c 6f77 6564 5f74 6f6b 656e 7328  erflowed_tokens(
+0000c6a0: 7365 6c66 2c20 6578 7065 7274 5f6d 6173  self, expert_mas
+0000c6b0: 6b2c 2065 7870 6572 745f 6361 7061 6369  k, expert_capaci
+0000c6c0: 7479 2c20 6578 7065 7274 5f67 6174 652c  ty, expert_gate,
+0000c6d0: 206c 6173 745f 6e75 6d2c 2065 7870 6572   last_num, exper
+0000c6e0: 745f 6368 6f73 656e 5f69 6e64 6578 293a  t_chosen_index):
+0000c6f0: 0a20 2020 2020 2020 2022 2222 0a20 2020  .        """.   
+0000c700: 2020 2020 204b 6565 7069 6e67 206f 6e6c       Keeping onl
+0000c710: 7920 7468 6520 746f 6b65 6e73 2074 6861  y the tokens tha
+0000c720: 7420 6669 7420 7769 7468 696e 2065 7870  t fit within exp
+0000c730: 6572 745f 6361 7061 6369 7479 2e0a 2020  ert_capacity..  
+0000c740: 2020 2020 2020 2222 220a 2020 2020 2020        """.      
+0000c750: 2020 6375 6d73 756d 203d 2073 656c 662e    cumsum = self.
+0000c760: 6375 6d73 756d 2865 7870 6572 745f 6d61  cumsum(expert_ma
+0000c770: 736b 2c20 3129 0a20 2020 2020 2020 2069  sk, 1).        i
+0000c780: 6620 6578 7065 7274 5f63 686f 7365 6e5f  f expert_chosen_
+0000c790: 696e 6465 7820 3e20 303a 0a20 2020 2020  index > 0:.     
+0000c7a0: 2020 2020 2020 2063 756d 7375 6d20 3d20         cumsum = 
+0000c7b0: 7365 6c66 2e61 6464 2863 756d 7375 6d2c  self.add(cumsum,
+0000c7c0: 206c 6173 745f 6e75 6d29 0a20 2020 2020   last_num).     
+0000c7d0: 2020 2069 6620 7365 6c66 2e73 6176 655f     if self.save_
+0000c7e0: 746f 6b65 6e5f 6469 7374 7269 6275 7469  token_distributi
+0000c7f0: 6f6e 3a0a 2020 2020 2020 2020 2020 2020  on:.            
+0000c800: 7265 636f 7264 5f6e 616d 6520 3d20 276c  record_name = 'l
+0000c810: 6179 6572 2d27 202b 2073 7472 2873 656c  ayer-' + str(sel
+0000c820: 662e 6375 725f 6c61 7965 7229 0a20 2020  f.cur_layer).   
+0000c830: 2020 2020 2020 2020 2073 656c 662e 7465           self.te
+0000c840: 6e73 6f72 5f73 756d 6d61 7279 2872 6563  nsor_summary(rec
+0000c850: 6f72 645f 6e61 6d65 2c20 6375 6d73 756d  ord_name, cumsum
+0000c860: 5b30 5d5b 2d31 5d29 0a20 2020 2020 2020  [0][-1]).       
+0000c870: 2069 6620 7365 6c66 2e65 6e61 626c 655f   if self.enable_
+0000c880: 636f 6c64 5f68 6f74 5f65 7870 6572 743a  cold_hot_expert:
+0000c890: 0a20 2020 2020 2020 2020 2020 2063 756d  .            cum
+0000c8a0: 7375 6d5f 696e 745f 7661 6c75 6520 3d20  sum_int_value = 
+0000c8b0: 7365 6c66 2e63 6173 7428 6375 6d73 756d  self.cast(cumsum
+0000c8c0: 5b30 5d5b 2d31 5d2c 206d 7374 7970 652e  [0][-1], mstype.
+0000c8d0: 696e 7433 3229 0a20 2020 2020 2020 2020  int32).         
+0000c8e0: 2020 2073 656c 662e 6173 7369 676e 2873     self.assign(s
+0000c8f0: 656c 662e 6375 6d73 756d 5f76 616c 7565  elf.cumsum_value
+0000c900: 2c20 6375 6d73 756d 5f69 6e74 5f76 616c  , cumsum_int_val
+0000c910: 7565 290a 2020 2020 2020 2020 2320 706f  ue).        # po
+0000c920: 7369 7469 6f6e 5f69 6e5f 6578 7065 7274  sition_in_expert
+0000c930: 2773 2073 6861 7065 3a20 2864 705f 6772  's shape: (dp_gr
+0000c940: 6f75 702c 2074 6f6b 656e 735f 7065 725f  oup, tokens_per_
+0000c950: 6772 6f75 702c 2073 656c 662e 6578 7065  group, self.expe
+0000c960: 7274 5f64 696d 290a 2020 2020 2020 2020  rt_dim).        
+0000c970: 706f 7369 7469 6f6e 5f69 6e5f 6578 7065  position_in_expe
+0000c980: 7274 203d 2073 656c 662e 6d75 6c34 2863  rt = self.mul4(c
+0000c990: 756d 7375 6d2c 2065 7870 6572 745f 6d61  umsum, expert_ma
+0000c9a0: 736b 290a 2020 2020 2020 2020 6c65 7373  sk).        less
+0000c9b0: 5f72 6573 756c 7420 3d20 7365 6c66 2e6c  _result = self.l
+0000c9c0: 6573 7328 706f 7369 7469 6f6e 5f69 6e5f  ess(position_in_
+0000c9d0: 6578 7065 7274 2c20 6578 7065 7274 5f63  expert, expert_c
+0000c9e0: 6170 6163 6974 7929 0a20 2020 2020 2020  apacity).       
+0000c9f0: 2023 2065 7870 6572 745f 6d61 736b 2773   # expert_mask's
+0000ca00: 2073 6861 7065 3a20 2864 705f 6772 6f75   shape: (dp_grou
+0000ca10: 702c 2074 6f6b 656e 735f 7065 725f 6772  p, tokens_per_gr
+0000ca20: 6f75 702c 2073 656c 662e 6578 7065 7274  oup, self.expert
+0000ca30: 5f64 696d 290a 2020 2020 2020 2020 6578  _dim).        ex
+0000ca40: 7065 7274 5f6d 6173 6b20 3d20 7365 6c66  pert_mask = self
+0000ca50: 2e6d 756c 3528 6c65 7373 5f72 6573 756c  .mul5(less_resul
+0000ca60: 742c 2065 7870 6572 745f 6d61 736b 290a  t, expert_mask).
+0000ca70: 2020 2020 2020 2020 2320 6578 7065 7274          # expert
+0000ca80: 5f6d 6173 6b5f 666c 6174 2773 2073 6861  _mask_flat's sha
+0000ca90: 7065 3a20 2864 705f 6772 6f75 702c 2074  pe: (dp_group, t
+0000caa0: 6f6b 656e 735f 7065 725f 6772 6f75 7029  okens_per_group)
+0000cab0: 0a20 2020 2020 2020 2065 7870 6572 745f  .        expert_
+0000cac0: 6d61 736b 5f66 6c61 7420 3d20 7365 6c66  mask_flat = self
+0000cad0: 2e72 6564 7563 655f 7375 6d28 6578 7065  .reduce_sum(expe
+0000cae0: 7274 5f6d 6173 6b2c 202d 3129 0a0a 2020  rt_mask, -1)..  
+0000caf0: 2020 2020 2020 2320 4d61 736b 206f 7574        # Mask out
+0000cb00: 2074 6865 2065 7870 6572 7473 2074 6861   the experts tha
+0000cb10: 7420 6861 7665 206f 7665 7266 6c6f 7765  t have overflowe
+0000cb20: 6420 7468 6520 6578 7065 7274 5f63 6170  d the expert_cap
+0000cb30: 6163 6974 792e 0a20 2020 2020 2020 2023  acity..        #
+0000cb40: 2065 7870 6572 745f 6761 7465 2773 2073   expert_gate's s
+0000cb50: 6861 7065 3a20 2864 705f 6772 6f75 702c  hape: (dp_group,
+0000cb60: 2074 6f6b 656e 735f 7065 725f 6772 6f75   tokens_per_grou
+0000cb70: 7029 0a20 2020 2020 2020 2065 7870 6572  p).        exper
+0000cb80: 745f 6761 7465 203d 2073 656c 662e 6d75  t_gate = self.mu
+0000cb90: 6c36 2865 7870 6572 745f 6761 7465 2c20  l6(expert_gate, 
+0000cba0: 6578 7065 7274 5f6d 6173 6b5f 666c 6174  expert_mask_flat
+0000cbb0: 290a 2020 2020 2020 2020 6f75 7470 7574  ).        output
+0000cbc0: 203d 2028 6578 7065 7274 5f6d 6173 6b2c   = (expert_mask,
+0000cbd0: 2065 7870 6572 745f 6761 7465 2c20 6578   expert_gate, ex
+0000cbe0: 7065 7274 5f6d 6173 6b5f 666c 6174 2c20  pert_mask_flat, 
+0000cbf0: 706f 7369 7469 6f6e 5f69 6e5f 6578 7065  position_in_expe
+0000cc00: 7274 290a 2020 2020 2020 2020 7265 7475  rt).        retu
+0000cc10: 726e 206f 7574 7075 740a 0a0a 636c 6173  rn output...clas
+0000cc20: 7320 546f 706b 526f 7574 6572 5632 2843  s TopkRouterV2(C
+0000cc30: 656c 6c29 3a0a 2020 2020 7222 2222 0a20  ell):.    r""". 
+0000cc40: 2020 2020 2020 2041 2072 6f75 7465 7220         A router 
+0000cc50: 696d 706c 656d 656e 7461 7469 6f6e 2077  implementation w
+0000cc60: 6869 6368 206d 6170 7320 6561 6368 2074  hich maps each t
+0000cc70: 6f6b 656e 7320 746f 2074 6865 2074 6f70  okens to the top
+0000cc80: 6b20 6578 7065 7274 2e0a 0a20 2020 2020  k expert...     
+0000cc90: 2020 2041 7267 733a 0a20 2020 2020 2020     Args:.       
+0000cca0: 2020 2020 2064 5f6d 6f64 656c 2028 696e       d_model (in
+0000ccb0: 7429 3a20 5468 6520 6869 6464 656e 2073  t): The hidden s
+0000ccc0: 697a 6520 6f66 2065 6163 6820 746f 6b65  ize of each toke
+0000ccd0: 6e2e 0a20 2020 2020 2020 2020 2020 206d  n..            m
+0000cce0: 6f65 5f63 6f6e 6669 6728 4d6f 4543 6f6e  oe_config(MoECon
+0000ccf0: 6669 6729 3a20 5468 6520 636f 6e66 6967  fig): The config
+0000cd00: 7572 6174 696f 6e20 6f66 204d 6f45 2028  uration of MoE (
+0000cd10: 4d69 7874 7572 6520 6f66 2045 7870 6572  Mixture of Exper
+0000cd20: 7429 2e0a 2020 2020 2020 2020 2020 2020  t)..            
+0000cd30: 7472 6169 6e69 6e67 2028 626f 6f6c 293a  training (bool):
+0000cd40: 2054 6865 2076 616c 7565 2069 6e64 6963   The value indic
+0000cd50: 6174 696e 6720 7768 6574 6865 7220 6973  ating whether is
+0000cd60: 2069 6e20 7472 6169 6e69 6e67 2070 6861   in training pha
+0000cd70: 7365 2e0a 2020 2020 2020 2020 2020 2020  se..            
+0000cd80: 636f 6e66 6967 3a20 5468 6520 7061 7261  config: The para
+0000cd90: 6c6c 656c 2d72 656c 6174 6564 2063 6f6e  llel-related con
+0000cda0: 6669 6775 7261 7469 6f6e 2e0a 2020 2020  figuration..    
+0000cdb0: 2020 2020 496e 7075 7473 3a0a 2020 2020      Inputs:.    
+0000cdc0: 2020 2020 2020 2020 2d20 2a2a 726f 7574          - **rout
+0000cdd0: 6572 5f6c 6f67 6974 732a 2a20 2854 656e  er_logits** (Ten
+0000cde0: 736f 7229 202d 2054 656e 736f 7220 6f66  sor) - Tensor of
+0000cdf0: 2073 6861 7065 203a 6d61 7468 3a60 2864   shape :math:`(d
+0000ce00: 6174 615c 5f70 6172 616c 6c65 6c2c 2074  ata\_parallel, t
+0000ce10: 6f6b 656e 735c 5f70 6572 5c5f 6772 6f75  okens\_per\_grou
+0000ce20: 702c 0a20 2020 2020 2020 2020 2020 2065  p,.            e
+0000ce30: 7870 6572 745c 5f64 696d 2960 2e28 6470  xpert\_dim)`.(dp
+0000ce40: 2c20 4e2c 2065 7870 6572 745f 6469 6d29  , N, expert_dim)
+0000ce50: 0a0a 2020 2020 2020 2020 4f75 7470 7574  ..        Output
+0000ce60: 733a 0a20 2020 2020 2020 2020 2020 202d  s:.            -
+0000ce70: 202a 2a64 6973 7061 7463 685f 696e 6465   **dispatch_inde
+0000ce80: 782a 2a20 2854 656e 736f 7229 202d 2054  x** (Tensor) - T
+0000ce90: 656e 736f 7220 6f66 2073 6861 7065 203a  ensor of shape :
+0000cea0: 6d61 7468 3a60 2864 6174 615c 5f70 6172  math:`(data\_par
+0000ceb0: 616c 6c65 6c2c 2065 7870 6572 745c 5f64  allel, expert\_d
+0000cec0: 696d 2c20 6578 7065 7274 5c5f 6361 7061  im, expert\_capa
+0000ced0: 6369 7479 2960 2c0a 2020 2020 2020 2020  city)`,.        
+0000cee0: 2020 2020 2d20 2a2a 636f 6d62 696e 655f      - **combine_
+0000cef0: 696e 6465 782a 2a20 2854 656e 736f 7229  index** (Tensor)
+0000cf00: 202d 2054 656e 736f 7220 6f66 2073 6861   - Tensor of sha
+0000cf10: 7065 203a 6d61 7468 3a60 2864 6174 615c  pe :math:`(data\
+0000cf20: 5f70 6172 616c 6c65 6c2c 2074 6f6b 656e  _parallel, token
+0000cf30: 735c 5f70 6572 5c5f 6772 6f75 702c 206b  s\_per\_group, k
+0000cf40: 2960 2c0a 2020 2020 2020 2020 2020 2020  )`,.            
+0000cf50: 2d20 2a2a 726f 7574 6572 5f63 6f65 6666  - **router_coeff
+0000cf60: 2a2a 2028 5465 6e73 6f72 2920 2d20 5465  ** (Tensor) - Te
+0000cf70: 6e73 6f72 206f 6620 7368 6170 6520 3a6d  nsor of shape :m
+0000cf80: 6174 683a 6028 6461 7461 5c5f 7061 7261  ath:`(data\_para
+0000cf90: 6c6c 656c 2c20 746f 6b65 6e73 5c5f 7065  llel, tokens\_pe
+0000cfa0: 725c 5f67 726f 7570 2c20 6b29 602e 0a20  r\_group, k)`.. 
+0000cfb0: 2020 2022 2222 0a0a 2020 2020 6465 6620     """..    def 
+0000cfc0: 5f5f 696e 6974 5f5f 2873 656c 662c 0a20  __init__(self,. 
+0000cfd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000cfe0: 645f 6d6f 6465 6c2c 0a20 2020 2020 2020  d_model,.       
+0000cff0: 2020 2020 2020 2020 2020 6d6f 655f 636f            moe_co
+0000d000: 6e66 6967 2c0a 2020 2020 2020 2020 2020  nfig,.          
+0000d010: 2020 2020 2020 2074 7261 696e 696e 673d         training=
+0000d020: 5472 7565 2c0a 2020 2020 2020 2020 2020  True,.          
+0000d030: 2020 2020 2020 2070 6172 616c 6c65 6c5f         parallel_
+0000d040: 636f 6e66 6967 3d4e 6f6e 6529 3a0a 2020  config=None):.  
+0000d050: 2020 2020 2020 7375 7065 7228 546f 706b        super(Topk
+0000d060: 526f 7574 6572 5632 2c20 7365 6c66 292e  RouterV2, self).
+0000d070: 5f5f 696e 6974 5f5f 2829 0a0a 2020 2020  __init__()..    
+0000d080: 2020 2020 6470 203d 2070 6172 616c 6c65      dp = paralle
+0000d090: 6c5f 636f 6e66 6967 2e64 6174 615f 7061  l_config.data_pa
+0000d0a0: 7261 6c6c 656c 0a20 2020 2020 2020 2073  rallel.        s
+0000d0b0: 656c 662e 645f 6d6f 6465 6c20 3d20 645f  elf.d_model = d_
+0000d0c0: 6d6f 6465 6c0a 2020 2020 2020 2020 7365  model.        se
+0000d0d0: 6c66 2e65 7870 6572 745f 6469 6d20 3d20  lf.expert_dim = 
+0000d0e0: 6d6f 655f 636f 6e66 6967 2e65 7870 6572  moe_config.exper
+0000d0f0: 745f 6e75 6d0a 2020 2020 2020 2020 7365  t_num.        se
+0000d100: 6c66 2e63 6170 6163 6974 795f 6661 6374  lf.capacity_fact
+0000d110: 6f72 203d 206d 6f65 5f63 6f6e 6669 672e  or = moe_config.
+0000d120: 6361 7061 6369 7479 5f66 6163 746f 720a  capacity_factor.
+0000d130: 2020 2020 2020 2020 7365 6c66 2e73 6176          self.sav
+0000d140: 655f 746f 6b65 6e5f 6469 7374 7269 6275  e_token_distribu
+0000d150: 7469 6f6e 203d 206d 6f65 5f63 6f6e 6669  tion = moe_confi
+0000d160: 672e 7361 7665 5f74 6f6b 656e 5f64 6973  g.save_token_dis
+0000d170: 7472 6962 7574 696f 6e0a 2020 2020 2020  tribution.      
+0000d180: 2020 7365 6c66 2e74 7261 696e 696e 6720    self.training 
+0000d190: 3d20 7472 6169 6e69 6e67 0a20 2020 2020  = training.     
+0000d1a0: 2020 2073 656c 662e 6470 5f67 726f 7570     self.dp_group
+0000d1b0: 203d 2064 700a 2020 2020 2020 2020 7365   = dp.        se
+0000d1c0: 6c66 2e6e 756d 5f65 7870 6572 7473 5f63  lf.num_experts_c
+0000d1d0: 686f 7365 6e20 3d20 6d6f 655f 636f 6e66  hosen = moe_conf
+0000d1e0: 6967 2e6e 756d 5f65 7870 6572 7473 5f63  ig.num_experts_c
+0000d1f0: 686f 7365 6e0a 2020 2020 2020 2020 7365  hosen.        se
+0000d200: 6c66 2e6f 6e5f 7661 6c75 6520 3d20 5465  lf.on_value = Te
+0000d210: 6e73 6f72 2831 2e30 2c20 6d73 7479 7065  nsor(1.0, mstype
+0000d220: 2e66 6c6f 6174 3332 290a 2020 2020 2020  .float32).      
+0000d230: 2020 7365 6c66 2e6f 6666 5f76 616c 7565    self.off_value
+0000d240: 203d 2054 656e 736f 7228 302e 302c 206d   = Tensor(0.0, m
+0000d250: 7374 7970 652e 666c 6f61 7433 3229 0a20  stype.float32). 
+0000d260: 2020 2020 2020 2073 656c 662e 7261 6e67         self.rang
+0000d270: 6520 3d20 5465 6e73 6f72 286e 702e 7469  e = Tensor(np.ti
+0000d280: 6c65 286e 702e 6172 616e 6765 2831 3331  le(np.arange(131
+0000d290: 3037 3229 2b31 2c20 2873 656c 662e 6e75  072)+1, (self.nu
+0000d2a0: 6d5f 6578 7065 7274 735f 6368 6f73 656e  m_experts_chosen
+0000d2b0: 2c20 3129 292c 206d 7374 7970 652e 666c  , 1)), mstype.fl
+0000d2c0: 6f61 7433 3229 0a0a 2020 2020 2020 2020  oat32)..        
+0000d2d0: 7365 6c66 2e63 6173 7420 3d20 502e 4361  self.cast = P.Ca
+0000d2e0: 7374 2829 0a20 2020 2020 2020 2073 656c  st().        sel
+0000d2f0: 662e 7265 7368 6170 6520 3d20 502e 5265  f.reshape = P.Re
+0000d300: 7368 6170 6528 290a 2020 2020 2020 2020  shape().        
+0000d310: 7365 6c66 2e73 6861 7065 203d 2050 2e53  self.shape = P.S
+0000d320: 6861 7065 2829 0a20 2020 2020 2020 2073  hape().        s
+0000d330: 656c 662e 736f 6674 6d61 7820 3d20 502e  elf.softmax = P.
+0000d340: 536f 6674 6d61 7828 6178 6973 3d2d 3129  Softmax(axis=-1)
+0000d350: 2e73 6861 7264 2828 2864 702c 2031 2c20  .shard(((dp, 1, 
+0000d360: 312c 292c 2929 0a20 2020 2020 2020 2073  1,),)).        s
+0000d370: 656c 662e 746f 706b 203d 2050 2e54 6f70  elf.topk = P.Top
+0000d380: 4b28 292e 7368 6172 6428 2828 6470 2c20  K().shard(((dp, 
+0000d390: 312c 2031 292c 2929 0a20 2020 2020 2020  1, 1),)).       
+0000d3a0: 2073 656c 662e 6172 676d 6178 203d 2050   self.argmax = P
+0000d3b0: 2e41 7267 4d61 7857 6974 6856 616c 7565  .ArgMaxWithValue
+0000d3c0: 2861 7869 733d 2d31 2c20 6b65 6570 5f64  (axis=-1, keep_d
+0000d3d0: 696d 733d 4661 6c73 6529 2e73 6861 7264  ims=False).shard
+0000d3e0: 2828 2864 702c 2031 2c20 3129 2c29 290a  (((dp, 1, 1),)).
+0000d3f0: 2020 2020 2020 2020 7365 6c66 2e6f 6e65          self.one
+0000d400: 686f 745f 3264 203d 2050 2e4f 6e65 486f  hot_2d = P.OneHo
+0000d410: 7428 292e 7368 6172 6428 2828 6470 2c20  t().shard(((dp, 
+0000d420: 312c 2031 292c 2028 292c 2028 2929 290a  1, 1), (), ())).
+0000d430: 2020 2020 2020 2020 7365 6c66 2e6f 6e65          self.one
+0000d440: 686f 745f 3364 203d 2050 2e4f 6e65 486f  hot_3d = P.OneHo
+0000d450: 7428 292e 7368 6172 6428 2828 6470 2c20  t().shard(((dp, 
+0000d460: 312c 2031 2c20 3129 2c20 2829 2c20 2829  1, 1, 1), (), ()
+0000d470: 2929 0a20 2020 2020 2020 2073 656c 662e  )).        self.
+0000d480: 6375 6d73 756d 203d 2050 2e43 756d 5375  cumsum = P.CumSu
+0000d490: 6d28 6578 636c 7573 6976 653d 4661 6c73  m(exclusive=Fals
+0000d4a0: 6529 2e73 6861 7264 2828 2864 702c 2031  e).shard(((dp, 1
+0000d4b0: 2c20 3129 2c29 290a 2020 2020 2020 2020  , 1),)).        
+0000d4c0: 7365 6c66 2e6d 756c 5f32 645f 3164 203d  self.mul_2d_1d =
+0000d4d0: 2050 2e4d 756c 2829 2e73 6861 7264 2828   P.Mul().shard((
+0000d4e0: 2864 702c 2031 292c 2028 2929 290a 2020  (dp, 1), ())).  
+0000d4f0: 2020 2020 2020 7365 6c66 2e6d 756c 5f32        self.mul_2
+0000d500: 6420 3d20 502e 4d75 6c28 292e 7368 6172  d = P.Mul().shar
+0000d510: 6428 2828 6470 2c20 3129 2c20 2864 702c  d(((dp, 1), (dp,
+0000d520: 2031 2929 290a 2020 2020 2020 2020 7365   1))).        se
+0000d530: 6c66 2e6d 756c 5f33 6420 3d20 502e 4d75  lf.mul_3d = P.Mu
+0000d540: 6c28 292e 7368 6172 6428 2828 6470 2c20  l().shard(((dp, 
+0000d550: 312c 2031 292c 2028 6470 2c20 312c 2031  1, 1), (dp, 1, 1
+0000d560: 2929 290a 2020 2020 2020 2020 7365 6c66  ))).        self
+0000d570: 2e6d 756c 5f34 6420 3d20 502e 4d75 6c28  .mul_4d = P.Mul(
+0000d580: 292e 7368 6172 6428 2828 6470 2c20 312c  ).shard(((dp, 1,
+0000d590: 2031 2c20 3129 2c20 2864 702c 2031 2c20   1, 1), (dp, 1, 
+0000d5a0: 312c 2031 2929 290a 2020 2020 2020 2020  1, 1))).        
+0000d5b0: 7365 6c66 2e61 6464 5f32 6420 3d20 502e  self.add_2d = P.
+0000d5c0: 4164 6428 292e 7368 6172 6428 2828 6470  Add().shard(((dp
+0000d5d0: 2c20 3129 2c20 2864 702c 2031 2929 290a  , 1), (dp, 1))).
+0000d5e0: 2020 2020 2020 2020 7365 6c66 2e61 6464          self.add
+0000d5f0: 5f33 6420 3d20 502e 4164 6428 292e 7368  _3d = P.Add().sh
+0000d600: 6172 6428 2828 6470 2c20 312c 2031 292c  ard(((dp, 1, 1),
+0000d610: 2028 6470 2c20 312c 2031 2929 290a 2020   (dp, 1, 1))).  
+0000d620: 2020 2020 2020 7365 6c66 2e6c 6573 7320        self.less 
+0000d630: 3d20 502e 4c65 7373 2829 2e73 6861 7264  = P.Less().shard
+0000d640: 2828 2864 702c 2031 2c20 3129 2c20 2829  (((dp, 1, 1), ()
+0000d650: 2929 0a20 2020 2020 2020 2073 656c 662e  )).        self.
+0000d660: 6774 203d 2050 2e47 7265 6174 6572 2829  gt = P.Greater()
+0000d670: 2e73 6861 7264 2828 2864 702c 2031 292c  .shard(((dp, 1),
+0000d680: 2028 2929 290a 2020 2020 2020 2020 7365   ())).        se
+0000d690: 6c66 2e72 6564 7563 655f 7375 6d20 3d20  lf.reduce_sum = 
+0000d6a0: 502e 5265 6475 6365 5375 6d28 6b65 6570  P.ReduceSum(keep
+0000d6b0: 5f64 696d 733d 4661 6c73 6529 2e73 6861  _dims=False).sha
+0000d6c0: 7264 2828 2864 702c 2031 2c20 3129 2c29  rd(((dp, 1, 1),)
+0000d6d0: 290a 2020 2020 2020 2020 7365 6c66 2e74  ).        self.t
+0000d6e0: 7261 6e73 706f 7365 5f33 6420 3d20 502e  ranspose_3d = P.
+0000d6f0: 5472 616e 7370 6f73 6528 292e 7368 6172  Transpose().shar
+0000d700: 6428 2828 6470 2c20 312c 2031 292c 2929  d(((dp, 1, 1),))
+0000d710: 0a20 2020 2020 2020 2073 656c 662e 7472  .        self.tr
+0000d720: 616e 7370 6f73 6520 3d20 502e 5472 616e  anspose = P.Tran
+0000d730: 7370 6f73 6528 292e 7368 6172 6428 2828  spose().shard(((
+0000d740: 6470 2c20 312c 2031 2c20 3129 2c29 290a  dp, 1, 1, 1),)).
+0000d750: 2020 2020 2020 2020 7365 6c66 2e73 6c69          self.sli
+0000d760: 6365 203d 2050 2e53 7472 6964 6564 536c  ce = P.StridedSl
+0000d770: 6963 6528 292e 7368 6172 6428 2828 6470  ice().shard(((dp
+0000d780: 2c20 312c 2031 292c 2929 0a20 2020 2020  , 1, 1),)).     
+0000d790: 2020 2073 656c 662e 736c 6963 655f 7261     self.slice_ra
+0000d7a0: 6e67 6520 3d20 502e 5374 7269 6465 6453  nge = P.StridedS
+0000d7b0: 6c69 6365 2829 2e73 6861 7264 2828 2831  lice().shard(((1
+0000d7c0: 2c20 3129 2c29 290a 2020 2020 2020 2020  , 1),)).        
+0000d7d0: 7365 6c66 2e62 6d6d 5f72 616e 6765 203d  self.bmm_range =
+0000d7e0: 2050 2e42 6174 6368 4d61 744d 756c 2829   P.BatchMatMul()
+0000d7f0: 2e73 6861 7264 2828 2831 2c20 3129 2c20  .shard(((1, 1), 
+0000d800: 2864 702c 2031 2c20 312c 2031 2929 290a  (dp, 1, 1, 1))).
+0000d810: 2020 2020 2020 2020 7365 6c66 2e61 6464          self.add
+0000d820: 5f65 7073 203d 2050 2e41 6464 2829 2e73  _eps = P.Add().s
+0000d830: 6861 7264 2828 2864 702c 2031 2c20 3129  hard(((dp, 1, 1)
+0000d840: 2c20 2829 2929 0a20 2020 2020 2020 2073  , ())).        s
+0000d850: 656c 662e 7265 6475 6365 5f73 756d 5f6b  elf.reduce_sum_k
+0000d860: 6565 7020 3d20 502e 5265 6475 6365 5375  eep = P.ReduceSu
+0000d870: 6d28 6b65 6570 5f64 696d 733d 5472 7565  m(keep_dims=True
+0000d880: 292e 7368 6172 6428 2828 6470 2c20 312c  ).shard(((dp, 1,
+0000d890: 2031 292c 2929 0a20 2020 2020 2020 2073   1),)).        s
+0000d8a0: 656c 662e 6469 765f 3364 203d 2050 2e52  elf.div_3d = P.R
+0000d8b0: 6561 6c44 6976 2829 2e73 6861 7264 2828  ealDiv().shard((
+0000d8c0: 2864 702c 2031 2c20 3129 2c20 2864 702c  (dp, 1, 1), (dp,
+0000d8d0: 2031 2c20 3129 2929 0a20 2020 2020 2020   1, 1))).       
+0000d8e0: 2073 656c 662e 636f 6e63 6174 5f33 6420   self.concat_3d 
+0000d8f0: 3d20 502e 436f 6e63 6174 2831 292e 7368  = P.Concat(1).sh
+0000d900: 6172 6428 2828 6470 2c20 312c 2031 292c  ard(((dp, 1, 1),
+0000d910: 2028 6470 2c20 312c 2031 2929 290a 2020   (dp, 1, 1))).  
+0000d920: 2020 2020 2020 7365 6c66 2e7a 6572 6f73        self.zeros
+0000d930: 203d 2054 656e 736f 7228 6e70 2e7a 6572   = Tensor(np.zer
+0000d940: 6f73 2828 6470 2c20 7365 6c66 2e65 7870  os((dp, self.exp
+0000d950: 6572 745f 6469 6d2c 2031 2c20 645f 6d6f  ert_dim, 1, d_mo
+0000d960: 6465 6c29 292c 206d 7374 7970 652e 666c  del)), mstype.fl
+0000d970: 6f61 7431 3629 0a20 2020 2020 2020 2073  oat16).        s
+0000d980: 656c 662e 7a65 726f 735f 3364 203d 2054  elf.zeros_3d = T
+0000d990: 656e 736f 7228 6e70 2e7a 6572 6f73 2828  ensor(np.zeros((
+0000d9a0: 6470 2c20 312c 2064 5f6d 6f64 656c 2929  dp, 1, d_model))
+0000d9b0: 2c20 6d73 7479 7065 2e66 6c6f 6174 3136  , mstype.float16
+0000d9c0: 290a 2020 2020 2020 2020 7365 6c66 2e64  ).        self.d
+0000d9d0: 6973 7061 7463 685f 6761 7468 6572 203d  ispatch_gather =
+0000d9e0: 2050 2e47 6174 6865 7228 6261 7463 685f   P.Gather(batch_
+0000d9f0: 6469 6d73 3d31 292e 7368 6172 6428 2828  dims=1).shard(((
+0000da00: 6470 2c20 312c 2031 292c 2028 6470 2c20  dp, 1, 1), (dp, 
+0000da10: 312c 2031 292c 2929 0a20 2020 2020 2020  1, 1),)).       
+0000da20: 2073 656c 662e 636f 6e63 6174 203d 2050   self.concat = P
+0000da30: 2e43 6f6e 6361 7428 3229 2e73 6861 7264  .Concat(2).shard
+0000da40: 2828 2864 702c 2031 2c20 312c 2031 292c  (((dp, 1, 1, 1),
+0000da50: 2028 6470 2c20 312c 2031 2c20 3129 2929   (dp, 1, 1, 1)))
+0000da60: 0a20 2020 2020 2020 2073 656c 662e 636f  .        self.co
+0000da70: 6d62 696e 655f 6761 7468 6572 203d 2050  mbine_gather = P
+0000da80: 2e47 6174 6865 7228 6261 7463 685f 6469  .Gather(batch_di
+0000da90: 6d73 3d31 292e 7368 6172 6428 2828 6470  ms=1).shard(((dp
+0000daa0: 2c20 312c 2031 292c 2028 6470 2c20 312c  , 1, 1), (dp, 1,
+0000dab0: 2031 292c 2929 0a20 2020 2020 2020 2073   1),)).        s
+0000dac0: 656c 662e 6d75 6c5f 726f 7574 6572 5f63  elf.mul_router_c
+0000dad0: 6f65 6666 203d 2050 2e4d 756c 2829 2e73  oeff = P.Mul().s
+0000dae0: 6861 7264 2828 2864 702c 2031 2c20 312c  hard(((dp, 1, 1,
+0000daf0: 2031 292c 2028 6470 2c20 312c 2031 2c20   1), (dp, 1, 1, 
+0000db00: 3129 2929 0a20 2020 2020 2020 2073 656c  1))).        sel
+0000db10: 662e 7375 6d5f 726f 7574 6572 5f63 6f65  f.sum_router_coe
+0000db20: 6666 203d 2050 2e52 6564 7563 6553 756d  ff = P.ReduceSum
+0000db30: 286b 6565 705f 6469 6d73 3d46 616c 7365  (keep_dims=False
+0000db40: 292e 7368 6172 6428 2828 6470 2c20 312c  ).shard(((dp, 1,
+0000db50: 2031 2c20 3129 2c29 290a 0a20 2020 2020   1, 1),))..     
+0000db60: 2020 2023 2073 6f72 7420 696e 6465 7869     # sort indexi
+0000db70: 6e67 0a20 2020 2020 2020 2073 656c 662e  ng.        self.
+0000db80: 7261 6e67 6532 203d 2054 656e 736f 7228  range2 = Tensor(
+0000db90: 6e70 2e74 696c 6528 6e70 2e61 7261 6e67  np.tile(np.arang
+0000dba0: 6528 3133 3130 3732 292c 2028 7365 6c66  e(131072), (self
+0000dbb0: 2e65 7870 6572 745f 6469 6d2c 2031 2929  .expert_dim, 1))
+0000dbc0: 2c20 6d73 7479 7065 2e66 6c6f 6174 3332  , mstype.float32
+0000dbd0: 290a 2020 2020 2020 2020 7365 6c66 2e61  ).        self.a
+0000dbe0: 6464 5f6f 6e65 203d 2050 2e41 6464 2829  dd_one = P.Add()
+0000dbf0: 2e73 6861 7264 2828 2864 702c 2031 2c20  .shard(((dp, 1, 
+0000dc00: 3129 2c20 2829 2929 0a20 2020 2020 2020  1), ())).       
+0000dc10: 2073 656c 662e 6164 645f 7261 6e67 6520   self.add_range 
+0000dc20: 3d20 502e 4164 6428 292e 7368 6172 6428  = P.Add().shard(
+0000dc30: 2828 312c 2031 2c20 3129 2c20 2829 2929  ((1, 1, 1), ()))
+0000dc40: 0a20 2020 2020 2020 2073 656c 662e 7375  .        self.su
+0000dc50: 625f 7261 6e67 6520 3d20 502e 5375 6228  b_range = P.Sub(
+0000dc60: 292e 7368 6172 6428 2828 292c 2028 6470  ).shard(((), (dp
+0000dc70: 2c20 312c 2031 2929 290a 2020 2020 2020  , 1, 1))).      
+0000dc80: 2020 7365 6c66 2e6d 756c 5f72 616e 6765    self.mul_range
+0000dc90: 203d 2050 2e4d 756c 2829 2e73 6861 7264   = P.Mul().shard
+0000dca0: 2828 2864 702c 2031 2c20 3129 2c20 2831  (((dp, 1, 1), (1
+0000dcb0: 2c20 312c 2031 2929 290a 2020 2020 2020  , 1, 1))).      
+0000dcc0: 2020 7365 6c66 2e73 6f72 745f 7261 6e67    self.sort_rang
+0000dcd0: 6520 3d20 502e 536f 7274 2829 2e73 6861  e = P.Sort().sha
+0000dce0: 7264 2828 2864 702c 2031 2c20 3129 2c29  rd(((dp, 1, 1),)
+0000dcf0: 290a 2020 2020 2020 2020 7365 6c66 2e6d  ).        self.m
+0000dd00: 6f64 203d 2050 2e4d 6f64 2829 2e73 6861  od = P.Mod().sha
+0000dd10: 7264 2828 2864 702c 2031 2c20 3129 2c20  rd(((dp, 1, 1), 
+0000dd20: 2829 2929 0a20 2020 2020 2020 2073 656c  ())).        sel
+0000dd30: 662e 7072 696e 7420 3d20 502e 5072 696e  f.print = P.Prin
+0000dd40: 7428 290a 0a20 2020 2064 6566 2064 6973  t()..    def dis
+0000dd50: 7061 7463 6828 7365 6c66 2c20 696e 7075  patch(self, inpu
+0000dd60: 745f 7465 6e73 6f72 2c20 6469 7370 6174  t_tensor, dispat
+0000dd70: 6368 5f69 6e64 6578 293a 0a20 2020 2020  ch_index):.     
+0000dd80: 2020 2072 2222 220a 2020 2020 2020 2020     r""".        
+0000dd90: 2020 2020 496d 706c 656d 656e 7469 6e67      Implementing
+0000dda0: 2064 6973 7061 7463 6820 6f70 6572 6174   dispatch operat
+0000ddb0: 696f 6e2e 0a20 2020 2020 2020 2020 2020  ion..           
+0000ddc0: 2049 6e70 7574 733a 0a20 2020 2020 2020   Inputs:.       
+0000ddd0: 2020 2020 2020 2020 202d 202a 2a69 6e70           - **inp
+0000dde0: 7574 5f74 656e 736f 722a 2a20 2854 656e  ut_tensor** (Ten
+0000ddf0: 736f 7229 202d 2054 656e 736f 7220 6f66  sor) - Tensor of
+0000de00: 2073 6861 7065 203a 6d61 7468 3a60 2864   shape :math:`(d
+0000de10: 6174 615c 5f70 6172 616c 6c65 6c2c 2074  ata\_parallel, t
+0000de20: 6f6b 656e 735c 5f70 6572 5c5f 6772 6f75  okens\_per\_grou
+0000de30: 702c 0a20 2020 2020 2020 2020 2020 2020  p,.             
+0000de40: 2020 2068 6964 6465 6e5c 5f73 697a 6529     hidden\_size)
+0000de50: 602e 2864 702c 204e 2c20 6829 2c0a 2020  `.(dp, N, h),.  
+0000de60: 2020 2020 2020 2020 2020 2020 2020 2d20                - 
+0000de70: 2a2a 6469 7370 6174 6368 5f69 6e64 6578  **dispatch_index
+0000de80: 2a2a 2028 5465 6e73 6f72 2920 2d20 5465  ** (Tensor) - Te
+0000de90: 6e73 6f72 206f 6620 7368 6170 6520 3a6d  nsor of shape :m
+0000dea0: 6174 683a 6028 6461 7461 5c5f 7061 7261  ath:`(data\_para
+0000deb0: 6c6c 656c 2c20 6578 7065 7274 5c5f 6e75  llel, expert\_nu
+0000dec0: 6d2c 0a20 2020 2020 2020 2020 2020 2020  m,.             
+0000ded0: 2020 2065 7870 6572 745c 5f63 6170 6163     expert\_capac
+0000dee0: 6974 7929 602e 2864 702c 2045 2c20 6e29  ity)`.(dp, E, n)
+0000def0: 2e0a 0a20 2020 2020 2020 2020 2020 204f  ...            O
+0000df00: 7574 7075 7473 3a0a 2020 2020 2020 2020  utputs:.        
+0000df10: 2020 2020 2020 2020 2d20 2a2a 6578 7065          - **expe
+0000df20: 7274 5f69 6e70 7574 2a2a 2028 5465 6e73  rt_input** (Tens
+0000df30: 6f72 2920 2d20 5465 6e73 6f72 206f 6620  or) - Tensor of 
+0000df40: 7368 6170 6520 3a6d 6174 683a 6028 6461  shape :math:`(da
+0000df50: 7461 5c5f 7061 7261 6c6c 656c 2c20 6578  ta\_parallel, ex
+0000df60: 7065 7274 5c5f 6e75 6d2c 0a20 2020 2020  pert\_num,.     
+0000df70: 2020 2020 2020 2020 2020 2065 7870 6572             exper
+0000df80: 745c 5f63 6170 6163 6974 792c 2068 6964  t\_capacity, hid
+0000df90: 6465 6e5c 5f73 697a 6529 602e 2864 702c  den\_size)`.(dp,
+0000dfa0: 2045 2c20 6e2c 2068 292e 0a20 2020 2020   E, n, h)..     
+0000dfb0: 2020 2022 2222 0a20 2020 2020 2020 2069     """.        i
+0000dfc0: 6e70 7574 5f74 656e 736f 725f 7061 6464  nput_tensor_padd
+0000dfd0: 6564 203d 2073 656c 662e 636f 6e63 6174  ed = self.concat
+0000dfe0: 5f33 6428 2873 656c 662e 7a65 726f 735f  _3d((self.zeros_
+0000dff0: 3364 2c20 696e 7075 745f 7465 6e73 6f72  3d, input_tensor
+0000e000: 2929 2023 2023 2028 6470 2c20 312b 4e2c  )) # # (dp, 1+N,
+0000e010: 2068 2920 3c2d 2d20 2864 702c 204e 2c20   h) <-- (dp, N, 
+0000e020: 6829 0a20 2020 2020 2020 2065 7870 6572  h).        exper
+0000e030: 745f 696e 7075 7420 3d20 7365 6c66 2e64  t_input = self.d
+0000e040: 6973 7061 7463 685f 6761 7468 6572 2869  ispatch_gather(i
+0000e050: 6e70 7574 5f74 656e 736f 725f 7061 6464  nput_tensor_padd
+0000e060: 6564 2c20 6469 7370 6174 6368 5f69 6e64  ed, dispatch_ind
+0000e070: 6578 2c20 3129 2023 2028 6470 2c20 452c  ex, 1) # (dp, E,
+0000e080: 206e 2c20 6829 203c 2d2d 2028 6470 2c20   n, h) <-- (dp, 
+0000e090: 4e2c 2068 292c 2028 6470 2c20 452c 206e  N, h), (dp, E, n
+0000e0a0: 290a 2020 2020 2020 2020 7265 7475 726e  ).        return
+0000e0b0: 2065 7870 6572 745f 696e 7075 740a 0a20   expert_input.. 
+0000e0c0: 2020 2064 6566 2063 6f6d 6269 6e65 2873     def combine(s
+0000e0d0: 656c 662c 2065 7870 6572 745f 6f75 7470  elf, expert_outp
+0000e0e0: 7574 2c20 636f 6d62 696e 655f 696e 6465  ut, combine_inde
+0000e0f0: 782c 2072 6f75 7465 725f 636f 6566 6629  x, router_coeff)
+0000e100: 3a0a 2020 2020 2020 2020 7222 2222 0a20  :.        r""". 
+0000e110: 2020 2020 2020 2020 2020 2049 6d70 6c65             Imple
+0000e120: 6d65 6e74 696e 6720 636f 6d62 696e 6520  menting combine 
+0000e130: 6f70 6572 6174 696f 6e2e 0a20 2020 2020  operation..     
+0000e140: 2020 2020 2020 2049 6e70 7574 733a 0a20         Inputs:. 
+0000e150: 2020 2020 2020 2020 2020 2020 2020 202d                 -
+0000e160: 202a 2a65 7870 6572 745f 6f75 7470 7574   **expert_output
+0000e170: 2a2a 2028 5465 6e73 6f72 2920 2d20 5465  ** (Tensor) - Te
+0000e180: 6e73 6f72 206f 6620 7368 6170 6520 3a6d  nsor of shape :m
+0000e190: 6174 683a 6028 6461 7461 5c5f 7061 7261  ath:`(data\_para
+0000e1a0: 6c6c 656c 2c20 6578 7065 7274 5c5f 6e75  llel, expert\_nu
+0000e1b0: 6d2c 0a20 2020 2020 2020 2020 2020 2020  m,.             
+0000e1c0: 2020 2065 7870 6572 745c 5f63 6170 6163     expert\_capac
+0000e1d0: 6974 792c 2068 6964 6465 6e5c 5f73 697a  ity, hidden\_siz
+0000e1e0: 6529 602e 2864 702c 2045 2c20 6e2c 2068  e)`.(dp, E, n, h
+0000e1f0: 292c 0a20 2020 2020 2020 2020 2020 2020  ),.             
+0000e200: 2020 202d 202a 2a63 6f6d 6269 6e65 5f69     - **combine_i
+0000e210: 6e64 6578 2a2a 2028 5465 6e73 6f72 2920  ndex** (Tensor) 
+0000e220: 2d20 5465 6e73 6f72 206f 6620 7368 6170  - Tensor of shap
+0000e230: 6520 3a6d 6174 683a 6028 6461 7461 5c5f  e :math:`(data\_
+0000e240: 7061 7261 6c6c 656c 2c20 746f 6b65 6e73  parallel, tokens
+0000e250: 5c5f 7065 725c 5f67 726f 7570 2c0a 2020  \_per\_group,.  
+0000e260: 2020 2020 2020 2020 2020 2020 2020 6e75                nu
+0000e270: 6d5c 5f65 7870 6572 7473 5c5f 6368 6f73  m\_experts\_chos
+0000e280: 656e 2960 2e28 6470 2c20 4e2c 206b 292c  en)`.(dp, N, k),
+0000e290: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0000e2a0: 202d 202a 2a72 6f75 7465 725f 636f 6566   - **router_coef
+0000e2b0: 662a 2a20 2854 656e 736f 7229 202d 2054  f** (Tensor) - T
+0000e2c0: 656e 736f 7220 6f66 2073 6861 7065 203a  ensor of shape :
+0000e2d0: 6d61 7468 3a60 2864 6174 615c 5f70 6172  math:`(data\_par
+0000e2e0: 616c 6c65 6c2c 2074 6f6b 656e 735c 5f70  allel, tokens\_p
+0000e2f0: 6572 5c5f 6772 6f75 702c 0a20 2020 2020  er\_group,.     
+0000e300: 2020 2020 2020 2020 2020 206e 756d 5c5f             num\_
+0000e310: 6578 7065 7274 735c 5f63 686f 7365 6e29  experts\_chosen)
+0000e320: 602e 2864 702c 204e 2c20 6b29 2e0a 0a20  `.(dp, N, k)... 
+0000e330: 2020 2020 2020 2020 2020 204f 7574 7075             Outpu
+0000e340: 7473 3a0a 2020 2020 2020 2020 2020 2020  ts:.            
+0000e350: 2020 2020 2d20 2a2a 6f75 7470 7574 5f74      - **output_t
+0000e360: 656e 736f 722a 2a20 2854 656e 736f 7229  ensor** (Tensor)
+0000e370: 202d 2054 656e 736f 7220 6f66 2073 6861   - Tensor of sha
+0000e380: 7065 203a 6d61 7468 3a60 2864 6174 615c  pe :math:`(data\
+0000e390: 5f70 6172 616c 6c65 6c2c 2074 6f6b 656e  _parallel, token
+0000e3a0: 735c 5f70 6572 5c5f 6772 6f75 702c 0a20  s\_per\_group,. 
+0000e3b0: 2020 2020 2020 2020 2020 2020 2020 2068                 h
+0000e3c0: 6964 6465 6e5c 5f73 697a 6529 602e 2864  idden\_size)`.(d
+0000e3d0: 702c 204e 2c20 6829 2e0a 2020 2020 2020  p, N, h)..      
+0000e3e0: 2020 2222 220a 2020 2020 2020 2020 6578    """.        ex
+0000e3f0: 7065 7274 5f6f 7574 7075 7420 3d20 7365  pert_output = se
+0000e400: 6c66 2e63 6f6e 6361 7428 2873 656c 662e  lf.concat((self.
+0000e410: 7a65 726f 732c 2065 7870 6572 745f 6f75  zeros, expert_ou
+0000e420: 7470 7574 2929 2023 2028 6470 2c20 452c  tput)) # (dp, E,
+0000e430: 2031 2b6e 2c20 6829 203c 2d2d 2028 6470   1+n, h) <-- (dp
+0000e440: 2c20 452c 206e 2c20 6829 0a20 2020 2020  , E, n, h).     
+0000e450: 2020 2065 7870 6572 745f 6f75 7470 7574     expert_output
+0000e460: 203d 2073 656c 662e 7265 7368 6170 6528   = self.reshape(
+0000e470: 6578 7065 7274 5f6f 7574 7075 742c 2028  expert_output, (
+0000e480: 6578 7065 7274 5f6f 7574 7075 742e 7368  expert_output.sh
+0000e490: 6170 655b 305d 2c20 6578 7065 7274 5f6f  ape[0], expert_o
+0000e4a0: 7574 7075 742e 7368 6170 655b 315d 2a65  utput.shape[1]*e
+0000e4b0: 7870 6572 745f 6f75 7470 7574 2e73 6861  xpert_output.sha
+0000e4c0: 7065 5b32 5d2c 2065 7870 6572 745f 6f75  pe[2], expert_ou
+0000e4d0: 7470 7574 2e73 6861 7065 5b33 5d29 2920  tput.shape[3])) 
+0000e4e0: 2320 2864 702c 2045 2a28 312b 6e29 2c20  # (dp, E*(1+n), 
+0000e4f0: 6829 203c 2d2d 2028 6470 2c20 452c 2031  h) <-- (dp, E, 1
+0000e500: 2b6e 2c20 6829 0a20 2020 2020 2020 206f  +n, h).        o
+0000e510: 7574 7075 745f 7465 6e73 6f72 203d 2073  utput_tensor = s
+0000e520: 656c 662e 636f 6d62 696e 655f 6761 7468  elf.combine_gath
+0000e530: 6572 2865 7870 6572 745f 6f75 7470 7574  er(expert_output
+0000e540: 2c20 636f 6d62 696e 655f 696e 6465 782c  , combine_index,
+0000e550: 2031 2920 2320 2864 702c 204e 2c20 6b2c   1) # (dp, N, k,
+0000e560: 2068 2920 3c2d 2d20 2864 702c 2045 2a28   h) <-- (dp, E*(
+0000e570: 312b 6e29 2c20 6829 2c20 2864 702c 204e  1+n), h), (dp, N
+0000e580: 2c20 6b29 0a20 2020 2020 2020 206f 7574  , k).        out
+0000e590: 7075 745f 7465 6e73 6f72 203d 2073 656c  put_tensor = sel
+0000e5a0: 662e 6d75 6c5f 726f 7574 6572 5f63 6f65  f.mul_router_coe
+0000e5b0: 6666 286f 7574 7075 745f 7465 6e73 6f72  ff(output_tensor
+0000e5c0: 2c20 7365 6c66 2e72 6573 6861 7065 2872  , self.reshape(r
+0000e5d0: 6f75 7465 725f 636f 6566 662c 2028 726f  outer_coeff, (ro
+0000e5e0: 7574 6572 5f63 6f65 6666 2e73 6861 7065  uter_coeff.shape
+0000e5f0: 5b30 5d2c 2072 6f75 7465 725f 636f 6566  [0], router_coef
+0000e600: 662e 7368 6170 655b 315d 2c20 726f 7574  f.shape[1], rout
+0000e610: 6572 5f63 6f65 6666 2e73 6861 7065 5b32  er_coeff.shape[2
+0000e620: 5d2c 2031 2929 2920 2320 2864 702c 204e  ], 1))) # (dp, N
+0000e630: 2c20 6b2c 2068 2920 3c2d 2d20 2864 702c  , k, h) <-- (dp,
+0000e640: 204e 2c20 6b2c 2068 2920 2864 702c 204e   N, k, h) (dp, N
+0000e650: 2c20 6b2c 2031 290a 2020 2020 2020 2020  , k, 1).        
+0000e660: 6f75 7470 7574 5f74 656e 736f 7220 3d20  output_tensor = 
+0000e670: 7365 6c66 2e73 756d 5f72 6f75 7465 725f  self.sum_router_
+0000e680: 636f 6566 6628 6f75 7470 7574 5f74 656e  coeff(output_ten
+0000e690: 736f 722c 2032 2920 2372 6564 7563 6520  sor, 2) #reduce 
+0000e6a0: 7375 6d20 2320 2864 702c 204e 2c20 6829  sum # (dp, N, h)
+0000e6b0: 203c 2d2d 2028 6470 2c20 4e2c 206b 2c20   <-- (dp, N, k, 
+0000e6c0: 6829 0a20 2020 2020 2020 2072 6574 7572  h).        retur
+0000e6d0: 6e20 6f75 7470 7574 5f74 656e 736f 720a  n output_tensor.
+0000e6e0: 0a20 2020 2064 6566 2063 6f6e 7374 7275  .    def constru
+0000e6f0: 6374 2873 656c 662c 2072 6f75 7465 725f  ct(self, router_
+0000e700: 6c6f 6769 7473 293a 0a20 2020 2020 2020  logits):.       
+0000e710: 2072 6f75 7465 725f 7072 6f62 203d 2073   router_prob = s
+0000e720: 656c 662e 736f 6674 6d61 7828 726f 7574  elf.softmax(rout
+0000e730: 6572 5f6c 6f67 6974 7329 2023 2028 6470  er_logits) # (dp
+0000e740: 2c20 4e2c 2065 7870 6572 745f 6469 6d29  , N, expert_dim)
+0000e750: 6670 3332 203c 2d2d 2028 6470 2c20 4e2c  fp32 <-- (dp, N,
+0000e760: 2065 7870 6572 745f 6469 6d29 6670 3332   expert_dim)fp32
+0000e770: 0a20 2020 2020 2020 2065 7870 6572 745f  .        expert_
+0000e780: 6761 7465 2c20 6578 7065 7274 5f69 6e64  gate, expert_ind
+0000e790: 6578 203d 2073 656c 662e 746f 706b 2872  ex = self.topk(r
+0000e7a0: 6f75 7465 725f 7072 6f62 2c20 7365 6c66  outer_prob, self
+0000e7b0: 2e6e 756d 5f65 7870 6572 7473 5f63 686f  .num_experts_cho
+0000e7c0: 7365 6e29 2023 2028 6470 2c20 4e2c 206b  sen) # (dp, N, k
+0000e7d0: 2969 6e74 3332 2c20 2864 702c 204e 2c20  )int32, (dp, N, 
+0000e7e0: 6b29 6670 3332 203c 2d2d 2028 6470 2c20  k)fp32 <-- (dp, 
+0000e7f0: 4e2c 2065 7870 6572 745f 6469 6d29 6670  N, expert_dim)fp
+0000e800: 3332 0a20 2020 2020 2020 2064 6973 7061  32.        dispa
+0000e810: 7463 685f 696e 6465 782c 2063 6f6d 6269  tch_index, combi
+0000e820: 6e65 5f69 6e64 6578 2c20 726f 7574 6572  ne_index, router
+0000e830: 5f63 6f65 6666 203d 2073 656c 662e 5f6d  _coeff = self._m
+0000e840: 6173 6b6f 7574 5f6f 7665 7266 6c6f 7765  askout_overflowe
+0000e850: 645f 746f 6b65 6e73 5f73 6f72 7428 6578  d_tokens_sort(ex
+0000e860: 7065 7274 5f69 6e64 6578 2c20 6578 7065  pert_index, expe
+0000e870: 7274 5f67 6174 6529 2023 2028 6470 2c20  rt_gate) # (dp, 
+0000e880: 452c 206e 2969 6e74 3332 2c20 2864 702c  E, n)int32, (dp,
+0000e890: 204e 2c20 6b29 2c20 2864 702c 204e 2c20   N, k), (dp, N, 
+0000e8a0: 6b29 203c 2d2d 2028 6470 2c20 4e2c 206b  k) <-- (dp, N, k
+0000e8b0: 292c 2028 6470 2c20 4e2c 206b 290a 2020  ), (dp, N, k).  
+0000e8c0: 2020 2020 2020 7265 7475 726e 2064 6973        return dis
+0000e8d0: 7061 7463 685f 696e 6465 782c 2063 6f6d  patch_index, com
+0000e8e0: 6269 6e65 5f69 6e64 6578 2c20 726f 7574  bine_index, rout
+0000e8f0: 6572 5f63 6f65 6666 2023 2028 6470 2c20  er_coeff # (dp, 
+0000e900: 452c 206e 2969 6e74 3332 2c20 2864 702c  E, n)int32, (dp,
+0000e910: 204e 2c20 6b29 2c20 2864 702c 204e 2c20   N, k), (dp, N, 
+0000e920: 6b29 0a0a 2020 2020 6465 6620 5f6d 6173  k)..    def _mas
+0000e930: 6b6f 7574 5f6f 7665 7266 6c6f 7765 645f  kout_overflowed_
+0000e940: 746f 6b65 6e73 5f73 6f72 7428 7365 6c66  tokens_sort(self
+0000e950: 2c20 6578 7065 7274 5f69 6e64 6578 2c20  , expert_index, 
+0000e960: 6578 7065 7274 5f67 6174 6529 3a0a 2020  expert_gate):.  
+0000e970: 2020 2020 2020 2222 220a 2020 2020 2020        """.      
+0000e980: 2020 4b65 6570 696e 6720 6f6e 6c79 2074    Keeping only t
+0000e990: 6865 2074 6f6b 656e 7320 7468 6174 2066  he tokens that f
+0000e9a0: 6974 2077 6974 6869 6e20 6578 7065 7274  it within expert
+0000e9b0: 5f63 6170 6163 6974 792e 0a20 2020 2020  _capacity..     
+0000e9c0: 2020 2023 2069 6620 746f 6b65 6e73 5f70     # if tokens_p
+0000e9d0: 6572 5f67 726f 7570 3e31 303a 2073 656c  er_group>10: sel
+0000e9e0: 662e 7072 696e 7428 2272 616e 6765 5f6b  f.print("range_k
+0000e9f0: 6e22 2c20 7261 6e67 655f 6b6e 290a 2020  n", range_kn).  
+0000ea00: 2020 2020 2020 2222 220a 2020 2020 2020        """.      
+0000ea10: 2020 6b20 3d20 7365 6c66 2e6e 756d 5f65    k = self.num_e
+0000ea20: 7870 6572 7473 5f63 686f 7365 6e0a 2020  xperts_chosen.  
+0000ea30: 2020 2020 2020 746f 6b65 6e73 5f70 6572        tokens_per
+0000ea40: 5f67 726f 7570 203d 2073 656c 662e 7368  _group = self.sh
+0000ea50: 6170 6528 6578 7065 7274 5f69 6e64 6578  ape(expert_index
+0000ea60: 295b 315d 0a20 2020 2020 2020 206b 6e20  )[1].        kn 
+0000ea70: 3d20 6b20 2a20 746f 6b65 6e73 5f70 6572  = k * tokens_per
+0000ea80: 5f67 726f 7570 2023 2074 6869 7320 6e20  _group # this n 
+0000ea90: 7265 6665 7273 2074 6f20 4e0a 2020 2020  refers to N.    
+0000eaa0: 2020 2020 6578 7065 7274 5f63 6170 6163      expert_capac
+0000eab0: 6974 7920 3d20 6361 6c63 756c 6174 655f  ity = calculate_
+0000eac0: 6578 7065 7274 5f63 6170 6163 6974 7928  expert_capacity(
+0000ead0: 7365 6c66 2e6e 756d 5f65 7870 6572 7473  self.num_experts
+0000eae0: 5f63 686f 7365 6e2c 2074 6f6b 656e 735f  _chosen, tokens_
+0000eaf0: 7065 725f 6772 6f75 702c 0a20 2020 2020  per_group,.     
+0000eb00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000eb10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000eb20: 2020 2020 2020 2020 2020 2020 2020 2073                 s
+0000eb30: 656c 662e 6361 7061 6369 7479 5f66 6163  elf.capacity_fac
+0000eb40: 746f 722c 2073 656c 662e 6578 7065 7274  tor, self.expert
+0000eb50: 5f64 696d 290a 0a20 2020 2020 2020 2023  _dim)..        #
+0000eb60: 2063 616c 6375 6c61 7465 2063 6f6d 6269   calculate combi
+0000eb70: 6e65 5f69 6e64 6578 2066 726f 6d20 6375  ne_index from cu
+0000eb80: 6d73 756d 0a20 2020 2020 2020 2065 7870  msum.        exp
+0000eb90: 6572 745f 696e 6465 7820 3d20 7365 6c66  ert_index = self
+0000eba0: 2e72 6573 6861 7065 2873 656c 662e 7472  .reshape(self.tr
+0000ebb0: 616e 7370 6f73 655f 3364 2865 7870 6572  anspose_3d(exper
+0000ebc0: 745f 696e 6465 782c 2028 302c 2032 2c20  t_index, (0, 2, 
+0000ebd0: 3129 292c 2028 6578 7065 7274 5f69 6e64  1)), (expert_ind
+0000ebe0: 6578 2e73 6861 7065 5b30 5d2c 202d 3129  ex.shape[0], -1)
+0000ebf0: 2920 2320 2864 702c 206b 4e29 203c 2d2d  ) # (dp, kN) <--
+0000ec00: 2028 6470 2c20 4e2c 206b 2920 6163 636f   (dp, N, k) acco
+0000ec10: 756e 7420 666f 7220 746f 706b 2070 7269  unt for topk pri
+0000ec20: 6f72 6974 790a 2020 2020 2020 2020 6578  ority.        ex
+0000ec30: 7065 7274 5f6d 6173 6b20 3d20 7365 6c66  pert_mask = self
+0000ec40: 2e6f 6e65 686f 745f 3264 2865 7870 6572  .onehot_2d(exper
+0000ec50: 745f 696e 6465 782c 2073 656c 662e 6578  t_index, self.ex
+0000ec60: 7065 7274 5f64 696d 2c20 7365 6c66 2e6f  pert_dim, self.o
+0000ec70: 6e5f 7661 6c75 652c 2073 656c 662e 6f66  n_value, self.of
+0000ec80: 665f 7661 6c75 6529 2023 2028 6470 2c20  f_value) # (dp, 
+0000ec90: 6b4e 2c20 4529 6670 3332 203c 2d2d 2028  kN, E)fp32 <-- (
+0000eca0: 6470 2c20 6b4e 2969 6e74 3332 0a20 2020  dp, kN)int32.   
+0000ecb0: 2020 2020 2070 6f73 6974 696f 6e5f 696e       position_in
+0000ecc0: 5f65 7870 6572 7420 3d20 7365 6c66 2e6d  _expert = self.m
+0000ecd0: 756c 5f33 6428 7365 6c66 2e63 756d 7375  ul_3d(self.cumsu
+0000ece0: 6d28 6578 7065 7274 5f6d 6173 6b2c 2031  m(expert_mask, 1
+0000ecf0: 292c 2065 7870 6572 745f 6d61 736b 2920  ), expert_mask) 
+0000ed00: 2320 2864 702c 206b 4e2c 2045 2966 7031  # (dp, kN, E)fp1
+0000ed10: 3620 3c2d 2d20 2864 702c 206b 4e2c 2045  6 <-- (dp, kN, E
+0000ed20: 2966 7033 322c 2028 6470 2c20 6b4e 2c20  )fp32, (dp, kN, 
+0000ed30: 4529 6670 3332 0a20 2020 2020 2020 2070  E)fp32.        p
+0000ed40: 6f73 6974 696f 6e5f 696e 5f65 7870 6572  osition_in_exper
+0000ed50: 7420 3d20 7365 6c66 2e6d 756c 5f33 6428  t = self.mul_3d(
+0000ed60: 706f 7369 7469 6f6e 5f69 6e5f 6578 7065  position_in_expe
+0000ed70: 7274 2c20 7365 6c66 2e6c 6573 7328 706f  rt, self.less(po
+0000ed80: 7369 7469 6f6e 5f69 6e5f 6578 7065 7274  sition_in_expert
+0000ed90: 2c20 6578 7065 7274 5f63 6170 6163 6974  , expert_capacit
+0000eda0: 792b 3129 2920 2320 2864 702c 206b 4e2c  y+1)) # (dp, kN,
+0000edb0: 2045 2966 7033 3220 3c2d 2d20 2864 702c   E)fp32 <-- (dp,
+0000edc0: 206b 4e2c 2045 2966 7033 322c 2028 6470   kN, E)fp32, (dp
+0000edd0: 2c20 6b4e 2c20 4529 626f 6f6c 2c20 7768  , kN, E)bool, wh
+0000ede0: 6572 6520 303c 3d70 6f73 6974 696f 6e5f  ere 0<=position_
+0000edf0: 696e 5f65 7870 6572 743c 2831 2b6e 290a  in_expert<(1+n).
+0000ee00: 2020 2020 2020 2020 706f 7369 7469 6f6e          position
+0000ee10: 5f69 6e5f 6578 7065 7274 5f32 6420 3d20  _in_expert_2d = 
+0000ee20: 7365 6c66 2e72 6564 7563 655f 7375 6d28  self.reduce_sum(
+0000ee30: 706f 7369 7469 6f6e 5f69 6e5f 6578 7065  position_in_expe
+0000ee40: 7274 2c20 2d31 2920 2320 2864 702c 206b  rt, -1) # (dp, k
+0000ee50: 4e29 6670 3332 203c 2d2d 2028 6470 2c20  N)fp32 <-- (dp, 
+0000ee60: 6b4e 2c20 4529 6670 3332 0a20 2020 2020  kN, E)fp32.     
+0000ee70: 2020 2063 6f6d 6269 6e65 5f69 6e64 6578     combine_index
+0000ee80: 203d 2073 656c 662e 6164 645f 3264 2873   = self.add_2d(s
+0000ee90: 656c 662e 6d75 6c5f 3264 5f31 6428 6578  elf.mul_2d_1d(ex
+0000eea0: 7065 7274 5f69 6e64 6578 2c20 6578 7065  pert_index, expe
+0000eeb0: 7274 5f63 6170 6163 6974 7920 2b20 3129  rt_capacity + 1)
+0000eec0: 2c20 706f 7369 7469 6f6e 5f69 6e5f 6578  , position_in_ex
+0000eed0: 7065 7274 5f32 6429 2023 2028 6470 2c20  pert_2d) # (dp, 
+0000eee0: 6b4e 2966 7033 3220 3c2d 2d20 2864 702c  kN)fp32 <-- (dp,
+0000eef0: 206b 4e29 6670 3332 2c20 2864 702c 206b   kN)fp32, (dp, k
+0000ef00: 4e29 6670 3332 2077 6865 7265 2030 3c3d  N)fp32 where 0<=
+0000ef10: 2063 6f6d 6269 6e65 5f69 6e64 6578 203c   combine_index <
+0000ef20: 452a 2831 2b6e 292c 2063 6f6d 6269 6e65  E*(1+n), combine
+0000ef30: 5f69 6e64 6578 203d 2065 7870 6572 745f  _index = expert_
+0000ef40: 6964 202a 2831 2b6e 2920 2b20 706f 7369  id *(1+n) + posi
+0000ef50: 7469 6f6e 5f69 6e5f 6578 7065 7274 5f32  tion_in_expert_2
+0000ef60: 640a 2020 2020 2020 2020 636f 6d62 696e  d.        combin
+0000ef70: 655f 696e 6465 7820 3d20 7365 6c66 2e74  e_index = self.t
+0000ef80: 7261 6e73 706f 7365 5f33 6428 7365 6c66  ranspose_3d(self
+0000ef90: 2e72 6573 6861 7065 2863 6f6d 6269 6e65  .reshape(combine
+0000efa0: 5f69 6e64 6578 2c20 2863 6f6d 6269 6e65  _index, (combine
+0000efb0: 5f69 6e64 6578 2e73 6861 7065 5b30 5d2c  _index.shape[0],
+0000efc0: 206b 2c20 746f 6b65 6e73 5f70 6572 5f67   k, tokens_per_g
+0000efd0: 726f 7570 2929 2c20 2830 2c20 322c 2031  roup)), (0, 2, 1
+0000efe0: 2929 2023 2028 6470 2c20 4e2c 206b 2920  )) # (dp, N, k) 
+0000eff0: 3c2d 2d20 2864 702c 206b 4e29 2061 6363  <-- (dp, kN) acc
+0000f000: 6f75 6e74 2066 6f72 2074 6f70 6b20 7072  ount for topk pr
+0000f010: 696f 7269 7479 0a20 2020 2020 2020 2077  iority.        w
+0000f020: 6974 6869 6e5f 6361 7061 6369 7479 203d  ithin_capacity =
+0000f030: 2073 656c 662e 6361 7374 2873 656c 662e   self.cast(self.
+0000f040: 6774 2870 6f73 6974 696f 6e5f 696e 5f65  gt(position_in_e
+0000f050: 7870 6572 745f 3264 2c20 3029 2c20 6d73  xpert_2d, 0), ms
+0000f060: 7479 7065 2e66 6c6f 6174 3332 2920 2320  type.float32) # 
+0000f070: 2864 702c 206b 4e29 626f 6f6c 0a0a 2020  (dp, kN)bool..  
+0000f080: 2020 2020 2020 2320 6361 6c63 756c 6174        # calculat
+0000f090: 6520 6469 7370 6174 6368 5f69 6e64 6578  e dispatch_index
+0000f0a0: 2066 726f 6d20 706f 7369 7469 6f6e 5f69   from position_i
+0000f0b0: 6e5f 6578 7065 7274 5f6f 6e65 686f 740a  n_expert_onehot.
+0000f0c0: 2020 2020 2020 2020 7361 6665 5f6b 6e20          safe_kn 
+0000f0d0: 3d20 3220 2a20 6b6e 2023 2066 6163 746f  = 2 * kn # facto
+0000f0e0: 723d 3220 666f 7220 7361 6665 7479 0a20  r=2 for safety. 
+0000f0f0: 2020 2020 2020 2072 616e 6765 5f6b 6e20         range_kn 
+0000f100: 3d20 7365 6c66 2e73 6c69 6365 5f72 616e  = self.slice_ran
+0000f110: 6765 2873 656c 662e 7261 6e67 6532 2c20  ge(self.range2, 
+0000f120: 2830 2c20 3029 2c20 2873 656c 662e 6578  (0, 0), (self.ex
+0000f130: 7065 7274 5f64 696d 2c20 6b6e 292c 2028  pert_dim, kn), (
+0000f140: 312c 2031 2929 2e72 6573 6861 7065 2831  1, 1)).reshape(1
+0000f150: 2c20 7365 6c66 2e65 7870 6572 745f 6469  , self.expert_di
+0000f160: 6d2c 206b 6e29 2023 2831 2c20 452c 206b  m, kn) #(1, E, k
+0000f170: 4e29 2066 7033 3220 3c2d 2d20 2845 2c20  N) fp32 <-- (E, 
+0000f180: 3133 3130 3732 290a 2020 2020 2020 2020  131072).        
+0000f190: 7365 6c65 6374 203d 2073 656c 662e 7472  select = self.tr
+0000f1a0: 616e 7370 6f73 655f 3364 2865 7870 6572  anspose_3d(exper
+0000f1b0: 745f 6d61 736b 2c20 2830 2c20 322c 2031  t_mask, (0, 2, 1
+0000f1c0: 2929 2023 2028 6470 2c20 452c 206b 4e29  )) # (dp, E, kN)
+0000f1d0: 2066 7033 3220 3c2d 2d20 2864 702c 206b   fp32 <-- (dp, k
+0000f1e0: 4e2c 2045 2920 6670 3332 0a20 2020 2020  N, E) fp32.     
+0000f1f0: 2020 2064 6973 7061 7463 685f 696e 6465     dispatch_inde
+0000f200: 785f 7261 7720 3d20 7365 6c66 2e61 6464  x_raw = self.add
+0000f210: 5f33 6428 7365 6c66 2e6d 756c 5f72 616e  _3d(self.mul_ran
+0000f220: 6765 2873 656c 6563 742c 2072 616e 6765  ge(select, range
+0000f230: 5f6b 6e29 2c20 7365 6c66 2e6d 756c 5f72  _kn), self.mul_r
+0000f240: 616e 6765 2873 656c 662e 7375 625f 7261  ange(self.sub_ra
+0000f250: 6e67 6528 312c 2073 656c 6563 7429 2c20  nge(1, select), 
+0000f260: 7365 6c66 2e61 6464 5f72 616e 6765 2872  self.add_range(r
+0000f270: 616e 6765 5f6b 6e2c 2073 6166 655f 6b6e  ange_kn, safe_kn
+0000f280: 2929 2920 2320 2864 702c 2045 2c20 6b4e  ))) # (dp, E, kN
+0000f290: 2920 3c2d 2d20 2864 702c 2045 2c20 6b4e  ) <-- (dp, E, kN
+0000f2a0: 2920 6670 3332 0a20 2020 2020 2020 2064  ) fp32.        d
+0000f2b0: 6973 7061 7463 685f 696e 6465 782c 205f  ispatch_index, _
+0000f2c0: 203d 2073 656c 662e 736f 7274 5f72 616e   = self.sort_ran
+0000f2d0: 6765 2864 6973 7061 7463 685f 696e 6465  ge(dispatch_inde
+0000f2e0: 785f 7261 7729 2023 2028 6470 2c20 452c  x_raw) # (dp, E,
+0000f2f0: 206b 2920 3c2d 2d20 2864 702c 2045 2c20   k) <-- (dp, E, 
+0000f300: 6b4e efbc 890a 2020 2020 2020 2020 6469  kN....        di
+0000f310: 7370 6174 6368 5f69 6e64 6578 203d 2073  spatch_index = s
+0000f320: 656c 662e 736c 6963 6528 6469 7370 6174  elf.slice(dispat
+0000f330: 6368 5f69 6e64 6578 2c20 2830 2c20 302c  ch_index, (0, 0,
+0000f340: 2030 292c 2028 6469 7370 6174 6368 5f69   0), (dispatch_i
+0000f350: 6e64 6578 2e73 6861 7065 5b30 5d2c 2064  ndex.shape[0], d
+0000f360: 6973 7061 7463 685f 696e 6465 782e 7368  ispatch_index.sh
+0000f370: 6170 655b 315d 2c20 6578 7065 7274 5f63  ape[1], expert_c
+0000f380: 6170 6163 6974 7929 2c20 2831 2c20 312c  apacity), (1, 1,
+0000f390: 2031 2929 2023 2028 6470 2c20 452c 206e   1)) # (dp, E, n
+0000f3a0: 2920 3c2d 2d20 2864 702c 2045 2c20 6b4e  ) <-- (dp, E, kN
+0000f3b0: 2920 6670 3332 0a20 2020 2020 2020 2069  ) fp32.        i
+0000f3c0: 735f 7361 6665 203d 2073 656c 662e 6c65  s_safe = self.le
+0000f3d0: 7373 2864 6973 7061 7463 685f 696e 6465  ss(dispatch_inde
+0000f3e0: 782c 2073 6166 655f 6b6e 2920 2320 2864  x, safe_kn) # (d
+0000f3f0: 702c 2045 2c20 6e29 2062 6f6f 6c0a 2020  p, E, n) bool.  
+0000f400: 2020 2020 2020 6469 7370 6174 6368 5f69        dispatch_i
+0000f410: 6e64 6578 203d 2073 656c 662e 6164 645f  ndex = self.add_
+0000f420: 6f6e 6528 7365 6c66 2e6d 6f64 2864 6973  one(self.mod(dis
+0000f430: 7061 7463 685f 696e 6465 782c 2074 6f6b  patch_index, tok
+0000f440: 656e 735f 7065 725f 6772 6f75 7029 2c20  ens_per_group), 
+0000f450: 3129 2023 2028 6470 2c20 452c 206e 2920  1) # (dp, E, n) 
+0000f460: 6670 3332 0a20 2020 2020 2020 2064 6973  fp32.        dis
+0000f470: 7061 7463 685f 696e 6465 7820 3d20 7365  patch_index = se
+0000f480: 6c66 2e6d 756c 5f33 6428 6469 7370 6174  lf.mul_3d(dispat
+0000f490: 6368 5f69 6e64 6578 2c20 6973 5f73 6166  ch_index, is_saf
+0000f4a0: 6529 2023 2028 6470 2c20 452c 206e 2920  e) # (dp, E, n) 
+0000f4b0: 6670 3332 0a0a 2020 2020 2020 2020 2320  fp32..        # 
+0000f4c0: 7265 7475 726e 0a20 2020 2020 2020 2064  return.        d
+0000f4d0: 6973 7061 7463 685f 696e 6465 7820 3d20  ispatch_index = 
+0000f4e0: 7365 6c66 2e63 6173 7428 6469 7370 6174  self.cast(dispat
+0000f4f0: 6368 5f69 6e64 6578 2c20 6d73 7479 7065  ch_index, mstype
+0000f500: 2e69 6e74 3332 290a 2020 2020 2020 2020  .int32).        
+0000f510: 636f 6d62 696e 655f 696e 6465 7820 3d20  combine_index = 
+0000f520: 7365 6c66 2e63 6173 7428 636f 6d62 696e  self.cast(combin
+0000f530: 655f 696e 6465 782c 206d 7374 7970 652e  e_index, mstype.
+0000f540: 696e 7433 3229 0a20 2020 2020 2020 2072  int32).        r
+0000f550: 6f75 7465 725f 636f 6566 665f 7261 7720  outer_coeff_raw 
+0000f560: 3d20 7365 6c66 2e6d 756c 5f33 6428 6578  = self.mul_3d(ex
+0000f570: 7065 7274 5f67 6174 652c 2073 656c 662e  pert_gate, self.
+0000f580: 7472 616e 7370 6f73 655f 3364 2873 656c  transpose_3d(sel
+0000f590: 662e 7265 7368 6170 6528 7769 7468 696e  f.reshape(within
+0000f5a0: 5f63 6170 6163 6974 792c 2028 7769 7468  _capacity, (with
+0000f5b0: 696e 5f63 6170 6163 6974 792e 7368 6170  in_capacity.shap
+0000f5c0: 655b 305d 2c20 6b2c 2074 6f6b 656e 735f  e[0], k, tokens_
+0000f5d0: 7065 725f 6772 6f75 7029 292c 2028 302c  per_group)), (0,
+0000f5e0: 2032 2c20 3129 2929 2023 2061 7070 6c79   2, 1))) # apply
+0000f5f0: 2077 6974 6869 6e5f 6361 7061 6369 7479   within_capacity
+0000f600: 2028 6470 2c20 4e2c 206b 2920 3c2d 2d20   (dp, N, k) <-- 
+0000f610: 2864 702c 204e 2c20 6b29 2c20 2864 702c  (dp, N, k), (dp,
+0000f620: 204e 2c20 6b29 203c 2d2d 2020 2864 702c   N, k) <--  (dp,
+0000f630: 206b 4e29 0a20 2020 2020 2020 2072 6f75   kN).        rou
+0000f640: 7465 725f 636f 6566 6620 3d20 7365 6c66  ter_coeff = self
+0000f650: 2e5f 6e6f 726d 616c 697a 6528 726f 7574  ._normalize(rout
+0000f660: 6572 5f63 6f65 6666 5f72 6177 2920 2320  er_coeff_raw) # 
+0000f670: 2864 702c 204e 2c20 6b29 203c 2d2d 2028  (dp, N, k) <-- (
+0000f680: 6470 2c20 4e2c 206b 290a 2020 2020 2020  dp, N, k).      
+0000f690: 2020 726f 7574 6572 5f63 6f65 6666 203d    router_coeff =
+0000f6a0: 2073 656c 662e 6361 7374 2872 6f75 7465   self.cast(route
+0000f6b0: 725f 636f 6566 662c 206d 7374 7970 652e  r_coeff, mstype.
+0000f6c0: 666c 6f61 7431 3629 2023 2028 6470 2c20  float16) # (dp, 
+0000f6d0: 4e2c 206b 2920 3c2d 2d20 2864 702c 204e  N, k) <-- (dp, N
+0000f6e0: 2c20 6b29 0a20 2020 2020 2020 2072 6574  , k).        ret
+0000f6f0: 7572 6e20 6469 7370 6174 6368 5f69 6e64  urn dispatch_ind
+0000f700: 6578 2c20 636f 6d62 696e 655f 696e 6465  ex, combine_inde
+0000f710: 782c 2072 6f75 7465 725f 636f 6566 6620  x, router_coeff 
+0000f720: 2320 2864 702c 2045 2c20 6e29 2c20 2864  # (dp, E, n), (d
+0000f730: 702c 204e 2c20 6b29 2c20 2864 702c 204e  p, N, k), (dp, N
+0000f740: 2c20 6b29 0a0a 0a20 2020 2064 6566 205f  , k)...    def _
+0000f750: 6e6f 726d 616c 697a 6528 7365 6c66 2c20  normalize(self, 
+0000f760: 726f 7574 6572 5f63 6f65 6666 5f72 6177  router_coeff_raw
+0000f770: 293a 0a20 2020 2020 2020 2072 6f75 7465  ):.        route
+0000f780: 725f 636f 6566 665f 7375 6d20 3d20 7365  r_coeff_sum = se
+0000f790: 6c66 2e72 6564 7563 655f 7375 6d5f 6b65  lf.reduce_sum_ke
+0000f7a0: 6570 2872 6f75 7465 725f 636f 6566 665f  ep(router_coeff_
+0000f7b0: 7261 772c 2032 2920 2320 2864 702c 204e  raw, 2) # (dp, N
+0000f7c0: 2c20 3129 203c 2d2d 2028 6470 2c20 4e2c  , 1) <-- (dp, N,
+0000f7d0: 206b 290a 2020 2020 2020 2020 726f 7574   k).        rout
+0000f7e0: 6572 5f63 6f65 6666 203d 2073 656c 662e  er_coeff = self.
+0000f7f0: 6469 765f 3364 2872 6f75 7465 725f 636f  div_3d(router_co
+0000f800: 6566 665f 7261 772c 2073 656c 662e 6164  eff_raw, self.ad
+0000f810: 645f 6570 7328 726f 7574 6572 5f63 6f65  d_eps(router_coe
+0000f820: 6666 5f73 756d 2c20 3165 2d39 2929 2023  ff_sum, 1e-9)) #
+0000f830: 2028 6470 2c20 4e2c 206b 2920 3c2d 2d20   (dp, N, k) <-- 
+0000f840: 2864 702c 204e 2c20 6b29 2028 6470 2c20  (dp, N, k) (dp, 
+0000f850: 4e2c 2031 290a 2020 2020 2020 2020 7265  N, 1).        re
+0000f860: 7475 726e 2072 6f75 7465 725f 636f 6566  turn router_coef
+0000f870: 6620 2320 2864 702c 204e 2c20 6b29 0a    f # (dp, N, k).
```

## mindformers/modules/transformer/op_parallel_config.py

```diff
@@ -58,14 +58,15 @@
 
     def __init__(self, data_parallel=1, model_parallel=1, expert_parallel=1,
                  use_seq_parallel=False, select_recompute=False):
         Validator.check_positive_int(data_parallel, "data_parallel")
         Validator.check_positive_int(model_parallel, "model_parallel")
         Validator.check_positive_int(expert_parallel, "expert_parallel")
         Validator.check_bool(use_seq_parallel, "use_seq_parallel")
+        Validator.check_bool(select_recompute, "select_recompute")
         self._dpmp = OpParallelConfig(data_parallel=data_parallel,
                                       model_parallel=model_parallel,
                                       use_seq_parallel=use_seq_parallel,
                                       select_recompute=select_recompute)
         self.expert_parallel = expert_parallel
         self.use_seq_parallel = use_seq_parallel
         self.select_recompute = select_recompute
@@ -118,14 +119,15 @@
             >>> config=OpParallelConfig(data_parallel=1, model_parallel=1)
     """
 
     def __init__(self, data_parallel=1, model_parallel=1, use_seq_parallel=False, select_recompute=False):
         Validator.check_positive_int(data_parallel, "data_parallel")
         Validator.check_positive_int(model_parallel, "model_parallel")
         Validator.check_bool(use_seq_parallel, "use_seq_parallel")
+        Validator.check_bool(select_recompute, "select_recompute")
         self.data_parallel = data_parallel
         self.model_parallel = model_parallel
         self.use_seq_parallel = use_seq_parallel
         self.select_recompute = select_recompute
 
     @property
     def data_parallel(self):
@@ -192,15 +194,14 @@
     def pipeline_stage(self):
         return self._pipeline_stage
 
     @pipeline_stage.setter
     def pipeline_stage(self, value):
         Validator.check_positive_int(value, "pipeline_stage")
         self._pipeline_stage = value
-        context.set_auto_parallel_context(pipeline_stages=value)
 
     @property
     def micro_batch_num(self):
         return self._micro_batch_num
 
     @micro_batch_num.setter
     def micro_batch_num(self, value):
```

## mindformers/modules/transformer/transformer.py

```diff
@@ -94,13240 +94,13315 @@
 000005d0: 6d6f 6465 0a66 726f 6d20 6d69 6e64 7370  mode.from mindsp
 000005e0: 6f72 652e 636f 6e74 6578 7420 696d 706f  ore.context impo
 000005f0: 7274 2050 6172 616c 6c65 6c4d 6f64 650a  rt ParallelMode.
 00000600: 0a74 7279 3a0a 2020 2020 6672 6f6d 206d  .try:.    from m
 00000610: 696e 6473 706f 7265 2e6f 7073 2e6f 7065  indspore.ops.ope
 00000620: 7261 7469 6f6e 732e 6e6e 5f6f 7073 2069  rations.nn_ops i
 00000630: 6d70 6f72 7420 5072 6f6d 7074 466c 6173  mport PromptFlas
-00000640: 6841 7474 656e 7469 6f6e 0a0a 2020 2020  hAttention..    
-00000650: 5052 4f4d 5054 464c 4153 4841 5454 454e  PROMPTFLASHATTEN
-00000660: 5449 4f4e 5f56 414c 4944 203d 2054 7275  TION_VALID = Tru
-00000670: 650a 6578 6365 7074 2049 6d70 6f72 7445  e.except ImportE
-00000680: 7272 6f72 3a0a 2020 2020 5052 4f4d 5054  rror:.    PROMPT
-00000690: 464c 4153 4841 5454 454e 5449 4f4e 5f56  FLASHATTENTION_V
-000006a0: 414c 4944 203d 2046 616c 7365 0a0a 6672  ALID = False..fr
-000006b0: 6f6d 206d 696e 6466 6f72 6d65 7273 2e6d  om mindformers.m
-000006c0: 6f64 756c 6573 2e66 6c61 7368 5f61 7474  odules.flash_att
-000006d0: 656e 7469 6f6e 2069 6d70 6f72 7420 466c  ention import Fl
-000006e0: 6173 6841 7474 656e 7469 6f6e 0a66 726f  ashAttention.fro
-000006f0: 6d20 6d69 6e64 666f 726d 6572 732e 6d6f  m mindformers.mo
-00000700: 6475 6c65 732e 6c61 7965 7273 2069 6d70  dules.layers imp
-00000710: 6f72 7420 4c61 7965 724e 6f72 6d2c 204c  ort LayerNorm, L
-00000720: 696e 6561 722c 205c 0a20 2020 205f 6172  inear, \.    _ar
-00000730: 6773 5f74 7970 655f 7661 6c69 6461 746f  gs_type_validato
-00000740: 725f 6368 6563 6b2c 205f 7661 6c69 645f  r_check, _valid_
-00000750: 7479 7065 5f63 6865 636b 732c 205f 7661  type_checks, _va
-00000760: 6c69 645f 7661 6c75 655f 6368 6563 6b73  lid_value_checks
-00000770: 2c20 5c0a 2020 2020 5f63 6865 636b 5f70  , \.    _check_p
-00000780: 6173 745f 6e6f 6e65 5f69 6e70 7574 5f6e  ast_none_input_n
-00000790: 6f6e 652c 205f 6368 6563 6b5f 696e 7075  one, _check_inpu
-000007a0: 745f 6474 7970 650a 6672 6f6d 206d 696e  t_dtype.from min
-000007b0: 6466 6f72 6d65 7273 2e6d 6f64 756c 6573  dformers.modules
-000007c0: 2e74 7261 6e73 666f 726d 6572 2e6f 705f  .transformer.op_
-000007d0: 7061 7261 6c6c 656c 5f63 6f6e 6669 6720  parallel_config 
-000007e0: 696d 706f 7274 2064 6566 6175 6c74 5f64  import default_d
-000007f0: 706d 705f 636f 6e66 6967 2c20 5f50 6970  pmp_config, _Pip
-00000800: 654c 696e 6543 6f6e 6669 672c 204f 7050  eLineConfig, OpP
-00000810: 6172 616c 6c65 6c43 6f6e 6669 672c 205c  arallelConfig, \
-00000820: 0a20 2020 205f 436f 6e66 6967 2c20 5f63  .    _Config, _c
-00000830: 6865 636b 5f63 6f6e 6669 672c 204d 6f45  heck_config, MoE
-00000840: 5061 7261 6c6c 656c 436f 6e66 6967 0a66  ParallelConfig.f
-00000850: 726f 6d20 6d69 6e64 666f 726d 6572 732e  rom mindformers.
-00000860: 6d6f 6475 6c65 732e 7472 616e 7366 6f72  modules.transfor
-00000870: 6d65 722e 6d6f 6520 696d 706f 7274 2064  mer.moe import d
-00000880: 6566 6175 6c74 5f6d 6f65 5f63 6f6e 6669  efault_moe_confi
-00000890: 672c 204d 6f45 2c20 5f63 6865 636b 5f6d  g, MoE, _check_m
-000008a0: 6f65 5f63 6f6e 6669 670a 6672 6f6d 206d  oe_config.from m
-000008b0: 696e 6466 6f72 6d65 7273 2e76 6572 7369  indformers.versi
-000008c0: 6f6e 5f63 6f6e 7472 6f6c 2069 6d70 6f72  on_control impor
-000008d0: 7420 6765 745f 6472 6f70 6f75 742c 2063  t get_dropout, c
-000008e0: 686f 6f73 655f 666c 6173 685f 6174 7465  hoose_flash_atte
-000008f0: 6e74 696f 6e5f 6474 7970 652c 205c 0a20  ntion_dtype, \. 
-00000900: 2020 2063 6865 636b 5f76 616c 6964 5f66     check_valid_f
-00000910: 6c61 7368 5f61 7474 656e 7469 6f6e 0a0a  lash_attention..
-00000920: 6672 6f6d 206d 696e 6466 6f72 6d65 7273  from mindformers
-00000930: 2e74 6f6f 6c73 2e6c 6f67 6765 7220 696d  .tools.logger im
-00000940: 706f 7274 205f 4c6f 6741 6374 696f 6e4f  port _LogActionO
-00000950: 6e63 650a 6672 6f6d 206d 696e 6466 6f72  nce.from mindfor
-00000960: 6d65 7273 2e74 6f6f 6c73 2e6c 6f67 6765  mers.tools.logge
-00000970: 7220 696d 706f 7274 206c 6f67 6765 7220  r import logger 
-00000980: 6173 206c 6f67 0a0a 5f5f 616c 6c5f 5f20  as log..__all__ 
-00000990: 3d20 5b0a 2020 2020 2241 7474 656e 7469  = [.    "Attenti
-000009a0: 6f6e 4d61 736b 222c 0a20 2020 2022 4174  onMask",.    "At
-000009b0: 7465 6e74 696f 6e4d 6173 6b48 4622 2c0a  tentionMaskHF",.
-000009c0: 2020 2020 224c 6f77 6572 5472 6961 6e67      "LowerTriang
-000009d0: 756c 6172 4d61 736b 5769 7468 4479 6e61  ularMaskWithDyna
-000009e0: 6d69 6322 2c0a 2020 2020 2256 6f63 6162  mic",.    "Vocab
-000009f0: 456d 6265 6464 696e 6722 2c0a 2020 2020  Embedding",.    
-00000a00: 224d 756c 7469 4865 6164 4174 7465 6e74  "MultiHeadAttent
-00000a10: 696f 6e22 2c0a 2020 2020 2246 6565 6446  ion",.    "FeedF
-00000a20: 6f72 7761 7264 222c 0a20 2020 2022 5472  orward",.    "Tr
-00000a30: 616e 7366 6f72 6d65 7245 6e63 6f64 6572  ansformerEncoder
-00000a40: 222c 0a20 2020 2022 5472 616e 7366 6f72  ",.    "Transfor
-00000a50: 6d65 7244 6563 6f64 6572 222c 0a20 2020  merDecoder",.   
-00000a60: 2022 5472 616e 7366 6f72 6d65 7245 6e63   "TransformerEnc
-00000a70: 6f64 6572 4c61 7965 7222 2c0a 2020 2020  oderLayer",.    
-00000a80: 2254 7261 6e73 666f 726d 6572 4465 636f  "TransformerDeco
-00000a90: 6465 724c 6179 6572 222c 0a20 2020 2022  derLayer",.    "
-00000aa0: 5472 616e 7366 6f72 6d65 7222 2c0a 2020  Transformer",.  
-00000ab0: 2020 2254 7261 6e73 666f 726d 6572 4f70    "TransformerOp
-00000ac0: 5061 7261 6c6c 656c 436f 6e66 6967 222c  ParallelConfig",
-00000ad0: 0a20 2020 2022 456d 6265 6464 696e 674f  .    "EmbeddingO
-00000ae0: 7050 6172 616c 6c65 6c43 6f6e 6669 6722  pParallelConfig"
-00000af0: 2c0a 2020 2020 2254 7261 6e73 666f 726d  ,.    "Transform
-00000b00: 6572 5265 636f 6d70 7574 6543 6f6e 6669  erRecomputeConfi
-00000b10: 6722 5d0a 0a0a 636c 6173 7320 456d 6265  g"]...class Embe
-00000b20: 6464 696e 674f 7050 6172 616c 6c65 6c43  ddingOpParallelC
-00000b30: 6f6e 6669 6728 5f43 6f6e 6669 6729 3a0a  onfig(_Config):.
-00000b40: 2020 2020 7222 2222 0a20 2020 2020 2020      r""".       
-00000b50: 2054 6865 2070 6172 616c 6c65 6c20 636f   The parallel co
-00000b60: 6e66 6967 206f 6620 3a63 6c61 7373 3a60  nfig of :class:`
-00000b70: 566f 6361 6245 6d62 6564 6469 6e67 600a  VocabEmbedding`.
-00000b80: 2020 2020 2020 2020 666f 7220 7468 6520          for the 
-00000b90: 7365 7474 696e 6720 6461 7461 2070 6172  setting data par
-00000ba0: 616c 6c65 6c20 6f72 206d 6f64 656c 2070  allel or model p
-00000bb0: 6172 616c 6c65 6c20 666f 7220 7468 6520  arallel for the 
-00000bc0: 656d 6265 6464 696e 6720 7461 626c 652e  embedding table.
-00000bd0: 0a0a 2020 2020 2020 2020 4172 6773 3a0a  ..        Args:.
-00000be0: 2020 2020 2020 2020 2020 2020 6461 7461              data
-00000bf0: 5f70 6172 616c 6c65 6c28 696e 7429 3a20  _parallel(int): 
-00000c00: 5468 6520 6461 7461 2070 6172 616c 6c65  The data paralle
-00000c10: 6c20 7761 792e 2054 6865 2069 6e70 7574  l way. The input
-00000c20: 2064 6174 6120 7769 6c6c 2062 6520 736c   data will be sl
-00000c30: 6963 6564 2069 6e74 6f20 6e20 7061 7274  iced into n part
-00000c40: 7320 666f 7220 656d 6265 6464 696e 6720  s for embedding 
-00000c50: 6c61 7965 720a 2020 2020 2020 2020 2020  layer.          
-00000c60: 2020 2020 2020 6163 636f 7264 696e 6720        according 
-00000c70: 746f 2074 6869 7320 7661 6c75 652e 2044  to this value. D
-00000c80: 6566 6175 6c74 3a20 312e 0a20 2020 2020  efault: 1..     
-00000c90: 2020 2020 2020 206d 6f64 656c 5f70 6172         model_par
-00000ca0: 616c 6c65 6c28 696e 7429 3a20 5468 6520  allel(int): The 
-00000cb0: 6d6f 6465 6c20 7061 7261 6c6c 656c 2077  model parallel w
-00000cc0: 6179 2e20 5468 6520 656d 6265 6464 696e  ay. The embeddin
-00000cd0: 6720 7461 626c 6520 7061 7261 6d65 7465  g table paramete
-00000ce0: 7273 0a20 2020 2020 2020 2020 2020 2020  rs.             
-00000cf0: 2020 2077 696c 6c20 6265 2073 6c69 6365     will be slice
-00000d00: 6420 6174 2030 2d74 6820 6178 6973 2061  d at 0-th axis a
-00000d10: 6363 6f72 6469 6e67 2074 6f20 7468 6520  ccording to the 
-00000d20: 6d6f 6465 6c20 7061 7261 6c6c 656c 2077  model parallel w
-00000d30: 6179 2e20 4465 6661 756c 743a 2031 2e0a  ay. Default: 1..
-00000d40: 2020 2020 2020 2020 2020 2020 766f 6361              voca
-00000d50: 625f 656d 625f 6470 2862 6f6f 6c29 3a20  b_emb_dp(bool): 
-00000d60: 5368 6172 6420 656d 6265 6464 696e 6720  Shard embedding 
-00000d70: 696e 206d 6f64 656c 2070 6172 616c 6c65  in model paralle
-00000d80: 6c20 6f72 2064 6174 6120 7061 7261 6c6c  l or data parall
-00000d90: 656c 2e20 4966 2054 7275 652c 2074 6865  el. If True, the
-00000da0: 2065 6d62 6564 6469 6e67 206c 6f6f 6b75   embedding looku
-00000db0: 700a 2020 2020 2020 2020 2020 2020 2020  p.              
-00000dc0: 2020 7769 6c6c 2062 6520 6120 6461 7461    will be a data
-00000dd0: 2070 6172 616c 6c65 6c20 7374 796c 6520   parallel style 
-00000de0: 7472 6169 6e69 6e67 2061 6e64 206d 6f64  training and mod
-00000df0: 656c 5f70 6172 616c 6c65 6c20 7661 6c75  el_parallel valu
-00000e00: 6520 7769 6c6c 2062 6520 6967 6e6f 7265  e will be ignore
-00000e10: 642e 2020 4966 2066 616c 7365 2c20 7468  d.  If false, th
-00000e20: 650a 2020 2020 2020 2020 2020 2020 2020  e.              
-00000e30: 2020 656d 6265 6464 696e 6720 7461 626c    embedding tabl
-00000e40: 6520 7769 6c6c 2062 6520 7368 6172 6465  e will be sharde
-00000e50: 6420 696e 746f 206e 2070 6172 7473 2061  d into n parts a
-00000e60: 7420 7468 6520 302d 7468 2064 696d 656e  t the 0-th dimen
-00000e70: 7369 6f6e 2072 6f77 2073 6c69 6365 206f  sion row slice o
-00000e80: 6620 7468 6520 656d 6265 6464 696e 6720  f the embedding 
-00000e90: 7461 626c 652c 0a20 2020 2020 2020 2020  table,.         
-00000ea0: 2020 2020 2020 2077 6865 7265 2074 6865         where the
-00000eb0: 206e 2069 7320 7468 6520 6d6f 6465 6c20   n is the model 
-00000ec0: 7061 7261 6c6c 656c 2077 6179 2064 6574  parallel way det
-00000ed0: 6572 6d69 6e65 6420 6279 2074 6869 7320  ermined by this 
-00000ee0: 7061 7261 6d65 7465 722e 2044 6566 6175  parameter. Defau
-00000ef0: 6c74 3a20 5472 7565 0a0a 2020 2020 2020  lt: True..      
-00000f00: 2020 5375 7070 6f72 7465 6420 506c 6174    Supported Plat
-00000f10: 666f 726d 733a 0a20 2020 2020 2020 2020  forms:.         
-00000f20: 2020 2060 6041 7363 656e 6460 6020 6060     ``Ascend`` ``
-00000f30: 4750 5560 600a 0a20 2020 2020 2020 2045  GPU``..        E
-00000f40: 7861 6d70 6c65 733a 0a20 2020 2020 2020  xamples:.       
-00000f50: 2020 2020 203e 3e3e 2066 726f 6d20 6d69       >>> from mi
-00000f60: 6e64 666f 726d 6572 732e 6d6f 6475 6c65  ndformers.module
-00000f70: 732e 7472 616e 7366 6f72 6d65 7220 696d  s.transformer im
-00000f80: 706f 7274 2045 6d62 6564 6469 6e67 4f70  port EmbeddingOp
-00000f90: 5061 7261 6c6c 656c 436f 6e66 6967 0a20  ParallelConfig. 
-00000fa0: 2020 2020 2020 2020 2020 203e 3e3e 2063             >>> c
-00000fb0: 6f6e 6669 673d 456d 6265 6464 696e 674f  onfig=EmbeddingO
-00000fc0: 7050 6172 616c 6c65 6c43 6f6e 6669 6728  pParallelConfig(
-00000fd0: 6461 7461 5f70 6172 616c 6c65 6c3d 312c  data_parallel=1,
-00000fe0: 206d 6f64 656c 5f70 6172 616c 6c65 6c3d   model_parallel=
-00000ff0: 312c 2076 6f63 6162 5f65 6d62 5f64 703d  1, vocab_emb_dp=
-00001000: 5472 7565 290a 2020 2020 2222 220a 0a20  True).    """.. 
-00001010: 2020 2064 6566 205f 5f69 6e69 745f 5f28     def __init__(
-00001020: 7365 6c66 2c20 6461 7461 5f70 6172 616c  self, data_paral
-00001030: 6c65 6c3d 312c 206d 6f64 656c 5f70 6172  lel=1, model_par
-00001040: 616c 6c65 6c3d 312c 0a20 2020 2020 2020  allel=1,.       
-00001050: 2020 2020 2020 2020 2020 7573 655f 7365            use_se
-00001060: 715f 7061 7261 6c6c 656c 3d46 616c 7365  q_parallel=False
-00001070: 2c20 7365 6c65 6374 5f72 6563 6f6d 7075  , select_recompu
-00001080: 7465 3d46 616c 7365 2c0a 2020 2020 2020  te=False,.      
-00001090: 2020 2020 2020 2020 2020 2076 6f63 6162             vocab
-000010a0: 5f65 6d62 5f64 703d 5472 7565 293a 0a20  _emb_dp=True):. 
-000010b0: 2020 2020 2020 2073 656c 662e 5f64 705f         self._dp_
-000010c0: 6d70 5f63 6f6e 6669 6720 3d20 4f70 5061  mp_config = OpPa
-000010d0: 7261 6c6c 656c 436f 6e66 6967 2864 6174  rallelConfig(dat
-000010e0: 615f 7061 7261 6c6c 656c 3d64 6174 615f  a_parallel=data_
-000010f0: 7061 7261 6c6c 656c 2c0a 2020 2020 2020  parallel,.      
-00001100: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001110: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001120: 2020 2020 2020 2020 7573 655f 7365 715f          use_seq_
-00001130: 7061 7261 6c6c 656c 3d75 7365 5f73 6571  parallel=use_seq
-00001140: 5f70 6172 616c 6c65 6c2c 0a20 2020 2020  _parallel,.     
-00001150: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001160: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001170: 2020 2020 2020 2020 206d 6f64 656c 5f70           model_p
-00001180: 6172 616c 6c65 6c3d 6d6f 6465 6c5f 7061  arallel=model_pa
-00001190: 7261 6c6c 656c 2c0a 2020 2020 2020 2020  rallel,.        
-000011a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000640: 6841 7474 656e 7469 6f6e 0a20 2020 2050  hAttention.    P
+00000650: 524f 4d50 5446 4c41 5348 4154 5445 4e54  ROMPTFLASHATTENT
+00000660: 494f 4e5f 5641 4c49 4420 3d20 5472 7565  ION_VALID = True
+00000670: 0a65 7863 6570 7420 496d 706f 7274 4572  .except ImportEr
+00000680: 726f 723a 0a20 2020 2050 524f 4d50 5446  ror:.    PROMPTF
+00000690: 4c41 5348 4154 5445 4e54 494f 4e5f 5641  LASHATTENTION_VA
+000006a0: 4c49 4420 3d20 4661 6c73 650a 0a74 7279  LID = False..try
+000006b0: 3a0a 2020 2020 6672 6f6d 206d 696e 6473  :.    from minds
+000006c0: 706f 7265 2e6f 7073 2e6f 7065 7261 7469  pore.ops.operati
+000006d0: 6f6e 732e 6e6e 5f6f 7073 2069 6d70 6f72  ons.nn_ops impor
+000006e0: 7420 496e 6372 6546 6c61 7368 4174 7465  t IncreFlashAtte
+000006f0: 6e74 696f 6e0a 2020 2020 494e 4352 4546  ntion.    INCREF
+00000700: 4c41 5348 4154 5445 4e54 494f 4e5f 5641  LASHATTENTION_VA
+00000710: 4c49 4420 3d20 5472 7565 0a65 7863 6570  LID = True.excep
+00000720: 7420 496d 706f 7274 4572 726f 723a 0a20  t ImportError:. 
+00000730: 2020 2049 4e43 5245 464c 4153 4841 5454     INCREFLASHATT
+00000740: 454e 5449 4f4e 5f56 414c 4944 203d 2046  ENTION_VALID = F
+00000750: 616c 7365 0a0a 6672 6f6d 206d 696e 6466  alse..from mindf
+00000760: 6f72 6d65 7273 2e6d 6f64 756c 6573 2e66  ormers.modules.f
+00000770: 6c61 7368 5f61 7474 656e 7469 6f6e 2069  lash_attention i
+00000780: 6d70 6f72 7420 466c 6173 6841 7474 656e  mport FlashAtten
+00000790: 7469 6f6e 0a66 726f 6d20 6d69 6e64 666f  tion.from mindfo
+000007a0: 726d 6572 732e 6d6f 6475 6c65 732e 6c61  rmers.modules.la
+000007b0: 7965 7273 2069 6d70 6f72 7420 4c61 7965  yers import Laye
+000007c0: 724e 6f72 6d2c 204c 696e 6561 722c 205c  rNorm, Linear, \
+000007d0: 0a20 2020 205f 6172 6773 5f74 7970 655f  .    _args_type_
+000007e0: 7661 6c69 6461 746f 725f 6368 6563 6b2c  validator_check,
+000007f0: 205f 7661 6c69 645f 7479 7065 5f63 6865   _valid_type_che
+00000800: 636b 732c 205f 7661 6c69 645f 7661 6c75  cks, _valid_valu
+00000810: 655f 6368 6563 6b73 2c20 5c0a 2020 2020  e_checks, \.    
+00000820: 5f63 6865 636b 5f70 6173 745f 6e6f 6e65  _check_past_none
+00000830: 5f69 6e70 7574 5f6e 6f6e 652c 205f 6368  _input_none, _ch
+00000840: 6563 6b5f 696e 7075 745f 6474 7970 650a  eck_input_dtype.
+00000850: 6672 6f6d 206d 696e 6466 6f72 6d65 7273  from mindformers
+00000860: 2e6d 6f64 756c 6573 2e74 7261 6e73 666f  .modules.transfo
+00000870: 726d 6572 2e6f 705f 7061 7261 6c6c 656c  rmer.op_parallel
+00000880: 5f63 6f6e 6669 6720 696d 706f 7274 2064  _config import d
+00000890: 6566 6175 6c74 5f64 706d 705f 636f 6e66  efault_dpmp_conf
+000008a0: 6967 2c20 5f50 6970 654c 696e 6543 6f6e  ig, _PipeLineCon
+000008b0: 6669 672c 204f 7050 6172 616c 6c65 6c43  fig, OpParallelC
+000008c0: 6f6e 6669 672c 205c 0a20 2020 205f 436f  onfig, \.    _Co
+000008d0: 6e66 6967 2c20 5f63 6865 636b 5f63 6f6e  nfig, _check_con
+000008e0: 6669 672c 204d 6f45 5061 7261 6c6c 656c  fig, MoEParallel
+000008f0: 436f 6e66 6967 0a66 726f 6d20 6d69 6e64  Config.from mind
+00000900: 666f 726d 6572 732e 6d6f 6475 6c65 732e  formers.modules.
+00000910: 7472 616e 7366 6f72 6d65 722e 6d6f 6520  transformer.moe 
+00000920: 696d 706f 7274 2064 6566 6175 6c74 5f6d  import default_m
+00000930: 6f65 5f63 6f6e 6669 672c 204d 6f45 2c20  oe_config, MoE, 
+00000940: 5f63 6865 636b 5f6d 6f65 5f63 6f6e 6669  _check_moe_confi
+00000950: 670a 6672 6f6d 206d 696e 6466 6f72 6d65  g.from mindforme
+00000960: 7273 2e76 6572 7369 6f6e 5f63 6f6e 7472  rs.version_contr
+00000970: 6f6c 2069 6d70 6f72 7420 6765 745f 6472  ol import get_dr
+00000980: 6f70 6f75 742c 2063 686f 6f73 655f 666c  opout, choose_fl
+00000990: 6173 685f 6174 7465 6e74 696f 6e5f 6474  ash_attention_dt
+000009a0: 7970 652c 205c 0a20 2020 2063 6865 636b  ype, \.    check
+000009b0: 5f76 616c 6964 5f66 6c61 7368 5f61 7474  _valid_flash_att
+000009c0: 656e 7469 6f6e 0a0a 6672 6f6d 206d 696e  ention..from min
+000009d0: 6466 6f72 6d65 7273 2e74 6f6f 6c73 2e6c  dformers.tools.l
+000009e0: 6f67 6765 7220 696d 706f 7274 205f 4c6f  ogger import _Lo
+000009f0: 6741 6374 696f 6e4f 6e63 650a 6672 6f6d  gActionOnce.from
+00000a00: 206d 696e 6466 6f72 6d65 7273 2e74 6f6f   mindformers.too
+00000a10: 6c73 2e6c 6f67 6765 7220 696d 706f 7274  ls.logger import
+00000a20: 206c 6f67 6765 7220 6173 206c 6f67 0a0a   logger as log..
+00000a30: 5f5f 616c 6c5f 5f20 3d20 5b0a 2020 2020  __all__ = [.    
+00000a40: 2241 7474 656e 7469 6f6e 4d61 736b 222c  "AttentionMask",
+00000a50: 0a20 2020 2022 4174 7465 6e74 696f 6e4d  .    "AttentionM
+00000a60: 6173 6b48 4622 2c0a 2020 2020 224c 6f77  askHF",.    "Low
+00000a70: 6572 5472 6961 6e67 756c 6172 4d61 736b  erTriangularMask
+00000a80: 5769 7468 4479 6e61 6d69 6322 2c0a 2020  WithDynamic",.  
+00000a90: 2020 2256 6f63 6162 456d 6265 6464 696e    "VocabEmbeddin
+00000aa0: 6722 2c0a 2020 2020 224d 756c 7469 4865  g",.    "MultiHe
+00000ab0: 6164 4174 7465 6e74 696f 6e22 2c0a 2020  adAttention",.  
+00000ac0: 2020 2246 6565 6446 6f72 7761 7264 222c    "FeedForward",
+00000ad0: 0a20 2020 2022 5472 616e 7366 6f72 6d65  .    "Transforme
+00000ae0: 7245 6e63 6f64 6572 222c 0a20 2020 2022  rEncoder",.    "
+00000af0: 5472 616e 7366 6f72 6d65 7244 6563 6f64  TransformerDecod
+00000b00: 6572 222c 0a20 2020 2022 5472 616e 7366  er",.    "Transf
+00000b10: 6f72 6d65 7245 6e63 6f64 6572 4c61 7965  ormerEncoderLaye
+00000b20: 7222 2c0a 2020 2020 2254 7261 6e73 666f  r",.    "Transfo
+00000b30: 726d 6572 4465 636f 6465 724c 6179 6572  rmerDecoderLayer
+00000b40: 222c 0a20 2020 2022 5472 616e 7366 6f72  ",.    "Transfor
+00000b50: 6d65 7222 2c0a 2020 2020 2254 7261 6e73  mer",.    "Trans
+00000b60: 666f 726d 6572 4f70 5061 7261 6c6c 656c  formerOpParallel
+00000b70: 436f 6e66 6967 222c 0a20 2020 2022 456d  Config",.    "Em
+00000b80: 6265 6464 696e 674f 7050 6172 616c 6c65  beddingOpParalle
+00000b90: 6c43 6f6e 6669 6722 2c0a 2020 2020 2254  lConfig",.    "T
+00000ba0: 7261 6e73 666f 726d 6572 5265 636f 6d70  ransformerRecomp
+00000bb0: 7574 6543 6f6e 6669 6722 5d0a 0a0a 636c  uteConfig"]...cl
+00000bc0: 6173 7320 456d 6265 6464 696e 674f 7050  ass EmbeddingOpP
+00000bd0: 6172 616c 6c65 6c43 6f6e 6669 6728 5f43  arallelConfig(_C
+00000be0: 6f6e 6669 6729 3a0a 2020 2020 7222 2222  onfig):.    r"""
+00000bf0: 0a20 2020 2020 2020 2054 6865 2070 6172  .        The par
+00000c00: 616c 6c65 6c20 636f 6e66 6967 206f 6620  allel config of 
+00000c10: 3a63 6c61 7373 3a60 566f 6361 6245 6d62  :class:`VocabEmb
+00000c20: 6564 6469 6e67 600a 2020 2020 2020 2020  edding`.        
+00000c30: 666f 7220 7468 6520 7365 7474 696e 6720  for the setting 
+00000c40: 6461 7461 2070 6172 616c 6c65 6c20 6f72  data parallel or
+00000c50: 206d 6f64 656c 2070 6172 616c 6c65 6c20   model parallel 
+00000c60: 666f 7220 7468 6520 656d 6265 6464 696e  for the embeddin
+00000c70: 6720 7461 626c 652e 0a0a 2020 2020 2020  g table...      
+00000c80: 2020 4172 6773 3a0a 2020 2020 2020 2020    Args:.        
+00000c90: 2020 2020 6461 7461 5f70 6172 616c 6c65      data_paralle
+00000ca0: 6c28 696e 7429 3a20 5468 6520 6461 7461  l(int): The data
+00000cb0: 2070 6172 616c 6c65 6c20 7761 792e 2054   parallel way. T
+00000cc0: 6865 2069 6e70 7574 2064 6174 6120 7769  he input data wi
+00000cd0: 6c6c 2062 6520 736c 6963 6564 2069 6e74  ll be sliced int
+00000ce0: 6f20 6e20 7061 7274 7320 666f 7220 656d  o n parts for em
+00000cf0: 6265 6464 696e 6720 6c61 7965 720a 2020  bedding layer.  
+00000d00: 2020 2020 2020 2020 2020 2020 2020 6163                ac
+00000d10: 636f 7264 696e 6720 746f 2074 6869 7320  cording to this 
+00000d20: 7661 6c75 652e 2044 6566 6175 6c74 3a20  value. Default: 
+00000d30: 312e 0a20 2020 2020 2020 2020 2020 206d  1..            m
+00000d40: 6f64 656c 5f70 6172 616c 6c65 6c28 696e  odel_parallel(in
+00000d50: 7429 3a20 5468 6520 6d6f 6465 6c20 7061  t): The model pa
+00000d60: 7261 6c6c 656c 2077 6179 2e20 5468 6520  rallel way. The 
+00000d70: 656d 6265 6464 696e 6720 7461 626c 6520  embedding table 
+00000d80: 7061 7261 6d65 7465 7273 0a20 2020 2020  parameters.     
+00000d90: 2020 2020 2020 2020 2020 2077 696c 6c20             will 
+00000da0: 6265 2073 6c69 6365 6420 6174 2030 2d74  be sliced at 0-t
+00000db0: 6820 6178 6973 2061 6363 6f72 6469 6e67  h axis according
+00000dc0: 2074 6f20 7468 6520 6d6f 6465 6c20 7061   to the model pa
+00000dd0: 7261 6c6c 656c 2077 6179 2e20 4465 6661  rallel way. Defa
+00000de0: 756c 743a 2031 2e0a 2020 2020 2020 2020  ult: 1..        
+00000df0: 2020 2020 766f 6361 625f 656d 625f 6470      vocab_emb_dp
+00000e00: 2862 6f6f 6c29 3a20 5368 6172 6420 656d  (bool): Shard em
+00000e10: 6265 6464 696e 6720 696e 206d 6f64 656c  bedding in model
+00000e20: 2070 6172 616c 6c65 6c20 6f72 2064 6174   parallel or dat
+00000e30: 6120 7061 7261 6c6c 656c 2e20 4966 2054  a parallel. If T
+00000e40: 7275 652c 2074 6865 2065 6d62 6564 6469  rue, the embeddi
+00000e50: 6e67 206c 6f6f 6b75 700a 2020 2020 2020  ng lookup.      
+00000e60: 2020 2020 2020 2020 2020 7769 6c6c 2062            will b
+00000e70: 6520 6120 6461 7461 2070 6172 616c 6c65  e a data paralle
+00000e80: 6c20 7374 796c 6520 7472 6169 6e69 6e67  l style training
+00000e90: 2061 6e64 206d 6f64 656c 5f70 6172 616c   and model_paral
+00000ea0: 6c65 6c20 7661 6c75 6520 7769 6c6c 2062  lel value will b
+00000eb0: 6520 6967 6e6f 7265 642e 2020 4966 2066  e ignored.  If f
+00000ec0: 616c 7365 2c20 7468 650a 2020 2020 2020  alse, the.      
+00000ed0: 2020 2020 2020 2020 2020 656d 6265 6464            embedd
+00000ee0: 696e 6720 7461 626c 6520 7769 6c6c 2062  ing table will b
+00000ef0: 6520 7368 6172 6465 6420 696e 746f 206e  e sharded into n
+00000f00: 2070 6172 7473 2061 7420 7468 6520 302d   parts at the 0-
+00000f10: 7468 2064 696d 656e 7369 6f6e 2072 6f77  th dimension row
+00000f20: 2073 6c69 6365 206f 6620 7468 6520 656d   slice of the em
+00000f30: 6265 6464 696e 6720 7461 626c 652c 0a20  bedding table,. 
+00000f40: 2020 2020 2020 2020 2020 2020 2020 2077                 w
+00000f50: 6865 7265 2074 6865 206e 2069 7320 7468  here the n is th
+00000f60: 6520 6d6f 6465 6c20 7061 7261 6c6c 656c  e model parallel
+00000f70: 2077 6179 2064 6574 6572 6d69 6e65 6420   way determined 
+00000f80: 6279 2074 6869 7320 7061 7261 6d65 7465  by this paramete
+00000f90: 722e 2044 6566 6175 6c74 3a20 5472 7565  r. Default: True
+00000fa0: 0a0a 2020 2020 2020 2020 5375 7070 6f72  ..        Suppor
+00000fb0: 7465 6420 506c 6174 666f 726d 733a 0a20  ted Platforms:. 
+00000fc0: 2020 2020 2020 2020 2020 2060 6041 7363             ``Asc
+00000fd0: 656e 6460 6020 6060 4750 5560 600a 0a20  end`` ``GPU``.. 
+00000fe0: 2020 2020 2020 2045 7861 6d70 6c65 733a         Examples:
+00000ff0: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
+00001000: 2066 726f 6d20 6d69 6e64 666f 726d 6572   from mindformer
+00001010: 732e 6d6f 6475 6c65 732e 7472 616e 7366  s.modules.transf
+00001020: 6f72 6d65 7220 696d 706f 7274 2045 6d62  ormer import Emb
+00001030: 6564 6469 6e67 4f70 5061 7261 6c6c 656c  eddingOpParallel
+00001040: 436f 6e66 6967 0a20 2020 2020 2020 2020  Config.         
+00001050: 2020 203e 3e3e 2063 6f6e 6669 673d 456d     >>> config=Em
+00001060: 6265 6464 696e 674f 7050 6172 616c 6c65  beddingOpParalle
+00001070: 6c43 6f6e 6669 6728 6461 7461 5f70 6172  lConfig(data_par
+00001080: 616c 6c65 6c3d 312c 206d 6f64 656c 5f70  allel=1, model_p
+00001090: 6172 616c 6c65 6c3d 312c 2076 6f63 6162  arallel=1, vocab
+000010a0: 5f65 6d62 5f64 703d 5472 7565 290a 2020  _emb_dp=True).  
+000010b0: 2020 2222 220a 0a20 2020 2064 6566 205f    """..    def _
+000010c0: 5f69 6e69 745f 5f28 7365 6c66 2c20 6461  _init__(self, da
+000010d0: 7461 5f70 6172 616c 6c65 6c3d 312c 206d  ta_parallel=1, m
+000010e0: 6f64 656c 5f70 6172 616c 6c65 6c3d 312c  odel_parallel=1,
+000010f0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00001100: 2020 7573 655f 7365 715f 7061 7261 6c6c    use_seq_parall
+00001110: 656c 3d46 616c 7365 2c20 7365 6c65 6374  el=False, select
+00001120: 5f72 6563 6f6d 7075 7465 3d46 616c 7365  _recompute=False
+00001130: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00001140: 2020 2076 6f63 6162 5f65 6d62 5f64 703d     vocab_emb_dp=
+00001150: 5472 7565 293a 0a20 2020 2020 2020 2073  True):.        s
+00001160: 656c 662e 5f64 705f 6d70 5f63 6f6e 6669  elf._dp_mp_confi
+00001170: 6720 3d20 4f70 5061 7261 6c6c 656c 436f  g = OpParallelCo
+00001180: 6e66 6967 2864 6174 615f 7061 7261 6c6c  nfig(data_parall
+00001190: 656c 3d64 6174 615f 7061 7261 6c6c 656c  el=data_parallel
+000011a0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
 000011b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000011c0: 2020 2020 2020 7365 6c65 6374 5f72 6563        select_rec
-000011d0: 6f6d 7075 7465 3d73 656c 6563 745f 7265  ompute=select_re
-000011e0: 636f 6d70 7574 6529 0a20 2020 2020 2020  compute).       
-000011f0: 2056 616c 6964 6174 6f72 2e63 6865 636b   Validator.check
-00001200: 5f62 6f6f 6c28 766f 6361 625f 656d 625f  _bool(vocab_emb_
-00001210: 6470 2c20 2276 6f63 6162 5f65 6d62 5f64  dp, "vocab_emb_d
-00001220: 7022 290a 2020 2020 2020 2020 7365 6c66  p").        self
-00001230: 2e76 6f63 6162 5f65 6d62 5f64 7020 3d20  .vocab_emb_dp = 
-00001240: 766f 6361 625f 656d 625f 6470 0a20 2020  vocab_emb_dp.   
-00001250: 2020 2020 2073 656c 662e 7573 655f 7365       self.use_se
-00001260: 715f 7061 7261 6c6c 656c 203d 2075 7365  q_parallel = use
-00001270: 5f73 6571 5f70 6172 616c 6c65 6c0a 2020  _seq_parallel.  
-00001280: 2020 2020 2020 7365 6c66 2e73 656c 6563        self.selec
-00001290: 745f 7265 636f 6d70 7574 6520 3d20 7365  t_recompute = se
-000012a0: 6c65 6374 5f72 6563 6f6d 7075 7465 0a0a  lect_recompute..
-000012b0: 2020 2020 4070 726f 7065 7274 790a 2020      @property.  
-000012c0: 2020 6465 6620 6461 7461 5f70 6172 616c    def data_paral
-000012d0: 6c65 6c28 7365 6c66 293a 0a20 2020 2020  lel(self):.     
-000012e0: 2020 2072 6574 7572 6e20 7365 6c66 2e5f     return self._
-000012f0: 6470 5f6d 705f 636f 6e66 6967 2e64 6174  dp_mp_config.dat
-00001300: 615f 7061 7261 6c6c 656c 0a0a 2020 2020  a_parallel..    
-00001310: 4064 6174 615f 7061 7261 6c6c 656c 2e73  @data_parallel.s
-00001320: 6574 7465 720a 2020 2020 6465 6620 6461  etter.    def da
-00001330: 7461 5f70 6172 616c 6c65 6c28 7365 6c66  ta_parallel(self
-00001340: 2c20 7661 6c75 6529 3a0a 2020 2020 2020  , value):.      
-00001350: 2020 7365 6c66 2e5f 6470 5f6d 705f 636f    self._dp_mp_co
-00001360: 6e66 6967 2e64 6174 615f 7061 7261 6c6c  nfig.data_parall
-00001370: 656c 203d 2076 616c 7565 0a0a 2020 2020  el = value..    
-00001380: 4070 726f 7065 7274 790a 2020 2020 6465  @property.    de
-00001390: 6620 6d6f 6465 6c5f 7061 7261 6c6c 656c  f model_parallel
-000013a0: 2873 656c 6629 3a0a 2020 2020 2020 2020  (self):.        
-000013b0: 7265 7475 726e 2073 656c 662e 5f64 705f  return self._dp_
-000013c0: 6d70 5f63 6f6e 6669 672e 6d6f 6465 6c5f  mp_config.model_
-000013d0: 7061 7261 6c6c 656c 0a0a 2020 2020 406d  parallel..    @m
-000013e0: 6f64 656c 5f70 6172 616c 6c65 6c2e 7365  odel_parallel.se
-000013f0: 7474 6572 0a20 2020 2064 6566 206d 6f64  tter.    def mod
-00001400: 656c 5f70 6172 616c 6c65 6c28 7365 6c66  el_parallel(self
-00001410: 2c20 7661 6c75 6529 3a0a 2020 2020 2020  , value):.      
-00001420: 2020 7365 6c66 2e5f 6470 5f6d 705f 636f    self._dp_mp_co
-00001430: 6e66 6967 2e6d 6f64 656c 5f70 6172 616c  nfig.model_paral
-00001440: 6c65 6c20 3d20 7661 6c75 650a 0a20 2020  lel = value..   
-00001450: 2040 7072 6f70 6572 7479 0a20 2020 2064   @property.    d
-00001460: 6566 2076 6f63 6162 5f65 6d62 5f64 7028  ef vocab_emb_dp(
-00001470: 7365 6c66 293a 0a20 2020 2020 2020 2072  self):.        r
-00001480: 6574 7572 6e20 7365 6c66 2e5f 766f 6361  eturn self._voca
-00001490: 625f 656d 625f 6470 0a0a 2020 2020 4076  b_emb_dp..    @v
-000014a0: 6f63 6162 5f65 6d62 5f64 702e 7365 7474  ocab_emb_dp.sett
-000014b0: 6572 0a20 2020 2064 6566 2076 6f63 6162  er.    def vocab
-000014c0: 5f65 6d62 5f64 7028 7365 6c66 2c20 7661  _emb_dp(self, va
-000014d0: 6c75 6529 3a0a 2020 2020 2020 2020 5661  lue):.        Va
-000014e0: 6c69 6461 746f 722e 6368 6563 6b5f 626f  lidator.check_bo
-000014f0: 6f6c 2876 616c 7565 2c20 2276 6f63 6162  ol(value, "vocab
-00001500: 5f65 6d62 5f64 7022 290a 2020 2020 2020  _emb_dp").      
-00001510: 2020 7365 6c66 2e5f 766f 6361 625f 656d    self._vocab_em
-00001520: 625f 6470 203d 2076 616c 7565 0a0a 2020  b_dp = value..  
-00001530: 2020 4070 726f 7065 7274 790a 2020 2020    @property.    
-00001540: 6465 6620 6470 5f6d 705f 636f 6e66 6967  def dp_mp_config
-00001550: 2873 656c 6629 3a0a 2020 2020 2020 2020  (self):.        
-00001560: 7265 7475 726e 2073 656c 662e 5f64 705f  return self._dp_
-00001570: 6d70 5f63 6f6e 6669 670a 0a20 2020 2064  mp_config..    d
-00001580: 6566 205f 5f65 715f 5f28 7365 6c66 2c20  ef __eq__(self, 
-00001590: 6f74 6865 7229 202d 3e20 626f 6f6c 3a0a  other) -> bool:.
-000015a0: 2020 2020 2020 2020 7265 7475 726e 2069          return i
-000015b0: 7369 6e73 7461 6e63 6528 6f74 6865 722c  sinstance(other,
-000015c0: 2045 6d62 6564 6469 6e67 4f70 5061 7261   EmbeddingOpPara
-000015d0: 6c6c 656c 436f 6e66 6967 2920 616e 6420  llelConfig) and 
-000015e0: 2873 656c 662e 746f 5f64 6963 7428 2920  (self.to_dict() 
-000015f0: 3d3d 206f 7468 6572 2e74 6f5f 6469 6374  == other.to_dict
-00001600: 2829 290a 0a20 2020 2064 6566 2074 6f5f  ())..    def to_
-00001610: 6469 6666 5f64 6963 7428 7365 6c66 293a  diff_dict(self):
-00001620: 0a20 2020 2020 2020 2063 6f6e 6669 675f  .        config_
-00001630: 6469 6374 203d 2073 656c 662e 746f 5f64  dict = self.to_d
-00001640: 6963 7428 290a 2020 2020 2020 2020 6465  ict().        de
-00001650: 6661 756c 745f 6469 6374 203d 2045 6d62  fault_dict = Emb
-00001660: 6564 6469 6e67 4f70 5061 7261 6c6c 656c  eddingOpParallel
-00001670: 436f 6e66 6967 2829 2e74 6f5f 6469 6374  Config().to_dict
-00001680: 2829 0a20 2020 2020 2020 2072 6573 5f64  ().        res_d
-00001690: 6963 7420 3d20 7b7d 0a20 2020 2020 2020  ict = {}.       
-000016a0: 2066 6f72 206b 2c20 7620 696e 2063 6f6e   for k, v in con
-000016b0: 6669 675f 6469 6374 2e69 7465 6d73 2829  fig_dict.items()
-000016c0: 3a0a 2020 2020 2020 2020 2020 2020 6966  :.            if
-000016d0: 2076 2021 3d20 6465 6661 756c 745f 6469   v != default_di
-000016e0: 6374 5b6b 5d3a 0a20 2020 2020 2020 2020  ct[k]:.         
-000016f0: 2020 2020 2020 2072 6573 5f64 6963 745b         res_dict[
-00001700: 6b5d 203d 2076 0a20 2020 2020 2020 2072  k] = v.        r
-00001710: 6574 7572 6e20 7265 735f 6469 6374 0a0a  eturn res_dict..
-00001720: 2020 2020 6465 6620 746f 5f64 6963 7428      def to_dict(
-00001730: 7365 6c66 293a 0a20 2020 2020 2020 2022  self):.        "
-00001740: 2222 746f 2064 6963 7422 2222 0a20 2020  ""to dict""".   
-00001750: 2020 2020 2063 6f6e 6669 675f 6469 6374       config_dict
-00001760: 203d 207b 0a20 2020 2020 2020 2020 2020   = {.           
-00001770: 2027 6461 7461 5f70 6172 616c 6c65 6c27   'data_parallel'
-00001780: 3a20 7365 6c66 2e64 6174 615f 7061 7261  : self.data_para
-00001790: 6c6c 656c 2c0a 2020 2020 2020 2020 2020  llel,.          
-000017a0: 2020 276d 6f64 656c 5f70 6172 616c 6c65    'model_paralle
-000017b0: 6c27 3a20 7365 6c66 2e6d 6f64 656c 5f70  l': self.model_p
-000017c0: 6172 616c 6c65 6c2c 0a20 2020 2020 2020  arallel,.       
-000017d0: 2020 2020 2027 7365 6c65 6374 5f72 6563       'select_rec
-000017e0: 6f6d 7075 7465 273a 2073 656c 662e 7365  ompute': self.se
-000017f0: 6c65 6374 5f72 6563 6f6d 7075 7465 2c0a  lect_recompute,.
-00001800: 2020 2020 2020 2020 2020 2020 2775 7365              'use
-00001810: 5f73 6571 5f70 6172 616c 6c65 6c27 3a20  _seq_parallel': 
-00001820: 7365 6c66 2e75 7365 5f73 6571 5f70 6172  self.use_seq_par
-00001830: 616c 6c65 6c2c 0a20 2020 2020 2020 2020  allel,.         
-00001840: 2020 2027 766f 6361 625f 656d 625f 6470     'vocab_emb_dp
-00001850: 273a 2073 656c 662e 766f 6361 625f 656d  ': self.vocab_em
-00001860: 625f 6470 0a20 2020 2020 2020 207d 0a20  b_dp.        }. 
-00001870: 2020 2020 2020 2072 6574 7572 6e20 636f         return co
-00001880: 6e66 6967 5f64 6963 740a 0a0a 636c 6173  nfig_dict...clas
-00001890: 7320 5472 616e 7366 6f72 6d65 7252 6563  s TransformerRec
-000018a0: 6f6d 7075 7465 436f 6e66 6967 285f 436f  omputeConfig(_Co
-000018b0: 6e66 6967 293a 0a20 2020 2072 2222 220a  nfig):.    r""".
-000018c0: 2020 2020 2020 2020 5472 616e 7366 6f72          Transfor
-000018d0: 6d65 7252 6563 6f6d 7075 7465 436f 6e66  merRecomputeConf
-000018e0: 6967 2066 6f72 2074 6865 2073 6574 7469  ig for the setti
-000018f0: 6e67 2072 6563 6f6d 7075 7465 2061 7474  ng recompute att
-00001900: 7269 6275 7465 7320 666f 7220 656e 636f  ributes for enco
-00001910: 6465 722f 6465 636f 6465 7220 6c61 7965  der/decoder laye
-00001920: 7273 2e0a 0a20 2020 2020 2020 2041 7267  rs...        Arg
-00001930: 733a 0a20 2020 2020 2020 2020 2020 2072  s:.            r
-00001940: 6563 6f6d 7075 7465 2028 626f 6f6c 293a  ecompute (bool):
-00001950: 2045 6e61 626c 6520 7265 636f 6d70 7574   Enable recomput
-00001960: 6174 696f 6e20 6f66 2074 6865 2074 7261  ation of the tra
-00001970: 6e73 666f 726d 6572 2062 6c6f 636b 206f  nsformer block o
-00001980: 7220 6e6f 742e 2044 6566 6175 6c74 3a20  r not. Default: 
-00001990: 4661 6c73 652e 0a20 2020 2020 2020 2020  False..         
-000019a0: 2020 2070 6172 616c 6c65 6c5f 6f70 7469     parallel_opti
-000019b0: 6d69 7a65 725f 636f 6d6d 5f72 6563 6f6d  mizer_comm_recom
-000019c0: 7075 7465 2028 626f 6f6c 293a 2053 7065  pute (bool): Spe
-000019d0: 6369 6669 6573 2077 6865 7468 6572 2074  cifies whether t
-000019e0: 6865 2063 6f6d 6d75 6e69 6361 7469 6f6e  he communication
-000019f0: 206f 7065 7261 746f 7220 616c 6c67 6174   operator allgat
-00001a00: 6865 7273 0a20 2020 2020 2020 2020 2020  hers.           
-00001a10: 2020 2020 2069 6e74 726f 6475 6365 6420       introduced 
-00001a20: 6279 206f 7074 696d 697a 6572 2073 6861  by optimizer sha
-00001a30: 7264 2061 7265 2072 6563 6f6d 7075 7465  rd are recompute
-00001a40: 6420 696e 2061 7574 6f20 7061 7261 6c6c  d in auto parall
-00001a50: 656c 206f 7220 7365 6d69 2061 7574 6f20  el or semi auto 
-00001a60: 7061 7261 6c6c 656c 206d 6f64 652e 0a20  parallel mode.. 
-00001a70: 2020 2020 2020 2020 2020 2020 2020 2044                 D
-00001a80: 6566 6175 6c74 3a20 4661 6c73 652e 0a20  efault: False.. 
-00001a90: 2020 2020 2020 2020 2020 206d 705f 636f             mp_co
-00001aa0: 6d6d 5f72 6563 6f6d 7075 7465 2028 626f  mm_recompute (bo
-00001ab0: 6f6c 293a 2053 7065 6369 6669 6573 2077  ol): Specifies w
-00001ac0: 6865 7468 6572 2074 6865 206d 6f64 656c  hether the model
-00001ad0: 2070 6172 616c 6c65 6c20 636f 6d6d 756e   parallel commun
-00001ae0: 6963 6174 696f 6e20 6f70 6572 6174 6f72  ication operator
-00001af0: 730a 2020 2020 2020 2020 2020 2020 2020  s.              
-00001b00: 2020 696e 2074 6865 2063 656c 6c20 6172    in the cell ar
-00001b10: 6520 7265 636f 6d70 7574 6564 2069 6e20  e recomputed in 
-00001b20: 6175 746f 2070 6172 616c 6c65 6c20 6f72  auto parallel or
-00001b30: 2073 656d 6920 6175 746f 2070 6172 616c   semi auto paral
-00001b40: 6c65 6c20 6d6f 6465 2e20 4465 6661 756c  lel mode. Defaul
-00001b50: 743a 2054 7275 652e 0a20 2020 2020 2020  t: True..       
-00001b60: 2020 2020 2072 6563 6f6d 7075 7465 5f73       recompute_s
-00001b70: 6c69 6365 5f61 6374 6976 6174 696f 6e20  lice_activation 
-00001b80: 2862 6f6f 6c29 3a20 536c 6963 6520 7468  (bool): Slice th
-00001b90: 6520 6365 6c6c 206f 7574 7075 7420 7768  e cell output wh
-00001ba0: 6963 6820 776f 756c 6420 7265 6d61 696e  ich would remain
-00001bb0: 7320 696e 206d 656d 6f72 792e 2044 6566  s in memory. Def
-00001bc0: 6175 6c74 3a20 4661 6c73 652e 0a0a 2020  ault: False...  
-00001bd0: 2020 2020 2020 5375 7070 6f72 7465 6420        Supported 
-00001be0: 506c 6174 666f 726d 733a 0a20 2020 2020  Platforms:.     
-00001bf0: 2020 2020 2020 2060 6041 7363 656e 6460         ``Ascend`
-00001c00: 6020 6060 4750 5560 600a 0a20 2020 2020  ` ``GPU``..     
-00001c10: 2020 2045 7861 6d70 6c65 733a 0a20 2020     Examples:.   
-00001c20: 2020 2020 2020 2020 203e 3e3e 2066 726f           >>> fro
-00001c30: 6d20 6d69 6e64 666f 726d 6572 732e 6d6f  m mindformers.mo
-00001c40: 6475 6c65 732e 7472 616e 7366 6f72 6d65  dules.transforme
-00001c50: 7220 696d 706f 7274 2054 7261 6e73 666f  r import Transfo
-00001c60: 726d 6572 5265 636f 6d70 7574 6543 6f6e  rmerRecomputeCon
-00001c70: 6669 670a 2020 2020 2020 2020 2020 2020  fig.            
-00001c80: 3e3e 3e20 636f 6e66 6967 3d54 7261 6e73  >>> config=Trans
-00001c90: 666f 726d 6572 5265 636f 6d70 7574 6543  formerRecomputeC
-00001ca0: 6f6e 6669 6728 7265 636f 6d70 7574 653d  onfig(recompute=
-00001cb0: 5472 7565 2c20 7061 7261 6c6c 656c 5f6f  True, parallel_o
-00001cc0: 7074 696d 697a 6572 5f63 6f6d 6d5f 7265  ptimizer_comm_re
-00001cd0: 636f 6d70 7574 653d 5472 7565 2c20 5c0a  compute=True, \.
-00001ce0: 2020 2020 2020 2020 2020 2020 2e2e 2e20              ... 
-00001cf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001d00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001d10: 2020 6d70 5f63 6f6d 6d5f 7265 636f 6d70    mp_comm_recomp
-00001d20: 7574 653d 5472 7565 2c20 7265 636f 6d70  ute=True, recomp
-00001d30: 7574 655f 736c 6963 655f 6163 7469 7661  ute_slice_activa
-00001d40: 7469 6f6e 3d54 7275 6529 0a20 2020 2022  tion=True).    "
-00001d50: 2222 0a0a 2020 2020 6465 6620 5f5f 696e  ""..    def __in
-00001d60: 6974 5f5f 2873 656c 662c 2072 6563 6f6d  it__(self, recom
-00001d70: 7075 7465 3d46 616c 7365 2c20 7365 6c65  pute=False, sele
-00001d80: 6374 5f72 6563 6f6d 7075 7465 3d46 616c  ct_recompute=Fal
-00001d90: 7365 2c0a 2020 2020 2020 2020 2020 2020  se,.            
-00001da0: 2020 2020 2070 6172 616c 6c65 6c5f 6f70       parallel_op
-00001db0: 7469 6d69 7a65 725f 636f 6d6d 5f72 6563  timizer_comm_rec
-00001dc0: 6f6d 7075 7465 3d46 616c 7365 2c20 7365  ompute=False, se
-00001dd0: 6c65 6374 5f63 6f6d 6d5f 7265 636f 6d70  lect_comm_recomp
-00001de0: 7574 653d 4661 6c73 652c 0a20 2020 2020  ute=False,.     
-00001df0: 2020 2020 2020 2020 2020 2020 6d70 5f63              mp_c
-00001e00: 6f6d 6d5f 7265 636f 6d70 7574 653d 5472  omm_recompute=Tr
-00001e10: 7565 2c20 7265 636f 6d70 7574 655f 736c  ue, recompute_sl
-00001e20: 6963 655f 6163 7469 7661 7469 6f6e 3d46  ice_activation=F
-00001e30: 616c 7365 293a 0a20 2020 2020 2020 2056  alse):.        V
-00001e40: 616c 6964 6174 6f72 2e63 6865 636b 5f62  alidator.check_b
-00001e50: 6f6f 6c28 7265 636f 6d70 7574 652c 2022  ool(recompute, "
-00001e60: 7265 636f 6d70 7574 6522 290a 2020 2020  recompute").    
-00001e70: 2020 2020 5661 6c69 6461 746f 722e 6368      Validator.ch
-00001e80: 6563 6b5f 626f 6f6c 2870 6172 616c 6c65  eck_bool(paralle
-00001e90: 6c5f 6f70 7469 6d69 7a65 725f 636f 6d6d  l_optimizer_comm
-00001ea0: 5f72 6563 6f6d 7075 7465 2c20 2270 6172  _recompute, "par
-00001eb0: 616c 6c65 6c5f 6f70 7469 6d69 7a65 725f  allel_optimizer_
-00001ec0: 636f 6d6d 5f72 6563 6f6d 7075 7465 2229  comm_recompute")
-00001ed0: 0a20 2020 2020 2020 2056 616c 6964 6174  .        Validat
-00001ee0: 6f72 2e63 6865 636b 5f62 6f6f 6c28 6d70  or.check_bool(mp
-00001ef0: 5f63 6f6d 6d5f 7265 636f 6d70 7574 652c  _comm_recompute,
-00001f00: 2022 6d70 5f63 6f6d 6d5f 7265 636f 6d70   "mp_comm_recomp
-00001f10: 7574 6522 290a 2020 2020 2020 2020 5661  ute").        Va
-00001f20: 6c69 6461 746f 722e 6368 6563 6b5f 626f  lidator.check_bo
-00001f30: 6f6c 2872 6563 6f6d 7075 7465 5f73 6c69  ol(recompute_sli
-00001f40: 6365 5f61 6374 6976 6174 696f 6e2c 2022  ce_activation, "
-00001f50: 7265 636f 6d70 7574 655f 736c 6963 655f  recompute_slice_
-00001f60: 6163 7469 7661 7469 6f6e 2229 0a20 2020  activation").   
-00001f70: 2020 2020 2073 656c 662e 5f72 6563 6f6d       self._recom
-00001f80: 7075 7465 203d 2072 6563 6f6d 7075 7465  pute = recompute
-00001f90: 0a20 2020 2020 2020 2073 656c 662e 5f73  .        self._s
-00001fa0: 656c 6563 745f 7265 636f 6d70 7574 6520  elect_recompute 
-00001fb0: 3d20 7365 6c65 6374 5f72 6563 6f6d 7075  = select_recompu
-00001fc0: 7465 0a20 2020 2020 2020 2073 656c 662e  te.        self.
-00001fd0: 5f73 656c 6563 745f 636f 6d6d 5f72 6563  _select_comm_rec
-00001fe0: 6f6d 7075 7465 203d 2073 656c 6563 745f  ompute = select_
-00001ff0: 636f 6d6d 5f72 6563 6f6d 7075 7465 0a20  comm_recompute. 
-00002000: 2020 2020 2020 2073 656c 662e 5f70 6172         self._par
-00002010: 616c 6c65 6c5f 6f70 7469 6d69 7a65 725f  allel_optimizer_
-00002020: 636f 6d6d 5f72 6563 6f6d 7075 7465 203d  comm_recompute =
-00002030: 2070 6172 616c 6c65 6c5f 6f70 7469 6d69   parallel_optimi
-00002040: 7a65 725f 636f 6d6d 5f72 6563 6f6d 7075  zer_comm_recompu
-00002050: 7465 0a20 2020 2020 2020 2073 656c 662e  te.        self.
-00002060: 5f6d 705f 636f 6d6d 5f72 6563 6f6d 7075  _mp_comm_recompu
-00002070: 7465 203d 206d 705f 636f 6d6d 5f72 6563  te = mp_comm_rec
-00002080: 6f6d 7075 7465 0a20 2020 2020 2020 2073  ompute.        s
-00002090: 656c 662e 5f72 6563 6f6d 7075 7465 5f73  elf._recompute_s
-000020a0: 6c69 6365 5f61 6374 6976 6174 696f 6e20  lice_activation 
-000020b0: 3d20 7265 636f 6d70 7574 655f 736c 6963  = recompute_slic
-000020c0: 655f 6163 7469 7661 7469 6f6e 0a0a 2020  e_activation..  
-000020d0: 2020 4070 726f 7065 7274 790a 2020 2020    @property.    
-000020e0: 6465 6620 7265 636f 6d70 7574 6528 7365  def recompute(se
-000020f0: 6c66 293a 0a20 2020 2020 2020 2072 6574  lf):.        ret
-00002100: 7572 6e20 7365 6c66 2e5f 7265 636f 6d70  urn self._recomp
-00002110: 7574 650a 0a20 2020 2040 7265 636f 6d70  ute..    @recomp
-00002120: 7574 652e 7365 7474 6572 0a20 2020 2064  ute.setter.    d
-00002130: 6566 2072 6563 6f6d 7075 7465 2873 656c  ef recompute(sel
-00002140: 662c 2076 616c 7565 293a 0a20 2020 2020  f, value):.     
-00002150: 2020 2056 616c 6964 6174 6f72 2e63 6865     Validator.che
-00002160: 636b 5f62 6f6f 6c28 7661 6c75 652c 2022  ck_bool(value, "
-00002170: 7265 636f 6d70 7574 6522 290a 2020 2020  recompute").    
-00002180: 2020 2020 7365 6c66 2e5f 7265 636f 6d70      self._recomp
-00002190: 7574 6520 3d20 7661 6c75 650a 0a20 2020  ute = value..   
-000021a0: 2040 7072 6f70 6572 7479 0a20 2020 2064   @property.    d
-000021b0: 6566 2073 656c 6563 745f 7265 636f 6d70  ef select_recomp
-000021c0: 7574 6528 7365 6c66 293a 0a20 2020 2020  ute(self):.     
-000021d0: 2020 2072 6574 7572 6e20 7365 6c66 2e5f     return self._
-000021e0: 7365 6c65 6374 5f72 6563 6f6d 7075 7465  select_recompute
-000021f0: 0a0a 2020 2020 4070 726f 7065 7274 790a  ..    @property.
-00002200: 2020 2020 6465 6620 7365 6c65 6374 5f63      def select_c
-00002210: 6f6d 6d5f 7265 636f 6d70 7574 6528 7365  omm_recompute(se
-00002220: 6c66 293a 0a20 2020 2020 2020 2072 6574  lf):.        ret
-00002230: 7572 6e20 7365 6c66 2e5f 7365 6c65 6374  urn self._select
-00002240: 5f63 6f6d 6d5f 7265 636f 6d70 7574 650a  _comm_recompute.
-00002250: 0a20 2020 2040 7365 6c65 6374 5f72 6563  .    @select_rec
-00002260: 6f6d 7075 7465 2e73 6574 7465 720a 2020  ompute.setter.  
-00002270: 2020 6465 6620 7365 6c65 6374 5f72 6563    def select_rec
-00002280: 6f6d 7075 7465 2873 656c 662c 2076 616c  ompute(self, val
-00002290: 7565 293a 0a20 2020 2020 2020 2056 616c  ue):.        Val
-000022a0: 6964 6174 6f72 2e63 6865 636b 5f62 6f6f  idator.check_boo
-000022b0: 6c28 7661 6c75 652c 2022 7365 6c65 6374  l(value, "select
-000022c0: 5f72 6563 6f6d 7075 7465 2229 0a20 2020  _recompute").   
-000022d0: 2020 2020 2073 656c 662e 5f73 656c 6563       self._selec
-000022e0: 745f 7265 636f 6d70 7574 6520 3d20 7661  t_recompute = va
-000022f0: 6c75 650a 0a20 2020 2040 7365 6c65 6374  lue..    @select
-00002300: 5f63 6f6d 6d5f 7265 636f 6d70 7574 652e  _comm_recompute.
-00002310: 7365 7474 6572 0a20 2020 2064 6566 2073  setter.    def s
-00002320: 656c 6563 745f 636f 6d6d 5f72 6563 6f6d  elect_comm_recom
-00002330: 7075 7465 2873 656c 662c 2076 616c 7565  pute(self, value
-00002340: 293a 0a20 2020 2020 2020 2056 616c 6964  ):.        Valid
-00002350: 6174 6f72 2e63 6865 636b 5f62 6f6f 6c28  ator.check_bool(
-00002360: 7661 6c75 652c 2022 7365 6c65 6374 5f63  value, "select_c
-00002370: 6f6d 6d5f 7265 636f 6d70 7574 6522 290a  omm_recompute").
-00002380: 2020 2020 2020 2020 7365 6c66 2e5f 7365          self._se
-00002390: 6c65 6374 5f63 6f6d 6d5f 7265 636f 6d70  lect_comm_recomp
-000023a0: 7574 6520 3d20 7661 6c75 650a 0a20 2020  ute = value..   
-000023b0: 2040 7072 6f70 6572 7479 0a20 2020 2064   @property.    d
-000023c0: 6566 2070 6172 616c 6c65 6c5f 6f70 7469  ef parallel_opti
-000023d0: 6d69 7a65 725f 636f 6d6d 5f72 6563 6f6d  mizer_comm_recom
-000023e0: 7075 7465 2873 656c 6629 3a0a 2020 2020  pute(self):.    
-000023f0: 2020 2020 7265 7475 726e 2073 656c 662e      return self.
-00002400: 5f70 6172 616c 6c65 6c5f 6f70 7469 6d69  _parallel_optimi
-00002410: 7a65 725f 636f 6d6d 5f72 6563 6f6d 7075  zer_comm_recompu
-00002420: 7465 0a0a 2020 2020 4070 6172 616c 6c65  te..    @paralle
-00002430: 6c5f 6f70 7469 6d69 7a65 725f 636f 6d6d  l_optimizer_comm
-00002440: 5f72 6563 6f6d 7075 7465 2e73 6574 7465  _recompute.sette
-00002450: 720a 2020 2020 6465 6620 7061 7261 6c6c  r.    def parall
-00002460: 656c 5f6f 7074 696d 697a 6572 5f63 6f6d  el_optimizer_com
-00002470: 6d5f 7265 636f 6d70 7574 6528 7365 6c66  m_recompute(self
-00002480: 2c20 7661 6c75 6529 3a0a 2020 2020 2020  , value):.      
-00002490: 2020 5661 6c69 6461 746f 722e 6368 6563    Validator.chec
-000024a0: 6b5f 626f 6f6c 2876 616c 7565 2c20 2270  k_bool(value, "p
-000024b0: 6172 616c 6c65 6c5f 6f70 7469 6d69 7a65  arallel_optimize
-000024c0: 725f 636f 6d6d 5f72 6563 6f6d 7075 7465  r_comm_recompute
-000024d0: 2229 0a20 2020 2020 2020 2073 656c 662e  ").        self.
-000024e0: 5f70 6172 616c 6c65 6c5f 6f70 7469 6d69  _parallel_optimi
-000024f0: 7a65 725f 636f 6d6d 5f72 6563 6f6d 7075  zer_comm_recompu
-00002500: 7465 203d 2076 616c 7565 0a0a 2020 2020  te = value..    
-00002510: 4070 726f 7065 7274 790a 2020 2020 6465  @property.    de
-00002520: 6620 6d70 5f63 6f6d 6d5f 7265 636f 6d70  f mp_comm_recomp
-00002530: 7574 6528 7365 6c66 293a 0a20 2020 2020  ute(self):.     
-00002540: 2020 2072 6574 7572 6e20 7365 6c66 2e5f     return self._
-00002550: 6d70 5f63 6f6d 6d5f 7265 636f 6d70 7574  mp_comm_recomput
-00002560: 650a 0a20 2020 2040 6d70 5f63 6f6d 6d5f  e..    @mp_comm_
-00002570: 7265 636f 6d70 7574 652e 7365 7474 6572  recompute.setter
-00002580: 0a20 2020 2064 6566 206d 705f 636f 6d6d  .    def mp_comm
-00002590: 5f72 6563 6f6d 7075 7465 2873 656c 662c  _recompute(self,
-000025a0: 2076 616c 7565 293a 0a20 2020 2020 2020   value):.       
-000025b0: 2056 616c 6964 6174 6f72 2e63 6865 636b   Validator.check
-000025c0: 5f62 6f6f 6c28 7661 6c75 652c 2022 6d70  _bool(value, "mp
-000025d0: 5f63 6f6d 6d5f 7265 636f 6d70 7574 6522  _comm_recompute"
-000025e0: 290a 2020 2020 2020 2020 7365 6c66 2e5f  ).        self._
-000025f0: 6d70 5f63 6f6d 6d5f 7265 636f 6d70 7574  mp_comm_recomput
-00002600: 6520 3d20 7661 6c75 650a 0a20 2020 2040  e = value..    @
-00002610: 7072 6f70 6572 7479 0a20 2020 2064 6566  property.    def
-00002620: 2072 6563 6f6d 7075 7465 5f73 6c69 6365   recompute_slice
-00002630: 5f61 6374 6976 6174 696f 6e28 7365 6c66  _activation(self
-00002640: 293a 0a20 2020 2020 2020 2072 6574 7572  ):.        retur
-00002650: 6e20 7365 6c66 2e5f 7265 636f 6d70 7574  n self._recomput
-00002660: 655f 736c 6963 655f 6163 7469 7661 7469  e_slice_activati
-00002670: 6f6e 0a0a 2020 2020 4072 6563 6f6d 7075  on..    @recompu
-00002680: 7465 5f73 6c69 6365 5f61 6374 6976 6174  te_slice_activat
-00002690: 696f 6e2e 7365 7474 6572 0a20 2020 2064  ion.setter.    d
-000026a0: 6566 2072 6563 6f6d 7075 7465 5f73 6c69  ef recompute_sli
-000026b0: 6365 5f61 6374 6976 6174 696f 6e28 7365  ce_activation(se
-000026c0: 6c66 2c20 7661 6c75 6529 3a0a 2020 2020  lf, value):.    
-000026d0: 2020 2020 5661 6c69 6461 746f 722e 6368      Validator.ch
-000026e0: 6563 6b5f 626f 6f6c 2876 616c 7565 2c20  eck_bool(value, 
-000026f0: 2272 6563 6f6d 7075 7465 5f73 6c69 6365  "recompute_slice
-00002700: 5f61 6374 6976 6174 696f 6e22 290a 2020  _activation").  
-00002710: 2020 2020 2020 7365 6c66 2e5f 7265 636f        self._reco
-00002720: 6d70 7574 655f 736c 6963 655f 6163 7469  mpute_slice_acti
-00002730: 7661 7469 6f6e 203d 2076 616c 7565 0a0a  vation = value..
-00002740: 2020 2020 6465 6620 5f5f 6571 5f5f 2873      def __eq__(s
-00002750: 656c 662c 206f 7468 6572 2920 2d3e 2062  elf, other) -> b
-00002760: 6f6f 6c3a 0a20 2020 2020 2020 2072 6574  ool:.        ret
-00002770: 7572 6e20 6973 696e 7374 616e 6365 286f  urn isinstance(o
-00002780: 7468 6572 2c20 5472 616e 7366 6f72 6d65  ther, Transforme
-00002790: 7252 6563 6f6d 7075 7465 436f 6e66 6967  rRecomputeConfig
-000027a0: 2920 616e 6420 2873 656c 662e 746f 5f64  ) and (self.to_d
-000027b0: 6963 7428 2920 3d3d 206f 7468 6572 2e74  ict() == other.t
-000027c0: 6f5f 6469 6374 2829 290a 0a20 2020 2064  o_dict())..    d
-000027d0: 6566 2074 6f5f 6469 6666 5f64 6963 7428  ef to_diff_dict(
-000027e0: 7365 6c66 293a 0a20 2020 2020 2020 2063  self):.        c
-000027f0: 6f6e 6669 675f 6469 6374 203d 2073 656c  onfig_dict = sel
-00002800: 662e 746f 5f64 6963 7428 290a 2020 2020  f.to_dict().    
-00002810: 2020 2020 6465 6661 756c 745f 6469 6374      default_dict
-00002820: 203d 2054 7261 6e73 666f 726d 6572 5265   = TransformerRe
-00002830: 636f 6d70 7574 6543 6f6e 6669 6728 292e  computeConfig().
-00002840: 746f 5f64 6963 7428 290a 2020 2020 2020  to_dict().      
-00002850: 2020 7265 735f 6469 6374 203d 207b 7d0a    res_dict = {}.
-00002860: 2020 2020 2020 2020 666f 7220 6b2c 2076          for k, v
-00002870: 2069 6e20 636f 6e66 6967 5f64 6963 742e   in config_dict.
-00002880: 6974 656d 7328 293a 0a20 2020 2020 2020  items():.       
-00002890: 2020 2020 2069 6620 7620 213d 2064 6566       if v != def
-000028a0: 6175 6c74 5f64 6963 745b 6b5d 3a0a 2020  ault_dict[k]:.  
-000028b0: 2020 2020 2020 2020 2020 2020 2020 7265                re
-000028c0: 735f 6469 6374 5b6b 5d20 3d20 760a 2020  s_dict[k] = v.  
-000028d0: 2020 2020 2020 7265 7475 726e 2072 6573        return res
-000028e0: 5f64 6963 740a 0a20 2020 2064 6566 2074  _dict..    def t
-000028f0: 6f5f 6469 6374 2873 656c 6629 3a0a 2020  o_dict(self):.  
-00002900: 2020 2020 2020 636f 6e66 6967 5f64 6963        config_dic
-00002910: 7420 3d20 7b0a 2020 2020 2020 2020 2020  t = {.          
-00002920: 2020 2272 6563 6f6d 7075 7465 223a 2073    "recompute": s
-00002930: 656c 662e 5f72 6563 6f6d 7075 7465 2c0a  elf._recompute,.
-00002940: 2020 2020 2020 2020 2020 2020 2273 656c              "sel
-00002950: 6563 745f 7265 636f 6d70 7574 6522 3a20  ect_recompute": 
-00002960: 7365 6c66 2e5f 7365 6c65 6374 5f72 6563  self._select_rec
-00002970: 6f6d 7075 7465 2c0a 2020 2020 2020 2020  ompute,.        
-00002980: 2020 2020 2270 6172 616c 6c65 6c5f 6f70      "parallel_op
-00002990: 7469 6d69 7a65 725f 636f 6d6d 5f72 6563  timizer_comm_rec
-000029a0: 6f6d 7075 7465 223a 2073 656c 662e 5f70  ompute": self._p
-000029b0: 6172 616c 6c65 6c5f 6f70 7469 6d69 7a65  arallel_optimize
-000029c0: 725f 636f 6d6d 5f72 6563 6f6d 7075 7465  r_comm_recompute
-000029d0: 2c0a 2020 2020 2020 2020 2020 2020 226d  ,.            "m
-000029e0: 705f 636f 6d6d 5f72 6563 6f6d 7075 7465  p_comm_recompute
-000029f0: 223a 2073 656c 662e 5f6d 705f 636f 6d6d  ": self._mp_comm
-00002a00: 5f72 6563 6f6d 7075 7465 2c0a 2020 2020  _recompute,.    
-00002a10: 2020 2020 2020 2020 2272 6563 6f6d 7075          "recompu
-00002a20: 7465 5f73 6c69 6365 5f61 6374 6976 6174  te_slice_activat
-00002a30: 696f 6e22 3a20 7365 6c66 2e5f 7265 636f  ion": self._reco
-00002a40: 6d70 7574 655f 736c 6963 655f 6163 7469  mpute_slice_acti
-00002a50: 7661 7469 6f6e 2c0a 2020 2020 2020 2020  vation,.        
-00002a60: 7d0a 2020 2020 2020 2020 7265 7475 726e  }.        return
-00002a70: 2063 6f6e 6669 675f 6469 6374 0a0a 0a64   config_dict...d
-00002a80: 6566 6175 6c74 5f74 7261 6e73 666f 726d  efault_transform
-00002a90: 6572 5f72 6563 6f6d 7075 7465 5f63 6f6e  er_recompute_con
-00002aa0: 6669 6720 3d20 5472 616e 7366 6f72 6d65  fig = Transforme
-00002ab0: 7252 6563 6f6d 7075 7465 436f 6e66 6967  rRecomputeConfig
-00002ac0: 2829 0a0a 0a63 6c61 7373 2054 7261 6e73  ()...class Trans
-00002ad0: 666f 726d 6572 4f70 5061 7261 6c6c 656c  formerOpParallel
-00002ae0: 436f 6e66 6967 285f 436f 6e66 6967 293a  Config(_Config):
-00002af0: 0a20 2020 2072 2222 220a 2020 2020 2020  .    r""".      
-00002b00: 2020 5472 616e 7366 6f72 6d65 724f 7050    TransformerOpP
-00002b10: 6172 616c 6c65 6c43 6f6e 6669 6720 666f  arallelConfig fo
-00002b20: 7220 7365 7474 696e 6720 7061 7261 6c6c  r setting parall
-00002b30: 656c 2063 6f6e 6669 6775 7261 7469 6f6e  el configuration
-00002b40: 2c20 7375 6368 2061 7320 7468 6520 6461  , such as the da
-00002b50: 7461 2070 6172 616c 6c65 6c20 616e 6420  ta parallel and 
-00002b60: 6d6f 6465 6c20 7061 7261 6c6c 656c 2e0a  model parallel..
-00002b70: 0a20 2020 2020 2020 204e 6f74 653a 0a20  .        Note:. 
-00002b80: 2020 2020 2020 2020 2020 2045 7863 6570             Excep
-00002b90: 7420 7468 6520 7265 636f 6d70 7574 6520  t the recompute 
-00002ba0: 6172 6775 6d65 6e74 2c20 6f74 6865 7220  argument, other 
-00002bb0: 6172 6775 6d65 6e74 7320 7769 6c6c 202a  arguments will *
-00002bc0: 2a6e 6f74 2a2a 2062 6520 6566 6665 6374  *not** be effect
-00002bd0: 6976 6520 7768 656e 2074 6865 2075 7365  ive when the use
-00002be0: 7220 646f 6573 6e27 7420 7365 740a 2020  r doesn't set.  
-00002bf0: 2020 2020 2020 2020 2020 6175 746f 5f70            auto_p
-00002c00: 6172 616c 6c65 6c5f 636f 6e74 6578 7420  arallel_context 
-00002c10: 746f 2060 5345 4d49 5f41 5554 4f5f 5041  to `SEMI_AUTO_PA
-00002c20: 5241 4c4c 454c 6020 6f72 2060 4155 544f  RALLEL` or `AUTO
-00002c30: 5f50 4152 414c 4c45 4c60 2e0a 2020 2020  _PARALLEL`..    
-00002c40: 2020 2020 2020 2020 5468 6520 6d69 6372          The micr
-00002c50: 6f5f 6261 7463 685f 6e75 6d20 6d75 7374  o_batch_num must
-00002c60: 2062 6520 6772 6561 7465 7220 7468 616e   be greater than
-00002c70: 206f 7220 6571 7561 6c20 746f 2070 6970   or equal to pip
-00002c80: 656c 696e 655f 7374 6167 6520 7768 656e  eline_stage when
-00002c90: 2074 7261 696e 696e 672e 0a20 2020 2020   training..     
-00002ca0: 2020 2020 2020 2054 6865 2064 6174 615f         The data_
-00002cb0: 7061 7261 6c6c 656c 5c2a 6d6f 6465 6c5f  parallel\*model_
-00002cc0: 7061 7261 6c6c 656c 205c 2a70 6970 656c  parallel \*pipel
-00002cd0: 696e 655f 7374 6167 6520 6d75 7374 2062  ine_stage must b
-00002ce0: 6520 6571 7561 6c20 6f72 206c 6573 7320  e equal or less 
-00002cf0: 6571 7561 6c20 746f 2074 6865 2064 6576  equal to the dev
-00002d00: 6963 652e 2057 6865 6e20 7365 7474 696e  ice. When settin
-00002d10: 670a 2020 2020 2020 2020 2020 2020 7468  g.            th
-00002d20: 6520 7069 7065 6c69 6e65 2073 7461 6765  e pipeline stage
-00002d30: 2061 6e64 206f 7074 696d 697a 6572 5f73   and optimizer_s
-00002d40: 6861 7264 2c20 7468 6520 636f 6e66 6967  hard, the config
-00002d50: 2077 696c 6c20 6f76 6572 7772 6974 6520   will overwrite 
-00002d60: 7468 6520 6175 746f 5f70 6172 616c 6c65  the auto_paralle
-00002d70: 6c5f 636f 6e74 6578 742e 2057 6865 6e20  l_context. When 
-00002d80: 6769 7665 6e20 7468 650a 2020 2020 2020  given the.      
-00002d90: 2020 2020 2020 3820 6465 7669 6365 7320        8 devices 
-00002da0: 616e 6420 7468 6520 6461 7461 5f70 6172  and the data_par
-00002db0: 616c 6c65 6c20 6973 2031 2061 6e64 206d  allel is 1 and m
-00002dc0: 6f64 656c 5f70 6172 616c 6c65 6c20 6973  odel_parallel is
-00002dd0: 2031 2c20 7468 6520 6361 6c63 756c 6174   1, the calculat
-00002de0: 696f 6e20 7769 6c6c 2062 6520 7265 7065  ion will be repe
-00002df0: 6174 6564 206f 6e20 6561 6368 0a20 2020  ated on each.   
-00002e00: 2020 2020 2020 2020 2064 6576 6963 652e           device.
-00002e10: 0a0a 2020 2020 2020 2020 4172 6773 3a0a  ..        Args:.
-00002e20: 2020 2020 2020 2020 2020 2020 6461 7461              data
-00002e30: 5f70 6172 616c 6c65 6c20 2869 6e74 293a  _parallel (int):
-00002e40: 2054 6865 2064 6174 6120 7061 7261 6c6c   The data parall
-00002e50: 656c 2077 6179 2e20 5468 6520 696e 7075  el way. The inpu
-00002e60: 7420 6461 7461 2077 696c 6c20 6265 2073  t data will be s
-00002e70: 6c69 6365 6420 696e 746f 206e 2070 6172  liced into n par
-00002e80: 7473 2066 6f72 2065 6163 6820 6c61 7965  ts for each laye
-00002e90: 720a 2020 2020 2020 2020 2020 2020 2020  r.              
-00002ea0: 2020 6163 636f 7264 696e 6720 746f 2074    according to t
-00002eb0: 6865 2064 6174 6120 7061 7261 6c6c 656c  he data parallel
-00002ec0: 2077 6179 2e20 4465 6661 756c 743a 2031   way. Default: 1
-00002ed0: 2e0a 2020 2020 2020 2020 2020 2020 6d6f  ..            mo
-00002ee0: 6465 6c5f 7061 7261 6c6c 656c 2028 696e  del_parallel (in
-00002ef0: 7429 3a20 5468 6520 6d6f 6465 6c20 7061  t): The model pa
-00002f00: 7261 6c6c 656c 2077 6179 2e20 5468 6520  rallel way. The 
-00002f10: 7061 7261 6d65 7465 7273 206f 6620 6465  parameters of de
-00002f20: 6e73 6520 6c61 7965 7273 2069 6e20 4d75  nse layers in Mu
-00002f30: 6c74 6968 6561 6441 7474 656e 7469 6f6e  ltiheadAttention
-00002f40: 2061 6e64 0a20 2020 2020 2020 2020 2020   and.           
-00002f50: 2020 2020 2046 6565 6446 6f72 7761 7264       FeedForward
-00002f60: 206c 6179 6572 2077 696c 6c20 6265 2073   layer will be s
-00002f70: 6c69 6365 6420 6163 636f 7264 696e 6720  liced according 
-00002f80: 746f 2074 6865 206d 6f64 656c 2070 6172  to the model par
-00002f90: 616c 6c65 6c20 7761 792e 2044 6566 6175  allel way. Defau
-00002fa0: 6c74 3a20 312e 0a20 2020 2020 2020 2020  lt: 1..         
-00002fb0: 2020 2065 7870 6572 745f 7061 7261 6c6c     expert_parall
-00002fc0: 656c 2028 696e 7429 3a20 5468 6520 6578  el (int): The ex
-00002fd0: 7065 7274 2070 6172 616c 6c65 6c20 7761  pert parallel wa
-00002fe0: 792e 2054 6869 7320 6973 2065 6666 6563  y. This is effec
-00002ff0: 7469 7665 206f 6e6c 7920 7768 656e 204d  tive only when M
-00003000: 6f45 2028 4d69 7874 7572 6520 6f66 2045  oE (Mixture of E
-00003010: 7870 6572 7473 290a 2020 2020 2020 2020  xperts).        
-00003020: 2020 2020 2020 2020 6973 2061 7070 6c69          is appli
-00003030: 6564 2e20 5468 6973 2076 616c 7565 2073  ed. This value s
-00003040: 7065 6369 6669 6573 2074 6865 206e 756d  pecifies the num
-00003050: 6265 7220 6f66 2070 6172 7469 7469 6f6e  ber of partition
-00003060: 7320 746f 2073 706c 6974 2074 6865 2065  s to split the e
-00003070: 7870 6572 7473 2069 6e74 6f2e 0a20 2020  xperts into..   
-00003080: 2020 2020 2020 2020 2070 6970 656c 696e           pipelin
-00003090: 655f 7374 6167 6520 2869 6e74 293a 2054  e_stage (int): T
-000030a0: 6865 206e 756d 6265 7220 6f66 2074 6865  he number of the
-000030b0: 2070 6970 656c 696e 6520 7374 6167 652e   pipeline stage.
-000030c0: 2053 686f 756c 6420 6265 2061 2070 6f73   Should be a pos
-000030d0: 6974 6976 6520 7661 6c75 652e 2044 6566  itive value. Def
-000030e0: 6175 6c74 3a20 312e 0a20 2020 2020 2020  ault: 1..       
-000030f0: 2020 2020 206d 6963 726f 5f62 6174 6368       micro_batch
-00003100: 5f6e 756d 2028 696e 7429 3a20 5468 6520  _num (int): The 
-00003110: 6d69 6372 6f20 7369 7a65 206f 6620 7468  micro size of th
-00003120: 6520 6261 7463 6865 7320 666f 7220 7468  e batches for th
-00003130: 6520 7069 7065 6c69 6e65 2074 7261 696e  e pipeline train
-00003140: 696e 672e 2044 6566 6175 6c74 3a20 312e  ing. Default: 1.
-00003150: 0a20 2020 2020 2020 2020 2020 206f 7074  .            opt
-00003160: 696d 697a 6572 5f73 6861 7264 2028 626f  imizer_shard (bo
-00003170: 6f6c 293a 202a 6f70 7469 6d69 7a65 725f  ol): *optimizer_
-00003180: 7368 6172 6420 6973 2064 6570 7265 6361  shard is depreca
-00003190: 7465 6420 6672 6f6d 204d 696e 6446 6f72  ted from MindFor
-000031a0: 6d65 7273 2072 302e 372e 2049 7420 7769  mers r0.7. It wi
-000031b0: 6c6c 206e 6f74 2068 6176 6520 616e 7920  ll not have any 
-000031c0: 6566 6665 6374 2e0a 2020 2020 2020 2020  effect..        
-000031d0: 2020 2020 2020 2020 4974 2077 696c 6c20          It will 
-000031e0: 6265 2072 656d 6f76 6564 2069 6e20 7468  be removed in th
-000031f0: 6520 6675 7475 7265 2076 6572 7369 6f6e  e future version
-00003200: 2e20 5573 696e 6720 7061 7261 6c6c 656c  . Using parallel
-00003210: 2e65 6e61 626c 655f 7061 7261 6c6c 656c  .enable_parallel
-00003220: 5f6f 7074 696d 697a 6572 2069 6e73 7465  _optimizer inste
-00003230: 6164 2e2a 0a20 2020 2020 2020 2020 2020  ad.*.           
-00003240: 2067 7261 6469 656e 745f 6167 6772 6567   gradient_aggreg
-00003250: 6174 696f 6e5f 6772 6f75 7020 2869 6e74  ation_group (int
-00003260: 293a 2054 6865 2066 7573 696f 6e20 6772  ): The fusion gr
-00003270: 6f75 7020 7369 7a65 206f 6620 7468 6520  oup size of the 
-00003280: 6f70 7469 6d69 7a65 7220 7374 6174 6520  optimizer state 
-00003290: 7368 6172 6469 6e67 2e20 4465 6661 756c  sharding. Defaul
-000032a0: 743a 2034 2e0a 2020 2020 2020 2020 2020  t: 4..          
-000032b0: 2020 7265 636f 6d70 7574 6520 2855 6e69    recompute (Uni
-000032c0: 6f6e 5b54 7261 6e73 666f 726d 6572 5265  on[TransformerRe
-000032d0: 636f 6d70 7574 6543 6f6e 6669 672c 2062  computeConfig, b
-000032e0: 6f6f 6c5d 293a 2054 6865 2063 6f6e 6669  ool]): The confi
-000032f0: 6775 7261 7469 6f6e 206f 6620 7265 636f  guration of reco
-00003300: 6d70 7574 6174 696f 6e20 666f 720a 2020  mputation for.  
-00003310: 2020 2020 2020 2020 2020 2020 2020 7468                th
-00003320: 6520 7472 616e 7366 6f72 6d65 7220 626c  e transformer bl
-00003330: 6f63 6b2e 2044 6566 6175 6c74 3a20 416e  ock. Default: An
-00003340: 2069 6e73 7461 6e63 6520 6f66 2054 7261   instance of Tra
-00003350: 6e73 666f 726d 6572 5265 636f 6d70 7574  nsformerRecomput
-00003360: 6543 6f6e 6669 6720 7769 7468 2064 6566  eConfig with def
-00003370: 6175 6c74 2076 616c 7565 732e 0a20 2020  ault values..   
-00003380: 2020 2020 2020 2020 2076 6f63 6162 5f65           vocab_e
-00003390: 6d62 5f64 7020 2862 6f6f 6c29 3a20 5368  mb_dp (bool): Sh
-000033a0: 6172 6420 656d 6265 6464 696e 6720 696e  ard embedding in
-000033b0: 206d 6f64 656c 2070 6172 616c 6c65 6c20   model parallel 
-000033c0: 6f72 2064 6174 6120 7061 7261 6c6c 656c  or data parallel
-000033d0: 2e20 4465 6661 756c 743a 2054 7275 652e  . Default: True.
-000033e0: 0a0a 2020 2020 2020 2020 5375 7070 6f72  ..        Suppor
-000033f0: 7465 6420 506c 6174 666f 726d 733a 0a20  ted Platforms:. 
-00003400: 2020 2020 2020 2020 2020 2060 6041 7363             ``Asc
-00003410: 656e 6460 6020 6060 4750 5560 600a 0a20  end`` ``GPU``.. 
-00003420: 2020 2020 2020 2045 7861 6d70 6c65 733a         Examples:
-00003430: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
-00003440: 2066 726f 6d20 6d69 6e64 666f 726d 6572   from mindformer
-00003450: 732e 6d6f 6475 6c65 732e 7472 616e 7366  s.modules.transf
-00003460: 6f72 6d65 7220 696d 706f 7274 2054 7261  ormer import Tra
-00003470: 6e73 666f 726d 6572 5265 636f 6d70 7574  nsformerRecomput
-00003480: 6543 6f6e 6669 670a 2020 2020 2020 2020  eConfig.        
-00003490: 2020 2020 3e3e 3e20 7265 636f 6d70 7574      >>> recomput
-000034a0: 655f 636f 6e66 6967 3d54 7261 6e73 666f  e_config=Transfo
-000034b0: 726d 6572 5265 636f 6d70 7574 6543 6f6e  rmerRecomputeCon
-000034c0: 6669 6728 7265 636f 6d70 7574 653d 5472  fig(recompute=Tr
-000034d0: 7565 2c20 7061 7261 6c6c 656c 5f6f 7074  ue, parallel_opt
-000034e0: 696d 697a 6572 5f63 6f6d 6d5f 7265 636f  imizer_comm_reco
-000034f0: 6d70 7574 653d 5472 7565 2c20 5c0a 2020  mpute=True, \.  
-00003500: 2020 2020 2020 2020 2020 2e2e 2e20 2020            ...   
-00003510: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00003520: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00003530: 2020 2020 2020 2020 2020 6d70 5f63 6f6d            mp_com
-00003540: 6d5f 7265 636f 6d70 7574 653d 5472 7565  m_recompute=True
-00003550: 2c20 7265 636f 6d70 7574 655f 736c 6963  , recompute_slic
-00003560: 655f 6163 7469 7661 7469 6f6e 3d54 7275  e_activation=Tru
-00003570: 6529 0a20 2020 2020 2020 2020 2020 203e  e).            >
-00003580: 3e3e 2063 6f6e 6669 673d 5472 616e 7366  >> config=Transf
-00003590: 6f72 6d65 724f 7050 6172 616c 6c65 6c43  ormerOpParallelC
-000035a0: 6f6e 6669 6728 6461 7461 5f70 6172 616c  onfig(data_paral
-000035b0: 6c65 6c3d 312c 206d 6f64 656c 5f70 6172  lel=1, model_par
-000035c0: 616c 6c65 6c3d 312c 2072 6563 6f6d 7075  allel=1, recompu
-000035d0: 7465 3d72 6563 6f6d 7075 7465 5f63 6f6e  te=recompute_con
-000035e0: 6669 6729 0a20 2020 2022 2222 0a0a 2020  fig).    """..  
-000035f0: 2020 4061 7267 735f 7479 7065 5f63 6865    @args_type_che
-00003600: 636b 2872 6563 6f6d 7075 7465 3d28 5472  ck(recompute=(Tr
-00003610: 616e 7366 6f72 6d65 7252 6563 6f6d 7075  ansformerRecompu
-00003620: 7465 436f 6e66 6967 2c20 6469 6374 2929  teConfig, dict))
-00003630: 0a20 2020 2064 6566 205f 5f69 6e69 745f  .    def __init_
-00003640: 5f28 7365 6c66 2c20 6461 7461 5f70 6172  _(self, data_par
-00003650: 616c 6c65 6c3d 312c 206d 6f64 656c 5f70  allel=1, model_p
-00003660: 6172 616c 6c65 6c3d 312c 2065 7870 6572  arallel=1, exper
-00003670: 745f 7061 7261 6c6c 656c 3d31 2c20 7069  t_parallel=1, pi
-00003680: 7065 6c69 6e65 5f73 7461 6765 3d31 2c20  peline_stage=1, 
-00003690: 6d69 6372 6f5f 6261 7463 685f 6e75 6d3d  micro_batch_num=
-000036a0: 312c 0a20 2020 2020 2020 2020 2020 2020  1,.             
-000036b0: 2020 2020 7265 636f 6d70 7574 653a 2055      recompute: U
-000036c0: 6e69 6f6e 5b54 7261 6e73 666f 726d 6572  nion[Transformer
-000036d0: 5265 636f 6d70 7574 6543 6f6e 6669 672c  RecomputeConfig,
-000036e0: 2064 6963 745d 203d 2064 6566 6175 6c74   dict] = default
-000036f0: 5f74 7261 6e73 666f 726d 6572 5f72 6563  _transformer_rec
-00003700: 6f6d 7075 7465 5f63 6f6e 6669 672c 0a20  ompute_config,. 
-00003710: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00003720: 7573 655f 7365 715f 7061 7261 6c6c 656c  use_seq_parallel
-00003730: 3d46 616c 7365 2c20 6f70 7469 6d69 7a65  =False, optimize
-00003740: 725f 7368 6172 643d 4e6f 6e65 2c20 6772  r_shard=None, gr
-00003750: 6164 6965 6e74 5f61 6767 7265 6761 7469  adient_aggregati
-00003760: 6f6e 5f67 726f 7570 3d34 2c20 766f 6361  on_group=4, voca
-00003770: 625f 656d 625f 6470 3d54 7275 6529 3a0a  b_emb_dp=True):.
-00003780: 2020 2020 2020 2020 6966 2069 7369 6e73          if isins
-00003790: 7461 6e63 6528 7265 636f 6d70 7574 652c  tance(recompute,
-000037a0: 2064 6963 7429 3a0a 2020 2020 2020 2020   dict):.        
-000037b0: 2020 2020 7265 636f 6d70 7574 6520 3d20      recompute = 
-000037c0: 5472 616e 7366 6f72 6d65 7252 6563 6f6d  TransformerRecom
-000037d0: 7075 7465 436f 6e66 6967 282a 2a72 6563  puteConfig(**rec
-000037e0: 6f6d 7075 7465 290a 2020 2020 2020 2020  ompute).        
-000037f0: 7365 6c66 2e72 6563 6f6d 7075 7465 203d  self.recompute =
-00003800: 2072 6563 6f6d 7075 7465 0a20 2020 2020   recompute.     
-00003810: 2020 2073 656c 662e 7365 6c65 6374 5f72     self.select_r
-00003820: 6563 6f6d 7075 7465 203d 2072 6563 6f6d  ecompute = recom
-00003830: 7075 7465 2e73 656c 6563 745f 7265 636f  pute.select_reco
-00003840: 6d70 7574 650a 2020 2020 2020 2020 7365  mpute.        se
-00003850: 6c66 2e75 7365 5f73 6571 5f70 6172 616c  lf.use_seq_paral
-00003860: 6c65 6c20 3d20 7573 655f 7365 715f 7061  lel = use_seq_pa
-00003870: 7261 6c6c 656c 0a20 2020 2020 2020 2073  rallel.        s
-00003880: 656c 662e 6f70 7469 6d69 7a65 725f 7368  elf.optimizer_sh
-00003890: 6172 6420 3d20 6f70 7469 6d69 7a65 725f  ard = optimizer_
-000038a0: 7368 6172 640a 2020 2020 2020 2020 7365  shard.        se
-000038b0: 6c66 2e67 7261 6469 656e 745f 6167 6772  lf.gradient_aggr
-000038c0: 6567 6174 696f 6e5f 6772 6f75 7020 3d20  egation_group = 
-000038d0: 6772 6164 6965 6e74 5f61 6767 7265 6761  gradient_aggrega
-000038e0: 7469 6f6e 5f67 726f 7570 0a20 2020 2020  tion_group.     
-000038f0: 2020 2073 656c 662e 5f65 6d62 6564 5f64     self._embed_d
-00003900: 705f 6d70 5f63 6f6e 6669 6720 3d20 456d  p_mp_config = Em
-00003910: 6265 6464 696e 674f 7050 6172 616c 6c65  beddingOpParalle
-00003920: 6c43 6f6e 6669 6728 0a20 2020 2020 2020  lConfig(.       
-00003930: 2020 2020 2064 6174 615f 7061 7261 6c6c       data_parall
-00003940: 656c 3d64 6174 615f 7061 7261 6c6c 656c  el=data_parallel
-00003950: 2c20 6d6f 6465 6c5f 7061 7261 6c6c 656c  , model_parallel
-00003960: 3d6d 6f64 656c 5f70 6172 616c 6c65 6c2c  =model_parallel,
-00003970: 0a20 2020 2020 2020 2020 2020 2076 6f63  .            voc
-00003980: 6162 5f65 6d62 5f64 703d 766f 6361 625f  ab_emb_dp=vocab_
-00003990: 656d 625f 6470 2c20 7573 655f 7365 715f  emb_dp, use_seq_
-000039a0: 7061 7261 6c6c 656c 3d75 7365 5f73 6571  parallel=use_seq
-000039b0: 5f70 6172 616c 6c65 6c2c 0a20 2020 2020  _parallel,.     
-000039c0: 2020 2020 2020 2073 656c 6563 745f 7265         select_re
-000039d0: 636f 6d70 7574 653d 7265 636f 6d70 7574  compute=recomput
-000039e0: 652e 7365 6c65 6374 5f72 6563 6f6d 7075  e.select_recompu
-000039f0: 7465 290a 2020 2020 2020 2020 7365 6c66  te).        self
-00003a00: 2e5f 7070 5f63 6f6e 6669 6720 3d20 5f50  ._pp_config = _P
-00003a10: 6970 654c 696e 6543 6f6e 6669 6728 7069  ipeLineConfig(pi
-00003a20: 7065 6c69 6e65 5f73 7461 6765 3d70 6970  peline_stage=pip
-00003a30: 656c 696e 655f 7374 6167 652c 206d 6963  eline_stage, mic
-00003a40: 726f 5f62 6174 6368 5f6e 756d 3d6d 6963  ro_batch_num=mic
-00003a50: 726f 5f62 6174 6368 5f6e 756d 290a 2020  ro_batch_num).  
-00003a60: 2020 2020 2020 7365 6c66 2e5f 6d6f 655f        self._moe_
-00003a70: 636f 6e66 6967 203d 204d 6f45 5061 7261  config = MoEPara
-00003a80: 6c6c 656c 436f 6e66 6967 280a 2020 2020  llelConfig(.    
-00003a90: 2020 2020 2020 2020 6461 7461 5f70 6172          data_par
-00003aa0: 616c 6c65 6c3d 6461 7461 5f70 6172 616c  allel=data_paral
-00003ab0: 6c65 6c2c 206d 6f64 656c 5f70 6172 616c  lel, model_paral
-00003ac0: 6c65 6c3d 6d6f 6465 6c5f 7061 7261 6c6c  lel=model_parall
-00003ad0: 656c 2c0a 2020 2020 2020 2020 2020 2020  el,.            
-00003ae0: 7365 6c65 6374 5f72 6563 6f6d 7075 7465  select_recompute
-00003af0: 3d72 6563 6f6d 7075 7465 2e73 656c 6563  =recompute.selec
-00003b00: 745f 7265 636f 6d70 7574 652c 0a20 2020  t_recompute,.   
-00003b10: 2020 2020 2020 2020 2065 7870 6572 745f           expert_
-00003b20: 7061 7261 6c6c 656c 3d65 7870 6572 745f  parallel=expert_
-00003b30: 7061 7261 6c6c 656c 2c20 7573 655f 7365  parallel, use_se
-00003b40: 715f 7061 7261 6c6c 656c 3d75 7365 5f73  q_parallel=use_s
-00003b50: 6571 5f70 6172 616c 6c65 6c29 0a0a 2020  eq_parallel)..  
-00003b60: 2020 6465 6620 5f5f 6571 5f5f 2873 656c    def __eq__(sel
-00003b70: 662c 206f 7468 6572 2920 2d3e 2062 6f6f  f, other) -> boo
-00003b80: 6c3a 0a20 2020 2020 2020 2072 6574 7572  l:.        retur
-00003b90: 6e20 6973 696e 7374 616e 6365 286f 7468  n isinstance(oth
-00003ba0: 6572 2c20 5472 616e 7366 6f72 6d65 724f  er, TransformerO
-00003bb0: 7050 6172 616c 6c65 6c43 6f6e 6669 6729  pParallelConfig)
-00003bc0: 2061 6e64 2028 7365 6c66 2e74 6f5f 6469   and (self.to_di
-00003bd0: 6374 2829 203d 3d20 6f74 6865 722e 746f  ct() == other.to
-00003be0: 5f64 6963 7428 2929 0a0a 2020 2020 6465  _dict())..    de
-00003bf0: 6620 746f 5f64 6966 665f 6469 6374 2873  f to_diff_dict(s
-00003c00: 656c 6629 3a0a 2020 2020 2020 2020 636f  elf):.        co
-00003c10: 6e66 6967 5f64 6963 7420 3d20 7365 6c66  nfig_dict = self
-00003c20: 2e74 6f5f 6469 6374 2829 0a20 2020 2020  .to_dict().     
-00003c30: 2020 2064 6566 6175 6c74 5f64 6963 7420     default_dict 
-00003c40: 3d20 6465 6661 756c 745f 7472 616e 7366  = default_transf
-00003c50: 6f72 6d65 725f 636f 6e66 6967 2e74 6f5f  ormer_config.to_
-00003c60: 6469 6374 2829 0a20 2020 2020 2020 2072  dict().        r
-00003c70: 6573 5f64 6963 7420 3d20 7b7d 0a20 2020  es_dict = {}.   
-00003c80: 2020 2020 2066 6f72 206b 2c20 7620 696e       for k, v in
-00003c90: 2063 6f6e 6669 675f 6469 6374 2e69 7465   config_dict.ite
-00003ca0: 6d73 2829 3a0a 2020 2020 2020 2020 2020  ms():.          
-00003cb0: 2020 6966 2076 2021 3d20 6465 6661 756c    if v != defaul
-00003cc0: 745f 6469 6374 5b6b 5d3a 0a20 2020 2020  t_dict[k]:.     
-00003cd0: 2020 2020 2020 2020 2020 2072 6573 5f64             res_d
-00003ce0: 6963 745b 6b5d 203d 2076 0a20 2020 2020  ict[k] = v.     
-00003cf0: 2020 2069 6620 2272 6563 6f6d 7075 7465     if "recompute
-00003d00: 2220 696e 2072 6573 5f64 6963 743a 0a20  " in res_dict:. 
-00003d10: 2020 2020 2020 2020 2020 2072 6573 5f64             res_d
-00003d20: 6963 745b 2272 6563 6f6d 7075 7465 225d  ict["recompute"]
-00003d30: 203d 2073 656c 662e 7265 636f 6d70 7574   = self.recomput
-00003d40: 652e 746f 5f64 6966 665f 6469 6374 2829  e.to_diff_dict()
-00003d50: 0a20 2020 2020 2020 2072 6574 7572 6e20  .        return 
-00003d60: 7265 735f 6469 6374 0a0a 2020 2020 6465  res_dict..    de
-00003d70: 6620 746f 5f64 6963 7428 7365 6c66 293a  f to_dict(self):
-00003d80: 0a20 2020 2020 2020 2022 2222 746f 2064  .        """to d
-00003d90: 6963 7422 2222 0a20 2020 2020 2020 2063  ict""".        c
-00003da0: 6f6e 6669 675f 6469 6374 203d 207b 0a20  onfig_dict = {. 
-00003db0: 2020 2020 2020 2020 2020 2027 6461 7461             'data
-00003dc0: 5f70 6172 616c 6c65 6c27 3a20 7365 6c66  _parallel': self
-00003dd0: 2e64 6174 615f 7061 7261 6c6c 656c 2c0a  .data_parallel,.
-00003de0: 2020 2020 2020 2020 2020 2020 276d 6f64              'mod
-00003df0: 656c 5f70 6172 616c 6c65 6c27 3a20 7365  el_parallel': se
-00003e00: 6c66 2e6d 6f64 656c 5f70 6172 616c 6c65  lf.model_paralle
-00003e10: 6c2c 0a20 2020 2020 2020 2020 2020 2027  l,.            '
-00003e20: 6578 7065 7274 5f70 6172 616c 6c65 6c27  expert_parallel'
-00003e30: 3a20 7365 6c66 2e65 7870 6572 745f 7061  : self.expert_pa
-00003e40: 7261 6c6c 656c 2c0a 2020 2020 2020 2020  rallel,.        
-00003e50: 2020 2020 2770 6970 656c 696e 655f 7374      'pipeline_st
-00003e60: 6167 6527 3a20 7365 6c66 2e70 6970 656c  age': self.pipel
-00003e70: 696e 655f 7374 6167 652c 0a20 2020 2020  ine_stage,.     
-00003e80: 2020 2020 2020 2027 6d69 6372 6f5f 6261         'micro_ba
-00003e90: 7463 685f 6e75 6d27 3a20 7365 6c66 2e6d  tch_num': self.m
-00003ea0: 6963 726f 5f62 6174 6368 5f6e 756d 2c0a  icro_batch_num,.
-00003eb0: 2020 2020 2020 2020 2020 2020 2775 7365              'use
-00003ec0: 5f73 6571 5f70 6172 616c 6c65 6c27 3a20  _seq_parallel': 
-00003ed0: 7365 6c66 2e75 7365 5f73 6571 5f70 6172  self.use_seq_par
-00003ee0: 616c 6c65 6c2c 0a20 2020 2020 2020 2020  allel,.         
-00003ef0: 2020 2027 6f70 7469 6d69 7a65 725f 7368     'optimizer_sh
-00003f00: 6172 6427 3a20 7365 6c66 2e6f 7074 696d  ard': self.optim
-00003f10: 697a 6572 5f73 6861 7264 2c0a 2020 2020  izer_shard,.    
-00003f20: 2020 2020 2020 2020 2767 7261 6469 656e          'gradien
-00003f30: 745f 6167 6772 6567 6174 696f 6e5f 6772  t_aggregation_gr
-00003f40: 6f75 7027 3a20 7365 6c66 2e67 7261 6469  oup': self.gradi
-00003f50: 656e 745f 6167 6772 6567 6174 696f 6e5f  ent_aggregation_
-00003f60: 6772 6f75 702c 0a20 2020 2020 2020 2020  group,.         
-00003f70: 2020 2027 766f 6361 625f 656d 625f 6470     'vocab_emb_dp
-00003f80: 273a 2073 656c 662e 766f 6361 625f 656d  ': self.vocab_em
-00003f90: 625f 6470 2c0a 2020 2020 2020 2020 2020  b_dp,.          
-00003fa0: 2020 2772 6563 6f6d 7075 7465 273a 2073    'recompute': s
-00003fb0: 656c 662e 7265 636f 6d70 7574 652e 746f  elf.recompute.to
-00003fc0: 5f64 6963 7428 290a 2020 2020 2020 2020  _dict().        
-00003fd0: 7d0a 2020 2020 2020 2020 7265 7475 726e  }.        return
-00003fe0: 2063 6f6e 6669 675f 6469 6374 0a0a 2020   config_dict..  
-00003ff0: 2020 4070 726f 7065 7274 790a 2020 2020    @property.    
-00004000: 6465 6620 7265 636f 6d70 7574 6528 7365  def recompute(se
-00004010: 6c66 293a 0a20 2020 2020 2020 2072 6574  lf):.        ret
-00004020: 7572 6e20 7365 6c66 2e5f 7265 636f 6d70  urn self._recomp
-00004030: 7574 650a 0a20 2020 2040 7265 636f 6d70  ute..    @recomp
-00004040: 7574 652e 7365 7474 6572 0a20 2020 2064  ute.setter.    d
-00004050: 6566 2072 6563 6f6d 7075 7465 2873 656c  ef recompute(sel
-00004060: 662c 2076 616c 7565 293a 0a20 2020 2020  f, value):.     
-00004070: 2020 2069 6620 6e6f 7420 6973 696e 7374     if not isinst
-00004080: 616e 6365 2876 616c 7565 2c20 5472 616e  ance(value, Tran
-00004090: 7366 6f72 6d65 7252 6563 6f6d 7075 7465  sformerRecompute
-000040a0: 436f 6e66 6967 2920 616e 6420 6e6f 7420  Config) and not 
-000040b0: 6973 696e 7374 616e 6365 2876 616c 7565  isinstance(value
-000040c0: 2c20 626f 6f6c 293a 0a20 2020 2020 2020  , bool):.       
-000040d0: 2020 2020 2072 6169 7365 2054 7970 6545       raise TypeE
-000040e0: 7272 6f72 2866 2272 6563 6f6d 7075 7465  rror(f"recompute
-000040f0: 206d 7573 7420 6265 2061 2054 7261 6e73   must be a Trans
-00004100: 666f 726d 6572 5265 636f 6d70 7574 6543  formerRecomputeC
-00004110: 6f6e 6669 672f 626f 6f6c 2c20 6275 7420  onfig/bool, but 
-00004120: 676f 7420 7b74 7970 6528 7661 6c75 6529  got {type(value)
-00004130: 2e5f 5f6e 616d 655f 5f7d 2e22 290a 2020  .__name__}.").  
-00004140: 2020 2020 2020 6966 2069 7369 6e73 7461        if isinsta
-00004150: 6e63 6528 7661 6c75 652c 2062 6f6f 6c29  nce(value, bool)
-00004160: 3a0a 2020 2020 2020 2020 2020 2020 6c6f  :.            lo
-00004170: 6767 6572 2e77 6172 6e69 6e67 2866 2254  gger.warning(f"T
-00004180: 7261 6e73 666f 726d 6572 5265 636f 6d70  ransformerRecomp
-00004190: 7574 6543 6f6e 6669 6720 6973 2072 6563  uteConfig is rec
-000041a0: 6f6d 6d65 6e64 6564 2061 7320 7468 6520  ommended as the 
-000041b0: 7265 636f 6d70 7574 6520 636f 6e66 6967  recompute config
-000041c0: 7572 6174 696f 6e20 7479 7065 2e22 290a  uration type.").
-000041d0: 2020 2020 2020 2020 7365 6c66 2e5f 7265          self._re
-000041e0: 636f 6d70 7574 6520 3d20 7661 6c75 650a  compute = value.
-000041f0: 0a20 2020 2040 7072 6f70 6572 7479 0a20  .    @property. 
-00004200: 2020 2064 6566 2076 6f63 6162 5f65 6d62     def vocab_emb
-00004210: 5f64 7028 7365 6c66 293a 0a20 2020 2020  _dp(self):.     
-00004220: 2020 2072 6574 7572 6e20 7365 6c66 2e5f     return self._
-00004230: 656d 6265 645f 6470 5f6d 705f 636f 6e66  embed_dp_mp_conf
-00004240: 6967 2e76 6f63 6162 5f65 6d62 5f64 700a  ig.vocab_emb_dp.
-00004250: 0a20 2020 2040 766f 6361 625f 656d 625f  .    @vocab_emb_
-00004260: 6470 2e73 6574 7465 720a 2020 2020 6465  dp.setter.    de
-00004270: 6620 766f 6361 625f 656d 625f 6470 2873  f vocab_emb_dp(s
-00004280: 656c 662c 2076 616c 7565 293a 0a20 2020  elf, value):.   
-00004290: 2020 2020 2073 656c 662e 5f65 6d62 6564       self._embed
-000042a0: 5f64 705f 6d70 5f63 6f6e 6669 672e 766f  _dp_mp_config.vo
-000042b0: 6361 625f 656d 625f 6470 203d 2076 616c  cab_emb_dp = val
-000042c0: 7565 0a0a 2020 2020 4070 726f 7065 7274  ue..    @propert
-000042d0: 790a 2020 2020 6465 6620 6772 6164 6965  y.    def gradie
-000042e0: 6e74 5f61 6767 7265 6761 7469 6f6e 5f67  nt_aggregation_g
-000042f0: 726f 7570 2873 656c 6629 3a0a 2020 2020  roup(self):.    
-00004300: 2020 2020 7265 7475 726e 2073 656c 662e      return self.
-00004310: 5f67 7261 6469 656e 745f 6167 6772 6567  _gradient_aggreg
-00004320: 6174 696f 6e5f 6772 6f75 700a 0a20 2020  ation_group..   
-00004330: 2040 6772 6164 6965 6e74 5f61 6767 7265   @gradient_aggre
-00004340: 6761 7469 6f6e 5f67 726f 7570 2e73 6574  gation_group.set
-00004350: 7465 720a 2020 2020 6465 6620 6772 6164  ter.    def grad
-00004360: 6965 6e74 5f61 6767 7265 6761 7469 6f6e  ient_aggregation
-00004370: 5f67 726f 7570 2873 656c 662c 2076 616c  _group(self, val
-00004380: 7565 293a 0a20 2020 2020 2020 2056 616c  ue):.        Val
-00004390: 6964 6174 6f72 2e63 6865 636b 5f70 6f73  idator.check_pos
-000043a0: 6974 6976 655f 696e 7428 7661 6c75 652c  itive_int(value,
-000043b0: 2022 6772 6164 6965 6e74 5f61 6767 7265   "gradient_aggre
-000043c0: 6761 7469 6f6e 5f67 726f 7570 2229 0a20  gation_group"). 
-000043d0: 2020 2020 2020 2073 656c 662e 5f67 7261         self._gra
-000043e0: 6469 656e 745f 6167 6772 6567 6174 696f  dient_aggregatio
-000043f0: 6e5f 6772 6f75 7020 3d20 7661 6c75 650a  n_group = value.
-00004400: 0a20 2020 2040 7072 6f70 6572 7479 0a20  .    @property. 
-00004410: 2020 2064 6566 206d 6963 726f 5f62 6174     def micro_bat
-00004420: 6368 5f6e 756d 2873 656c 6629 3a0a 2020  ch_num(self):.  
-00004430: 2020 2020 2020 7265 7475 726e 2073 656c        return sel
-00004440: 662e 5f70 705f 636f 6e66 6967 2e6d 6963  f._pp_config.mic
-00004450: 726f 5f62 6174 6368 5f6e 756d 0a0a 2020  ro_batch_num..  
-00004460: 2020 406d 6963 726f 5f62 6174 6368 5f6e    @micro_batch_n
-00004470: 756d 2e73 6574 7465 720a 2020 2020 6465  um.setter.    de
-00004480: 6620 6d69 6372 6f5f 6261 7463 685f 6e75  f micro_batch_nu
-00004490: 6d28 7365 6c66 2c20 7661 6c75 6529 3a0a  m(self, value):.
-000044a0: 2020 2020 2020 2020 7365 6c66 2e5f 7070          self._pp
-000044b0: 5f63 6f6e 6669 672e 6d69 6372 6f5f 6261  _config.micro_ba
-000044c0: 7463 685f 6e75 6d20 3d20 7661 6c75 650a  tch_num = value.
-000044d0: 0a20 2020 2040 7072 6f70 6572 7479 0a20  .    @property. 
-000044e0: 2020 2064 6566 206d 6f64 656c 5f70 6172     def model_par
-000044f0: 616c 6c65 6c28 7365 6c66 293a 0a20 2020  allel(self):.   
-00004500: 2020 2020 2072 6574 7572 6e20 7365 6c66       return self
-00004510: 2e5f 656d 6265 645f 6470 5f6d 705f 636f  ._embed_dp_mp_co
-00004520: 6e66 6967 2e6d 6f64 656c 5f70 6172 616c  nfig.model_paral
-00004530: 6c65 6c0a 0a20 2020 2040 6d6f 6465 6c5f  lel..    @model_
-00004540: 7061 7261 6c6c 656c 2e73 6574 7465 720a  parallel.setter.
-00004550: 2020 2020 6465 6620 6d6f 6465 6c5f 7061      def model_pa
-00004560: 7261 6c6c 656c 2873 656c 662c 2076 616c  rallel(self, val
-00004570: 7565 293a 0a20 2020 2020 2020 2073 656c  ue):.        sel
-00004580: 662e 5f65 6d62 6564 5f64 705f 6d70 5f63  f._embed_dp_mp_c
-00004590: 6f6e 6669 672e 6d6f 6465 6c5f 7061 7261  onfig.model_para
-000045a0: 6c6c 656c 203d 2076 616c 7565 0a20 2020  llel = value.   
-000045b0: 2020 2020 2073 656c 662e 5f6d 6f65 5f63       self._moe_c
-000045c0: 6f6e 6669 672e 6d6f 6465 6c5f 7061 7261  onfig.model_para
-000045d0: 6c6c 656c 203d 2076 616c 7565 0a0a 2020  llel = value..  
-000045e0: 2020 4070 726f 7065 7274 790a 2020 2020    @property.    
-000045f0: 6465 6620 6461 7461 5f70 6172 616c 6c65  def data_paralle
-00004600: 6c28 7365 6c66 293a 0a20 2020 2020 2020  l(self):.       
-00004610: 2072 6574 7572 6e20 7365 6c66 2e5f 656d   return self._em
-00004620: 6265 645f 6470 5f6d 705f 636f 6e66 6967  bed_dp_mp_config
-00004630: 2e64 6174 615f 7061 7261 6c6c 656c 0a0a  .data_parallel..
-00004640: 2020 2020 4064 6174 615f 7061 7261 6c6c      @data_parall
-00004650: 656c 2e73 6574 7465 720a 2020 2020 6465  el.setter.    de
-00004660: 6620 6461 7461 5f70 6172 616c 6c65 6c28  f data_parallel(
-00004670: 7365 6c66 2c20 7661 6c75 6529 3a0a 2020  self, value):.  
-00004680: 2020 2020 2020 7365 6c66 2e5f 656d 6265        self._embe
-00004690: 645f 6470 5f6d 705f 636f 6e66 6967 2e64  d_dp_mp_config.d
-000046a0: 6174 615f 7061 7261 6c6c 656c 203d 2076  ata_parallel = v
-000046b0: 616c 7565 0a20 2020 2020 2020 2073 656c  alue.        sel
-000046c0: 662e 5f6d 6f65 5f63 6f6e 6669 672e 6461  f._moe_config.da
-000046d0: 7461 5f70 6172 616c 6c65 6c20 3d20 7661  ta_parallel = va
-000046e0: 6c75 650a 0a20 2020 2040 7072 6f70 6572  lue..    @proper
-000046f0: 7479 0a20 2020 2064 6566 2065 7870 6572  ty.    def exper
-00004700: 745f 7061 7261 6c6c 656c 2873 656c 6629  t_parallel(self)
-00004710: 3a0a 2020 2020 2020 2020 7265 7475 726e  :.        return
-00004720: 2073 656c 662e 5f6d 6f65 5f63 6f6e 6669   self._moe_confi
-00004730: 672e 6578 7065 7274 5f70 6172 616c 6c65  g.expert_paralle
-00004740: 6c0a 0a20 2020 2040 6578 7065 7274 5f70  l..    @expert_p
-00004750: 6172 616c 6c65 6c2e 7365 7474 6572 0a20  arallel.setter. 
-00004760: 2020 2064 6566 2065 7870 6572 745f 7061     def expert_pa
-00004770: 7261 6c6c 656c 2873 656c 662c 2076 616c  rallel(self, val
-00004780: 7565 293a 0a20 2020 2020 2020 2073 656c  ue):.        sel
-00004790: 662e 5f6d 6f65 5f63 6f6e 6669 672e 6578  f._moe_config.ex
-000047a0: 7065 7274 5f70 6172 616c 6c65 6c20 3d20  pert_parallel = 
-000047b0: 7661 6c75 650a 0a20 2020 2040 7072 6f70  value..    @prop
-000047c0: 6572 7479 0a20 2020 2064 6566 2070 6970  erty.    def pip
-000047d0: 656c 696e 655f 7374 6167 6528 7365 6c66  eline_stage(self
-000047e0: 293a 0a20 2020 2020 2020 2072 6574 7572  ):.        retur
-000047f0: 6e20 7365 6c66 2e5f 7070 5f63 6f6e 6669  n self._pp_confi
-00004800: 672e 7069 7065 6c69 6e65 5f73 7461 6765  g.pipeline_stage
-00004810: 0a0a 2020 2020 4070 6970 656c 696e 655f  ..    @pipeline_
-00004820: 7374 6167 652e 7365 7474 6572 0a20 2020  stage.setter.   
-00004830: 2064 6566 2070 6970 656c 696e 655f 7374   def pipeline_st
-00004840: 6167 6528 7365 6c66 2c20 7661 6c75 6529  age(self, value)
-00004850: 3a0a 2020 2020 2020 2020 7365 6c66 2e5f  :.        self._
-00004860: 7070 5f63 6f6e 6669 672e 7069 7065 6c69  pp_config.pipeli
-00004870: 6e65 5f73 7461 6765 203d 2076 616c 7565  ne_stage = value
-00004880: 0a0a 2020 2020 4070 726f 7065 7274 790a  ..    @property.
-00004890: 2020 2020 6465 6620 6f70 7469 6d69 7a65      def optimize
-000048a0: 725f 7368 6172 6428 7365 6c66 293a 0a20  r_shard(self):. 
-000048b0: 2020 2020 2020 2072 6574 7572 6e20 7365         return se
-000048c0: 6c66 2e5f 6f70 7469 6d69 7a65 725f 7368  lf._optimizer_sh
-000048d0: 6172 640a 0a20 2020 2040 6f70 7469 6d69  ard..    @optimi
-000048e0: 7a65 725f 7368 6172 642e 7365 7474 6572  zer_shard.setter
-000048f0: 0a20 2020 2064 6566 206f 7074 696d 697a  .    def optimiz
-00004900: 6572 5f73 6861 7264 2873 656c 662c 2076  er_shard(self, v
-00004910: 616c 7565 293a 0a20 2020 2020 2020 2073  alue):.        s
-00004920: 656c 662e 5f6f 7074 696d 697a 6572 5f73  elf._optimizer_s
-00004930: 6861 7264 203d 2076 616c 7565 0a20 2020  hard = value.   
-00004940: 2020 2020 2069 6620 7661 6c75 653a 0a20       if value:. 
-00004950: 2020 2020 2020 2020 2020 206c 6f67 6765             logge
-00004960: 722e 7761 726e 696e 6728 225c 2270 6172  r.warning("\"par
-00004970: 616c 6c65 6c5f 636f 6e66 6967 2e6f 7074  allel_config.opt
-00004980: 696d 697a 6572 5f73 6861 7264 5c22 2069  imizer_shard\" i
-00004990: 7320 6465 7072 6563 6174 6564 2066 726f  s deprecated fro
-000049a0: 6d20 4d69 6e64 466f 726d 6572 7320 7230  m MindFormers r0
-000049b0: 2e37 2e20 4974 2077 696c 6c20 6e6f 7420  .7. It will not 
-000049c0: 6861 7665 2022 0a20 2020 2020 2020 2020  have ".         
-000049d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000049e0: 2020 2261 6e79 2065 6666 6563 742e 2050    "any effect. P
-000049f0: 6c65 6173 6520 7573 6520 5c22 7061 7261  lease use \"para
-00004a00: 6c6c 656c 2e65 6e61 626c 655f 7061 7261  llel.enable_para
-00004a10: 6c6c 656c 5f6f 7074 696d 697a 6572 5c22  llel_optimizer\"
-00004a20: 2074 6f20 7475 726e 206f 6e20 6f72 206f   to turn on or o
-00004a30: 6666 2074 6865 2022 0a20 2020 2020 2020  ff the ".       
-00004a40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00004a50: 2020 2020 226f 7074 696d 697a 6572 2070      "optimizer p
-00004a60: 6172 616c 6c65 6c2e 2229 0a0a 2020 2020  arallel.")..    
-00004a70: 4070 726f 7065 7274 790a 2020 2020 6465  @property.    de
-00004a80: 6620 656d 6265 6464 696e 675f 6470 5f6d  f embedding_dp_m
-00004a90: 705f 636f 6e66 6967 2873 656c 6629 3a0a  p_config(self):.
-00004aa0: 2020 2020 2020 2020 7265 7475 726e 2073          return s
-00004ab0: 656c 662e 5f65 6d62 6564 5f64 705f 6d70  elf._embed_dp_mp
-00004ac0: 5f63 6f6e 6669 670a 0a20 2020 2040 7072  _config..    @pr
-00004ad0: 6f70 6572 7479 0a20 2020 2064 6566 2064  operty.    def d
-00004ae0: 705f 6d70 5f63 6f6e 6669 6728 7365 6c66  p_mp_config(self
-00004af0: 293a 0a20 2020 2020 2020 2072 6574 7572  ):.        retur
-00004b00: 6e20 7365 6c66 2e5f 656d 6265 645f 6470  n self._embed_dp
-00004b10: 5f6d 705f 636f 6e66 6967 2e64 705f 6d70  _mp_config.dp_mp
-00004b20: 5f63 6f6e 6669 670a 0a20 2020 2040 7072  _config..    @pr
-00004b30: 6f70 6572 7479 0a20 2020 2064 6566 206d  operty.    def m
-00004b40: 6f65 5f70 6172 616c 6c65 6c5f 636f 6e66  oe_parallel_conf
-00004b50: 6967 2873 656c 6629 3a0a 2020 2020 2020  ig(self):.      
-00004b60: 2020 7265 7475 726e 2073 656c 662e 5f6d    return self._m
-00004b70: 6f65 5f63 6f6e 6669 670a 0a0a 6465 6661  oe_config...defa
-00004b80: 756c 745f 7472 616e 7366 6f72 6d65 725f  ult_transformer_
-00004b90: 636f 6e66 6967 203d 2054 7261 6e73 666f  config = Transfo
-00004ba0: 726d 6572 4f70 5061 7261 6c6c 656c 436f  rmerOpParallelCo
-00004bb0: 6e66 6967 2829 0a64 6566 6175 6c74 5f65  nfig().default_e
-00004bc0: 6d62 6564 6469 6e67 5f70 6172 616c 6c65  mbedding_paralle
-00004bd0: 6c5f 636f 6e66 6967 203d 2045 6d62 6564  l_config = Embed
-00004be0: 6469 6e67 4f70 5061 7261 6c6c 656c 436f  dingOpParallelCo
-00004bf0: 6e66 6967 2829 0a0a 0a63 6c61 7373 2046  nfig()...class F
-00004c00: 6565 6446 6f72 7761 7264 2843 656c 6c29  eedForward(Cell)
-00004c10: 3a0a 2020 2020 7222 2222 0a20 2020 2020  :.    r""".     
-00004c20: 2020 2054 6865 206d 756c 7469 6c61 7965     The multilaye
-00004c30: 7220 7065 7263 6570 7472 6f6e 2077 6974  r perceptron wit
-00004c40: 6820 7477 6f20 6c69 6e65 6172 206c 6179  h two linear lay
-00004c50: 6572 7320 7769 7468 2064 726f 706f 7574  ers with dropout
-00004c60: 2061 7070 6c69 6564 2061 7420 6669 6e61   applied at fina
-00004c70: 6c20 6f75 7470 7574 2e20 5468 6520 6669  l output. The fi
-00004c80: 7273 7420 6c69 6e65 6172 0a20 2020 2020  rst linear.     
-00004c90: 2020 2077 696c 6c20 7072 6f6a 6563 7420     will project 
-00004ca0: 7468 6520 696e 7075 7420 6469 6d65 6e73  the input dimens
-00004cb0: 696f 6e20 6672 6f6d 2068 6964 6465 6e5f  ion from hidden_
-00004cc0: 7369 7a65 2074 6f20 6666 6e5f 6869 6464  size to ffn_hidd
-00004cd0: 656e 5f73 697a 652e 2054 6865 2073 6563  en_size. The sec
-00004ce0: 6f6e 6420 6c69 6e65 6172 2077 696c 6c20  ond linear will 
-00004cf0: 7072 6f6a 6563 7420 7468 650a 2020 2020  project the.    
-00004d00: 2020 2020 6469 6d65 6e73 696f 6e20 6672      dimension fr
-00004d10: 6f6d 2066 666e 5f68 6964 6465 6e5f 7369  om ffn_hidden_si
-00004d20: 7a65 2074 6f20 6869 6464 656e 5f73 697a  ze to hidden_siz
-00004d30: 652e 2054 6865 2066 6972 7374 206c 696e  e. The first lin
-00004d40: 6561 7220 6973 2073 6861 7264 6564 206f  ear is sharded o
-00004d50: 6e20 7468 6520 7265 6c61 7469 7665 2064  n the relative d
-00004d60: 696d 656e 7369 6f6e 2c0a 2020 2020 2020  imension,.      
-00004d70: 2020 616e 6420 7468 6520 7365 636f 6e64    and the second
-00004d80: 206c 696e 6561 7220 6973 2073 6861 7264   linear is shard
-00004d90: 6564 206f 6e20 7468 6520 6f75 7470 7574  ed on the output
-00004da0: 2064 696d 656e 7369 6f6e 2e20 5468 6520   dimension. The 
-00004db0: 6f76 6572 7669 6577 2070 726f 6365 7373  overview process
-00004dc0: 2063 616e 2062 653a 0a0a 2020 2020 2020   can be:..      
-00004dd0: 2020 2e2e 206d 6174 683a 3a0a 2020 2020    .. math::.    
-00004de0: 2020 2020 2020 2020 4472 6f70 6f75 7428          Dropout(
-00004df0: 2878 575f 312b 625f 3129 575f 3220 2b20  (xW_1+b_1)W_2 + 
-00004e00: 625f 3229 0a0a 2020 2020 2020 2020 7768  b_2)..        wh
-00004e10: 6572 6520 7468 6520 3a6d 6174 683a 6057  ere the :math:`W
-00004e20: 5f31 2c20 575f 322c 2062 5f31 6020 616e  _1, W_2, b_1` an
-00004e30: 6420 3a6d 6174 683a 6062 5f32 6020 6172  d :math:`b_2` ar
-00004e40: 6520 7472 6169 6e61 626c 6520 7061 7261  e trainable para
-00004e50: 6d65 7465 7273 2e0a 0a20 2020 2020 2020  meters...       
-00004e60: 2041 7267 733a 0a20 2020 2020 2020 2020   Args:.         
-00004e70: 2020 2068 6964 6465 6e5f 7369 7a65 2028     hidden_size (
-00004e80: 696e 7429 3a20 5468 6520 6469 6d65 6e73  int): The dimens
-00004e90: 696f 6e20 6f66 2074 6865 2069 6e70 7574  ion of the input
-00004ea0: 732e 0a20 2020 2020 2020 2020 2020 2066  s..            f
-00004eb0: 666e 5f68 6964 6465 6e5f 7369 7a65 2028  fn_hidden_size (
-00004ec0: 696e 7429 3a20 5468 6520 696e 7465 726d  int): The interm
-00004ed0: 6564 6961 7465 2068 6964 6465 6e20 7369  ediate hidden si
-00004ee0: 7a65 2e0a 2020 2020 2020 2020 2020 2020  ze..            
-00004ef0: 6472 6f70 6f75 745f 7261 7465 2028 666c  dropout_rate (fl
-00004f00: 6f61 7429 3a20 5468 6520 6472 6f70 6f75  oat): The dropou
-00004f10: 7420 7261 7465 2066 6f72 2074 6865 2073  t rate for the s
-00004f20: 6563 6f6e 6420 6c69 6e65 6172 2773 206f  econd linear's o
-00004f30: 7574 7075 742e 0a20 2020 2020 2020 2020  utput..         
-00004f40: 2020 2068 6964 6465 6e5f 6163 7420 2873     hidden_act (s
-00004f50: 7472 2c20 6e6e 2e43 656c 6c29 3a20 5468  tr, nn.Cell): Th
-00004f60: 6520 6163 7469 7661 7469 6f6e 206f 6620  e activation of 
-00004f70: 7468 6520 696e 7465 726e 616c 2066 6565  the internal fee
-00004f80: 6466 6f72 7761 7264 206c 6179 6572 2e20  dforward layer. 
-00004f90: 5375 7070 6f72 7473 2027 7265 6c75 272c  Supports 'relu',
-00004fa0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00004fb0: 2027 7265 6c75 3627 2c20 2774 616e 6827   'relu6', 'tanh'
-00004fc0: 2c20 2767 656c 7527 2c20 2766 6173 745f  , 'gelu', 'fast_
-00004fd0: 6765 6c75 272c 2027 656c 7527 2c20 2773  gelu', 'elu', 's
-00004fe0: 6967 6d6f 6964 272c 2027 7072 656c 7527  igmoid', 'prelu'
-00004ff0: 2c20 276c 6561 6b79 7265 6c75 272c 2027  , 'leakyrelu', '
-00005000: 6873 7769 7368 272c 0a20 2020 2020 2020  hswish',.       
-00005010: 2020 2020 2020 2020 2027 6873 6967 6d6f           'hsigmo
-00005020: 6964 272c 2027 6c6f 6773 6967 6d6f 6964  id', 'logsigmoid
-00005030: 2720 616e 6420 736f 206f 6e2e 2055 7365  ' and so on. Use
-00005040: 7220 6361 6e20 7072 6f76 6964 6520 6375  r can provide cu
-00005050: 7374 6f6d 2061 6374 6976 6974 696f 6e20  stom activition 
-00005060: 746f 2074 6865 2061 7267 756d 656e 742e  to the argument.
-00005070: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00005080: 2049 6620 7573 6572 2077 616e 7473 2074   If user wants t
-00005090: 6f20 7275 6e20 7468 6520 6e65 7420 696e  o run the net in
-000050a0: 2074 6865 2070 6172 616c 6c65 6c20 6d6f   the parallel mo
-000050b0: 6465 2c20 7468 6520 6375 7374 6f6d 2061  de, the custom a
-000050c0: 6374 6976 6174 696f 6e20 6d75 7374 2061  ctivation must a
-000050d0: 6c73 6f20 7072 6f76 6964 650a 2020 2020  lso provide.    
-000050e0: 2020 2020 2020 2020 2020 2020 7468 6520              the 
-000050f0: 6061 6374 6976 6174 696f 6e5f 7368 6172  `activation_shar
-00005100: 6460 2066 756e 6374 696f 6e2e 2050 6c65  d` function. Ple
-00005110: 6173 6520 7365 6520 6578 616d 706c 6573  ase see examples
-00005120: 2e20 4465 6661 756c 743a 2067 656c 752e  . Default: gelu.
-00005130: 0a20 2020 2020 2020 2020 2020 2065 7870  .            exp
-00005140: 6572 745f 6e75 6d20 2869 6e74 293a 2054  ert_num (int): T
-00005150: 6865 206e 756d 6265 7220 6f66 2065 7870  he number of exp
-00005160: 6572 7473 2075 7365 6420 696e 204c 696e  erts used in Lin
-00005170: 6561 722e 2046 6f72 2074 6865 2063 6173  ear. For the cas
-00005180: 6520 6578 7065 7274 5f6e 756d 203e 2031  e expert_num > 1
-00005190: 2c20 4261 7463 684d 6174 4d75 6c20 6973  , BatchMatMul is
-000051a0: 2075 7365 640a 2020 2020 2020 2020 2020   used.          
-000051b0: 2020 2020 2020 616e 6420 7468 6520 6669        and the fi
-000051c0: 7273 7420 6469 6d65 6e73 696f 6e20 696e  rst dimension in
-000051d0: 2042 6174 6368 4d61 744d 756c 2069 6e64   BatchMatMul ind
-000051e0: 6963 6174 6520 6578 7065 7274 5f6e 756d  icate expert_num
-000051f0: 2e20 4465 6661 756c 743a 2031 2e0a 2020  . Default: 1..  
-00005200: 2020 2020 2020 2020 2020 6578 7065 7274            expert
-00005210: 5f67 726f 7570 5f73 697a 6520 2869 6e74  _group_size (int
-00005220: 293a 2054 6865 206e 756d 6265 7220 6f66  ): The number of
-00005230: 2074 6f6b 656e 7320 696e 2065 6163 6820   tokens in each 
-00005240: 6461 7461 2070 6172 616c 6c65 6c20 6772  data parallel gr
-00005250: 6f75 702e 2044 6566 6175 6c74 3a20 4e6f  oup. Default: No
-00005260: 6e65 2e20 5468 6973 2070 6172 616d 6574  ne. This paramet
-00005270: 6572 2069 730a 2020 2020 2020 2020 2020  er is.          
-00005280: 2020 2020 2020 6566 6665 6374 6976 6520        effective 
-00005290: 6f6e 6c79 2077 6865 6e20 696e 2041 5554  only when in AUT
-000052a0: 4f5f 5041 5241 4c4c 454c 206d 6f64 652c  O_PARALLEL mode,
-000052b0: 2061 6e64 204e 4f54 2053 4841 5244 494e   and NOT SHARDIN
-000052c0: 475f 5052 4f50 4147 4154 494f 4e2e 0a20  G_PROPAGATION.. 
-000052d0: 2020 2020 2020 2020 2020 2070 6172 616d             param
-000052e0: 5f69 6e69 745f 7479 7065 2028 6474 7970  _init_type (dtyp
-000052f0: 652e 4e75 6d62 6572 293a 2054 6865 2070  e.Number): The p
-00005300: 6172 616d 6574 6572 2069 6e69 7469 616c  arameter initial
-00005310: 697a 6174 696f 6e20 7479 7065 2e20 5368  ization type. Sh
-00005320: 6f75 6c64 2062 6520 6d73 7479 7065 2e66  ould be mstype.f
-00005330: 6c6f 6174 3332 206f 720a 2020 2020 2020  loat32 or.      
-00005340: 2020 2020 2020 2020 2020 6d73 7479 7065            mstype
-00005350: 2e66 6c6f 6174 3136 2e20 4465 6661 756c  .float16. Defaul
-00005360: 743a 206d 7374 7970 652e 666c 6f61 7433  t: mstype.float3
-00005370: 322e 0a20 2020 2020 2020 2020 2020 2070  2..            p
-00005380: 6172 616c 6c65 6c5f 636f 6e66 6967 2028  arallel_config (
-00005390: 4f70 5061 7261 6c6c 656c 436f 6e66 6967  OpParallelConfig
-000053a0: 2c20 4d6f 4550 6172 616c 6c65 6c43 6f6e  , MoEParallelCon
-000053b0: 6669 6729 3a20 5468 6520 636f 6e66 6967  fig): The config
-000053c0: 206f 6620 7061 7261 6c6c 656c 2073 6574   of parallel set
-000053d0: 7469 6e67 2c20 7365 650a 2020 2020 2020  ting, see.      
-000053e0: 2020 2020 2020 2020 2020 604f 7050 6172            `OpPar
-000053f0: 616c 6c65 6c43 6f6e 6669 6760 206f 7220  allelConfig` or 
-00005400: 604d 6f45 5061 7261 6c6c 656c 436f 6e66  `MoEParallelConf
-00005410: 6967 602e 2057 6865 6e20 4d6f 4520 6973  ig`. When MoE is
-00005420: 2061 7070 6c69 6564 2c20 4d6f 4550 6172   applied, MoEPar
-00005430: 616c 6c65 6c43 6f6e 6669 6720 6973 2065  allelConfig is e
-00005440: 6666 6563 7469 7665 2c0a 2020 2020 2020  ffective,.      
-00005450: 2020 2020 2020 2020 2020 6f74 6865 7277            otherw
-00005460: 6973 6520 4f70 5061 7261 6c6c 656c 436f  ise OpParallelCo
-00005470: 6e66 6967 2069 7320 6566 6665 6374 6976  nfig is effectiv
-00005480: 652e 2044 6566 6175 6c74 2060 6465 6661  e. Default `defa
-00005490: 756c 745f 6470 6d70 5f63 6f6e 6669 6760  ult_dpmp_config`
-000054a0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-000054b0: 2020 616e 2069 6e73 7461 6e63 6520 6f66    an instance of
-000054c0: 2060 4f70 5061 7261 6c6c 656c 436f 6e66   `OpParallelConf
-000054d0: 6967 6020 7769 7468 2064 6566 6175 6c74  ig` with default
-000054e0: 2061 7267 732e 0a0a 2020 2020 2020 2020   args...        
-000054f0: 496e 7075 7473 3a0a 2020 2020 2020 2020  Inputs:.        
-00005500: 2020 2020 2d20 2a2a 782a 2a20 2854 656e      - **x** (Ten
-00005510: 736f 7229 202d 2073 686f 756c 6420 6265  sor) - should be
-00005520: 2060 5b62 6174 6368 2c20 7365 715f 6c65   `[batch, seq_le
-00005530: 6e67 7468 2c20 6869 6464 656e 5f73 697a  ngth, hidden_siz
-00005540: 655d 206f 7220 5b62 6174 6368 202a 2073  e] or [batch * s
-00005550: 6571 5f6c 656e 6774 682c 2068 6964 6465  eq_length, hidde
-00005560: 6e5f 7369 7a65 5d60 2e0a 2020 2020 2020  n_size]`..      
-00005570: 2020 2020 2020 2020 466c 6f61 7420 7465          Float te
-00005580: 6e73 6f72 2e0a 0a20 2020 2020 2020 204f  nsor...        O
-00005590: 7574 7075 7473 3a0a 2020 2020 2020 2020  utputs:.        
-000055a0: 2020 2020 5465 6e73 6f72 2c20 7468 6520      Tensor, the 
-000055b0: 6f75 7470 7574 206f 6620 7468 6973 206c  output of this l
-000055c0: 6179 6572 2061 6674 6572 206d 6170 7069  ayer after mappi
-000055d0: 6e67 2e20 5468 6520 7368 6170 6520 6973  ng. The shape is
-000055e0: 2060 5b62 6174 6368 2c20 7365 715f 6c65   `[batch, seq_le
-000055f0: 6e67 7468 2c20 6869 6464 656e 5f73 697a  ngth, hidden_siz
-00005600: 655d 206f 720a 2020 2020 2020 2020 2020  e] or.          
-00005610: 2020 5b62 6174 6368 202a 2073 6571 5f6c    [batch * seq_l
-00005620: 656e 6774 682c 2068 6964 6465 6e5f 7369  ength, hidden_si
-00005630: 7a65 5d60 2e0a 0a20 2020 2020 2020 2052  ze]`...        R
-00005640: 6169 7365 733a 0a20 2020 2020 2020 2020  aises:.         
-00005650: 2020 2054 7970 6545 7272 6f72 3a20 6068     TypeError: `h
-00005660: 6964 6465 6e5f 6163 7460 2069 7320 6e6f  idden_act` is no
-00005670: 7420 6120 7374 7269 6e67 206f 7220 6e6e  t a string or nn
-00005680: 2e43 656c 6c2e 0a20 2020 2020 2020 2020  .Cell..         
-00005690: 2020 2054 7970 6545 7272 6f72 3a20 6070     TypeError: `p
-000056a0: 6172 616c 6c65 6c5f 636f 6e66 6967 6020  arallel_config` 
-000056b0: 6973 206e 6f74 2061 2073 7562 636c 6173  is not a subclas
-000056c0: 7320 6f66 204f 7050 6172 616c 6c65 6c43  s of OpParallelC
-000056d0: 6f6e 6669 672e 0a20 2020 2020 2020 2020  onfig..         
-000056e0: 2020 2056 616c 7565 4572 726f 723a 2060     ValueError: `
-000056f0: 6666 6e5f 6869 6464 656e 5f73 697a 6560  ffn_hidden_size`
-00005700: 2069 7320 6e6f 7420 6120 6d75 6c74 6970   is not a multip
-00005710: 6c65 206f 6620 7468 6520 6d6f 6465 6c20  le of the model 
-00005720: 7061 7261 6c6c 656c 2077 6179 2e0a 2020  parallel way..  
-00005730: 2020 2020 2020 2020 2020 5661 6c75 6545            ValueE
-00005740: 7272 6f72 3a20 6068 6964 6465 6e5f 7369  rror: `hidden_si
-00005750: 7a65 6020 6973 206e 6f74 2061 206d 756c  ze` is not a mul
-00005760: 7469 706c 6520 6f66 2074 6865 206d 6f64  tiple of the mod
-00005770: 656c 2070 6172 616c 6c65 6c20 7761 792e  el parallel way.
-00005780: 0a0a 2020 2020 2020 2020 5375 7070 6f72  ..        Suppor
-00005790: 7465 6420 506c 6174 666f 726d 733a 0a20  ted Platforms:. 
-000057a0: 2020 2020 2020 2020 2020 2060 6041 7363             ``Asc
-000057b0: 656e 6460 6020 6060 4750 5560 600a 0a20  end`` ``GPU``.. 
-000057c0: 2020 2020 2020 2045 7861 6d70 6c65 733a         Examples:
-000057d0: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
-000057e0: 2069 6d70 6f72 7420 6e75 6d70 7920 6173   import numpy as
-000057f0: 206e 700a 2020 2020 2020 2020 2020 2020   np.            
-00005800: 3e3e 3e20 6672 6f6d 206d 696e 6466 6f72  >>> from mindfor
-00005810: 6d65 7273 2e6d 6f64 756c 6573 2e74 7261  mers.modules.tra
-00005820: 6e73 666f 726d 6572 2069 6d70 6f72 7420  nsformer import 
-00005830: 4665 6564 466f 7277 6172 640a 2020 2020  FeedForward.    
-00005840: 2020 2020 2020 2020 3e3e 3e20 6672 6f6d          >>> from
-00005850: 206d 696e 6473 706f 7265 2069 6d70 6f72   mindspore impor
-00005860: 7420 6474 7970 6520 6173 206d 7374 7970  t dtype as mstyp
-00005870: 650a 2020 2020 2020 2020 2020 2020 3e3e  e.            >>
-00005880: 3e20 6672 6f6d 206d 696e 6473 706f 7265  > from mindspore
-00005890: 2069 6d70 6f72 7420 5465 6e73 6f72 2c20   import Tensor, 
-000058a0: 6e6e 0a20 2020 2020 2020 2020 2020 203e  nn.            >
-000058b0: 3e3e 2069 6d70 6f72 7420 6d69 6e64 7370  >> import mindsp
-000058c0: 6f72 652e 6f70 7320 6173 206f 7073 0a20  ore.ops as ops. 
-000058d0: 2020 2020 2020 2020 2020 203e 3e3e 206d             >>> m
-000058e0: 6f64 656c 203d 2046 6565 6446 6f72 7761  odel = FeedForwa
-000058f0: 7264 2868 6964 6465 6e5f 7369 7a65 3d31  rd(hidden_size=1
-00005900: 352c 2066 666e 5f68 6964 6465 6e5f 7369  5, ffn_hidden_si
-00005910: 7a65 3d33 302c 2064 726f 706f 7574 5f72  ze=30, dropout_r
-00005920: 6174 653d 302e 3129 0a20 2020 2020 2020  ate=0.1).       
-00005930: 2020 2020 203e 3e3e 2074 656e 736f 7220       >>> tensor 
-00005940: 3d20 5465 6e73 6f72 286e 702e 6f6e 6573  = Tensor(np.ones
-00005950: 2828 322c 2032 302c 2031 3529 292c 206d  ((2, 20, 15)), m
-00005960: 7374 7970 652e 666c 6f61 7433 3229 0a20  stype.float32). 
-00005970: 2020 2020 2020 2020 2020 203e 3e3e 206f             >>> o
-00005980: 7574 7075 7420 3d20 6d6f 6465 6c28 7465  utput = model(te
-00005990: 6e73 6f72 290a 2020 2020 2020 2020 2020  nsor).          
-000059a0: 2020 3e3e 3e20 7072 696e 7428 6f75 7470    >>> print(outp
-000059b0: 7574 2e73 6861 7065 290a 2020 2020 2020  ut.shape).      
-000059c0: 2020 2020 2020 2832 2c20 3230 2c20 3135        (2, 20, 15
-000059d0: 290a 2020 2020 2020 2020 2020 2020 3e3e  ).            >>
-000059e0: 3e20 2320 4578 616d 706c 6520 3220 7573  > # Example 2 us
-000059f0: 696e 6720 6375 7374 6f6d 2068 6964 6465  ing custom hidde
-00005a00: 6e20 6163 7469 7661 7469 6f6e 0a20 2020  n activation.   
-00005a10: 2020 2020 2020 2020 203e 3e3e 2063 6c61           >>> cla
-00005a20: 7373 204d 7941 6374 6976 6174 696f 6e4e  ss MyActivationN
-00005a30: 6f53 6861 7264 286e 6e2e 4365 6c6c 293a  oShard(nn.Cell):
-00005a40: 0a20 2020 2020 2020 2020 2020 202e 2e2e  .            ...
-00005a50: 2020 2020 2064 6566 205f 5f69 6e69 745f       def __init_
-00005a60: 5f28 7365 6c66 293a 0a20 2020 2020 2020  _(self):.       
-00005a70: 2020 2020 202e 2e2e 2020 2020 2020 2020       ...        
-00005a80: 2073 7570 6572 284d 7941 6374 6976 6174   super(MyActivat
-00005a90: 696f 6e4e 6f53 6861 7264 2c20 7365 6c66  ionNoShard, self
-00005aa0: 292e 5f5f 696e 6974 5f5f 2829 0a20 2020  ).__init__().   
-00005ab0: 2020 2020 2020 2020 202e 2e2e 2020 2020           ...    
-00005ac0: 2020 2020 2073 656c 662e 6164 6420 3d20       self.add = 
-00005ad0: 6f70 732e 4164 6428 290a 2020 2020 2020  ops.Add().      
-00005ae0: 2020 2020 2020 2e2e 2e20 2020 2020 6465        ...     de
-00005af0: 6620 636f 6e73 7472 7563 7428 7365 6c66  f construct(self
-00005b00: 2c20 7829 3a0a 2020 2020 2020 2020 2020  , x):.          
-00005b10: 2020 2e2e 2e20 2020 2020 2020 2020 7265    ...         re
-00005b20: 7475 726e 2073 656c 662e 6164 6428 782c  turn self.add(x,
-00005b30: 2030 2e31 290a 2020 2020 2020 2020 2020   0.1).          
-00005b40: 2020 3e3e 3e20 6d6f 6465 6c20 3d20 4665    >>> model = Fe
-00005b50: 6564 466f 7277 6172 6428 6869 6464 656e  edForward(hidden
-00005b60: 5f73 697a 653d 3135 2c20 6666 6e5f 6869  _size=15, ffn_hi
-00005b70: 6464 656e 5f73 697a 653d 3330 2c20 6472  dden_size=30, dr
-00005b80: 6f70 6f75 745f 7261 7465 3d30 2e31 2c0a  opout_rate=0.1,.
-00005b90: 2020 2020 2020 2020 2020 2020 2e2e 2e20              ... 
-00005ba0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00005bb0: 2020 2020 6869 6464 656e 5f61 6374 3d4d      hidden_act=M
-00005bc0: 7941 6374 6976 6174 696f 6e4e 6f53 6861  yActivationNoSha
-00005bd0: 7264 290a 2020 2020 2020 2020 2020 2020  rd).            
-00005be0: 3e3e 3e20 7465 6e73 6f72 203d 2054 656e  >>> tensor = Ten
-00005bf0: 736f 7228 6e70 2e6f 6e65 7328 2832 2c20  sor(np.ones((2, 
-00005c00: 3230 2c20 3135 2929 2c20 6d73 7479 7065  20, 15)), mstype
-00005c10: 2e66 6c6f 6174 3332 290a 2020 2020 2020  .float32).      
-00005c20: 2020 2020 2020 3e3e 3e20 6f75 7470 7574        >>> output
-00005c30: 203d 206d 6f64 656c 2874 656e 736f 7229   = model(tensor)
-00005c40: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
-00005c50: 2070 7269 6e74 286f 7574 7075 742e 7368   print(output.sh
-00005c60: 6170 6529 0a20 2020 2020 2020 2020 2020  ape).           
-00005c70: 2028 322c 2032 302c 2031 3529 0a20 2020   (2, 20, 15).   
-00005c80: 2020 2020 2020 2020 203e 3e3e 2023 2045           >>> # E
-00005c90: 7861 6d70 6c65 2033 2075 7369 6e67 2063  xample 3 using c
-00005ca0: 7573 746f 6d20 6869 6464 656e 2061 6374  ustom hidden act
-00005cb0: 6976 6174 696f 6e20 7769 7468 2061 6374  ivation with act
-00005cc0: 6976 6174 696f 6e5f 7368 6172 640a 2020  ivation_shard.  
-00005cd0: 2020 2020 2020 2020 2020 3e3e 3e20 2320            >>> # 
-00005ce0: 4966 2075 7365 7220 7761 6e74 7373 2074  If user wantss t
-00005cf0: 6f20 7275 6e20 6f6e 2074 6865 2053 454d  o run on the SEM
-00005d00: 492f 4155 544f 2070 6172 616c 6c65 6c20  I/AUTO parallel 
-00005d10: 6d6f 6465 2c20 7468 6520 6375 7374 6f6d  mode, the custom
-00005d20: 2061 6374 6976 6174 696f 6e20 6d75 7374   activation must
-00005d30: 2070 726f 7669 6465 0a20 2020 2020 2020   provide.       
-00005d40: 2020 2020 203e 3e3e 2023 2061 2063 6c61       >>> # a cla
-00005d50: 7373 2066 756e 6374 696f 6e20 6e61 6d65  ss function name
-00005d60: 6420 6163 7469 7661 7469 6f6e 5f73 6861  d activation_sha
-00005d70: 7264 2e20 4974 2061 6363 6570 7473 2074  rd. It accepts t
-00005d80: 6865 2061 7267 756d 656e 7420 7061 7261  he argument para
-00005d90: 6c6c 656c 5f63 6f6e 6669 6720 284f 7050  llel_config (OpP
-00005da0: 6172 616c 6c65 6c43 6f6e 6669 672c 0a20  arallelConfig,. 
-00005db0: 2020 2020 2020 2020 2020 203e 3e3e 2023             >>> #
-00005dc0: 204d 6f45 5061 7261 6c6c 656c 436f 6e66   MoEParallelConf
-00005dd0: 6967 2920 616e 6420 7365 7420 7468 6520  ig) and set the 
-00005de0: 7368 6172 6420 666f 7220 7468 6520 7072  shard for the pr
-00005df0: 696d 6974 6976 6573 2075 7365 6420 696e  imitives used in
-00005e00: 2074 6865 2063 6f6e 7374 7275 6374 2e0a   the construct..
-00005e10: 2020 2020 2020 2020 2020 2020 3e3e 3e20              >>> 
-00005e20: 636c 6173 7320 4d79 4163 7469 7661 7469  class MyActivati
-00005e30: 6f6e 5769 7468 5368 6172 6428 6e6e 2e43  onWithShard(nn.C
-00005e40: 656c 6c29 3a0a 2020 2020 2020 2020 2020  ell):.          
-00005e50: 2020 2e2e 2e20 2020 2020 6465 6620 5f5f    ...     def __
-00005e60: 696e 6974 5f5f 2873 656c 6629 3a0a 2020  init__(self):.  
-00005e70: 2020 2020 2020 2020 2020 2e2e 2e20 2020            ...   
-00005e80: 2020 2020 2020 7375 7065 7228 4d79 4163        super(MyAc
-00005e90: 7469 7661 7469 6f6e 5769 7468 5368 6172  tivationWithShar
-00005ea0: 642c 2073 656c 6629 2e5f 5f69 6e69 745f  d, self).__init_
-00005eb0: 5f28 290a 2020 2020 2020 2020 2020 2020  _().            
-00005ec0: 2e2e 2e20 2020 2020 2020 2020 7365 6c66  ...         self
-00005ed0: 2e61 6464 203d 206f 7073 2e41 6464 2829  .add = ops.Add()
-00005ee0: 0a20 2020 2020 2020 2020 2020 202e 2e2e  .            ...
-00005ef0: 2020 2020 2064 6566 2063 6f6e 7374 7275       def constru
-00005f00: 6374 2873 656c 662c 2078 293a 0a20 2020  ct(self, x):.   
-00005f10: 2020 2020 2020 2020 202e 2e2e 2020 2020           ...    
-00005f20: 2020 2020 2072 6574 7572 6e20 7365 6c66       return self
-00005f30: 2e61 6464 2878 2c20 302e 3129 0a20 2020  .add(x, 0.1).   
-00005f40: 2020 2020 2020 2020 202e 2e2e 2020 2020           ...    
-00005f50: 2064 6566 2061 6374 6976 6174 696f 6e5f   def activation_
-00005f60: 7368 6172 6428 7365 6c66 2c20 7061 7261  shard(self, para
-00005f70: 6c6c 656c 5f63 6f6e 6669 6729 3a0a 2020  llel_config):.  
-00005f80: 2020 2020 2020 2020 2020 2e2e 2e20 2020            ...   
-00005f90: 2020 2020 2020 7365 6c66 2e61 6464 2e73        self.add.s
-00005fa0: 6861 7264 2828 2870 6172 616c 6c65 6c5f  hard(((parallel_
-00005fb0: 636f 6e66 6967 2e64 6174 615f 7061 7261  config.data_para
-00005fc0: 6c6c 656c 2c20 7061 7261 6c6c 656c 5f63  llel, parallel_c
-00005fd0: 6f6e 6669 672e 6d6f 6465 6c5f 7061 7261  onfig.model_para
-00005fe0: 6c6c 656c 292c 2028 2929 290a 2020 2020  llel), ())).    
-00005ff0: 2020 2020 2020 2020 3e3e 3e0a 2020 2020          >>>.    
-00006000: 2020 2020 2020 2020 3e3e 3e20 6d6f 6465          >>> mode
-00006010: 6c20 3d20 4665 6564 466f 7277 6172 6428  l = FeedForward(
-00006020: 6869 6464 656e 5f73 697a 653d 3135 2c20  hidden_size=15, 
-00006030: 6666 6e5f 6869 6464 656e 5f73 697a 653d  ffn_hidden_size=
-00006040: 3330 2c20 6472 6f70 6f75 745f 7261 7465  30, dropout_rate
-00006050: 3d30 2e31 2c0a 2020 2020 2020 2020 2020  =0.1,.          
-00006060: 2020 2e2e 2e20 2020 2020 2020 2020 2020    ...           
-00006070: 2020 2020 2020 2020 2020 6869 6464 656e            hidden
-00006080: 5f61 6374 3d4d 7941 6374 6976 6174 696f  _act=MyActivatio
-00006090: 6e57 6974 6853 6861 7264 290a 2020 2020  nWithShard).    
-000060a0: 2020 2020 2020 2020 3e3e 3e20 7465 6e73          >>> tens
-000060b0: 6f72 203d 2054 656e 736f 7228 6e70 2e6f  or = Tensor(np.o
-000060c0: 6e65 7328 2832 2c20 3230 2c20 3135 2929  nes((2, 20, 15))
-000060d0: 2c20 6d73 7479 7065 2e66 6c6f 6174 3332  , mstype.float32
-000060e0: 290a 2020 2020 2020 2020 2020 2020 3e3e  ).            >>
-000060f0: 3e20 6f75 7470 7574 203d 206d 6f64 656c  > output = model
-00006100: 2874 656e 736f 7229 0a20 2020 2020 2020  (tensor).       
-00006110: 2020 2020 203e 3e3e 2070 7269 6e74 286f       >>> print(o
-00006120: 7574 7075 742e 7368 6170 6529 0a20 2020  utput.shape).   
-00006130: 2020 2020 2020 2020 2028 322c 2032 302c           (2, 20,
-00006140: 2031 3529 0a20 2020 2022 2222 0a0a 2020   15).    """..  
-00006150: 2020 405f 4c6f 6741 6374 696f 6e4f 6e63    @_LogActionOnc
-00006160: 6528 6d5f 6c6f 6767 6572 3d6c 6f67 6765  e(m_logger=logge
-00006170: 722c 206b 6579 3d27 4665 6564 466f 7277  r, key='FeedForw
-00006180: 6172 6427 2c0a 2020 2020 2020 2020 2020  ard',.          
-00006190: 2020 2020 2020 2020 2020 6e6f 5f77 6172            no_war
-000061a0: 6e69 6e67 3d5f 6765 745f 7061 7261 6c6c  ning=_get_parall
-000061b0: 656c 5f6d 6f64 6528 2920 696e 2028 5061  el_mode() in (Pa
-000061c0: 7261 6c6c 656c 4d6f 6465 2e53 5441 4e44  rallelMode.STAND
-000061d0: 5f41 4c4f 4e45 2c29 290a 2020 2020 405f  _ALONE,)).    @_
-000061e0: 6172 6773 5f74 7970 655f 7661 6c69 6461  args_type_valida
-000061f0: 746f 725f 6368 6563 6b28 6869 6464 656e  tor_check(hidden
-00006200: 5f73 697a 653d 5661 6c69 6461 746f 722e  _size=Validator.
-00006210: 6368 6563 6b5f 706f 7369 7469 7665 5f69  check_positive_i
-00006220: 6e74 2c0a 2020 2020 2020 2020 2020 2020  nt,.            
-00006230: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00006240: 2020 2020 6666 6e5f 6869 6464 656e 5f73      ffn_hidden_s
-00006250: 697a 653d 5661 6c69 6461 746f 722e 6368  ize=Validator.ch
-00006260: 6563 6b5f 706f 7369 7469 7665 5f69 6e74  eck_positive_int
-00006270: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00006280: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00006290: 2020 6472 6f70 6f75 745f 7261 7465 3d56    dropout_rate=V
-000062a0: 616c 6964 6174 6f72 2e63 6865 636b 5f6e  alidator.check_n
-000062b0: 6f6e 5f6e 6567 6174 6976 655f 666c 6f61  on_negative_floa
-000062c0: 742c 0a20 2020 2020 2020 2020 2020 2020  t,.             
+000011c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000011d0: 7573 655f 7365 715f 7061 7261 6c6c 656c  use_seq_parallel
+000011e0: 3d75 7365 5f73 6571 5f70 6172 616c 6c65  =use_seq_paralle
+000011f0: 6c2c 0a20 2020 2020 2020 2020 2020 2020  l,.             
+00001200: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001210: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001220: 206d 6f64 656c 5f70 6172 616c 6c65 6c3d   model_parallel=
+00001230: 6d6f 6465 6c5f 7061 7261 6c6c 656c 2c0a  model_parallel,.
+00001240: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001250: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001260: 2020 2020 2020 2020 2020 2020 2020 7365                se
+00001270: 6c65 6374 5f72 6563 6f6d 7075 7465 3d73  lect_recompute=s
+00001280: 656c 6563 745f 7265 636f 6d70 7574 6529  elect_recompute)
+00001290: 0a20 2020 2020 2020 2056 616c 6964 6174  .        Validat
+000012a0: 6f72 2e63 6865 636b 5f62 6f6f 6c28 766f  or.check_bool(vo
+000012b0: 6361 625f 656d 625f 6470 2c20 2276 6f63  cab_emb_dp, "voc
+000012c0: 6162 5f65 6d62 5f64 7022 290a 2020 2020  ab_emb_dp").    
+000012d0: 2020 2020 7365 6c66 2e76 6f63 6162 5f65      self.vocab_e
+000012e0: 6d62 5f64 7020 3d20 766f 6361 625f 656d  mb_dp = vocab_em
+000012f0: 625f 6470 0a20 2020 2020 2020 2073 656c  b_dp.        sel
+00001300: 662e 7573 655f 7365 715f 7061 7261 6c6c  f.use_seq_parall
+00001310: 656c 203d 2075 7365 5f73 6571 5f70 6172  el = use_seq_par
+00001320: 616c 6c65 6c0a 2020 2020 2020 2020 7365  allel.        se
+00001330: 6c66 2e73 656c 6563 745f 7265 636f 6d70  lf.select_recomp
+00001340: 7574 6520 3d20 7365 6c65 6374 5f72 6563  ute = select_rec
+00001350: 6f6d 7075 7465 0a0a 2020 2020 4070 726f  ompute..    @pro
+00001360: 7065 7274 790a 2020 2020 6465 6620 6461  perty.    def da
+00001370: 7461 5f70 6172 616c 6c65 6c28 7365 6c66  ta_parallel(self
+00001380: 293a 0a20 2020 2020 2020 2072 6574 7572  ):.        retur
+00001390: 6e20 7365 6c66 2e5f 6470 5f6d 705f 636f  n self._dp_mp_co
+000013a0: 6e66 6967 2e64 6174 615f 7061 7261 6c6c  nfig.data_parall
+000013b0: 656c 0a0a 2020 2020 4064 6174 615f 7061  el..    @data_pa
+000013c0: 7261 6c6c 656c 2e73 6574 7465 720a 2020  rallel.setter.  
+000013d0: 2020 6465 6620 6461 7461 5f70 6172 616c    def data_paral
+000013e0: 6c65 6c28 7365 6c66 2c20 7661 6c75 6529  lel(self, value)
+000013f0: 3a0a 2020 2020 2020 2020 7365 6c66 2e5f  :.        self._
+00001400: 6470 5f6d 705f 636f 6e66 6967 2e64 6174  dp_mp_config.dat
+00001410: 615f 7061 7261 6c6c 656c 203d 2076 616c  a_parallel = val
+00001420: 7565 0a0a 2020 2020 4070 726f 7065 7274  ue..    @propert
+00001430: 790a 2020 2020 6465 6620 6d6f 6465 6c5f  y.    def model_
+00001440: 7061 7261 6c6c 656c 2873 656c 6629 3a0a  parallel(self):.
+00001450: 2020 2020 2020 2020 7265 7475 726e 2073          return s
+00001460: 656c 662e 5f64 705f 6d70 5f63 6f6e 6669  elf._dp_mp_confi
+00001470: 672e 6d6f 6465 6c5f 7061 7261 6c6c 656c  g.model_parallel
+00001480: 0a0a 2020 2020 406d 6f64 656c 5f70 6172  ..    @model_par
+00001490: 616c 6c65 6c2e 7365 7474 6572 0a20 2020  allel.setter.   
+000014a0: 2064 6566 206d 6f64 656c 5f70 6172 616c   def model_paral
+000014b0: 6c65 6c28 7365 6c66 2c20 7661 6c75 6529  lel(self, value)
+000014c0: 3a0a 2020 2020 2020 2020 7365 6c66 2e5f  :.        self._
+000014d0: 6470 5f6d 705f 636f 6e66 6967 2e6d 6f64  dp_mp_config.mod
+000014e0: 656c 5f70 6172 616c 6c65 6c20 3d20 7661  el_parallel = va
+000014f0: 6c75 650a 0a20 2020 2040 7072 6f70 6572  lue..    @proper
+00001500: 7479 0a20 2020 2064 6566 2076 6f63 6162  ty.    def vocab
+00001510: 5f65 6d62 5f64 7028 7365 6c66 293a 0a20  _emb_dp(self):. 
+00001520: 2020 2020 2020 2072 6574 7572 6e20 7365         return se
+00001530: 6c66 2e5f 766f 6361 625f 656d 625f 6470  lf._vocab_emb_dp
+00001540: 0a0a 2020 2020 4076 6f63 6162 5f65 6d62  ..    @vocab_emb
+00001550: 5f64 702e 7365 7474 6572 0a20 2020 2064  _dp.setter.    d
+00001560: 6566 2076 6f63 6162 5f65 6d62 5f64 7028  ef vocab_emb_dp(
+00001570: 7365 6c66 2c20 7661 6c75 6529 3a0a 2020  self, value):.  
+00001580: 2020 2020 2020 5661 6c69 6461 746f 722e        Validator.
+00001590: 6368 6563 6b5f 626f 6f6c 2876 616c 7565  check_bool(value
+000015a0: 2c20 2276 6f63 6162 5f65 6d62 5f64 7022  , "vocab_emb_dp"
+000015b0: 290a 2020 2020 2020 2020 7365 6c66 2e5f  ).        self._
+000015c0: 766f 6361 625f 656d 625f 6470 203d 2076  vocab_emb_dp = v
+000015d0: 616c 7565 0a0a 2020 2020 4070 726f 7065  alue..    @prope
+000015e0: 7274 790a 2020 2020 6465 6620 6470 5f6d  rty.    def dp_m
+000015f0: 705f 636f 6e66 6967 2873 656c 6629 3a0a  p_config(self):.
+00001600: 2020 2020 2020 2020 7265 7475 726e 2073          return s
+00001610: 656c 662e 5f64 705f 6d70 5f63 6f6e 6669  elf._dp_mp_confi
+00001620: 670a 0a20 2020 2064 6566 205f 5f65 715f  g..    def __eq_
+00001630: 5f28 7365 6c66 2c20 6f74 6865 7229 202d  _(self, other) -
+00001640: 3e20 626f 6f6c 3a0a 2020 2020 2020 2020  > bool:.        
+00001650: 7265 7475 726e 2069 7369 6e73 7461 6e63  return isinstanc
+00001660: 6528 6f74 6865 722c 2045 6d62 6564 6469  e(other, Embeddi
+00001670: 6e67 4f70 5061 7261 6c6c 656c 436f 6e66  ngOpParallelConf
+00001680: 6967 2920 616e 6420 2873 656c 662e 746f  ig) and (self.to
+00001690: 5f64 6963 7428 2920 3d3d 206f 7468 6572  _dict() == other
+000016a0: 2e74 6f5f 6469 6374 2829 290a 0a20 2020  .to_dict())..   
+000016b0: 2064 6566 2074 6f5f 6469 6666 5f64 6963   def to_diff_dic
+000016c0: 7428 7365 6c66 293a 0a20 2020 2020 2020  t(self):.       
+000016d0: 2063 6f6e 6669 675f 6469 6374 203d 2073   config_dict = s
+000016e0: 656c 662e 746f 5f64 6963 7428 290a 2020  elf.to_dict().  
+000016f0: 2020 2020 2020 6465 6661 756c 745f 6469        default_di
+00001700: 6374 203d 2045 6d62 6564 6469 6e67 4f70  ct = EmbeddingOp
+00001710: 5061 7261 6c6c 656c 436f 6e66 6967 2829  ParallelConfig()
+00001720: 2e74 6f5f 6469 6374 2829 0a20 2020 2020  .to_dict().     
+00001730: 2020 2072 6573 5f64 6963 7420 3d20 7b7d     res_dict = {}
+00001740: 0a20 2020 2020 2020 2066 6f72 206b 2c20  .        for k, 
+00001750: 7620 696e 2063 6f6e 6669 675f 6469 6374  v in config_dict
+00001760: 2e69 7465 6d73 2829 3a0a 2020 2020 2020  .items():.      
+00001770: 2020 2020 2020 6966 2076 2021 3d20 6465        if v != de
+00001780: 6661 756c 745f 6469 6374 5b6b 5d3a 0a20  fault_dict[k]:. 
+00001790: 2020 2020 2020 2020 2020 2020 2020 2072                 r
+000017a0: 6573 5f64 6963 745b 6b5d 203d 2076 0a20  es_dict[k] = v. 
+000017b0: 2020 2020 2020 2072 6574 7572 6e20 7265         return re
+000017c0: 735f 6469 6374 0a0a 2020 2020 6465 6620  s_dict..    def 
+000017d0: 746f 5f64 6963 7428 7365 6c66 293a 0a20  to_dict(self):. 
+000017e0: 2020 2020 2020 2022 2222 746f 2064 6963         """to dic
+000017f0: 7422 2222 0a20 2020 2020 2020 2063 6f6e  t""".        con
+00001800: 6669 675f 6469 6374 203d 207b 0a20 2020  fig_dict = {.   
+00001810: 2020 2020 2020 2020 2027 6461 7461 5f70           'data_p
+00001820: 6172 616c 6c65 6c27 3a20 7365 6c66 2e64  arallel': self.d
+00001830: 6174 615f 7061 7261 6c6c 656c 2c0a 2020  ata_parallel,.  
+00001840: 2020 2020 2020 2020 2020 276d 6f64 656c            'model
+00001850: 5f70 6172 616c 6c65 6c27 3a20 7365 6c66  _parallel': self
+00001860: 2e6d 6f64 656c 5f70 6172 616c 6c65 6c2c  .model_parallel,
+00001870: 0a20 2020 2020 2020 2020 2020 2027 7365  .            'se
+00001880: 6c65 6374 5f72 6563 6f6d 7075 7465 273a  lect_recompute':
+00001890: 2073 656c 662e 7365 6c65 6374 5f72 6563   self.select_rec
+000018a0: 6f6d 7075 7465 2c0a 2020 2020 2020 2020  ompute,.        
+000018b0: 2020 2020 2775 7365 5f73 6571 5f70 6172      'use_seq_par
+000018c0: 616c 6c65 6c27 3a20 7365 6c66 2e75 7365  allel': self.use
+000018d0: 5f73 6571 5f70 6172 616c 6c65 6c2c 0a20  _seq_parallel,. 
+000018e0: 2020 2020 2020 2020 2020 2027 766f 6361             'voca
+000018f0: 625f 656d 625f 6470 273a 2073 656c 662e  b_emb_dp': self.
+00001900: 766f 6361 625f 656d 625f 6470 0a20 2020  vocab_emb_dp.   
+00001910: 2020 2020 207d 0a20 2020 2020 2020 2072       }.        r
+00001920: 6574 7572 6e20 636f 6e66 6967 5f64 6963  eturn config_dic
+00001930: 740a 0a0a 636c 6173 7320 5472 616e 7366  t...class Transf
+00001940: 6f72 6d65 7252 6563 6f6d 7075 7465 436f  ormerRecomputeCo
+00001950: 6e66 6967 285f 436f 6e66 6967 293a 0a20  nfig(_Config):. 
+00001960: 2020 2072 2222 220a 2020 2020 2020 2020     r""".        
+00001970: 5472 616e 7366 6f72 6d65 7252 6563 6f6d  TransformerRecom
+00001980: 7075 7465 436f 6e66 6967 2066 6f72 2074  puteConfig for t
+00001990: 6865 2073 6574 7469 6e67 2072 6563 6f6d  he setting recom
+000019a0: 7075 7465 2061 7474 7269 6275 7465 7320  pute attributes 
+000019b0: 666f 7220 656e 636f 6465 722f 6465 636f  for encoder/deco
+000019c0: 6465 7220 6c61 7965 7273 2e0a 0a20 2020  der layers...   
+000019d0: 2020 2020 2041 7267 733a 0a20 2020 2020       Args:.     
+000019e0: 2020 2020 2020 2072 6563 6f6d 7075 7465         recompute
+000019f0: 2028 626f 6f6c 293a 2045 6e61 626c 6520   (bool): Enable 
+00001a00: 7265 636f 6d70 7574 6174 696f 6e20 6f66  recomputation of
+00001a10: 2074 6865 2074 7261 6e73 666f 726d 6572   the transformer
+00001a20: 2062 6c6f 636b 206f 7220 6e6f 742e 2044   block or not. D
+00001a30: 6566 6175 6c74 3a20 4661 6c73 652e 0a20  efault: False.. 
+00001a40: 2020 2020 2020 2020 2020 2070 6172 616c             paral
+00001a50: 6c65 6c5f 6f70 7469 6d69 7a65 725f 636f  lel_optimizer_co
+00001a60: 6d6d 5f72 6563 6f6d 7075 7465 2028 626f  mm_recompute (bo
+00001a70: 6f6c 293a 2053 7065 6369 6669 6573 2077  ol): Specifies w
+00001a80: 6865 7468 6572 2074 6865 2063 6f6d 6d75  hether the commu
+00001a90: 6e69 6361 7469 6f6e 206f 7065 7261 746f  nication operato
+00001aa0: 7220 616c 6c67 6174 6865 7273 0a20 2020  r allgathers.   
+00001ab0: 2020 2020 2020 2020 2020 2020 2069 6e74               int
+00001ac0: 726f 6475 6365 6420 6279 206f 7074 696d  roduced by optim
+00001ad0: 697a 6572 2073 6861 7264 2061 7265 2072  izer shard are r
+00001ae0: 6563 6f6d 7075 7465 6420 696e 2061 7574  ecomputed in aut
+00001af0: 6f20 7061 7261 6c6c 656c 206f 7220 7365  o parallel or se
+00001b00: 6d69 2061 7574 6f20 7061 7261 6c6c 656c  mi auto parallel
+00001b10: 206d 6f64 652e 0a20 2020 2020 2020 2020   mode..         
+00001b20: 2020 2020 2020 2044 6566 6175 6c74 3a20         Default: 
+00001b30: 4661 6c73 652e 0a20 2020 2020 2020 2020  False..         
+00001b40: 2020 206d 705f 636f 6d6d 5f72 6563 6f6d     mp_comm_recom
+00001b50: 7075 7465 2028 626f 6f6c 293a 2053 7065  pute (bool): Spe
+00001b60: 6369 6669 6573 2077 6865 7468 6572 2074  cifies whether t
+00001b70: 6865 206d 6f64 656c 2070 6172 616c 6c65  he model paralle
+00001b80: 6c20 636f 6d6d 756e 6963 6174 696f 6e20  l communication 
+00001b90: 6f70 6572 6174 6f72 730a 2020 2020 2020  operators.      
+00001ba0: 2020 2020 2020 2020 2020 696e 2074 6865            in the
+00001bb0: 2063 656c 6c20 6172 6520 7265 636f 6d70   cell are recomp
+00001bc0: 7574 6564 2069 6e20 6175 746f 2070 6172  uted in auto par
+00001bd0: 616c 6c65 6c20 6f72 2073 656d 6920 6175  allel or semi au
+00001be0: 746f 2070 6172 616c 6c65 6c20 6d6f 6465  to parallel mode
+00001bf0: 2e20 4465 6661 756c 743a 2054 7275 652e  . Default: True.
+00001c00: 0a20 2020 2020 2020 2020 2020 2072 6563  .            rec
+00001c10: 6f6d 7075 7465 5f73 6c69 6365 5f61 6374  ompute_slice_act
+00001c20: 6976 6174 696f 6e20 2862 6f6f 6c29 3a20  ivation (bool): 
+00001c30: 536c 6963 6520 7468 6520 6365 6c6c 206f  Slice the cell o
+00001c40: 7574 7075 7420 7768 6963 6820 776f 756c  utput which woul
+00001c50: 6420 7265 6d61 696e 7320 696e 206d 656d  d remains in mem
+00001c60: 6f72 792e 2044 6566 6175 6c74 3a20 4661  ory. Default: Fa
+00001c70: 6c73 652e 0a0a 2020 2020 2020 2020 5375  lse...        Su
+00001c80: 7070 6f72 7465 6420 506c 6174 666f 726d  pported Platform
+00001c90: 733a 0a20 2020 2020 2020 2020 2020 2060  s:.            `
+00001ca0: 6041 7363 656e 6460 6020 6060 4750 5560  `Ascend`` ``GPU`
+00001cb0: 600a 0a20 2020 2020 2020 2045 7861 6d70  `..        Examp
+00001cc0: 6c65 733a 0a20 2020 2020 2020 2020 2020  les:.           
+00001cd0: 203e 3e3e 2066 726f 6d20 6d69 6e64 666f   >>> from mindfo
+00001ce0: 726d 6572 732e 6d6f 6475 6c65 732e 7472  rmers.modules.tr
+00001cf0: 616e 7366 6f72 6d65 7220 696d 706f 7274  ansformer import
+00001d00: 2054 7261 6e73 666f 726d 6572 5265 636f   TransformerReco
+00001d10: 6d70 7574 6543 6f6e 6669 670a 2020 2020  mputeConfig.    
+00001d20: 2020 2020 2020 2020 3e3e 3e20 636f 6e66          >>> conf
+00001d30: 6967 3d54 7261 6e73 666f 726d 6572 5265  ig=TransformerRe
+00001d40: 636f 6d70 7574 6543 6f6e 6669 6728 7265  computeConfig(re
+00001d50: 636f 6d70 7574 653d 5472 7565 2c20 7061  compute=True, pa
+00001d60: 7261 6c6c 656c 5f6f 7074 696d 697a 6572  rallel_optimizer
+00001d70: 5f63 6f6d 6d5f 7265 636f 6d70 7574 653d  _comm_recompute=
+00001d80: 5472 7565 2c20 5c0a 2020 2020 2020 2020  True, \.        
+00001d90: 2020 2020 2e2e 2e20 2020 2020 2020 2020      ...         
+00001da0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001db0: 2020 2020 2020 2020 2020 6d70 5f63 6f6d            mp_com
+00001dc0: 6d5f 7265 636f 6d70 7574 653d 5472 7565  m_recompute=True
+00001dd0: 2c20 7265 636f 6d70 7574 655f 736c 6963  , recompute_slic
+00001de0: 655f 6163 7469 7661 7469 6f6e 3d54 7275  e_activation=Tru
+00001df0: 6529 0a20 2020 2022 2222 0a0a 2020 2020  e).    """..    
+00001e00: 6465 6620 5f5f 696e 6974 5f5f 2873 656c  def __init__(sel
+00001e10: 662c 2072 6563 6f6d 7075 7465 3d46 616c  f, recompute=Fal
+00001e20: 7365 2c20 7365 6c65 6374 5f72 6563 6f6d  se, select_recom
+00001e30: 7075 7465 3d46 616c 7365 2c0a 2020 2020  pute=False,.    
+00001e40: 2020 2020 2020 2020 2020 2020 2070 6172               par
+00001e50: 616c 6c65 6c5f 6f70 7469 6d69 7a65 725f  allel_optimizer_
+00001e60: 636f 6d6d 5f72 6563 6f6d 7075 7465 3d46  comm_recompute=F
+00001e70: 616c 7365 2c0a 2020 2020 2020 2020 2020  alse,.          
+00001e80: 2020 2020 2020 206d 705f 636f 6d6d 5f72         mp_comm_r
+00001e90: 6563 6f6d 7075 7465 3d54 7275 652c 2072  ecompute=True, r
+00001ea0: 6563 6f6d 7075 7465 5f73 6c69 6365 5f61  ecompute_slice_a
+00001eb0: 6374 6976 6174 696f 6e3d 4661 6c73 6529  ctivation=False)
+00001ec0: 3a0a 2020 2020 2020 2020 5661 6c69 6461  :.        Valida
+00001ed0: 746f 722e 6368 6563 6b5f 626f 6f6c 2872  tor.check_bool(r
+00001ee0: 6563 6f6d 7075 7465 2c20 2272 6563 6f6d  ecompute, "recom
+00001ef0: 7075 7465 2229 0a20 2020 2020 2020 2056  pute").        V
+00001f00: 616c 6964 6174 6f72 2e63 6865 636b 5f62  alidator.check_b
+00001f10: 6f6f 6c28 7061 7261 6c6c 656c 5f6f 7074  ool(parallel_opt
+00001f20: 696d 697a 6572 5f63 6f6d 6d5f 7265 636f  imizer_comm_reco
+00001f30: 6d70 7574 652c 2022 7061 7261 6c6c 656c  mpute, "parallel
+00001f40: 5f6f 7074 696d 697a 6572 5f63 6f6d 6d5f  _optimizer_comm_
+00001f50: 7265 636f 6d70 7574 6522 290a 2020 2020  recompute").    
+00001f60: 2020 2020 5661 6c69 6461 746f 722e 6368      Validator.ch
+00001f70: 6563 6b5f 626f 6f6c 286d 705f 636f 6d6d  eck_bool(mp_comm
+00001f80: 5f72 6563 6f6d 7075 7465 2c20 226d 705f  _recompute, "mp_
+00001f90: 636f 6d6d 5f72 6563 6f6d 7075 7465 2229  comm_recompute")
+00001fa0: 0a20 2020 2020 2020 2056 616c 6964 6174  .        Validat
+00001fb0: 6f72 2e63 6865 636b 5f62 6f6f 6c28 7365  or.check_bool(se
+00001fc0: 6c65 6374 5f72 6563 6f6d 7075 7465 2c20  lect_recompute, 
+00001fd0: 2273 656c 6563 745f 7265 636f 6d70 7574  "select_recomput
+00001fe0: 6522 290a 2020 2020 2020 2020 5661 6c69  e").        Vali
+00001ff0: 6461 746f 722e 6368 6563 6b5f 626f 6f6c  dator.check_bool
+00002000: 2872 6563 6f6d 7075 7465 5f73 6c69 6365  (recompute_slice
+00002010: 5f61 6374 6976 6174 696f 6e2c 2022 7265  _activation, "re
+00002020: 636f 6d70 7574 655f 736c 6963 655f 6163  compute_slice_ac
+00002030: 7469 7661 7469 6f6e 2229 0a20 2020 2020  tivation").     
+00002040: 2020 2073 656c 662e 5f72 6563 6f6d 7075     self._recompu
+00002050: 7465 203d 2072 6563 6f6d 7075 7465 0a20  te = recompute. 
+00002060: 2020 2020 2020 2073 656c 662e 5f73 656c         self._sel
+00002070: 6563 745f 7265 636f 6d70 7574 6520 3d20  ect_recompute = 
+00002080: 7365 6c65 6374 5f72 6563 6f6d 7075 7465  select_recompute
+00002090: 0a20 2020 2020 2020 2073 656c 662e 5f70  .        self._p
+000020a0: 6172 616c 6c65 6c5f 6f70 7469 6d69 7a65  arallel_optimize
+000020b0: 725f 636f 6d6d 5f72 6563 6f6d 7075 7465  r_comm_recompute
+000020c0: 203d 2070 6172 616c 6c65 6c5f 6f70 7469   = parallel_opti
+000020d0: 6d69 7a65 725f 636f 6d6d 5f72 6563 6f6d  mizer_comm_recom
+000020e0: 7075 7465 0a20 2020 2020 2020 2073 656c  pute.        sel
+000020f0: 662e 5f6d 705f 636f 6d6d 5f72 6563 6f6d  f._mp_comm_recom
+00002100: 7075 7465 203d 206d 705f 636f 6d6d 5f72  pute = mp_comm_r
+00002110: 6563 6f6d 7075 7465 0a20 2020 2020 2020  ecompute.       
+00002120: 2073 656c 662e 5f72 6563 6f6d 7075 7465   self._recompute
+00002130: 5f73 6c69 6365 5f61 6374 6976 6174 696f  _slice_activatio
+00002140: 6e20 3d20 7265 636f 6d70 7574 655f 736c  n = recompute_sl
+00002150: 6963 655f 6163 7469 7661 7469 6f6e 0a0a  ice_activation..
+00002160: 2020 2020 4070 726f 7065 7274 790a 2020      @property.  
+00002170: 2020 6465 6620 7265 636f 6d70 7574 6528    def recompute(
+00002180: 7365 6c66 293a 0a20 2020 2020 2020 2072  self):.        r
+00002190: 6574 7572 6e20 7365 6c66 2e5f 7265 636f  eturn self._reco
+000021a0: 6d70 7574 650a 0a20 2020 2040 7265 636f  mpute..    @reco
+000021b0: 6d70 7574 652e 7365 7474 6572 0a20 2020  mpute.setter.   
+000021c0: 2064 6566 2072 6563 6f6d 7075 7465 2873   def recompute(s
+000021d0: 656c 662c 2076 616c 7565 293a 0a20 2020  elf, value):.   
+000021e0: 2020 2020 2056 616c 6964 6174 6f72 2e63       Validator.c
+000021f0: 6865 636b 5f62 6f6f 6c28 7661 6c75 652c  heck_bool(value,
+00002200: 2022 7265 636f 6d70 7574 6522 290a 2020   "recompute").  
+00002210: 2020 2020 2020 7365 6c66 2e5f 7265 636f        self._reco
+00002220: 6d70 7574 6520 3d20 7661 6c75 650a 0a20  mpute = value.. 
+00002230: 2020 2040 7072 6f70 6572 7479 0a20 2020     @property.   
+00002240: 2064 6566 2073 656c 6563 745f 7265 636f   def select_reco
+00002250: 6d70 7574 6528 7365 6c66 293a 0a20 2020  mpute(self):.   
+00002260: 2020 2020 2072 6574 7572 6e20 7365 6c66       return self
+00002270: 2e5f 7365 6c65 6374 5f72 6563 6f6d 7075  ._select_recompu
+00002280: 7465 0a0a 2020 2020 4073 656c 6563 745f  te..    @select_
+00002290: 7265 636f 6d70 7574 652e 7365 7474 6572  recompute.setter
+000022a0: 0a20 2020 2064 6566 2073 656c 6563 745f  .    def select_
+000022b0: 7265 636f 6d70 7574 6528 7365 6c66 2c20  recompute(self, 
+000022c0: 7661 6c75 6529 3a0a 2020 2020 2020 2020  value):.        
+000022d0: 5661 6c69 6461 746f 722e 6368 6563 6b5f  Validator.check_
+000022e0: 626f 6f6c 2876 616c 7565 2c20 2273 656c  bool(value, "sel
+000022f0: 6563 745f 7265 636f 6d70 7574 6522 290a  ect_recompute").
+00002300: 2020 2020 2020 2020 7365 6c66 2e5f 7365          self._se
+00002310: 6c65 6374 5f72 6563 6f6d 7075 7465 203d  lect_recompute =
+00002320: 2076 616c 7565 0a0a 2020 2020 4070 726f   value..    @pro
+00002330: 7065 7274 790a 2020 2020 6465 6620 7061  perty.    def pa
+00002340: 7261 6c6c 656c 5f6f 7074 696d 697a 6572  rallel_optimizer
+00002350: 5f63 6f6d 6d5f 7265 636f 6d70 7574 6528  _comm_recompute(
+00002360: 7365 6c66 293a 0a20 2020 2020 2020 2072  self):.        r
+00002370: 6574 7572 6e20 7365 6c66 2e5f 7061 7261  eturn self._para
+00002380: 6c6c 656c 5f6f 7074 696d 697a 6572 5f63  llel_optimizer_c
+00002390: 6f6d 6d5f 7265 636f 6d70 7574 650a 0a20  omm_recompute.. 
+000023a0: 2020 2040 7061 7261 6c6c 656c 5f6f 7074     @parallel_opt
+000023b0: 696d 697a 6572 5f63 6f6d 6d5f 7265 636f  imizer_comm_reco
+000023c0: 6d70 7574 652e 7365 7474 6572 0a20 2020  mpute.setter.   
+000023d0: 2064 6566 2070 6172 616c 6c65 6c5f 6f70   def parallel_op
+000023e0: 7469 6d69 7a65 725f 636f 6d6d 5f72 6563  timizer_comm_rec
+000023f0: 6f6d 7075 7465 2873 656c 662c 2076 616c  ompute(self, val
+00002400: 7565 293a 0a20 2020 2020 2020 2056 616c  ue):.        Val
+00002410: 6964 6174 6f72 2e63 6865 636b 5f62 6f6f  idator.check_boo
+00002420: 6c28 7661 6c75 652c 2022 7061 7261 6c6c  l(value, "parall
+00002430: 656c 5f6f 7074 696d 697a 6572 5f63 6f6d  el_optimizer_com
+00002440: 6d5f 7265 636f 6d70 7574 6522 290a 2020  m_recompute").  
+00002450: 2020 2020 2020 7365 6c66 2e5f 7061 7261        self._para
+00002460: 6c6c 656c 5f6f 7074 696d 697a 6572 5f63  llel_optimizer_c
+00002470: 6f6d 6d5f 7265 636f 6d70 7574 6520 3d20  omm_recompute = 
+00002480: 7661 6c75 650a 0a20 2020 2040 7072 6f70  value..    @prop
+00002490: 6572 7479 0a20 2020 2064 6566 206d 705f  erty.    def mp_
+000024a0: 636f 6d6d 5f72 6563 6f6d 7075 7465 2873  comm_recompute(s
+000024b0: 656c 6629 3a0a 2020 2020 2020 2020 7265  elf):.        re
+000024c0: 7475 726e 2073 656c 662e 5f6d 705f 636f  turn self._mp_co
+000024d0: 6d6d 5f72 6563 6f6d 7075 7465 0a0a 2020  mm_recompute..  
+000024e0: 2020 406d 705f 636f 6d6d 5f72 6563 6f6d    @mp_comm_recom
+000024f0: 7075 7465 2e73 6574 7465 720a 2020 2020  pute.setter.    
+00002500: 6465 6620 6d70 5f63 6f6d 6d5f 7265 636f  def mp_comm_reco
+00002510: 6d70 7574 6528 7365 6c66 2c20 7661 6c75  mpute(self, valu
+00002520: 6529 3a0a 2020 2020 2020 2020 5661 6c69  e):.        Vali
+00002530: 6461 746f 722e 6368 6563 6b5f 626f 6f6c  dator.check_bool
+00002540: 2876 616c 7565 2c20 226d 705f 636f 6d6d  (value, "mp_comm
+00002550: 5f72 6563 6f6d 7075 7465 2229 0a20 2020  _recompute").   
+00002560: 2020 2020 2073 656c 662e 5f6d 705f 636f       self._mp_co
+00002570: 6d6d 5f72 6563 6f6d 7075 7465 203d 2076  mm_recompute = v
+00002580: 616c 7565 0a0a 2020 2020 4070 726f 7065  alue..    @prope
+00002590: 7274 790a 2020 2020 6465 6620 7265 636f  rty.    def reco
+000025a0: 6d70 7574 655f 736c 6963 655f 6163 7469  mpute_slice_acti
+000025b0: 7661 7469 6f6e 2873 656c 6629 3a0a 2020  vation(self):.  
+000025c0: 2020 2020 2020 7265 7475 726e 2073 656c        return sel
+000025d0: 662e 5f72 6563 6f6d 7075 7465 5f73 6c69  f._recompute_sli
+000025e0: 6365 5f61 6374 6976 6174 696f 6e0a 0a20  ce_activation.. 
+000025f0: 2020 2040 7265 636f 6d70 7574 655f 736c     @recompute_sl
+00002600: 6963 655f 6163 7469 7661 7469 6f6e 2e73  ice_activation.s
+00002610: 6574 7465 720a 2020 2020 6465 6620 7265  etter.    def re
+00002620: 636f 6d70 7574 655f 736c 6963 655f 6163  compute_slice_ac
+00002630: 7469 7661 7469 6f6e 2873 656c 662c 2076  tivation(self, v
+00002640: 616c 7565 293a 0a20 2020 2020 2020 2056  alue):.        V
+00002650: 616c 6964 6174 6f72 2e63 6865 636b 5f62  alidator.check_b
+00002660: 6f6f 6c28 7661 6c75 652c 2022 7265 636f  ool(value, "reco
+00002670: 6d70 7574 655f 736c 6963 655f 6163 7469  mpute_slice_acti
+00002680: 7661 7469 6f6e 2229 0a20 2020 2020 2020  vation").       
+00002690: 2073 656c 662e 5f72 6563 6f6d 7075 7465   self._recompute
+000026a0: 5f73 6c69 6365 5f61 6374 6976 6174 696f  _slice_activatio
+000026b0: 6e20 3d20 7661 6c75 650a 0a20 2020 2064  n = value..    d
+000026c0: 6566 205f 5f65 715f 5f28 7365 6c66 2c20  ef __eq__(self, 
+000026d0: 6f74 6865 7229 202d 3e20 626f 6f6c 3a0a  other) -> bool:.
+000026e0: 2020 2020 2020 2020 7265 7475 726e 2069          return i
+000026f0: 7369 6e73 7461 6e63 6528 6f74 6865 722c  sinstance(other,
+00002700: 2054 7261 6e73 666f 726d 6572 5265 636f   TransformerReco
+00002710: 6d70 7574 6543 6f6e 6669 6729 2061 6e64  mputeConfig) and
+00002720: 2028 7365 6c66 2e74 6f5f 6469 6374 2829   (self.to_dict()
+00002730: 203d 3d20 6f74 6865 722e 746f 5f64 6963   == other.to_dic
+00002740: 7428 2929 0a0a 2020 2020 6465 6620 746f  t())..    def to
+00002750: 5f64 6966 665f 6469 6374 2873 656c 6629  _diff_dict(self)
+00002760: 3a0a 2020 2020 2020 2020 636f 6e66 6967  :.        config
+00002770: 5f64 6963 7420 3d20 7365 6c66 2e74 6f5f  _dict = self.to_
+00002780: 6469 6374 2829 0a20 2020 2020 2020 2064  dict().        d
+00002790: 6566 6175 6c74 5f64 6963 7420 3d20 5472  efault_dict = Tr
+000027a0: 616e 7366 6f72 6d65 7252 6563 6f6d 7075  ansformerRecompu
+000027b0: 7465 436f 6e66 6967 2829 2e74 6f5f 6469  teConfig().to_di
+000027c0: 6374 2829 0a20 2020 2020 2020 2072 6573  ct().        res
+000027d0: 5f64 6963 7420 3d20 7b7d 0a20 2020 2020  _dict = {}.     
+000027e0: 2020 2066 6f72 206b 2c20 7620 696e 2063     for k, v in c
+000027f0: 6f6e 6669 675f 6469 6374 2e69 7465 6d73  onfig_dict.items
+00002800: 2829 3a0a 2020 2020 2020 2020 2020 2020  ():.            
+00002810: 6966 2076 2021 3d20 6465 6661 756c 745f  if v != default_
+00002820: 6469 6374 5b6b 5d3a 0a20 2020 2020 2020  dict[k]:.       
+00002830: 2020 2020 2020 2020 2072 6573 5f64 6963           res_dic
+00002840: 745b 6b5d 203d 2076 0a20 2020 2020 2020  t[k] = v.       
+00002850: 2072 6574 7572 6e20 7265 735f 6469 6374   return res_dict
+00002860: 0a0a 2020 2020 6465 6620 746f 5f64 6963  ..    def to_dic
+00002870: 7428 7365 6c66 293a 0a20 2020 2020 2020  t(self):.       
+00002880: 2063 6f6e 6669 675f 6469 6374 203d 207b   config_dict = {
+00002890: 0a20 2020 2020 2020 2020 2020 2022 7265  .            "re
+000028a0: 636f 6d70 7574 6522 3a20 7365 6c66 2e5f  compute": self._
+000028b0: 7265 636f 6d70 7574 652c 0a20 2020 2020  recompute,.     
+000028c0: 2020 2020 2020 2022 7365 6c65 6374 5f72         "select_r
+000028d0: 6563 6f6d 7075 7465 223a 2073 656c 662e  ecompute": self.
+000028e0: 5f73 656c 6563 745f 7265 636f 6d70 7574  _select_recomput
+000028f0: 652c 0a20 2020 2020 2020 2020 2020 2022  e,.            "
+00002900: 7061 7261 6c6c 656c 5f6f 7074 696d 697a  parallel_optimiz
+00002910: 6572 5f63 6f6d 6d5f 7265 636f 6d70 7574  er_comm_recomput
+00002920: 6522 3a20 7365 6c66 2e5f 7061 7261 6c6c  e": self._parall
+00002930: 656c 5f6f 7074 696d 697a 6572 5f63 6f6d  el_optimizer_com
+00002940: 6d5f 7265 636f 6d70 7574 652c 0a20 2020  m_recompute,.   
+00002950: 2020 2020 2020 2020 2022 6d70 5f63 6f6d           "mp_com
+00002960: 6d5f 7265 636f 6d70 7574 6522 3a20 7365  m_recompute": se
+00002970: 6c66 2e5f 6d70 5f63 6f6d 6d5f 7265 636f  lf._mp_comm_reco
+00002980: 6d70 7574 652c 0a20 2020 2020 2020 2020  mpute,.         
+00002990: 2020 2022 7265 636f 6d70 7574 655f 736c     "recompute_sl
+000029a0: 6963 655f 6163 7469 7661 7469 6f6e 223a  ice_activation":
+000029b0: 2073 656c 662e 5f72 6563 6f6d 7075 7465   self._recompute
+000029c0: 5f73 6c69 6365 5f61 6374 6976 6174 696f  _slice_activatio
+000029d0: 6e2c 0a20 2020 2020 2020 207d 0a20 2020  n,.        }.   
+000029e0: 2020 2020 2072 6574 7572 6e20 636f 6e66       return conf
+000029f0: 6967 5f64 6963 740a 0a0a 6465 6661 756c  ig_dict...defaul
+00002a00: 745f 7472 616e 7366 6f72 6d65 725f 7265  t_transformer_re
+00002a10: 636f 6d70 7574 655f 636f 6e66 6967 203d  compute_config =
+00002a20: 2054 7261 6e73 666f 726d 6572 5265 636f   TransformerReco
+00002a30: 6d70 7574 6543 6f6e 6669 6728 290a 0a0a  mputeConfig()...
+00002a40: 636c 6173 7320 5472 616e 7366 6f72 6d65  class Transforme
+00002a50: 724f 7050 6172 616c 6c65 6c43 6f6e 6669  rOpParallelConfi
+00002a60: 6728 5f43 6f6e 6669 6729 3a0a 2020 2020  g(_Config):.    
+00002a70: 7222 2222 0a20 2020 2020 2020 2054 7261  r""".        Tra
+00002a80: 6e73 666f 726d 6572 4f70 5061 7261 6c6c  nsformerOpParall
+00002a90: 656c 436f 6e66 6967 2066 6f72 2073 6574  elConfig for set
+00002aa0: 7469 6e67 2070 6172 616c 6c65 6c20 636f  ting parallel co
+00002ab0: 6e66 6967 7572 6174 696f 6e2c 2073 7563  nfiguration, suc
+00002ac0: 6820 6173 2074 6865 2064 6174 6120 7061  h as the data pa
+00002ad0: 7261 6c6c 656c 2061 6e64 206d 6f64 656c  rallel and model
+00002ae0: 2070 6172 616c 6c65 6c2e 0a0a 2020 2020   parallel...    
+00002af0: 2020 2020 4e6f 7465 3a0a 2020 2020 2020      Note:.      
+00002b00: 2020 2020 2020 4578 6365 7074 2074 6865        Except the
+00002b10: 2072 6563 6f6d 7075 7465 2061 7267 756d   recompute argum
+00002b20: 656e 742c 206f 7468 6572 2061 7267 756d  ent, other argum
+00002b30: 656e 7473 2077 696c 6c20 2a2a 6e6f 742a  ents will **not*
+00002b40: 2a20 6265 2065 6666 6563 7469 7665 2077  * be effective w
+00002b50: 6865 6e20 7468 6520 7573 6572 2064 6f65  hen the user doe
+00002b60: 736e 2774 2073 6574 0a20 2020 2020 2020  sn't set.       
+00002b70: 2020 2020 2061 7574 6f5f 7061 7261 6c6c       auto_parall
+00002b80: 656c 5f63 6f6e 7465 7874 2074 6f20 6053  el_context to `S
+00002b90: 454d 495f 4155 544f 5f50 4152 414c 4c45  EMI_AUTO_PARALLE
+00002ba0: 4c60 206f 7220 6041 5554 4f5f 5041 5241  L` or `AUTO_PARA
+00002bb0: 4c4c 454c 602e 0a20 2020 2020 2020 2020  LLEL`..         
+00002bc0: 2020 2054 6865 206d 6963 726f 5f62 6174     The micro_bat
+00002bd0: 6368 5f6e 756d 206d 7573 7420 6265 2067  ch_num must be g
+00002be0: 7265 6174 6572 2074 6861 6e20 6f72 2065  reater than or e
+00002bf0: 7175 616c 2074 6f20 7069 7065 6c69 6e65  qual to pipeline
+00002c00: 5f73 7461 6765 2077 6865 6e20 7472 6169  _stage when trai
+00002c10: 6e69 6e67 2e0a 2020 2020 2020 2020 2020  ning..          
+00002c20: 2020 5468 6520 6461 7461 5f70 6172 616c    The data_paral
+00002c30: 6c65 6c5c 2a6d 6f64 656c 5f70 6172 616c  lel\*model_paral
+00002c40: 6c65 6c20 5c2a 7069 7065 6c69 6e65 5f73  lel \*pipeline_s
+00002c50: 7461 6765 206d 7573 7420 6265 2065 7175  tage must be equ
+00002c60: 616c 206f 7220 6c65 7373 2065 7175 616c  al or less equal
+00002c70: 2074 6f20 7468 6520 6465 7669 6365 2e20   to the device. 
+00002c80: 5768 656e 2073 6574 7469 6e67 0a20 2020  When setting.   
+00002c90: 2020 2020 2020 2020 2074 6865 2070 6970           the pip
+00002ca0: 656c 696e 6520 7374 6167 6520 616e 6420  eline stage and 
+00002cb0: 6f70 7469 6d69 7a65 725f 7368 6172 642c  optimizer_shard,
+00002cc0: 2074 6865 2063 6f6e 6669 6720 7769 6c6c   the config will
+00002cd0: 206f 7665 7277 7269 7465 2074 6865 2061   overwrite the a
+00002ce0: 7574 6f5f 7061 7261 6c6c 656c 5f63 6f6e  uto_parallel_con
+00002cf0: 7465 7874 2e20 5768 656e 2067 6976 656e  text. When given
+00002d00: 2074 6865 0a20 2020 2020 2020 2020 2020   the.           
+00002d10: 2038 2064 6576 6963 6573 2061 6e64 2074   8 devices and t
+00002d20: 6865 2064 6174 615f 7061 7261 6c6c 656c  he data_parallel
+00002d30: 2069 7320 3120 616e 6420 6d6f 6465 6c5f   is 1 and model_
+00002d40: 7061 7261 6c6c 656c 2069 7320 312c 2074  parallel is 1, t
+00002d50: 6865 2063 616c 6375 6c61 7469 6f6e 2077  he calculation w
+00002d60: 696c 6c20 6265 2072 6570 6561 7465 6420  ill be repeated 
+00002d70: 6f6e 2065 6163 680a 2020 2020 2020 2020  on each.        
+00002d80: 2020 2020 6465 7669 6365 2e0a 0a20 2020      device...   
+00002d90: 2020 2020 2041 7267 733a 0a20 2020 2020       Args:.     
+00002da0: 2020 2020 2020 2064 6174 615f 7061 7261         data_para
+00002db0: 6c6c 656c 2028 696e 7429 3a20 5468 6520  llel (int): The 
+00002dc0: 6461 7461 2070 6172 616c 6c65 6c20 7761  data parallel wa
+00002dd0: 792e 2054 6865 2069 6e70 7574 2064 6174  y. The input dat
+00002de0: 6120 7769 6c6c 2062 6520 736c 6963 6564  a will be sliced
+00002df0: 2069 6e74 6f20 6e20 7061 7274 7320 666f   into n parts fo
+00002e00: 7220 6561 6368 206c 6179 6572 0a20 2020  r each layer.   
+00002e10: 2020 2020 2020 2020 2020 2020 2061 6363               acc
+00002e20: 6f72 6469 6e67 2074 6f20 7468 6520 6461  ording to the da
+00002e30: 7461 2070 6172 616c 6c65 6c20 7761 792e  ta parallel way.
+00002e40: 2044 6566 6175 6c74 3a20 312e 0a20 2020   Default: 1..   
+00002e50: 2020 2020 2020 2020 206d 6f64 656c 5f70           model_p
+00002e60: 6172 616c 6c65 6c20 2869 6e74 293a 2054  arallel (int): T
+00002e70: 6865 206d 6f64 656c 2070 6172 616c 6c65  he model paralle
+00002e80: 6c20 7761 792e 2054 6865 2070 6172 616d  l way. The param
+00002e90: 6574 6572 7320 6f66 2064 656e 7365 206c  eters of dense l
+00002ea0: 6179 6572 7320 696e 204d 756c 7469 6865  ayers in Multihe
+00002eb0: 6164 4174 7465 6e74 696f 6e20 616e 640a  adAttention and.
+00002ec0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002ed0: 4665 6564 466f 7277 6172 6420 6c61 7965  FeedForward laye
+00002ee0: 7220 7769 6c6c 2062 6520 736c 6963 6564  r will be sliced
+00002ef0: 2061 6363 6f72 6469 6e67 2074 6f20 7468   according to th
+00002f00: 6520 6d6f 6465 6c20 7061 7261 6c6c 656c  e model parallel
+00002f10: 2077 6179 2e20 4465 6661 756c 743a 2031   way. Default: 1
+00002f20: 2e0a 2020 2020 2020 2020 2020 2020 6578  ..            ex
+00002f30: 7065 7274 5f70 6172 616c 6c65 6c20 2869  pert_parallel (i
+00002f40: 6e74 293a 2054 6865 2065 7870 6572 7420  nt): The expert 
+00002f50: 7061 7261 6c6c 656c 2077 6179 2e20 5468  parallel way. Th
+00002f60: 6973 2069 7320 6566 6665 6374 6976 6520  is is effective 
+00002f70: 6f6e 6c79 2077 6865 6e20 4d6f 4520 284d  only when MoE (M
+00002f80: 6978 7475 7265 206f 6620 4578 7065 7274  ixture of Expert
+00002f90: 7329 0a20 2020 2020 2020 2020 2020 2020  s).             
+00002fa0: 2020 2069 7320 6170 706c 6965 642e 2054     is applied. T
+00002fb0: 6869 7320 7661 6c75 6520 7370 6563 6966  his value specif
+00002fc0: 6965 7320 7468 6520 6e75 6d62 6572 206f  ies the number o
+00002fd0: 6620 7061 7274 6974 696f 6e73 2074 6f20  f partitions to 
+00002fe0: 7370 6c69 7420 7468 6520 6578 7065 7274  split the expert
+00002ff0: 7320 696e 746f 2e0a 2020 2020 2020 2020  s into..        
+00003000: 2020 2020 7069 7065 6c69 6e65 5f73 7461      pipeline_sta
+00003010: 6765 2028 696e 7429 3a20 5468 6520 6e75  ge (int): The nu
+00003020: 6d62 6572 206f 6620 7468 6520 7069 7065  mber of the pipe
+00003030: 6c69 6e65 2073 7461 6765 2e20 5368 6f75  line stage. Shou
+00003040: 6c64 2062 6520 6120 706f 7369 7469 7665  ld be a positive
+00003050: 2076 616c 7565 2e20 4465 6661 756c 743a   value. Default:
+00003060: 2031 2e0a 2020 2020 2020 2020 2020 2020   1..            
+00003070: 6d69 6372 6f5f 6261 7463 685f 6e75 6d20  micro_batch_num 
+00003080: 2869 6e74 293a 2054 6865 206d 6963 726f  (int): The micro
+00003090: 2073 697a 6520 6f66 2074 6865 2062 6174   size of the bat
+000030a0: 6368 6573 2066 6f72 2074 6865 2070 6970  ches for the pip
+000030b0: 656c 696e 6520 7472 6169 6e69 6e67 2e20  eline training. 
+000030c0: 4465 6661 756c 743a 2031 2e0a 2020 2020  Default: 1..    
+000030d0: 2020 2020 2020 2020 6f70 7469 6d69 7a65          optimize
+000030e0: 725f 7368 6172 6420 2862 6f6f 6c29 3a20  r_shard (bool): 
+000030f0: 2a6f 7074 696d 697a 6572 5f73 6861 7264  *optimizer_shard
+00003100: 2069 7320 6465 7072 6563 6174 6564 2066   is deprecated f
+00003110: 726f 6d20 4d69 6e64 466f 726d 6572 7320  rom MindFormers 
+00003120: 7230 2e37 2e20 4974 2077 696c 6c20 6e6f  r0.7. It will no
+00003130: 7420 6861 7665 2061 6e79 2065 6666 6563  t have any effec
+00003140: 742e 0a20 2020 2020 2020 2020 2020 2020  t..             
+00003150: 2020 2049 7420 7769 6c6c 2062 6520 7265     It will be re
+00003160: 6d6f 7665 6420 696e 2074 6865 2066 7574  moved in the fut
+00003170: 7572 6520 7665 7273 696f 6e2e 2055 7369  ure version. Usi
+00003180: 6e67 2070 6172 616c 6c65 6c2e 656e 6162  ng parallel.enab
+00003190: 6c65 5f70 6172 616c 6c65 6c5f 6f70 7469  le_parallel_opti
+000031a0: 6d69 7a65 7220 696e 7374 6561 642e 2a0a  mizer instead.*.
+000031b0: 2020 2020 2020 2020 2020 2020 6772 6164              grad
+000031c0: 6965 6e74 5f61 6767 7265 6761 7469 6f6e  ient_aggregation
+000031d0: 5f67 726f 7570 2028 696e 7429 3a20 5468  _group (int): Th
+000031e0: 6520 6675 7369 6f6e 2067 726f 7570 2073  e fusion group s
+000031f0: 697a 6520 6f66 2074 6865 206f 7074 696d  ize of the optim
+00003200: 697a 6572 2073 7461 7465 2073 6861 7264  izer state shard
+00003210: 696e 672e 2044 6566 6175 6c74 3a20 342e  ing. Default: 4.
+00003220: 0a20 2020 2020 2020 2020 2020 2072 6563  .            rec
+00003230: 6f6d 7075 7465 2028 556e 696f 6e5b 5472  ompute (Union[Tr
+00003240: 616e 7366 6f72 6d65 7252 6563 6f6d 7075  ansformerRecompu
+00003250: 7465 436f 6e66 6967 2c20 626f 6f6c 5d29  teConfig, bool])
+00003260: 3a20 5468 6520 636f 6e66 6967 7572 6174  : The configurat
+00003270: 696f 6e20 6f66 2072 6563 6f6d 7075 7461  ion of recomputa
+00003280: 7469 6f6e 2066 6f72 0a20 2020 2020 2020  tion for.       
+00003290: 2020 2020 2020 2020 2074 6865 2074 7261           the tra
+000032a0: 6e73 666f 726d 6572 2062 6c6f 636b 2e20  nsformer block. 
+000032b0: 4465 6661 756c 743a 2041 6e20 696e 7374  Default: An inst
+000032c0: 616e 6365 206f 6620 5472 616e 7366 6f72  ance of Transfor
+000032d0: 6d65 7252 6563 6f6d 7075 7465 436f 6e66  merRecomputeConf
+000032e0: 6967 2077 6974 6820 6465 6661 756c 7420  ig with default 
+000032f0: 7661 6c75 6573 2e0a 2020 2020 2020 2020  values..        
+00003300: 2020 2020 766f 6361 625f 656d 625f 6470      vocab_emb_dp
+00003310: 2028 626f 6f6c 293a 2053 6861 7264 2065   (bool): Shard e
+00003320: 6d62 6564 6469 6e67 2069 6e20 6d6f 6465  mbedding in mode
+00003330: 6c20 7061 7261 6c6c 656c 206f 7220 6461  l parallel or da
+00003340: 7461 2070 6172 616c 6c65 6c2e 2044 6566  ta parallel. Def
+00003350: 6175 6c74 3a20 5472 7565 2e0a 0a20 2020  ault: True...   
+00003360: 2020 2020 2053 7570 706f 7274 6564 2050       Supported P
+00003370: 6c61 7466 6f72 6d73 3a0a 2020 2020 2020  latforms:.      
+00003380: 2020 2020 2020 6060 4173 6365 6e64 6060        ``Ascend``
+00003390: 2060 6047 5055 6060 0a0a 2020 2020 2020   ``GPU``..      
+000033a0: 2020 4578 616d 706c 6573 3a0a 2020 2020    Examples:.    
+000033b0: 2020 2020 2020 2020 3e3e 3e20 6672 6f6d          >>> from
+000033c0: 206d 696e 6466 6f72 6d65 7273 2e6d 6f64   mindformers.mod
+000033d0: 756c 6573 2e74 7261 6e73 666f 726d 6572  ules.transformer
+000033e0: 2069 6d70 6f72 7420 5472 616e 7366 6f72   import Transfor
+000033f0: 6d65 7252 6563 6f6d 7075 7465 436f 6e66  merRecomputeConf
+00003400: 6967 0a20 2020 2020 2020 2020 2020 203e  ig.            >
+00003410: 3e3e 2072 6563 6f6d 7075 7465 5f63 6f6e  >> recompute_con
+00003420: 6669 673d 5472 616e 7366 6f72 6d65 7252  fig=TransformerR
+00003430: 6563 6f6d 7075 7465 436f 6e66 6967 2872  ecomputeConfig(r
+00003440: 6563 6f6d 7075 7465 3d54 7275 652c 2070  ecompute=True, p
+00003450: 6172 616c 6c65 6c5f 6f70 7469 6d69 7a65  arallel_optimize
+00003460: 725f 636f 6d6d 5f72 6563 6f6d 7075 7465  r_comm_recompute
+00003470: 3d54 7275 652c 205c 0a20 2020 2020 2020  =True, \.       
+00003480: 2020 2020 202e 2e2e 2020 2020 2020 2020       ...        
+00003490: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000034a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000034b0: 2020 2020 206d 705f 636f 6d6d 5f72 6563       mp_comm_rec
+000034c0: 6f6d 7075 7465 3d54 7275 652c 2072 6563  ompute=True, rec
+000034d0: 6f6d 7075 7465 5f73 6c69 6365 5f61 6374  ompute_slice_act
+000034e0: 6976 6174 696f 6e3d 5472 7565 290a 2020  ivation=True).  
+000034f0: 2020 2020 2020 2020 2020 3e3e 3e20 636f            >>> co
+00003500: 6e66 6967 3d54 7261 6e73 666f 726d 6572  nfig=Transformer
+00003510: 4f70 5061 7261 6c6c 656c 436f 6e66 6967  OpParallelConfig
+00003520: 2864 6174 615f 7061 7261 6c6c 656c 3d31  (data_parallel=1
+00003530: 2c20 6d6f 6465 6c5f 7061 7261 6c6c 656c  , model_parallel
+00003540: 3d31 2c20 7265 636f 6d70 7574 653d 7265  =1, recompute=re
+00003550: 636f 6d70 7574 655f 636f 6e66 6967 290a  compute_config).
+00003560: 2020 2020 2222 220a 0a20 2020 2040 6172      """..    @ar
+00003570: 6773 5f74 7970 655f 6368 6563 6b28 7265  gs_type_check(re
+00003580: 636f 6d70 7574 653d 2854 7261 6e73 666f  compute=(Transfo
+00003590: 726d 6572 5265 636f 6d70 7574 6543 6f6e  rmerRecomputeCon
+000035a0: 6669 672c 2064 6963 7429 290a 2020 2020  fig, dict)).    
+000035b0: 6465 6620 5f5f 696e 6974 5f5f 2873 656c  def __init__(sel
+000035c0: 662c 2064 6174 615f 7061 7261 6c6c 656c  f, data_parallel
+000035d0: 3d31 2c20 6d6f 6465 6c5f 7061 7261 6c6c  =1, model_parall
+000035e0: 656c 3d31 2c20 6578 7065 7274 5f70 6172  el=1, expert_par
+000035f0: 616c 6c65 6c3d 312c 2070 6970 656c 696e  allel=1, pipelin
+00003600: 655f 7374 6167 653d 312c 206d 6963 726f  e_stage=1, micro
+00003610: 5f62 6174 6368 5f6e 756d 3d31 2c0a 2020  _batch_num=1,.  
+00003620: 2020 2020 2020 2020 2020 2020 2020 2072                 r
+00003630: 6563 6f6d 7075 7465 3a20 556e 696f 6e5b  ecompute: Union[
+00003640: 5472 616e 7366 6f72 6d65 7252 6563 6f6d  TransformerRecom
+00003650: 7075 7465 436f 6e66 6967 2c20 6469 6374  puteConfig, dict
+00003660: 5d20 3d20 6465 6661 756c 745f 7472 616e  ] = default_tran
+00003670: 7366 6f72 6d65 725f 7265 636f 6d70 7574  sformer_recomput
+00003680: 655f 636f 6e66 6967 2c0a 2020 2020 2020  e_config,.      
+00003690: 2020 2020 2020 2020 2020 2075 7365 5f73             use_s
+000036a0: 6571 5f70 6172 616c 6c65 6c3d 4661 6c73  eq_parallel=Fals
+000036b0: 652c 206f 7074 696d 697a 6572 5f73 6861  e, optimizer_sha
+000036c0: 7264 3d4e 6f6e 652c 2067 7261 6469 656e  rd=None, gradien
+000036d0: 745f 6167 6772 6567 6174 696f 6e5f 6772  t_aggregation_gr
+000036e0: 6f75 703d 342c 2076 6f63 6162 5f65 6d62  oup=4, vocab_emb
+000036f0: 5f64 703d 5472 7565 293a 0a20 2020 2020  _dp=True):.     
+00003700: 2020 2069 6620 6973 696e 7374 616e 6365     if isinstance
+00003710: 2872 6563 6f6d 7075 7465 2c20 6469 6374  (recompute, dict
+00003720: 293a 0a20 2020 2020 2020 2020 2020 2072  ):.            r
+00003730: 6563 6f6d 7075 7465 203d 2054 7261 6e73  ecompute = Trans
+00003740: 666f 726d 6572 5265 636f 6d70 7574 6543  formerRecomputeC
+00003750: 6f6e 6669 6728 2a2a 7265 636f 6d70 7574  onfig(**recomput
+00003760: 6529 0a20 2020 2020 2020 2073 656c 662e  e).        self.
+00003770: 7265 636f 6d70 7574 6520 3d20 7265 636f  recompute = reco
+00003780: 6d70 7574 650a 2020 2020 2020 2020 7365  mpute.        se
+00003790: 6c66 2e73 656c 6563 745f 7265 636f 6d70  lf.select_recomp
+000037a0: 7574 6520 3d20 7265 636f 6d70 7574 652e  ute = recompute.
+000037b0: 7365 6c65 6374 5f72 6563 6f6d 7075 7465  select_recompute
+000037c0: 0a20 2020 2020 2020 2073 656c 662e 7573  .        self.us
+000037d0: 655f 7365 715f 7061 7261 6c6c 656c 203d  e_seq_parallel =
+000037e0: 2075 7365 5f73 6571 5f70 6172 616c 6c65   use_seq_paralle
+000037f0: 6c0a 2020 2020 2020 2020 7365 6c66 2e6f  l.        self.o
+00003800: 7074 696d 697a 6572 5f73 6861 7264 203d  ptimizer_shard =
+00003810: 206f 7074 696d 697a 6572 5f73 6861 7264   optimizer_shard
+00003820: 0a20 2020 2020 2020 2073 656c 662e 6772  .        self.gr
+00003830: 6164 6965 6e74 5f61 6767 7265 6761 7469  adient_aggregati
+00003840: 6f6e 5f67 726f 7570 203d 2067 7261 6469  on_group = gradi
+00003850: 656e 745f 6167 6772 6567 6174 696f 6e5f  ent_aggregation_
+00003860: 6772 6f75 700a 2020 2020 2020 2020 7365  group.        se
+00003870: 6c66 2e5f 656d 6265 645f 6470 5f6d 705f  lf._embed_dp_mp_
+00003880: 636f 6e66 6967 203d 2045 6d62 6564 6469  config = Embeddi
+00003890: 6e67 4f70 5061 7261 6c6c 656c 436f 6e66  ngOpParallelConf
+000038a0: 6967 280a 2020 2020 2020 2020 2020 2020  ig(.            
+000038b0: 6461 7461 5f70 6172 616c 6c65 6c3d 6461  data_parallel=da
+000038c0: 7461 5f70 6172 616c 6c65 6c2c 206d 6f64  ta_parallel, mod
+000038d0: 656c 5f70 6172 616c 6c65 6c3d 6d6f 6465  el_parallel=mode
+000038e0: 6c5f 7061 7261 6c6c 656c 2c0a 2020 2020  l_parallel,.    
+000038f0: 2020 2020 2020 2020 766f 6361 625f 656d          vocab_em
+00003900: 625f 6470 3d76 6f63 6162 5f65 6d62 5f64  b_dp=vocab_emb_d
+00003910: 702c 2075 7365 5f73 6571 5f70 6172 616c  p, use_seq_paral
+00003920: 6c65 6c3d 7573 655f 7365 715f 7061 7261  lel=use_seq_para
+00003930: 6c6c 656c 2c0a 2020 2020 2020 2020 2020  llel,.          
+00003940: 2020 7365 6c65 6374 5f72 6563 6f6d 7075    select_recompu
+00003950: 7465 3d72 6563 6f6d 7075 7465 2e73 656c  te=recompute.sel
+00003960: 6563 745f 7265 636f 6d70 7574 6529 0a20  ect_recompute). 
+00003970: 2020 2020 2020 2073 656c 662e 5f70 705f         self._pp_
+00003980: 636f 6e66 6967 203d 205f 5069 7065 4c69  config = _PipeLi
+00003990: 6e65 436f 6e66 6967 2870 6970 656c 696e  neConfig(pipelin
+000039a0: 655f 7374 6167 653d 7069 7065 6c69 6e65  e_stage=pipeline
+000039b0: 5f73 7461 6765 2c20 6d69 6372 6f5f 6261  _stage, micro_ba
+000039c0: 7463 685f 6e75 6d3d 6d69 6372 6f5f 6261  tch_num=micro_ba
+000039d0: 7463 685f 6e75 6d29 0a20 2020 2020 2020  tch_num).       
+000039e0: 2073 656c 662e 5f6d 6f65 5f63 6f6e 6669   self._moe_confi
+000039f0: 6720 3d20 4d6f 4550 6172 616c 6c65 6c43  g = MoEParallelC
+00003a00: 6f6e 6669 6728 0a20 2020 2020 2020 2020  onfig(.         
+00003a10: 2020 2064 6174 615f 7061 7261 6c6c 656c     data_parallel
+00003a20: 3d64 6174 615f 7061 7261 6c6c 656c 2c20  =data_parallel, 
+00003a30: 6d6f 6465 6c5f 7061 7261 6c6c 656c 3d6d  model_parallel=m
+00003a40: 6f64 656c 5f70 6172 616c 6c65 6c2c 0a20  odel_parallel,. 
+00003a50: 2020 2020 2020 2020 2020 2073 656c 6563             selec
+00003a60: 745f 7265 636f 6d70 7574 653d 7265 636f  t_recompute=reco
+00003a70: 6d70 7574 652e 7365 6c65 6374 5f72 6563  mpute.select_rec
+00003a80: 6f6d 7075 7465 2c0a 2020 2020 2020 2020  ompute,.        
+00003a90: 2020 2020 6578 7065 7274 5f70 6172 616c      expert_paral
+00003aa0: 6c65 6c3d 6578 7065 7274 5f70 6172 616c  lel=expert_paral
+00003ab0: 6c65 6c2c 2075 7365 5f73 6571 5f70 6172  lel, use_seq_par
+00003ac0: 616c 6c65 6c3d 7573 655f 7365 715f 7061  allel=use_seq_pa
+00003ad0: 7261 6c6c 656c 290a 0a20 2020 2064 6566  rallel)..    def
+00003ae0: 205f 5f65 715f 5f28 7365 6c66 2c20 6f74   __eq__(self, ot
+00003af0: 6865 7229 202d 3e20 626f 6f6c 3a0a 2020  her) -> bool:.  
+00003b00: 2020 2020 2020 7265 7475 726e 2069 7369        return isi
+00003b10: 6e73 7461 6e63 6528 6f74 6865 722c 2054  nstance(other, T
+00003b20: 7261 6e73 666f 726d 6572 4f70 5061 7261  ransformerOpPara
+00003b30: 6c6c 656c 436f 6e66 6967 2920 616e 6420  llelConfig) and 
+00003b40: 2873 656c 662e 746f 5f64 6963 7428 2920  (self.to_dict() 
+00003b50: 3d3d 206f 7468 6572 2e74 6f5f 6469 6374  == other.to_dict
+00003b60: 2829 290a 0a20 2020 2064 6566 2074 6f5f  ())..    def to_
+00003b70: 6469 6666 5f64 6963 7428 7365 6c66 293a  diff_dict(self):
+00003b80: 0a20 2020 2020 2020 2063 6f6e 6669 675f  .        config_
+00003b90: 6469 6374 203d 2073 656c 662e 746f 5f64  dict = self.to_d
+00003ba0: 6963 7428 290a 2020 2020 2020 2020 6465  ict().        de
+00003bb0: 6661 756c 745f 6469 6374 203d 2064 6566  fault_dict = def
+00003bc0: 6175 6c74 5f74 7261 6e73 666f 726d 6572  ault_transformer
+00003bd0: 5f63 6f6e 6669 672e 746f 5f64 6963 7428  _config.to_dict(
+00003be0: 290a 2020 2020 2020 2020 7265 735f 6469  ).        res_di
+00003bf0: 6374 203d 207b 7d0a 2020 2020 2020 2020  ct = {}.        
+00003c00: 666f 7220 6b2c 2076 2069 6e20 636f 6e66  for k, v in conf
+00003c10: 6967 5f64 6963 742e 6974 656d 7328 293a  ig_dict.items():
+00003c20: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if 
+00003c30: 7620 213d 2064 6566 6175 6c74 5f64 6963  v != default_dic
+00003c40: 745b 6b5d 3a0a 2020 2020 2020 2020 2020  t[k]:.          
+00003c50: 2020 2020 2020 7265 735f 6469 6374 5b6b        res_dict[k
+00003c60: 5d20 3d20 760a 2020 2020 2020 2020 6966  ] = v.        if
+00003c70: 2022 7265 636f 6d70 7574 6522 2069 6e20   "recompute" in 
+00003c80: 7265 735f 6469 6374 3a0a 2020 2020 2020  res_dict:.      
+00003c90: 2020 2020 2020 7265 735f 6469 6374 5b22        res_dict["
+00003ca0: 7265 636f 6d70 7574 6522 5d20 3d20 7365  recompute"] = se
+00003cb0: 6c66 2e72 6563 6f6d 7075 7465 2e74 6f5f  lf.recompute.to_
+00003cc0: 6469 6666 5f64 6963 7428 290a 2020 2020  diff_dict().    
+00003cd0: 2020 2020 7265 7475 726e 2072 6573 5f64      return res_d
+00003ce0: 6963 740a 0a20 2020 2064 6566 2074 6f5f  ict..    def to_
+00003cf0: 6469 6374 2873 656c 6629 3a0a 2020 2020  dict(self):.    
+00003d00: 2020 2020 2222 2274 6f20 6469 6374 2222      """to dict""
+00003d10: 220a 2020 2020 2020 2020 636f 6e66 6967  ".        config
+00003d20: 5f64 6963 7420 3d20 7b0a 2020 2020 2020  _dict = {.      
+00003d30: 2020 2020 2020 2764 6174 615f 7061 7261        'data_para
+00003d40: 6c6c 656c 273a 2073 656c 662e 6461 7461  llel': self.data
+00003d50: 5f70 6172 616c 6c65 6c2c 0a20 2020 2020  _parallel,.     
+00003d60: 2020 2020 2020 2027 6d6f 6465 6c5f 7061         'model_pa
+00003d70: 7261 6c6c 656c 273a 2073 656c 662e 6d6f  rallel': self.mo
+00003d80: 6465 6c5f 7061 7261 6c6c 656c 2c0a 2020  del_parallel,.  
+00003d90: 2020 2020 2020 2020 2020 2765 7870 6572            'exper
+00003da0: 745f 7061 7261 6c6c 656c 273a 2073 656c  t_parallel': sel
+00003db0: 662e 6578 7065 7274 5f70 6172 616c 6c65  f.expert_paralle
+00003dc0: 6c2c 0a20 2020 2020 2020 2020 2020 2027  l,.            '
+00003dd0: 7069 7065 6c69 6e65 5f73 7461 6765 273a  pipeline_stage':
+00003de0: 2073 656c 662e 7069 7065 6c69 6e65 5f73   self.pipeline_s
+00003df0: 7461 6765 2c0a 2020 2020 2020 2020 2020  tage,.          
+00003e00: 2020 276d 6963 726f 5f62 6174 6368 5f6e    'micro_batch_n
+00003e10: 756d 273a 2073 656c 662e 6d69 6372 6f5f  um': self.micro_
+00003e20: 6261 7463 685f 6e75 6d2c 0a20 2020 2020  batch_num,.     
+00003e30: 2020 2020 2020 2027 7573 655f 7365 715f         'use_seq_
+00003e40: 7061 7261 6c6c 656c 273a 2073 656c 662e  parallel': self.
+00003e50: 7573 655f 7365 715f 7061 7261 6c6c 656c  use_seq_parallel
+00003e60: 2c0a 2020 2020 2020 2020 2020 2020 276f  ,.            'o
+00003e70: 7074 696d 697a 6572 5f73 6861 7264 273a  ptimizer_shard':
+00003e80: 2073 656c 662e 6f70 7469 6d69 7a65 725f   self.optimizer_
+00003e90: 7368 6172 642c 0a20 2020 2020 2020 2020  shard,.         
+00003ea0: 2020 2027 6772 6164 6965 6e74 5f61 6767     'gradient_agg
+00003eb0: 7265 6761 7469 6f6e 5f67 726f 7570 273a  regation_group':
+00003ec0: 2073 656c 662e 6772 6164 6965 6e74 5f61   self.gradient_a
+00003ed0: 6767 7265 6761 7469 6f6e 5f67 726f 7570  ggregation_group
+00003ee0: 2c0a 2020 2020 2020 2020 2020 2020 2776  ,.            'v
+00003ef0: 6f63 6162 5f65 6d62 5f64 7027 3a20 7365  ocab_emb_dp': se
+00003f00: 6c66 2e76 6f63 6162 5f65 6d62 5f64 702c  lf.vocab_emb_dp,
+00003f10: 0a20 2020 2020 2020 2020 2020 2027 7265  .            're
+00003f20: 636f 6d70 7574 6527 3a20 7365 6c66 2e72  compute': self.r
+00003f30: 6563 6f6d 7075 7465 2e74 6f5f 6469 6374  ecompute.to_dict
+00003f40: 2829 0a20 2020 2020 2020 207d 0a20 2020  ().        }.   
+00003f50: 2020 2020 2072 6574 7572 6e20 636f 6e66       return conf
+00003f60: 6967 5f64 6963 740a 0a20 2020 2040 7072  ig_dict..    @pr
+00003f70: 6f70 6572 7479 0a20 2020 2064 6566 2072  operty.    def r
+00003f80: 6563 6f6d 7075 7465 2873 656c 6629 3a0a  ecompute(self):.
+00003f90: 2020 2020 2020 2020 7265 7475 726e 2073          return s
+00003fa0: 656c 662e 5f72 6563 6f6d 7075 7465 0a0a  elf._recompute..
+00003fb0: 2020 2020 4072 6563 6f6d 7075 7465 2e73      @recompute.s
+00003fc0: 6574 7465 720a 2020 2020 6465 6620 7265  etter.    def re
+00003fd0: 636f 6d70 7574 6528 7365 6c66 2c20 7661  compute(self, va
+00003fe0: 6c75 6529 3a0a 2020 2020 2020 2020 6966  lue):.        if
+00003ff0: 206e 6f74 2069 7369 6e73 7461 6e63 6528   not isinstance(
+00004000: 7661 6c75 652c 2054 7261 6e73 666f 726d  value, Transform
+00004010: 6572 5265 636f 6d70 7574 6543 6f6e 6669  erRecomputeConfi
+00004020: 6729 2061 6e64 206e 6f74 2069 7369 6e73  g) and not isins
+00004030: 7461 6e63 6528 7661 6c75 652c 2062 6f6f  tance(value, boo
+00004040: 6c29 3a0a 2020 2020 2020 2020 2020 2020  l):.            
+00004050: 7261 6973 6520 5479 7065 4572 726f 7228  raise TypeError(
+00004060: 6622 7265 636f 6d70 7574 6520 6d75 7374  f"recompute must
+00004070: 2062 6520 6120 5472 616e 7366 6f72 6d65   be a Transforme
+00004080: 7252 6563 6f6d 7075 7465 436f 6e66 6967  rRecomputeConfig
+00004090: 2f62 6f6f 6c2c 2062 7574 2067 6f74 207b  /bool, but got {
+000040a0: 7479 7065 2876 616c 7565 292e 5f5f 6e61  type(value).__na
+000040b0: 6d65 5f5f 7d2e 2229 0a20 2020 2020 2020  me__}.").       
+000040c0: 2069 6620 6973 696e 7374 616e 6365 2876   if isinstance(v
+000040d0: 616c 7565 2c20 626f 6f6c 293a 0a20 2020  alue, bool):.   
+000040e0: 2020 2020 2020 2020 206c 6f67 6765 722e           logger.
+000040f0: 7761 726e 696e 6728 6622 5472 616e 7366  warning(f"Transf
+00004100: 6f72 6d65 7252 6563 6f6d 7075 7465 436f  ormerRecomputeCo
+00004110: 6e66 6967 2069 7320 7265 636f 6d6d 656e  nfig is recommen
+00004120: 6465 6420 6173 2074 6865 2072 6563 6f6d  ded as the recom
+00004130: 7075 7465 2063 6f6e 6669 6775 7261 7469  pute configurati
+00004140: 6f6e 2074 7970 652e 2229 0a20 2020 2020  on type.").     
+00004150: 2020 2073 656c 662e 5f72 6563 6f6d 7075     self._recompu
+00004160: 7465 203d 2076 616c 7565 0a0a 2020 2020  te = value..    
+00004170: 4070 726f 7065 7274 790a 2020 2020 6465  @property.    de
+00004180: 6620 766f 6361 625f 656d 625f 6470 2873  f vocab_emb_dp(s
+00004190: 656c 6629 3a0a 2020 2020 2020 2020 7265  elf):.        re
+000041a0: 7475 726e 2073 656c 662e 5f65 6d62 6564  turn self._embed
+000041b0: 5f64 705f 6d70 5f63 6f6e 6669 672e 766f  _dp_mp_config.vo
+000041c0: 6361 625f 656d 625f 6470 0a0a 2020 2020  cab_emb_dp..    
+000041d0: 4076 6f63 6162 5f65 6d62 5f64 702e 7365  @vocab_emb_dp.se
+000041e0: 7474 6572 0a20 2020 2064 6566 2076 6f63  tter.    def voc
+000041f0: 6162 5f65 6d62 5f64 7028 7365 6c66 2c20  ab_emb_dp(self, 
+00004200: 7661 6c75 6529 3a0a 2020 2020 2020 2020  value):.        
+00004210: 7365 6c66 2e5f 656d 6265 645f 6470 5f6d  self._embed_dp_m
+00004220: 705f 636f 6e66 6967 2e76 6f63 6162 5f65  p_config.vocab_e
+00004230: 6d62 5f64 7020 3d20 7661 6c75 650a 0a20  mb_dp = value.. 
+00004240: 2020 2040 7072 6f70 6572 7479 0a20 2020     @property.   
+00004250: 2064 6566 2067 7261 6469 656e 745f 6167   def gradient_ag
+00004260: 6772 6567 6174 696f 6e5f 6772 6f75 7028  gregation_group(
+00004270: 7365 6c66 293a 0a20 2020 2020 2020 2072  self):.        r
+00004280: 6574 7572 6e20 7365 6c66 2e5f 6772 6164  eturn self._grad
+00004290: 6965 6e74 5f61 6767 7265 6761 7469 6f6e  ient_aggregation
+000042a0: 5f67 726f 7570 0a0a 2020 2020 4067 7261  _group..    @gra
+000042b0: 6469 656e 745f 6167 6772 6567 6174 696f  dient_aggregatio
+000042c0: 6e5f 6772 6f75 702e 7365 7474 6572 0a20  n_group.setter. 
+000042d0: 2020 2064 6566 2067 7261 6469 656e 745f     def gradient_
+000042e0: 6167 6772 6567 6174 696f 6e5f 6772 6f75  aggregation_grou
+000042f0: 7028 7365 6c66 2c20 7661 6c75 6529 3a0a  p(self, value):.
+00004300: 2020 2020 2020 2020 5661 6c69 6461 746f          Validato
+00004310: 722e 6368 6563 6b5f 706f 7369 7469 7665  r.check_positive
+00004320: 5f69 6e74 2876 616c 7565 2c20 2267 7261  _int(value, "gra
+00004330: 6469 656e 745f 6167 6772 6567 6174 696f  dient_aggregatio
+00004340: 6e5f 6772 6f75 7022 290a 2020 2020 2020  n_group").      
+00004350: 2020 7365 6c66 2e5f 6772 6164 6965 6e74    self._gradient
+00004360: 5f61 6767 7265 6761 7469 6f6e 5f67 726f  _aggregation_gro
+00004370: 7570 203d 2076 616c 7565 0a0a 2020 2020  up = value..    
+00004380: 4070 726f 7065 7274 790a 2020 2020 6465  @property.    de
+00004390: 6620 6d69 6372 6f5f 6261 7463 685f 6e75  f micro_batch_nu
+000043a0: 6d28 7365 6c66 293a 0a20 2020 2020 2020  m(self):.       
+000043b0: 2072 6574 7572 6e20 7365 6c66 2e5f 7070   return self._pp
+000043c0: 5f63 6f6e 6669 672e 6d69 6372 6f5f 6261  _config.micro_ba
+000043d0: 7463 685f 6e75 6d0a 0a20 2020 2040 6d69  tch_num..    @mi
+000043e0: 6372 6f5f 6261 7463 685f 6e75 6d2e 7365  cro_batch_num.se
+000043f0: 7474 6572 0a20 2020 2064 6566 206d 6963  tter.    def mic
+00004400: 726f 5f62 6174 6368 5f6e 756d 2873 656c  ro_batch_num(sel
+00004410: 662c 2076 616c 7565 293a 0a20 2020 2020  f, value):.     
+00004420: 2020 2073 656c 662e 5f70 705f 636f 6e66     self._pp_conf
+00004430: 6967 2e6d 6963 726f 5f62 6174 6368 5f6e  ig.micro_batch_n
+00004440: 756d 203d 2076 616c 7565 0a0a 2020 2020  um = value..    
+00004450: 4070 726f 7065 7274 790a 2020 2020 6465  @property.    de
+00004460: 6620 6d6f 6465 6c5f 7061 7261 6c6c 656c  f model_parallel
+00004470: 2873 656c 6629 3a0a 2020 2020 2020 2020  (self):.        
+00004480: 7265 7475 726e 2073 656c 662e 5f65 6d62  return self._emb
+00004490: 6564 5f64 705f 6d70 5f63 6f6e 6669 672e  ed_dp_mp_config.
+000044a0: 6d6f 6465 6c5f 7061 7261 6c6c 656c 0a0a  model_parallel..
+000044b0: 2020 2020 406d 6f64 656c 5f70 6172 616c      @model_paral
+000044c0: 6c65 6c2e 7365 7474 6572 0a20 2020 2064  lel.setter.    d
+000044d0: 6566 206d 6f64 656c 5f70 6172 616c 6c65  ef model_paralle
+000044e0: 6c28 7365 6c66 2c20 7661 6c75 6529 3a0a  l(self, value):.
+000044f0: 2020 2020 2020 2020 7365 6c66 2e5f 656d          self._em
+00004500: 6265 645f 6470 5f6d 705f 636f 6e66 6967  bed_dp_mp_config
+00004510: 2e6d 6f64 656c 5f70 6172 616c 6c65 6c20  .model_parallel 
+00004520: 3d20 7661 6c75 650a 2020 2020 2020 2020  = value.        
+00004530: 7365 6c66 2e5f 6d6f 655f 636f 6e66 6967  self._moe_config
+00004540: 2e6d 6f64 656c 5f70 6172 616c 6c65 6c20  .model_parallel 
+00004550: 3d20 7661 6c75 650a 0a20 2020 2040 7072  = value..    @pr
+00004560: 6f70 6572 7479 0a20 2020 2064 6566 2064  operty.    def d
+00004570: 6174 615f 7061 7261 6c6c 656c 2873 656c  ata_parallel(sel
+00004580: 6629 3a0a 2020 2020 2020 2020 7265 7475  f):.        retu
+00004590: 726e 2073 656c 662e 5f65 6d62 6564 5f64  rn self._embed_d
+000045a0: 705f 6d70 5f63 6f6e 6669 672e 6461 7461  p_mp_config.data
+000045b0: 5f70 6172 616c 6c65 6c0a 0a20 2020 2040  _parallel..    @
+000045c0: 6461 7461 5f70 6172 616c 6c65 6c2e 7365  data_parallel.se
+000045d0: 7474 6572 0a20 2020 2064 6566 2064 6174  tter.    def dat
+000045e0: 615f 7061 7261 6c6c 656c 2873 656c 662c  a_parallel(self,
+000045f0: 2076 616c 7565 293a 0a20 2020 2020 2020   value):.       
+00004600: 2073 656c 662e 5f65 6d62 6564 5f64 705f   self._embed_dp_
+00004610: 6d70 5f63 6f6e 6669 672e 6461 7461 5f70  mp_config.data_p
+00004620: 6172 616c 6c65 6c20 3d20 7661 6c75 650a  arallel = value.
+00004630: 2020 2020 2020 2020 7365 6c66 2e5f 6d6f          self._mo
+00004640: 655f 636f 6e66 6967 2e64 6174 615f 7061  e_config.data_pa
+00004650: 7261 6c6c 656c 203d 2076 616c 7565 0a0a  rallel = value..
+00004660: 2020 2020 4070 726f 7065 7274 790a 2020      @property.  
+00004670: 2020 6465 6620 6578 7065 7274 5f70 6172    def expert_par
+00004680: 616c 6c65 6c28 7365 6c66 293a 0a20 2020  allel(self):.   
+00004690: 2020 2020 2072 6574 7572 6e20 7365 6c66       return self
+000046a0: 2e5f 6d6f 655f 636f 6e66 6967 2e65 7870  ._moe_config.exp
+000046b0: 6572 745f 7061 7261 6c6c 656c 0a0a 2020  ert_parallel..  
+000046c0: 2020 4065 7870 6572 745f 7061 7261 6c6c    @expert_parall
+000046d0: 656c 2e73 6574 7465 720a 2020 2020 6465  el.setter.    de
+000046e0: 6620 6578 7065 7274 5f70 6172 616c 6c65  f expert_paralle
+000046f0: 6c28 7365 6c66 2c20 7661 6c75 6529 3a0a  l(self, value):.
+00004700: 2020 2020 2020 2020 7365 6c66 2e5f 6d6f          self._mo
+00004710: 655f 636f 6e66 6967 2e65 7870 6572 745f  e_config.expert_
+00004720: 7061 7261 6c6c 656c 203d 2076 616c 7565  parallel = value
+00004730: 0a0a 2020 2020 4070 726f 7065 7274 790a  ..    @property.
+00004740: 2020 2020 6465 6620 7069 7065 6c69 6e65      def pipeline
+00004750: 5f73 7461 6765 2873 656c 6629 3a0a 2020  _stage(self):.  
+00004760: 2020 2020 2020 7265 7475 726e 2073 656c        return sel
+00004770: 662e 5f70 705f 636f 6e66 6967 2e70 6970  f._pp_config.pip
+00004780: 656c 696e 655f 7374 6167 650a 0a20 2020  eline_stage..   
+00004790: 2040 7069 7065 6c69 6e65 5f73 7461 6765   @pipeline_stage
+000047a0: 2e73 6574 7465 720a 2020 2020 6465 6620  .setter.    def 
+000047b0: 7069 7065 6c69 6e65 5f73 7461 6765 2873  pipeline_stage(s
+000047c0: 656c 662c 2076 616c 7565 293a 0a20 2020  elf, value):.   
+000047d0: 2020 2020 2073 656c 662e 5f70 705f 636f       self._pp_co
+000047e0: 6e66 6967 2e70 6970 656c 696e 655f 7374  nfig.pipeline_st
+000047f0: 6167 6520 3d20 7661 6c75 650a 0a20 2020  age = value..   
+00004800: 2040 7072 6f70 6572 7479 0a20 2020 2064   @property.    d
+00004810: 6566 206f 7074 696d 697a 6572 5f73 6861  ef optimizer_sha
+00004820: 7264 2873 656c 6629 3a0a 2020 2020 2020  rd(self):.      
+00004830: 2020 7265 7475 726e 2073 656c 662e 5f6f    return self._o
+00004840: 7074 696d 697a 6572 5f73 6861 7264 0a0a  ptimizer_shard..
+00004850: 2020 2020 406f 7074 696d 697a 6572 5f73      @optimizer_s
+00004860: 6861 7264 2e73 6574 7465 720a 2020 2020  hard.setter.    
+00004870: 6465 6620 6f70 7469 6d69 7a65 725f 7368  def optimizer_sh
+00004880: 6172 6428 7365 6c66 2c20 7661 6c75 6529  ard(self, value)
+00004890: 3a0a 2020 2020 2020 2020 7365 6c66 2e5f  :.        self._
+000048a0: 6f70 7469 6d69 7a65 725f 7368 6172 6420  optimizer_shard 
+000048b0: 3d20 7661 6c75 650a 2020 2020 2020 2020  = value.        
+000048c0: 6966 2076 616c 7565 3a0a 2020 2020 2020  if value:.      
+000048d0: 2020 2020 2020 6c6f 6767 6572 2e77 6172        logger.war
+000048e0: 6e69 6e67 2822 5c22 7061 7261 6c6c 656c  ning("\"parallel
+000048f0: 5f63 6f6e 6669 672e 6f70 7469 6d69 7a65  _config.optimize
+00004900: 725f 7368 6172 645c 2220 6973 2064 6570  r_shard\" is dep
+00004910: 7265 6361 7465 6420 6672 6f6d 204d 696e  recated from Min
+00004920: 6446 6f72 6d65 7273 2072 302e 372e 2049  dFormers r0.7. I
+00004930: 7420 7769 6c6c 206e 6f74 2068 6176 6520  t will not have 
+00004940: 220a 2020 2020 2020 2020 2020 2020 2020  ".              
+00004950: 2020 2020 2020 2020 2020 2020 2022 616e               "an
+00004960: 7920 6566 6665 6374 2e20 506c 6561 7365  y effect. Please
+00004970: 2075 7365 205c 2270 6172 616c 6c65 6c2e   use \"parallel.
+00004980: 656e 6162 6c65 5f70 6172 616c 6c65 6c5f  enable_parallel_
+00004990: 6f70 7469 6d69 7a65 725c 2220 746f 2074  optimizer\" to t
+000049a0: 7572 6e20 6f6e 206f 7220 6f66 6620 7468  urn on or off th
+000049b0: 6520 220a 2020 2020 2020 2020 2020 2020  e ".            
+000049c0: 2020 2020 2020 2020 2020 2020 2020 2022                 "
+000049d0: 6f70 7469 6d69 7a65 7220 7061 7261 6c6c  optimizer parall
+000049e0: 656c 2e22 290a 0a20 2020 2040 7072 6f70  el.")..    @prop
+000049f0: 6572 7479 0a20 2020 2064 6566 2065 6d62  erty.    def emb
+00004a00: 6564 6469 6e67 5f64 705f 6d70 5f63 6f6e  edding_dp_mp_con
+00004a10: 6669 6728 7365 6c66 293a 0a20 2020 2020  fig(self):.     
+00004a20: 2020 2072 6574 7572 6e20 7365 6c66 2e5f     return self._
+00004a30: 656d 6265 645f 6470 5f6d 705f 636f 6e66  embed_dp_mp_conf
+00004a40: 6967 0a0a 2020 2020 4070 726f 7065 7274  ig..    @propert
+00004a50: 790a 2020 2020 6465 6620 6470 5f6d 705f  y.    def dp_mp_
+00004a60: 636f 6e66 6967 2873 656c 6629 3a0a 2020  config(self):.  
+00004a70: 2020 2020 2020 7265 7475 726e 2073 656c        return sel
+00004a80: 662e 5f65 6d62 6564 5f64 705f 6d70 5f63  f._embed_dp_mp_c
+00004a90: 6f6e 6669 672e 6470 5f6d 705f 636f 6e66  onfig.dp_mp_conf
+00004aa0: 6967 0a0a 2020 2020 4070 726f 7065 7274  ig..    @propert
+00004ab0: 790a 2020 2020 6465 6620 6d6f 655f 7061  y.    def moe_pa
+00004ac0: 7261 6c6c 656c 5f63 6f6e 6669 6728 7365  rallel_config(se
+00004ad0: 6c66 293a 0a20 2020 2020 2020 2072 6574  lf):.        ret
+00004ae0: 7572 6e20 7365 6c66 2e5f 6d6f 655f 636f  urn self._moe_co
+00004af0: 6e66 6967 0a0a 0a64 6566 6175 6c74 5f74  nfig...default_t
+00004b00: 7261 6e73 666f 726d 6572 5f63 6f6e 6669  ransformer_confi
+00004b10: 6720 3d20 5472 616e 7366 6f72 6d65 724f  g = TransformerO
+00004b20: 7050 6172 616c 6c65 6c43 6f6e 6669 6728  pParallelConfig(
+00004b30: 290a 6465 6661 756c 745f 656d 6265 6464  ).default_embedd
+00004b40: 696e 675f 7061 7261 6c6c 656c 5f63 6f6e  ing_parallel_con
+00004b50: 6669 6720 3d20 456d 6265 6464 696e 674f  fig = EmbeddingO
+00004b60: 7050 6172 616c 6c65 6c43 6f6e 6669 6728  pParallelConfig(
+00004b70: 290a 0a0a 636c 6173 7320 4665 6564 466f  )...class FeedFo
+00004b80: 7277 6172 6428 4365 6c6c 293a 0a20 2020  rward(Cell):.   
+00004b90: 2072 2222 220a 2020 2020 2020 2020 5468   r""".        Th
+00004ba0: 6520 6d75 6c74 696c 6179 6572 2070 6572  e multilayer per
+00004bb0: 6365 7074 726f 6e20 7769 7468 2074 776f  ceptron with two
+00004bc0: 206c 696e 6561 7220 6c61 7965 7273 2077   linear layers w
+00004bd0: 6974 6820 6472 6f70 6f75 7420 6170 706c  ith dropout appl
+00004be0: 6965 6420 6174 2066 696e 616c 206f 7574  ied at final out
+00004bf0: 7075 742e 2054 6865 2066 6972 7374 206c  put. The first l
+00004c00: 696e 6561 720a 2020 2020 2020 2020 7769  inear.        wi
+00004c10: 6c6c 2070 726f 6a65 6374 2074 6865 2069  ll project the i
+00004c20: 6e70 7574 2064 696d 656e 7369 6f6e 2066  nput dimension f
+00004c30: 726f 6d20 6869 6464 656e 5f73 697a 6520  rom hidden_size 
+00004c40: 746f 2066 666e 5f68 6964 6465 6e5f 7369  to ffn_hidden_si
+00004c50: 7a65 2e20 5468 6520 7365 636f 6e64 206c  ze. The second l
+00004c60: 696e 6561 7220 7769 6c6c 2070 726f 6a65  inear will proje
+00004c70: 6374 2074 6865 0a20 2020 2020 2020 2064  ct the.        d
+00004c80: 696d 656e 7369 6f6e 2066 726f 6d20 6666  imension from ff
+00004c90: 6e5f 6869 6464 656e 5f73 697a 6520 746f  n_hidden_size to
+00004ca0: 2068 6964 6465 6e5f 7369 7a65 2e20 5468   hidden_size. Th
+00004cb0: 6520 6669 7273 7420 6c69 6e65 6172 2069  e first linear i
+00004cc0: 7320 7368 6172 6465 6420 6f6e 2074 6865  s sharded on the
+00004cd0: 2072 656c 6174 6976 6520 6469 6d65 6e73   relative dimens
+00004ce0: 696f 6e2c 0a20 2020 2020 2020 2061 6e64  ion,.        and
+00004cf0: 2074 6865 2073 6563 6f6e 6420 6c69 6e65   the second line
+00004d00: 6172 2069 7320 7368 6172 6465 6420 6f6e  ar is sharded on
+00004d10: 2074 6865 206f 7574 7075 7420 6469 6d65   the output dime
+00004d20: 6e73 696f 6e2e 2054 6865 206f 7665 7276  nsion. The overv
+00004d30: 6965 7720 7072 6f63 6573 7320 6361 6e20  iew process can 
+00004d40: 6265 3a0a 0a20 2020 2020 2020 202e 2e20  be:..        .. 
+00004d50: 6d61 7468 3a3a 0a20 2020 2020 2020 2020  math::.         
+00004d60: 2020 2044 726f 706f 7574 2828 7857 5f31     Dropout((xW_1
+00004d70: 2b62 5f31 2957 5f32 202b 2062 5f32 290a  +b_1)W_2 + b_2).
+00004d80: 0a20 2020 2020 2020 2077 6865 7265 2074  .        where t
+00004d90: 6865 203a 6d61 7468 3a60 575f 312c 2057  he :math:`W_1, W
+00004da0: 5f32 2c20 625f 3160 2061 6e64 203a 6d61  _2, b_1` and :ma
+00004db0: 7468 3a60 625f 3260 2061 7265 2074 7261  th:`b_2` are tra
+00004dc0: 696e 6162 6c65 2070 6172 616d 6574 6572  inable parameter
+00004dd0: 732e 0a0a 2020 2020 2020 2020 4172 6773  s...        Args
+00004de0: 3a0a 2020 2020 2020 2020 2020 2020 6869  :.            hi
+00004df0: 6464 656e 5f73 697a 6520 2869 6e74 293a  dden_size (int):
+00004e00: 2054 6865 2064 696d 656e 7369 6f6e 206f   The dimension o
+00004e10: 6620 7468 6520 696e 7075 7473 2e0a 2020  f the inputs..  
+00004e20: 2020 2020 2020 2020 2020 6666 6e5f 6869            ffn_hi
+00004e30: 6464 656e 5f73 697a 6520 2869 6e74 293a  dden_size (int):
+00004e40: 2054 6865 2069 6e74 6572 6d65 6469 6174   The intermediat
+00004e50: 6520 6869 6464 656e 2073 697a 652e 0a20  e hidden size.. 
+00004e60: 2020 2020 2020 2020 2020 2064 726f 706f             dropo
+00004e70: 7574 5f72 6174 6520 2866 6c6f 6174 293a  ut_rate (float):
+00004e80: 2054 6865 2064 726f 706f 7574 2072 6174   The dropout rat
+00004e90: 6520 666f 7220 7468 6520 7365 636f 6e64  e for the second
+00004ea0: 206c 696e 6561 7227 7320 6f75 7470 7574   linear's output
+00004eb0: 2e0a 2020 2020 2020 2020 2020 2020 6869  ..            hi
+00004ec0: 6464 656e 5f61 6374 2028 7374 722c 206e  dden_act (str, n
+00004ed0: 6e2e 4365 6c6c 293a 2054 6865 2061 6374  n.Cell): The act
+00004ee0: 6976 6174 696f 6e20 6f66 2074 6865 2069  ivation of the i
+00004ef0: 6e74 6572 6e61 6c20 6665 6564 666f 7277  nternal feedforw
+00004f00: 6172 6420 6c61 7965 722e 2053 7570 706f  ard layer. Suppo
+00004f10: 7274 7320 2772 656c 7527 2c0a 2020 2020  rts 'relu',.    
+00004f20: 2020 2020 2020 2020 2020 2020 2772 656c              'rel
+00004f30: 7536 272c 2027 7461 6e68 272c 2027 6765  u6', 'tanh', 'ge
+00004f40: 6c75 272c 2027 6661 7374 5f67 656c 7527  lu', 'fast_gelu'
+00004f50: 2c20 2765 6c75 272c 2027 7369 676d 6f69  , 'elu', 'sigmoi
+00004f60: 6427 2c20 2770 7265 6c75 272c 2027 6c65  d', 'prelu', 'le
+00004f70: 616b 7972 656c 7527 2c20 2768 7377 6973  akyrelu', 'hswis
+00004f80: 6827 2c0a 2020 2020 2020 2020 2020 2020  h',.            
+00004f90: 2020 2020 2768 7369 676d 6f69 6427 2c20      'hsigmoid', 
+00004fa0: 276c 6f67 7369 676d 6f69 6427 2061 6e64  'logsigmoid' and
+00004fb0: 2073 6f20 6f6e 2e20 5573 6572 2063 616e   so on. User can
+00004fc0: 2070 726f 7669 6465 2063 7573 746f 6d20   provide custom 
+00004fd0: 6163 7469 7669 7469 6f6e 2074 6f20 7468  activition to th
+00004fe0: 6520 6172 6775 6d65 6e74 2e0a 2020 2020  e argument..    
+00004ff0: 2020 2020 2020 2020 2020 2020 4966 2075              If u
+00005000: 7365 7220 7761 6e74 7320 746f 2072 756e  ser wants to run
+00005010: 2074 6865 206e 6574 2069 6e20 7468 6520   the net in the 
+00005020: 7061 7261 6c6c 656c 206d 6f64 652c 2074  parallel mode, t
+00005030: 6865 2063 7573 746f 6d20 6163 7469 7661  he custom activa
+00005040: 7469 6f6e 206d 7573 7420 616c 736f 2070  tion must also p
+00005050: 726f 7669 6465 0a20 2020 2020 2020 2020  rovide.         
+00005060: 2020 2020 2020 2074 6865 2060 6163 7469         the `acti
+00005070: 7661 7469 6f6e 5f73 6861 7264 6020 6675  vation_shard` fu
+00005080: 6e63 7469 6f6e 2e20 506c 6561 7365 2073  nction. Please s
+00005090: 6565 2065 7861 6d70 6c65 732e 2044 6566  ee examples. Def
+000050a0: 6175 6c74 3a20 6765 6c75 2e0a 2020 2020  ault: gelu..    
+000050b0: 2020 2020 2020 2020 6578 7065 7274 5f6e          expert_n
+000050c0: 756d 2028 696e 7429 3a20 5468 6520 6e75  um (int): The nu
+000050d0: 6d62 6572 206f 6620 6578 7065 7274 7320  mber of experts 
+000050e0: 7573 6564 2069 6e20 4c69 6e65 6172 2e20  used in Linear. 
+000050f0: 466f 7220 7468 6520 6361 7365 2065 7870  For the case exp
+00005100: 6572 745f 6e75 6d20 3e20 312c 2042 6174  ert_num > 1, Bat
+00005110: 6368 4d61 744d 756c 2069 7320 7573 6564  chMatMul is used
+00005120: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00005130: 2061 6e64 2074 6865 2066 6972 7374 2064   and the first d
+00005140: 696d 656e 7369 6f6e 2069 6e20 4261 7463  imension in Batc
+00005150: 684d 6174 4d75 6c20 696e 6469 6361 7465  hMatMul indicate
+00005160: 2065 7870 6572 745f 6e75 6d2e 2044 6566   expert_num. Def
+00005170: 6175 6c74 3a20 312e 0a20 2020 2020 2020  ault: 1..       
+00005180: 2020 2020 2065 7870 6572 745f 6772 6f75       expert_grou
+00005190: 705f 7369 7a65 2028 696e 7429 3a20 5468  p_size (int): Th
+000051a0: 6520 6e75 6d62 6572 206f 6620 746f 6b65  e number of toke
+000051b0: 6e73 2069 6e20 6561 6368 2064 6174 6120  ns in each data 
+000051c0: 7061 7261 6c6c 656c 2067 726f 7570 2e20  parallel group. 
+000051d0: 4465 6661 756c 743a 204e 6f6e 652e 2054  Default: None. T
+000051e0: 6869 7320 7061 7261 6d65 7465 7220 6973  his parameter is
+000051f0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00005200: 2065 6666 6563 7469 7665 206f 6e6c 7920   effective only 
+00005210: 7768 656e 2069 6e20 4155 544f 5f50 4152  when in AUTO_PAR
+00005220: 414c 4c45 4c20 6d6f 6465 2c20 616e 6420  ALLEL mode, and 
+00005230: 4e4f 5420 5348 4152 4449 4e47 5f50 524f  NOT SHARDING_PRO
+00005240: 5041 4741 5449 4f4e 2e0a 2020 2020 2020  PAGATION..      
+00005250: 2020 2020 2020 7061 7261 6d5f 696e 6974        param_init
+00005260: 5f74 7970 6520 2864 7479 7065 2e4e 756d  _type (dtype.Num
+00005270: 6265 7229 3a20 5468 6520 7061 7261 6d65  ber): The parame
+00005280: 7465 7220 696e 6974 6961 6c69 7a61 7469  ter initializati
+00005290: 6f6e 2074 7970 652e 2053 686f 756c 6420  on type. Should 
+000052a0: 6265 206d 7374 7970 652e 666c 6f61 7433  be mstype.float3
+000052b0: 3220 6f72 0a20 2020 2020 2020 2020 2020  2 or.           
+000052c0: 2020 2020 206d 7374 7970 652e 666c 6f61       mstype.floa
+000052d0: 7431 362e 2044 6566 6175 6c74 3a20 6d73  t16. Default: ms
+000052e0: 7479 7065 2e66 6c6f 6174 3332 2e0a 2020  type.float32..  
+000052f0: 2020 2020 2020 2020 2020 7061 7261 6c6c            parall
+00005300: 656c 5f63 6f6e 6669 6720 284f 7050 6172  el_config (OpPar
+00005310: 616c 6c65 6c43 6f6e 6669 672c 204d 6f45  allelConfig, MoE
+00005320: 5061 7261 6c6c 656c 436f 6e66 6967 293a  ParallelConfig):
+00005330: 2054 6865 2063 6f6e 6669 6720 6f66 2070   The config of p
+00005340: 6172 616c 6c65 6c20 7365 7474 696e 672c  arallel setting,
+00005350: 2073 6565 0a20 2020 2020 2020 2020 2020   see.           
+00005360: 2020 2020 2060 4f70 5061 7261 6c6c 656c       `OpParallel
+00005370: 436f 6e66 6967 6020 6f72 2060 4d6f 4550  Config` or `MoEP
+00005380: 6172 616c 6c65 6c43 6f6e 6669 6760 2e20  arallelConfig`. 
+00005390: 5768 656e 204d 6f45 2069 7320 6170 706c  When MoE is appl
+000053a0: 6965 642c 204d 6f45 5061 7261 6c6c 656c  ied, MoEParallel
+000053b0: 436f 6e66 6967 2069 7320 6566 6665 6374  Config is effect
+000053c0: 6976 652c 0a20 2020 2020 2020 2020 2020  ive,.           
+000053d0: 2020 2020 206f 7468 6572 7769 7365 204f       otherwise O
+000053e0: 7050 6172 616c 6c65 6c43 6f6e 6669 6720  pParallelConfig 
+000053f0: 6973 2065 6666 6563 7469 7665 2e20 4465  is effective. De
+00005400: 6661 756c 7420 6064 6566 6175 6c74 5f64  fault `default_d
+00005410: 706d 705f 636f 6e66 6967 602c 0a20 2020  pmp_config`,.   
+00005420: 2020 2020 2020 2020 2020 2020 2061 6e20               an 
+00005430: 696e 7374 616e 6365 206f 6620 604f 7050  instance of `OpP
+00005440: 6172 616c 6c65 6c43 6f6e 6669 6760 2077  arallelConfig` w
+00005450: 6974 6820 6465 6661 756c 7420 6172 6773  ith default args
+00005460: 2e0a 0a20 2020 2020 2020 2049 6e70 7574  ...        Input
+00005470: 733a 0a20 2020 2020 2020 2020 2020 202d  s:.            -
+00005480: 202a 2a78 2a2a 2028 5465 6e73 6f72 2920   **x** (Tensor) 
+00005490: 2d20 7368 6f75 6c64 2062 6520 605b 6261  - should be `[ba
+000054a0: 7463 682c 2073 6571 5f6c 656e 6774 682c  tch, seq_length,
+000054b0: 2068 6964 6465 6e5f 7369 7a65 5d20 6f72   hidden_size] or
+000054c0: 205b 6261 7463 6820 2a20 7365 715f 6c65   [batch * seq_le
+000054d0: 6e67 7468 2c20 6869 6464 656e 5f73 697a  ngth, hidden_siz
+000054e0: 655d 602e 0a20 2020 2020 2020 2020 2020  e]`..           
+000054f0: 2020 2046 6c6f 6174 2074 656e 736f 722e     Float tensor.
+00005500: 0a0a 2020 2020 2020 2020 4f75 7470 7574  ..        Output
+00005510: 733a 0a20 2020 2020 2020 2020 2020 2054  s:.            T
+00005520: 656e 736f 722c 2074 6865 206f 7574 7075  ensor, the outpu
+00005530: 7420 6f66 2074 6869 7320 6c61 7965 7220  t of this layer 
+00005540: 6166 7465 7220 6d61 7070 696e 672e 2054  after mapping. T
+00005550: 6865 2073 6861 7065 2069 7320 605b 6261  he shape is `[ba
+00005560: 7463 682c 2073 6571 5f6c 656e 6774 682c  tch, seq_length,
+00005570: 2068 6964 6465 6e5f 7369 7a65 5d20 6f72   hidden_size] or
+00005580: 0a20 2020 2020 2020 2020 2020 205b 6261  .            [ba
+00005590: 7463 6820 2a20 7365 715f 6c65 6e67 7468  tch * seq_length
+000055a0: 2c20 6869 6464 656e 5f73 697a 655d 602e  , hidden_size]`.
+000055b0: 0a0a 2020 2020 2020 2020 5261 6973 6573  ..        Raises
+000055c0: 3a0a 2020 2020 2020 2020 2020 2020 5479  :.            Ty
+000055d0: 7065 4572 726f 723a 2060 6869 6464 656e  peError: `hidden
+000055e0: 5f61 6374 6020 6973 206e 6f74 2061 2073  _act` is not a s
+000055f0: 7472 696e 6720 6f72 206e 6e2e 4365 6c6c  tring or nn.Cell
+00005600: 2e0a 2020 2020 2020 2020 2020 2020 5479  ..            Ty
+00005610: 7065 4572 726f 723a 2060 7061 7261 6c6c  peError: `parall
+00005620: 656c 5f63 6f6e 6669 6760 2069 7320 6e6f  el_config` is no
+00005630: 7420 6120 7375 6263 6c61 7373 206f 6620  t a subclass of 
+00005640: 4f70 5061 7261 6c6c 656c 436f 6e66 6967  OpParallelConfig
+00005650: 2e0a 2020 2020 2020 2020 2020 2020 5661  ..            Va
+00005660: 6c75 6545 7272 6f72 3a20 6066 666e 5f68  lueError: `ffn_h
+00005670: 6964 6465 6e5f 7369 7a65 6020 6973 206e  idden_size` is n
+00005680: 6f74 2061 206d 756c 7469 706c 6520 6f66  ot a multiple of
+00005690: 2074 6865 206d 6f64 656c 2070 6172 616c   the model paral
+000056a0: 6c65 6c20 7761 792e 0a20 2020 2020 2020  lel way..       
+000056b0: 2020 2020 2056 616c 7565 4572 726f 723a       ValueError:
+000056c0: 2060 6869 6464 656e 5f73 697a 6560 2069   `hidden_size` i
+000056d0: 7320 6e6f 7420 6120 6d75 6c74 6970 6c65  s not a multiple
+000056e0: 206f 6620 7468 6520 6d6f 6465 6c20 7061   of the model pa
+000056f0: 7261 6c6c 656c 2077 6179 2e0a 0a20 2020  rallel way...   
+00005700: 2020 2020 2053 7570 706f 7274 6564 2050       Supported P
+00005710: 6c61 7466 6f72 6d73 3a0a 2020 2020 2020  latforms:.      
+00005720: 2020 2020 2020 6060 4173 6365 6e64 6060        ``Ascend``
+00005730: 2060 6047 5055 6060 0a0a 2020 2020 2020   ``GPU``..      
+00005740: 2020 4578 616d 706c 6573 3a0a 2020 2020    Examples:.    
+00005750: 2020 2020 2020 2020 3e3e 3e20 696d 706f          >>> impo
+00005760: 7274 206e 756d 7079 2061 7320 6e70 0a20  rt numpy as np. 
+00005770: 2020 2020 2020 2020 2020 203e 3e3e 2066             >>> f
+00005780: 726f 6d20 6d69 6e64 666f 726d 6572 732e  rom mindformers.
+00005790: 6d6f 6475 6c65 732e 7472 616e 7366 6f72  modules.transfor
+000057a0: 6d65 7220 696d 706f 7274 2046 6565 6446  mer import FeedF
+000057b0: 6f72 7761 7264 0a20 2020 2020 2020 2020  orward.         
+000057c0: 2020 203e 3e3e 2066 726f 6d20 6d69 6e64     >>> from mind
+000057d0: 7370 6f72 6520 696d 706f 7274 2064 7479  spore import dty
+000057e0: 7065 2061 7320 6d73 7479 7065 0a20 2020  pe as mstype.   
+000057f0: 2020 2020 2020 2020 203e 3e3e 2066 726f           >>> fro
+00005800: 6d20 6d69 6e64 7370 6f72 6520 696d 706f  m mindspore impo
+00005810: 7274 2054 656e 736f 722c 206e 6e0a 2020  rt Tensor, nn.  
+00005820: 2020 2020 2020 2020 2020 3e3e 3e20 696d            >>> im
+00005830: 706f 7274 206d 696e 6473 706f 7265 2e6f  port mindspore.o
+00005840: 7073 2061 7320 6f70 730a 2020 2020 2020  ps as ops.      
+00005850: 2020 2020 2020 3e3e 3e20 6d6f 6465 6c20        >>> model 
+00005860: 3d20 4665 6564 466f 7277 6172 6428 6869  = FeedForward(hi
+00005870: 6464 656e 5f73 697a 653d 3135 2c20 6666  dden_size=15, ff
+00005880: 6e5f 6869 6464 656e 5f73 697a 653d 3330  n_hidden_size=30
+00005890: 2c20 6472 6f70 6f75 745f 7261 7465 3d30  , dropout_rate=0
+000058a0: 2e31 290a 2020 2020 2020 2020 2020 2020  .1).            
+000058b0: 3e3e 3e20 7465 6e73 6f72 203d 2054 656e  >>> tensor = Ten
+000058c0: 736f 7228 6e70 2e6f 6e65 7328 2832 2c20  sor(np.ones((2, 
+000058d0: 3230 2c20 3135 2929 2c20 6d73 7479 7065  20, 15)), mstype
+000058e0: 2e66 6c6f 6174 3332 290a 2020 2020 2020  .float32).      
+000058f0: 2020 2020 2020 3e3e 3e20 6f75 7470 7574        >>> output
+00005900: 203d 206d 6f64 656c 2874 656e 736f 7229   = model(tensor)
+00005910: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
+00005920: 2070 7269 6e74 286f 7574 7075 742e 7368   print(output.sh
+00005930: 6170 6529 0a20 2020 2020 2020 2020 2020  ape).           
+00005940: 2028 322c 2032 302c 2031 3529 0a20 2020   (2, 20, 15).   
+00005950: 2020 2020 2020 2020 203e 3e3e 2023 2045           >>> # E
+00005960: 7861 6d70 6c65 2032 2075 7369 6e67 2063  xample 2 using c
+00005970: 7573 746f 6d20 6869 6464 656e 2061 6374  ustom hidden act
+00005980: 6976 6174 696f 6e0a 2020 2020 2020 2020  ivation.        
+00005990: 2020 2020 3e3e 3e20 636c 6173 7320 4d79      >>> class My
+000059a0: 4163 7469 7661 7469 6f6e 4e6f 5368 6172  ActivationNoShar
+000059b0: 6428 6e6e 2e43 656c 6c29 3a0a 2020 2020  d(nn.Cell):.    
+000059c0: 2020 2020 2020 2020 2e2e 2e20 2020 2020          ...     
+000059d0: 6465 6620 5f5f 696e 6974 5f5f 2873 656c  def __init__(sel
+000059e0: 6629 3a0a 2020 2020 2020 2020 2020 2020  f):.            
+000059f0: 2e2e 2e20 2020 2020 2020 2020 7375 7065  ...         supe
+00005a00: 7228 4d79 4163 7469 7661 7469 6f6e 4e6f  r(MyActivationNo
+00005a10: 5368 6172 642c 2073 656c 6629 2e5f 5f69  Shard, self).__i
+00005a20: 6e69 745f 5f28 290a 2020 2020 2020 2020  nit__().        
+00005a30: 2020 2020 2e2e 2e20 2020 2020 2020 2020      ...         
+00005a40: 7365 6c66 2e61 6464 203d 206f 7073 2e41  self.add = ops.A
+00005a50: 6464 2829 0a20 2020 2020 2020 2020 2020  dd().           
+00005a60: 202e 2e2e 2020 2020 2064 6566 2063 6f6e   ...     def con
+00005a70: 7374 7275 6374 2873 656c 662c 2078 293a  struct(self, x):
+00005a80: 0a20 2020 2020 2020 2020 2020 202e 2e2e  .            ...
+00005a90: 2020 2020 2020 2020 2072 6574 7572 6e20           return 
+00005aa0: 7365 6c66 2e61 6464 2878 2c20 302e 3129  self.add(x, 0.1)
+00005ab0: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
+00005ac0: 206d 6f64 656c 203d 2046 6565 6446 6f72   model = FeedFor
+00005ad0: 7761 7264 2868 6964 6465 6e5f 7369 7a65  ward(hidden_size
+00005ae0: 3d31 352c 2066 666e 5f68 6964 6465 6e5f  =15, ffn_hidden_
+00005af0: 7369 7a65 3d33 302c 2064 726f 706f 7574  size=30, dropout
+00005b00: 5f72 6174 653d 302e 312c 0a20 2020 2020  _rate=0.1,.     
+00005b10: 2020 2020 2020 202e 2e2e 2020 2020 2020         ...      
+00005b20: 2020 2020 2020 2020 2020 2020 2020 2068                 h
+00005b30: 6964 6465 6e5f 6163 743d 4d79 4163 7469  idden_act=MyActi
+00005b40: 7661 7469 6f6e 4e6f 5368 6172 6429 0a20  vationNoShard). 
+00005b50: 2020 2020 2020 2020 2020 203e 3e3e 2074             >>> t
+00005b60: 656e 736f 7220 3d20 5465 6e73 6f72 286e  ensor = Tensor(n
+00005b70: 702e 6f6e 6573 2828 322c 2032 302c 2031  p.ones((2, 20, 1
+00005b80: 3529 292c 206d 7374 7970 652e 666c 6f61  5)), mstype.floa
+00005b90: 7433 3229 0a20 2020 2020 2020 2020 2020  t32).           
+00005ba0: 203e 3e3e 206f 7574 7075 7420 3d20 6d6f   >>> output = mo
+00005bb0: 6465 6c28 7465 6e73 6f72 290a 2020 2020  del(tensor).    
+00005bc0: 2020 2020 2020 2020 3e3e 3e20 7072 696e          >>> prin
+00005bd0: 7428 6f75 7470 7574 2e73 6861 7065 290a  t(output.shape).
+00005be0: 2020 2020 2020 2020 2020 2020 2832 2c20              (2, 
+00005bf0: 3230 2c20 3135 290a 2020 2020 2020 2020  20, 15).        
+00005c00: 2020 2020 3e3e 3e20 2320 4578 616d 706c      >>> # Exampl
+00005c10: 6520 3320 7573 696e 6720 6375 7374 6f6d  e 3 using custom
+00005c20: 2068 6964 6465 6e20 6163 7469 7661 7469   hidden activati
+00005c30: 6f6e 2077 6974 6820 6163 7469 7661 7469  on with activati
+00005c40: 6f6e 5f73 6861 7264 0a20 2020 2020 2020  on_shard.       
+00005c50: 2020 2020 203e 3e3e 2023 2049 6620 7573       >>> # If us
+00005c60: 6572 2077 616e 7473 7320 746f 2072 756e  er wantss to run
+00005c70: 206f 6e20 7468 6520 5345 4d49 2f41 5554   on the SEMI/AUT
+00005c80: 4f20 7061 7261 6c6c 656c 206d 6f64 652c  O parallel mode,
+00005c90: 2074 6865 2063 7573 746f 6d20 6163 7469   the custom acti
+00005ca0: 7661 7469 6f6e 206d 7573 7420 7072 6f76  vation must prov
+00005cb0: 6964 650a 2020 2020 2020 2020 2020 2020  ide.            
+00005cc0: 3e3e 3e20 2320 6120 636c 6173 7320 6675  >>> # a class fu
+00005cd0: 6e63 7469 6f6e 206e 616d 6564 2061 6374  nction named act
+00005ce0: 6976 6174 696f 6e5f 7368 6172 642e 2049  ivation_shard. I
+00005cf0: 7420 6163 6365 7074 7320 7468 6520 6172  t accepts the ar
+00005d00: 6775 6d65 6e74 2070 6172 616c 6c65 6c5f  gument parallel_
+00005d10: 636f 6e66 6967 2028 4f70 5061 7261 6c6c  config (OpParall
+00005d20: 656c 436f 6e66 6967 2c0a 2020 2020 2020  elConfig,.      
+00005d30: 2020 2020 2020 3e3e 3e20 2320 4d6f 4550        >>> # MoEP
+00005d40: 6172 616c 6c65 6c43 6f6e 6669 6729 2061  arallelConfig) a
+00005d50: 6e64 2073 6574 2074 6865 2073 6861 7264  nd set the shard
+00005d60: 2066 6f72 2074 6865 2070 7269 6d69 7469   for the primiti
+00005d70: 7665 7320 7573 6564 2069 6e20 7468 6520  ves used in the 
+00005d80: 636f 6e73 7472 7563 742e 0a20 2020 2020  construct..     
+00005d90: 2020 2020 2020 203e 3e3e 2063 6c61 7373         >>> class
+00005da0: 204d 7941 6374 6976 6174 696f 6e57 6974   MyActivationWit
+00005db0: 6853 6861 7264 286e 6e2e 4365 6c6c 293a  hShard(nn.Cell):
+00005dc0: 0a20 2020 2020 2020 2020 2020 202e 2e2e  .            ...
+00005dd0: 2020 2020 2064 6566 205f 5f69 6e69 745f       def __init_
+00005de0: 5f28 7365 6c66 293a 0a20 2020 2020 2020  _(self):.       
+00005df0: 2020 2020 202e 2e2e 2020 2020 2020 2020       ...        
+00005e00: 2073 7570 6572 284d 7941 6374 6976 6174   super(MyActivat
+00005e10: 696f 6e57 6974 6853 6861 7264 2c20 7365  ionWithShard, se
+00005e20: 6c66 292e 5f5f 696e 6974 5f5f 2829 0a20  lf).__init__(). 
+00005e30: 2020 2020 2020 2020 2020 202e 2e2e 2020             ...  
+00005e40: 2020 2020 2020 2073 656c 662e 6164 6420         self.add 
+00005e50: 3d20 6f70 732e 4164 6428 290a 2020 2020  = ops.Add().    
+00005e60: 2020 2020 2020 2020 2e2e 2e20 2020 2020          ...     
+00005e70: 6465 6620 636f 6e73 7472 7563 7428 7365  def construct(se
+00005e80: 6c66 2c20 7829 3a0a 2020 2020 2020 2020  lf, x):.        
+00005e90: 2020 2020 2e2e 2e20 2020 2020 2020 2020      ...         
+00005ea0: 7265 7475 726e 2073 656c 662e 6164 6428  return self.add(
+00005eb0: 782c 2030 2e31 290a 2020 2020 2020 2020  x, 0.1).        
+00005ec0: 2020 2020 2e2e 2e20 2020 2020 6465 6620      ...     def 
+00005ed0: 6163 7469 7661 7469 6f6e 5f73 6861 7264  activation_shard
+00005ee0: 2873 656c 662c 2070 6172 616c 6c65 6c5f  (self, parallel_
+00005ef0: 636f 6e66 6967 293a 0a20 2020 2020 2020  config):.       
+00005f00: 2020 2020 202e 2e2e 2020 2020 2020 2020       ...        
+00005f10: 2073 656c 662e 6164 642e 7368 6172 6428   self.add.shard(
+00005f20: 2828 7061 7261 6c6c 656c 5f63 6f6e 6669  ((parallel_confi
+00005f30: 672e 6461 7461 5f70 6172 616c 6c65 6c2c  g.data_parallel,
+00005f40: 2070 6172 616c 6c65 6c5f 636f 6e66 6967   parallel_config
+00005f50: 2e6d 6f64 656c 5f70 6172 616c 6c65 6c29  .model_parallel)
+00005f60: 2c20 2829 2929 0a20 2020 2020 2020 2020  , ())).         
+00005f70: 2020 203e 3e3e 0a20 2020 2020 2020 2020     >>>.         
+00005f80: 2020 203e 3e3e 206d 6f64 656c 203d 2046     >>> model = F
+00005f90: 6565 6446 6f72 7761 7264 2868 6964 6465  eedForward(hidde
+00005fa0: 6e5f 7369 7a65 3d31 352c 2066 666e 5f68  n_size=15, ffn_h
+00005fb0: 6964 6465 6e5f 7369 7a65 3d33 302c 2064  idden_size=30, d
+00005fc0: 726f 706f 7574 5f72 6174 653d 302e 312c  ropout_rate=0.1,
+00005fd0: 0a20 2020 2020 2020 2020 2020 202e 2e2e  .            ...
+00005fe0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00005ff0: 2020 2020 2068 6964 6465 6e5f 6163 743d       hidden_act=
+00006000: 4d79 4163 7469 7661 7469 6f6e 5769 7468  MyActivationWith
+00006010: 5368 6172 6429 0a20 2020 2020 2020 2020  Shard).         
+00006020: 2020 203e 3e3e 2074 656e 736f 7220 3d20     >>> tensor = 
+00006030: 5465 6e73 6f72 286e 702e 6f6e 6573 2828  Tensor(np.ones((
+00006040: 322c 2032 302c 2031 3529 292c 206d 7374  2, 20, 15)), mst
+00006050: 7970 652e 666c 6f61 7433 3229 0a20 2020  ype.float32).   
+00006060: 2020 2020 2020 2020 203e 3e3e 206f 7574           >>> out
+00006070: 7075 7420 3d20 6d6f 6465 6c28 7465 6e73  put = model(tens
+00006080: 6f72 290a 2020 2020 2020 2020 2020 2020  or).            
+00006090: 3e3e 3e20 7072 696e 7428 6f75 7470 7574  >>> print(output
+000060a0: 2e73 6861 7065 290a 2020 2020 2020 2020  .shape).        
+000060b0: 2020 2020 2832 2c20 3230 2c20 3135 290a      (2, 20, 15).
+000060c0: 2020 2020 2222 220a 0a20 2020 2040 5f4c      """..    @_L
+000060d0: 6f67 4163 7469 6f6e 4f6e 6365 286d 5f6c  ogActionOnce(m_l
+000060e0: 6f67 6765 723d 6c6f 6767 6572 2c20 6b65  ogger=logger, ke
+000060f0: 793d 2746 6565 6446 6f72 7761 7264 272c  y='FeedForward',
+00006100: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00006110: 2020 2020 206e 6f5f 7761 726e 696e 673d       no_warning=
+00006120: 5f67 6574 5f70 6172 616c 6c65 6c5f 6d6f  _get_parallel_mo
+00006130: 6465 2829 2069 6e20 2850 6172 616c 6c65  de() in (Paralle
+00006140: 6c4d 6f64 652e 5354 414e 445f 414c 4f4e  lMode.STAND_ALON
+00006150: 452c 2929 0a20 2020 2040 5f61 7267 735f  E,)).    @_args_
+00006160: 7479 7065 5f76 616c 6964 6174 6f72 5f63  type_validator_c
+00006170: 6865 636b 2868 6964 6465 6e5f 7369 7a65  heck(hidden_size
+00006180: 3d56 616c 6964 6174 6f72 2e63 6865 636b  =Validator.check
+00006190: 5f70 6f73 6974 6976 655f 696e 742c 0a20  _positive_int,. 
+000061a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000061b0: 2020 2020 2020 2020 2020 2020 2020 2066                 f
+000061c0: 666e 5f68 6964 6465 6e5f 7369 7a65 3d56  fn_hidden_size=V
+000061d0: 616c 6964 6174 6f72 2e63 6865 636b 5f70  alidator.check_p
+000061e0: 6f73 6974 6976 655f 696e 742c 0a20 2020  ositive_int,.   
+000061f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00006200: 2020 2020 2020 2020 2020 2020 2064 726f               dro
+00006210: 706f 7574 5f72 6174 653d 5661 6c69 6461  pout_rate=Valida
+00006220: 746f 722e 6368 6563 6b5f 6e6f 6e5f 6e65  tor.check_non_ne
+00006230: 6761 7469 7665 5f66 6c6f 6174 2c0a 2020  gative_float,.  
+00006240: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00006250: 2020 2020 2020 2020 2020 2020 2020 7061                pa
+00006260: 7261 6d5f 696e 6974 5f74 7970 653d 5f76  ram_init_type=_v
+00006270: 616c 6964 5f76 616c 7565 5f63 6865 636b  alid_value_check
+00006280: 7328 5b6d 7374 7970 652e 666c 6f61 7433  s([mstype.float3
+00006290: 322c 206d 7374 7970 652e 6266 6c6f 6174  2, mstype.bfloat
+000062a0: 3136 2c20 6d73 7479 7065 2e66 6c6f 6174  16, mstype.float
+000062b0: 3136 5d2c 0a20 2020 2020 2020 2020 2020  16],.           
+000062c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 000062d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000062e0: 2020 2070 6172 616d 5f69 6e69 745f 7479     param_init_ty
-000062f0: 7065 3d5f 7661 6c69 645f 7661 6c75 655f  pe=_valid_value_
-00006300: 6368 6563 6b73 285b 6d73 7479 7065 2e66  checks([mstype.f
-00006310: 6c6f 6174 3332 2c20 6d73 7479 7065 2e62  loat32, mstype.b
-00006320: 666c 6f61 7431 362c 206d 7374 7970 652e  float16, mstype.
-00006330: 666c 6f61 7431 365d 2c0a 2020 2020 2020  float16],.      
-00006340: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00006350: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00006360: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00006370: 2020 2020 2020 2020 2020 2020 2020 2246                "F
-00006380: 6565 6446 6f72 7761 7264 2229 2c0a 2020  eedForward"),.  
+000062e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000062f0: 2020 2020 2020 2020 2022 4665 6564 466f           "FeedFo
+00006300: 7277 6172 6422 292c 0a20 2020 2020 2020  rward"),.       
+00006310: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00006320: 2020 2020 2020 2020 2070 6172 616c 6c65           paralle
+00006330: 6c5f 636f 6e66 6967 3d5f 7661 6c69 645f  l_config=_valid_
+00006340: 7479 7065 5f63 6865 636b 7328 5b4f 7050  type_checks([OpP
+00006350: 6172 616c 6c65 6c43 6f6e 6669 672c 204d  arallelConfig, M
+00006360: 6f45 5061 7261 6c6c 656c 436f 6e66 6967  oEParallelConfig
+00006370: 5d2c 0a20 2020 2020 2020 2020 2020 2020  ],.             
+00006380: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00006390: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000063a0: 2020 2020 2020 2020 2020 2020 2020 636f                co
-000063b0: 6d70 7574 655f 6474 7970 653d 5f76 616c  mpute_dtype=_val
-000063c0: 6964 5f76 616c 7565 5f63 6865 636b 7328  id_value_checks(
-000063d0: 5b6d 7374 7970 652e 666c 6f61 7433 322c  [mstype.float32,
-000063e0: 206d 7374 7970 652e 6266 6c6f 6174 3136   mstype.bfloat16
-000063f0: 2c20 6d73 7479 7065 2e66 6c6f 6174 3136  , mstype.float16
-00006400: 5d2c 0a20 2020 2020 2020 2020 2020 2020  ],.             
-00006410: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00006420: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00006430: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00006440: 2020 2020 2022 4665 6564 466f 7277 6172       "FeedForwar
-00006450: 6422 292c 0a20 2020 2020 2020 2020 2020  d"),.           
-00006460: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00006470: 2020 2020 2070 6172 616c 6c65 6c5f 636f       parallel_co
-00006480: 6e66 6967 3d5f 7661 6c69 645f 7479 7065  nfig=_valid_type
-00006490: 5f63 6865 636b 7328 5b4f 7050 6172 616c  _checks([OpParal
-000064a0: 6c65 6c43 6f6e 6669 672c 204d 6f45 5061  lelConfig, MoEPa
-000064b0: 7261 6c6c 656c 436f 6e66 6967 5d2c 0a20  rallelConfig],. 
-000064c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000064d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000064e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000064f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00006500: 2020 2246 6565 6446 6f72 7761 7264 2229    "FeedForward")
-00006510: 290a 2020 2020 6465 6620 5f5f 696e 6974  ).    def __init
-00006520: 5f5f 2873 656c 662c 2068 6964 6465 6e5f  __(self, hidden_
-00006530: 7369 7a65 2c0a 2020 2020 2020 2020 2020  size,.          
-00006540: 2020 2020 2020 2066 666e 5f68 6964 6465         ffn_hidde
-00006550: 6e5f 7369 7a65 2c0a 2020 2020 2020 2020  n_size,.        
-00006560: 2020 2020 2020 2020 2064 726f 706f 7574           dropout
-00006570: 5f72 6174 652c 0a20 2020 2020 2020 2020  _rate,.         
-00006580: 2020 2020 2020 2020 6869 6464 656e 5f61          hidden_a
-00006590: 6374 3d27 6765 6c75 272c 0a20 2020 2020  ct='gelu',.     
-000065a0: 2020 2020 2020 2020 2020 2020 6578 7065              expe
-000065b0: 7274 5f6e 756d 3d31 2c0a 2020 2020 2020  rt_num=1,.      
-000065c0: 2020 2020 2020 2020 2020 2065 7870 6572             exper
-000065d0: 745f 6772 6f75 705f 7369 7a65 3d4e 6f6e  t_group_size=Non
-000065e0: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
-000065f0: 2020 2020 7061 7261 6d5f 696e 6974 5f74      param_init_t
-00006600: 7970 653d 6d73 7479 7065 2e66 6c6f 6174  ype=mstype.float
-00006610: 3332 2c0a 2020 2020 2020 2020 2020 2020  32,.            
-00006620: 2020 2020 2070 6172 616c 6c65 6c5f 636f       parallel_co
-00006630: 6e66 6967 3d64 6566 6175 6c74 5f64 706d  nfig=default_dpm
-00006640: 705f 636f 6e66 6967 2c0a 2020 2020 2020  p_config,.      
-00006650: 2020 2020 2020 2020 2020 2063 6f6d 7075             compu
-00006660: 7465 5f64 7479 7065 3d6d 7374 7970 652e  te_dtype=mstype.
-00006670: 666c 6f61 7431 3629 3a0a 2020 2020 2020  float16):.      
-00006680: 2020 7375 7065 7228 4665 6564 466f 7277    super(FeedForw
-00006690: 6172 642c 2073 656c 6629 2e5f 5f69 6e69  ard, self).__ini
-000066a0: 745f 5f28 290a 2020 2020 2020 2020 7365  t__().        se
-000066b0: 6c66 2e64 7479 7065 203d 2063 6f6d 7075  lf.dtype = compu
-000066c0: 7465 5f64 7479 7065 0a20 2020 2020 2020  te_dtype.       
-000066d0: 2069 6620 6869 6464 656e 5f61 6374 2069   if hidden_act i
-000066e0: 7320 4e6f 6e65 206f 7220 6e6f 7420 2869  s None or not (i
-000066f0: 7369 6e73 7461 6e63 6528 6869 6464 656e  sinstance(hidden
-00006700: 5f61 6374 2c20 7374 7229 206f 7220 6973  _act, str) or is
-00006710: 7375 6263 6c61 7373 2868 6964 6465 6e5f  subclass(hidden_
-00006720: 6163 742c 206e 6e2e 4365 6c6c 2929 3a0a  act, nn.Cell)):.
-00006730: 2020 2020 2020 2020 2020 2020 7261 6973              rais
-00006740: 6520 5479 7065 4572 726f 7228 6622 466f  e TypeError(f"Fo
-00006750: 7220 4665 6564 466f 7277 6172 6420 6365  r FeedForward ce
-00006760: 6c6c 2c20 7468 6520 6869 6464 656e 5f61  ll, the hidden_a
-00006770: 6374 2073 686f 756c 6420 7374 7220 7479  ct should str ty
-00006780: 7065 206f 7220 6e6e 2e43 656c 6c20 7479  pe or nn.Cell ty
-00006790: 7065 2c20 220a 2020 2020 2020 2020 2020  pe, ".          
-000067a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000067b0: 2020 6622 6275 7420 676f 7420 7b68 6964    f"but got {hid
-000067c0: 6465 6e5f 6163 747d 2e22 290a 2020 2020  den_act}.").    
-000067d0: 2020 2020 6966 205f 6765 745f 7061 7261      if _get_para
-000067e0: 6c6c 656c 5f6d 6f64 6528 2920 696e 2028  llel_mode() in (
-000067f0: 5061 7261 6c6c 656c 4d6f 6465 2e41 5554  ParallelMode.AUT
-00006800: 4f5f 5041 5241 4c4c 454c 2c29 3a0a 2020  O_PARALLEL,):.  
-00006810: 2020 2020 2020 2020 2020 5f63 6865 636b            _check
-00006820: 5f63 6f6e 6669 6728 7061 7261 6c6c 656c  _config(parallel
-00006830: 5f63 6f6e 6669 6729 0a20 2020 2020 2020  _config).       
-00006840: 2020 2020 206d 7020 3d20 7061 7261 6c6c       mp = parall
-00006850: 656c 5f63 6f6e 6669 672e 6d6f 6465 6c5f  el_config.model_
-00006860: 7061 7261 6c6c 656c 0a20 2020 2020 2020  parallel.       
-00006870: 2020 2020 2069 6620 6578 7065 7274 5f6e       if expert_n
-00006880: 756d 203e 2031 3a0a 2020 2020 2020 2020  um > 1:.        
-00006890: 2020 2020 2020 2020 6570 203d 2070 6172          ep = par
-000068a0: 616c 6c65 6c5f 636f 6e66 6967 2e65 7870  allel_config.exp
-000068b0: 6572 745f 7061 7261 6c6c 656c 0a20 2020  ert_parallel.   
-000068c0: 2020 2020 2020 2020 2065 6c73 653a 0a20           else:. 
-000068d0: 2020 2020 2020 2020 2020 2020 2020 2065                 e
-000068e0: 7020 3d20 310a 2020 2020 2020 2020 2020  p = 1.          
-000068f0: 2020 2320 6666 6e20 7573 6520 6c65 7373    # ffn use less
-00006900: 2064 7020 7468 616e 206f 7468 6572 206f   dp than other o
-00006910: 7073 2077 6865 6e20 7573 655f 6d6f 652c  ps when use_moe,
-00006920: 2064 7565 2074 6f20 7468 6572 6520 6172   due to there ar
-00006930: 6520 6f70 7320 7573 6520 6470 2061 6e64  e ops use dp and
-00006940: 2065 702e 0a20 2020 2020 2020 2020 2020   ep..           
-00006950: 2064 7020 3d20 7061 7261 6c6c 656c 5f63   dp = parallel_c
-00006960: 6f6e 6669 672e 6461 7461 5f70 6172 616c  onfig.data_paral
-00006970: 6c65 6c20 2f2f 2065 700a 2020 2020 2020  lel // ep.      
-00006980: 2020 2020 2020 6966 2066 666e 5f68 6964        if ffn_hid
-00006990: 6465 6e5f 7369 7a65 2025 206d 7020 213d  den_size % mp !=
-000069a0: 2030 3a0a 2020 2020 2020 2020 2020 2020   0:.            
-000069b0: 2020 2020 7261 6973 6520 5661 6c75 6545      raise ValueE
-000069c0: 7272 6f72 2822 466f 7220 2746 6565 6446  rror("For 'FeedF
-000069d0: 6f72 7761 7264 272c 2074 6865 2063 6c61  orward', the cla
-000069e0: 7373 2076 6172 6961 626c 6520 2766 666e  ss variable 'ffn
-000069f0: 5f68 6964 6465 6e5f 7369 7a65 2720 6d75  _hidden_size' mu
-00006a00: 7374 2062 6520 6120 6d75 6c74 6970 6c65  st be a multiple
-00006a10: 206f 6620 7468 6522 0a20 2020 2020 2020   of the".       
-00006a20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00006a30: 2020 2020 2020 2020 2020 226e 756d 206f            "num o
-00006a40: 6620 6d6f 6465 6c20 7061 7261 6c6c 656c  f model parallel
-00006a50: 2c20 6275 7420 676f 7420 7468 6520 6666  , but got the ff
-00006a60: 6e5f 6869 6464 656e 5f73 697a 6520 6973  n_hidden_size is
-00006a70: 207b 7d20 616e 6420 7468 6520 6e75 6d20   {} and the num 
-00006a80: 6f66 206d 6f64 656c 2022 0a20 2020 2020  of model ".     
-00006a90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00006aa0: 2020 2020 2020 2020 2020 2020 2270 6172              "par
-00006ab0: 616c 6c65 6c20 6973 207b 7d2e 222e 666f  allel is {}.".fo
-00006ac0: 726d 6174 2866 666e 5f68 6964 6465 6e5f  rmat(ffn_hidden_
-00006ad0: 7369 7a65 2c20 6d70 2929 0a20 2020 2020  size, mp)).     
-00006ae0: 2020 2020 2020 2069 6620 6869 6464 656e         if hidden
-00006af0: 5f73 697a 6520 2520 6d70 2021 3d20 303a  _size % mp != 0:
-00006b00: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00006b10: 2072 6169 7365 2056 616c 7565 4572 726f   raise ValueErro
-00006b20: 7228 2246 6f72 2027 4665 6564 466f 7277  r("For 'FeedForw
-00006b30: 6172 6427 2c20 7468 6520 636c 6173 7320  ard', the class 
-00006b40: 7661 7269 6162 6c65 2027 6869 6464 656e  variable 'hidden
-00006b50: 5f73 697a 6527 206d 7573 7420 6265 2061  _size' must be a
-00006b60: 206d 756c 7469 706c 6520 6f66 2074 6865   multiple of the
-00006b70: 206e 756d 206f 6620 220a 2020 2020 2020   num of ".      
-00006b80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00006b90: 2020 2020 2020 2020 2020 2022 6d6f 6465             "mode
-00006ba0: 6c20 7061 7261 6c6c 656c 2c20 6275 7420  l parallel, but 
-00006bb0: 676f 7420 7468 6520 6869 6464 656e 5f73  got the hidden_s
-00006bc0: 697a 6520 6973 207b 7d20 616e 6420 7468  ize is {} and th
-00006bd0: 6520 6e75 6d20 6f66 206d 6f64 656c 2070  e num of model p
-00006be0: 6172 616c 6c65 6c20 6973 207b 7d2e 220a  arallel is {}.".
-00006bf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00006c00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00006c10: 202e 666f 726d 6174 2868 6964 6465 6e5f   .format(hidden_
-00006c20: 7369 7a65 2c20 6d70 2929 0a20 2020 2020  size, mp)).     
-00006c30: 2020 2020 2020 2069 6620 6472 6f70 6f75         if dropou
-00006c40: 745f 7261 7465 203c 2030 206f 7220 6472  t_rate < 0 or dr
-00006c50: 6f70 6f75 745f 7261 7465 203e 3d20 313a  opout_rate >= 1:
-00006c60: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00006c70: 2072 6169 7365 2056 616c 7565 4572 726f   raise ValueErro
-00006c80: 7228 2246 6f72 2027 4665 6564 466f 7277  r("For 'FeedForw
-00006c90: 6172 6427 2c20 7468 6520 636c 6173 7320  ard', the class 
-00006ca0: 7661 7269 6162 6c65 2027 6472 6f70 6f75  variable 'dropou
-00006cb0: 745f 7261 7465 2720 6d75 7374 2062 6520  t_rate' must be 
-00006cc0: 696e 2074 6865 2072 616e 6765 205b 302c  in the range [0,
-00006cd0: 2031 2e30 292c 2022 0a20 2020 2020 2020   1.0), ".       
-00006ce0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00006cf0: 2020 2020 2020 2020 2020 2262 7574 2067            "but g
-00006d00: 6f74 2074 6865 2076 616c 7565 203a 207b  ot the value : {
-00006d10: 7d2e 222e 666f 726d 6174 2864 726f 706f  }.".format(dropo
-00006d20: 7574 5f72 6174 6529 290a 2020 2020 2020  ut_rate)).      
-00006d30: 2020 2020 2020 696e 7075 745f 7369 7a65        input_size
-00006d40: 203d 2068 6964 6465 6e5f 7369 7a65 0a20   = hidden_size. 
-00006d50: 2020 2020 2020 2020 2020 206f 7574 7075             outpu
-00006d60: 745f 7369 7a65 203d 2066 666e 5f68 6964  t_size = ffn_hid
-00006d70: 6465 6e5f 7369 7a65 0a0a 2020 2020 2020  den_size..      
-00006d80: 2020 2020 2020 2320 5072 6f6a 6563 7420        # Project 
-00006d90: 746f 2066 666e 5f68 6964 6465 6e5f 7369  to ffn_hidden_si
-00006da0: 7a65 0a20 2020 2020 2020 2020 2020 2073  ze.            s
-00006db0: 656c 662e 6d61 7070 696e 6720 3d20 4c69  elf.mapping = Li
-00006dc0: 6e65 6172 2869 6e5f 6368 616e 6e65 6c73  near(in_channels
-00006dd0: 3d69 6e70 7574 5f73 697a 652c 0a20 2020  =input_size,.   
-00006de0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00006df0: 2020 2020 2020 2020 2020 2020 2020 206f                 o
-00006e00: 7574 5f63 6861 6e6e 656c 733d 6f75 7470  ut_channels=outp
-00006e10: 7574 5f73 697a 652c 0a20 2020 2020 2020  ut_size,.       
-00006e20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00006e30: 2020 2020 2020 2020 2020 2061 6374 6976             activ
-00006e40: 6174 696f 6e3d 6869 6464 656e 5f61 6374  ation=hidden_act
-00006e50: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00006e60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00006e70: 2020 2020 7472 616e 7370 6f73 655f 623d      transpose_b=
-00006e80: 4661 6c73 652c 0a20 2020 2020 2020 2020  False,.         
+000063a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000063b0: 2020 2020 2020 2246 6565 6446 6f72 7761        "FeedForwa
+000063c0: 7264 2229 290a 2020 2020 6465 6620 5f5f  rd")).    def __
+000063d0: 696e 6974 5f5f 2873 656c 662c 2068 6964  init__(self, hid
+000063e0: 6465 6e5f 7369 7a65 2c0a 2020 2020 2020  den_size,.      
+000063f0: 2020 2020 2020 2020 2020 2066 666e 5f68             ffn_h
+00006400: 6964 6465 6e5f 7369 7a65 2c0a 2020 2020  idden_size,.    
+00006410: 2020 2020 2020 2020 2020 2020 2064 726f               dro
+00006420: 706f 7574 5f72 6174 652c 0a20 2020 2020  pout_rate,.     
+00006430: 2020 2020 2020 2020 2020 2020 6869 6464              hidd
+00006440: 656e 5f61 6374 3d27 6765 6c75 272c 0a20  en_act='gelu',. 
+00006450: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00006460: 6578 7065 7274 5f6e 756d 3d31 2c0a 2020  expert_num=1,.  
+00006470: 2020 2020 2020 2020 2020 2020 2020 2065                 e
+00006480: 7870 6572 745f 6772 6f75 705f 7369 7a65  xpert_group_size
+00006490: 3d4e 6f6e 652c 0a20 2020 2020 2020 2020  =None,.         
+000064a0: 2020 2020 2020 2020 7061 7261 6d5f 696e          param_in
+000064b0: 6974 5f74 7970 653d 6d73 7479 7065 2e66  it_type=mstype.f
+000064c0: 6c6f 6174 3332 2c0a 2020 2020 2020 2020  loat32,.        
+000064d0: 2020 2020 2020 2020 2070 6172 616c 6c65           paralle
+000064e0: 6c5f 636f 6e66 6967 3d64 6566 6175 6c74  l_config=default
+000064f0: 5f64 706d 705f 636f 6e66 6967 293a 0a20  _dpmp_config):. 
+00006500: 2020 2020 2020 2073 7570 6572 2846 6565         super(Fee
+00006510: 6446 6f72 7761 7264 2c20 7365 6c66 292e  dForward, self).
+00006520: 5f5f 696e 6974 5f5f 2829 0a20 2020 2020  __init__().     
+00006530: 2020 2069 6620 6869 6464 656e 5f61 6374     if hidden_act
+00006540: 2069 7320 4e6f 6e65 206f 7220 6e6f 7420   is None or not 
+00006550: 2869 7369 6e73 7461 6e63 6528 6869 6464  (isinstance(hidd
+00006560: 656e 5f61 6374 2c20 7374 7229 206f 7220  en_act, str) or 
+00006570: 6973 7375 6263 6c61 7373 2868 6964 6465  issubclass(hidde
+00006580: 6e5f 6163 742c 206e 6e2e 4365 6c6c 2929  n_act, nn.Cell))
+00006590: 3a0a 2020 2020 2020 2020 2020 2020 7261  :.            ra
+000065a0: 6973 6520 5479 7065 4572 726f 7228 6622  ise TypeError(f"
+000065b0: 466f 7220 4665 6564 466f 7277 6172 6420  For FeedForward 
+000065c0: 6365 6c6c 2c20 7468 6520 6869 6464 656e  cell, the hidden
+000065d0: 5f61 6374 2073 686f 756c 6420 7374 7220  _act should str 
+000065e0: 7479 7065 206f 7220 6e6e 2e43 656c 6c20  type or nn.Cell 
+000065f0: 7479 7065 2c20 220a 2020 2020 2020 2020  type, ".        
+00006600: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00006610: 2020 2020 6622 6275 7420 676f 7420 7b68      f"but got {h
+00006620: 6964 6465 6e5f 6163 747d 2e22 290a 2020  idden_act}.").  
+00006630: 2020 2020 2020 6966 205f 6765 745f 7061        if _get_pa
+00006640: 7261 6c6c 656c 5f6d 6f64 6528 2920 696e  rallel_mode() in
+00006650: 2028 5061 7261 6c6c 656c 4d6f 6465 2e41   (ParallelMode.A
+00006660: 5554 4f5f 5041 5241 4c4c 454c 2c29 3a0a  UTO_PARALLEL,):.
+00006670: 2020 2020 2020 2020 2020 2020 5f63 6865              _che
+00006680: 636b 5f63 6f6e 6669 6728 7061 7261 6c6c  ck_config(parall
+00006690: 656c 5f63 6f6e 6669 6729 0a20 2020 2020  el_config).     
+000066a0: 2020 2020 2020 206d 7020 3d20 7061 7261         mp = para
+000066b0: 6c6c 656c 5f63 6f6e 6669 672e 6d6f 6465  llel_config.mode
+000066c0: 6c5f 7061 7261 6c6c 656c 0a20 2020 2020  l_parallel.     
+000066d0: 2020 2020 2020 2069 6620 6578 7065 7274         if expert
+000066e0: 5f6e 756d 203e 2031 3a0a 2020 2020 2020  _num > 1:.      
+000066f0: 2020 2020 2020 2020 2020 6570 203d 2070            ep = p
+00006700: 6172 616c 6c65 6c5f 636f 6e66 6967 2e65  arallel_config.e
+00006710: 7870 6572 745f 7061 7261 6c6c 656c 0a20  xpert_parallel. 
+00006720: 2020 2020 2020 2020 2020 2065 6c73 653a             else:
+00006730: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00006740: 2065 7020 3d20 310a 2020 2020 2020 2020   ep = 1.        
+00006750: 2020 2020 2320 6666 6e20 7573 6520 6c65      # ffn use le
+00006760: 7373 2064 7020 7468 616e 206f 7468 6572  ss dp than other
+00006770: 206f 7073 2077 6865 6e20 7573 655f 6d6f   ops when use_mo
+00006780: 652c 2064 7565 2074 6f20 7468 6572 6520  e, due to there 
+00006790: 6172 6520 6f70 7320 7573 6520 6470 2061  are ops use dp a
+000067a0: 6e64 2065 702e 0a20 2020 2020 2020 2020  nd ep..         
+000067b0: 2020 2064 7020 3d20 7061 7261 6c6c 656c     dp = parallel
+000067c0: 5f63 6f6e 6669 672e 6461 7461 5f70 6172  _config.data_par
+000067d0: 616c 6c65 6c20 2f2f 2065 700a 2020 2020  allel // ep.    
+000067e0: 2020 2020 2020 2020 6966 2066 666e 5f68          if ffn_h
+000067f0: 6964 6465 6e5f 7369 7a65 2025 206d 7020  idden_size % mp 
+00006800: 213d 2030 3a0a 2020 2020 2020 2020 2020  != 0:.          
+00006810: 2020 2020 2020 7261 6973 6520 5661 6c75        raise Valu
+00006820: 6545 7272 6f72 2822 466f 7220 2746 6565  eError("For 'Fee
+00006830: 6446 6f72 7761 7264 272c 2074 6865 2063  dForward', the c
+00006840: 6c61 7373 2076 6172 6961 626c 6520 2766  lass variable 'f
+00006850: 666e 5f68 6964 6465 6e5f 7369 7a65 2720  fn_hidden_size' 
+00006860: 6d75 7374 2062 6520 6120 6d75 6c74 6970  must be a multip
+00006870: 6c65 206f 6620 7468 6522 0a20 2020 2020  le of the".     
+00006880: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00006890: 2020 2020 2020 2020 2020 2020 226e 756d              "num
+000068a0: 206f 6620 6d6f 6465 6c20 7061 7261 6c6c   of model parall
+000068b0: 656c 2c20 6275 7420 676f 7420 7468 6520  el, but got the 
+000068c0: 6666 6e5f 6869 6464 656e 5f73 697a 6520  ffn_hidden_size 
+000068d0: 6973 207b 7d20 616e 6420 7468 6520 6e75  is {} and the nu
+000068e0: 6d20 6f66 206d 6f64 656c 2022 0a20 2020  m of model ".   
+000068f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00006900: 2020 2020 2020 2020 2020 2020 2020 2270                "p
+00006910: 6172 616c 6c65 6c20 6973 207b 7d2e 222e  arallel is {}.".
+00006920: 666f 726d 6174 2866 666e 5f68 6964 6465  format(ffn_hidde
+00006930: 6e5f 7369 7a65 2c20 6d70 2929 0a20 2020  n_size, mp)).   
+00006940: 2020 2020 2020 2020 2069 6620 6869 6464           if hidd
+00006950: 656e 5f73 697a 6520 2520 6d70 2021 3d20  en_size % mp != 
+00006960: 303a 0a20 2020 2020 2020 2020 2020 2020  0:.             
+00006970: 2020 2072 6169 7365 2056 616c 7565 4572     raise ValueEr
+00006980: 726f 7228 2246 6f72 2027 4665 6564 466f  ror("For 'FeedFo
+00006990: 7277 6172 6427 2c20 7468 6520 636c 6173  rward', the clas
+000069a0: 7320 7661 7269 6162 6c65 2027 6869 6464  s variable 'hidd
+000069b0: 656e 5f73 697a 6527 206d 7573 7420 6265  en_size' must be
+000069c0: 2061 206d 756c 7469 706c 6520 6f66 2074   a multiple of t
+000069d0: 6865 206e 756d 206f 6620 220a 2020 2020  he num of ".    
+000069e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000069f0: 2020 2020 2020 2020 2020 2020 2022 6d6f               "mo
+00006a00: 6465 6c20 7061 7261 6c6c 656c 2c20 6275  del parallel, bu
+00006a10: 7420 676f 7420 7468 6520 6869 6464 656e  t got the hidden
+00006a20: 5f73 697a 6520 6973 207b 7d20 616e 6420  _size is {} and 
+00006a30: 7468 6520 6e75 6d20 6f66 206d 6f64 656c  the num of model
+00006a40: 2070 6172 616c 6c65 6c20 6973 207b 7d2e   parallel is {}.
+00006a50: 220a 2020 2020 2020 2020 2020 2020 2020  ".              
+00006a60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00006a70: 2020 202e 666f 726d 6174 2868 6964 6465     .format(hidde
+00006a80: 6e5f 7369 7a65 2c20 6d70 2929 0a20 2020  n_size, mp)).   
+00006a90: 2020 2020 2020 2020 2069 6620 6472 6f70           if drop
+00006aa0: 6f75 745f 7261 7465 203c 2030 206f 7220  out_rate < 0 or 
+00006ab0: 6472 6f70 6f75 745f 7261 7465 203e 3d20  dropout_rate >= 
+00006ac0: 313a 0a20 2020 2020 2020 2020 2020 2020  1:.             
+00006ad0: 2020 2072 6169 7365 2056 616c 7565 4572     raise ValueEr
+00006ae0: 726f 7228 2246 6f72 2027 4665 6564 466f  ror("For 'FeedFo
+00006af0: 7277 6172 6427 2c20 7468 6520 636c 6173  rward', the clas
+00006b00: 7320 7661 7269 6162 6c65 2027 6472 6f70  s variable 'drop
+00006b10: 6f75 745f 7261 7465 2720 6d75 7374 2062  out_rate' must b
+00006b20: 6520 696e 2074 6865 2072 616e 6765 205b  e in the range [
+00006b30: 302c 2031 2e30 292c 2022 0a20 2020 2020  0, 1.0), ".     
+00006b40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00006b50: 2020 2020 2020 2020 2020 2020 2262 7574              "but
+00006b60: 2067 6f74 2074 6865 2076 616c 7565 203a   got the value :
+00006b70: 207b 7d2e 222e 666f 726d 6174 2864 726f   {}.".format(dro
+00006b80: 706f 7574 5f72 6174 6529 290a 2020 2020  pout_rate)).    
+00006b90: 2020 2020 2020 2020 696e 7075 745f 7369          input_si
+00006ba0: 7a65 203d 2068 6964 6465 6e5f 7369 7a65  ze = hidden_size
+00006bb0: 0a20 2020 2020 2020 2020 2020 206f 7574  .            out
+00006bc0: 7075 745f 7369 7a65 203d 2066 666e 5f68  put_size = ffn_h
+00006bd0: 6964 6465 6e5f 7369 7a65 0a0a 2020 2020  idden_size..    
+00006be0: 2020 2020 2020 2020 2320 5072 6f6a 6563          # Projec
+00006bf0: 7420 746f 2066 666e 5f68 6964 6465 6e5f  t to ffn_hidden_
+00006c00: 7369 7a65 0a20 2020 2020 2020 2020 2020  size.           
+00006c10: 2073 656c 662e 6d61 7070 696e 6720 3d20   self.mapping = 
+00006c20: 4c69 6e65 6172 2869 6e5f 6368 616e 6e65  Linear(in_channe
+00006c30: 6c73 3d69 6e70 7574 5f73 697a 652c 0a20  ls=input_size,. 
+00006c40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00006c50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00006c60: 206f 7574 5f63 6861 6e6e 656c 733d 6f75   out_channels=ou
+00006c70: 7470 7574 5f73 697a 652c 0a20 2020 2020  tput_size,.     
+00006c80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00006c90: 2020 2020 2020 2020 2020 2020 2061 6374               act
+00006ca0: 6976 6174 696f 6e3d 6869 6464 656e 5f61  ivation=hidden_a
+00006cb0: 6374 2c0a 2020 2020 2020 2020 2020 2020  ct,.            
+00006cc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00006cd0: 2020 2020 2020 7472 616e 7370 6f73 655f        transpose_
+00006ce0: 623d 4661 6c73 652c 0a20 2020 2020 2020  b=False,.       
+00006cf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00006d00: 2020 2020 2020 2020 2020 2065 7870 6572             exper
+00006d10: 745f 6e75 6d3d 6578 7065 7274 5f6e 756d  t_num=expert_num
+00006d20: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00006d30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00006d40: 2020 2020 6578 7065 7274 5f67 726f 7570      expert_group
+00006d50: 5f73 697a 653d 6578 7065 7274 5f67 726f  _size=expert_gro
+00006d60: 7570 5f73 697a 652c 0a20 2020 2020 2020  up_size,.       
+00006d70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00006d80: 2020 2020 2020 2020 2020 206f 7574 6572             outer
+00006d90: 5f62 6174 6368 3d64 702c 0a20 2020 2020  _batch=dp,.     
+00006da0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00006db0: 2020 2020 2020 2020 2020 2020 2070 6172               par
+00006dc0: 616d 5f69 6e69 745f 7479 7065 3d70 6172  am_init_type=par
+00006dd0: 616d 5f69 6e69 745f 7479 7065 290a 0a20  am_init_type).. 
+00006de0: 2020 2020 2020 2020 2020 2023 2050 726f             # Pro
+00006df0: 6a65 6374 2062 6163 6b20 746f 2068 6964  ject back to hid
+00006e00: 6465 6e5f 7369 7a65 0a20 2020 2020 2020  den_size.       
+00006e10: 2020 2020 2073 656c 662e 7072 6f6a 6563       self.projec
+00006e20: 7469 6f6e 203d 204c 696e 6561 7228 696e  tion = Linear(in
+00006e30: 5f63 6861 6e6e 656c 733d 6f75 7470 7574  _channels=output
+00006e40: 5f73 697a 652c 0a20 2020 2020 2020 2020  _size,.         
+00006e50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00006e60: 2020 2020 2020 2020 2020 2020 6f75 745f              out_
+00006e70: 6368 616e 6e65 6c73 3d69 6e70 7574 5f73  channels=input_s
+00006e80: 697a 652c 0a20 2020 2020 2020 2020 2020  ize,.           
 00006e90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00006ea0: 2020 2020 2020 2020 2065 7870 6572 745f           expert_
-00006eb0: 6e75 6d3d 6578 7065 7274 5f6e 756d 2c0a  num=expert_num,.
+00006ea0: 2020 2020 2020 2020 2020 7472 616e 7370            transp
+00006eb0: 6f73 655f 623d 4661 6c73 652c 0a20 2020  ose_b=False,.   
 00006ec0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00006ed0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00006ee0: 2020 6578 7065 7274 5f67 726f 7570 5f73    expert_group_s
-00006ef0: 697a 653d 6578 7065 7274 5f67 726f 7570  ize=expert_group
-00006f00: 5f73 697a 652c 0a20 2020 2020 2020 2020  _size,.         
-00006f10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00006f20: 2020 2020 2020 2020 206f 7574 6572 5f62           outer_b
-00006f30: 6174 6368 3d64 702c 0a20 2020 2020 2020  atch=dp,.       
-00006f40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00006f50: 2020 2020 2020 2020 2020 2070 6172 616d             param
-00006f60: 5f69 6e69 745f 7479 7065 3d70 6172 616d  _init_type=param
-00006f70: 5f69 6e69 745f 7479 7065 2c0a 2020 2020  _init_type,.    
+00006ee0: 2020 6578 7065 7274 5f6e 756d 3d65 7870    expert_num=exp
+00006ef0: 6572 745f 6e75 6d2c 0a20 2020 2020 2020  ert_num,.       
+00006f00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00006f10: 2020 2020 2020 2020 2020 2020 2020 6578                ex
+00006f20: 7065 7274 5f67 726f 7570 5f73 697a 653d  pert_group_size=
+00006f30: 6578 7065 7274 5f67 726f 7570 5f73 697a  expert_group_siz
+00006f40: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
+00006f50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00006f60: 2020 2020 2020 2020 6f75 7465 725f 6261          outer_ba
+00006f70: 7463 683d 6470 2c0a 2020 2020 2020 2020  tch=dp,.        
 00006f80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00006f90: 2020 2020 2020 2020 2020 2020 2020 636f                co
-00006fa0: 6d70 7574 655f 6474 7970 653d 636f 6d70  mpute_dtype=comp
-00006fb0: 7574 655f 6474 7970 6529 0a0a 2020 2020  ute_dtype)..    
-00006fc0: 2020 2020 2020 2020 2320 5072 6f6a 6563          # Projec
-00006fd0: 7420 6261 636b 2074 6f20 6869 6464 656e  t back to hidden
-00006fe0: 5f73 697a 650a 2020 2020 2020 2020 2020  _size.          
-00006ff0: 2020 7365 6c66 2e70 726f 6a65 6374 696f    self.projectio
-00007000: 6e20 3d20 4c69 6e65 6172 2869 6e5f 6368  n = Linear(in_ch
-00007010: 616e 6e65 6c73 3d6f 7574 7075 745f 7369  annels=output_si
-00007020: 7a65 2c0a 2020 2020 2020 2020 2020 2020  ze,.            
-00007030: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00007040: 2020 2020 2020 2020 206f 7574 5f63 6861           out_cha
-00007050: 6e6e 656c 733d 696e 7075 745f 7369 7a65  nnels=input_size
-00007060: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00007070: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00007080: 2020 2020 2020 2074 7261 6e73 706f 7365         transpose
-00007090: 5f62 3d46 616c 7365 2c0a 2020 2020 2020  _b=False,.      
-000070a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000070b0: 2020 2020 2020 2020 2020 2020 2020 2065                 e
-000070c0: 7870 6572 745f 6e75 6d3d 6578 7065 7274  xpert_num=expert
-000070d0: 5f6e 756d 2c0a 2020 2020 2020 2020 2020  _num,.          
-000070e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000070f0: 2020 2020 2020 2020 2020 2065 7870 6572             exper
-00007100: 745f 6772 6f75 705f 7369 7a65 3d65 7870  t_group_size=exp
-00007110: 6572 745f 6772 6f75 705f 7369 7a65 2c0a  ert_group_size,.
-00007120: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00007130: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00007140: 2020 2020 206f 7574 6572 5f62 6174 6368       outer_batch
-00007150: 3d64 702c 0a20 2020 2020 2020 2020 2020  =dp,.           
-00007160: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00007170: 2020 2020 2020 2020 2020 7061 7261 6d5f            param_
-00007180: 696e 6974 5f74 7970 653d 7061 7261 6d5f  init_type=param_
-00007190: 696e 6974 5f74 7970 652c 0a20 2020 2020  init_type,.     
-000071a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000071b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000071c0: 636f 6d70 7574 655f 6474 7970 653d 636f  compute_dtype=co
-000071d0: 6d70 7574 655f 6474 7970 6529 0a20 2020  mpute_dtype).   
-000071e0: 2020 2020 2020 2020 2069 6620 6578 7065           if expe
-000071f0: 7274 5f6e 756d 203e 2031 3a0a 2020 2020  rt_num > 1:.    
-00007200: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-00007210: 2e70 726f 6a65 6374 696f 6e2e 7368 6172  .projection.shar
-00007220: 6428 7374 7261 7465 6779 5f6d 6174 6d75  d(strategy_matmu
-00007230: 6c3d 2828 6470 2c20 6570 2c20 312c 206d  l=((dp, ep, 1, m
-00007240: 7029 2c20 2865 702c 206d 702c 2031 2929  p), (ep, mp, 1))
-00007250: 290a 2020 2020 2020 2020 2020 2020 656c  ).            el
-00007260: 7365 3a0a 2020 2020 2020 2020 2020 2020  se:.            
-00007270: 2020 2020 7365 6c66 2e70 726f 6a65 6374      self.project
-00007280: 696f 6e2e 7368 6172 6428 7374 7261 7465  ion.shard(strate
-00007290: 6779 5f6d 6174 6d75 6c3d 2828 6470 2c20  gy_matmul=((dp, 
-000072a0: 6d70 292c 2028 6d70 2c20 3129 2929 0a20  mp), (mp, 1))). 
-000072b0: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-000072c0: 7072 6f6a 6563 7469 6f6e 2e62 6961 732e  projection.bias.
-000072d0: 7061 7261 6c6c 656c 5f6f 7074 696d 697a  parallel_optimiz
-000072e0: 6572 203d 2046 616c 7365 0a20 2020 2020  er = False.     
-000072f0: 2020 2020 2020 2073 656c 662e 6472 6f70         self.drop
-00007300: 6f75 7420 3d20 6765 745f 6472 6f70 6f75  out = get_dropou
-00007310: 7428 6472 6f70 6f75 745f 7261 7465 290a  t(dropout_rate).
-00007320: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-00007330: 2e64 726f 706f 7574 5f33 6420 3d20 6765  .dropout_3d = ge
-00007340: 745f 6472 6f70 6f75 7428 6472 6f70 6f75  t_dropout(dropou
-00007350: 745f 7261 7465 290a 2020 2020 2020 2020  t_rate).        
-00007360: 2020 2020 7365 6c66 2e64 726f 706f 7574      self.dropout
-00007370: 5f34 6420 3d20 6765 745f 6472 6f70 6f75  _4d = get_dropou
-00007380: 7428 6472 6f70 6f75 745f 7261 7465 290a  t(dropout_rate).
-00007390: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-000073a0: 2e63 6173 7420 3d20 502e 4361 7374 2829  .cast = P.Cast()
-000073b0: 0a20 2020 2020 2020 2065 6c73 653a 0a20  .        else:. 
-000073c0: 2020 2020 2020 2020 2020 205f 6368 6563             _chec
-000073d0: 6b5f 636f 6e66 6967 2870 6172 616c 6c65  k_config(paralle
-000073e0: 6c5f 636f 6e66 6967 290a 2020 2020 2020  l_config).      
-000073f0: 2020 2020 2020 6d70 203d 2070 6172 616c        mp = paral
-00007400: 6c65 6c5f 636f 6e66 6967 2e6d 6f64 656c  lel_config.model
-00007410: 5f70 6172 616c 6c65 6c0a 2020 2020 2020  _parallel.      
-00007420: 2020 2020 2020 6966 2065 7870 6572 745f        if expert_
-00007430: 6e75 6d20 3e20 313a 0a20 2020 2020 2020  num > 1:.       
-00007440: 2020 2020 2020 2020 2065 7020 3d20 7061           ep = pa
-00007450: 7261 6c6c 656c 5f63 6f6e 6669 672e 6578  rallel_config.ex
-00007460: 7065 7274 5f70 6172 616c 6c65 6c0a 2020  pert_parallel.  
-00007470: 2020 2020 2020 2020 2020 656c 7365 3a0a            else:.
-00007480: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00007490: 6570 203d 2031 0a20 2020 2020 2020 2020  ep = 1.         
-000074a0: 2020 2023 2066 666e 2075 7365 206c 6573     # ffn use les
-000074b0: 7320 6470 2074 6861 6e20 6f74 6865 7220  s dp than other 
-000074c0: 6f70 7320 7768 656e 2075 7365 5f6d 6f65  ops when use_moe
-000074d0: 2c20 6475 6520 746f 2074 6865 7265 2061  , due to there a
-000074e0: 7265 206f 7073 2075 7365 2064 7020 616e  re ops use dp an
-000074f0: 6420 6570 2e0a 2020 2020 2020 2020 2020  d ep..          
-00007500: 2020 6470 203d 2070 6172 616c 6c65 6c5f    dp = parallel_
-00007510: 636f 6e66 6967 2e64 6174 615f 7061 7261  config.data_para
-00007520: 6c6c 656c 202f 2f20 6570 0a20 2020 2020  llel // ep.     
-00007530: 2020 2020 2020 2069 6620 6666 6e5f 6869         if ffn_hi
-00007540: 6464 656e 5f73 697a 6520 2520 6d70 2021  dden_size % mp !
-00007550: 3d20 303a 0a20 2020 2020 2020 2020 2020  = 0:.           
-00007560: 2020 2020 2072 6169 7365 2056 616c 7565       raise Value
-00007570: 4572 726f 7228 2246 6f72 2027 4665 6564  Error("For 'Feed
-00007580: 466f 7277 6172 6427 2c20 7468 6520 636c  Forward', the cl
-00007590: 6173 7320 7661 7269 6162 6c65 2027 6666  ass variable 'ff
-000075a0: 6e5f 6869 6464 656e 5f73 697a 6527 206d  n_hidden_size' m
-000075b0: 7573 7420 6265 2061 206d 756c 7469 706c  ust be a multipl
-000075c0: 6520 6f66 2074 6865 220a 2020 2020 2020  e of the".      
-000075d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000075e0: 2020 2020 2020 2020 2020 2022 6e75 6d20             "num 
-000075f0: 6f66 206d 6f64 656c 2070 6172 616c 6c65  of model paralle
-00007600: 6c2c 2062 7574 2067 6f74 2074 6865 2066  l, but got the f
-00007610: 666e 5f68 6964 6465 6e5f 7369 7a65 2069  fn_hidden_size i
-00007620: 7320 7b7d 2061 6e64 2074 6865 206e 756d  s {} and the num
-00007630: 206f 6620 6d6f 6465 6c20 220a 2020 2020   of model ".    
-00007640: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00007650: 2020 2020 2020 2020 2020 2020 2022 7061               "pa
-00007660: 7261 6c6c 656c 2069 7320 7b7d 2e22 2e66  rallel is {}.".f
-00007670: 6f72 6d61 7428 6666 6e5f 6869 6464 656e  ormat(ffn_hidden
-00007680: 5f73 697a 652c 206d 7029 290a 2020 2020  _size, mp)).    
-00007690: 2020 2020 2020 2020 6966 2068 6964 6465          if hidde
-000076a0: 6e5f 7369 7a65 2025 206d 7020 213d 2030  n_size % mp != 0
-000076b0: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
-000076c0: 2020 7261 6973 6520 5661 6c75 6545 7272    raise ValueErr
-000076d0: 6f72 2822 466f 7220 2746 6565 6446 6f72  or("For 'FeedFor
-000076e0: 7761 7264 272c 2074 6865 2063 6c61 7373  ward', the class
-000076f0: 2076 6172 6961 626c 6520 2768 6964 6465   variable 'hidde
-00007700: 6e5f 7369 7a65 2720 6d75 7374 2062 6520  n_size' must be 
-00007710: 6120 6d75 6c74 6970 6c65 206f 6620 7468  a multiple of th
-00007720: 6520 6e75 6d20 6f66 2022 0a20 2020 2020  e num of ".     
-00007730: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00007740: 2020 2020 2020 2020 2020 2020 226d 6f64              "mod
-00007750: 656c 2070 6172 616c 6c65 6c2c 2062 7574  el parallel, but
-00007760: 2067 6f74 2074 6865 2068 6964 6465 6e5f   got the hidden_
-00007770: 7369 7a65 2069 7320 7b7d 2061 6e64 2074  size is {} and t
-00007780: 6865 206e 756d 206f 6620 6d6f 6465 6c20  he num of model 
-00007790: 7061 7261 6c6c 656c 2069 7320 7b7d 2e22  parallel is {}."
-000077a0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00006f90: 2020 2020 2020 2020 2020 2020 2070 6172               par
+00006fa0: 616d 5f69 6e69 745f 7479 7065 3d70 6172  am_init_type=par
+00006fb0: 616d 5f69 6e69 745f 7479 7065 290a 2020  am_init_type).  
+00006fc0: 2020 2020 2020 2020 2020 6966 2065 7870            if exp
+00006fd0: 6572 745f 6e75 6d20 3e20 313a 0a20 2020  ert_num > 1:.   
+00006fe0: 2020 2020 2020 2020 2020 2020 2073 656c               sel
+00006ff0: 662e 7072 6f6a 6563 7469 6f6e 2e73 6861  f.projection.sha
+00007000: 7264 2873 7472 6174 6567 795f 6d61 746d  rd(strategy_matm
+00007010: 756c 3d28 2864 702c 2065 702c 2031 2c20  ul=((dp, ep, 1, 
+00007020: 6d70 292c 2028 6570 2c20 6d70 2c20 3129  mp), (ep, mp, 1)
+00007030: 2929 0a20 2020 2020 2020 2020 2020 2065  )).            e
+00007040: 6c73 653a 0a20 2020 2020 2020 2020 2020  lse:.           
+00007050: 2020 2020 2073 656c 662e 7072 6f6a 6563       self.projec
+00007060: 7469 6f6e 2e73 6861 7264 2873 7472 6174  tion.shard(strat
+00007070: 6567 795f 6d61 746d 756c 3d28 2864 702c  egy_matmul=((dp,
+00007080: 206d 7029 2c20 286d 702c 2031 2929 290a   mp), (mp, 1))).
+00007090: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+000070a0: 2e70 726f 6a65 6374 696f 6e2e 6269 6173  .projection.bias
+000070b0: 2e70 6172 616c 6c65 6c5f 6f70 7469 6d69  .parallel_optimi
+000070c0: 7a65 7220 3d20 4661 6c73 650a 2020 2020  zer = False.    
+000070d0: 2020 2020 2020 2020 7365 6c66 2e64 726f          self.dro
+000070e0: 706f 7574 203d 2067 6574 5f64 726f 706f  pout = get_dropo
+000070f0: 7574 2864 726f 706f 7574 5f72 6174 6529  ut(dropout_rate)
+00007100: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
+00007110: 662e 6472 6f70 6f75 745f 3364 203d 2067  f.dropout_3d = g
+00007120: 6574 5f64 726f 706f 7574 2864 726f 706f  et_dropout(dropo
+00007130: 7574 5f72 6174 6529 0a20 2020 2020 2020  ut_rate).       
+00007140: 2020 2020 2073 656c 662e 6472 6f70 6f75       self.dropou
+00007150: 745f 3464 203d 2067 6574 5f64 726f 706f  t_4d = get_dropo
+00007160: 7574 2864 726f 706f 7574 5f72 6174 6529  ut(dropout_rate)
+00007170: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
+00007180: 662e 6361 7374 203d 2050 2e43 6173 7428  f.cast = P.Cast(
+00007190: 290a 2020 2020 2020 2020 656c 7365 3a0a  ).        else:.
+000071a0: 2020 2020 2020 2020 2020 2020 5f63 6865              _che
+000071b0: 636b 5f63 6f6e 6669 6728 7061 7261 6c6c  ck_config(parall
+000071c0: 656c 5f63 6f6e 6669 6729 0a20 2020 2020  el_config).     
+000071d0: 2020 2020 2020 206d 7020 3d20 7061 7261         mp = para
+000071e0: 6c6c 656c 5f63 6f6e 6669 672e 6d6f 6465  llel_config.mode
+000071f0: 6c5f 7061 7261 6c6c 656c 0a20 2020 2020  l_parallel.     
+00007200: 2020 2020 2020 2069 6620 6578 7065 7274         if expert
+00007210: 5f6e 756d 203e 2031 3a0a 2020 2020 2020  _num > 1:.      
+00007220: 2020 2020 2020 2020 2020 6570 203d 2070            ep = p
+00007230: 6172 616c 6c65 6c5f 636f 6e66 6967 2e65  arallel_config.e
+00007240: 7870 6572 745f 7061 7261 6c6c 656c 0a20  xpert_parallel. 
+00007250: 2020 2020 2020 2020 2020 2065 6c73 653a             else:
+00007260: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00007270: 2065 7020 3d20 310a 2020 2020 2020 2020   ep = 1.        
+00007280: 2020 2020 2320 6666 6e20 7573 6520 6c65      # ffn use le
+00007290: 7373 2064 7020 7468 616e 206f 7468 6572  ss dp than other
+000072a0: 206f 7073 2077 6865 6e20 7573 655f 6d6f   ops when use_mo
+000072b0: 652c 2064 7565 2074 6f20 7468 6572 6520  e, due to there 
+000072c0: 6172 6520 6f70 7320 7573 6520 6470 2061  are ops use dp a
+000072d0: 6e64 2065 702e 0a20 2020 2020 2020 2020  nd ep..         
+000072e0: 2020 2064 7020 3d20 7061 7261 6c6c 656c     dp = parallel
+000072f0: 5f63 6f6e 6669 672e 6461 7461 5f70 6172  _config.data_par
+00007300: 616c 6c65 6c20 2f2f 2065 700a 2020 2020  allel // ep.    
+00007310: 2020 2020 2020 2020 6966 2066 666e 5f68          if ffn_h
+00007320: 6964 6465 6e5f 7369 7a65 2025 206d 7020  idden_size % mp 
+00007330: 213d 2030 3a0a 2020 2020 2020 2020 2020  != 0:.          
+00007340: 2020 2020 2020 7261 6973 6520 5661 6c75        raise Valu
+00007350: 6545 7272 6f72 2822 466f 7220 2746 6565  eError("For 'Fee
+00007360: 6446 6f72 7761 7264 272c 2074 6865 2063  dForward', the c
+00007370: 6c61 7373 2076 6172 6961 626c 6520 2766  lass variable 'f
+00007380: 666e 5f68 6964 6465 6e5f 7369 7a65 2720  fn_hidden_size' 
+00007390: 6d75 7374 2062 6520 6120 6d75 6c74 6970  must be a multip
+000073a0: 6c65 206f 6620 7468 6522 0a20 2020 2020  le of the".     
+000073b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000073c0: 2020 2020 2020 2020 2020 2020 226e 756d              "num
+000073d0: 206f 6620 6d6f 6465 6c20 7061 7261 6c6c   of model parall
+000073e0: 656c 2c20 6275 7420 676f 7420 7468 6520  el, but got the 
+000073f0: 6666 6e5f 6869 6464 656e 5f73 697a 6520  ffn_hidden_size 
+00007400: 6973 207b 7d20 616e 6420 7468 6520 6e75  is {} and the nu
+00007410: 6d20 6f66 206d 6f64 656c 2022 0a20 2020  m of model ".   
+00007420: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00007430: 2020 2020 2020 2020 2020 2020 2020 2270                "p
+00007440: 6172 616c 6c65 6c20 6973 207b 7d2e 222e  arallel is {}.".
+00007450: 666f 726d 6174 2866 666e 5f68 6964 6465  format(ffn_hidde
+00007460: 6e5f 7369 7a65 2c20 6d70 2929 0a20 2020  n_size, mp)).   
+00007470: 2020 2020 2020 2020 2069 6620 6869 6464           if hidd
+00007480: 656e 5f73 697a 6520 2520 6d70 2021 3d20  en_size % mp != 
+00007490: 303a 0a20 2020 2020 2020 2020 2020 2020  0:.             
+000074a0: 2020 2072 6169 7365 2056 616c 7565 4572     raise ValueEr
+000074b0: 726f 7228 2246 6f72 2027 4665 6564 466f  ror("For 'FeedFo
+000074c0: 7277 6172 6427 2c20 7468 6520 636c 6173  rward', the clas
+000074d0: 7320 7661 7269 6162 6c65 2027 6869 6464  s variable 'hidd
+000074e0: 656e 5f73 697a 6527 206d 7573 7420 6265  en_size' must be
+000074f0: 2061 206d 756c 7469 706c 6520 6f66 2074   a multiple of t
+00007500: 6865 206e 756d 206f 6620 220a 2020 2020  he num of ".    
+00007510: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00007520: 2020 2020 2020 2020 2020 2020 2022 6d6f               "mo
+00007530: 6465 6c20 7061 7261 6c6c 656c 2c20 6275  del parallel, bu
+00007540: 7420 676f 7420 7468 6520 6869 6464 656e  t got the hidden
+00007550: 5f73 697a 6520 6973 207b 7d20 616e 6420  _size is {} and 
+00007560: 7468 6520 6e75 6d20 6f66 206d 6f64 656c  the num of model
+00007570: 2070 6172 616c 6c65 6c20 6973 207b 7d2e   parallel is {}.
+00007580: 220a 2020 2020 2020 2020 2020 2020 2020  ".              
+00007590: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000075a0: 2020 202e 666f 726d 6174 2868 6964 6465     .format(hidde
+000075b0: 6e5f 7369 7a65 2c20 6d70 2929 0a20 2020  n_size, mp)).   
+000075c0: 2020 2020 2020 2020 2069 6620 6472 6f70           if drop
+000075d0: 6f75 745f 7261 7465 203c 2030 206f 7220  out_rate < 0 or 
+000075e0: 6472 6f70 6f75 745f 7261 7465 203e 3d20  dropout_rate >= 
+000075f0: 313a 0a20 2020 2020 2020 2020 2020 2020  1:.             
+00007600: 2020 2072 6169 7365 2056 616c 7565 4572     raise ValueEr
+00007610: 726f 7228 2246 6f72 2027 4665 6564 466f  ror("For 'FeedFo
+00007620: 7277 6172 6427 2c20 7468 6520 636c 6173  rward', the clas
+00007630: 7320 7661 7269 6162 6c65 2027 6472 6f70  s variable 'drop
+00007640: 6f75 745f 7261 7465 2720 6d75 7374 2062  out_rate' must b
+00007650: 6520 696e 2074 6865 2072 616e 6765 205b  e in the range [
+00007660: 302c 2031 2e30 292c 2022 0a20 2020 2020  0, 1.0), ".     
+00007670: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00007680: 2020 2020 2020 2020 2020 2020 2262 7574              "but
+00007690: 2067 6f74 2074 6865 2076 616c 7565 203a   got the value :
+000076a0: 207b 7d2e 222e 666f 726d 6174 2864 726f   {}.".format(dro
+000076b0: 706f 7574 5f72 6174 6529 290a 2020 2020  pout_rate)).    
+000076c0: 2020 2020 2020 2020 696e 7075 745f 7369          input_si
+000076d0: 7a65 203d 2068 6964 6465 6e5f 7369 7a65  ze = hidden_size
+000076e0: 0a20 2020 2020 2020 2020 2020 206f 7574  .            out
+000076f0: 7075 745f 7369 7a65 203d 2066 666e 5f68  put_size = ffn_h
+00007700: 6964 6465 6e5f 7369 7a65 0a0a 2020 2020  idden_size..    
+00007710: 2020 2020 2020 2020 2320 5072 6f6a 6563          # Projec
+00007720: 7420 746f 2066 666e 5f68 6964 6465 6e5f  t to ffn_hidden_
+00007730: 7369 7a65 0a20 2020 2020 2020 2020 2020  size.           
+00007740: 2073 656c 662e 6d61 7070 696e 6720 3d20   self.mapping = 
+00007750: 4c69 6e65 6172 2869 6e5f 6368 616e 6e65  Linear(in_channe
+00007760: 6c73 3d69 6e70 7574 5f73 697a 652c 0a20  ls=input_size,. 
+00007770: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00007780: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00007790: 206f 7574 5f63 6861 6e6e 656c 733d 6f75   out_channels=ou
+000077a0: 7470 7574 5f73 697a 652c 0a20 2020 2020  tput_size,.     
 000077b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000077c0: 2020 2e66 6f72 6d61 7428 6869 6464 656e    .format(hidden
-000077d0: 5f73 697a 652c 206d 7029 290a 2020 2020  _size, mp)).    
-000077e0: 2020 2020 2020 2020 6966 2064 726f 706f          if dropo
-000077f0: 7574 5f72 6174 6520 3c20 3020 6f72 2064  ut_rate < 0 or d
-00007800: 726f 706f 7574 5f72 6174 6520 3e3d 2031  ropout_rate >= 1
-00007810: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
-00007820: 2020 7261 6973 6520 5661 6c75 6545 7272    raise ValueErr
-00007830: 6f72 2822 466f 7220 2746 6565 6446 6f72  or("For 'FeedFor
-00007840: 7761 7264 272c 2074 6865 2063 6c61 7373  ward', the class
-00007850: 2076 6172 6961 626c 6520 2764 726f 706f   variable 'dropo
-00007860: 7574 5f72 6174 6527 206d 7573 7420 6265  ut_rate' must be
-00007870: 2069 6e20 7468 6520 7261 6e67 6520 5b30   in the range [0
-00007880: 2c20 312e 3029 2c20 220a 2020 2020 2020  , 1.0), ".      
-00007890: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000078a0: 2020 2020 2020 2020 2020 2022 6275 7420             "but 
-000078b0: 676f 7420 7468 6520 7661 6c75 6520 3a20  got the value : 
-000078c0: 7b7d 2e22 2e66 6f72 6d61 7428 6472 6f70  {}.".format(drop
-000078d0: 6f75 745f 7261 7465 2929 0a20 2020 2020  out_rate)).     
-000078e0: 2020 2020 2020 2069 6e70 7574 5f73 697a         input_siz
-000078f0: 6520 3d20 6869 6464 656e 5f73 697a 650a  e = hidden_size.
-00007900: 2020 2020 2020 2020 2020 2020 6f75 7470              outp
-00007910: 7574 5f73 697a 6520 3d20 6666 6e5f 6869  ut_size = ffn_hi
-00007920: 6464 656e 5f73 697a 650a 0a20 2020 2020  dden_size..     
-00007930: 2020 2020 2020 2023 2050 726f 6a65 6374         # Project
-00007940: 2074 6f20 6666 6e5f 6869 6464 656e 5f73   to ffn_hidden_s
-00007950: 697a 650a 2020 2020 2020 2020 2020 2020  ize.            
-00007960: 7365 6c66 2e6d 6170 7069 6e67 203d 204c  self.mapping = L
-00007970: 696e 6561 7228 696e 5f63 6861 6e6e 656c  inear(in_channel
-00007980: 733d 696e 7075 745f 7369 7a65 2c0a 2020  s=input_size,.  
+000077c0: 2020 2020 2020 2020 2020 2020 2061 6374               act
+000077d0: 6976 6174 696f 6e3d 6869 6464 656e 5f61  ivation=hidden_a
+000077e0: 6374 2c0a 2020 2020 2020 2020 2020 2020  ct,.            
+000077f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00007800: 2020 2020 2020 7472 616e 7370 6f73 655f        transpose_
+00007810: 623d 4661 6c73 652c 0a20 2020 2020 2020  b=False,.       
+00007820: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00007830: 2020 2020 2020 2020 2020 2065 7870 6572             exper
+00007840: 745f 6e75 6d3d 6578 7065 7274 5f6e 756d  t_num=expert_num
+00007850: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00007860: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00007870: 2020 2020 6578 7065 7274 5f67 726f 7570      expert_group
+00007880: 5f73 697a 653d 6578 7065 7274 5f67 726f  _size=expert_gro
+00007890: 7570 5f73 697a 652c 0a20 2020 2020 2020  up_size,.       
+000078a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000078b0: 2020 2020 2020 2020 2020 206f 7574 6572             outer
+000078c0: 5f62 6174 6368 3d64 702c 0a20 2020 2020  _batch=dp,.     
+000078d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000078e0: 2020 2020 2020 2020 2020 2020 2070 6172               par
+000078f0: 616d 5f69 6e69 745f 7479 7065 3d70 6172  am_init_type=par
+00007900: 616d 5f69 6e69 745f 7479 7065 290a 0a20  am_init_type).. 
+00007910: 2020 2020 2020 2020 2020 2069 6620 6578             if ex
+00007920: 7065 7274 5f6e 756d 203e 2031 3a0a 2020  pert_num > 1:.  
+00007930: 2020 2020 2020 2020 2020 2020 2020 7365                se
+00007940: 6c66 2e6d 6170 7069 6e67 2e73 6861 7264  lf.mapping.shard
+00007950: 2873 7472 6174 6567 795f 6d61 746d 756c  (strategy_matmul
+00007960: 3d28 2864 702c 2065 702c 2031 2c20 3129  =((dp, ep, 1, 1)
+00007970: 2c20 2865 702c 2031 2c20 6d70 2929 2c0a  , (ep, 1, mp)),.
+00007980: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00007990: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000079a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000079b0: 6f75 745f 6368 616e 6e65 6c73 3d6f 7574  out_channels=out
-000079c0: 7075 745f 7369 7a65 2c0a 2020 2020 2020  put_size,.      
-000079d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000079e0: 2020 2020 2020 2020 2020 2020 6163 7469              acti
-000079f0: 7661 7469 6f6e 3d68 6964 6465 6e5f 6163  vation=hidden_ac
-00007a00: 742c 0a20 2020 2020 2020 2020 2020 2020  t,.             
-00007a10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00007a20: 2020 2020 2074 7261 6e73 706f 7365 5f62       transpose_b
-00007a30: 3d46 616c 7365 2c0a 2020 2020 2020 2020  =False,.        
-00007a40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00007a50: 2020 2020 2020 2020 2020 6578 7065 7274            expert
-00007a60: 5f6e 756d 3d65 7870 6572 745f 6e75 6d2c  _num=expert_num,
-00007a70: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+000079a0: 2020 2073 7472 6174 6567 795f 6269 6173     strategy_bias
+000079b0: 3d28 2864 702c 2065 702c 2031 2c20 6d70  =((dp, ep, 1, mp
+000079c0: 292c 2028 312c 2065 702c 2031 2c20 6d70  ), (1, ep, 1, mp
+000079d0: 2929 2c0a 2020 2020 2020 2020 2020 2020  )),.            
+000079e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000079f0: 2020 2020 2020 2073 7472 6174 6567 795f         strategy_
+00007a00: 6163 7469 7661 7469 6f6e 3d28 2864 702c  activation=((dp,
+00007a10: 2065 702c 2031 2c20 6d70 292c 2929 0a20   ep, 1, mp),)). 
+00007a20: 2020 2020 2020 2020 2020 2065 6c73 653a             else:
+00007a30: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00007a40: 2073 656c 662e 6d61 7070 696e 672e 7368   self.mapping.sh
+00007a50: 6172 6428 7374 7261 7465 6779 5f6d 6174  ard(strategy_mat
+00007a60: 6d75 6c3d 2828 6470 2c20 3129 2c20 2831  mul=((dp, 1), (1
+00007a70: 2c20 6d70 2929 2c0a 2020 2020 2020 2020  , mp)),.        
 00007a80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00007a90: 2020 2065 7870 6572 745f 6772 6f75 705f     expert_group_
-00007aa0: 7369 7a65 3d65 7870 6572 745f 6772 6f75  size=expert_grou
-00007ab0: 705f 7369 7a65 2c0a 2020 2020 2020 2020  p_size,.        
+00007a90: 2020 2020 2020 2020 2020 2073 7472 6174             strat
+00007aa0: 6567 795f 6269 6173 3d28 2864 702c 206d  egy_bias=((dp, m
+00007ab0: 7029 2c20 286d 702c 2929 2c0a 2020 2020  p), (mp,)),.    
 00007ac0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00007ad0: 2020 2020 2020 2020 2020 6f75 7465 725f            outer_
-00007ae0: 6261 7463 683d 6470 2c0a 2020 2020 2020  batch=dp,.      
-00007af0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00007b00: 2020 2020 2020 2020 2020 2020 7061 7261              para
-00007b10: 6d5f 696e 6974 5f74 7970 653d 7061 7261  m_init_type=para
-00007b20: 6d5f 696e 6974 5f74 7970 652c 0a20 2020  m_init_type,.   
-00007b30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00007b40: 2020 2020 2020 2020 2020 2020 2020 2063                 c
-00007b50: 6f6d 7075 7465 5f64 7479 7065 3d63 6f6d  ompute_dtype=com
-00007b60: 7075 7465 5f64 7479 7065 290a 0a20 2020  pute_dtype)..   
-00007b70: 2020 2020 2020 2020 2069 6620 6578 7065           if expe
-00007b80: 7274 5f6e 756d 203e 2031 3a0a 2020 2020  rt_num > 1:.    
-00007b90: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-00007ba0: 2e6d 6170 7069 6e67 2e73 6861 7264 2873  .mapping.shard(s
-00007bb0: 7472 6174 6567 795f 6d61 746d 756c 3d28  trategy_matmul=(
-00007bc0: 2864 702c 2065 702c 2031 2c20 3129 2c20  (dp, ep, 1, 1), 
-00007bd0: 2865 702c 2031 2c20 6d70 2929 2c0a 2020  (ep, 1, mp)),.  
+00007ad0: 2020 2020 2020 2020 2020 2020 2020 2073                 s
+00007ae0: 7472 6174 6567 795f 6163 7469 7661 7469  trategy_activati
+00007af0: 6f6e 3d28 2864 702c 206d 7029 2c29 290a  on=((dp, mp),)).
+00007b00: 2020 2020 2020 2020 2020 2020 2320 5072              # Pr
+00007b10: 6f6a 6563 7420 6261 636b 2074 6f20 6869  oject back to hi
+00007b20: 6464 656e 5f73 697a 650a 2020 2020 2020  dden_size.      
+00007b30: 2020 2020 2020 7365 6c66 2e70 726f 6a65        self.proje
+00007b40: 6374 696f 6e20 3d20 4c69 6e65 6172 2869  ction = Linear(i
+00007b50: 6e5f 6368 616e 6e65 6c73 3d6f 7574 7075  n_channels=outpu
+00007b60: 745f 7369 7a65 2c0a 2020 2020 2020 2020  t_size,.        
+00007b70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00007b80: 2020 2020 2020 2020 2020 2020 206f 7574               out
+00007b90: 5f63 6861 6e6e 656c 733d 696e 7075 745f  _channels=input_
+00007ba0: 7369 7a65 2c0a 2020 2020 2020 2020 2020  size,.          
+00007bb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00007bc0: 2020 2020 2020 2020 2020 2074 7261 6e73             trans
+00007bd0: 706f 7365 5f62 3d46 616c 7365 2c0a 2020  pose_b=False,.  
 00007be0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00007bf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00007c00: 2073 7472 6174 6567 795f 6269 6173 3d28   strategy_bias=(
-00007c10: 2864 702c 2065 702c 2031 2c20 6d70 292c  (dp, ep, 1, mp),
-00007c20: 2028 312c 2065 702c 2031 2c20 6d70 2929   (1, ep, 1, mp))
-00007c30: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00007c40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00007c50: 2020 2020 2073 7472 6174 6567 795f 6163       strategy_ac
-00007c60: 7469 7661 7469 6f6e 3d28 2864 702c 2065  tivation=((dp, e
-00007c70: 702c 2031 2c20 6d70 292c 2929 0a20 2020  p, 1, mp),)).   
-00007c80: 2020 2020 2020 2020 2065 6c73 653a 0a20           else:. 
-00007c90: 2020 2020 2020 2020 2020 2020 2020 2073                 s
-00007ca0: 656c 662e 6d61 7070 696e 672e 7368 6172  elf.mapping.shar
-00007cb0: 6428 7374 7261 7465 6779 5f6d 6174 6d75  d(strategy_matmu
-00007cc0: 6c3d 2828 6470 2c20 3129 2c20 2831 2c20  l=((dp, 1), (1, 
-00007cd0: 6d70 2929 2c0a 2020 2020 2020 2020 2020  mp)),.          
-00007ce0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00007cf0: 2020 2020 2020 2020 2073 7472 6174 6567           strateg
-00007d00: 795f 6269 6173 3d28 2864 702c 206d 7029  y_bias=((dp, mp)
-00007d10: 2c20 286d 702c 2929 2c0a 2020 2020 2020  , (mp,)),.      
-00007d20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00007d30: 2020 2020 2020 2020 2020 2020 2073 7472               str
-00007d40: 6174 6567 795f 6163 7469 7661 7469 6f6e  ategy_activation
-00007d50: 3d28 2864 702c 206d 7029 2c29 290a 2020  =((dp, mp),)).  
-00007d60: 2020 2020 2020 2020 2020 2320 5072 6f6a            # Proj
-00007d70: 6563 7420 6261 636b 2074 6f20 6869 6464  ect back to hidd
-00007d80: 656e 5f73 697a 650a 2020 2020 2020 2020  en_size.        
-00007d90: 2020 2020 7365 6c66 2e70 726f 6a65 6374      self.project
-00007da0: 696f 6e20 3d20 4c69 6e65 6172 2869 6e5f  ion = Linear(in_
-00007db0: 6368 616e 6e65 6c73 3d6f 7574 7075 745f  channels=output_
-00007dc0: 7369 7a65 2c0a 2020 2020 2020 2020 2020  size,.          
-00007dd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00007de0: 2020 2020 2020 2020 2020 206f 7574 5f63             out_c
-00007df0: 6861 6e6e 656c 733d 696e 7075 745f 7369  hannels=input_si
-00007e00: 7a65 2c0a 2020 2020 2020 2020 2020 2020  ze,.            
+00007c00: 2020 2065 7870 6572 745f 6e75 6d3d 6578     expert_num=ex
+00007c10: 7065 7274 5f6e 756d 2c0a 2020 2020 2020  pert_num,.      
+00007c20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00007c30: 2020 2020 2020 2020 2020 2020 2020 2065                 e
+00007c40: 7870 6572 745f 6772 6f75 705f 7369 7a65  xpert_group_size
+00007c50: 3d65 7870 6572 745f 6772 6f75 705f 7369  =expert_group_si
+00007c60: 7a65 2c0a 2020 2020 2020 2020 2020 2020  ze,.            
+00007c70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00007c80: 2020 2020 2020 2020 206f 7574 6572 5f62           outer_b
+00007c90: 6174 6368 3d64 702c 0a20 2020 2020 2020  atch=dp,.       
+00007ca0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00007cb0: 2020 2020 2020 2020 2020 2020 2020 7061                pa
+00007cc0: 7261 6d5f 696e 6974 5f74 7970 653d 7061  ram_init_type=pa
+00007cd0: 7261 6d5f 696e 6974 5f74 7970 6529 0a20  ram_init_type). 
+00007ce0: 2020 2020 2020 2020 2020 2069 6620 6578             if ex
+00007cf0: 7065 7274 5f6e 756d 203e 2031 3a0a 2020  pert_num > 1:.  
+00007d00: 2020 2020 2020 2020 2020 2020 2020 7365                se
+00007d10: 6c66 2e70 726f 6a65 6374 696f 6e2e 7368  lf.projection.sh
+00007d20: 6172 6428 7374 7261 7465 6779 5f6d 6174  ard(strategy_mat
+00007d30: 6d75 6c3d 2828 6470 2c20 6570 2c20 312c  mul=((dp, ep, 1,
+00007d40: 206d 7029 2c20 2865 702c 206d 702c 2031   mp), (ep, mp, 1
+00007d50: 2929 2c0a 2020 2020 2020 2020 2020 2020  )),.            
+00007d60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00007d70: 2020 2020 2020 2020 2020 7374 7261 7465            strate
+00007d80: 6779 5f62 6961 733d 2828 6470 2c20 6570  gy_bias=((dp, ep
+00007d90: 2c20 312c 2031 292c 2028 312c 2065 702c  , 1, 1), (1, ep,
+00007da0: 2031 2c20 3129 2929 0a20 2020 2020 2020   1, 1))).       
+00007db0: 2020 2020 2065 6c73 653a 0a20 2020 2020       else:.     
+00007dc0: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+00007dd0: 7072 6f6a 6563 7469 6f6e 2e73 6861 7264  projection.shard
+00007de0: 2873 7472 6174 6567 795f 6d61 746d 756c  (strategy_matmul
+00007df0: 3d28 2864 702c 206d 7029 2c20 286d 702c  =((dp, mp), (mp,
+00007e00: 2031 2929 2c0a 2020 2020 2020 2020 2020   1)),.          
 00007e10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00007e20: 2020 2020 2020 2020 2074 7261 6e73 706f           transpo
-00007e30: 7365 5f62 3d46 616c 7365 2c0a 2020 2020  se_b=False,.    
-00007e40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00007e50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00007e60: 2065 7870 6572 745f 6e75 6d3d 6578 7065   expert_num=expe
-00007e70: 7274 5f6e 756d 2c0a 2020 2020 2020 2020  rt_num,.        
-00007e80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00007e90: 2020 2020 2020 2020 2020 2020 2065 7870               exp
-00007ea0: 6572 745f 6772 6f75 705f 7369 7a65 3d65  ert_group_size=e
-00007eb0: 7870 6572 745f 6772 6f75 705f 7369 7a65  xpert_group_size
-00007ec0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00007ed0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00007ee0: 2020 2020 2020 206f 7574 6572 5f62 6174         outer_bat
-00007ef0: 6368 3d64 702c 0a20 2020 2020 2020 2020  ch=dp,.         
-00007f00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00007f10: 2020 2020 2020 2020 2020 2020 7061 7261              para
-00007f20: 6d5f 696e 6974 5f74 7970 653d 7061 7261  m_init_type=para
-00007f30: 6d5f 696e 6974 5f74 7970 652c 0a20 2020  m_init_type,.   
-00007f40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00007f50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00007f60: 2020 636f 6d70 7574 655f 6474 7970 653d    compute_dtype=
-00007f70: 636f 6d70 7574 655f 6474 7970 6529 0a20  compute_dtype). 
-00007f80: 2020 2020 2020 2020 2020 2069 6620 6578             if ex
-00007f90: 7065 7274 5f6e 756d 203e 2031 3a0a 2020  pert_num > 1:.  
-00007fa0: 2020 2020 2020 2020 2020 2020 2020 7365                se
-00007fb0: 6c66 2e70 726f 6a65 6374 696f 6e2e 7368  lf.projection.sh
-00007fc0: 6172 6428 7374 7261 7465 6779 5f6d 6174  ard(strategy_mat
-00007fd0: 6d75 6c3d 2828 6470 2c20 6570 2c20 312c  mul=((dp, ep, 1,
-00007fe0: 206d 7029 2c20 2865 702c 206d 702c 2031   mp), (ep, mp, 1
-00007ff0: 2929 2c0a 2020 2020 2020 2020 2020 2020  )),.            
-00008000: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00008010: 2020 2020 2020 2020 2020 7374 7261 7465            strate
-00008020: 6779 5f62 6961 733d 2828 6470 2c20 6570  gy_bias=((dp, ep
-00008030: 2c20 312c 2031 292c 2028 312c 2065 702c  , 1, 1), (1, ep,
-00008040: 2031 2c20 3129 2929 0a20 2020 2020 2020   1, 1))).       
-00008050: 2020 2020 2065 6c73 653a 0a20 2020 2020       else:.     
-00008060: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-00008070: 7072 6f6a 6563 7469 6f6e 2e73 6861 7264  projection.shard
-00008080: 2873 7472 6174 6567 795f 6d61 746d 756c  (strategy_matmul
-00008090: 3d28 2864 702c 206d 7029 2c20 286d 702c  =((dp, mp), (mp,
-000080a0: 2031 2929 2c0a 2020 2020 2020 2020 2020   1)),.          
-000080b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000080c0: 2020 2020 2020 2020 2020 2020 7374 7261              stra
-000080d0: 7465 6779 5f62 6961 733d 2828 6470 2c20  tegy_bias=((dp, 
-000080e0: 3129 2c20 2831 2c29 2929 0a20 2020 2020  1), (1,))).     
-000080f0: 2020 2020 2020 2073 656c 662e 7072 6f6a         self.proj
-00008100: 6563 7469 6f6e 2e62 6961 732e 7061 7261  ection.bias.para
-00008110: 6c6c 656c 5f6f 7074 696d 697a 6572 203d  llel_optimizer =
-00008120: 2046 616c 7365 0a20 2020 2020 2020 2020   False.         
-00008130: 2020 2073 656c 662e 6472 6f70 6f75 7420     self.dropout 
-00008140: 3d20 6765 745f 6472 6f70 6f75 7428 6472  = get_dropout(dr
-00008150: 6f70 6f75 745f 7261 7465 290a 2020 2020  opout_rate).    
-00008160: 2020 2020 2020 2020 7365 6c66 2e64 726f          self.dro
-00008170: 706f 7574 5f33 6420 3d20 6765 745f 6472  pout_3d = get_dr
-00008180: 6f70 6f75 7428 6472 6f70 6f75 745f 7261  opout(dropout_ra
-00008190: 7465 290a 2020 2020 2020 2020 2020 2020  te).            
-000081a0: 7365 6c66 2e64 726f 706f 7574 5f34 6420  self.dropout_4d 
-000081b0: 3d20 6765 745f 6472 6f70 6f75 7428 6472  = get_dropout(dr
-000081c0: 6f70 6f75 745f 7261 7465 290a 2020 2020  opout_rate).    
-000081d0: 2020 2020 2020 2020 7365 6c66 2e64 726f          self.dro
-000081e0: 706f 7574 2e64 726f 706f 7574 2e73 6861  pout.dropout.sha
-000081f0: 7264 2828 2864 702c 2031 292c 2929 0a20  rd(((dp, 1),)). 
-00008200: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-00008210: 6472 6f70 6f75 745f 3364 2e64 726f 706f  dropout_3d.dropo
-00008220: 7574 2e73 6861 7264 2828 2864 702c 2031  ut.shard(((dp, 1
-00008230: 2c20 3129 2c29 290a 2020 2020 2020 2020  , 1),)).        
-00008240: 2020 2020 7365 6c66 2e64 726f 706f 7574      self.dropout
-00008250: 5f34 642e 6472 6f70 6f75 742e 7368 6172  _4d.dropout.shar
-00008260: 6428 2828 6470 2c20 6570 2c20 312c 2031  d(((dp, ep, 1, 1
-00008270: 292c 2929 0a20 2020 2020 2020 2020 2020  ),)).           
-00008280: 2073 656c 662e 6361 7374 203d 2050 2e43   self.cast = P.C
-00008290: 6173 7428 290a 0a20 2020 2064 6566 2063  ast()..    def c
-000082a0: 6f6e 7374 7275 6374 2873 656c 662c 2078  onstruct(self, x
-000082b0: 293a 0a20 2020 2020 2020 2022 2222 466f  ):.        """Fo
-000082c0: 7277 6172 6420 7072 6f63 6573 7320 6f66  rward process of
-000082d0: 2074 6865 2046 6565 6446 6f72 7761 7264   the FeedForward
-000082e0: 2222 220a 2020 2020 2020 2020 5f63 6865  """.        _che
-000082f0: 636b 5f69 6e70 7574 5f64 7479 7065 2846  ck_input_dtype(F
-00008300: 2e64 7479 7065 2878 292c 2022 7822 2c20  .dtype(x), "x", 
-00008310: 5b6d 7374 7970 652e 666c 6f61 7433 322c  [mstype.float32,
-00008320: 206d 7374 7970 652e 666c 6f61 7431 362c   mstype.float16,
-00008330: 206d 7374 7970 652e 6266 6c6f 6174 3136   mstype.bfloat16
-00008340: 5d2c 2073 656c 662e 636c 735f 6e61 6d65  ], self.cls_name
-00008350: 290a 2020 2020 2020 2020 7820 3d20 7365  ).        x = se
-00008360: 6c66 2e63 6173 7428 782c 2073 656c 662e  lf.cast(x, self.
-00008370: 6474 7970 6529 0a20 2020 2020 2020 2023  dtype).        #
-00008380: 2072 6574 7572 6e65 6420 7368 6170 6520   returned shape 
-00008390: 6973 205b 6273 2c20 7365 715f 6c65 6e67  is [bs, seq_leng
-000083a0: 7468 2c20 6666 6e5f 6869 6464 656e 5f73  th, ffn_hidden_s
-000083b0: 697a 655d 206f 7220 5b62 7320 2a20 7365  ize] or [bs * se
-000083c0: 715f 6c65 6e67 7468 2c20 6666 6e5f 6869  q_length, ffn_hi
-000083d0: 6464 656e 5f73 697a 655d 0a20 2020 2020  dden_size].     
-000083e0: 2020 2068 6964 6465 6e20 3d20 7365 6c66     hidden = self
-000083f0: 2e6d 6170 7069 6e67 2878 290a 2020 2020  .mapping(x).    
-00008400: 2020 2020 6f75 7470 7574 203d 2073 656c      output = sel
-00008410: 662e 7072 6f6a 6563 7469 6f6e 2868 6964  f.projection(hid
-00008420: 6465 6e29 0a20 2020 2020 2020 2023 2072  den).        # r
-00008430: 6574 7572 6e65 6420 7368 6170 6520 6973  eturned shape is
-00008440: 205b 6273 2c20 7365 715f 6c65 6e67 7468   [bs, seq_length
-00008450: 2c20 6666 6e5f 6869 6464 656e 5f73 697a  , ffn_hidden_siz
-00008460: 655d 206f 7220 5b62 7320 2a20 7365 715f  e] or [bs * seq_
-00008470: 6c65 6e67 7468 2c20 6666 6e5f 6869 6464  length, ffn_hidd
-00008480: 656e 5f73 697a 655d 0a20 2020 2020 2020  en_size].       
-00008490: 2069 6620 6c65 6e28 462e 7368 6170 6528   if len(F.shape(
-000084a0: 6f75 7470 7574 2929 203d 3d20 333a 0a20  output)) == 3:. 
-000084b0: 2020 2020 2020 2020 2020 206f 7574 7075             outpu
-000084c0: 7420 3d20 7365 6c66 2e64 726f 706f 7574  t = self.dropout
-000084d0: 5f33 6428 6f75 7470 7574 290a 2020 2020  _3d(output).    
-000084e0: 2020 2020 656c 6966 206c 656e 2846 2e73      elif len(F.s
-000084f0: 6861 7065 286f 7574 7075 7429 2920 3d3d  hape(output)) ==
-00008500: 2032 3a0a 2020 2020 2020 2020 2020 2020   2:.            
-00008510: 6f75 7470 7574 203d 2073 656c 662e 6472  output = self.dr
-00008520: 6f70 6f75 7428 6f75 7470 7574 290a 2020  opout(output).  
-00008530: 2020 2020 2020 656c 7365 3a0a 2020 2020        else:.    
-00008540: 2020 2020 2020 2020 6f75 7470 7574 203d          output =
-00008550: 2073 656c 662e 6472 6f70 6f75 745f 3464   self.dropout_4d
-00008560: 286f 7574 7075 7429 0a20 2020 2020 2020  (output).       
-00008570: 2072 6574 7572 6e20 6f75 7470 7574 0a0a   return output..
-00008580: 0a63 6c61 7373 2041 7474 656e 7469 6f6e  .class Attention
-00008590: 4d61 736b 2843 656c 6c29 3a0a 2020 2020  Mask(Cell):.    
-000085a0: 7222 2222 0a20 2020 2020 2020 2047 6574  r""".        Get
-000085b0: 2074 6865 204c 6f77 6572 2074 7269 616e   the Lower trian
-000085c0: 6775 6c61 7220 6d61 7472 6978 2066 726f  gular matrix fro
-000085d0: 6d20 7468 6520 696e 7075 7420 6d61 736b  m the input mask
-000085e0: 2e20 5468 6520 696e 7075 7420 6d61 736b  . The input mask
-000085f0: 2069 7320 6120 3244 2074 656e 736f 7220   is a 2D tensor 
-00008600: 2862 6174 6368 5f73 697a 652c 2073 6571  (batch_size, seq
-00008610: 5f6c 656e 6774 6829 0a20 2020 2020 2020  _length).       
-00008620: 2077 6974 6820 3120 616e 6420 302c 2077   with 1 and 0, w
-00008630: 6865 7265 2031 2069 6e64 6963 6174 6573  here 1 indicates
-00008640: 2074 6865 2063 7572 7265 6e74 2070 6f73   the current pos
-00008650: 6974 696f 6e20 6973 2061 2076 616c 6964  ition is a valid
-00008660: 2074 6f6b 656e 2c20 6f74 6865 7277 6973   token, otherwis
-00008670: 6520 6e6f 742e 0a0a 2020 2020 2020 2020  e not...        
-00008680: 4172 6773 3a0a 2020 2020 2020 2020 2020  Args:.          
-00008690: 2020 7365 715f 6c65 6e67 7468 2869 6e74    seq_length(int
-000086a0: 293a 2054 6865 2073 6571 7565 6e63 6520  ): The sequence 
-000086b0: 6c65 6e67 7468 206f 6620 7468 6520 696e  length of the in
-000086c0: 7075 7420 7465 6e73 6f72 2e0a 2020 2020  put tensor..    
-000086d0: 2020 2020 2020 2020 7061 7261 6c6c 656c          parallel
-000086e0: 5f63 6f6e 6669 6728 4f70 5061 7261 6c6c  _config(OpParall
-000086f0: 656c 436f 6e66 6967 293a 2054 6865 2070  elConfig): The p
-00008700: 6172 616c 6c65 6c20 636f 6e66 6967 7572  arallel configur
-00008710: 652e 2044 6566 6175 6c74 2060 6465 6661  e. Default `defa
-00008720: 756c 745f 6470 6d70 5f63 6f6e 6669 6760  ult_dpmp_config`
-00008730: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00008740: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00008750: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00008760: 2061 6e20 696e 7374 616e 6365 206f 6620   an instance of 
-00008770: 604f 7050 6172 616c 6c65 6c43 6f6e 6669  `OpParallelConfi
-00008780: 6760 2077 6974 6820 6465 6661 756c 7420  g` with default 
-00008790: 6172 6773 2e0a 0a20 2020 2020 2020 2049  args...        I
-000087a0: 6e70 7574 733a 0a20 2020 2020 2020 2020  nputs:.         
-000087b0: 2020 202d 202a 2a69 6e70 7574 5f6d 6173     - **input_mas
-000087c0: 6b2a 2a20 2854 656e 736f 7229 202d 2054  k** (Tensor) - T
-000087d0: 6865 206d 6173 6b20 696e 6469 6361 7469  he mask indicati
-000087e0: 6e67 2077 6865 7468 6572 2065 6163 6820  ng whether each 
-000087f0: 706f 7369 7469 6f6e 2069 7320 6120 7661  position is a va
-00008800: 6c69 6420 696e 7075 7420 7769 7468 0a20  lid input with. 
-00008810: 2020 2020 2020 2020 2020 2020 2028 6261               (ba
-00008820: 7463 685f 7369 7a65 2c20 7365 715f 6c65  tch_size, seq_le
-00008830: 6e67 7468 292e 0a0a 2020 2020 2020 2020  ngth)...        
-00008840: 4f75 7470 7574 733a 0a20 2020 2020 2020  Outputs:.       
-00008850: 2020 2020 2054 656e 736f 722e 2054 6865       Tensor. The
-00008860: 2061 7474 656e 7469 6f6e 206d 6173 6b20   attention mask 
-00008870: 6d61 7472 6978 2077 6974 6820 7368 6170  matrix with shap
-00008880: 6520 2862 6174 6368 5f73 697a 652c 2073  e (batch_size, s
-00008890: 6571 5f6c 656e 6774 682c 2073 6571 5f6c  eq_length, seq_l
-000088a0: 656e 6774 6829 2e0a 0a20 2020 2020 2020  ength)...       
-000088b0: 2052 6169 7365 733a 0a20 2020 2020 2020   Raises:.       
-000088c0: 2020 2020 2054 7970 6545 7272 6f72 3a20       TypeError: 
-000088d0: 6073 6571 5f6c 656e 6774 6860 2069 7320  `seq_length` is 
-000088e0: 6e6f 7420 616e 2069 6e74 6567 6572 2e0a  not an integer..
-000088f0: 2020 2020 2020 2020 2020 2020 5661 6c75              Valu
-00008900: 6545 7272 6f72 3a20 6073 6571 5f6c 656e  eError: `seq_len
-00008910: 6774 6860 2069 7320 6e6f 7420 6120 706f  gth` is not a po
-00008920: 7369 7469 7665 2076 616c 7565 2e0a 2020  sitive value..  
-00008930: 2020 2020 2020 2020 2020 5479 7065 4572            TypeEr
-00008940: 726f 723a 2060 7061 7261 6c6c 656c 5f63  ror: `parallel_c
-00008950: 6f6e 6669 6760 2069 7320 6e6f 7420 6120  onfig` is not a 
-00008960: 7375 6263 6c61 7373 206f 6620 4f70 5061  subclass of OpPa
-00008970: 7261 6c6c 656c 436f 6e66 6967 2e0a 0a20  rallelConfig... 
-00008980: 2020 2020 2020 2053 7570 706f 7274 6564         Supported
-00008990: 2050 6c61 7466 6f72 6d73 3a0a 2020 2020   Platforms:.    
-000089a0: 2020 2020 2020 2020 6060 4173 6365 6e64          ``Ascend
-000089b0: 6060 2060 6047 5055 6060 0a0a 2020 2020  `` ``GPU``..    
-000089c0: 2020 2020 4578 616d 706c 6573 3a0a 2020      Examples:.  
-000089d0: 2020 2020 2020 2020 2020 3e3e 3e20 696d            >>> im
-000089e0: 706f 7274 206e 756d 7079 2061 7320 6e70  port numpy as np
-000089f0: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
-00008a00: 2066 726f 6d20 6d69 6e64 666f 726d 6572   from mindformer
-00008a10: 732e 6d6f 6475 6c65 732e 7472 616e 7366  s.modules.transf
-00008a20: 6f72 6d65 7220 696d 706f 7274 2041 7474  ormer import Att
-00008a30: 656e 7469 6f6e 4d61 736b 0a20 2020 2020  entionMask.     
-00008a40: 2020 2020 2020 203e 3e3e 2066 726f 6d20         >>> from 
-00008a50: 6d69 6e64 7370 6f72 6520 696d 706f 7274  mindspore import
-00008a60: 2054 656e 736f 720a 2020 2020 2020 2020   Tensor.        
-00008a70: 2020 2020 3e3e 3e20 6d61 736b 203d 2041      >>> mask = A
-00008a80: 7474 656e 7469 6f6e 4d61 736b 2873 6571  ttentionMask(seq
-00008a90: 5f6c 656e 6774 683d 3429 0a20 2020 2020  _length=4).     
-00008aa0: 2020 2020 2020 203e 3e3e 206d 6173 6b5f         >>> mask_
-00008ab0: 6172 7261 7920 3d20 6e70 2e61 7272 6179  array = np.array
-00008ac0: 285b 5b31 2c20 312c 2031 2c20 305d 5d2c  ([[1, 1, 1, 0]],
-00008ad0: 206e 702e 666c 6f61 7433 3229 0a20 2020   np.float32).   
-00008ae0: 2020 2020 2020 2020 203e 3e3e 2069 6e70           >>> inp
-00008af0: 7574 7320 3d20 5465 6e73 6f72 286d 6173  uts = Tensor(mas
-00008b00: 6b5f 6172 7261 7929 0a20 2020 2020 2020  k_array).       
-00008b10: 2020 2020 203e 3e3e 2072 6573 203d 206d       >>> res = m
-00008b20: 6173 6b28 696e 7075 7473 290a 2020 2020  ask(inputs).    
-00008b30: 2020 2020 2020 2020 3e3e 3e20 7072 696e          >>> prin
-00008b40: 7428 7265 7329 0a20 2020 2020 2020 2020  t(res).         
-00008b50: 2020 205b 5b5b 312e 2030 2e20 302e 2030     [[[1. 0. 0. 0
-00008b60: 5d0a 2020 2020 2020 2020 2020 2020 2020  ].              
-00008b70: 5b31 2e20 312e 2030 2e20 305d 0a20 2020  [1. 1. 0. 0].   
-00008b80: 2020 2020 2020 2020 2020 205b 312e 2031             [1. 1
-00008b90: 2e20 312e 2030 5d0a 2020 2020 2020 2020  . 1. 0].        
-00008ba0: 2020 2020 2020 5b30 2e20 302e 2030 2e20        [0. 0. 0. 
-00008bb0: 305d 5d5d 0a20 2020 2022 2222 0a0a 2020  0]]].    """..  
-00008bc0: 2020 405f 4c6f 6741 6374 696f 6e4f 6e63    @_LogActionOnc
-00008bd0: 6528 6d5f 6c6f 6767 6572 3d6c 6f67 6765  e(m_logger=logge
-00008be0: 722c 206b 6579 3d27 4174 7465 6e74 696f  r, key='Attentio
-00008bf0: 6e4d 6173 6b27 2c0a 2020 2020 2020 2020  nMask',.        
-00008c00: 2020 2020 2020 2020 2020 2020 6e6f 5f77              no_w
-00008c10: 6172 6e69 6e67 3d5f 6765 745f 7061 7261  arning=_get_para
-00008c20: 6c6c 656c 5f6d 6f64 6528 2920 696e 2028  llel_mode() in (
-00008c30: 5061 7261 6c6c 656c 4d6f 6465 2e53 5441  ParallelMode.STA
-00008c40: 4e44 5f41 4c4f 4e45 2c29 290a 2020 2020  ND_ALONE,)).    
-00008c50: 405f 6172 6773 5f74 7970 655f 7661 6c69  @_args_type_vali
-00008c60: 6461 746f 725f 6368 6563 6b28 7365 715f  dator_check(seq_
-00008c70: 6c65 6e67 7468 3d56 616c 6964 6174 6f72  length=Validator
-00008c80: 2e63 6865 636b 5f70 6f73 6974 6976 655f  .check_positive_
-00008c90: 696e 742c 0a20 2020 2020 2020 2020 2020  int,.           
-00008ca0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00008cb0: 2020 2020 2070 6172 616c 6c65 6c5f 636f       parallel_co
-00008cc0: 6e66 6967 3d5f 7661 6c69 645f 7479 7065  nfig=_valid_type
-00008cd0: 5f63 6865 636b 7328 5b4f 7050 6172 616c  _checks([OpParal
-00008ce0: 6c65 6c43 6f6e 6669 675d 2c20 2241 7474  lelConfig], "Att
-00008cf0: 656e 7469 6f6e 4d61 736b 2229 290a 2020  entionMask")).  
-00008d00: 2020 6465 6620 5f5f 696e 6974 5f5f 2873    def __init__(s
-00008d10: 656c 662c 2073 6571 5f6c 656e 6774 682c  elf, seq_length,
-00008d20: 2070 6172 616c 6c65 6c5f 636f 6e66 6967   parallel_config
-00008d30: 3d64 6566 6175 6c74 5f64 706d 705f 636f  =default_dpmp_co
-00008d40: 6e66 6967 2c20 636f 6d70 7574 655f 6474  nfig, compute_dt
-00008d50: 7970 653d 6d73 7479 7065 2e66 6c6f 6174  ype=mstype.float
-00008d60: 3136 293a 0a20 2020 2020 2020 2073 7570  16):.        sup
-00008d70: 6572 2841 7474 656e 7469 6f6e 4d61 736b  er(AttentionMask
-00008d80: 2c20 7365 6c66 292e 5f5f 696e 6974 5f5f  , self).__init__
-00008d90: 2829 0a20 2020 2020 2020 2073 656c 662e  ().        self.
-00008da0: 7365 715f 6c65 6e67 7468 203d 2073 6571  seq_length = seq
-00008db0: 5f6c 656e 6774 680a 2020 2020 2020 2020  _length.        
-00008dc0: 7365 6c66 2e63 6f6d 7075 7465 5f64 7479  self.compute_dty
-00008dd0: 7065 203d 2063 6f6d 7075 7465 5f64 7479  pe = compute_dty
-00008de0: 7065 0a20 2020 2020 2020 2073 656c 662e  pe.        self.
-00008df0: 6e6f 745f 6571 7561 6c20 3d20 502e 4e6f  not_equal = P.No
-00008e00: 7445 7175 616c 2829 2e73 6861 7264 2828  tEqual().shard((
-00008e10: 2870 6172 616c 6c65 6c5f 636f 6e66 6967  (parallel_config
-00008e20: 2e64 6174 615f 7061 7261 6c6c 656c 2c20  .data_parallel, 
-00008e30: 3129 2c20 2829 2929 0a20 2020 2020 2020  1), ())).       
-00008e40: 2073 656c 662e 7265 7368 6170 6520 3d20   self.reshape = 
-00008e50: 502e 5265 7368 6170 6528 290a 2020 2020  P.Reshape().    
-00008e60: 2020 2020 7365 6c66 2e6d 756c 203d 2050      self.mul = P
-00008e70: 2e42 6174 6368 4d61 744d 756c 2829 2e73  .BatchMatMul().s
-00008e80: 6861 7264 280a 2020 2020 2020 2020 2020  hard(.          
-00008e90: 2020 2828 7061 7261 6c6c 656c 5f63 6f6e    ((parallel_con
-00008ea0: 6669 672e 6461 7461 5f70 6172 616c 6c65  fig.data_paralle
-00008eb0: 6c2c 2031 2c20 3129 2c20 2870 6172 616c  l, 1, 1), (paral
-00008ec0: 6c65 6c5f 636f 6e66 6967 2e64 6174 615f  lel_config.data_
-00008ed0: 7061 7261 6c6c 656c 2c20 312c 2031 2929  parallel, 1, 1))
-00008ee0: 290a 2020 2020 2020 2020 7365 6c66 2e65  ).        self.e
-00008ef0: 7870 616e 645f 6469 6d20 3d20 502e 4578  xpand_dim = P.Ex
-00008f00: 7061 6e64 4469 6d73 2829 2e73 6861 7264  pandDims().shard
-00008f10: 2828 2831 2c20 3129 2c29 290a 2020 2020  (((1, 1),)).    
-00008f20: 2020 2020 6f6e 6573 203d 206e 702e 6f6e      ones = np.on
-00008f30: 6573 2873 6861 7065 3d28 7365 715f 6c65  es(shape=(seq_le
-00008f40: 6e67 7468 2c20 7365 715f 6c65 6e67 7468  ngth, seq_length
-00008f50: 2929 0a20 2020 2020 2020 2023 2044 6566  )).        # Def
-00008f60: 6175 6c74 206c 6f77 6572 2074 7269 616e  ault lower trian
-00008f70: 676c 6520 6d61 736b 206d 6174 7269 780a  gle mask matrix.
-00008f80: 2020 2020 2020 2020 7365 6c66 2e6c 6f77          self.low
-00008f90: 6572 5f74 7269 616e 676c 655f 6d61 736b  er_triangle_mask
-00008fa0: 203d 2054 656e 736f 7228 6e70 2e74 7269   = Tensor(np.tri
-00008fb0: 6c28 6f6e 6573 292c 206d 7374 7970 652e  l(ones), mstype.
-00008fc0: 666c 6f61 7433 3229 0a20 2020 2020 2020  float32).       
-00008fd0: 2073 656c 662e 6d75 6c74 6970 6c79 203d   self.multiply =
-00008fe0: 2050 2e4d 756c 2829 2e73 6861 7264 2828   P.Mul().shard((
-00008ff0: 2870 6172 616c 6c65 6c5f 636f 6e66 6967  (parallel_config
-00009000: 2e64 6174 615f 7061 7261 6c6c 656c 2c20  .data_parallel, 
-00009010: 312c 2031 292c 2028 312c 2031 2c20 3129  1, 1), (1, 1, 1)
-00009020: 2929 0a0a 2020 2020 6465 6620 636f 6e73  ))..    def cons
-00009030: 7472 7563 7428 7365 6c66 2c20 696e 7075  truct(self, inpu
-00009040: 745f 6d61 736b 293a 0a20 2020 2020 2020  t_mask):.       
-00009050: 2022 2222 466f 7277 6172 6420 7072 6f63   """Forward proc
-00009060: 6573 7320 6f66 2074 6865 2041 7474 656e  ess of the Atten
-00009070: 7469 6f6e 4d61 736b 2222 220a 2020 2020  tionMask""".    
-00009080: 2020 2020 5f63 6865 636b 5f69 6e70 7574      _check_input
-00009090: 5f64 7479 7065 2846 2e64 7479 7065 2869  _dtype(F.dtype(i
-000090a0: 6e70 7574 5f6d 6173 6b29 2c20 2269 6e70  nput_mask), "inp
-000090b0: 7574 5f6d 6173 6b22 2c0a 2020 2020 2020  ut_mask",.      
-000090c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000090d0: 2020 2020 205b 6d73 7479 7065 2e66 6c6f       [mstype.flo
-000090e0: 6174 3332 2c20 6d73 7479 7065 2e66 6c6f  at32, mstype.flo
-000090f0: 6174 3136 2c20 6d73 7479 7065 2e62 666c  at16, mstype.bfl
-00009100: 6f61 7431 365d 2c20 7365 6c66 2e63 6c73  oat16], self.cls
-00009110: 5f6e 616d 6529 0a20 2020 2020 2020 2069  _name).        i
-00009120: 6e70 7574 5f6d 6173 6b20 3d20 502e 4361  nput_mask = P.Ca
-00009130: 7374 2829 2873 656c 662e 6e6f 745f 6571  st()(self.not_eq
-00009140: 7561 6c28 696e 7075 745f 6d61 736b 2c20  ual(input_mask, 
-00009150: 3029 2c20 7365 6c66 2e63 6f6d 7075 7465  0), self.compute
-00009160: 5f64 7479 7065 290a 2020 2020 2020 2020  _dtype).        
-00009170: 696e 7075 745f 7368 6170 6520 3d20 502e  input_shape = P.
-00009180: 5368 6170 6528 2928 696e 7075 745f 6d61  Shape()(input_ma
-00009190: 736b 290a 2020 2020 2020 2020 7368 6170  sk).        shap
-000091a0: 655f 7269 6768 7420 3d20 2869 6e70 7574  e_right = (input
-000091b0: 5f73 6861 7065 5b30 5d2c 2031 2c20 696e  _shape[0], 1, in
-000091c0: 7075 745f 7368 6170 655b 315d 290a 2020  put_shape[1]).  
-000091d0: 2020 2020 2020 7368 6170 655f 6c65 6674        shape_left
-000091e0: 203d 2069 6e70 7574 5f73 6861 7065 202b   = input_shape +
-000091f0: 2028 312c 290a 2020 2020 2020 2020 2320   (1,).        # 
-00009200: 4d61 736b 2074 6865 2070 6164 6465 6420  Mask the padded 
-00009210: 696e 7075 7473 0a20 2020 2020 2020 206d  inputs.        m
-00009220: 6173 6b5f 6c65 6674 203d 2073 656c 662e  ask_left = self.
-00009230: 7265 7368 6170 6528 696e 7075 745f 6d61  reshape(input_ma
-00009240: 736b 2c20 7368 6170 655f 6c65 6674 290a  sk, shape_left).
-00009250: 2020 2020 2020 2020 6d61 736b 5f72 6967          mask_rig
-00009260: 6874 203d 2073 656c 662e 7265 7368 6170  ht = self.reshap
-00009270: 6528 696e 7075 745f 6d61 736b 2c20 7368  e(input_mask, sh
-00009280: 6170 655f 7269 6768 7429 0a20 2020 2020  ape_right).     
-00009290: 2020 2061 7474 656e 7469 6f6e 5f6d 6173     attention_mas
-000092a0: 6b20 3d20 7365 6c66 2e6d 756c 286d 6173  k = self.mul(mas
-000092b0: 6b5f 6c65 6674 2c20 6d61 736b 5f72 6967  k_left, mask_rig
-000092c0: 6874 290a 2020 2020 2020 2020 6c6f 7765  ht).        lowe
-000092d0: 725f 7472 6169 616e 676c 6520 3d20 7365  r_traiangle = se
-000092e0: 6c66 2e65 7870 616e 645f 6469 6d28 7365  lf.expand_dim(se
-000092f0: 6c66 2e6c 6f77 6572 5f74 7269 616e 676c  lf.lower_triangl
-00009300: 655f 6d61 736b 2c20 3029 0a20 2020 2020  e_mask, 0).     
-00009310: 2020 2023 2074 6865 2072 6574 7572 6e65     # the returne
-00009320: 6420 7368 6170 6520 6973 205b 6273 2c20  d shape is [bs, 
-00009330: 7365 715f 6c65 6e67 7468 2c20 7365 715f  seq_length, seq_
-00009340: 6c65 6e67 7468 5d0a 2020 2020 2020 2020  length].        
-00009350: 6174 7465 6e74 696f 6e5f 6d61 736b 203d  attention_mask =
-00009360: 2073 656c 662e 6d75 6c74 6970 6c79 2861   self.multiply(a
-00009370: 7474 656e 7469 6f6e 5f6d 6173 6b2c 206c  ttention_mask, l
-00009380: 6f77 6572 5f74 7261 6961 6e67 6c65 290a  ower_traiangle).
-00009390: 2020 2020 2020 2020 7265 7475 726e 2061          return a
-000093a0: 7474 656e 7469 6f6e 5f6d 6173 6b0a 0a0a  ttention_mask...
-000093b0: 636c 6173 7320 4174 7465 6e74 696f 6e4d  class AttentionM
-000093c0: 6173 6b48 4628 4365 6c6c 293a 0a20 2020  askHF(Cell):.   
-000093d0: 2072 2222 220a 2020 2020 2020 2020 4765   r""".        Ge
-000093e0: 7420 7468 6520 4c6f 7765 7220 7472 6961  t the Lower tria
-000093f0: 6e67 756c 6172 206d 6174 7269 7820 6672  ngular matrix fr
-00009400: 6f6d 2074 6865 2069 6e70 7574 206d 6173  om the input mas
-00009410: 6b2e 2054 6865 2069 6e70 7574 206d 6173  k. The input mas
-00009420: 6b20 6973 2061 2032 4420 7465 6e73 6f72  k is a 2D tensor
-00009430: 2028 6261 7463 685f 7369 7a65 2c20 7365   (batch_size, se
-00009440: 715f 6c65 6e67 7468 292e 0a0a 2020 2020  q_length)...    
-00009450: 2020 2020 4172 6773 3a0a 2020 2020 2020      Args:.      
-00009460: 2020 2020 2020 7365 715f 6c65 6e67 7468        seq_length
-00009470: 2869 6e74 293a 2054 6865 2073 6571 7565  (int): The seque
-00009480: 6e63 6520 6c65 6e67 7468 206f 6620 7468  nce length of th
-00009490: 6520 696e 7075 7420 7465 6e73 6f72 2e0a  e input tensor..
-000094a0: 2020 2020 2020 2020 2020 2020 7061 7261              para
-000094b0: 6c6c 656c 5f63 6f6e 6669 6728 4f70 5061  llel_config(OpPa
-000094c0: 7261 6c6c 656c 436f 6e66 6967 293a 2054  rallelConfig): T
-000094d0: 6865 2070 6172 616c 6c65 6c20 636f 6e66  he parallel conf
-000094e0: 6967 7572 652e 2044 6566 6175 6c74 2060  igure. Default `
-000094f0: 6465 6661 756c 745f 6470 6d70 5f63 6f6e  default_dpmp_con
-00009500: 6669 6760 2c0a 2020 2020 2020 2020 2020  fig`,.          
-00009510: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00009520: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00009530: 2020 2020 2061 6e20 696e 7374 616e 6365       an instance
-00009540: 206f 6620 604f 7050 6172 616c 6c65 6c43   of `OpParallelC
-00009550: 6f6e 6669 6760 2077 6974 6820 6465 6661  onfig` with defa
-00009560: 756c 7420 6172 6773 2e0a 0a20 2020 2020  ult args...     
-00009570: 2020 2049 6e70 7574 733a 0a20 2020 2020     Inputs:.     
-00009580: 2020 2020 2020 202d 202a 2a69 6e70 7574         - **input
-00009590: 5f6d 6173 6b2a 2a20 2854 656e 736f 7229  _mask** (Tensor)
-000095a0: 202d 2054 6865 206d 6173 6b20 696e 6469   - The mask indi
-000095b0: 6361 7469 6e67 2077 6865 7468 6572 2065  cating whether e
-000095c0: 6163 6820 706f 7369 7469 6f6e 2069 7320  ach position is 
-000095d0: 6120 7661 6c69 6420 696e 7075 7420 7769  a valid input wi
-000095e0: 7468 0a20 2020 2020 2020 2020 2020 2020  th.             
-000095f0: 2028 6261 7463 685f 7369 7a65 2c20 7365   (batch_size, se
-00009600: 715f 6c65 6e67 7468 292e 0a0a 2020 2020  q_length)...    
-00009610: 2020 2020 4f75 7470 7574 733a 0a20 2020      Outputs:.   
-00009620: 2020 2020 2020 2020 2054 656e 736f 722e           Tensor.
-00009630: 2054 6865 2061 7474 656e 7469 6f6e 206d   The attention m
-00009640: 6173 6b20 6d61 7472 6978 2077 6974 6820  ask matrix with 
-00009650: 7368 6170 6520 2862 6174 6368 5f73 697a  shape (batch_siz
-00009660: 652c 2073 6571 5f6c 656e 6774 682c 2073  e, seq_length, s
-00009670: 6571 5f6c 656e 6774 6829 2e0a 0a20 2020  eq_length)...   
-00009680: 2020 2020 2052 6169 7365 733a 0a20 2020       Raises:.   
-00009690: 2020 2020 2020 2020 2054 7970 6545 7272           TypeErr
-000096a0: 6f72 3a20 6073 6571 5f6c 656e 6774 6860  or: `seq_length`
-000096b0: 2069 7320 6e6f 7420 616e 2069 6e74 6567   is not an integ
-000096c0: 6572 2e0a 2020 2020 2020 2020 2020 2020  er..            
-000096d0: 5661 6c75 6545 7272 6f72 3a20 6073 6571  ValueError: `seq
-000096e0: 5f6c 656e 6774 6860 2069 7320 6e6f 7420  _length` is not 
-000096f0: 6120 706f 7369 7469 7665 2076 616c 7565  a positive value
-00009700: 2e0a 2020 2020 2020 2020 2020 2020 5479  ..            Ty
-00009710: 7065 4572 726f 723a 2060 7061 7261 6c6c  peError: `parall
-00009720: 656c 5f63 6f6e 6669 6760 2069 7320 6e6f  el_config` is no
-00009730: 7420 6120 7375 6263 6c61 7373 206f 6620  t a subclass of 
-00009740: 4f70 5061 7261 6c6c 656c 436f 6e66 6967  OpParallelConfig
-00009750: 2e0a 0a20 2020 2020 2020 2053 7570 706f  ...        Suppo
-00009760: 7274 6564 2050 6c61 7466 6f72 6d73 3a0a  rted Platforms:.
-00009770: 2020 2020 2020 2020 2020 2020 6060 4173              ``As
-00009780: 6365 6e64 6060 2060 6047 5055 6060 0a0a  cend`` ``GPU``..
-00009790: 2020 2020 2020 2020 4578 616d 706c 6573          Examples
-000097a0: 3a0a 2020 2020 2020 2020 2020 2020 3e3e  :.            >>
-000097b0: 3e20 696d 706f 7274 206e 756d 7079 2061  > import numpy a
-000097c0: 7320 6e70 0a20 2020 2020 2020 2020 2020  s np.           
-000097d0: 203e 3e3e 2066 726f 6d20 6d69 6e64 666f   >>> from mindfo
-000097e0: 726d 6572 732e 6d6f 6475 6c65 732e 7472  rmers.modules.tr
-000097f0: 616e 7366 6f72 6d65 7220 696d 706f 7274  ansformer import
-00009800: 2041 7474 656e 7469 6f6e 4d61 736b 4846   AttentionMaskHF
-00009810: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
-00009820: 2066 726f 6d20 6d69 6e64 7370 6f72 6520   from mindspore 
-00009830: 696d 706f 7274 2054 656e 736f 720a 2020  import Tensor.  
-00009840: 2020 2020 2020 2020 2020 3e3e 3e20 6d61            >>> ma
-00009850: 736b 203d 2041 7474 656e 7469 6f6e 4d61  sk = AttentionMa
-00009860: 736b 4846 2873 6571 5f6c 656e 6774 683d  skHF(seq_length=
-00009870: 3429 0a20 2020 2020 2020 2020 2020 203e  4).            >
-00009880: 3e3e 206d 6173 6b5f 6172 7261 7920 3d20  >> mask_array = 
-00009890: 6e70 2e61 7272 6179 285b 5b31 2c20 312c  np.array([[1, 1,
-000098a0: 2031 2c20 305d 5d2c 206e 702e 666c 6f61   1, 0]], np.floa
-000098b0: 7433 3229 0a20 2020 2020 2020 2020 2020  t32).           
-000098c0: 203e 3e3e 2069 6e70 7574 7320 3d20 5465   >>> inputs = Te
-000098d0: 6e73 6f72 286d 6173 6b5f 6172 7261 7929  nsor(mask_array)
-000098e0: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
-000098f0: 2072 6573 203d 206d 6173 6b28 696e 7075   res = mask(inpu
-00009900: 7473 290a 2020 2020 2020 2020 2020 2020  ts).            
-00009910: 3e3e 3e20 7072 696e 7428 7265 7329 0a20  >>> print(res). 
-00009920: 2020 2020 2020 2020 2020 205b 5b5b 312e             [[[1.
-00009930: 2030 2e20 302e 2030 5d0a 2020 2020 2020   0. 0. 0].      
-00009940: 2020 2020 2020 2020 5b31 2e20 312e 2030          [1. 1. 0
-00009950: 2e20 305d 0a20 2020 2020 2020 2020 2020  . 0].           
-00009960: 2020 205b 312e 2031 2e20 312e 2030 5d0a     [1. 1. 1. 0].
-00009970: 2020 2020 2020 2020 2020 2020 2020 5b31                [1
-00009980: 2e20 312e 2031 2e20 315d 5d5d 0a20 2020  . 1. 1. 1]]].   
-00009990: 2022 2222 0a0a 2020 2020 405f 4c6f 6741   """..    @_LogA
-000099a0: 6374 696f 6e4f 6e63 6528 6d5f 6c6f 6767  ctionOnce(m_logg
-000099b0: 6572 3d6c 6f67 6765 722c 206b 6579 3d27  er=logger, key='
-000099c0: 4174 7465 6e74 696f 6e4d 6173 6b48 4627  AttentionMaskHF'
-000099d0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-000099e0: 2020 2020 2020 6e6f 5f77 6172 6e69 6e67        no_warning
-000099f0: 3d5f 6765 745f 7061 7261 6c6c 656c 5f6d  =_get_parallel_m
-00009a00: 6f64 6528 2920 696e 2028 5061 7261 6c6c  ode() in (Parall
-00009a10: 656c 4d6f 6465 2e53 5441 4e44 5f41 4c4f  elMode.STAND_ALO
-00009a20: 4e45 2c29 290a 2020 2020 405f 6172 6773  NE,)).    @_args
-00009a30: 5f74 7970 655f 7661 6c69 6461 746f 725f  _type_validator_
-00009a40: 6368 6563 6b28 7365 715f 6c65 6e67 7468  check(seq_length
-00009a50: 3d56 616c 6964 6174 6f72 2e63 6865 636b  =Validator.check
-00009a60: 5f70 6f73 6974 6976 655f 696e 742c 0a20  _positive_int,. 
-00009a70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00009a80: 2020 2020 2020 2020 2020 2020 2020 2070                 p
-00009a90: 6172 616c 6c65 6c5f 636f 6e66 6967 3d5f  arallel_config=_
-00009aa0: 7661 6c69 645f 7479 7065 5f63 6865 636b  valid_type_check
-00009ab0: 7328 5b4f 7050 6172 616c 6c65 6c43 6f6e  s([OpParallelCon
-00009ac0: 6669 675d 2c20 2241 7474 656e 7469 6f6e  fig], "Attention
-00009ad0: 4d61 736b 4846 2229 290a 2020 2020 6465  MaskHF")).    de
-00009ae0: 6620 5f5f 696e 6974 5f5f 2873 656c 662c  f __init__(self,
-00009af0: 2073 6571 5f6c 656e 6774 682c 2070 6172   seq_length, par
-00009b00: 616c 6c65 6c5f 636f 6e66 6967 3d64 6566  allel_config=def
-00009b10: 6175 6c74 5f64 706d 705f 636f 6e66 6967  ault_dpmp_config
-00009b20: 2c20 636f 6d70 7574 655f 6474 7970 653d  , compute_dtype=
-00009b30: 6d73 7479 7065 2e66 6c6f 6174 3136 293a  mstype.float16):
-00009b40: 0a20 2020 2020 2020 2073 7570 6572 2841  .        super(A
-00009b50: 7474 656e 7469 6f6e 4d61 736b 4846 2c20  ttentionMaskHF, 
-00009b60: 7365 6c66 292e 5f5f 696e 6974 5f5f 2829  self).__init__()
-00009b70: 0a20 2020 2020 2020 2073 656c 662e 7365  .        self.se
-00009b80: 715f 6c65 6e67 7468 203d 2073 6571 5f6c  q_length = seq_l
-00009b90: 656e 6774 680a 2020 2020 2020 2020 7365  ength.        se
-00009ba0: 6c66 2e63 6f6d 7075 7465 5f64 7479 7065  lf.compute_dtype
-00009bb0: 203d 2063 6f6d 7075 7465 5f64 7479 7065   = compute_dtype
-00009bc0: 0a20 2020 2020 2020 2073 656c 662e 6e6f  .        self.no
-00009bd0: 745f 6571 7561 6c20 3d20 502e 4e6f 7445  t_equal = P.NotE
-00009be0: 7175 616c 2829 2e73 6861 7264 2828 2870  qual().shard(((p
-00009bf0: 6172 616c 6c65 6c5f 636f 6e66 6967 2e64  arallel_config.d
-00009c00: 6174 615f 7061 7261 6c6c 656c 2c20 3129  ata_parallel, 1)
-00009c10: 2c20 2829 2929 0a20 2020 2020 2020 2073  , ())).        s
-00009c20: 656c 662e 7265 7368 6170 6520 3d20 502e  elf.reshape = P.
-00009c30: 5265 7368 6170 6528 290a 2020 2020 2020  Reshape().      
-00009c40: 2020 7365 6c66 2e6d 756c 203d 2050 2e42    self.mul = P.B
-00009c50: 6174 6368 4d61 744d 756c 2829 2e73 6861  atchMatMul().sha
-00009c60: 7264 280a 2020 2020 2020 2020 2020 2020  rd(.            
-00009c70: 2828 7061 7261 6c6c 656c 5f63 6f6e 6669  ((parallel_confi
-00009c80: 672e 6461 7461 5f70 6172 616c 6c65 6c2c  g.data_parallel,
-00009c90: 2031 2c20 3129 2c20 2870 6172 616c 6c65   1, 1), (paralle
-00009ca0: 6c5f 636f 6e66 6967 2e64 6174 615f 7061  l_config.data_pa
-00009cb0: 7261 6c6c 656c 2c20 312c 2031 2929 290a  rallel, 1, 1))).
-00009cc0: 2020 2020 2020 2020 7365 6c66 2e65 7870          self.exp
-00009cd0: 616e 645f 6469 6d20 3d20 502e 4578 7061  and_dim = P.Expa
-00009ce0: 6e64 4469 6d73 2829 2e73 6861 7264 2828  ndDims().shard((
-00009cf0: 2831 2c20 3129 2c29 290a 2020 2020 2020  (1, 1),)).      
-00009d00: 2020 6f6e 6573 203d 206e 702e 6f6e 6573    ones = np.ones
-00009d10: 2873 6861 7065 3d28 7365 715f 6c65 6e67  (shape=(seq_leng
-00009d20: 7468 2c20 7365 715f 6c65 6e67 7468 2929  th, seq_length))
-00009d30: 0a20 2020 2020 2020 2023 2044 6566 6175  .        # Defau
-00009d40: 6c74 206c 6f77 6572 2074 7269 616e 676c  lt lower triangl
-00009d50: 6520 6d61 736b 206d 6174 7269 780a 2020  e mask matrix.  
-00009d60: 2020 2020 2020 7365 6c66 2e6c 6f77 6572        self.lower
-00009d70: 5f74 7269 616e 676c 655f 6d61 736b 203d  _triangle_mask =
-00009d80: 2054 656e 736f 7228 6e70 2e74 7269 6c28   Tensor(np.tril(
-00009d90: 6f6e 6573 292c 206d 7374 7970 652e 666c  ones), mstype.fl
-00009da0: 6f61 7433 3229 0a20 2020 2020 2020 2073  oat32).        s
-00009db0: 656c 662e 6d75 6c74 6970 6c79 203d 2050  elf.multiply = P
-00009dc0: 2e4d 756c 2829 2e73 6861 7264 2828 2870  .Mul().shard(((p
-00009dd0: 6172 616c 6c65 6c5f 636f 6e66 6967 2e64  arallel_config.d
-00009de0: 6174 615f 7061 7261 6c6c 656c 2c20 312c  ata_parallel, 1,
-00009df0: 2031 292c 2028 312c 2031 2c20 3129 2929   1), (1, 1, 1)))
-00009e00: 0a0a 2020 2020 6465 6620 636f 6e73 7472  ..    def constr
-00009e10: 7563 7428 7365 6c66 2c20 696e 7075 745f  uct(self, input_
-00009e20: 6d61 736b 293a 0a20 2020 2020 2020 2022  mask):.        "
-00009e30: 2222 466f 7277 6172 6420 7072 6f63 6573  ""Forward proces
-00009e40: 7320 6f66 2074 6865 2041 7474 656e 7469  s of the Attenti
-00009e50: 6f6e 4d61 736b 4846 2222 220a 2020 2020  onMaskHF""".    
-00009e60: 2020 2020 5f63 6865 636b 5f69 6e70 7574      _check_input
-00009e70: 5f64 7479 7065 2846 2e64 7479 7065 2869  _dtype(F.dtype(i
-00009e80: 6e70 7574 5f6d 6173 6b29 2c20 2269 6e70  nput_mask), "inp
-00009e90: 7574 5f6d 6173 6b22 2c0a 2020 2020 2020  ut_mask",.      
-00009ea0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00009eb0: 2020 2020 205b 6d73 7479 7065 2e66 6c6f       [mstype.flo
-00009ec0: 6174 3332 2c20 6d73 7479 7065 2e66 6c6f  at32, mstype.flo
-00009ed0: 6174 3136 2c20 6d73 7479 7065 2e62 666c  at16, mstype.bfl
-00009ee0: 6f61 7431 365d 2c20 7365 6c66 2e63 6c73  oat16], self.cls
-00009ef0: 5f6e 616d 6529 0a20 2020 2020 2020 2069  _name).        i
-00009f00: 6e70 7574 5f6d 6173 6b20 3d20 502e 4361  nput_mask = P.Ca
-00009f10: 7374 2829 2850 2e4f 6e65 734c 696b 6528  st()(P.OnesLike(
-00009f20: 2928 696e 7075 745f 6d61 736b 292c 2073  )(input_mask), s
-00009f30: 656c 662e 636f 6d70 7574 655f 6474 7970  elf.compute_dtyp
-00009f40: 6529 0a20 2020 2020 2020 2069 6e70 7574  e).        input
-00009f50: 5f73 6861 7065 203d 2050 2e53 6861 7065  _shape = P.Shape
-00009f60: 2829 2869 6e70 7574 5f6d 6173 6b29 0a20  ()(input_mask). 
-00009f70: 2020 2020 2020 2073 6861 7065 5f72 6967         shape_rig
-00009f80: 6874 203d 2028 696e 7075 745f 7368 6170  ht = (input_shap
-00009f90: 655b 305d 2c20 312c 2069 6e70 7574 5f73  e[0], 1, input_s
-00009fa0: 6861 7065 5b31 5d29 0a20 2020 2020 2020  hape[1]).       
-00009fb0: 2073 6861 7065 5f6c 6566 7420 3d20 696e   shape_left = in
-00009fc0: 7075 745f 7368 6170 6520 2b20 2831 2c29  put_shape + (1,)
-00009fd0: 0a20 2020 2020 2020 2023 204d 6173 6b20  .        # Mask 
-00009fe0: 7468 6520 7061 6464 6564 2069 6e70 7574  the padded input
-00009ff0: 730a 2020 2020 2020 2020 6d61 736b 5f6c  s.        mask_l
-0000a000: 6566 7420 3d20 7365 6c66 2e72 6573 6861  eft = self.resha
-0000a010: 7065 2869 6e70 7574 5f6d 6173 6b2c 2073  pe(input_mask, s
-0000a020: 6861 7065 5f6c 6566 7429 0a20 2020 2020  hape_left).     
-0000a030: 2020 206d 6173 6b5f 7269 6768 7420 3d20     mask_right = 
-0000a040: 7365 6c66 2e72 6573 6861 7065 2869 6e70  self.reshape(inp
-0000a050: 7574 5f6d 6173 6b2c 2073 6861 7065 5f72  ut_mask, shape_r
-0000a060: 6967 6874 290a 2020 2020 2020 2020 6174  ight).        at
-0000a070: 7465 6e74 696f 6e5f 6d61 736b 203d 2073  tention_mask = s
-0000a080: 656c 662e 6d75 6c28 6d61 736b 5f6c 6566  elf.mul(mask_lef
-0000a090: 742c 206d 6173 6b5f 7269 6768 7429 0a20  t, mask_right). 
-0000a0a0: 2020 2020 2020 206c 6f77 6572 5f74 7261         lower_tra
-0000a0b0: 6961 6e67 6c65 203d 2073 656c 662e 6578  iangle = self.ex
-0000a0c0: 7061 6e64 5f64 696d 2873 656c 662e 6c6f  pand_dim(self.lo
-0000a0d0: 7765 725f 7472 6961 6e67 6c65 5f6d 6173  wer_triangle_mas
-0000a0e0: 6b2c 2030 290a 2020 2020 2020 2020 2320  k, 0).        # 
-0000a0f0: 7468 6520 7265 7475 726e 6564 2073 6861  the returned sha
-0000a100: 7065 2069 7320 5b62 732c 2073 6571 5f6c  pe is [bs, seq_l
-0000a110: 656e 6774 682c 2073 6571 5f6c 656e 6774  ength, seq_lengt
-0000a120: 685d 0a20 2020 2020 2020 2061 7474 656e  h].        atten
-0000a130: 7469 6f6e 5f6d 6173 6b20 3d20 7365 6c66  tion_mask = self
-0000a140: 2e6d 756c 7469 706c 7928 0a20 2020 2020  .multiply(.     
-0000a150: 2020 2020 2020 2061 7474 656e 7469 6f6e         attention
-0000a160: 5f6d 6173 6b2c 206c 6f77 6572 5f74 7261  _mask, lower_tra
-0000a170: 6961 6e67 6c65 290a 2020 2020 2020 2020  iangle).        
-0000a180: 7265 7475 726e 2061 7474 656e 7469 6f6e  return attention
-0000a190: 5f6d 6173 6b0a 0a0a 636c 6173 7320 4c6f  _mask...class Lo
-0000a1a0: 7765 7254 7269 616e 6775 6c61 724d 6173  werTriangularMas
-0000a1b0: 6b57 6974 6844 796e 616d 6963 2843 656c  kWithDynamic(Cel
-0000a1c0: 6c29 3a0a 2020 2020 7222 2222 0a20 2020  l):.    r""".   
-0000a1d0: 2020 2020 2020 2020 2047 6574 2074 6865           Get the
-0000a1e0: 2053 7472 6963 746c 7920 4c6f 7765 7220   Strictly Lower 
-0000a1f0: 7472 6961 6e67 756c 6172 206d 6174 7269  triangular matri
-0000a200: 7820 6672 6f6d 2074 6865 2069 6e70 7574  x from the input
-0000a210: 5f69 6473 2e0a 2020 2020 2222 220a 0a20  _ids..    """.. 
-0000a220: 2020 2040 5f4c 6f67 4163 7469 6f6e 4f6e     @_LogActionOn
-0000a230: 6365 286d 5f6c 6f67 6765 723d 6c6f 6767  ce(m_logger=logg
-0000a240: 6572 2c20 6b65 793d 2741 7474 656e 7469  er, key='Attenti
-0000a250: 6f6e 4d61 736b 272c 0a20 2020 2020 2020  onMask',.       
-0000a260: 2020 2020 2020 2020 2020 2020 206e 6f5f               no_
-0000a270: 7761 726e 696e 673d 5f67 6574 5f70 6172  warning=_get_par
-0000a280: 616c 6c65 6c5f 6d6f 6465 2829 2069 6e20  allel_mode() in 
-0000a290: 2850 6172 616c 6c65 6c4d 6f64 652e 5354  (ParallelMode.ST
-0000a2a0: 414e 445f 414c 4f4e 452c 2929 0a20 2020  AND_ALONE,)).   
-0000a2b0: 2064 6566 205f 5f69 6e69 745f 5f28 7365   def __init__(se
-0000a2c0: 6c66 2c20 7365 715f 6c65 6e67 7468 2c20  lf, seq_length, 
-0000a2d0: 636f 6d70 7574 655f 7479 7065 3d6d 7374  compute_type=mst
-0000a2e0: 7970 652e 666c 6f61 7431 362c 0a20 2020  ype.float16,.   
-0000a2f0: 2020 2020 2020 2020 2020 2020 2020 6973                is
-0000a300: 5f64 796e 616d 6963 3d46 616c 7365 2c20  _dynamic=False, 
-0000a310: 7061 645f 746f 6b65 6e5f 6964 3d30 2c20  pad_token_id=0, 
-0000a320: 7573 655f 666c 6173 685f 6174 7465 6e74  use_flash_attent
-0000a330: 696f 6e3d 4661 6c73 652c 0a20 2020 2020  ion=False,.     
-0000a340: 2020 2020 2020 2020 2020 2020 7573 655f              use_
-0000a350: 7072 6f6d 7074 5f66 6c61 7368 5f61 7474  prompt_flash_att
-0000a360: 656e 7469 6f6e 3d46 616c 7365 2c20 7573  ention=False, us
-0000a370: 655f 696e 6372 655f 666c 6173 685f 6174  e_incre_flash_at
-0000a380: 7465 6e74 696f 6e3d 4661 6c73 6529 3a0a  tention=False):.
-0000a390: 2020 2020 2020 2020 7375 7065 7228 292e          super().
-0000a3a0: 5f5f 696e 6974 5f5f 2829 0a20 2020 2020  __init__().     
-0000a3b0: 2020 2073 656c 662e 6474 7970 6520 3d20     self.dtype = 
-0000a3c0: 636f 6d70 7574 655f 7479 7065 0a20 2020  compute_type.   
-0000a3d0: 2020 2020 2073 656c 662e 6973 5f64 796e       self.is_dyn
-0000a3e0: 616d 6963 203d 2069 735f 6479 6e61 6d69  amic = is_dynami
-0000a3f0: 630a 2020 2020 2020 2020 7365 6c66 2e70  c.        self.p
-0000a400: 6164 5f74 6f6b 656e 5f69 6420 3d20 7061  ad_token_id = pa
-0000a410: 645f 746f 6b65 6e5f 6964 0a20 2020 2020  d_token_id.     
-0000a420: 2020 2073 656c 662e 7573 655f 666c 6173     self.use_flas
-0000a430: 685f 6174 7465 6e74 696f 6e20 3d20 7573  h_attention = us
-0000a440: 655f 666c 6173 685f 6174 7465 6e74 696f  e_flash_attentio
-0000a450: 6e0a 2020 2020 2020 2020 7365 6c66 2e75  n.        self.u
-0000a460: 7365 5f70 726f 6d70 745f 666c 6173 685f  se_prompt_flash_
-0000a470: 6174 7465 6e74 696f 6e20 3d20 7573 655f  attention = use_
-0000a480: 7072 6f6d 7074 5f66 6c61 7368 5f61 7474  prompt_flash_att
-0000a490: 656e 7469 6f6e 0a20 2020 2020 2020 2073  ention.        s
-0000a4a0: 656c 662e 7573 655f 696e 6372 655f 666c  elf.use_incre_fl
-0000a4b0: 6173 685f 6174 7465 6e74 696f 6e20 3d20  ash_attention = 
-0000a4c0: 7573 655f 696e 6372 655f 666c 6173 685f  use_incre_flash_
-0000a4d0: 6174 7465 6e74 696f 6e0a 2020 2020 2020  attention.      
-0000a4e0: 2020 7365 6c66 2e69 735f 6669 7273 745f    self.is_first_
-0000a4f0: 6974 6572 6174 696f 6e20 3d20 5472 7565  iteration = True
-0000a500: 0a20 2020 2020 2020 2073 656c 662e 6d75  .        self.mu
-0000a510: 6c74 6970 6c79 5f64 6174 6120 3d20 5465  ltiply_data = Te
-0000a520: 6e73 6f72 285b 2d31 3030 3030 2e30 5d2c  nsor([-10000.0],
-0000a530: 2064 7479 7065 3d63 6f6d 7075 7465 5f74   dtype=compute_t
-0000a540: 7970 6529 0a20 2020 2020 2020 2073 656c  ype).        sel
-0000a550: 662e 6f6e 6520 3d20 5465 6e73 6f72 285b  f.one = Tensor([
-0000a560: 312e 305d 2c20 6474 7970 653d 636f 6d70  1.0], dtype=comp
-0000a570: 7574 655f 7479 7065 290a 2020 2020 2020  ute_type).      
-0000a580: 2020 7365 6c66 2e6c 6f77 6572 5f74 7269    self.lower_tri
-0000a590: 616e 676c 655f 6d61 736b 203d 206f 7073  angle_mask = ops
-0000a5a0: 2e63 6173 7428 5465 6e73 6f72 286e 702e  .cast(Tensor(np.
-0000a5b0: 7472 696c 286e 702e 6f6e 6573 2873 6861  tril(np.ones(sha
-0000a5c0: 7065 3d28 7365 715f 6c65 6e67 7468 2c20  pe=(seq_length, 
-0000a5d0: 7365 715f 6c65 6e67 7468 2929 292c 206d  seq_length))), m
-0000a5e0: 7374 7970 652e 666c 6f61 7433 3229 2c0a  stype.float32),.
-0000a5f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000a600: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000a610: 2020 2020 2020 2020 2020 2020 636f 6d70              comp
-0000a620: 7574 655f 7479 7065 290a 0a20 2020 2020  ute_type)..     
-0000a630: 2020 2073 656c 662e 7368 6170 6520 3d20     self.shape = 
-0000a640: 502e 5368 6170 6528 290a 2020 2020 2020  P.Shape().      
-0000a650: 2020 7365 6c66 2e63 6173 7420 3d20 502e    self.cast = P.
-0000a660: 4361 7374 2829 0a20 2020 2020 2020 2073  Cast().        s
-0000a670: 656c 662e 7265 7368 6170 6520 3d20 502e  elf.reshape = P.
-0000a680: 5265 7368 6170 6528 290a 2020 2020 2020  Reshape().      
-0000a690: 2020 7365 6c66 2e6e 6f74 5f65 7175 616c    self.not_equal
-0000a6a0: 203d 2050 2e4e 6f74 4571 7561 6c28 290a   = P.NotEqual().
-0000a6b0: 2020 2020 2020 2020 7365 6c66 2e6c 6573          self.les
-0000a6c0: 735f 6571 7561 6c20 3d20 502e 4c65 7373  s_equal = P.Less
-0000a6d0: 4571 7561 6c28 290a 2020 2020 2020 2020  Equal().        
-0000a6e0: 7365 6c66 2e62 6d6d 203d 2050 2e42 6174  self.bmm = P.Bat
-0000a6f0: 6368 4d61 744d 756c 2829 0a20 2020 2020  chMatMul().     
-0000a700: 2020 2073 656c 662e 6578 7061 6e64 5f64     self.expand_d
-0000a710: 696d 203d 2050 2e45 7870 616e 6444 696d  im = P.ExpandDim
-0000a720: 7328 290a 2020 2020 2020 2020 7365 6c66  s().        self
-0000a730: 2e73 6c69 6365 203d 2050 2e53 7472 6964  .slice = P.Strid
-0000a740: 6564 536c 6963 6528 290a 2020 2020 2020  edSlice().      
-0000a750: 2020 7365 6c66 2e6d 756c 203d 2050 2e4d    self.mul = P.M
-0000a760: 756c 2829 0a20 2020 2020 2020 2073 656c  ul().        sel
-0000a770: 662e 7375 6220 3d20 502e 5375 6228 290a  f.sub = P.Sub().
-0000a780: 2020 2020 2020 2020 7365 6c66 2e6d 756c          self.mul
-0000a790: 5f70 6f73 7420 3d20 502e 4d75 6c28 290a  _post = P.Mul().
-0000a7a0: 2020 2020 2020 2020 7365 6c66 2e65 7870          self.exp
-0000a7b0: 616e 645f 6469 6d5f 706f 7374 203d 2050  and_dim_post = P
-0000a7c0: 2e45 7870 616e 6444 696d 7328 290a 0a20  .ExpandDims().. 
-0000a7d0: 2020 2064 6566 2063 6f6e 7374 7275 6374     def construct
-0000a7e0: 2873 656c 662c 2074 6f6b 656e 733d 4e6f  (self, tokens=No
-0000a7f0: 6e65 2c20 6d61 736b 733d 4e6f 6e65 293a  ne, masks=None):
-0000a800: 0a20 2020 2020 2020 2022 2222 466f 7277  .        """Forw
-0000a810: 6172 6420 7072 6f63 6573 7320 6f66 2074  ard process of t
-0000a820: 6865 2043 6175 7361 6c4d 6173 6b22 2222  he CausalMask"""
-0000a830: 0a20 2020 2020 2020 2069 6620 746f 6b65  .        if toke
-0000a840: 6e73 2069 7320 6e6f 7420 4e6f 6e65 3a0a  ns is not None:.
-0000a850: 2020 2020 2020 2020 2020 2020 6273 203d              bs =
-0000a860: 2073 656c 662e 7368 6170 6528 746f 6b65   self.shape(toke
-0000a870: 6e73 295b 305d 0a20 2020 2020 2020 2020  ns)[0].         
-0000a880: 2020 2073 6571 5f6c 656e 203d 2073 656c     seq_len = sel
-0000a890: 662e 7368 6170 6528 746f 6b65 6e73 295b  f.shape(tokens)[
-0000a8a0: 315d 0a20 2020 2020 2020 2020 2020 2069  1].            i
-0000a8b0: 6e70 7574 5f6d 6173 6b20 3d20 7365 6c66  nput_mask = self
-0000a8c0: 2e63 6173 7428 7365 6c66 2e6e 6f74 5f65  .cast(self.not_e
-0000a8d0: 7175 616c 2874 6f6b 656e 732c 2073 656c  qual(tokens, sel
-0000a8e0: 662e 7061 645f 746f 6b65 6e5f 6964 292c  f.pad_token_id),
-0000a8f0: 2073 656c 662e 6474 7970 6529 0a20 2020   self.dtype).   
-0000a900: 2020 2020 2065 6c73 653a 0a20 2020 2020       else:.     
-0000a910: 2020 2020 2020 2062 7320 3d20 7365 6c66         bs = self
-0000a920: 2e73 6861 7065 286d 6173 6b73 295b 305d  .shape(masks)[0]
-0000a930: 0a20 2020 2020 2020 2020 2020 2073 6571  .            seq
-0000a940: 5f6c 656e 203d 2073 656c 662e 7368 6170  _len = self.shap
-0000a950: 6528 6d61 736b 7329 5b31 5d0a 2020 2020  e(masks)[1].    
-0000a960: 2020 2020 2020 2020 696e 7075 745f 6d61          input_ma
-0000a970: 736b 203d 2073 656c 662e 6361 7374 286d  sk = self.cast(m
-0000a980: 6173 6b73 2c20 7365 6c66 2e64 7479 7065  asks, self.dtype
-0000a990: 290a 2020 2020 2020 2020 7368 6170 655f  ).        shape_
-0000a9a0: 7269 6768 7420 3d20 2862 732c 2031 2c20  right = (bs, 1, 
-0000a9b0: 7365 715f 6c65 6e29 0a0a 2020 2020 2020  seq_len)..      
-0000a9c0: 2020 2320 4d61 736b 2074 6865 2070 6164    # Mask the pad
-0000a9d0: 6465 6420 696e 7075 7473 0a20 2020 2020  ded inputs.     
-0000a9e0: 2020 206d 6173 6b5f 7269 6768 7420 3d20     mask_right = 
-0000a9f0: 7365 6c66 2e72 6573 6861 7065 2869 6e70  self.reshape(inp
-0000aa00: 7574 5f6d 6173 6b2c 2073 6861 7065 5f72  ut_mask, shape_r
-0000aa10: 6967 6874 290a 2020 2020 2020 2020 6174  ight).        at
-0000aa20: 7465 6e74 696f 6e5f 6d61 736b 203d 206d  tention_mask = m
-0000aa30: 6173 6b5f 7269 6768 740a 2020 2020 2020  ask_right.      
-0000aa40: 2020 6966 206e 6f74 2073 656c 662e 6973    if not self.is
-0000aa50: 5f64 796e 616d 6963 3a0a 2020 2020 2020  _dynamic:.      
-0000aa60: 2020 2020 2020 6c6f 7765 725f 7472 6961        lower_tria
-0000aa70: 6e67 6c65 203d 2073 656c 662e 6578 7061  ngle = self.expa
-0000aa80: 6e64 5f64 696d 2873 656c 662e 6c6f 7765  nd_dim(self.lowe
-0000aa90: 725f 7472 6961 6e67 6c65 5f6d 6173 6b2c  r_triangle_mask,
-0000aaa0: 2030 290a 2020 2020 2020 2020 656c 7365   0).        else
-0000aab0: 3a0a 2020 2020 2020 2020 2020 2020 6c6f  :.            lo
-0000aac0: 7765 725f 7472 6961 6e67 6c65 5f6d 6173  wer_triangle_mas
-0000aad0: 6b20 3d20 7365 6c66 2e73 6c69 6365 2873  k = self.slice(s
-0000aae0: 656c 662e 6c6f 7765 725f 7472 6961 6e67  elf.lower_triang
-0000aaf0: 6c65 5f6d 6173 6b2c 2028 302c 2030 292c  le_mask, (0, 0),
-0000ab00: 2028 7365 715f 6c65 6e2c 2073 6571 5f6c   (seq_len, seq_l
-0000ab10: 656e 292c 2028 312c 2031 2929 0a20 2020  en), (1, 1)).   
-0000ab20: 2020 2020 2020 2020 206c 6f77 6572 5f74           lower_t
-0000ab30: 7269 616e 676c 6520 3d20 7365 6c66 2e65  riangle = self.e
-0000ab40: 7870 616e 645f 6469 6d28 6c6f 7765 725f  xpand_dim(lower_
-0000ab50: 7472 6961 6e67 6c65 5f6d 6173 6b2c 2030  triangle_mask, 0
-0000ab60: 290a 0a20 2020 2020 2020 2023 2074 6865  )..        # the
-0000ab70: 2072 6574 7572 6e65 6420 7368 6170 6520   returned shape 
-0000ab80: 6973 205b 6273 2c20 312c 2073 6571 5f6c  is [bs, 1, seq_l
-0000ab90: 656e 6774 682c 2073 6571 5f6c 656e 6774  ength, seq_lengt
-0000aba0: 685d 0a20 2020 2020 2020 2061 7474 656e  h].        atten
-0000abb0: 7469 6f6e 5f6d 6173 6b20 3d20 7365 6c66  tion_mask = self
-0000abc0: 2e6d 756c 2861 7474 656e 7469 6f6e 5f6d  .mul(attention_m
-0000abd0: 6173 6b2c 206c 6f77 6572 5f74 7269 616e  ask, lower_trian
-0000abe0: 676c 6529 0a20 2020 2020 2020 2061 7474  gle).        att
-0000abf0: 656e 7469 6f6e 5f6d 6173 6b20 3d20 7365  ention_mask = se
-0000ac00: 6c66 2e73 7562 2873 656c 662e 6f6e 652c  lf.sub(self.one,
-0000ac10: 2061 7474 656e 7469 6f6e 5f6d 6173 6b29   attention_mask)
-0000ac20: 0a20 2020 2020 2020 2061 7474 656e 7469  .        attenti
-0000ac30: 6f6e 5f6d 6173 6b20 3d20 7365 6c66 2e65  on_mask = self.e
-0000ac40: 7870 616e 645f 6469 6d5f 706f 7374 2861  xpand_dim_post(a
-0000ac50: 7474 656e 7469 6f6e 5f6d 6173 6b2c 2031  ttention_mask, 1
-0000ac60: 290a 2020 2020 2020 2020 6966 206e 6f74  ).        if not
-0000ac70: 2073 656c 662e 7573 655f 666c 6173 685f   self.use_flash_
-0000ac80: 6174 7465 6e74 696f 6e20 616e 6420 6e6f  attention and no
-0000ac90: 7420 7365 6c66 2e75 7365 5f70 726f 6d70  t self.use_promp
-0000aca0: 745f 666c 6173 685f 6174 7465 6e74 696f  t_flash_attentio
-0000acb0: 6e3a 0a20 2020 2020 2020 2020 2020 2061  n:.            a
-0000acc0: 7474 656e 7469 6f6e 5f6d 6173 6b20 3d20  ttention_mask = 
-0000acd0: 7365 6c66 2e6d 756c 5f70 6f73 7428 6174  self.mul_post(at
-0000ace0: 7465 6e74 696f 6e5f 6d61 736b 2c20 7365  tention_mask, se
-0000acf0: 6c66 2e6d 756c 7469 706c 795f 6461 7461  lf.multiply_data
-0000ad00: 290a 2020 2020 2020 2020 656c 6966 2073  ).        elif s
-0000ad10: 656c 662e 7573 655f 666c 6173 685f 6174  elf.use_flash_at
-0000ad20: 7465 6e74 696f 6e20 6f72 2073 656c 662e  tention or self.
-0000ad30: 7573 655f 7072 6f6d 7074 5f66 6c61 7368  use_prompt_flash
-0000ad40: 5f61 7474 656e 7469 6f6e 3a0a 2020 2020  _attention:.    
-0000ad50: 2020 2020 2020 2020 6174 7465 6e74 696f          attentio
-0000ad60: 6e5f 6d61 736b 203d 2073 656c 662e 6361  n_mask = self.ca
-0000ad70: 7374 2861 7474 656e 7469 6f6e 5f6d 6173  st(attention_mas
-0000ad80: 6b2c 206d 7374 7970 652e 7569 6e74 3829  k, mstype.uint8)
-0000ad90: 0a20 2020 2020 2020 2072 6574 7572 6e20  .        return 
-0000ada0: 6174 7465 6e74 696f 6e5f 6d61 736b 0a0a  attention_mask..
-0000adb0: 2020 2020 6465 6620 696e 6372 656d 656e      def incremen
-0000adc0: 7428 7365 6c66 2c20 7365 715f 7261 6e67  t(self, seq_rang
-0000add0: 652c 2062 6174 6368 5f76 616c 6964 5f6c  e, batch_valid_l
-0000ade0: 656e 6774 682c 207a 6163 7469 7661 7465  ength, zactivate
-0000adf0: 5f6c 656e 3d4e 6f6e 6529 3a0a 2020 2020  _len=None):.    
-0000ae00: 2020 2020 2222 2247 6574 206d 6173 6b20      """Get mask 
-0000ae10: 666f 7220 696e 6372 656d 656e 7461 6c20  for incremental 
-0000ae20: 696e 6665 7265 6e63 652e 2222 220a 2020  inference.""".  
-0000ae30: 2020 2020 2020 6966 207a 6163 7469 7661        if zactiva
-0000ae40: 7465 5f6c 656e 2069 7320 6e6f 7420 4e6f  te_len is not No
-0000ae50: 6e65 3a0a 2020 2020 2020 2020 2020 2020  ne:.            
-0000ae60: 7365 715f 7261 6e67 6520 3d20 7365 6c66  seq_range = self
-0000ae70: 2e73 6c69 6365 2873 6571 5f72 616e 6765  .slice(seq_range
-0000ae80: 2c20 2830 2c20 302c 2030 292c 2028 312c  , (0, 0, 0), (1,
-0000ae90: 2031 2c20 7365 6c66 2e73 6861 7065 287a   1, self.shape(z
-0000aea0: 6163 7469 7661 7465 5f6c 656e 295b 305d  activate_len)[0]
-0000aeb0: 292c 2028 312c 2031 2c20 3129 290a 2020  ), (1, 1, 1)).  
-0000aec0: 2020 2020 2020 6d61 736b 203d 2073 656c        mask = sel
-0000aed0: 662e 6c65 7373 5f65 7175 616c 2873 656c  f.less_equal(sel
-0000aee0: 662e 7265 7368 6170 6528 7365 715f 7261  f.reshape(seq_ra
-0000aef0: 6e67 652c 2028 312c 2031 2c20 2d31 2929  nge, (1, 1, -1))
-0000af00: 2c20 7365 6c66 2e72 6573 6861 7065 2862  , self.reshape(b
-0000af10: 6174 6368 5f76 616c 6964 5f6c 656e 6774  atch_valid_lengt
-0000af20: 682c 2028 2d31 2c20 312c 2031 2929 290a  h, (-1, 1, 1))).
-0000af30: 2020 2020 2020 2020 6d61 736b 203d 2073          mask = s
-0000af40: 656c 662e 6361 7374 286d 6173 6b2c 2073  elf.cast(mask, s
-0000af50: 656c 662e 6474 7970 6529 0a20 2020 2020  elf.dtype).     
-0000af60: 2020 206d 6173 6b20 3d20 7365 6c66 2e73     mask = self.s
-0000af70: 7562 2873 656c 662e 6f6e 652c 206d 6173  ub(self.one, mas
-0000af80: 6b29 0a20 2020 2020 2020 206d 6173 6b20  k).        mask 
-0000af90: 3d20 7365 6c66 2e65 7870 616e 645f 6469  = self.expand_di
-0000afa0: 6d5f 706f 7374 286d 6173 6b2c 2031 290a  m_post(mask, 1).
-0000afb0: 2020 2020 2020 2020 6966 206e 6f74 2073          if not s
-0000afc0: 656c 662e 7573 655f 696e 6372 655f 666c  elf.use_incre_fl
-0000afd0: 6173 685f 6174 7465 6e74 696f 6e3a 0a20  ash_attention:. 
-0000afe0: 2020 2020 2020 2020 2020 206d 6173 6b20             mask 
-0000aff0: 3d20 7365 6c66 2e6d 756c 5f70 6f73 7428  = self.mul_post(
-0000b000: 6d61 736b 2c20 7365 6c66 2e6d 756c 7469  mask, self.multi
-0000b010: 706c 795f 6461 7461 290a 2020 2020 2020  ply_data).      
-0000b020: 2020 7265 7475 726e 206d 6173 6b0a 0a20    return mask.. 
-0000b030: 2020 2064 6566 2069 6e63 7265 6d65 6e74     def increment
-0000b040: 5f73 6c69 6365 2873 656c 662c 2073 6571  _slice(self, seq
-0000b050: 5f72 616e 6765 2c20 7365 715f 6c65 6e67  _range, seq_leng
-0000b060: 7468 2c20 6261 7463 685f 7661 6c69 645f  th, batch_valid_
-0000b070: 6c65 6e67 7468 2c20 7a61 6374 6976 6174  length, zactivat
-0000b080: 655f 6c65 6e3d 4e6f 6e65 293a 0a20 2020  e_len=None):.   
-0000b090: 2020 2020 2022 2222 4765 7420 6d61 736b       """Get mask
-0000b0a0: 2066 6f72 2069 6e63 7265 6d65 6e74 616c   for incremental
-0000b0b0: 2069 6e66 6572 656e 6365 2061 6e64 2061   inference and a
-0000b0c0: 7070 6c79 2073 6c69 6365 2e22 2222 0a20  pply slice.""". 
-0000b0d0: 2020 2020 2020 2069 6620 7a61 6374 6976         if zactiv
-0000b0e0: 6174 655f 6c65 6e20 6973 206e 6f74 204e  ate_len is not N
-0000b0f0: 6f6e 653a 0a20 2020 2020 2020 2020 2020  one:.           
-0000b100: 2073 6571 5f72 616e 6765 5f6d 6173 6b20   seq_range_mask 
-0000b110: 3d20 7365 6c66 2e73 6c69 6365 2873 6571  = self.slice(seq
-0000b120: 5f72 616e 6765 2c20 2830 2c20 302c 2030  _range, (0, 0, 0
-0000b130: 292c 2028 312c 2031 2c20 7365 6c66 2e73  ), (1, 1, self.s
-0000b140: 6861 7065 287a 6163 7469 7661 7465 5f6c  hape(zactivate_l
-0000b150: 656e 295b 305d 292c 2028 312c 2031 2c20  en)[0]), (1, 1, 
-0000b160: 3129 290a 2020 2020 2020 2020 656c 7365  1)).        else
-0000b170: 3a0a 2020 2020 2020 2020 2020 2020 7365  :.            se
-0000b180: 715f 7261 6e67 655f 6d61 736b 203d 2073  q_range_mask = s
-0000b190: 656c 662e 736c 6963 6528 7365 715f 7261  elf.slice(seq_ra
-0000b1a0: 6e67 652c 2028 302c 2030 2c20 3029 2c20  nge, (0, 0, 0), 
-0000b1b0: 2831 2c20 312c 2073 6571 5f6c 656e 6774  (1, 1, seq_lengt
-0000b1c0: 6829 2c20 2831 2c20 312c 2031 2929 0a20  h), (1, 1, 1)). 
-0000b1d0: 2020 2020 2020 206d 6173 6b20 3d20 7365         mask = se
-0000b1e0: 6c66 2e6c 6573 735f 6571 7561 6c28 7365  lf.less_equal(se
-0000b1f0: 6c66 2e72 6573 6861 7065 2873 6571 5f72  lf.reshape(seq_r
-0000b200: 616e 6765 5f6d 6173 6b2c 2028 312c 2031  ange_mask, (1, 1
-0000b210: 2c20 2d31 2929 2c20 7365 6c66 2e72 6573  , -1)), self.res
-0000b220: 6861 7065 2862 6174 6368 5f76 616c 6964  hape(batch_valid
-0000b230: 5f6c 656e 6774 682c 2028 2d31 2c20 312c  _length, (-1, 1,
-0000b240: 2031 2929 290a 2020 2020 2020 2020 6d61   1))).        ma
-0000b250: 736b 203d 2073 656c 662e 6361 7374 286d  sk = self.cast(m
-0000b260: 6173 6b2c 2073 656c 662e 6474 7970 6529  ask, self.dtype)
-0000b270: 0a20 2020 2020 2020 206d 6173 6b20 3d20  .        mask = 
-0000b280: 7365 6c66 2e73 7562 2873 656c 662e 6f6e  self.sub(self.on
-0000b290: 652c 206d 6173 6b29 0a20 2020 2020 2020  e, mask).       
-0000b2a0: 206d 6173 6b20 3d20 7365 6c66 2e65 7870   mask = self.exp
-0000b2b0: 616e 645f 6469 6d5f 706f 7374 286d 6173  and_dim_post(mas
-0000b2c0: 6b2c 2031 290a 2020 2020 2020 2020 6966  k, 1).        if
-0000b2d0: 206e 6f74 2073 656c 662e 7573 655f 696e   not self.use_in
-0000b2e0: 6372 655f 666c 6173 685f 6174 7465 6e74  cre_flash_attent
-0000b2f0: 696f 6e3a 0a20 2020 2020 2020 2020 2020  ion:.           
-0000b300: 206d 6173 6b20 3d20 7365 6c66 2e6d 756c   mask = self.mul
-0000b310: 5f70 6f73 7428 6d61 736b 2c20 7365 6c66  _post(mask, self
-0000b320: 2e6d 756c 7469 706c 795f 6461 7461 290a  .multiply_data).
-0000b330: 2020 2020 2020 2020 7265 7475 726e 206d          return m
-0000b340: 6173 6b0a 0a20 2020 2064 6566 2073 6861  ask..    def sha
-0000b350: 7264 2873 656c 662c 2070 6172 616c 6c65  rd(self, paralle
-0000b360: 6c5f 636f 6e66 6967 293a 0a20 2020 2020  l_config):.     
-0000b370: 2020 2064 7020 3d20 7061 7261 6c6c 656c     dp = parallel
-0000b380: 5f63 6f6e 6669 672e 6461 7461 5f70 6172  _config.data_par
-0000b390: 616c 6c65 6c0a 2020 2020 2020 2020 7365  allel.        se
-0000b3a0: 6c66 2e6e 6f74 5f65 7175 616c 2e73 6861  lf.not_equal.sha
-0000b3b0: 7264 2828 2864 702c 2031 292c 2028 2929  rd(((dp, 1), ())
-0000b3c0: 290a 2020 2020 2020 2020 7365 6c66 2e62  ).        self.b
-0000b3d0: 6d6d 2e73 6861 7264 2828 2864 702c 2031  mm.shard(((dp, 1
-0000b3e0: 2c20 3129 2c20 2864 702c 2031 2c20 3129  , 1), (dp, 1, 1)
-0000b3f0: 2929 0a20 2020 2020 2020 2073 656c 662e  )).        self.
-0000b400: 6578 7061 6e64 5f64 696d 2e73 6861 7264  expand_dim.shard
-0000b410: 2828 2831 2c20 3129 2c29 290a 2020 2020  (((1, 1),)).    
-0000b420: 2020 2020 7365 6c66 2e6d 756c 2e73 6861      self.mul.sha
-0000b430: 7264 2828 2864 702c 2031 2c20 3129 2c20  rd(((dp, 1, 1), 
-0000b440: 2831 2c20 312c 2031 2929 290a 2020 2020  (1, 1, 1))).    
-0000b450: 2020 2020 7365 6c66 2e6c 6573 735f 6571      self.less_eq
-0000b460: 7561 6c2e 7368 6172 6428 2828 312c 2031  ual.shard(((1, 1
-0000b470: 2c20 3129 2c20 2831 2c20 312c 2031 2929  , 1), (1, 1, 1))
-0000b480: 290a 2020 2020 2020 2020 7365 6c66 2e73  ).        self.s
-0000b490: 7562 2e73 6861 7264 2828 2831 2c29 2c20  ub.shard(((1,), 
-0000b4a0: 2864 702c 2031 2c20 3129 2929 0a20 2020  (dp, 1, 1))).   
-0000b4b0: 2020 2020 2073 656c 662e 6d75 6c5f 706f       self.mul_po
-0000b4c0: 7374 2e73 6861 7264 2828 2864 702c 2031  st.shard(((dp, 1
-0000b4d0: 2c20 312c 2031 292c 2028 312c 2929 290a  , 1, 1), (1,))).
-0000b4e0: 2020 2020 2020 2020 7365 6c66 2e65 7870          self.exp
-0000b4f0: 616e 645f 6469 6d5f 706f 7374 2e73 6861  and_dim_post.sha
-0000b500: 7264 2828 2864 702c 2031 2c20 3129 2c29  rd(((dp, 1, 1),)
-0000b510: 290a 0a0a 636c 6173 7320 566f 6361 6245  )...class VocabE
-0000b520: 6d62 6564 6469 6e67 2843 656c 6c29 3a0a  mbedding(Cell):.
-0000b530: 2020 2020 2222 220a 2020 2020 2020 2020      """.        
-0000b540: 5468 6520 656d 6265 6464 696e 6720 6c6f  The embedding lo
-0000b550: 6f6b 7570 2074 6162 6c65 2066 726f 6d20  okup table from 
-0000b560: 7468 6520 302d 7468 2064 696d 206f 6620  the 0-th dim of 
-0000b570: 7468 6520 7061 7261 6d65 7465 7220 7461  the parameter ta
-0000b580: 626c 652e 2057 6865 6e20 7468 6520 7061  ble. When the pa
-0000b590: 7261 6c6c 656c 5f63 6f6e 6669 672e 766f  rallel_config.vo
-0000b5a0: 6361 625f 656d 625f 6470 2069 730a 2020  cab_emb_dp is.  
-0000b5b0: 2020 2020 2020 5472 7565 2061 6e64 2069        True and i
-0000b5c0: 6e20 7468 6520 6041 5554 4f5f 5041 5241  n the `AUTO_PARA
-0000b5d0: 4c4c 454c 6020 6d6f 6465 2c20 7468 6520  LLEL` mode, the 
-0000b5e0: 656d 6265 6464 696e 6720 6c6f 6f6b 7570  embedding lookup
-0000b5f0: 2077 696c 6c20 6265 2074 7261 696e 6564   will be trained
-0000b600: 2062 7920 7468 6520 6461 7461 2070 6172   by the data par
-0000b610: 616c 6c65 6c20 7761 792c 2061 7320 7468  allel way, as th
-0000b620: 650a 2020 2020 2020 2020 7061 7261 6d65  e.        parame
-0000b630: 7465 7273 2077 696c 6c20 6265 2072 6570  ters will be rep
-0000b640: 6561 7465 6420 6f6e 2065 6163 6820 6465  eated on each de
-0000b650: 7669 6365 2e20 4966 2066 616c 7365 2c20  vice. If false, 
-0000b660: 7468 6520 656d 6265 6464 696e 6720 7461  the embedding ta
-0000b670: 626c 6520 7769 6c6c 2062 6520 7368 6172  ble will be shar
-0000b680: 6465 6420 696e 746f 206e 2070 6172 7473  ded into n parts
-0000b690: 2061 740a 2020 2020 2020 2020 7468 6520   at.        the 
-0000b6a0: 302d 7468 2064 696d 656e 7369 6f6e 206f  0-th dimension o
-0000b6b0: 6620 7468 6520 656d 6265 6464 696e 6720  f the embedding 
-0000b6c0: 7461 626c 652c 2077 6865 7265 2074 6865  table, where the
-0000b6d0: 206e 2069 7320 7468 6520 6d6f 6465 6c20   n is the model 
-0000b6e0: 7061 7261 6c6c 656c 2077 6179 2064 6574  parallel way det
-0000b6f0: 6572 6d69 6e65 6420 6279 0a20 2020 2020  ermined by.     
-0000b700: 2020 2060 7061 7261 6c6c 656c 5f63 6f6e     `parallel_con
-0000b710: 6669 672e 6d6f 6465 6c5f 7061 7261 6c6c  fig.model_parall
-0000b720: 656c 6020 2845 6d62 6564 6469 6e67 4f70  el` (EmbeddingOp
-0000b730: 5061 7261 6c6c 656c 436f 6e66 6967 292e  ParallelConfig).
-0000b740: 0a0a 2020 2020 2020 2020 4e6f 7465 3a0a  ..        Note:.
-0000b750: 2020 2020 2020 2020 2020 2020 5768 656e              When
-0000b760: 2060 4155 544f 5f50 4152 414c 4c45 4c60   `AUTO_PARALLEL`
-0000b770: 206f 7220 6053 454d 495f 4155 544f 5f50   or `SEMI_AUTO_P
-0000b780: 4152 414c 4c45 4c60 206d 6f64 6520 6973  ARALLEL` mode is
-0000b790: 2065 6e61 626c 6564 2c20 7468 6973 206c   enabled, this l
-0000b7a0: 6179 6572 2073 7570 706f 7274 206f 6e6c  ayer support onl
-0000b7b0: 7920 322d 6420 6469 6d65 6e73 696f 6e20  y 2-d dimension 
-0000b7c0: 696e 7075 7473 2c0a 2020 2020 2020 2020  inputs,.        
-0000b7d0: 2020 2020 6173 2074 6865 2073 6861 7264      as the shard
-0000b7e0: 2069 7320 6465 7369 676e 6564 2066 6f72   is designed for
-0000b7f0: 2032 6420 696e 7075 7473 2e0a 0a20 2020   2d inputs...   
-0000b800: 2020 2020 2041 7267 733a 0a20 2020 2020       Args:.     
-0000b810: 2020 2020 2020 2076 6f63 6162 5f73 697a         vocab_siz
-0000b820: 6520 2869 6e74 293a 2053 697a 6520 6f66  e (int): Size of
-0000b830: 2074 6865 2064 6963 7469 6f6e 6172 7920   the dictionary 
-0000b840: 6f66 2065 6d62 6564 6469 6e67 732e 0a20  of embeddings.. 
-0000b850: 2020 2020 2020 2020 2020 2065 6d62 6564             embed
-0000b860: 6469 6e67 5f73 697a 6520 2869 6e74 293a  ding_size (int):
-0000b870: 2054 6865 2073 697a 6520 6f66 2065 6163   The size of eac
-0000b880: 6820 656d 6265 6464 696e 6720 7665 6374  h embedding vect
-0000b890: 6f72 2e0a 2020 2020 2020 2020 2020 2020  or..            
-0000b8a0: 7061 7261 6c6c 656c 5f63 6f6e 6669 6720  parallel_config 
-0000b8b0: 2845 6d62 6564 6469 6e67 4f70 5061 7261  (EmbeddingOpPara
-0000b8c0: 6c6c 656c 436f 6e66 6967 293a 2054 6865  llelConfig): The
-0000b8d0: 2070 6172 616c 6c65 6c20 636f 6e66 6967   parallel config
-0000b8e0: 206f 6620 6e65 7477 6f72 6b2e 2044 6566   of network. Def
-0000b8f0: 6175 6c74 0a20 2020 2020 2020 2020 2020  ault.           
-0000b900: 2020 2020 2060 6465 6661 756c 745f 656d       `default_em
-0000b910: 6265 6464 696e 675f 7061 7261 6c6c 656c  bedding_parallel
-0000b920: 5f63 6f6e 6669 6760 2c20 616e 2069 6e73  _config`, an ins
-0000b930: 7461 6e63 6520 6f66 2060 456d 6265 6464  tance of `Embedd
-0000b940: 696e 674f 7050 6172 616c 6c65 6c43 6f6e  ingOpParallelCon
-0000b950: 6669 6760 2077 6974 6820 6465 6661 756c  fig` with defaul
-0000b960: 7420 6172 6773 2e0a 2020 2020 2020 2020  t args..        
-0000b970: 2020 2020 7061 7261 6d5f 696e 6974 2028      param_init (
-0000b980: 556e 696f 6e5b 5465 6e73 6f72 2c20 7374  Union[Tensor, st
-0000b990: 722c 2049 6e69 7469 616c 697a 6572 2c20  r, Initializer, 
-0000b9a0: 6e75 6d62 6572 732e 4e75 6d62 6572 5d29  numbers.Number])
-0000b9b0: 3a20 496e 6974 6961 6c69 7a65 7220 666f  : Initializer fo
-0000b9c0: 7220 7468 6520 656d 6265 6464 696e 675f  r the embedding_
-0000b9d0: 7461 626c 652e 0a20 2020 2020 2020 2020  table..         
-0000b9e0: 2020 2020 2020 2052 6566 6572 2074 6f20         Refer to 
-0000b9f0: 636c 6173 7320 6069 6e69 7469 616c 697a  class `initializ
-0000ba00: 6572 6020 666f 7220 7468 6520 7661 6c75  er` for the valu
-0000ba10: 6573 206f 6620 7374 7269 6e67 2077 6865  es of string whe
-0000ba20: 6e20 6120 7374 7269 6e67 0a20 2020 2020  n a string.     
-0000ba30: 2020 2020 2020 2020 2020 2069 7320 7370             is sp
-0000ba40: 6563 6966 6965 642e 2044 6566 6175 6c74  ecified. Default
-0000ba50: 3a20 276e 6f72 6d61 6c27 2e0a 0a20 2020  : 'normal'...   
-0000ba60: 2020 2020 2049 6e70 7574 733a 0a20 2020       Inputs:.   
-0000ba70: 2020 2020 2020 2020 202d 202a 2a69 6e70           - **inp
-0000ba80: 7574 5f69 6473 2a2a 2028 5465 6e73 6f72  ut_ids** (Tensor
-0000ba90: 2920 2d20 5468 6520 746f 6b65 6e69 7a65  ) - The tokenize
-0000baa0: 6420 696e 7075 7473 2077 6974 6820 6461  d inputs with da
-0000bab0: 7461 7479 7065 2069 6e74 3332 2077 6974  tatype int32 wit
-0000bac0: 6820 7368 6170 6520 2862 6174 6368 5f73  h shape (batch_s
-0000bad0: 697a 652c 2073 6571 5f6c 656e 6774 6829  ize, seq_length)
-0000bae0: 0a0a 2020 2020 2020 2020 4f75 7470 7574  ..        Output
-0000baf0: 733a 0a20 2020 2020 2020 2020 2020 2054  s:.            T
-0000bb00: 7570 6c65 2c20 6120 7475 706c 6520 636f  uple, a tuple co
-0000bb10: 6e74 6169 6e73 2028 606f 7574 7075 7460  ntains (`output`
-0000bb20: 2c20 6065 6d62 6564 6469 6e67 5f74 6162  , `embedding_tab
-0000bb30: 6c65 6029 0a0a 2020 2020 2020 2020 2020  le`)..          
-0000bb40: 2020 2d20 2a2a 6f75 7470 7574 2a2a 2028    - **output** (
-0000bb50: 5465 6e73 6f72 2920 2d20 5468 6520 656d  Tensor) - The em
-0000bb60: 6265 6464 696e 6720 7665 6374 6f72 2066  bedding vector f
-0000bb70: 6f72 2074 6865 2069 6e70 7574 2077 6974  or the input wit
-0000bb80: 6820 7368 6170 6520 2862 6174 6368 5f73  h shape (batch_s
-0000bb90: 697a 652c 0a20 2020 2020 2020 2020 2020  ize,.           
-0000bba0: 2020 2073 6571 5f6c 656e 6774 682c 2065     seq_length, e
-0000bbb0: 6d62 6564 6469 6e67 5f73 697a 6529 2e0a  mbedding_size)..
-0000bbc0: 2020 2020 2020 2020 2020 2020 2d20 2a2a              - **
-0000bbd0: 656d 6265 6464 696e 675f 7461 626c 652a  embedding_table*
-0000bbe0: 2a20 2854 656e 736f 7229 202d 2054 6865  * (Tensor) - The
-0000bbf0: 2065 6d62 6564 6469 6e67 2074 6162 6c65   embedding table
-0000bc00: 2077 6974 6820 7368 6170 6520 2876 6f63   with shape (voc
-0000bc10: 6162 5f73 697a 652c 2065 6d62 6564 6469  ab_size, embeddi
-0000bc20: 6e67 5f73 697a 6529 2e0a 0a20 2020 2020  ng_size)...     
-0000bc30: 2020 2052 6169 7365 733a 0a20 2020 2020     Raises:.     
-0000bc40: 2020 2020 2020 2056 616c 7565 4572 726f         ValueErro
-0000bc50: 723a 2049 6620 7468 6520 7061 7261 6c6c  r: If the parall
-0000bc60: 656c 5f63 6f6e 6669 672e 766f 6361 625f  el_config.vocab_
-0000bc70: 656d 625f 6470 2069 7320 5472 7565 2c20  emb_dp is True, 
-0000bc80: 7468 6520 766f 6361 6220 7369 7a65 2069  the vocab size i
-0000bc90: 7320 6e6f 7420 6120 6d75 6c74 6970 6c65  s not a multiple
-0000bca0: 206f 660a 2020 2020 2020 2020 2020 2020   of.            
-0000bcb0: 2020 2020 7061 7261 6c6c 656c 5f63 6f6e      parallel_con
-0000bcc0: 6669 672e 6d6f 6465 6c5f 7061 7261 6c6c  fig.model_parall
-0000bcd0: 656c 0a20 2020 2020 2020 2020 2020 2056  el.            V
-0000bce0: 616c 7565 4572 726f 723a 2060 766f 6361  alueError: `voca
-0000bcf0: 625f 7369 7a65 6020 6973 206e 6f74 2061  b_size` is not a
-0000bd00: 2070 6f73 6974 6976 6520 7661 6c75 652e   positive value.
-0000bd10: 0a20 2020 2020 2020 2020 2020 2056 616c  .            Val
-0000bd20: 7565 4572 726f 723a 2060 656d 6265 6464  ueError: `embedd
-0000bd30: 696e 675f 7369 7a65 6020 6973 206e 6f74  ing_size` is not
-0000bd40: 2061 2070 6f73 6974 6976 6520 7661 6c75   a positive valu
-0000bd50: 652e 0a20 2020 2020 2020 2020 2020 2054  e..            T
-0000bd60: 7970 6545 7272 6f72 3a20 6070 6172 616c  ypeError: `paral
-0000bd70: 6c65 6c5f 636f 6e66 6967 6020 6973 206e  lel_config` is n
-0000bd80: 6f74 2061 2073 7562 636c 6173 7320 6f66  ot a subclass of
-0000bd90: 204f 7050 6172 616c 6c65 6c43 6f6e 6669   OpParallelConfi
-0000bda0: 672e 0a0a 2020 2020 2020 2020 5375 7070  g...        Supp
-0000bdb0: 6f72 7465 6420 506c 6174 666f 726d 733a  orted Platforms:
-0000bdc0: 0a20 2020 2020 2020 2020 2020 2060 6041  .            ``A
-0000bdd0: 7363 656e 6460 6020 6060 4750 5560 600a  scend`` ``GPU``.
-0000bde0: 0a20 2020 2020 2020 2045 7861 6d70 6c65  .        Example
-0000bdf0: 733a 0a20 2020 2020 2020 2020 2020 203e  s:.            >
-0000be00: 3e3e 2069 6d70 6f72 7420 6e75 6d70 7920  >> import numpy 
-0000be10: 6173 206e 700a 2020 2020 2020 2020 2020  as np.          
-0000be20: 2020 3e3e 3e20 6672 6f6d 206d 696e 6466    >>> from mindf
-0000be30: 6f72 6d65 7273 2e6d 6f64 756c 6573 2e74  ormers.modules.t
-0000be40: 7261 6e73 666f 726d 6572 2069 6d70 6f72  ransformer impor
-0000be50: 7420 566f 6361 6245 6d62 6564 6469 6e67  t VocabEmbedding
-0000be60: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
-0000be70: 2066 726f 6d20 6d69 6e64 7370 6f72 6520   from mindspore 
-0000be80: 696d 706f 7274 2054 656e 736f 720a 2020  import Tensor.  
-0000be90: 2020 2020 2020 2020 2020 3e3e 3e20 6672            >>> fr
-0000bea0: 6f6d 206d 696e 6473 706f 7265 2069 6d70  om mindspore imp
-0000beb0: 6f72 7420 6474 7970 6520 6173 206d 7374  ort dtype as mst
-0000bec0: 7970 650a 2020 2020 2020 2020 2020 2020  ype.            
-0000bed0: 3e3e 3e20 6d6f 6465 6c20 3d20 566f 6361  >>> model = Voca
-0000bee0: 6245 6d62 6564 6469 6e67 2876 6f63 6162  bEmbedding(vocab
-0000bef0: 5f73 697a 653d 3330 2c20 656d 6265 6464  _size=30, embedd
-0000bf00: 696e 675f 7369 7a65 3d33 3029 0a20 2020  ing_size=30).   
-0000bf10: 2020 2020 2020 2020 203e 3e3e 2074 656e           >>> ten
-0000bf20: 736f 7220 3d20 5465 6e73 6f72 286e 702e  sor = Tensor(np.
-0000bf30: 6f6e 6573 2828 3230 2c20 3135 2929 2c20  ones((20, 15)), 
-0000bf40: 6d73 7479 7065 2e69 6e74 3332 290a 2020  mstype.int32).  
-0000bf50: 2020 2020 2020 2020 2020 3e3e 3e20 6f75            >>> ou
-0000bf60: 7470 7574 2c20 7461 626c 6520 3d20 6d6f  tput, table = mo
-0000bf70: 6465 6c28 7465 6e73 6f72 290a 2020 2020  del(tensor).    
-0000bf80: 2020 2020 2020 2020 3e3e 3e20 7072 696e          >>> prin
-0000bf90: 7428 6f75 7470 7574 2e73 6861 7065 290a  t(output.shape).
-0000bfa0: 2020 2020 2020 2020 2020 2020 2832 302c              (20,
-0000bfb0: 2031 352c 2033 3029 0a20 2020 2020 2020   15, 30).       
-0000bfc0: 2020 2020 203e 3e3e 2070 7269 6e74 2874       >>> print(t
-0000bfd0: 6162 6c65 2e73 6861 7065 290a 2020 2020  able.shape).    
-0000bfe0: 2020 2020 2020 2020 2833 302c 2033 3029          (30, 30)
-0000bff0: 0a20 2020 2022 2222 0a0a 2020 2020 405f  .    """..    @_
-0000c000: 4c6f 6741 6374 696f 6e4f 6e63 6528 6d5f  LogActionOnce(m_
-0000c010: 6c6f 6767 6572 3d6c 6f67 6765 722c 206b  logger=logger, k
-0000c020: 6579 3d27 566f 6361 6245 6d62 6564 6469  ey='VocabEmbeddi
-0000c030: 6e67 272c 0a20 2020 2020 2020 2020 2020  ng',.           
-0000c040: 2020 2020 2020 2020 206e 6f5f 7761 726e           no_warn
-0000c050: 696e 673d 5f67 6574 5f70 6172 616c 6c65  ing=_get_paralle
-0000c060: 6c5f 6d6f 6465 2829 2069 6e20 2850 6172  l_mode() in (Par
-0000c070: 616c 6c65 6c4d 6f64 652e 5354 414e 445f  allelMode.STAND_
-0000c080: 414c 4f4e 452c 2929 0a20 2020 2040 5f61  ALONE,)).    @_a
-0000c090: 7267 735f 7479 7065 5f76 616c 6964 6174  rgs_type_validat
-0000c0a0: 6f72 5f63 6865 636b 2876 6f63 6162 5f73  or_check(vocab_s
-0000c0b0: 697a 653d 5661 6c69 6461 746f 722e 6368  ize=Validator.ch
-0000c0c0: 6563 6b5f 706f 7369 7469 7665 5f69 6e74  eck_positive_int
-0000c0d0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-0000c0e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000c0f0: 2020 656d 6265 6464 696e 675f 7369 7a65    embedding_size
-0000c100: 3d56 616c 6964 6174 6f72 2e63 6865 636b  =Validator.check
-0000c110: 5f70 6f73 6974 6976 655f 696e 742c 0a20  _positive_int,. 
-0000c120: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000c130: 2020 2020 2020 2020 2020 2020 2020 2070                 p
-0000c140: 6172 616c 6c65 6c5f 636f 6e66 6967 3d5f  arallel_config=_
-0000c150: 7661 6c69 645f 7479 7065 5f63 6865 636b  valid_type_check
-0000c160: 7328 5b45 6d62 6564 6469 6e67 4f70 5061  s([EmbeddingOpPa
-0000c170: 7261 6c6c 656c 436f 6e66 6967 5d2c 2022  rallelConfig], "
-0000c180: 566f 6361 6245 6d62 6564 6469 6e67 2229  VocabEmbedding")
-0000c190: 290a 2020 2020 6465 6620 5f5f 696e 6974  ).    def __init
-0000c1a0: 5f5f 2873 656c 662c 2076 6f63 6162 5f73  __(self, vocab_s
-0000c1b0: 697a 652c 2065 6d62 6564 6469 6e67 5f73  ize, embedding_s
-0000c1c0: 697a 652c 2070 6172 616c 6c65 6c5f 636f  ize, parallel_co
-0000c1d0: 6e66 6967 3d64 6566 6175 6c74 5f65 6d62  nfig=default_emb
-0000c1e0: 6564 6469 6e67 5f70 6172 616c 6c65 6c5f  edding_parallel_
-0000c1f0: 636f 6e66 6967 2c0a 2020 2020 2020 2020  config,.        
-0000c200: 2020 2020 2020 2020 2070 6172 616d 5f69           param_i
-0000c210: 6e69 743d 276e 6f72 6d61 6c27 2c20 7061  nit='normal', pa
-0000c220: 7261 6d5f 696e 6974 5f74 7970 653d 6d73  ram_init_type=ms
-0000c230: 7479 7065 2e66 6c6f 6174 3332 293a 0a20  type.float32):. 
-0000c240: 2020 2020 2020 2073 7570 6572 2856 6f63         super(Voc
-0000c250: 6162 456d 6265 6464 696e 672c 2073 656c  abEmbedding, sel
-0000c260: 6629 2e5f 5f69 6e69 745f 5f28 290a 2020  f).__init__().  
-0000c270: 2020 2020 2020 5f63 6865 636b 5f63 6f6e        _check_con
-0000c280: 6669 6728 7061 7261 6c6c 656c 5f63 6f6e  fig(parallel_con
-0000c290: 6669 6729 0a20 2020 2020 2020 2073 656c  fig).        sel
-0000c2a0: 662e 766f 6361 625f 7369 7a65 203d 2076  f.vocab_size = v
-0000c2b0: 6f63 6162 5f73 697a 650a 2020 2020 2020  ocab_size.      
-0000c2c0: 2020 7365 6c66 2e65 6d62 6564 6469 6e67    self.embedding
-0000c2d0: 5f73 697a 6520 3d20 656d 6265 6464 696e  _size = embeddin
-0000c2e0: 675f 7369 7a65 0a20 2020 2020 2020 2073  g_size.        s
-0000c2f0: 656c 662e 656d 6265 6464 696e 675f 7461  elf.embedding_ta
-0000c300: 626c 6520 3d20 5061 7261 6d65 7465 7228  ble = Parameter(
-0000c310: 0a20 2020 2020 2020 2020 2020 2069 6e69  .            ini
-0000c320: 7469 616c 697a 6572 2870 6172 616d 5f69  tializer(param_i
-0000c330: 6e69 742c 205b 7365 6c66 2e76 6f63 6162  nit, [self.vocab
-0000c340: 5f73 697a 652c 2073 656c 662e 656d 6265  _size, self.embe
-0000c350: 6464 696e 675f 7369 7a65 5d2c 2070 6172  dding_size], par
-0000c360: 616d 5f69 6e69 745f 7479 7065 292c 0a20  am_init_type),. 
-0000c370: 2020 2020 2020 2020 2020 206e 616d 653d             name=
-0000c380: 2765 6d62 6564 6469 6e67 5f74 6162 6c65  'embedding_table
-0000c390: 272c 2070 6172 616c 6c65 6c5f 6f70 7469  ', parallel_opti
-0000c3a0: 6d69 7a65 723d 4661 6c73 6529 0a0a 2020  mizer=False)..  
-0000c3b0: 2020 2020 2020 6966 2070 6172 616c 6c65        if paralle
-0000c3c0: 6c5f 636f 6e66 6967 2e76 6f63 6162 5f65  l_config.vocab_e
-0000c3d0: 6d62 5f64 703a 0a20 2020 2020 2020 2020  mb_dp:.         
-0000c3e0: 2020 2073 656c 662e 6761 7468 6572 203d     self.gather =
-0000c3f0: 2050 2e47 6174 6865 7228 292e 7368 6172   P.Gather().shar
-0000c400: 6428 2828 312c 2031 292c 2028 7061 7261  d(((1, 1), (para
-0000c410: 6c6c 656c 5f63 6f6e 6669 672e 6461 7461  llel_config.data
-0000c420: 5f70 6172 616c 6c65 6c2c 2031 2929 290a  _parallel, 1))).
-0000c430: 2020 2020 2020 2020 2020 2020 6c6f 6767              logg
-0000c440: 6572 2e69 6e66 6f28 6622 5573 696e 6720  er.info(f"Using 
-0000c450: 7b70 6172 616c 6c65 6c5f 636f 6e66 6967  {parallel_config
-0000c460: 2e64 6174 615f 7061 7261 6c6c 656c 7d20  .data_parallel} 
-0000c470: 6461 7461 2070 6172 616c 6c65 6c20 666f  data parallel fo
-0000c480: 7220 7468 6520 656d 6265 6464 696e 6720  r the embedding 
-0000c490: 6c6f 6f6b 7570 2e22 290a 2020 2020 2020  lookup.").      
-0000c4a0: 2020 656c 7365 3a0a 2020 2020 2020 2020    else:.        
-0000c4b0: 2020 2020 6966 2073 656c 662e 766f 6361      if self.voca
-0000c4c0: 625f 7369 7a65 2025 2070 6172 616c 6c65  b_size % paralle
-0000c4d0: 6c5f 636f 6e66 6967 2e6d 6f64 656c 5f70  l_config.model_p
-0000c4e0: 6172 616c 6c65 6c20 213d 2030 3a0a 2020  arallel != 0:.  
-0000c4f0: 2020 2020 2020 2020 2020 2020 2020 7261                ra
-0000c500: 6973 6520 5661 6c75 6545 7272 6f72 2866  ise ValueError(f
-0000c510: 2254 6865 2076 6f63 6162 2073 697a 6520  "The vocab size 
-0000c520: 6f66 2074 6865 2065 6d62 6564 6469 6e67  of the embedding
-0000c530: 207b 7365 6c66 2e76 6f63 6162 5f73 697a   {self.vocab_siz
-0000c540: 657d 206d 7573 7420 6265 2061 2022 0a20  e} must be a ". 
-0000c550: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000c560: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000c570: 6622 6d75 6c74 6970 6c65 206f 6620 7061  f"multiple of pa
-0000c580: 7261 6c6c 656c 5f63 6f6e 6669 672e 6d6f  rallel_config.mo
-0000c590: 6465 6c5f 7061 7261 6c6c 656c 207b 7061  del_parallel {pa
-0000c5a0: 7261 6c6c 656c 5f63 6f6e 6669 672e 6d6f  rallel_config.mo
-0000c5b0: 6465 6c5f 7061 7261 6c6c 656c 7d2e 2229  del_parallel}.")
-0000c5c0: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-0000c5d0: 662e 6761 7468 6572 203d 2050 2e47 6174  f.gather = P.Gat
-0000c5e0: 6865 7228 292e 7368 6172 6428 2828 7061  her().shard(((pa
-0000c5f0: 7261 6c6c 656c 5f63 6f6e 6669 672e 6d6f  rallel_config.mo
-0000c600: 6465 6c5f 7061 7261 6c6c 656c 2c20 3129  del_parallel, 1)
-0000c610: 2c20 2870 6172 616c 6c65 6c5f 636f 6e66  , (parallel_conf
-0000c620: 6967 2e64 6174 615f 7061 7261 6c6c 656c  ig.data_parallel
-0000c630: 2c20 3129 2929 0a20 2020 2020 2020 2020  , 1))).         
-0000c640: 2020 206c 6f67 6765 722e 696e 666f 2866     logger.info(f
-0000c650: 2255 7369 6e67 207b 7061 7261 6c6c 656c  "Using {parallel
-0000c660: 5f63 6f6e 6669 672e 6461 7461 5f70 6172  _config.data_par
-0000c670: 616c 6c65 6c7d 2064 6174 6120 7061 7261  allel} data para
-0000c680: 6c6c 656c 2061 6e64 207b 7061 7261 6c6c  llel and {parall
-0000c690: 656c 5f63 6f6e 6669 672e 6d6f 6465 6c5f  el_config.model_
-0000c6a0: 7061 7261 6c6c 656c 7d20 220a 2020 2020  parallel} ".    
-0000c6b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000c6c0: 2020 2020 6622 6d6f 6465 6c20 7061 7261      f"model para
-0000c6d0: 6c6c 656c 2066 6f72 2074 6865 2065 6d62  llel for the emb
-0000c6e0: 6564 6469 6e67 206c 6f6f 6b75 702e 2229  edding lookup.")
-0000c6f0: 0a0a 2020 2020 6465 6620 636f 6e73 7472  ..    def constr
-0000c700: 7563 7428 7365 6c66 2c20 696e 7075 745f  uct(self, input_
-0000c710: 6964 7329 3a0a 2020 2020 2020 2020 5f63  ids):.        _c
-0000c720: 6865 636b 5f69 6e70 7574 5f64 7479 7065  heck_input_dtype
-0000c730: 2846 2e64 7479 7065 2869 6e70 7574 5f69  (F.dtype(input_i
-0000c740: 6473 292c 2022 696e 7075 745f 6964 7322  ds), "input_ids"
-0000c750: 2c20 5b6d 7374 7970 652e 696e 7433 325d  , [mstype.int32]
-0000c760: 2c20 7365 6c66 2e63 6c73 5f6e 616d 6529  , self.cls_name)
-0000c770: 0a20 2020 2020 2020 206f 7574 7075 7420  .        output 
-0000c780: 3d20 7365 6c66 2e67 6174 6865 7228 7365  = self.gather(se
-0000c790: 6c66 2e65 6d62 6564 6469 6e67 5f74 6162  lf.embedding_tab
-0000c7a0: 6c65 2c20 696e 7075 745f 6964 732c 2030  le, input_ids, 0
-0000c7b0: 290a 2020 2020 2020 2020 7265 7475 726e  ).        return
-0000c7c0: 206f 7574 7075 742c 2073 656c 662e 656d   output, self.em
-0000c7d0: 6265 6464 696e 675f 7461 626c 652e 7661  bedding_table.va
-0000c7e0: 6c75 6528 290a 0a0a 636c 6173 7320 4d75  lue()...class Mu
-0000c7f0: 6c74 6948 6561 6441 7474 656e 7469 6f6e  ltiHeadAttention
-0000c800: 2843 656c 6c29 3a0a 2020 2020 7222 2222  (Cell):.    r"""
-0000c810: 0a20 2020 2020 2020 2054 6869 7320 6973  .        This is
-0000c820: 2061 6e20 696d 706c 656d 656e 7461 7469   an implementati
-0000c830: 6f6e 206f 6620 6d75 6c74 6968 6561 6420  on of multihead 
-0000c840: 6174 7465 6e74 696f 6e20 696e 2074 6865  attention in the
-0000c850: 2070 6170 6572 2060 4174 7465 6e74 696f   paper `Attentio
-0000c860: 6e20 6973 2061 6c6c 2079 6f75 206e 6565  n is all you nee
-0000c870: 640a 2020 2020 2020 2020 3c68 7474 7073  d.        <https
-0000c880: 3a2f 2f61 7278 6976 2e6f 7267 2f70 6466  ://arxiv.org/pdf
-0000c890: 2f31 3730 362e 3033 3736 3276 352e 7064  /1706.03762v5.pd
-0000c8a0: 663e 605f 2e20 4769 7665 6e20 7468 6520  f>`_. Given the 
-0000c8b0: 7175 6572 7920 7665 6374 6f72 2077 6974  query vector wit
-0000c8c0: 6820 736f 7572 6365 206c 656e 6774 682c  h source length,
-0000c8d0: 2061 6e64 2074 6865 0a20 2020 2020 2020   and the.       
-0000c8e0: 206b 6579 2061 6e64 2076 616c 7565 2076   key and value v
-0000c8f0: 6563 746f 7220 7769 7468 2074 6172 6765  ector with targe
-0000c900: 7420 6c65 6e67 7468 2c20 7468 6520 6174  t length, the at
-0000c910: 7465 6e74 696f 6e20 7769 6c6c 2062 6520  tention will be 
-0000c920: 7065 7266 6f72 6d65 6420 6173 2074 6865  performed as the
-0000c930: 2066 6f6c 6c6f 7769 6e67 0a0a 2020 2020   following..    
-0000c940: 2020 2020 2e2e 206d 6174 683a 3a0a 2020      .. math::.  
-0000c950: 2020 2020 2020 2020 2020 2020 204d 756c               Mul
-0000c960: 7469 4865 6164 4174 7465 6e74 696f 6e28  tiHeadAttention(
-0000c970: 7175 6572 792c 206b 6579 2c20 7665 6374  query, key, vect
-0000c980: 6f72 2920 3d20 436f 6e63 6174 2868 6561  or) = Concat(hea
-0000c990: 645f 312c 205c 646f 7473 2c20 6865 6164  d_1, \dots, head
-0000c9a0: 5f68 2957 5e4f 0a0a 2020 2020 2020 2020  _h)W^O..        
-0000c9b0: 7768 6572 6520 3a6d 6174 683a 6068 6561  where :math:`hea
-0000c9c0: 645f 6920 3d20 4174 7465 6e74 696f 6e28  d_i = Attention(
-0000c9d0: 5157 5f69 5e51 2c20 4b57 5f69 5e4b 2c20  QW_i^Q, KW_i^K, 
-0000c9e0: 5657 5f69 5e56 2960 2e20 5468 6520 6465  VW_i^V)`. The de
-0000c9f0: 6661 756c 7420 6973 2077 6974 6820 6120  fault is with a 
-0000ca00: 6269 6173 2e0a 0a20 2020 2020 2020 2069  bias...        i
-0000ca10: 6620 7175 6572 792c 206b 6579 2061 6e64  f query, key and
-0000ca20: 2076 616c 7565 2074 656e 736f 7220 6973   value tensor is
-0000ca30: 2073 616d 652c 2074 6865 6e20 6974 2077   same, then it w
-0000ca40: 696c 6c20 6265 2073 656c 6620 6174 7465  ill be self atte
-0000ca50: 6e74 696f 6e2e 0a0a 2020 2020 2020 2020  ntion...        
-0000ca60: 4172 6773 3a0a 2020 2020 2020 2020 2020  Args:.          
-0000ca70: 2020 6261 7463 685f 7369 7a65 2869 6e74    batch_size(int
-0000ca80: 293a 2054 6865 2062 6174 6368 2073 697a  ): The batch siz
-0000ca90: 6520 6f66 2074 6865 2069 6e70 7574 2074  e of the input t
-0000caa0: 656e 736f 7220 7768 656e 2064 6f20 696e  ensor when do in
-0000cab0: 6372 656e 6d65 6e74 616c 2070 7265 6469  crenmental predi
-0000cac0: 6374 696f 6e2e 2053 686f 756c 6420 6265  ction. Should be
-0000cad0: 2061 2070 6f73 6974 6976 650a 2020 2020   a positive.    
-0000cae0: 2020 2020 2020 2020 2020 2020 7661 6c75              valu
-0000caf0: 652e 2057 6865 6e20 646f 2074 7261 696e  e. When do train
-0000cb00: 696e 6720 6f72 2070 7265 6469 6374 696f  ing or predictio
-0000cb10: 6e2c 2074 6865 2061 7267 756d 656e 7420  n, the argument 
-0000cb20: 7769 6c6c 206e 6f74 2077 6f72 6b20 616e  will not work an
-0000cb30: 6420 7468 6520 7573 6572 2063 616e 206a  d the user can j
-0000cb40: 7573 7420 7061 7373 204e 6f6e 6520 746f  ust pass None to
-0000cb50: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0000cb60: 2074 6865 2061 7267 756d 656e 742e 0a20   the argument.. 
-0000cb70: 2020 2020 2020 2020 2020 2073 7263 5f73             src_s
-0000cb80: 6571 5f6c 656e 6774 6828 696e 7429 3a20  eq_length(int): 
-0000cb90: 5468 6520 7365 7175 656e 6365 206c 656e  The sequence len
-0000cba0: 6774 6820 6f66 2074 6865 2071 7565 7279  gth of the query
-0000cbb0: 2076 6563 746f 722e 0a20 2020 2020 2020   vector..       
-0000cbc0: 2020 2020 2074 6774 5f73 6571 5f6c 656e       tgt_seq_len
-0000cbd0: 6774 6828 696e 7429 3a20 5468 6520 7365  gth(int): The se
-0000cbe0: 7175 656e 6365 206c 656e 6774 6820 6f66  quence length of
-0000cbf0: 2074 6865 206b 6579 2061 6e64 2076 616c   the key and val
-0000cc00: 7565 2076 6563 746f 722e 0a20 2020 2020  ue vector..     
-0000cc10: 2020 2020 2020 2068 6964 6465 6e5f 7369         hidden_si
-0000cc20: 7a65 2869 6e74 293a 2054 6865 2068 6964  ze(int): The hid
-0000cc30: 6465 6e20 7369 7a65 206f 6620 7468 6520  den size of the 
-0000cc40: 696e 7075 742e 0a20 2020 2020 2020 2020  input..         
-0000cc50: 2020 206e 756d 5f68 6561 6473 2869 6e74     num_heads(int
-0000cc60: 293a 2054 6865 206e 756d 6265 7220 6f66  ): The number of
-0000cc70: 2074 6865 2068 6561 6473 2e0a 2020 2020   the heads..    
-0000cc80: 2020 2020 2020 2020 6869 6464 656e 5f64          hidden_d
-0000cc90: 726f 706f 7574 5f72 6174 6528 666c 6f61  ropout_rate(floa
-0000cca0: 7429 3a20 5468 6520 6472 6f70 6f75 7420  t): The dropout 
-0000ccb0: 7261 7465 206f 6620 7468 6520 6669 6e61  rate of the fina
-0000ccc0: 6c20 6f75 7470 7574 206f 6620 7468 6520  l output of the 
-0000ccd0: 6c61 7965 722e 2044 6566 6175 6c74 3a30  layer. Default:0
-0000cce0: 2e31 2e0a 2020 2020 2020 2020 2020 2020  .1..            
-0000ccf0: 6174 7465 6e74 696f 6e5f 6472 6f70 6f75  attention_dropou
-0000cd00: 745f 7261 7465 2866 6c6f 6174 293a 2054  t_rate(float): T
-0000cd10: 6865 2064 726f 706f 7574 2072 6174 6520  he dropout rate 
-0000cd20: 6f66 2074 6865 2061 7474 656e 7469 6f6e  of the attention
-0000cd30: 2073 636f 7265 732e 2044 6566 6175 6c74   scores. Default
-0000cd40: 3a30 2e31 2e0a 2020 2020 2020 2020 2020  :0.1..          
-0000cd50: 2020 636f 6d70 7574 655f 6474 7970 6528    compute_dtype(
-0000cd60: 6474 7970 652e 4e75 6d62 6572 293a 2054  dtype.Number): T
-0000cd70: 6865 2063 6f6d 7075 7461 7469 6f6e 2074  he computation t
-0000cd80: 7970 6520 6f66 2064 656e 7365 2e20 4465  ype of dense. De
-0000cd90: 6661 756c 7420 6d73 7479 7065 2e66 6c6f  fault mstype.flo
-0000cda0: 6174 3136 2e0a 2020 2020 2020 2020 2020  at16..          
-0000cdb0: 2020 2020 2020 5368 6f75 6c64 2062 6520        Should be 
-0000cdc0: 6d73 7479 7065 2e66 6c6f 6174 3332 206f  mstype.float32 o
-0000cdd0: 7220 6d73 7479 7065 2e66 6c6f 6174 3136  r mstype.float16
-0000cde0: 2e0a 2020 2020 2020 2020 2020 2020 736f  ..            so
-0000cdf0: 6674 6d61 785f 636f 6d70 7574 655f 7479  ftmax_compute_ty
-0000ce00: 7065 2864 7479 7065 2e4e 756d 6265 7229  pe(dtype.Number)
-0000ce10: 3a20 5468 6520 7479 7065 206f 6620 736f  : The type of so
-0000ce20: 6674 6d61 7820 636f 6d70 7574 6174 696f  ftmax computatio
-0000ce30: 6e20 6d6f 6475 6c65 2e20 4465 6661 756c  n module. Defaul
-0000ce40: 7420 6d73 7479 7065 2e66 6c6f 6174 3332  t mstype.float32
-0000ce50: 2e0a 2020 2020 2020 2020 2020 2020 2020  ..              
-0000ce60: 2020 5368 6f75 6c64 2062 6520 6d73 7479    Should be msty
-0000ce70: 7065 2e66 6c6f 6174 3332 206f 7220 6d73  pe.float32 or ms
-0000ce80: 7479 7065 2e66 6c6f 6174 3136 2e0a 2020  type.float16..  
-0000ce90: 2020 2020 2020 2020 2020 7061 7261 6d5f            param_
-0000cea0: 696e 6974 5f74 7970 6528 6474 7970 652e  init_type(dtype.
-0000ceb0: 4e75 6d62 6572 293a 2054 6865 2070 6172  Number): The par
-0000cec0: 616d 6574 6572 2069 6e69 7469 616c 697a  ameter initializ
-0000ced0: 6174 696f 6e20 7479 7065 206f 6620 7468  ation type of th
-0000cee0: 6520 6d6f 6475 6c65 2e20 4465 6661 756c  e module. Defaul
-0000cef0: 7420 6d73 7479 7065 2e66 6c6f 6174 3332  t mstype.float32
-0000cf00: 2e0a 2020 2020 2020 2020 2020 2020 2020  ..              
-0000cf10: 2020 5368 6f75 6c64 2062 6520 6d73 7479    Should be msty
-0000cf20: 7065 2e66 6c6f 6174 3332 206f 7220 6d73  pe.float32 or ms
-0000cf30: 7479 7065 2e66 6c6f 6174 3136 2e0a 2020  type.float16..  
-0000cf40: 2020 2020 2020 2020 2020 7573 655f 7061            use_pa
-0000cf50: 7374 2862 6f6f 6c29 3a20 5573 6520 7468  st(bool): Use th
-0000cf60: 6520 7061 7374 2073 7461 7465 2074 6f20  e past state to 
-0000cf70: 636f 6d70 7574 652c 2075 7365 6420 666f  compute, used fo
-0000cf80: 7220 696e 6372 656d 656e 7461 6c20 7072  r incremental pr
-0000cf90: 6564 6963 7469 6f6e 2e20 466f 7220 6578  ediction. For ex
-0000cfa0: 616d 706c 652c 2069 6620 7765 2068 6176  ample, if we hav
-0000cfb0: 6520 7477 6f0a 2020 2020 2020 2020 2020  e two.          
-0000cfc0: 2020 2020 2020 776f 7264 7320 616e 6420        words and 
-0000cfd0: 7761 6e74 2074 6f20 6765 6e65 7261 7465  want to generate
-0000cfe0: 2074 6865 2074 656e 206d 6f72 6520 776f   the ten more wo
-0000cff0: 7264 732e 2057 6520 6a75 7374 206e 6565  rds. We just nee
-0000d000: 6420 746f 2063 6f6d 7075 7465 2074 6865  d to compute the
-0000d010: 2074 776f 2077 6f72 6473 2720 7374 6174   two words' stat
-0000d020: 6520 6f6e 6c79 206f 6e63 652c 0a20 2020  e only once,.   
-0000d030: 2020 2020 2020 2020 2020 2020 2061 6e64               and
-0000d040: 2067 656e 6572 6174 6520 7468 6520 6e65   generate the ne
-0000d050: 7874 2077 6f72 6420 6f6e 6520 6279 206f  xt word one by o
-0000d060: 6e65 2e20 5768 656e 2075 7365 5f70 6173  ne. When use_pas
-0000d070: 7420 6973 2054 7275 652c 2074 6865 7265  t is True, there
-0000d080: 2061 7265 2074 776f 2073 7465 7073 2074   are two steps t
-0000d090: 6f20 7275 6e20 7468 6520 7072 6564 6963  o run the predic
-0000d0a0: 7469 6f6e 2e0a 2020 2020 2020 2020 2020  tion..          
-0000d0b0: 2020 2020 2020 496e 2074 6865 2066 6972        In the fir
-0000d0c0: 7374 2073 7465 702c 2073 6574 2074 6865  st step, set the
-0000d0d0: 2069 735f 6669 7273 745f 6974 6572 6174   is_first_iterat
-0000d0e0: 696f 6e20 746f 2062 6520 5472 7565 2062  ion to be True b
-0000d0f0: 790a 2020 2020 2020 2020 2020 2020 2020  y.              
-0000d100: 2020 606d 6f64 656c 2e61 6464 5f66 6c61    `model.add_fla
-0000d110: 6773 5f72 6563 7572 7369 7665 2869 735f  gs_recursive(is_
-0000d120: 6669 7273 745f 6974 6572 6174 696f 6e3d  first_iteration=
-0000d130: 5472 7565 2960 2c20 616e 6420 7061 7373  True)`, and pass
-0000d140: 2074 6865 2066 756c 6c20 696e 7075 7473   the full inputs
-0000d150: 2e20 5468 656e 2c20 7365 7420 7468 650a  . Then, set the.
-0000d160: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000d170: 6973 5f66 6972 7374 5f69 7465 7261 7469  is_first_iterati
-0000d180: 6f6e 2074 6f20 6265 2046 616c 7365 2062  on to be False b
-0000d190: 7920 606d 6f64 656c 2e61 6464 5f66 6c61  y `model.add_fla
-0000d1a0: 6773 5f72 6563 7572 7369 7665 2869 735f  gs_recursive(is_
-0000d1b0: 6669 7273 745f 6974 6572 6174 696f 6e3d  first_iteration=
-0000d1c0: 4661 6c73 6529 602e 2041 7420 7468 6973  False)`. At this
-0000d1d0: 206d 6f6d 656e 742c 0a20 2020 2020 2020   moment,.       
-0000d1e0: 2020 2020 2020 2020 2070 6173 7320 7468           pass th
-0000d1f0: 6520 7369 6e67 6c65 2073 7465 7027 7320  e single step's 
-0000d200: 696e 7075 7420 7465 6e73 6f72 2c20 616e  input tensor, an
-0000d210: 6420 6c6f 6f70 2069 742e 2044 6566 6175  d loop it. Defau
-0000d220: 6c74 2046 616c 7365 2e0a 2020 2020 2020  lt False..      
-0000d230: 2020 2020 2020 7061 7261 6c6c 656c 5f63        parallel_c
-0000d240: 6f6e 6669 6728 4f70 5061 7261 6c6c 656c  onfig(OpParallel
-0000d250: 436f 6e66 6967 293a 2054 6865 2070 6172  Config): The par
-0000d260: 616c 6c65 6c20 636f 6e66 6967 7572 652e  allel configure.
-0000d270: 2044 6566 6175 6c74 2060 6465 6661 756c   Default `defaul
-0000d280: 745f 6470 6d70 5f63 6f6e 6669 6760 2c0a  t_dpmp_config`,.
-0000d290: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000d2a0: 616e 2069 6e73 7461 6e63 6520 6f66 2060  an instance of `
-0000d2b0: 4f70 5061 7261 6c6c 656c 436f 6e66 6967  OpParallelConfig
-0000d2c0: 6020 7769 7468 2064 6566 6175 6c74 2061  ` with default a
-0000d2d0: 7267 732e 0a0a 2020 2020 2020 2020 496e  rgs...        In
-0000d2e0: 7075 7473 3a0a 2020 2020 2020 2020 2020  puts:.          
-0000d2f0: 2020 2d20 2a2a 7175 6572 795f 7465 6e73    - **query_tens
-0000d300: 6f72 2a2a 2028 5465 6e73 6f72 2920 2d20  or** (Tensor) - 
-0000d310: 5468 6520 7175 6572 7920 7665 6374 6f72  The query vector
-0000d320: 2077 6974 6820 7368 6170 6520 2862 6174   with shape (bat
-0000d330: 6368 5f73 697a 652c 2073 7263 5f73 6571  ch_size, src_seq
-0000d340: 5f6c 656e 6774 682c 2068 6964 6465 6e5f  _length, hidden_
-0000d350: 7369 7a65 2920 6f72 0a20 2020 2020 2020  size) or.       
-0000d360: 2020 2020 2020 2028 6261 7463 685f 7369         (batch_si
-0000d370: 7a65 202a 2073 7263 5f73 6571 5f6c 656e  ze * src_seq_len
-0000d380: 6774 682c 2068 6964 6465 6e5f 7369 7a65  gth, hidden_size
-0000d390: 292c 2069 6620 7468 6520 7573 655f 7061  ), if the use_pa
-0000d3a0: 7374 2069 7320 4661 6c73 6520 6f72 2069  st is False or i
-0000d3b0: 735f 6669 7273 745f 6974 6572 6174 696f  s_first_iteratio
-0000d3c0: 6e3d 5472 7565 2e0a 2020 2020 2020 2020  n=True..        
-0000d3d0: 2020 2020 2020 4f74 6865 7277 6973 652c        Otherwise,
-0000d3e0: 206d 7573 7420 6265 2028 6261 7463 685f   must be (batch_
-0000d3f0: 7369 7a65 2c20 312c 2068 6964 6465 6e5f  size, 1, hidden_
-0000d400: 7369 7a65 290a 2020 2020 2020 2020 2020  size).          
-0000d410: 2020 2d20 2a2a 6b65 795f 7465 6e73 6f72    - **key_tensor
-0000d420: 2a2a 2028 5465 6e73 6f72 2920 2d20 5468  ** (Tensor) - Th
-0000d430: 6520 6b65 7920 7665 6374 6f72 2077 6974  e key vector wit
-0000d440: 6820 7368 6170 6520 2862 6174 6368 5f73  h shape (batch_s
-0000d450: 697a 652c 2074 6774 5f73 6571 5f6c 656e  ize, tgt_seq_len
-0000d460: 6774 682c 2068 6964 6465 6e5f 7369 7a65  gth, hidden_size
-0000d470: 2920 6f72 0a20 2020 2020 2020 2020 2020  ) or.           
-0000d480: 2020 2028 6261 7463 685f 7369 7a65 202a     (batch_size *
-0000d490: 2074 6774 5f73 6571 5f6c 656e 6774 682c   tgt_seq_length,
-0000d4a0: 2068 6964 6465 6e5f 7369 7a65 292c 2069   hidden_size), i
-0000d4b0: 6620 7468 6520 7573 655f 7061 7374 2069  f the use_past i
-0000d4c0: 7320 4661 6c73 6520 6f72 2069 735f 6669  s False or is_fi
-0000d4d0: 7273 745f 6974 6572 6174 696f 6e3d 5472  rst_iteration=Tr
-0000d4e0: 7565 2e0a 2020 2020 2020 2020 2020 2020  ue..            
-0000d4f0: 2020 4f74 6865 7277 6973 652c 206d 7573    Otherwise, mus
-0000d500: 7420 6265 2028 6261 7463 685f 7369 7a65  t be (batch_size
-0000d510: 2c20 312c 2068 6964 6465 6e5f 7369 7a65  , 1, hidden_size
-0000d520: 290a 2020 2020 2020 2020 2020 2020 2d20  ).            - 
-0000d530: 2a2a 7661 6c75 655f 7465 6e73 6f72 2a2a  **value_tensor**
-0000d540: 2028 5465 6e73 6f72 2920 2d20 5468 6520   (Tensor) - The 
-0000d550: 7661 6c75 6520 7665 6374 6f72 2077 6974  value vector wit
-0000d560: 6820 7368 6170 6520 2862 6174 6368 5f73  h shape (batch_s
-0000d570: 697a 652c 2074 6774 5f73 6571 5f6c 656e  ize, tgt_seq_len
-0000d580: 6774 682c 2068 6964 6465 6e5f 7369 7a65  gth, hidden_size
-0000d590: 2920 6f72 0a20 2020 2020 2020 2020 2020  ) or.           
-0000d5a0: 2020 2028 6261 7463 685f 7369 7a65 202a     (batch_size *
-0000d5b0: 2074 6774 5f73 6571 5f6c 656e 6774 682c   tgt_seq_length,
-0000d5c0: 2068 6964 6465 6e5f 7369 7a65 292c 2069   hidden_size), i
-0000d5d0: 6620 7468 6520 7573 655f 7061 7374 2069  f the use_past i
-0000d5e0: 7320 4661 6c73 6520 6f72 2069 735f 6669  s False or is_fi
-0000d5f0: 7273 745f 6974 6572 6174 696f 6e3d 5472  rst_iteration=Tr
-0000d600: 7565 2e0a 2020 2020 2020 2020 2020 2020  ue..            
-0000d610: 2020 4f74 6865 7277 6973 652c 206d 7573    Otherwise, mus
-0000d620: 7420 6265 2028 6261 7463 685f 7369 7a65  t be (batch_size
-0000d630: 2c20 312c 2068 6964 6465 6e5f 7369 7a65  , 1, hidden_size
-0000d640: 290a 2020 2020 2020 2020 2020 2020 2d20  ).            - 
-0000d650: 2a2a 6174 7465 6e74 696f 6e5f 6d61 736b  **attention_mask
-0000d660: 2a2a 2028 5465 6e73 6f72 2920 2d20 4966  ** (Tensor) - If
-0000d670: 2074 6865 2075 7365 5f70 6173 7420 6973   the use_past is
-0000d680: 2046 616c 7365 206f 7220 6973 5f66 6972   False or is_fir
-0000d690: 7374 5f69 7465 7261 7469 6f6e 3d54 7275  st_iteration=Tru
-0000d6a0: 652c 2074 6865 2061 7474 656e 7469 6f6e  e, the attention
-0000d6b0: 206d 6173 6b0a 2020 2020 2020 2020 2020   mask.          
-0000d6c0: 2020 2020 6d61 7472 6978 2073 686f 756c      matrix shoul
-0000d6d0: 6420 6261 2028 6261 7463 685f 7369 7a65  d ba (batch_size
-0000d6e0: 2c20 7372 635f 7365 715f 6c65 6e67 7468  , src_seq_length
-0000d6f0: 2c20 7467 745f 7365 715f 6c65 6e67 7468  , tgt_seq_length
-0000d700: 292c 206f 7220 4e6f 6e65 2e20 4e6f 6e65  ), or None. None
-0000d710: 206d 6561 6e73 2074 6865 7265 2077 696c   means there wil
-0000d720: 6c20 6265 206e 6f20 6d61 736b 0a20 2020  l be no mask.   
-0000d730: 2020 2020 2020 2020 2020 2069 6e20 736f             in so
-0000d740: 6674 6d61 7820 636f 6d70 7574 6174 696f  ftmax computatio
-0000d750: 6e2e 204f 7468 6572 7769 7365 2c20 7468  n. Otherwise, th
-0000d760: 6520 6d61 736b 206d 7573 7420 6265 2028  e mask must be (
-0000d770: 6261 7463 685f 7369 7a65 2c20 312c 2074  batch_size, 1, t
-0000d780: 6774 5f73 6571 5f6c 656e 6774 6829 0a20  gt_seq_length). 
-0000d790: 2020 2020 2020 2020 2020 202d 202a 2a6b             - **k
-0000d7a0: 6579 5f70 6173 742a 2a20 2854 656e 736f  ey_past** (Tenso
-0000d7b0: 7229 202d 2046 6c6f 6174 3136 2074 656e  r) - Float16 ten
-0000d7c0: 736f 7220 7769 7468 2073 6861 7065 2028  sor with shape (
-0000d7d0: 6261 7463 685f 7369 7a65 2c20 6e75 6d5f  batch_size, num_
-0000d7e0: 6865 6164 732c 2073 697a 655f 7065 725f  heads, size_per_
-0000d7f0: 6865 6164 2c20 7467 745f 7365 715f 6c65  head, tgt_seq_le
-0000d800: 6e67 7468 292e 0a20 2020 2020 2020 2020  ngth)..         
-0000d810: 2020 2020 2054 6865 2070 6173 7420 6361       The past ca
-0000d820: 6c63 756c 6174 6564 206b 6579 2076 6563  lculated key vec
-0000d830: 746f 722e 2055 7365 6420 666f 7220 696e  tor. Used for in
-0000d840: 6372 656d 656e 7461 6c20 7072 6564 6963  cremental predic
-0000d850: 7469 6f6e 2077 6865 6e20 7468 6520 7573  tion when the us
-0000d860: 655f 7061 7374 2069 7320 5472 7565 2e0a  e_past is True..
-0000d870: 2020 2020 2020 2020 2020 2020 2020 4465                De
-0000d880: 6661 756c 7420 4e6f 6e65 2e0a 2020 2020  fault None..    
-0000d890: 2020 2020 2020 2020 2d20 2a2a 7661 6c75          - **valu
-0000d8a0: 655f 7061 7374 2a2a 2028 5465 6e73 6f72  e_past** (Tensor
-0000d8b0: 2920 2d20 466c 6f61 7431 3620 7465 6e73  ) - Float16 tens
-0000d8c0: 6f72 2077 6974 6820 7368 6170 650a 2020  or with shape.  
-0000d8d0: 2020 2020 2020 2020 2020 2020 2862 6174              (bat
-0000d8e0: 6368 5f73 697a 652c 206e 756d 5f68 6561  ch_size, num_hea
-0000d8f0: 6473 2c20 7467 745f 7365 715f 6c65 6e67  ds, tgt_seq_leng
-0000d900: 7468 2c20 7369 7a65 5f70 6572 5f68 6561  th, size_per_hea
-0000d910: 6429 2e0a 2020 2020 2020 2020 2020 2020  d)..            
-0000d920: 2020 5468 6520 7061 7374 2063 616c 6375    The past calcu
-0000d930: 6c61 7465 6420 7661 6c75 6520 7665 6374  lated value vect
-0000d940: 6f72 2e20 5573 6564 2066 6f72 2069 6e63  or. Used for inc
-0000d950: 7265 6d65 6e74 616c 2070 7265 6469 6374  remental predict
-0000d960: 696f 6e20 7768 656e 2074 6865 2075 7365  ion when the use
-0000d970: 5f70 6173 7420 6973 2054 7275 652e 0a20  _past is True.. 
-0000d980: 2020 2020 2020 2020 2020 2020 2044 6566               Def
-0000d990: 6175 6c74 204e 6f6e 652e 0a20 2020 2020  ault None..     
-0000d9a0: 2020 2020 2020 202d 202a 2a62 6174 6368         - **batch
-0000d9b0: 5f76 616c 6964 5f6c 656e 6774 682a 2a20  _valid_length** 
-0000d9c0: 2854 656e 736f 7229 202d 2049 6e74 3332  (Tensor) - Int32
-0000d9d0: 2074 656e 736f 7220 7769 7468 2073 6861   tensor with sha
-0000d9e0: 7065 2028 6261 7463 685f 7369 7a65 2c29  pe (batch_size,)
-0000d9f0: 2074 6865 2070 6173 7420 6361 6c63 756c   the past calcul
-0000da00: 6174 6564 2074 6865 2069 6e64 6578 2e0a  ated the index..
-0000da10: 2020 2020 2020 2020 2020 2020 2020 5573                Us
-0000da20: 6564 2066 6f72 2069 6e63 7265 6d65 6e74  ed for increment
-0000da30: 616c 2070 7265 6469 6374 696f 6e20 7768  al prediction wh
-0000da40: 656e 2074 6865 2075 7365 5f70 6173 7420  en the use_past 
-0000da50: 6973 2054 7275 652e 2044 6566 6175 6c74  is True. Default
-0000da60: 204e 6f6e 652e 0a0a 2020 2020 2020 2020   None...        
-0000da70: 4f75 7470 7574 733a 0a20 2020 2020 2020  Outputs:.       
-0000da80: 2020 2020 2054 7570 6c65 2c20 6120 7475       Tuple, a tu
-0000da90: 706c 6520 636f 6e74 6169 6e73 2860 6f75  ple contains(`ou
-0000daa0: 7470 7574 602c 2060 6c61 7965 725f 7072  tput`, `layer_pr
-0000dab0: 6573 656e 7460 290a 0a20 2020 2020 2020  esent`)..       
-0000dac0: 2020 2020 202d 202a 2a6f 7574 7075 742a       - **output*
-0000dad0: 2a20 2854 656e 736f 7229 202d 2054 656e  * (Tensor) - Ten
-0000dae0: 736f 722c 2074 6865 2066 6c6f 6174 2074  sor, the float t
-0000daf0: 656e 736f 7220 6f66 2074 6865 206f 7574  ensor of the out
-0000db00: 7075 7420 6f66 2074 6865 206c 6179 6572  put of the layer
-0000db10: 2077 6974 680a 2020 2020 2020 2020 2020   with.          
-0000db20: 2020 2020 7368 6170 6520 2862 6174 6368      shape (batch
-0000db30: 5f73 697a 652c 2073 7263 5f73 6571 5f6c  _size, src_seq_l
-0000db40: 656e 6774 682c 2068 6964 6465 6e5f 7369  ength, hidden_si
-0000db50: 7a65 2920 6f72 2028 6261 7463 685f 7369  ze) or (batch_si
-0000db60: 7a65 202a 2073 7263 5f73 6571 5f6c 656e  ze * src_seq_len
-0000db70: 6774 682c 2068 6964 6465 6e5f 7369 7a65  gth, hidden_size
-0000db80: 292c 0a20 2020 2020 2020 2020 2020 2020  ),.             
-0000db90: 2069 6620 7468 6520 7573 655f 7061 7374   if the use_past
-0000dba0: 2069 7320 4661 6c73 6520 6f72 2069 735f   is False or is_
-0000dbb0: 6669 7273 745f 6974 6572 6174 696f 6e3d  first_iteration=
-0000dbc0: 5472 7565 2e20 4f74 6865 7277 6973 652c  True. Otherwise,
-0000dbd0: 2069 7420 7769 6c6c 2062 6520 2862 6174   it will be (bat
-0000dbe0: 6368 5f73 697a 652c 2031 2c20 6869 6464  ch_size, 1, hidd
-0000dbf0: 656e 5f73 697a 6529 2e0a 0a20 2020 2020  en_size)...     
-0000dc00: 2020 2020 2020 202d 202a 2a6c 6179 6572         - **layer
-0000dc10: 5f70 7265 7365 6e74 2a2a 2028 5475 706c  _present** (Tupl
-0000dc20: 6529 202d 2041 2074 7570 6c65 206f 6620  e) - A tuple of 
-0000dc30: 7468 6520 5465 6e73 6f72 206f 6620 7468  the Tensor of th
-0000dc40: 6520 7072 6f6a 6563 7465 6420 6b65 7920  e projected key 
-0000dc50: 616e 6420 7661 6c75 6520 7665 6374 6f72  and value vector
-0000dc60: 2077 6974 680a 2020 2020 2020 2020 2020   with.          
-0000dc70: 2020 2020 2828 6261 7463 685f 7369 7a65      ((batch_size
-0000dc80: 2c20 6e75 6d5f 6865 6164 732c 2073 697a  , num_heads, siz
-0000dc90: 655f 7065 725f 6865 6164 2c20 7467 745f  e_per_head, tgt_
-0000dca0: 7365 715f 6c65 6e67 7468 292c 0a20 2020  seq_length),.   
-0000dcb0: 2020 2020 2020 2020 2020 2028 6261 7463             (batc
-0000dcc0: 685f 7369 7a65 2c20 6e75 6d5f 6865 6164  h_size, num_head
-0000dcd0: 732c 2074 6774 5f73 6571 5f6c 656e 6774  s, tgt_seq_lengt
-0000dce0: 682c 2073 697a 655f 7065 725f 6865 6164  h, size_per_head
-0000dcf0: 2929 2e0a 0a20 2020 2020 2020 2053 7570  ))...        Sup
-0000dd00: 706f 7274 6564 2050 6c61 7466 6f72 6d73  ported Platforms
-0000dd10: 3a0a 2020 2020 2020 2020 2020 2020 6060  :.            ``
-0000dd20: 4173 6365 6e64 6060 2060 6047 5055 6060  Ascend`` ``GPU``
-0000dd30: 0a0a 2020 2020 2020 2020 4578 616d 706c  ..        Exampl
-0000dd40: 6573 3a0a 2020 2020 2020 2020 2020 2020  es:.            
-0000dd50: 3e3e 3e20 696d 706f 7274 206e 756d 7079  >>> import numpy
-0000dd60: 2061 7320 6e70 0a20 2020 2020 2020 2020   as np.         
-0000dd70: 2020 203e 3e3e 2066 726f 6d20 6d69 6e64     >>> from mind
-0000dd80: 666f 726d 6572 732e 6d6f 6475 6c65 732e  formers.modules.
-0000dd90: 7472 616e 7366 6f72 6d65 7220 696d 706f  transformer impo
-0000dda0: 7274 204d 756c 7469 4865 6164 4174 7465  rt MultiHeadAtte
-0000ddb0: 6e74 696f 6e0a 2020 2020 2020 2020 2020  ntion.          
-0000ddc0: 2020 3e3e 3e20 6672 6f6d 206d 696e 6473    >>> from minds
-0000ddd0: 706f 7265 2069 6d70 6f72 7420 6474 7970  pore import dtyp
-0000dde0: 6520 6173 206d 7374 7970 650a 2020 2020  e as mstype.    
-0000ddf0: 2020 2020 2020 2020 3e3e 3e20 6672 6f6d          >>> from
-0000de00: 206d 696e 6473 706f 7265 2069 6d70 6f72   mindspore impor
-0000de10: 7420 5465 6e73 6f72 0a20 2020 2020 2020  t Tensor.       
-0000de20: 2020 2020 203e 3e3e 206d 6f64 656c 203d       >>> model =
-0000de30: 204d 756c 7469 4865 6164 4174 7465 6e74   MultiHeadAttent
-0000de40: 696f 6e28 6261 7463 685f 7369 7a65 3d4e  ion(batch_size=N
-0000de50: 6f6e 652c 2068 6964 6465 6e5f 7369 7a65  one, hidden_size
-0000de60: 3d31 352c 2073 7263 5f73 6571 5f6c 656e  =15, src_seq_len
-0000de70: 6774 683d 3230 2c20 7467 745f 7365 715f  gth=20, tgt_seq_
-0000de80: 6c65 6e67 7468 3d32 302c 0a20 2020 2020  length=20,.     
-0000de90: 2020 2020 2020 202e 2e2e 2020 2020 2020         ...      
-0000dea0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000deb0: 2020 2020 2020 6e75 6d5f 6865 6164 733d        num_heads=
-0000dec0: 3329 0a20 2020 2020 2020 2020 2020 203e  3).            >
-0000ded0: 3e3e 2066 726f 6d5f 7465 6e73 6f72 203d  >> from_tensor =
-0000dee0: 2054 656e 736f 7228 6e70 2e6f 6e65 7328   Tensor(np.ones(
-0000def0: 2832 2c20 3230 2c20 3135 2929 2c20 6d73  (2, 20, 15)), ms
-0000df00: 7479 7065 2e66 6c6f 6174 3332 290a 2020  type.float32).  
-0000df10: 2020 2020 2020 2020 2020 3e3e 3e20 746f            >>> to
-0000df20: 5f74 656e 736f 7220 3d20 5465 6e73 6f72  _tensor = Tensor
-0000df30: 286e 702e 6f6e 6573 2828 322c 2032 302c  (np.ones((2, 20,
-0000df40: 2031 3529 292c 206d 7374 7970 652e 666c   15)), mstype.fl
-0000df50: 6f61 7431 3629 0a20 2020 2020 2020 2020  oat16).         
-0000df60: 2020 203e 3e3e 2061 7474 656e 7469 6f6e     >>> attention
-0000df70: 5f6d 6173 6b20 3d20 5465 6e73 6f72 286e  _mask = Tensor(n
-0000df80: 702e 6f6e 6573 2828 322c 2032 302c 2032  p.ones((2, 20, 2
-0000df90: 3029 292c 206d 7374 7970 652e 666c 6f61  0)), mstype.floa
-0000dfa0: 7431 3629 0a20 2020 2020 2020 2020 2020  t16).           
-0000dfb0: 203e 3e3e 2061 7474 6e5f 6f75 742c 2070   >>> attn_out, p
-0000dfc0: 6173 7420 3d20 6d6f 6465 6c28 6672 6f6d  ast = model(from
-0000dfd0: 5f74 656e 736f 722c 2074 6f5f 7465 6e73  _tensor, to_tens
-0000dfe0: 6f72 2c20 746f 5f74 656e 736f 722c 2061  or, to_tensor, a
-0000dff0: 7474 656e 7469 6f6e 5f6d 6173 6b29 0a20  ttention_mask). 
-0000e000: 2020 2020 2020 2020 2020 203e 3e3e 2070             >>> p
-0000e010: 7269 6e74 2861 7474 6e5f 6f75 742e 7368  rint(attn_out.sh
-0000e020: 6170 6529 0a20 2020 2020 2020 2020 2020  ape).           
-0000e030: 2028 322c 2032 302c 2031 3529 0a20 2020   (2, 20, 15).   
-0000e040: 2020 2020 2020 2020 203e 3e3e 2070 7269           >>> pri
-0000e050: 6e74 2870 6173 745b 305d 2e73 6861 7065  nt(past[0].shape
-0000e060: 290a 2020 2020 2020 2020 2020 2020 2832  ).            (2
-0000e070: 2c20 332c 2035 2c20 3230 290a 2020 2020  , 3, 5, 20).    
-0000e080: 2020 2020 2020 2020 3e3e 3e20 7072 696e          >>> prin
-0000e090: 7428 7061 7374 5b31 5d2e 7368 6170 6529  t(past[1].shape)
-0000e0a0: 0a20 2020 2020 2020 2020 2020 2028 322c  .            (2,
-0000e0b0: 2033 2c20 3230 2c20 3529 0a20 2020 2020   3, 20, 5).     
-0000e0c0: 2020 2020 2020 203e 3e3e 2023 2057 6865         >>> # Whe
-0000e0d0: 6e20 7573 6520 7573 655f 7061 7374 3d54  n use use_past=T
-0000e0e0: 7275 652c 2069 7420 696e 636c 7564 6573  rue, it includes
-0000e0f0: 2074 776f 2073 7465 7073 2074 6f20 696d   two steps to im
-0000e100: 706c 656d 656e 7420 7468 6520 696e 6372  plement the incr
-0000e110: 656d 656e 7461 6c20 7072 6564 6963 7469  emental predicti
-0000e120: 6f6e 2e0a 2020 2020 2020 2020 2020 2020  on..            
-0000e130: 3e3e 3e20 2320 5374 6570 2031 3a20 7365  >>> # Step 1: se
-0000e140: 7420 6973 5f66 6972 7374 5f69 7465 7261  t is_first_itera
-0000e150: 7469 6f6e 3d54 7275 652c 2061 6e64 2069  tion=True, and i
-0000e160: 6e70 7574 2074 6865 2066 756c 6c20 7365  nput the full se
-0000e170: 7175 656e 6365 206c 656e 6774 6827 7320  quence length's 
-0000e180: 7374 6174 652e 0a20 2020 2020 2020 2020  state..         
-0000e190: 2020 203e 3e3e 2023 2057 6520 6e65 6564     >>> # We need
-0000e1a0: 2074 6f20 7072 6570 6172 6520 7468 6520   to prepare the 
-0000e1b0: 6d65 6d6f 7279 2070 6172 616d 6574 6572  memory parameter
-0000e1c0: 7320 666f 7220 7361 7669 6e67 206b 6579  s for saving key
-0000e1d0: 2061 6e64 2076 616c 7565 2073 7461 7465   and value state
-0000e1e0: 7320 6669 7273 746c 792e 0a20 2020 2020  s firstly..     
-0000e1f0: 2020 2020 2020 203e 3e3e 206d 6f64 656c         >>> model
-0000e200: 203d 204d 756c 7469 4865 6164 4174 7465   = MultiHeadAtte
-0000e210: 6e74 696f 6e28 6261 7463 685f 7369 7a65  ntion(batch_size
-0000e220: 3d32 2c20 6869 6464 656e 5f73 697a 653d  =2, hidden_size=
-0000e230: 3135 2c20 7372 635f 7365 715f 6c65 6e67  15, src_seq_leng
-0000e240: 7468 3d32 302c 2074 6774 5f73 6571 5f6c  th=20, tgt_seq_l
-0000e250: 656e 6774 683d 3230 2c0a 2020 2020 2020  ength=20,.      
-0000e260: 2020 2020 2020 2e2e 2e20 2020 2020 2020        ...       
-0000e270: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000e280: 2020 2020 206e 756d 5f68 6561 6473 3d33       num_heads=3
-0000e290: 2c20 7573 655f 7061 7374 3d54 7275 6529  , use_past=True)
-0000e2a0: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
-0000e2b0: 206b 6579 5f70 6173 7420 3d20 5465 6e73   key_past = Tens
-0000e2c0: 6f72 286e 702e 7a65 726f 7328 7368 6170  or(np.zeros(shap
-0000e2d0: 653d 2832 2c20 332c 2035 2c20 3230 2929  e=(2, 3, 5, 20))
-0000e2e0: 2c20 6d73 7479 7065 2e66 6c6f 6174 3136  , mstype.float16
-0000e2f0: 290a 2020 2020 2020 2020 2020 2020 3e3e  ).            >>
-0000e300: 3e20 7661 6c75 655f 7061 7374 203d 2054  > value_past = T
-0000e310: 656e 736f 7228 6e70 2e7a 6572 6f73 2873  ensor(np.zeros(s
-0000e320: 6861 7065 3d28 322c 2033 2c20 3230 2c20  hape=(2, 3, 20, 
-0000e330: 3529 292c 206d 7374 7970 652e 666c 6f61  5)), mstype.floa
-0000e340: 7431 3629 0a20 2020 2020 2020 2020 2020  t16).           
-0000e350: 203e 3e3e 2062 6174 6368 5f76 616c 6964   >>> batch_valid
-0000e360: 5f6c 656e 6774 6820 3d20 5465 6e73 6f72  _length = Tensor
-0000e370: 286e 702e 6f6e 6573 2828 322c 2929 2c20  (np.ones((2,)), 
-0000e380: 6d73 7479 7065 2e69 6e74 3332 290a 2020  mstype.int32).  
-0000e390: 2020 2020 2020 2020 2020 3e3e 3e20 2320            >>> # 
-0000e3a0: 5365 7420 6973 5f66 6972 7374 5f69 7465  Set is_first_ite
-0000e3b0: 7261 7469 6f6e 3d54 7275 6520 746f 2067  ration=True to g
-0000e3c0: 656e 6572 6174 6520 7468 6520 6675 6c6c  enerate the full
-0000e3d0: 206d 656d 6f72 7920 7374 6174 6573 0a20   memory states. 
-0000e3e0: 2020 2020 2020 2020 2020 203e 3e3e 206d             >>> m
-0000e3f0: 6f64 656c 2e61 6464 5f66 6c61 6773 5f72  odel.add_flags_r
-0000e400: 6563 7572 7369 7665 2869 735f 6669 7273  ecursive(is_firs
-0000e410: 745f 6974 6572 6174 696f 6e3d 5472 7565  t_iteration=True
-0000e420: 290a 2020 2020 2020 2020 2020 2020 3e3e  ).            >>
-0000e430: 3e20 6174 746e 5f6f 7574 2c20 7061 7374  > attn_out, past
-0000e440: 203d 206d 6f64 656c 2866 726f 6d5f 7465   = model(from_te
-0000e450: 6e73 6f72 2c20 746f 5f74 656e 736f 722c  nsor, to_tensor,
-0000e460: 2074 6f5f 7465 6e73 6f72 2c20 6174 7465   to_tensor, atte
-0000e470: 6e74 696f 6e5f 6d61 736b 2c20 6b65 795f  ntion_mask, key_
-0000e480: 7061 7374 2c20 7661 6c75 655f 7061 7374  past, value_past
-0000e490: 2c0a 2020 2020 2020 2020 2020 2020 2e2e  ,.            ..
-0000e4a0: 2e20 2020 2020 2020 2020 2020 2020 2020  .               
-0000e4b0: 2020 2020 2020 2020 2062 6174 6368 5f76           batch_v
-0000e4c0: 616c 6964 5f6c 656e 6774 6829 0a20 2020  alid_length).   
-0000e4d0: 2020 2020 2020 2020 203e 3e3e 2070 7269           >>> pri
-0000e4e0: 6e74 2861 7474 6e5f 6f75 742e 7368 6170  nt(attn_out.shap
-0000e4f0: 6529 0a20 2020 2020 2020 2020 2020 2028  e).            (
-0000e500: 322c 2032 302c 2031 3529 0a20 2020 2020  2, 20, 15).     
-0000e510: 2020 2020 2020 203e 3e3e 2070 7269 6e74         >>> print
-0000e520: 2870 6173 745b 305d 2e73 6861 7065 290a  (past[0].shape).
-0000e530: 2020 2020 2020 2020 2020 2020 2832 2c20              (2, 
-0000e540: 332c 2035 2c20 3230 290a 2020 2020 2020  3, 5, 20).      
-0000e550: 2020 2020 2020 3e3e 3e20 7072 696e 7428        >>> print(
-0000e560: 7061 7374 5b31 5d2e 7368 6170 6529 0a20  past[1].shape). 
-0000e570: 2020 2020 2020 2020 2020 2028 322c 2033             (2, 3
-0000e580: 2c20 3230 2c20 3529 0a20 2020 2020 2020  , 20, 5).       
-0000e590: 2020 2020 203e 3e3e 2066 726f 6d5f 7465       >>> from_te
-0000e5a0: 6e73 6f72 203d 2054 656e 736f 7228 6e70  nsor = Tensor(np
-0000e5b0: 2e6f 6e65 7328 2832 2c20 312c 2031 3529  .ones((2, 1, 15)
-0000e5c0: 292c 206d 7374 7970 652e 666c 6f61 7433  ), mstype.float3
-0000e5d0: 3229 0a20 2020 2020 2020 2020 2020 203e  2).            >
-0000e5e0: 3e3e 2074 6f5f 7465 6e73 6f72 203d 2054  >> to_tensor = T
-0000e5f0: 656e 736f 7228 6e70 2e6f 6e65 7328 2832  ensor(np.ones((2
-0000e600: 2c20 312c 2031 3529 292c 206d 7374 7970  , 1, 15)), mstyp
-0000e610: 652e 666c 6f61 7431 3629 0a20 2020 2020  e.float16).     
-0000e620: 2020 2020 2020 203e 3e3e 2061 7474 656e         >>> atten
-0000e630: 7469 6f6e 5f6d 6173 6b20 3d20 5465 6e73  tion_mask = Tens
-0000e640: 6f72 286e 702e 6f6e 6573 2828 322c 2031  or(np.ones((2, 1
-0000e650: 2c20 3230 2929 2c20 6d73 7479 7065 2e66  , 20)), mstype.f
-0000e660: 6c6f 6174 3136 290a 2020 2020 2020 2020  loat16).        
-0000e670: 2020 2020 3e3e 3e20 2320 5374 6570 2032      >>> # Step 2
-0000e680: 3a20 7365 7420 6973 5f66 6972 7374 5f69  : set is_first_i
-0000e690: 7465 7261 7469 6f6e 3d46 616c 7365 2c20  teration=False, 
-0000e6a0: 616e 6420 7061 7373 2074 6865 2073 696e  and pass the sin
-0000e6b0: 676c 6520 776f 7264 2074 6f20 7275 6e20  gle word to run 
-0000e6c0: 7468 6520 7072 6564 6963 7469 6f6e 2072  the prediction r
-0000e6d0: 6174 6865 7220 7468 616e 2074 6865 0a20  ather than the. 
-0000e6e0: 2020 2020 2020 2020 2020 203e 3e3e 2023             >>> #
-0000e6f0: 2066 756c 6c20 7365 7175 656e 6365 2e0a   full sequence..
-0000e700: 2020 2020 2020 2020 2020 2020 3e3e 3e20              >>> 
-0000e710: 6d6f 6465 6c2e 6164 645f 666c 6167 735f  model.add_flags_
-0000e720: 7265 6375 7273 6976 6528 6973 5f66 6972  recursive(is_fir
-0000e730: 7374 5f69 7465 7261 7469 6f6e 3d46 616c  st_iteration=Fal
-0000e740: 7365 290a 2020 2020 2020 2020 2020 2020  se).            
-0000e750: 3e3e 3e20 6174 746e 5f6f 7574 2c20 7061  >>> attn_out, pa
-0000e760: 7374 203d 206d 6f64 656c 2866 726f 6d5f  st = model(from_
-0000e770: 7465 6e73 6f72 2c20 746f 5f74 656e 736f  tensor, to_tenso
-0000e780: 722c 2074 6f5f 7465 6e73 6f72 2c20 6174  r, to_tensor, at
-0000e790: 7465 6e74 696f 6e5f 6d61 736b 2c20 6b65  tention_mask, ke
-0000e7a0: 795f 7061 7374 2c20 7661 6c75 655f 7061  y_past, value_pa
-0000e7b0: 7374 2c0a 2020 2020 2020 2020 2020 2020  st,.            
-0000e7c0: 2e2e 2e20 2020 2020 2020 2020 2020 2020  ...             
-0000e7d0: 2020 2020 2020 2020 2020 2062 6174 6368             batch
-0000e7e0: 5f76 616c 6964 5f6c 656e 6774 6829 0a20  _valid_length). 
-0000e7f0: 2020 2020 2020 2020 2020 203e 3e3e 2070             >>> p
-0000e800: 7269 6e74 2861 7474 6e5f 6f75 742e 7368  rint(attn_out.sh
-0000e810: 6170 6529 0a20 2020 2020 2020 2020 2020  ape).           
-0000e820: 2028 322c 2031 2c20 3135 290a 2020 2020   (2, 1, 15).    
-0000e830: 2020 2020 2020 2020 3e3e 3e20 7072 696e          >>> prin
-0000e840: 7428 7061 7374 5b30 5d2e 7368 6170 6529  t(past[0].shape)
-0000e850: 0a20 2020 2020 2020 2020 2020 2028 322c  .            (2,
-0000e860: 2033 2c20 352c 2032 3029 0a20 2020 2020   3, 5, 20).     
-0000e870: 2020 2020 2020 203e 3e3e 2070 7269 6e74         >>> print
-0000e880: 2870 6173 745b 315d 2e73 6861 7065 290a  (past[1].shape).
-0000e890: 2020 2020 2020 2020 2020 2020 2832 2c20              (2, 
-0000e8a0: 332c 2032 302c 2035 290a 2020 2020 2222  3, 20, 5).    ""
-0000e8b0: 220a 0a20 2020 2040 5f4c 6f67 4163 7469  "..    @_LogActi
-0000e8c0: 6f6e 4f6e 6365 286d 5f6c 6f67 6765 723d  onOnce(m_logger=
-0000e8d0: 6c6f 6767 6572 2c20 6b65 793d 274d 756c  logger, key='Mul
-0000e8e0: 7469 4865 6164 4174 7465 6e74 696f 6e27  tiHeadAttention'
-0000e8f0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-0000e900: 2020 2020 2020 6e6f 5f77 6172 6e69 6e67        no_warning
-0000e910: 3d5f 6765 745f 7061 7261 6c6c 656c 5f6d  =_get_parallel_m
-0000e920: 6f64 6528 2920 696e 2028 5061 7261 6c6c  ode() in (Parall
-0000e930: 656c 4d6f 6465 2e53 5441 4e44 5f41 4c4f  elMode.STAND_ALO
-0000e940: 4e45 2c29 290a 2020 2020 405f 6172 6773  NE,)).    @_args
-0000e950: 5f74 7970 655f 7661 6c69 6461 746f 725f  _type_validator_
-0000e960: 6368 6563 6b28 6869 6464 656e 5f73 697a  check(hidden_siz
-0000e970: 653d 5661 6c69 6461 746f 722e 6368 6563  e=Validator.chec
-0000e980: 6b5f 706f 7369 7469 7665 5f69 6e74 2c0a  k_positive_int,.
+00007e20: 2020 2020 2020 2020 2020 2020 7374 7261              stra
+00007e30: 7465 6779 5f62 6961 733d 2828 6470 2c20  tegy_bias=((dp, 
+00007e40: 3129 2c20 2831 2c29 2929 0a20 2020 2020  1), (1,))).     
+00007e50: 2020 2020 2020 2073 656c 662e 7072 6f6a         self.proj
+00007e60: 6563 7469 6f6e 2e62 6961 732e 7061 7261  ection.bias.para
+00007e70: 6c6c 656c 5f6f 7074 696d 697a 6572 203d  llel_optimizer =
+00007e80: 2046 616c 7365 0a20 2020 2020 2020 2020   False.         
+00007e90: 2020 2073 656c 662e 6472 6f70 6f75 7420     self.dropout 
+00007ea0: 3d20 6765 745f 6472 6f70 6f75 7428 6472  = get_dropout(dr
+00007eb0: 6f70 6f75 745f 7261 7465 290a 2020 2020  opout_rate).    
+00007ec0: 2020 2020 2020 2020 7365 6c66 2e64 726f          self.dro
+00007ed0: 706f 7574 5f33 6420 3d20 6765 745f 6472  pout_3d = get_dr
+00007ee0: 6f70 6f75 7428 6472 6f70 6f75 745f 7261  opout(dropout_ra
+00007ef0: 7465 290a 2020 2020 2020 2020 2020 2020  te).            
+00007f00: 7365 6c66 2e64 726f 706f 7574 5f34 6420  self.dropout_4d 
+00007f10: 3d20 6765 745f 6472 6f70 6f75 7428 6472  = get_dropout(dr
+00007f20: 6f70 6f75 745f 7261 7465 290a 2020 2020  opout_rate).    
+00007f30: 2020 2020 2020 2020 7365 6c66 2e64 726f          self.dro
+00007f40: 706f 7574 2e64 726f 706f 7574 2e73 6861  pout.dropout.sha
+00007f50: 7264 2828 2864 702c 2031 292c 2929 0a20  rd(((dp, 1),)). 
+00007f60: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+00007f70: 6472 6f70 6f75 745f 3364 2e64 726f 706f  dropout_3d.dropo
+00007f80: 7574 2e73 6861 7264 2828 2864 702c 2031  ut.shard(((dp, 1
+00007f90: 2c20 3129 2c29 290a 2020 2020 2020 2020  , 1),)).        
+00007fa0: 2020 2020 7365 6c66 2e64 726f 706f 7574      self.dropout
+00007fb0: 5f34 642e 6472 6f70 6f75 742e 7368 6172  _4d.dropout.shar
+00007fc0: 6428 2828 6470 2c20 6570 2c20 312c 2031  d(((dp, ep, 1, 1
+00007fd0: 292c 2929 0a20 2020 2020 2020 2020 2020  ),)).           
+00007fe0: 2073 656c 662e 6361 7374 203d 2050 2e43   self.cast = P.C
+00007ff0: 6173 7428 290a 0a20 2020 2064 6566 2063  ast()..    def c
+00008000: 6f6e 7374 7275 6374 2873 656c 662c 2078  onstruct(self, x
+00008010: 293a 0a20 2020 2020 2020 2022 2222 466f  ):.        """Fo
+00008020: 7277 6172 6420 7072 6f63 6573 7320 6f66  rward process of
+00008030: 2074 6865 2046 6565 6446 6f72 7761 7264   the FeedForward
+00008040: 2222 220a 2020 2020 2020 2020 5f63 6865  """.        _che
+00008050: 636b 5f69 6e70 7574 5f64 7479 7065 2846  ck_input_dtype(F
+00008060: 2e64 7479 7065 2878 292c 2022 7822 2c20  .dtype(x), "x", 
+00008070: 5b6d 7374 7970 652e 666c 6f61 7433 322c  [mstype.float32,
+00008080: 206d 7374 7970 652e 666c 6f61 7431 362c   mstype.float16,
+00008090: 206d 7374 7970 652e 6266 6c6f 6174 3136   mstype.bfloat16
+000080a0: 5d2c 2073 656c 662e 636c 735f 6e61 6d65  ], self.cls_name
+000080b0: 290a 2020 2020 2020 2020 7820 3d20 7365  ).        x = se
+000080c0: 6c66 2e63 6173 7428 782c 206d 7374 7970  lf.cast(x, mstyp
+000080d0: 652e 666c 6f61 7431 3629 0a20 2020 2020  e.float16).     
+000080e0: 2020 2023 2072 6574 7572 6e65 6420 7368     # returned sh
+000080f0: 6170 6520 6973 205b 6273 2c20 7365 715f  ape is [bs, seq_
+00008100: 6c65 6e67 7468 2c20 6666 6e5f 6869 6464  length, ffn_hidd
+00008110: 656e 5f73 697a 655d 206f 7220 5b62 7320  en_size] or [bs 
+00008120: 2a20 7365 715f 6c65 6e67 7468 2c20 6666  * seq_length, ff
+00008130: 6e5f 6869 6464 656e 5f73 697a 655d 0a20  n_hidden_size]. 
+00008140: 2020 2020 2020 2068 6964 6465 6e20 3d20         hidden = 
+00008150: 7365 6c66 2e6d 6170 7069 6e67 2878 290a  self.mapping(x).
+00008160: 2020 2020 2020 2020 6f75 7470 7574 203d          output =
+00008170: 2073 656c 662e 7072 6f6a 6563 7469 6f6e   self.projection
+00008180: 2868 6964 6465 6e29 0a20 2020 2020 2020  (hidden).       
+00008190: 2023 2072 6574 7572 6e65 6420 7368 6170   # returned shap
+000081a0: 6520 6973 205b 6273 2c20 7365 715f 6c65  e is [bs, seq_le
+000081b0: 6e67 7468 2c20 6666 6e5f 6869 6464 656e  ngth, ffn_hidden
+000081c0: 5f73 697a 655d 206f 7220 5b62 7320 2a20  _size] or [bs * 
+000081d0: 7365 715f 6c65 6e67 7468 2c20 6666 6e5f  seq_length, ffn_
+000081e0: 6869 6464 656e 5f73 697a 655d 0a20 2020  hidden_size].   
+000081f0: 2020 2020 2069 6620 6c65 6e28 462e 7368       if len(F.sh
+00008200: 6170 6528 6f75 7470 7574 2929 203d 3d20  ape(output)) == 
+00008210: 333a 0a20 2020 2020 2020 2020 2020 206f  3:.            o
+00008220: 7574 7075 7420 3d20 7365 6c66 2e64 726f  utput = self.dro
+00008230: 706f 7574 5f33 6428 6f75 7470 7574 290a  pout_3d(output).
+00008240: 2020 2020 2020 2020 656c 6966 206c 656e          elif len
+00008250: 2846 2e73 6861 7065 286f 7574 7075 7429  (F.shape(output)
+00008260: 2920 3d3d 2032 3a0a 2020 2020 2020 2020  ) == 2:.        
+00008270: 2020 2020 6f75 7470 7574 203d 2073 656c      output = sel
+00008280: 662e 6472 6f70 6f75 7428 6f75 7470 7574  f.dropout(output
+00008290: 290a 2020 2020 2020 2020 656c 7365 3a0a  ).        else:.
+000082a0: 2020 2020 2020 2020 2020 2020 6f75 7470              outp
+000082b0: 7574 203d 2073 656c 662e 6472 6f70 6f75  ut = self.dropou
+000082c0: 745f 3464 286f 7574 7075 7429 0a20 2020  t_4d(output).   
+000082d0: 2020 2020 2072 6574 7572 6e20 6f75 7470       return outp
+000082e0: 7574 0a0a 0a63 6c61 7373 2041 7474 656e  ut...class Atten
+000082f0: 7469 6f6e 4d61 736b 2843 656c 6c29 3a0a  tionMask(Cell):.
+00008300: 2020 2020 7222 2222 0a20 2020 2020 2020      r""".       
+00008310: 2047 6574 2074 6865 204c 6f77 6572 2074   Get the Lower t
+00008320: 7269 616e 6775 6c61 7220 6d61 7472 6978  riangular matrix
+00008330: 2066 726f 6d20 7468 6520 696e 7075 7420   from the input 
+00008340: 6d61 736b 2e20 5468 6520 696e 7075 7420  mask. The input 
+00008350: 6d61 736b 2069 7320 6120 3244 2074 656e  mask is a 2D ten
+00008360: 736f 7220 2862 6174 6368 5f73 697a 652c  sor (batch_size,
+00008370: 2073 6571 5f6c 656e 6774 6829 0a20 2020   seq_length).   
+00008380: 2020 2020 2077 6974 6820 3120 616e 6420       with 1 and 
+00008390: 302c 2077 6865 7265 2031 2069 6e64 6963  0, where 1 indic
+000083a0: 6174 6573 2074 6865 2063 7572 7265 6e74  ates the current
+000083b0: 2070 6f73 6974 696f 6e20 6973 2061 2076   position is a v
+000083c0: 616c 6964 2074 6f6b 656e 2c20 6f74 6865  alid token, othe
+000083d0: 7277 6973 6520 6e6f 742e 0a0a 2020 2020  rwise not...    
+000083e0: 2020 2020 4172 6773 3a0a 2020 2020 2020      Args:.      
+000083f0: 2020 2020 2020 7365 715f 6c65 6e67 7468        seq_length
+00008400: 2869 6e74 293a 2054 6865 2073 6571 7565  (int): The seque
+00008410: 6e63 6520 6c65 6e67 7468 206f 6620 7468  nce length of th
+00008420: 6520 696e 7075 7420 7465 6e73 6f72 2e0a  e input tensor..
+00008430: 2020 2020 2020 2020 2020 2020 7061 7261              para
+00008440: 6c6c 656c 5f63 6f6e 6669 6728 4f70 5061  llel_config(OpPa
+00008450: 7261 6c6c 656c 436f 6e66 6967 293a 2054  rallelConfig): T
+00008460: 6865 2070 6172 616c 6c65 6c20 636f 6e66  he parallel conf
+00008470: 6967 7572 652e 2044 6566 6175 6c74 2060  igure. Default `
+00008480: 6465 6661 756c 745f 6470 6d70 5f63 6f6e  default_dpmp_con
+00008490: 6669 6760 2c0a 2020 2020 2020 2020 2020  fig`,.          
+000084a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000084b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000084c0: 2020 2020 2061 6e20 696e 7374 616e 6365       an instance
+000084d0: 206f 6620 604f 7050 6172 616c 6c65 6c43   of `OpParallelC
+000084e0: 6f6e 6669 6760 2077 6974 6820 6465 6661  onfig` with defa
+000084f0: 756c 7420 6172 6773 2e0a 0a20 2020 2020  ult args...     
+00008500: 2020 2049 6e70 7574 733a 0a20 2020 2020     Inputs:.     
+00008510: 2020 2020 2020 202d 202a 2a69 6e70 7574         - **input
+00008520: 5f6d 6173 6b2a 2a20 2854 656e 736f 7229  _mask** (Tensor)
+00008530: 202d 2054 6865 206d 6173 6b20 696e 6469   - The mask indi
+00008540: 6361 7469 6e67 2077 6865 7468 6572 2065  cating whether e
+00008550: 6163 6820 706f 7369 7469 6f6e 2069 7320  ach position is 
+00008560: 6120 7661 6c69 6420 696e 7075 7420 7769  a valid input wi
+00008570: 7468 0a20 2020 2020 2020 2020 2020 2020  th.             
+00008580: 2028 6261 7463 685f 7369 7a65 2c20 7365   (batch_size, se
+00008590: 715f 6c65 6e67 7468 292e 0a0a 2020 2020  q_length)...    
+000085a0: 2020 2020 4f75 7470 7574 733a 0a20 2020      Outputs:.   
+000085b0: 2020 2020 2020 2020 2054 656e 736f 722e           Tensor.
+000085c0: 2054 6865 2061 7474 656e 7469 6f6e 206d   The attention m
+000085d0: 6173 6b20 6d61 7472 6978 2077 6974 6820  ask matrix with 
+000085e0: 7368 6170 6520 2862 6174 6368 5f73 697a  shape (batch_siz
+000085f0: 652c 2073 6571 5f6c 656e 6774 682c 2073  e, seq_length, s
+00008600: 6571 5f6c 656e 6774 6829 2e0a 0a20 2020  eq_length)...   
+00008610: 2020 2020 2052 6169 7365 733a 0a20 2020       Raises:.   
+00008620: 2020 2020 2020 2020 2054 7970 6545 7272           TypeErr
+00008630: 6f72 3a20 6073 6571 5f6c 656e 6774 6860  or: `seq_length`
+00008640: 2069 7320 6e6f 7420 616e 2069 6e74 6567   is not an integ
+00008650: 6572 2e0a 2020 2020 2020 2020 2020 2020  er..            
+00008660: 5661 6c75 6545 7272 6f72 3a20 6073 6571  ValueError: `seq
+00008670: 5f6c 656e 6774 6860 2069 7320 6e6f 7420  _length` is not 
+00008680: 6120 706f 7369 7469 7665 2076 616c 7565  a positive value
+00008690: 2e0a 2020 2020 2020 2020 2020 2020 5479  ..            Ty
+000086a0: 7065 4572 726f 723a 2060 7061 7261 6c6c  peError: `parall
+000086b0: 656c 5f63 6f6e 6669 6760 2069 7320 6e6f  el_config` is no
+000086c0: 7420 6120 7375 6263 6c61 7373 206f 6620  t a subclass of 
+000086d0: 4f70 5061 7261 6c6c 656c 436f 6e66 6967  OpParallelConfig
+000086e0: 2e0a 0a20 2020 2020 2020 2053 7570 706f  ...        Suppo
+000086f0: 7274 6564 2050 6c61 7466 6f72 6d73 3a0a  rted Platforms:.
+00008700: 2020 2020 2020 2020 2020 2020 6060 4173              ``As
+00008710: 6365 6e64 6060 2060 6047 5055 6060 0a0a  cend`` ``GPU``..
+00008720: 2020 2020 2020 2020 4578 616d 706c 6573          Examples
+00008730: 3a0a 2020 2020 2020 2020 2020 2020 3e3e  :.            >>
+00008740: 3e20 696d 706f 7274 206e 756d 7079 2061  > import numpy a
+00008750: 7320 6e70 0a20 2020 2020 2020 2020 2020  s np.           
+00008760: 203e 3e3e 2066 726f 6d20 6d69 6e64 666f   >>> from mindfo
+00008770: 726d 6572 732e 6d6f 6475 6c65 732e 7472  rmers.modules.tr
+00008780: 616e 7366 6f72 6d65 7220 696d 706f 7274  ansformer import
+00008790: 2041 7474 656e 7469 6f6e 4d61 736b 0a20   AttentionMask. 
+000087a0: 2020 2020 2020 2020 2020 203e 3e3e 2066             >>> f
+000087b0: 726f 6d20 6d69 6e64 7370 6f72 6520 696d  rom mindspore im
+000087c0: 706f 7274 2054 656e 736f 720a 2020 2020  port Tensor.    
+000087d0: 2020 2020 2020 2020 3e3e 3e20 6d61 736b          >>> mask
+000087e0: 203d 2041 7474 656e 7469 6f6e 4d61 736b   = AttentionMask
+000087f0: 2873 6571 5f6c 656e 6774 683d 3429 0a20  (seq_length=4). 
+00008800: 2020 2020 2020 2020 2020 203e 3e3e 206d             >>> m
+00008810: 6173 6b5f 6172 7261 7920 3d20 6e70 2e61  ask_array = np.a
+00008820: 7272 6179 285b 5b31 2c20 312c 2031 2c20  rray([[1, 1, 1, 
+00008830: 305d 5d2c 206e 702e 666c 6f61 7433 3229  0]], np.float32)
+00008840: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
+00008850: 2069 6e70 7574 7320 3d20 5465 6e73 6f72   inputs = Tensor
+00008860: 286d 6173 6b5f 6172 7261 7929 0a20 2020  (mask_array).   
+00008870: 2020 2020 2020 2020 203e 3e3e 2072 6573           >>> res
+00008880: 203d 206d 6173 6b28 696e 7075 7473 290a   = mask(inputs).
+00008890: 2020 2020 2020 2020 2020 2020 3e3e 3e20              >>> 
+000088a0: 7072 696e 7428 7265 7329 0a20 2020 2020  print(res).     
+000088b0: 2020 2020 2020 205b 5b5b 312e 2030 2e20         [[[1. 0. 
+000088c0: 302e 2030 5d0a 2020 2020 2020 2020 2020  0. 0].          
+000088d0: 2020 2020 5b31 2e20 312e 2030 2e20 305d      [1. 1. 0. 0]
+000088e0: 0a20 2020 2020 2020 2020 2020 2020 205b  .              [
+000088f0: 312e 2031 2e20 312e 2030 5d0a 2020 2020  1. 1. 1. 0].    
+00008900: 2020 2020 2020 2020 2020 5b30 2e20 302e            [0. 0.
+00008910: 2030 2e20 305d 5d5d 0a20 2020 2022 2222   0. 0]]].    """
+00008920: 0a0a 2020 2020 405f 4c6f 6741 6374 696f  ..    @_LogActio
+00008930: 6e4f 6e63 6528 6d5f 6c6f 6767 6572 3d6c  nOnce(m_logger=l
+00008940: 6f67 6765 722c 206b 6579 3d27 4174 7465  ogger, key='Atte
+00008950: 6e74 696f 6e4d 6173 6b27 2c0a 2020 2020  ntionMask',.    
+00008960: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00008970: 6e6f 5f77 6172 6e69 6e67 3d5f 6765 745f  no_warning=_get_
+00008980: 7061 7261 6c6c 656c 5f6d 6f64 6528 2920  parallel_mode() 
+00008990: 696e 2028 5061 7261 6c6c 656c 4d6f 6465  in (ParallelMode
+000089a0: 2e53 5441 4e44 5f41 4c4f 4e45 2c29 290a  .STAND_ALONE,)).
+000089b0: 2020 2020 405f 6172 6773 5f74 7970 655f      @_args_type_
+000089c0: 7661 6c69 6461 746f 725f 6368 6563 6b28  validator_check(
+000089d0: 7365 715f 6c65 6e67 7468 3d56 616c 6964  seq_length=Valid
+000089e0: 6174 6f72 2e63 6865 636b 5f70 6f73 6974  ator.check_posit
+000089f0: 6976 655f 696e 742c 0a20 2020 2020 2020  ive_int,.       
+00008a00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00008a10: 2020 2020 2020 2020 2070 6172 616c 6c65           paralle
+00008a20: 6c5f 636f 6e66 6967 3d5f 7661 6c69 645f  l_config=_valid_
+00008a30: 7479 7065 5f63 6865 636b 7328 5b4f 7050  type_checks([OpP
+00008a40: 6172 616c 6c65 6c43 6f6e 6669 675d 2c20  arallelConfig], 
+00008a50: 2241 7474 656e 7469 6f6e 4d61 736b 2229  "AttentionMask")
+00008a60: 290a 2020 2020 6465 6620 5f5f 696e 6974  ).    def __init
+00008a70: 5f5f 2873 656c 662c 2073 6571 5f6c 656e  __(self, seq_len
+00008a80: 6774 682c 2070 6172 616c 6c65 6c5f 636f  gth, parallel_co
+00008a90: 6e66 6967 3d64 6566 6175 6c74 5f64 706d  nfig=default_dpm
+00008aa0: 705f 636f 6e66 6967 2c20 636f 6d70 7574  p_config, comput
+00008ab0: 655f 6474 7970 653d 6d73 7479 7065 2e66  e_dtype=mstype.f
+00008ac0: 6c6f 6174 3136 293a 0a20 2020 2020 2020  loat16):.       
+00008ad0: 2073 7570 6572 2841 7474 656e 7469 6f6e   super(Attention
+00008ae0: 4d61 736b 2c20 7365 6c66 292e 5f5f 696e  Mask, self).__in
+00008af0: 6974 5f5f 2829 0a20 2020 2020 2020 2073  it__().        s
+00008b00: 656c 662e 7365 715f 6c65 6e67 7468 203d  elf.seq_length =
+00008b10: 2073 6571 5f6c 656e 6774 680a 2020 2020   seq_length.    
+00008b20: 2020 2020 7365 6c66 2e63 6f6d 7075 7465      self.compute
+00008b30: 5f64 7479 7065 203d 2063 6f6d 7075 7465  _dtype = compute
+00008b40: 5f64 7479 7065 0a20 2020 2020 2020 2073  _dtype.        s
+00008b50: 656c 662e 6e6f 745f 6571 7561 6c20 3d20  elf.not_equal = 
+00008b60: 502e 4e6f 7445 7175 616c 2829 2e73 6861  P.NotEqual().sha
+00008b70: 7264 2828 2870 6172 616c 6c65 6c5f 636f  rd(((parallel_co
+00008b80: 6e66 6967 2e64 6174 615f 7061 7261 6c6c  nfig.data_parall
+00008b90: 656c 2c20 3129 2c20 2829 2929 0a20 2020  el, 1), ())).   
+00008ba0: 2020 2020 2073 656c 662e 7265 7368 6170       self.reshap
+00008bb0: 6520 3d20 502e 5265 7368 6170 6528 290a  e = P.Reshape().
+00008bc0: 2020 2020 2020 2020 7365 6c66 2e6d 756c          self.mul
+00008bd0: 203d 2050 2e42 6174 6368 4d61 744d 756c   = P.BatchMatMul
+00008be0: 2829 2e73 6861 7264 280a 2020 2020 2020  ().shard(.      
+00008bf0: 2020 2020 2020 2828 7061 7261 6c6c 656c        ((parallel
+00008c00: 5f63 6f6e 6669 672e 6461 7461 5f70 6172  _config.data_par
+00008c10: 616c 6c65 6c2c 2031 2c20 3129 2c20 2870  allel, 1, 1), (p
+00008c20: 6172 616c 6c65 6c5f 636f 6e66 6967 2e64  arallel_config.d
+00008c30: 6174 615f 7061 7261 6c6c 656c 2c20 312c  ata_parallel, 1,
+00008c40: 2031 2929 290a 2020 2020 2020 2020 7365   1))).        se
+00008c50: 6c66 2e65 7870 616e 645f 6469 6d20 3d20  lf.expand_dim = 
+00008c60: 502e 4578 7061 6e64 4469 6d73 2829 2e73  P.ExpandDims().s
+00008c70: 6861 7264 2828 2831 2c20 3129 2c29 290a  hard(((1, 1),)).
+00008c80: 2020 2020 2020 2020 6f6e 6573 203d 206e          ones = n
+00008c90: 702e 6f6e 6573 2873 6861 7065 3d28 7365  p.ones(shape=(se
+00008ca0: 715f 6c65 6e67 7468 2c20 7365 715f 6c65  q_length, seq_le
+00008cb0: 6e67 7468 2929 0a20 2020 2020 2020 2023  ngth)).        #
+00008cc0: 2044 6566 6175 6c74 206c 6f77 6572 2074   Default lower t
+00008cd0: 7269 616e 676c 6520 6d61 736b 206d 6174  riangle mask mat
+00008ce0: 7269 780a 2020 2020 2020 2020 7365 6c66  rix.        self
+00008cf0: 2e6c 6f77 6572 5f74 7269 616e 676c 655f  .lower_triangle_
+00008d00: 6d61 736b 203d 2054 656e 736f 7228 6e70  mask = Tensor(np
+00008d10: 2e74 7269 6c28 6f6e 6573 292c 206d 7374  .tril(ones), mst
+00008d20: 7970 652e 666c 6f61 7433 3229 0a20 2020  ype.float32).   
+00008d30: 2020 2020 2073 656c 662e 6d75 6c74 6970       self.multip
+00008d40: 6c79 203d 2050 2e4d 756c 2829 2e73 6861  ly = P.Mul().sha
+00008d50: 7264 2828 2870 6172 616c 6c65 6c5f 636f  rd(((parallel_co
+00008d60: 6e66 6967 2e64 6174 615f 7061 7261 6c6c  nfig.data_parall
+00008d70: 656c 2c20 312c 2031 292c 2028 312c 2031  el, 1, 1), (1, 1
+00008d80: 2c20 3129 2929 0a0a 2020 2020 6465 6620  , 1)))..    def 
+00008d90: 636f 6e73 7472 7563 7428 7365 6c66 2c20  construct(self, 
+00008da0: 696e 7075 745f 6d61 736b 293a 0a20 2020  input_mask):.   
+00008db0: 2020 2020 2022 2222 466f 7277 6172 6420       """Forward 
+00008dc0: 7072 6f63 6573 7320 6f66 2074 6865 2041  process of the A
+00008dd0: 7474 656e 7469 6f6e 4d61 736b 2222 220a  ttentionMask""".
+00008de0: 2020 2020 2020 2020 5f63 6865 636b 5f69          _check_i
+00008df0: 6e70 7574 5f64 7479 7065 2846 2e64 7479  nput_dtype(F.dty
+00008e00: 7065 2869 6e70 7574 5f6d 6173 6b29 2c20  pe(input_mask), 
+00008e10: 2269 6e70 7574 5f6d 6173 6b22 2c0a 2020  "input_mask",.  
+00008e20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00008e30: 2020 2020 2020 2020 205b 6d73 7479 7065           [mstype
+00008e40: 2e66 6c6f 6174 3332 2c20 6d73 7479 7065  .float32, mstype
+00008e50: 2e66 6c6f 6174 3136 2c20 6d73 7479 7065  .float16, mstype
+00008e60: 2e62 666c 6f61 7431 365d 2c20 7365 6c66  .bfloat16], self
+00008e70: 2e63 6c73 5f6e 616d 6529 0a20 2020 2020  .cls_name).     
+00008e80: 2020 2069 6e70 7574 5f6d 6173 6b20 3d20     input_mask = 
+00008e90: 502e 4361 7374 2829 2873 656c 662e 6e6f  P.Cast()(self.no
+00008ea0: 745f 6571 7561 6c28 696e 7075 745f 6d61  t_equal(input_ma
+00008eb0: 736b 2c20 3029 2c20 7365 6c66 2e63 6f6d  sk, 0), self.com
+00008ec0: 7075 7465 5f64 7479 7065 290a 2020 2020  pute_dtype).    
+00008ed0: 2020 2020 696e 7075 745f 7368 6170 6520      input_shape 
+00008ee0: 3d20 502e 5368 6170 6528 2928 696e 7075  = P.Shape()(inpu
+00008ef0: 745f 6d61 736b 290a 2020 2020 2020 2020  t_mask).        
+00008f00: 7368 6170 655f 7269 6768 7420 3d20 2869  shape_right = (i
+00008f10: 6e70 7574 5f73 6861 7065 5b30 5d2c 2031  nput_shape[0], 1
+00008f20: 2c20 696e 7075 745f 7368 6170 655b 315d  , input_shape[1]
+00008f30: 290a 2020 2020 2020 2020 7368 6170 655f  ).        shape_
+00008f40: 6c65 6674 203d 2069 6e70 7574 5f73 6861  left = input_sha
+00008f50: 7065 202b 2028 312c 290a 2020 2020 2020  pe + (1,).      
+00008f60: 2020 2320 4d61 736b 2074 6865 2070 6164    # Mask the pad
+00008f70: 6465 6420 696e 7075 7473 0a20 2020 2020  ded inputs.     
+00008f80: 2020 206d 6173 6b5f 6c65 6674 203d 2073     mask_left = s
+00008f90: 656c 662e 7265 7368 6170 6528 696e 7075  elf.reshape(inpu
+00008fa0: 745f 6d61 736b 2c20 7368 6170 655f 6c65  t_mask, shape_le
+00008fb0: 6674 290a 2020 2020 2020 2020 6d61 736b  ft).        mask
+00008fc0: 5f72 6967 6874 203d 2073 656c 662e 7265  _right = self.re
+00008fd0: 7368 6170 6528 696e 7075 745f 6d61 736b  shape(input_mask
+00008fe0: 2c20 7368 6170 655f 7269 6768 7429 0a20  , shape_right). 
+00008ff0: 2020 2020 2020 2061 7474 656e 7469 6f6e         attention
+00009000: 5f6d 6173 6b20 3d20 7365 6c66 2e6d 756c  _mask = self.mul
+00009010: 286d 6173 6b5f 6c65 6674 2c20 6d61 736b  (mask_left, mask
+00009020: 5f72 6967 6874 290a 2020 2020 2020 2020  _right).        
+00009030: 6c6f 7765 725f 7472 6169 616e 676c 6520  lower_traiangle 
+00009040: 3d20 7365 6c66 2e65 7870 616e 645f 6469  = self.expand_di
+00009050: 6d28 7365 6c66 2e6c 6f77 6572 5f74 7269  m(self.lower_tri
+00009060: 616e 676c 655f 6d61 736b 2c20 3029 0a20  angle_mask, 0). 
+00009070: 2020 2020 2020 2023 2074 6865 2072 6574         # the ret
+00009080: 7572 6e65 6420 7368 6170 6520 6973 205b  urned shape is [
+00009090: 6273 2c20 7365 715f 6c65 6e67 7468 2c20  bs, seq_length, 
+000090a0: 7365 715f 6c65 6e67 7468 5d0a 2020 2020  seq_length].    
+000090b0: 2020 2020 6174 7465 6e74 696f 6e5f 6d61      attention_ma
+000090c0: 736b 203d 2073 656c 662e 6d75 6c74 6970  sk = self.multip
+000090d0: 6c79 2861 7474 656e 7469 6f6e 5f6d 6173  ly(attention_mas
+000090e0: 6b2c 206c 6f77 6572 5f74 7261 6961 6e67  k, lower_traiang
+000090f0: 6c65 290a 2020 2020 2020 2020 7265 7475  le).        retu
+00009100: 726e 2061 7474 656e 7469 6f6e 5f6d 6173  rn attention_mas
+00009110: 6b0a 0a0a 636c 6173 7320 4174 7465 6e74  k...class Attent
+00009120: 696f 6e4d 6173 6b48 4628 4365 6c6c 293a  ionMaskHF(Cell):
+00009130: 0a20 2020 2072 2222 220a 2020 2020 2020  .    r""".      
+00009140: 2020 4765 7420 7468 6520 4c6f 7765 7220    Get the Lower 
+00009150: 7472 6961 6e67 756c 6172 206d 6174 7269  triangular matri
+00009160: 7820 6672 6f6d 2074 6865 2069 6e70 7574  x from the input
+00009170: 206d 6173 6b2e 2054 6865 2069 6e70 7574   mask. The input
+00009180: 206d 6173 6b20 6973 2061 2032 4420 7465   mask is a 2D te
+00009190: 6e73 6f72 2028 6261 7463 685f 7369 7a65  nsor (batch_size
+000091a0: 2c20 7365 715f 6c65 6e67 7468 292e 0a0a  , seq_length)...
+000091b0: 2020 2020 2020 2020 4172 6773 3a0a 2020          Args:.  
+000091c0: 2020 2020 2020 2020 2020 7365 715f 6c65            seq_le
+000091d0: 6e67 7468 2869 6e74 293a 2054 6865 2073  ngth(int): The s
+000091e0: 6571 7565 6e63 6520 6c65 6e67 7468 206f  equence length o
+000091f0: 6620 7468 6520 696e 7075 7420 7465 6e73  f the input tens
+00009200: 6f72 2e0a 2020 2020 2020 2020 2020 2020  or..            
+00009210: 7061 7261 6c6c 656c 5f63 6f6e 6669 6728  parallel_config(
+00009220: 4f70 5061 7261 6c6c 656c 436f 6e66 6967  OpParallelConfig
+00009230: 293a 2054 6865 2070 6172 616c 6c65 6c20  ): The parallel 
+00009240: 636f 6e66 6967 7572 652e 2044 6566 6175  configure. Defau
+00009250: 6c74 2060 6465 6661 756c 745f 6470 6d70  lt `default_dpmp
+00009260: 5f63 6f6e 6669 6760 2c0a 2020 2020 2020  _config`,.      
+00009270: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00009280: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00009290: 2020 2020 2020 2020 2061 6e20 696e 7374           an inst
+000092a0: 616e 6365 206f 6620 604f 7050 6172 616c  ance of `OpParal
+000092b0: 6c65 6c43 6f6e 6669 6760 2077 6974 6820  lelConfig` with 
+000092c0: 6465 6661 756c 7420 6172 6773 2e0a 0a20  default args... 
+000092d0: 2020 2020 2020 2049 6e70 7574 733a 0a20         Inputs:. 
+000092e0: 2020 2020 2020 2020 2020 202d 202a 2a69             - **i
+000092f0: 6e70 7574 5f6d 6173 6b2a 2a20 2854 656e  nput_mask** (Ten
+00009300: 736f 7229 202d 2054 6865 206d 6173 6b20  sor) - The mask 
+00009310: 696e 6469 6361 7469 6e67 2077 6865 7468  indicating wheth
+00009320: 6572 2065 6163 6820 706f 7369 7469 6f6e  er each position
+00009330: 2069 7320 6120 7661 6c69 6420 696e 7075   is a valid inpu
+00009340: 7420 7769 7468 0a20 2020 2020 2020 2020  t with.         
+00009350: 2020 2020 2028 6261 7463 685f 7369 7a65       (batch_size
+00009360: 2c20 7365 715f 6c65 6e67 7468 292e 0a0a  , seq_length)...
+00009370: 2020 2020 2020 2020 4f75 7470 7574 733a          Outputs:
+00009380: 0a20 2020 2020 2020 2020 2020 2054 656e  .            Ten
+00009390: 736f 722e 2054 6865 2061 7474 656e 7469  sor. The attenti
+000093a0: 6f6e 206d 6173 6b20 6d61 7472 6978 2077  on mask matrix w
+000093b0: 6974 6820 7368 6170 6520 2862 6174 6368  ith shape (batch
+000093c0: 5f73 697a 652c 2073 6571 5f6c 656e 6774  _size, seq_lengt
+000093d0: 682c 2073 6571 5f6c 656e 6774 6829 2e0a  h, seq_length)..
+000093e0: 0a20 2020 2020 2020 2052 6169 7365 733a  .        Raises:
+000093f0: 0a20 2020 2020 2020 2020 2020 2054 7970  .            Typ
+00009400: 6545 7272 6f72 3a20 6073 6571 5f6c 656e  eError: `seq_len
+00009410: 6774 6860 2069 7320 6e6f 7420 616e 2069  gth` is not an i
+00009420: 6e74 6567 6572 2e0a 2020 2020 2020 2020  nteger..        
+00009430: 2020 2020 5661 6c75 6545 7272 6f72 3a20      ValueError: 
+00009440: 6073 6571 5f6c 656e 6774 6860 2069 7320  `seq_length` is 
+00009450: 6e6f 7420 6120 706f 7369 7469 7665 2076  not a positive v
+00009460: 616c 7565 2e0a 2020 2020 2020 2020 2020  alue..          
+00009470: 2020 5479 7065 4572 726f 723a 2060 7061    TypeError: `pa
+00009480: 7261 6c6c 656c 5f63 6f6e 6669 6760 2069  rallel_config` i
+00009490: 7320 6e6f 7420 6120 7375 6263 6c61 7373  s not a subclass
+000094a0: 206f 6620 4f70 5061 7261 6c6c 656c 436f   of OpParallelCo
+000094b0: 6e66 6967 2e0a 0a20 2020 2020 2020 2053  nfig...        S
+000094c0: 7570 706f 7274 6564 2050 6c61 7466 6f72  upported Platfor
+000094d0: 6d73 3a0a 2020 2020 2020 2020 2020 2020  ms:.            
+000094e0: 6060 4173 6365 6e64 6060 2060 6047 5055  ``Ascend`` ``GPU
+000094f0: 6060 0a0a 2020 2020 2020 2020 4578 616d  ``..        Exam
+00009500: 706c 6573 3a0a 2020 2020 2020 2020 2020  ples:.          
+00009510: 2020 3e3e 3e20 696d 706f 7274 206e 756d    >>> import num
+00009520: 7079 2061 7320 6e70 0a20 2020 2020 2020  py as np.       
+00009530: 2020 2020 203e 3e3e 2066 726f 6d20 6d69       >>> from mi
+00009540: 6e64 666f 726d 6572 732e 6d6f 6475 6c65  ndformers.module
+00009550: 732e 7472 616e 7366 6f72 6d65 7220 696d  s.transformer im
+00009560: 706f 7274 2041 7474 656e 7469 6f6e 4d61  port AttentionMa
+00009570: 736b 4846 0a20 2020 2020 2020 2020 2020  skHF.           
+00009580: 203e 3e3e 2066 726f 6d20 6d69 6e64 7370   >>> from mindsp
+00009590: 6f72 6520 696d 706f 7274 2054 656e 736f  ore import Tenso
+000095a0: 720a 2020 2020 2020 2020 2020 2020 3e3e  r.            >>
+000095b0: 3e20 6d61 736b 203d 2041 7474 656e 7469  > mask = Attenti
+000095c0: 6f6e 4d61 736b 4846 2873 6571 5f6c 656e  onMaskHF(seq_len
+000095d0: 6774 683d 3429 0a20 2020 2020 2020 2020  gth=4).         
+000095e0: 2020 203e 3e3e 206d 6173 6b5f 6172 7261     >>> mask_arra
+000095f0: 7920 3d20 6e70 2e61 7272 6179 285b 5b31  y = np.array([[1
+00009600: 2c20 312c 2031 2c20 305d 5d2c 206e 702e  , 1, 1, 0]], np.
+00009610: 666c 6f61 7433 3229 0a20 2020 2020 2020  float32).       
+00009620: 2020 2020 203e 3e3e 2069 6e70 7574 7320       >>> inputs 
+00009630: 3d20 5465 6e73 6f72 286d 6173 6b5f 6172  = Tensor(mask_ar
+00009640: 7261 7929 0a20 2020 2020 2020 2020 2020  ray).           
+00009650: 203e 3e3e 2072 6573 203d 206d 6173 6b28   >>> res = mask(
+00009660: 696e 7075 7473 290a 2020 2020 2020 2020  inputs).        
+00009670: 2020 2020 3e3e 3e20 7072 696e 7428 7265      >>> print(re
+00009680: 7329 0a20 2020 2020 2020 2020 2020 205b  s).            [
+00009690: 5b5b 312e 2030 2e20 302e 2030 5d0a 2020  [[1. 0. 0. 0].  
+000096a0: 2020 2020 2020 2020 2020 2020 5b31 2e20              [1. 
+000096b0: 312e 2030 2e20 305d 0a20 2020 2020 2020  1. 0. 0].       
+000096c0: 2020 2020 2020 205b 312e 2031 2e20 312e         [1. 1. 1.
+000096d0: 2030 5d0a 2020 2020 2020 2020 2020 2020   0].            
+000096e0: 2020 5b31 2e20 312e 2031 2e20 315d 5d5d    [1. 1. 1. 1]]]
+000096f0: 0a20 2020 2022 2222 0a0a 2020 2020 405f  .    """..    @_
+00009700: 4c6f 6741 6374 696f 6e4f 6e63 6528 6d5f  LogActionOnce(m_
+00009710: 6c6f 6767 6572 3d6c 6f67 6765 722c 206b  logger=logger, k
+00009720: 6579 3d27 4174 7465 6e74 696f 6e4d 6173  ey='AttentionMas
+00009730: 6b48 4627 2c0a 2020 2020 2020 2020 2020  kHF',.          
+00009740: 2020 2020 2020 2020 2020 6e6f 5f77 6172            no_war
+00009750: 6e69 6e67 3d5f 6765 745f 7061 7261 6c6c  ning=_get_parall
+00009760: 656c 5f6d 6f64 6528 2920 696e 2028 5061  el_mode() in (Pa
+00009770: 7261 6c6c 656c 4d6f 6465 2e53 5441 4e44  rallelMode.STAND
+00009780: 5f41 4c4f 4e45 2c29 290a 2020 2020 405f  _ALONE,)).    @_
+00009790: 6172 6773 5f74 7970 655f 7661 6c69 6461  args_type_valida
+000097a0: 746f 725f 6368 6563 6b28 7365 715f 6c65  tor_check(seq_le
+000097b0: 6e67 7468 3d56 616c 6964 6174 6f72 2e63  ngth=Validator.c
+000097c0: 6865 636b 5f70 6f73 6974 6976 655f 696e  heck_positive_in
+000097d0: 742c 0a20 2020 2020 2020 2020 2020 2020  t,.             
+000097e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000097f0: 2020 2070 6172 616c 6c65 6c5f 636f 6e66     parallel_conf
+00009800: 6967 3d5f 7661 6c69 645f 7479 7065 5f63  ig=_valid_type_c
+00009810: 6865 636b 7328 5b4f 7050 6172 616c 6c65  hecks([OpParalle
+00009820: 6c43 6f6e 6669 675d 2c20 2241 7474 656e  lConfig], "Atten
+00009830: 7469 6f6e 4d61 736b 4846 2229 290a 2020  tionMaskHF")).  
+00009840: 2020 6465 6620 5f5f 696e 6974 5f5f 2873    def __init__(s
+00009850: 656c 662c 2073 6571 5f6c 656e 6774 682c  elf, seq_length,
+00009860: 2070 6172 616c 6c65 6c5f 636f 6e66 6967   parallel_config
+00009870: 3d64 6566 6175 6c74 5f64 706d 705f 636f  =default_dpmp_co
+00009880: 6e66 6967 2c20 636f 6d70 7574 655f 6474  nfig, compute_dt
+00009890: 7970 653d 6d73 7479 7065 2e66 6c6f 6174  ype=mstype.float
+000098a0: 3136 293a 0a20 2020 2020 2020 2073 7570  16):.        sup
+000098b0: 6572 2841 7474 656e 7469 6f6e 4d61 736b  er(AttentionMask
+000098c0: 4846 2c20 7365 6c66 292e 5f5f 696e 6974  HF, self).__init
+000098d0: 5f5f 2829 0a20 2020 2020 2020 2073 656c  __().        sel
+000098e0: 662e 7365 715f 6c65 6e67 7468 203d 2073  f.seq_length = s
+000098f0: 6571 5f6c 656e 6774 680a 2020 2020 2020  eq_length.      
+00009900: 2020 7365 6c66 2e63 6f6d 7075 7465 5f64    self.compute_d
+00009910: 7479 7065 203d 2063 6f6d 7075 7465 5f64  type = compute_d
+00009920: 7479 7065 0a20 2020 2020 2020 2073 656c  type.        sel
+00009930: 662e 6e6f 745f 6571 7561 6c20 3d20 502e  f.not_equal = P.
+00009940: 4e6f 7445 7175 616c 2829 2e73 6861 7264  NotEqual().shard
+00009950: 2828 2870 6172 616c 6c65 6c5f 636f 6e66  (((parallel_conf
+00009960: 6967 2e64 6174 615f 7061 7261 6c6c 656c  ig.data_parallel
+00009970: 2c20 3129 2c20 2829 2929 0a20 2020 2020  , 1), ())).     
+00009980: 2020 2073 656c 662e 7265 7368 6170 6520     self.reshape 
+00009990: 3d20 502e 5265 7368 6170 6528 290a 2020  = P.Reshape().  
+000099a0: 2020 2020 2020 7365 6c66 2e6d 756c 203d        self.mul =
+000099b0: 2050 2e42 6174 6368 4d61 744d 756c 2829   P.BatchMatMul()
+000099c0: 2e73 6861 7264 280a 2020 2020 2020 2020  .shard(.        
+000099d0: 2020 2020 2828 7061 7261 6c6c 656c 5f63      ((parallel_c
+000099e0: 6f6e 6669 672e 6461 7461 5f70 6172 616c  onfig.data_paral
+000099f0: 6c65 6c2c 2031 2c20 3129 2c20 2870 6172  lel, 1, 1), (par
+00009a00: 616c 6c65 6c5f 636f 6e66 6967 2e64 6174  allel_config.dat
+00009a10: 615f 7061 7261 6c6c 656c 2c20 312c 2031  a_parallel, 1, 1
+00009a20: 2929 290a 2020 2020 2020 2020 7365 6c66  ))).        self
+00009a30: 2e65 7870 616e 645f 6469 6d20 3d20 502e  .expand_dim = P.
+00009a40: 4578 7061 6e64 4469 6d73 2829 2e73 6861  ExpandDims().sha
+00009a50: 7264 2828 2831 2c20 3129 2c29 290a 2020  rd(((1, 1),)).  
+00009a60: 2020 2020 2020 6f6e 6573 203d 206e 702e        ones = np.
+00009a70: 6f6e 6573 2873 6861 7065 3d28 7365 715f  ones(shape=(seq_
+00009a80: 6c65 6e67 7468 2c20 7365 715f 6c65 6e67  length, seq_leng
+00009a90: 7468 2929 0a20 2020 2020 2020 2023 2044  th)).        # D
+00009aa0: 6566 6175 6c74 206c 6f77 6572 2074 7269  efault lower tri
+00009ab0: 616e 676c 6520 6d61 736b 206d 6174 7269  angle mask matri
+00009ac0: 780a 2020 2020 2020 2020 7365 6c66 2e6c  x.        self.l
+00009ad0: 6f77 6572 5f74 7269 616e 676c 655f 6d61  ower_triangle_ma
+00009ae0: 736b 203d 2054 656e 736f 7228 6e70 2e74  sk = Tensor(np.t
+00009af0: 7269 6c28 6f6e 6573 292c 206d 7374 7970  ril(ones), mstyp
+00009b00: 652e 666c 6f61 7433 3229 0a20 2020 2020  e.float32).     
+00009b10: 2020 2073 656c 662e 6d75 6c74 6970 6c79     self.multiply
+00009b20: 203d 2050 2e4d 756c 2829 2e73 6861 7264   = P.Mul().shard
+00009b30: 2828 2870 6172 616c 6c65 6c5f 636f 6e66  (((parallel_conf
+00009b40: 6967 2e64 6174 615f 7061 7261 6c6c 656c  ig.data_parallel
+00009b50: 2c20 312c 2031 292c 2028 312c 2031 2c20  , 1, 1), (1, 1, 
+00009b60: 3129 2929 0a0a 2020 2020 6465 6620 636f  1)))..    def co
+00009b70: 6e73 7472 7563 7428 7365 6c66 2c20 696e  nstruct(self, in
+00009b80: 7075 745f 6d61 736b 293a 0a20 2020 2020  put_mask):.     
+00009b90: 2020 2022 2222 466f 7277 6172 6420 7072     """Forward pr
+00009ba0: 6f63 6573 7320 6f66 2074 6865 2041 7474  ocess of the Att
+00009bb0: 656e 7469 6f6e 4d61 736b 4846 2222 220a  entionMaskHF""".
+00009bc0: 2020 2020 2020 2020 5f63 6865 636b 5f69          _check_i
+00009bd0: 6e70 7574 5f64 7479 7065 2846 2e64 7479  nput_dtype(F.dty
+00009be0: 7065 2869 6e70 7574 5f6d 6173 6b29 2c20  pe(input_mask), 
+00009bf0: 2269 6e70 7574 5f6d 6173 6b22 2c0a 2020  "input_mask",.  
+00009c00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00009c10: 2020 2020 2020 2020 205b 6d73 7479 7065           [mstype
+00009c20: 2e66 6c6f 6174 3332 2c20 6d73 7479 7065  .float32, mstype
+00009c30: 2e66 6c6f 6174 3136 2c20 6d73 7479 7065  .float16, mstype
+00009c40: 2e62 666c 6f61 7431 365d 2c20 7365 6c66  .bfloat16], self
+00009c50: 2e63 6c73 5f6e 616d 6529 0a20 2020 2020  .cls_name).     
+00009c60: 2020 2069 6e70 7574 5f6d 6173 6b20 3d20     input_mask = 
+00009c70: 502e 4361 7374 2829 2850 2e4f 6e65 734c  P.Cast()(P.OnesL
+00009c80: 696b 6528 2928 696e 7075 745f 6d61 736b  ike()(input_mask
+00009c90: 292c 2073 656c 662e 636f 6d70 7574 655f  ), self.compute_
+00009ca0: 6474 7970 6529 0a20 2020 2020 2020 2069  dtype).        i
+00009cb0: 6e70 7574 5f73 6861 7065 203d 2050 2e53  nput_shape = P.S
+00009cc0: 6861 7065 2829 2869 6e70 7574 5f6d 6173  hape()(input_mas
+00009cd0: 6b29 0a20 2020 2020 2020 2073 6861 7065  k).        shape
+00009ce0: 5f72 6967 6874 203d 2028 696e 7075 745f  _right = (input_
+00009cf0: 7368 6170 655b 305d 2c20 312c 2069 6e70  shape[0], 1, inp
+00009d00: 7574 5f73 6861 7065 5b31 5d29 0a20 2020  ut_shape[1]).   
+00009d10: 2020 2020 2073 6861 7065 5f6c 6566 7420       shape_left 
+00009d20: 3d20 696e 7075 745f 7368 6170 6520 2b20  = input_shape + 
+00009d30: 2831 2c29 0a20 2020 2020 2020 2023 204d  (1,).        # M
+00009d40: 6173 6b20 7468 6520 7061 6464 6564 2069  ask the padded i
+00009d50: 6e70 7574 730a 2020 2020 2020 2020 6d61  nputs.        ma
+00009d60: 736b 5f6c 6566 7420 3d20 7365 6c66 2e72  sk_left = self.r
+00009d70: 6573 6861 7065 2869 6e70 7574 5f6d 6173  eshape(input_mas
+00009d80: 6b2c 2073 6861 7065 5f6c 6566 7429 0a20  k, shape_left). 
+00009d90: 2020 2020 2020 206d 6173 6b5f 7269 6768         mask_righ
+00009da0: 7420 3d20 7365 6c66 2e72 6573 6861 7065  t = self.reshape
+00009db0: 2869 6e70 7574 5f6d 6173 6b2c 2073 6861  (input_mask, sha
+00009dc0: 7065 5f72 6967 6874 290a 2020 2020 2020  pe_right).      
+00009dd0: 2020 6174 7465 6e74 696f 6e5f 6d61 736b    attention_mask
+00009de0: 203d 2073 656c 662e 6d75 6c28 6d61 736b   = self.mul(mask
+00009df0: 5f6c 6566 742c 206d 6173 6b5f 7269 6768  _left, mask_righ
+00009e00: 7429 0a20 2020 2020 2020 206c 6f77 6572  t).        lower
+00009e10: 5f74 7261 6961 6e67 6c65 203d 2073 656c  _traiangle = sel
+00009e20: 662e 6578 7061 6e64 5f64 696d 2873 656c  f.expand_dim(sel
+00009e30: 662e 6c6f 7765 725f 7472 6961 6e67 6c65  f.lower_triangle
+00009e40: 5f6d 6173 6b2c 2030 290a 2020 2020 2020  _mask, 0).      
+00009e50: 2020 2320 7468 6520 7265 7475 726e 6564    # the returned
+00009e60: 2073 6861 7065 2069 7320 5b62 732c 2073   shape is [bs, s
+00009e70: 6571 5f6c 656e 6774 682c 2073 6571 5f6c  eq_length, seq_l
+00009e80: 656e 6774 685d 0a20 2020 2020 2020 2061  ength].        a
+00009e90: 7474 656e 7469 6f6e 5f6d 6173 6b20 3d20  ttention_mask = 
+00009ea0: 7365 6c66 2e6d 756c 7469 706c 7928 0a20  self.multiply(. 
+00009eb0: 2020 2020 2020 2020 2020 2061 7474 656e             atten
+00009ec0: 7469 6f6e 5f6d 6173 6b2c 206c 6f77 6572  tion_mask, lower
+00009ed0: 5f74 7261 6961 6e67 6c65 290a 2020 2020  _traiangle).    
+00009ee0: 2020 2020 7265 7475 726e 2061 7474 656e      return atten
+00009ef0: 7469 6f6e 5f6d 6173 6b0a 0a0a 636c 6173  tion_mask...clas
+00009f00: 7320 4c6f 7765 7254 7269 616e 6775 6c61  s LowerTriangula
+00009f10: 724d 6173 6b57 6974 6844 796e 616d 6963  rMaskWithDynamic
+00009f20: 2843 656c 6c29 3a0a 2020 2020 7222 2222  (Cell):.    r"""
+00009f30: 0a20 2020 2020 2020 2020 2020 2047 6574  .            Get
+00009f40: 2074 6865 2053 7472 6963 746c 7920 4c6f   the Strictly Lo
+00009f50: 7765 7220 7472 6961 6e67 756c 6172 206d  wer triangular m
+00009f60: 6174 7269 7820 6672 6f6d 2074 6865 2069  atrix from the i
+00009f70: 6e70 7574 5f69 6473 2e0a 2020 2020 2222  nput_ids..    ""
+00009f80: 220a 2020 2020 405f 4c6f 6741 6374 696f  ".    @_LogActio
+00009f90: 6e4f 6e63 6528 6d5f 6c6f 6767 6572 3d6c  nOnce(m_logger=l
+00009fa0: 6f67 6765 722c 206b 6579 3d27 4174 7465  ogger, key='Atte
+00009fb0: 6e74 696f 6e4d 6173 6b27 2c0a 2020 2020  ntionMask',.    
+00009fc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00009fd0: 6e6f 5f77 6172 6e69 6e67 3d5f 6765 745f  no_warning=_get_
+00009fe0: 7061 7261 6c6c 656c 5f6d 6f64 6528 2920  parallel_mode() 
+00009ff0: 696e 2028 5061 7261 6c6c 656c 4d6f 6465  in (ParallelMode
+0000a000: 2e53 5441 4e44 5f41 4c4f 4e45 2c29 290a  .STAND_ALONE,)).
+0000a010: 2020 2020 6465 6620 5f5f 696e 6974 5f5f      def __init__
+0000a020: 2873 656c 662c 2073 6571 5f6c 656e 6774  (self, seq_lengt
+0000a030: 682c 2063 6f6d 7075 7465 5f74 7970 653d  h, compute_type=
+0000a040: 6d73 7479 7065 2e66 6c6f 6174 3136 2c0a  mstype.float16,.
+0000a050: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000a060: 2069 735f 6479 6e61 6d69 633d 4661 6c73   is_dynamic=Fals
+0000a070: 652c 2070 6164 5f74 6f6b 656e 5f69 643d  e, pad_token_id=
+0000a080: 302c 2075 7365 5f66 6c61 7368 5f61 7474  0, use_flash_att
+0000a090: 656e 7469 6f6e 3d46 616c 7365 2c0a 2020  ention=False,.  
+0000a0a0: 2020 2020 2020 2020 2020 2020 2020 2075                 u
+0000a0b0: 7365 5f70 726f 6d70 745f 666c 6173 685f  se_prompt_flash_
+0000a0c0: 6174 7465 6e74 696f 6e3d 4661 6c73 652c  attention=False,
+0000a0d0: 2075 7365 5f69 6e63 7265 5f66 6c61 7368   use_incre_flash
+0000a0e0: 5f61 7474 656e 7469 6f6e 3d46 616c 7365  _attention=False
+0000a0f0: 293a 0a20 2020 2020 2020 2073 7570 6572  ):.        super
+0000a100: 2829 2e5f 5f69 6e69 745f 5f28 290a 2020  ().__init__().  
+0000a110: 2020 2020 2020 7365 6c66 2e64 7479 7065        self.dtype
+0000a120: 203d 2063 6f6d 7075 7465 5f74 7970 650a   = compute_type.
+0000a130: 2020 2020 2020 2020 7365 6c66 2e69 735f          self.is_
+0000a140: 6479 6e61 6d69 6320 3d20 6973 5f64 796e  dynamic = is_dyn
+0000a150: 616d 6963 0a20 2020 2020 2020 2073 656c  amic.        sel
+0000a160: 662e 7061 645f 746f 6b65 6e5f 6964 203d  f.pad_token_id =
+0000a170: 2070 6164 5f74 6f6b 656e 5f69 640a 2020   pad_token_id.  
+0000a180: 2020 2020 2020 7365 6c66 2e75 7365 5f66        self.use_f
+0000a190: 6c61 7368 5f61 7474 656e 7469 6f6e 203d  lash_attention =
+0000a1a0: 2075 7365 5f66 6c61 7368 5f61 7474 656e   use_flash_atten
+0000a1b0: 7469 6f6e 0a20 2020 2020 2020 2073 656c  tion.        sel
+0000a1c0: 662e 7573 655f 7072 6f6d 7074 5f66 6c61  f.use_prompt_fla
+0000a1d0: 7368 5f61 7474 656e 7469 6f6e 203d 2075  sh_attention = u
+0000a1e0: 7365 5f70 726f 6d70 745f 666c 6173 685f  se_prompt_flash_
+0000a1f0: 6174 7465 6e74 696f 6e0a 2020 2020 2020  attention.      
+0000a200: 2020 7365 6c66 2e75 7365 5f69 6e63 7265    self.use_incre
+0000a210: 5f66 6c61 7368 5f61 7474 656e 7469 6f6e  _flash_attention
+0000a220: 203d 2075 7365 5f69 6e63 7265 5f66 6c61   = use_incre_fla
+0000a230: 7368 5f61 7474 656e 7469 6f6e 0a20 2020  sh_attention.   
+0000a240: 2020 2020 2073 656c 662e 6973 5f66 6972       self.is_fir
+0000a250: 7374 5f69 7465 7261 7469 6f6e 203d 2054  st_iteration = T
+0000a260: 7275 650a 2020 2020 2020 2020 7365 6c66  rue.        self
+0000a270: 2e6d 756c 7469 706c 795f 6461 7461 203d  .multiply_data =
+0000a280: 2054 656e 736f 7228 5b2d 3130 3030 302e   Tensor([-10000.
+0000a290: 305d 2c20 6474 7970 653d 636f 6d70 7574  0], dtype=comput
+0000a2a0: 655f 7479 7065 290a 2020 2020 2020 2020  e_type).        
+0000a2b0: 7365 6c66 2e6f 6e65 203d 2054 656e 736f  self.one = Tenso
+0000a2c0: 7228 5b31 2e30 5d2c 2064 7479 7065 3d63  r([1.0], dtype=c
+0000a2d0: 6f6d 7075 7465 5f74 7970 6529 0a20 2020  ompute_type).   
+0000a2e0: 2020 2020 2073 656c 662e 6c6f 7765 725f       self.lower_
+0000a2f0: 7472 6961 6e67 6c65 5f6d 6173 6b20 3d20  triangle_mask = 
+0000a300: 6f70 732e 6361 7374 2854 656e 736f 7228  ops.cast(Tensor(
+0000a310: 6e70 2e74 7269 6c28 6e70 2e6f 6e65 7328  np.tril(np.ones(
+0000a320: 7368 6170 653d 2873 6571 5f6c 656e 6774  shape=(seq_lengt
+0000a330: 682c 2073 6571 5f6c 656e 6774 6829 2929  h, seq_length)))
+0000a340: 2c20 6d73 7479 7065 2e66 6c6f 6174 3332  , mstype.float32
+0000a350: 292c 0a20 2020 2020 2020 2020 2020 2020  ),.             
+0000a360: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000a370: 2020 2020 2020 2020 2020 2020 2020 2063                 c
+0000a380: 6f6d 7075 7465 5f74 7970 6529 0a0a 2020  ompute_type)..  
+0000a390: 2020 2020 2020 7365 6c66 2e73 6861 7065        self.shape
+0000a3a0: 203d 2050 2e53 6861 7065 2829 0a20 2020   = P.Shape().   
+0000a3b0: 2020 2020 2073 656c 662e 6361 7374 203d       self.cast =
+0000a3c0: 2050 2e43 6173 7428 290a 2020 2020 2020   P.Cast().      
+0000a3d0: 2020 7365 6c66 2e72 6573 6861 7065 203d    self.reshape =
+0000a3e0: 2050 2e52 6573 6861 7065 2829 0a20 2020   P.Reshape().   
+0000a3f0: 2020 2020 2073 656c 662e 6e6f 745f 6571       self.not_eq
+0000a400: 7561 6c20 3d20 502e 4e6f 7445 7175 616c  ual = P.NotEqual
+0000a410: 2829 0a20 2020 2020 2020 2073 656c 662e  ().        self.
+0000a420: 6c65 7373 5f65 7175 616c 203d 2050 2e4c  less_equal = P.L
+0000a430: 6573 7345 7175 616c 2829 0a20 2020 2020  essEqual().     
+0000a440: 2020 2073 656c 662e 626d 6d20 3d20 502e     self.bmm = P.
+0000a450: 4261 7463 684d 6174 4d75 6c28 290a 2020  BatchMatMul().  
+0000a460: 2020 2020 2020 7365 6c66 2e65 7870 616e        self.expan
+0000a470: 645f 6469 6d20 3d20 502e 4578 7061 6e64  d_dim = P.Expand
+0000a480: 4469 6d73 2829 0a20 2020 2020 2020 2073  Dims().        s
+0000a490: 656c 662e 736c 6963 6520 3d20 502e 5374  elf.slice = P.St
+0000a4a0: 7269 6465 6453 6c69 6365 2829 0a20 2020  ridedSlice().   
+0000a4b0: 2020 2020 2073 656c 662e 6d75 6c20 3d20       self.mul = 
+0000a4c0: 502e 4d75 6c28 290a 2020 2020 2020 2020  P.Mul().        
+0000a4d0: 7365 6c66 2e73 7562 203d 2050 2e53 7562  self.sub = P.Sub
+0000a4e0: 2829 0a20 2020 2020 2020 2073 656c 662e  ().        self.
+0000a4f0: 6d75 6c5f 706f 7374 203d 2050 2e4d 756c  mul_post = P.Mul
+0000a500: 2829 0a20 2020 2020 2020 2073 656c 662e  ().        self.
+0000a510: 6578 7061 6e64 5f64 696d 5f70 6f73 7420  expand_dim_post 
+0000a520: 3d20 502e 4578 7061 6e64 4469 6d73 2829  = P.ExpandDims()
+0000a530: 0a0a 2020 2020 6465 6620 636f 6e73 7472  ..    def constr
+0000a540: 7563 7428 7365 6c66 2c20 746f 6b65 6e73  uct(self, tokens
+0000a550: 3d4e 6f6e 652c 206d 6173 6b73 3d4e 6f6e  =None, masks=Non
+0000a560: 6529 3a0a 2020 2020 2020 2020 2222 2246  e):.        """F
+0000a570: 6f72 7761 7264 2070 726f 6365 7373 206f  orward process o
+0000a580: 6620 7468 6520 4361 7573 616c 4d61 736b  f the CausalMask
+0000a590: 2222 220a 2020 2020 2020 2020 6966 2074  """.        if t
+0000a5a0: 6f6b 656e 7320 6973 206e 6f74 204e 6f6e  okens is not Non
+0000a5b0: 653a 0a20 2020 2020 2020 2020 2020 2062  e:.            b
+0000a5c0: 7320 3d20 7365 6c66 2e73 6861 7065 2874  s = self.shape(t
+0000a5d0: 6f6b 656e 7329 5b30 5d0a 2020 2020 2020  okens)[0].      
+0000a5e0: 2020 2020 2020 7365 715f 6c65 6e20 3d20        seq_len = 
+0000a5f0: 7365 6c66 2e73 6861 7065 2874 6f6b 656e  self.shape(token
+0000a600: 7329 5b31 5d0a 2020 2020 2020 2020 2020  s)[1].          
+0000a610: 2020 696e 7075 745f 6d61 736b 203d 2073    input_mask = s
+0000a620: 656c 662e 6361 7374 2873 656c 662e 6e6f  elf.cast(self.no
+0000a630: 745f 6571 7561 6c28 746f 6b65 6e73 2c20  t_equal(tokens, 
+0000a640: 7365 6c66 2e70 6164 5f74 6f6b 656e 5f69  self.pad_token_i
+0000a650: 6429 2c20 7365 6c66 2e64 7479 7065 290a  d), self.dtype).
+0000a660: 2020 2020 2020 2020 656c 7365 3a0a 2020          else:.  
+0000a670: 2020 2020 2020 2020 2020 6273 203d 2073            bs = s
+0000a680: 656c 662e 7368 6170 6528 6d61 736b 7329  elf.shape(masks)
+0000a690: 5b30 5d0a 2020 2020 2020 2020 2020 2020  [0].            
+0000a6a0: 7365 715f 6c65 6e20 3d20 7365 6c66 2e73  seq_len = self.s
+0000a6b0: 6861 7065 286d 6173 6b73 295b 315d 0a20  hape(masks)[1]. 
+0000a6c0: 2020 2020 2020 2020 2020 2069 6e70 7574             input
+0000a6d0: 5f6d 6173 6b20 3d20 7365 6c66 2e63 6173  _mask = self.cas
+0000a6e0: 7428 6d61 736b 732c 2073 656c 662e 6474  t(masks, self.dt
+0000a6f0: 7970 6529 0a20 2020 2020 2020 2073 6861  ype).        sha
+0000a700: 7065 5f72 6967 6874 203d 2028 6273 2c20  pe_right = (bs, 
+0000a710: 312c 2073 6571 5f6c 656e 290a 0a20 2020  1, seq_len)..   
+0000a720: 2020 2020 2023 204d 6173 6b20 7468 6520       # Mask the 
+0000a730: 7061 6464 6564 2069 6e70 7574 730a 2020  padded inputs.  
+0000a740: 2020 2020 2020 6d61 736b 5f72 6967 6874        mask_right
+0000a750: 203d 2073 656c 662e 7265 7368 6170 6528   = self.reshape(
+0000a760: 696e 7075 745f 6d61 736b 2c20 7368 6170  input_mask, shap
+0000a770: 655f 7269 6768 7429 0a20 2020 2020 2020  e_right).       
+0000a780: 2061 7474 656e 7469 6f6e 5f6d 6173 6b20   attention_mask 
+0000a790: 3d20 6d61 736b 5f72 6967 6874 0a20 2020  = mask_right.   
+0000a7a0: 2020 2020 2069 6620 6e6f 7420 7365 6c66       if not self
+0000a7b0: 2e69 735f 6479 6e61 6d69 633a 0a20 2020  .is_dynamic:.   
+0000a7c0: 2020 2020 2020 2020 206c 6f77 6572 5f74           lower_t
+0000a7d0: 7269 616e 676c 6520 3d20 7365 6c66 2e65  riangle = self.e
+0000a7e0: 7870 616e 645f 6469 6d28 7365 6c66 2e6c  xpand_dim(self.l
+0000a7f0: 6f77 6572 5f74 7269 616e 676c 655f 6d61  ower_triangle_ma
+0000a800: 736b 2c20 3029 0a20 2020 2020 2020 2065  sk, 0).        e
+0000a810: 6c73 653a 0a20 2020 2020 2020 2020 2020  lse:.           
+0000a820: 206c 6f77 6572 5f74 7269 616e 676c 655f   lower_triangle_
+0000a830: 6d61 736b 203d 2073 656c 662e 736c 6963  mask = self.slic
+0000a840: 6528 7365 6c66 2e6c 6f77 6572 5f74 7269  e(self.lower_tri
+0000a850: 616e 676c 655f 6d61 736b 2c20 2830 2c20  angle_mask, (0, 
+0000a860: 3029 2c20 2873 6571 5f6c 656e 2c20 7365  0), (seq_len, se
+0000a870: 715f 6c65 6e29 2c20 2831 2c20 3129 290a  q_len), (1, 1)).
+0000a880: 2020 2020 2020 2020 2020 2020 6c6f 7765              lowe
+0000a890: 725f 7472 6961 6e67 6c65 203d 2073 656c  r_triangle = sel
+0000a8a0: 662e 6578 7061 6e64 5f64 696d 286c 6f77  f.expand_dim(low
+0000a8b0: 6572 5f74 7269 616e 676c 655f 6d61 736b  er_triangle_mask
+0000a8c0: 2c20 3029 0a0a 2020 2020 2020 2020 2320  , 0)..        # 
+0000a8d0: 7468 6520 7265 7475 726e 6564 2073 6861  the returned sha
+0000a8e0: 7065 2069 7320 5b62 732c 2031 2c20 7365  pe is [bs, 1, se
+0000a8f0: 715f 6c65 6e67 7468 2c20 7365 715f 6c65  q_length, seq_le
+0000a900: 6e67 7468 5d0a 2020 2020 2020 2020 6174  ngth].        at
+0000a910: 7465 6e74 696f 6e5f 6d61 736b 203d 2073  tention_mask = s
+0000a920: 656c 662e 6d75 6c28 6174 7465 6e74 696f  elf.mul(attentio
+0000a930: 6e5f 6d61 736b 2c20 6c6f 7765 725f 7472  n_mask, lower_tr
+0000a940: 6961 6e67 6c65 290a 2020 2020 2020 2020  iangle).        
+0000a950: 6174 7465 6e74 696f 6e5f 6d61 736b 203d  attention_mask =
+0000a960: 2073 656c 662e 7375 6228 7365 6c66 2e6f   self.sub(self.o
+0000a970: 6e65 2c20 6174 7465 6e74 696f 6e5f 6d61  ne, attention_ma
+0000a980: 736b 290a 2020 2020 2020 2020 6174 7465  sk).        atte
+0000a990: 6e74 696f 6e5f 6d61 736b 203d 2073 656c  ntion_mask = sel
+0000a9a0: 662e 6578 7061 6e64 5f64 696d 5f70 6f73  f.expand_dim_pos
+0000a9b0: 7428 6174 7465 6e74 696f 6e5f 6d61 736b  t(attention_mask
+0000a9c0: 2c20 3129 0a20 2020 2020 2020 2069 6620  , 1).        if 
+0000a9d0: 6e6f 7420 7365 6c66 2e75 7365 5f66 6c61  not self.use_fla
+0000a9e0: 7368 5f61 7474 656e 7469 6f6e 2061 6e64  sh_attention and
+0000a9f0: 206e 6f74 2073 656c 662e 7573 655f 7072   not self.use_pr
+0000aa00: 6f6d 7074 5f66 6c61 7368 5f61 7474 656e  ompt_flash_atten
+0000aa10: 7469 6f6e 3a0a 2020 2020 2020 2020 2020  tion:.          
+0000aa20: 2020 6174 7465 6e74 696f 6e5f 6d61 736b    attention_mask
+0000aa30: 203d 2073 656c 662e 6d75 6c5f 706f 7374   = self.mul_post
+0000aa40: 2861 7474 656e 7469 6f6e 5f6d 6173 6b2c  (attention_mask,
+0000aa50: 2073 656c 662e 6d75 6c74 6970 6c79 5f64   self.multiply_d
+0000aa60: 6174 6129 0a20 2020 2020 2020 2065 6c69  ata).        eli
+0000aa70: 6620 7365 6c66 2e75 7365 5f66 6c61 7368  f self.use_flash
+0000aa80: 5f61 7474 656e 7469 6f6e 206f 7220 7365  _attention or se
+0000aa90: 6c66 2e75 7365 5f70 726f 6d70 745f 666c  lf.use_prompt_fl
+0000aaa0: 6173 685f 6174 7465 6e74 696f 6e3a 0a20  ash_attention:. 
+0000aab0: 2020 2020 2020 2020 2020 2061 7474 656e             atten
+0000aac0: 7469 6f6e 5f6d 6173 6b20 3d20 7365 6c66  tion_mask = self
+0000aad0: 2e63 6173 7428 6174 7465 6e74 696f 6e5f  .cast(attention_
+0000aae0: 6d61 736b 2c20 6d73 7479 7065 2e75 696e  mask, mstype.uin
+0000aaf0: 7438 290a 2020 2020 2020 2020 7265 7475  t8).        retu
+0000ab00: 726e 2061 7474 656e 7469 6f6e 5f6d 6173  rn attention_mas
+0000ab10: 6b0a 0a20 2020 2064 6566 2069 6e63 7265  k..    def incre
+0000ab20: 6d65 6e74 2873 656c 662c 2073 6571 5f72  ment(self, seq_r
+0000ab30: 616e 6765 2c20 6261 7463 685f 7661 6c69  ange, batch_vali
+0000ab40: 645f 6c65 6e67 7468 2c20 7a61 6374 6976  d_length, zactiv
+0000ab50: 6174 655f 6c65 6e3d 4e6f 6e65 293a 0a20  ate_len=None):. 
+0000ab60: 2020 2020 2020 2022 2222 4765 7420 6d61         """Get ma
+0000ab70: 736b 2066 6f72 2069 6e63 7265 6d65 6e74  sk for increment
+0000ab80: 616c 2069 6e66 6572 656e 6365 2e22 2222  al inference."""
+0000ab90: 0a20 2020 2020 2020 2069 6620 7a61 6374  .        if zact
+0000aba0: 6976 6174 655f 6c65 6e20 6973 206e 6f74  ivate_len is not
+0000abb0: 204e 6f6e 653a 0a20 2020 2020 2020 2020   None:.         
+0000abc0: 2020 2073 6571 5f72 616e 6765 203d 2073     seq_range = s
+0000abd0: 656c 662e 736c 6963 6528 7365 715f 7261  elf.slice(seq_ra
+0000abe0: 6e67 652c 2028 302c 2030 2c20 3029 2c20  nge, (0, 0, 0), 
+0000abf0: 2831 2c20 312c 2073 656c 662e 7368 6170  (1, 1, self.shap
+0000ac00: 6528 7a61 6374 6976 6174 655f 6c65 6e29  e(zactivate_len)
+0000ac10: 5b30 5d29 2c20 2831 2c20 312c 2031 2929  [0]), (1, 1, 1))
+0000ac20: 0a20 2020 2020 2020 206d 6173 6b20 3d20  .        mask = 
+0000ac30: 7365 6c66 2e6c 6573 735f 6571 7561 6c28  self.less_equal(
+0000ac40: 7365 6c66 2e72 6573 6861 7065 2873 6571  self.reshape(seq
+0000ac50: 5f72 616e 6765 2c20 2831 2c20 312c 202d  _range, (1, 1, -
+0000ac60: 3129 292c 2073 656c 662e 7265 7368 6170  1)), self.reshap
+0000ac70: 6528 6261 7463 685f 7661 6c69 645f 6c65  e(batch_valid_le
+0000ac80: 6e67 7468 2c20 282d 312c 2031 2c20 3129  ngth, (-1, 1, 1)
+0000ac90: 2929 0a20 2020 2020 2020 206d 6173 6b20  )).        mask 
+0000aca0: 3d20 7365 6c66 2e63 6173 7428 6d61 736b  = self.cast(mask
+0000acb0: 2c20 7365 6c66 2e64 7479 7065 290a 2020  , self.dtype).  
+0000acc0: 2020 2020 2020 6d61 736b 203d 2073 656c        mask = sel
+0000acd0: 662e 7375 6228 7365 6c66 2e6f 6e65 2c20  f.sub(self.one, 
+0000ace0: 6d61 736b 290a 2020 2020 2020 2020 6d61  mask).        ma
+0000acf0: 736b 203d 2073 656c 662e 6578 7061 6e64  sk = self.expand
+0000ad00: 5f64 696d 5f70 6f73 7428 6d61 736b 2c20  _dim_post(mask, 
+0000ad10: 3129 0a20 2020 2020 2020 2069 6620 6e6f  1).        if no
+0000ad20: 7420 7365 6c66 2e75 7365 5f69 6e63 7265  t self.use_incre
+0000ad30: 5f66 6c61 7368 5f61 7474 656e 7469 6f6e  _flash_attention
+0000ad40: 3a0a 2020 2020 2020 2020 2020 2020 6d61  :.            ma
+0000ad50: 736b 203d 2073 656c 662e 6d75 6c5f 706f  sk = self.mul_po
+0000ad60: 7374 286d 6173 6b2c 2073 656c 662e 6d75  st(mask, self.mu
+0000ad70: 6c74 6970 6c79 5f64 6174 6129 0a20 2020  ltiply_data).   
+0000ad80: 2020 2020 2072 6574 7572 6e20 6d61 736b       return mask
+0000ad90: 0a0a 2020 2020 6465 6620 696e 6372 656d  ..    def increm
+0000ada0: 656e 745f 736c 6963 6528 7365 6c66 2c20  ent_slice(self, 
+0000adb0: 7365 715f 7261 6e67 652c 2073 6571 5f6c  seq_range, seq_l
+0000adc0: 656e 6774 682c 2062 6174 6368 5f76 616c  ength, batch_val
+0000add0: 6964 5f6c 656e 6774 682c 207a 6163 7469  id_length, zacti
+0000ade0: 7661 7465 5f6c 656e 3d4e 6f6e 6529 3a0a  vate_len=None):.
+0000adf0: 2020 2020 2020 2020 2222 2247 6574 206d          """Get m
+0000ae00: 6173 6b20 666f 7220 696e 6372 656d 656e  ask for incremen
+0000ae10: 7461 6c20 696e 6665 7265 6e63 6520 616e  tal inference an
+0000ae20: 6420 6170 706c 7920 736c 6963 652e 2222  d apply slice.""
+0000ae30: 220a 2020 2020 2020 2020 6966 207a 6163  ".        if zac
+0000ae40: 7469 7661 7465 5f6c 656e 2069 7320 6e6f  tivate_len is no
+0000ae50: 7420 4e6f 6e65 3a0a 2020 2020 2020 2020  t None:.        
+0000ae60: 2020 2020 7365 715f 7261 6e67 655f 6d61      seq_range_ma
+0000ae70: 736b 203d 2073 656c 662e 736c 6963 6528  sk = self.slice(
+0000ae80: 7365 715f 7261 6e67 652c 2028 302c 2030  seq_range, (0, 0
+0000ae90: 2c20 3029 2c20 2831 2c20 312c 2073 656c  , 0), (1, 1, sel
+0000aea0: 662e 7368 6170 6528 7a61 6374 6976 6174  f.shape(zactivat
+0000aeb0: 655f 6c65 6e29 5b30 5d29 2c20 2831 2c20  e_len)[0]), (1, 
+0000aec0: 312c 2031 2929 0a20 2020 2020 2020 2065  1, 1)).        e
+0000aed0: 6c73 653a 0a20 2020 2020 2020 2020 2020  lse:.           
+0000aee0: 2073 6571 5f72 616e 6765 5f6d 6173 6b20   seq_range_mask 
+0000aef0: 3d20 7365 6c66 2e73 6c69 6365 2873 6571  = self.slice(seq
+0000af00: 5f72 616e 6765 2c20 2830 2c20 302c 2030  _range, (0, 0, 0
+0000af10: 292c 2028 312c 2031 2c20 7365 715f 6c65  ), (1, 1, seq_le
+0000af20: 6e67 7468 292c 2028 312c 2031 2c20 3129  ngth), (1, 1, 1)
+0000af30: 290a 2020 2020 2020 2020 6d61 736b 203d  ).        mask =
+0000af40: 2073 656c 662e 6c65 7373 5f65 7175 616c   self.less_equal
+0000af50: 2873 656c 662e 7265 7368 6170 6528 7365  (self.reshape(se
+0000af60: 715f 7261 6e67 655f 6d61 736b 2c20 2831  q_range_mask, (1
+0000af70: 2c20 312c 202d 3129 292c 2073 656c 662e  , 1, -1)), self.
+0000af80: 7265 7368 6170 6528 6261 7463 685f 7661  reshape(batch_va
+0000af90: 6c69 645f 6c65 6e67 7468 2c20 282d 312c  lid_length, (-1,
+0000afa0: 2031 2c20 3129 2929 0a20 2020 2020 2020   1, 1))).       
+0000afb0: 206d 6173 6b20 3d20 7365 6c66 2e63 6173   mask = self.cas
+0000afc0: 7428 6d61 736b 2c20 7365 6c66 2e64 7479  t(mask, self.dty
+0000afd0: 7065 290a 2020 2020 2020 2020 6d61 736b  pe).        mask
+0000afe0: 203d 2073 656c 662e 7375 6228 7365 6c66   = self.sub(self
+0000aff0: 2e6f 6e65 2c20 6d61 736b 290a 2020 2020  .one, mask).    
+0000b000: 2020 2020 6d61 736b 203d 2073 656c 662e      mask = self.
+0000b010: 6578 7061 6e64 5f64 696d 5f70 6f73 7428  expand_dim_post(
+0000b020: 6d61 736b 2c20 3129 0a20 2020 2020 2020  mask, 1).       
+0000b030: 2069 6620 6e6f 7420 7365 6c66 2e75 7365   if not self.use
+0000b040: 5f69 6e63 7265 5f66 6c61 7368 5f61 7474  _incre_flash_att
+0000b050: 656e 7469 6f6e 3a0a 2020 2020 2020 2020  ention:.        
+0000b060: 2020 2020 6d61 736b 203d 2073 656c 662e      mask = self.
+0000b070: 6d75 6c5f 706f 7374 286d 6173 6b2c 2073  mul_post(mask, s
+0000b080: 656c 662e 6d75 6c74 6970 6c79 5f64 6174  elf.multiply_dat
+0000b090: 6129 0a20 2020 2020 2020 2072 6574 7572  a).        retur
+0000b0a0: 6e20 6d61 736b 0a0a 2020 2020 6465 6620  n mask..    def 
+0000b0b0: 7368 6172 6428 7365 6c66 2c20 7061 7261  shard(self, para
+0000b0c0: 6c6c 656c 5f63 6f6e 6669 6729 3a0a 2020  llel_config):.  
+0000b0d0: 2020 2020 2020 6470 203d 2070 6172 616c        dp = paral
+0000b0e0: 6c65 6c5f 636f 6e66 6967 2e64 6174 615f  lel_config.data_
+0000b0f0: 7061 7261 6c6c 656c 0a20 2020 2020 2020  parallel.       
+0000b100: 2073 656c 662e 6e6f 745f 6571 7561 6c2e   self.not_equal.
+0000b110: 7368 6172 6428 2828 6470 2c20 3129 2c20  shard(((dp, 1), 
+0000b120: 2829 2929 0a20 2020 2020 2020 2073 656c  ())).        sel
+0000b130: 662e 626d 6d2e 7368 6172 6428 2828 6470  f.bmm.shard(((dp
+0000b140: 2c20 312c 2031 292c 2028 6470 2c20 312c  , 1, 1), (dp, 1,
+0000b150: 2031 2929 290a 2020 2020 2020 2020 7365   1))).        se
+0000b160: 6c66 2e65 7870 616e 645f 6469 6d2e 7368  lf.expand_dim.sh
+0000b170: 6172 6428 2828 312c 2031 292c 2929 0a20  ard(((1, 1),)). 
+0000b180: 2020 2020 2020 2073 656c 662e 6d75 6c2e         self.mul.
+0000b190: 7368 6172 6428 2828 6470 2c20 312c 2031  shard(((dp, 1, 1
+0000b1a0: 292c 2028 312c 2031 2c20 3129 2929 0a20  ), (1, 1, 1))). 
+0000b1b0: 2020 2020 2020 2073 656c 662e 6c65 7373         self.less
+0000b1c0: 5f65 7175 616c 2e73 6861 7264 2828 2831  _equal.shard(((1
+0000b1d0: 2c20 312c 2031 292c 2028 312c 2031 2c20  , 1, 1), (1, 1, 
+0000b1e0: 3129 2929 0a20 2020 2020 2020 2073 656c  1))).        sel
+0000b1f0: 662e 7375 622e 7368 6172 6428 2828 312c  f.sub.shard(((1,
+0000b200: 292c 2028 6470 2c20 312c 2031 2929 290a  ), (dp, 1, 1))).
+0000b210: 2020 2020 2020 2020 7365 6c66 2e6d 756c          self.mul
+0000b220: 5f70 6f73 742e 7368 6172 6428 2828 6470  _post.shard(((dp
+0000b230: 2c20 312c 2031 2c20 3129 2c20 2831 2c29  , 1, 1, 1), (1,)
+0000b240: 2929 0a20 2020 2020 2020 2073 656c 662e  )).        self.
+0000b250: 6578 7061 6e64 5f64 696d 5f70 6f73 742e  expand_dim_post.
+0000b260: 7368 6172 6428 2828 6470 2c20 312c 2031  shard(((dp, 1, 1
+0000b270: 292c 2929 0a0a 0a63 6c61 7373 2056 6f63  ),))...class Voc
+0000b280: 6162 456d 6265 6464 696e 6728 4365 6c6c  abEmbedding(Cell
+0000b290: 293a 0a20 2020 2022 2222 0a20 2020 2020  ):.    """.     
+0000b2a0: 2020 2054 6865 2065 6d62 6564 6469 6e67     The embedding
+0000b2b0: 206c 6f6f 6b75 7020 7461 626c 6520 6672   lookup table fr
+0000b2c0: 6f6d 2074 6865 2030 2d74 6820 6469 6d20  om the 0-th dim 
+0000b2d0: 6f66 2074 6865 2070 6172 616d 6574 6572  of the parameter
+0000b2e0: 2074 6162 6c65 2e20 5768 656e 2074 6865   table. When the
+0000b2f0: 2070 6172 616c 6c65 6c5f 636f 6e66 6967   parallel_config
+0000b300: 2e76 6f63 6162 5f65 6d62 5f64 7020 6973  .vocab_emb_dp is
+0000b310: 0a20 2020 2020 2020 2054 7275 6520 616e  .        True an
+0000b320: 6420 696e 2074 6865 2060 4155 544f 5f50  d in the `AUTO_P
+0000b330: 4152 414c 4c45 4c60 206d 6f64 652c 2074  ARALLEL` mode, t
+0000b340: 6865 2065 6d62 6564 6469 6e67 206c 6f6f  he embedding loo
+0000b350: 6b75 7020 7769 6c6c 2062 6520 7472 6169  kup will be trai
+0000b360: 6e65 6420 6279 2074 6865 2064 6174 6120  ned by the data 
+0000b370: 7061 7261 6c6c 656c 2077 6179 2c20 6173  parallel way, as
+0000b380: 2074 6865 0a20 2020 2020 2020 2070 6172   the.        par
+0000b390: 616d 6574 6572 7320 7769 6c6c 2062 6520  ameters will be 
+0000b3a0: 7265 7065 6174 6564 206f 6e20 6561 6368  repeated on each
+0000b3b0: 2064 6576 6963 652e 2049 6620 6661 6c73   device. If fals
+0000b3c0: 652c 2074 6865 2065 6d62 6564 6469 6e67  e, the embedding
+0000b3d0: 2074 6162 6c65 2077 696c 6c20 6265 2073   table will be s
+0000b3e0: 6861 7264 6564 2069 6e74 6f20 6e20 7061  harded into n pa
+0000b3f0: 7274 7320 6174 0a20 2020 2020 2020 2074  rts at.        t
+0000b400: 6865 2030 2d74 6820 6469 6d65 6e73 696f  he 0-th dimensio
+0000b410: 6e20 6f66 2074 6865 2065 6d62 6564 6469  n of the embeddi
+0000b420: 6e67 2074 6162 6c65 2c20 7768 6572 6520  ng table, where 
+0000b430: 7468 6520 6e20 6973 2074 6865 206d 6f64  the n is the mod
+0000b440: 656c 2070 6172 616c 6c65 6c20 7761 7920  el parallel way 
+0000b450: 6465 7465 726d 696e 6564 2062 790a 2020  determined by.  
+0000b460: 2020 2020 2020 6070 6172 616c 6c65 6c5f        `parallel_
+0000b470: 636f 6e66 6967 2e6d 6f64 656c 5f70 6172  config.model_par
+0000b480: 616c 6c65 6c60 2028 456d 6265 6464 696e  allel` (Embeddin
+0000b490: 674f 7050 6172 616c 6c65 6c43 6f6e 6669  gOpParallelConfi
+0000b4a0: 6729 2e0a 0a20 2020 2020 2020 204e 6f74  g)...        Not
+0000b4b0: 653a 0a20 2020 2020 2020 2020 2020 2057  e:.            W
+0000b4c0: 6865 6e20 6041 5554 4f5f 5041 5241 4c4c  hen `AUTO_PARALL
+0000b4d0: 454c 6020 6f72 2060 5345 4d49 5f41 5554  EL` or `SEMI_AUT
+0000b4e0: 4f5f 5041 5241 4c4c 454c 6020 6d6f 6465  O_PARALLEL` mode
+0000b4f0: 2069 7320 656e 6162 6c65 642c 2074 6869   is enabled, thi
+0000b500: 7320 6c61 7965 7220 7375 7070 6f72 7420  s layer support 
+0000b510: 6f6e 6c79 2032 2d64 2064 696d 656e 7369  only 2-d dimensi
+0000b520: 6f6e 2069 6e70 7574 732c 0a20 2020 2020  on inputs,.     
+0000b530: 2020 2020 2020 2061 7320 7468 6520 7368         as the sh
+0000b540: 6172 6420 6973 2064 6573 6967 6e65 6420  ard is designed 
+0000b550: 666f 7220 3264 2069 6e70 7574 732e 0a0a  for 2d inputs...
+0000b560: 2020 2020 2020 2020 4172 6773 3a0a 2020          Args:.  
+0000b570: 2020 2020 2020 2020 2020 766f 6361 625f            vocab_
+0000b580: 7369 7a65 2028 696e 7429 3a20 5369 7a65  size (int): Size
+0000b590: 206f 6620 7468 6520 6469 6374 696f 6e61   of the dictiona
+0000b5a0: 7279 206f 6620 656d 6265 6464 696e 6773  ry of embeddings
+0000b5b0: 2e0a 2020 2020 2020 2020 2020 2020 656d  ..            em
+0000b5c0: 6265 6464 696e 675f 7369 7a65 2028 696e  bedding_size (in
+0000b5d0: 7429 3a20 5468 6520 7369 7a65 206f 6620  t): The size of 
+0000b5e0: 6561 6368 2065 6d62 6564 6469 6e67 2076  each embedding v
+0000b5f0: 6563 746f 722e 0a20 2020 2020 2020 2020  ector..         
+0000b600: 2020 2070 6172 616c 6c65 6c5f 636f 6e66     parallel_conf
+0000b610: 6967 2028 456d 6265 6464 696e 674f 7050  ig (EmbeddingOpP
+0000b620: 6172 616c 6c65 6c43 6f6e 6669 6729 3a20  arallelConfig): 
+0000b630: 5468 6520 7061 7261 6c6c 656c 2063 6f6e  The parallel con
+0000b640: 6669 6720 6f66 206e 6574 776f 726b 2e20  fig of network. 
+0000b650: 4465 6661 756c 740a 2020 2020 2020 2020  Default.        
+0000b660: 2020 2020 2020 2020 6064 6566 6175 6c74          `default
+0000b670: 5f65 6d62 6564 6469 6e67 5f70 6172 616c  _embedding_paral
+0000b680: 6c65 6c5f 636f 6e66 6967 602c 2061 6e20  lel_config`, an 
+0000b690: 696e 7374 616e 6365 206f 6620 6045 6d62  instance of `Emb
+0000b6a0: 6564 6469 6e67 4f70 5061 7261 6c6c 656c  eddingOpParallel
+0000b6b0: 436f 6e66 6967 6020 7769 7468 2064 6566  Config` with def
+0000b6c0: 6175 6c74 2061 7267 732e 0a20 2020 2020  ault args..     
+0000b6d0: 2020 2020 2020 2070 6172 616d 5f69 6e69         param_ini
+0000b6e0: 7420 2855 6e69 6f6e 5b54 656e 736f 722c  t (Union[Tensor,
+0000b6f0: 2073 7472 2c20 496e 6974 6961 6c69 7a65   str, Initialize
+0000b700: 722c 206e 756d 6265 7273 2e4e 756d 6265  r, numbers.Numbe
+0000b710: 725d 293a 2049 6e69 7469 616c 697a 6572  r]): Initializer
+0000b720: 2066 6f72 2074 6865 2065 6d62 6564 6469   for the embeddi
+0000b730: 6e67 5f74 6162 6c65 2e0a 2020 2020 2020  ng_table..      
+0000b740: 2020 2020 2020 2020 2020 5265 6665 7220            Refer 
+0000b750: 746f 2063 6c61 7373 2060 696e 6974 6961  to class `initia
+0000b760: 6c69 7a65 7260 2066 6f72 2074 6865 2076  lizer` for the v
+0000b770: 616c 7565 7320 6f66 2073 7472 696e 6720  alues of string 
+0000b780: 7768 656e 2061 2073 7472 696e 670a 2020  when a string.  
+0000b790: 2020 2020 2020 2020 2020 2020 2020 6973                is
+0000b7a0: 2073 7065 6369 6669 6564 2e20 4465 6661   specified. Defa
+0000b7b0: 756c 743a 2027 6e6f 726d 616c 272e 0a0a  ult: 'normal'...
+0000b7c0: 2020 2020 2020 2020 496e 7075 7473 3a0a          Inputs:.
+0000b7d0: 2020 2020 2020 2020 2020 2020 2d20 2a2a              - **
+0000b7e0: 696e 7075 745f 6964 732a 2a20 2854 656e  input_ids** (Ten
+0000b7f0: 736f 7229 202d 2054 6865 2074 6f6b 656e  sor) - The token
+0000b800: 697a 6564 2069 6e70 7574 7320 7769 7468  ized inputs with
+0000b810: 2064 6174 6174 7970 6520 696e 7433 3220   datatype int32 
+0000b820: 7769 7468 2073 6861 7065 2028 6261 7463  with shape (batc
+0000b830: 685f 7369 7a65 2c20 7365 715f 6c65 6e67  h_size, seq_leng
+0000b840: 7468 290a 0a20 2020 2020 2020 204f 7574  th)..        Out
+0000b850: 7075 7473 3a0a 2020 2020 2020 2020 2020  puts:.          
+0000b860: 2020 5475 706c 652c 2061 2074 7570 6c65    Tuple, a tuple
+0000b870: 2063 6f6e 7461 696e 7320 2860 6f75 7470   contains (`outp
+0000b880: 7574 602c 2060 656d 6265 6464 696e 675f  ut`, `embedding_
+0000b890: 7461 626c 6560 290a 0a20 2020 2020 2020  table`)..       
+0000b8a0: 2020 2020 202d 202a 2a6f 7574 7075 742a       - **output*
+0000b8b0: 2a20 2854 656e 736f 7229 202d 2054 6865  * (Tensor) - The
+0000b8c0: 2065 6d62 6564 6469 6e67 2076 6563 746f   embedding vecto
+0000b8d0: 7220 666f 7220 7468 6520 696e 7075 7420  r for the input 
+0000b8e0: 7769 7468 2073 6861 7065 2028 6261 7463  with shape (batc
+0000b8f0: 685f 7369 7a65 2c0a 2020 2020 2020 2020  h_size,.        
+0000b900: 2020 2020 2020 7365 715f 6c65 6e67 7468        seq_length
+0000b910: 2c20 656d 6265 6464 696e 675f 7369 7a65  , embedding_size
+0000b920: 292e 0a20 2020 2020 2020 2020 2020 202d  )..            -
+0000b930: 202a 2a65 6d62 6564 6469 6e67 5f74 6162   **embedding_tab
+0000b940: 6c65 2a2a 2028 5465 6e73 6f72 2920 2d20  le** (Tensor) - 
+0000b950: 5468 6520 656d 6265 6464 696e 6720 7461  The embedding ta
+0000b960: 626c 6520 7769 7468 2073 6861 7065 2028  ble with shape (
+0000b970: 766f 6361 625f 7369 7a65 2c20 656d 6265  vocab_size, embe
+0000b980: 6464 696e 675f 7369 7a65 292e 0a0a 2020  dding_size)...  
+0000b990: 2020 2020 2020 5261 6973 6573 3a0a 2020        Raises:.  
+0000b9a0: 2020 2020 2020 2020 2020 5661 6c75 6545            ValueE
+0000b9b0: 7272 6f72 3a20 4966 2074 6865 2070 6172  rror: If the par
+0000b9c0: 616c 6c65 6c5f 636f 6e66 6967 2e76 6f63  allel_config.voc
+0000b9d0: 6162 5f65 6d62 5f64 7020 6973 2054 7275  ab_emb_dp is Tru
+0000b9e0: 652c 2074 6865 2076 6f63 6162 2073 697a  e, the vocab siz
+0000b9f0: 6520 6973 206e 6f74 2061 206d 756c 7469  e is not a multi
+0000ba00: 706c 6520 6f66 0a20 2020 2020 2020 2020  ple of.         
+0000ba10: 2020 2020 2020 2070 6172 616c 6c65 6c5f         parallel_
+0000ba20: 636f 6e66 6967 2e6d 6f64 656c 5f70 6172  config.model_par
+0000ba30: 616c 6c65 6c0a 2020 2020 2020 2020 2020  allel.          
+0000ba40: 2020 5661 6c75 6545 7272 6f72 3a20 6076    ValueError: `v
+0000ba50: 6f63 6162 5f73 697a 6560 2069 7320 6e6f  ocab_size` is no
+0000ba60: 7420 6120 706f 7369 7469 7665 2076 616c  t a positive val
+0000ba70: 7565 2e0a 2020 2020 2020 2020 2020 2020  ue..            
+0000ba80: 5661 6c75 6545 7272 6f72 3a20 6065 6d62  ValueError: `emb
+0000ba90: 6564 6469 6e67 5f73 697a 6560 2069 7320  edding_size` is 
+0000baa0: 6e6f 7420 6120 706f 7369 7469 7665 2076  not a positive v
+0000bab0: 616c 7565 2e0a 2020 2020 2020 2020 2020  alue..          
+0000bac0: 2020 5479 7065 4572 726f 723a 2060 7061    TypeError: `pa
+0000bad0: 7261 6c6c 656c 5f63 6f6e 6669 6760 2069  rallel_config` i
+0000bae0: 7320 6e6f 7420 6120 7375 6263 6c61 7373  s not a subclass
+0000baf0: 206f 6620 4f70 5061 7261 6c6c 656c 436f   of OpParallelCo
+0000bb00: 6e66 6967 2e0a 0a20 2020 2020 2020 2053  nfig...        S
+0000bb10: 7570 706f 7274 6564 2050 6c61 7466 6f72  upported Platfor
+0000bb20: 6d73 3a0a 2020 2020 2020 2020 2020 2020  ms:.            
+0000bb30: 6060 4173 6365 6e64 6060 2060 6047 5055  ``Ascend`` ``GPU
+0000bb40: 6060 0a0a 2020 2020 2020 2020 4578 616d  ``..        Exam
+0000bb50: 706c 6573 3a0a 2020 2020 2020 2020 2020  ples:.          
+0000bb60: 2020 3e3e 3e20 696d 706f 7274 206e 756d    >>> import num
+0000bb70: 7079 2061 7320 6e70 0a20 2020 2020 2020  py as np.       
+0000bb80: 2020 2020 203e 3e3e 2066 726f 6d20 6d69       >>> from mi
+0000bb90: 6e64 666f 726d 6572 732e 6d6f 6475 6c65  ndformers.module
+0000bba0: 732e 7472 616e 7366 6f72 6d65 7220 696d  s.transformer im
+0000bbb0: 706f 7274 2056 6f63 6162 456d 6265 6464  port VocabEmbedd
+0000bbc0: 696e 670a 2020 2020 2020 2020 2020 2020  ing.            
+0000bbd0: 3e3e 3e20 6672 6f6d 206d 696e 6473 706f  >>> from mindspo
+0000bbe0: 7265 2069 6d70 6f72 7420 5465 6e73 6f72  re import Tensor
+0000bbf0: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
+0000bc00: 2066 726f 6d20 6d69 6e64 7370 6f72 6520   from mindspore 
+0000bc10: 696d 706f 7274 2064 7479 7065 2061 7320  import dtype as 
+0000bc20: 6d73 7479 7065 0a20 2020 2020 2020 2020  mstype.         
+0000bc30: 2020 203e 3e3e 206d 6f64 656c 203d 2056     >>> model = V
+0000bc40: 6f63 6162 456d 6265 6464 696e 6728 766f  ocabEmbedding(vo
+0000bc50: 6361 625f 7369 7a65 3d33 302c 2065 6d62  cab_size=30, emb
+0000bc60: 6564 6469 6e67 5f73 697a 653d 3330 290a  edding_size=30).
+0000bc70: 2020 2020 2020 2020 2020 2020 3e3e 3e20              >>> 
+0000bc80: 7465 6e73 6f72 203d 2054 656e 736f 7228  tensor = Tensor(
+0000bc90: 6e70 2e6f 6e65 7328 2832 302c 2031 3529  np.ones((20, 15)
+0000bca0: 292c 206d 7374 7970 652e 696e 7433 3229  ), mstype.int32)
+0000bcb0: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
+0000bcc0: 206f 7574 7075 742c 2074 6162 6c65 203d   output, table =
+0000bcd0: 206d 6f64 656c 2874 656e 736f 7229 0a20   model(tensor). 
+0000bce0: 2020 2020 2020 2020 2020 203e 3e3e 2070             >>> p
+0000bcf0: 7269 6e74 286f 7574 7075 742e 7368 6170  rint(output.shap
+0000bd00: 6529 0a20 2020 2020 2020 2020 2020 2028  e).            (
+0000bd10: 3230 2c20 3135 2c20 3330 290a 2020 2020  20, 15, 30).    
+0000bd20: 2020 2020 2020 2020 3e3e 3e20 7072 696e          >>> prin
+0000bd30: 7428 7461 626c 652e 7368 6170 6529 0a20  t(table.shape). 
+0000bd40: 2020 2020 2020 2020 2020 2028 3330 2c20             (30, 
+0000bd50: 3330 290a 2020 2020 2222 220a 0a20 2020  30).    """..   
+0000bd60: 2040 5f4c 6f67 4163 7469 6f6e 4f6e 6365   @_LogActionOnce
+0000bd70: 286d 5f6c 6f67 6765 723d 6c6f 6767 6572  (m_logger=logger
+0000bd80: 2c20 6b65 793d 2756 6f63 6162 456d 6265  , key='VocabEmbe
+0000bd90: 6464 696e 6727 2c0a 2020 2020 2020 2020  dding',.        
+0000bda0: 2020 2020 2020 2020 2020 2020 6e6f 5f77              no_w
+0000bdb0: 6172 6e69 6e67 3d5f 6765 745f 7061 7261  arning=_get_para
+0000bdc0: 6c6c 656c 5f6d 6f64 6528 2920 696e 2028  llel_mode() in (
+0000bdd0: 5061 7261 6c6c 656c 4d6f 6465 2e53 5441  ParallelMode.STA
+0000bde0: 4e44 5f41 4c4f 4e45 2c29 290a 2020 2020  ND_ALONE,)).    
+0000bdf0: 405f 6172 6773 5f74 7970 655f 7661 6c69  @_args_type_vali
+0000be00: 6461 746f 725f 6368 6563 6b28 766f 6361  dator_check(voca
+0000be10: 625f 7369 7a65 3d56 616c 6964 6174 6f72  b_size=Validator
+0000be20: 2e63 6865 636b 5f70 6f73 6974 6976 655f  .check_positive_
+0000be30: 696e 742c 0a20 2020 2020 2020 2020 2020  int,.           
+0000be40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000be50: 2020 2020 2065 6d62 6564 6469 6e67 5f73       embedding_s
+0000be60: 697a 653d 5661 6c69 6461 746f 722e 6368  ize=Validator.ch
+0000be70: 6563 6b5f 706f 7369 7469 7665 5f69 6e74  eck_positive_int
+0000be80: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+0000be90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000bea0: 2020 7061 7261 6c6c 656c 5f63 6f6e 6669    parallel_confi
+0000beb0: 673d 5f76 616c 6964 5f74 7970 655f 6368  g=_valid_type_ch
+0000bec0: 6563 6b73 285b 456d 6265 6464 696e 674f  ecks([EmbeddingO
+0000bed0: 7050 6172 616c 6c65 6c43 6f6e 6669 675d  pParallelConfig]
+0000bee0: 2c20 2256 6f63 6162 456d 6265 6464 696e  , "VocabEmbeddin
+0000bef0: 6722 2929 0a20 2020 2064 6566 205f 5f69  g")).    def __i
+0000bf00: 6e69 745f 5f28 7365 6c66 2c20 766f 6361  nit__(self, voca
+0000bf10: 625f 7369 7a65 2c20 656d 6265 6464 696e  b_size, embeddin
+0000bf20: 675f 7369 7a65 2c20 7061 7261 6c6c 656c  g_size, parallel
+0000bf30: 5f63 6f6e 6669 673d 6465 6661 756c 745f  _config=default_
+0000bf40: 656d 6265 6464 696e 675f 7061 7261 6c6c  embedding_parall
+0000bf50: 656c 5f63 6f6e 6669 672c 0a20 2020 2020  el_config,.     
+0000bf60: 2020 2020 2020 2020 2020 2020 7061 7261              para
+0000bf70: 6d5f 696e 6974 3d27 6e6f 726d 616c 2729  m_init='normal')
+0000bf80: 3a0a 2020 2020 2020 2020 7375 7065 7228  :.        super(
+0000bf90: 566f 6361 6245 6d62 6564 6469 6e67 2c20  VocabEmbedding, 
+0000bfa0: 7365 6c66 292e 5f5f 696e 6974 5f5f 2829  self).__init__()
+0000bfb0: 0a20 2020 2020 2020 205f 6368 6563 6b5f  .        _check_
+0000bfc0: 636f 6e66 6967 2870 6172 616c 6c65 6c5f  config(parallel_
+0000bfd0: 636f 6e66 6967 290a 2020 2020 2020 2020  config).        
+0000bfe0: 7365 6c66 2e76 6f63 6162 5f73 697a 6520  self.vocab_size 
+0000bff0: 3d20 766f 6361 625f 7369 7a65 0a20 2020  = vocab_size.   
+0000c000: 2020 2020 2073 656c 662e 656d 6265 6464       self.embedd
+0000c010: 696e 675f 7369 7a65 203d 2065 6d62 6564  ing_size = embed
+0000c020: 6469 6e67 5f73 697a 650a 2020 2020 2020  ding_size.      
+0000c030: 2020 7365 6c66 2e65 6d62 6564 6469 6e67    self.embedding
+0000c040: 5f74 6162 6c65 203d 2050 6172 616d 6574  _table = Paramet
+0000c050: 6572 2869 6e69 7469 616c 697a 6572 2870  er(initializer(p
+0000c060: 6172 616d 5f69 6e69 742c 205b 7365 6c66  aram_init, [self
+0000c070: 2e76 6f63 6162 5f73 697a 652c 2073 656c  .vocab_size, sel
+0000c080: 662e 656d 6265 6464 696e 675f 7369 7a65  f.embedding_size
+0000c090: 5d29 2c0a 2020 2020 2020 2020 2020 2020  ]),.            
+0000c0a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000c0b0: 2020 2020 2020 2020 2020 2020 206e 616d               nam
+0000c0c0: 653d 2765 6d62 6564 6469 6e67 5f74 6162  e='embedding_tab
+0000c0d0: 6c65 272c 2070 6172 616c 6c65 6c5f 6f70  le', parallel_op
+0000c0e0: 7469 6d69 7a65 723d 4661 6c73 6529 0a0a  timizer=False)..
+0000c0f0: 2020 2020 2020 2020 6966 2070 6172 616c          if paral
+0000c100: 6c65 6c5f 636f 6e66 6967 2e76 6f63 6162  lel_config.vocab
+0000c110: 5f65 6d62 5f64 703a 0a20 2020 2020 2020  _emb_dp:.       
+0000c120: 2020 2020 2073 656c 662e 6761 7468 6572       self.gather
+0000c130: 203d 2050 2e47 6174 6865 7228 292e 7368   = P.Gather().sh
+0000c140: 6172 6428 2828 312c 2031 292c 2028 7061  ard(((1, 1), (pa
+0000c150: 7261 6c6c 656c 5f63 6f6e 6669 672e 6461  rallel_config.da
+0000c160: 7461 5f70 6172 616c 6c65 6c2c 2031 2929  ta_parallel, 1))
+0000c170: 290a 2020 2020 2020 2020 2020 2020 6c6f  ).            lo
+0000c180: 6767 6572 2e69 6e66 6f28 6622 5573 696e  gger.info(f"Usin
+0000c190: 6720 7b70 6172 616c 6c65 6c5f 636f 6e66  g {parallel_conf
+0000c1a0: 6967 2e64 6174 615f 7061 7261 6c6c 656c  ig.data_parallel
+0000c1b0: 7d20 6461 7461 2070 6172 616c 6c65 6c20  } data parallel 
+0000c1c0: 666f 7220 7468 6520 656d 6265 6464 696e  for the embeddin
+0000c1d0: 6720 6c6f 6f6b 7570 2e22 290a 2020 2020  g lookup.").    
+0000c1e0: 2020 2020 656c 7365 3a0a 2020 2020 2020      else:.      
+0000c1f0: 2020 2020 2020 6966 2073 656c 662e 766f        if self.vo
+0000c200: 6361 625f 7369 7a65 2025 2070 6172 616c  cab_size % paral
+0000c210: 6c65 6c5f 636f 6e66 6967 2e6d 6f64 656c  lel_config.model
+0000c220: 5f70 6172 616c 6c65 6c20 213d 2030 3a0a  _parallel != 0:.
+0000c230: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000c240: 7261 6973 6520 5661 6c75 6545 7272 6f72  raise ValueError
+0000c250: 2866 2254 6865 2076 6f63 6162 2073 697a  (f"The vocab siz
+0000c260: 6520 6f66 2074 6865 2065 6d62 6564 6469  e of the embeddi
+0000c270: 6e67 207b 7365 6c66 2e76 6f63 6162 5f73  ng {self.vocab_s
+0000c280: 697a 657d 206d 7573 7420 6265 2061 2022  ize} must be a "
+0000c290: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0000c2a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000c2b0: 2020 6622 6d75 6c74 6970 6c65 206f 6620    f"multiple of 
+0000c2c0: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
+0000c2d0: 6d6f 6465 6c5f 7061 7261 6c6c 656c 207b  model_parallel {
+0000c2e0: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
+0000c2f0: 6d6f 6465 6c5f 7061 7261 6c6c 656c 7d2e  model_parallel}.
+0000c300: 2229 0a20 2020 2020 2020 2020 2020 2073  ").            s
+0000c310: 656c 662e 6761 7468 6572 203d 2050 2e47  elf.gather = P.G
+0000c320: 6174 6865 7228 292e 7368 6172 6428 2828  ather().shard(((
+0000c330: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
+0000c340: 6d6f 6465 6c5f 7061 7261 6c6c 656c 2c20  model_parallel, 
+0000c350: 3129 2c20 2870 6172 616c 6c65 6c5f 636f  1), (parallel_co
+0000c360: 6e66 6967 2e64 6174 615f 7061 7261 6c6c  nfig.data_parall
+0000c370: 656c 2c20 3129 2929 0a20 2020 2020 2020  el, 1))).       
+0000c380: 2020 2020 206c 6f67 6765 722e 696e 666f       logger.info
+0000c390: 2866 2255 7369 6e67 207b 7061 7261 6c6c  (f"Using {parall
+0000c3a0: 656c 5f63 6f6e 6669 672e 6461 7461 5f70  el_config.data_p
+0000c3b0: 6172 616c 6c65 6c7d 2064 6174 6120 7061  arallel} data pa
+0000c3c0: 7261 6c6c 656c 2061 6e64 207b 7061 7261  rallel and {para
+0000c3d0: 6c6c 656c 5f63 6f6e 6669 672e 6d6f 6465  llel_config.mode
+0000c3e0: 6c5f 7061 7261 6c6c 656c 7d20 220a 2020  l_parallel} ".  
+0000c3f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000c400: 2020 2020 2020 6622 6d6f 6465 6c20 7061        f"model pa
+0000c410: 7261 6c6c 656c 2066 6f72 2074 6865 2065  rallel for the e
+0000c420: 6d62 6564 6469 6e67 206c 6f6f 6b75 702e  mbedding lookup.
+0000c430: 2229 0a0a 2020 2020 6465 6620 636f 6e73  ")..    def cons
+0000c440: 7472 7563 7428 7365 6c66 2c20 696e 7075  truct(self, inpu
+0000c450: 745f 6964 7329 3a0a 2020 2020 2020 2020  t_ids):.        
+0000c460: 5f63 6865 636b 5f69 6e70 7574 5f64 7479  _check_input_dty
+0000c470: 7065 2846 2e64 7479 7065 2869 6e70 7574  pe(F.dtype(input
+0000c480: 5f69 6473 292c 2022 696e 7075 745f 6964  _ids), "input_id
+0000c490: 7322 2c20 5b6d 7374 7970 652e 696e 7433  s", [mstype.int3
+0000c4a0: 325d 2c20 7365 6c66 2e63 6c73 5f6e 616d  2], self.cls_nam
+0000c4b0: 6529 0a20 2020 2020 2020 206f 7574 7075  e).        outpu
+0000c4c0: 7420 3d20 7365 6c66 2e67 6174 6865 7228  t = self.gather(
+0000c4d0: 7365 6c66 2e65 6d62 6564 6469 6e67 5f74  self.embedding_t
+0000c4e0: 6162 6c65 2c20 696e 7075 745f 6964 732c  able, input_ids,
+0000c4f0: 2030 290a 2020 2020 2020 2020 7265 7475   0).        retu
+0000c500: 726e 206f 7574 7075 742c 2073 656c 662e  rn output, self.
+0000c510: 656d 6265 6464 696e 675f 7461 626c 652e  embedding_table.
+0000c520: 7661 6c75 6528 290a 0a0a 636c 6173 7320  value()...class 
+0000c530: 4d75 6c74 6948 6561 6441 7474 656e 7469  MultiHeadAttenti
+0000c540: 6f6e 2843 656c 6c29 3a0a 2020 2020 7222  on(Cell):.    r"
+0000c550: 2222 0a20 2020 2020 2020 2054 6869 7320  "".        This 
+0000c560: 6973 2061 6e20 696d 706c 656d 656e 7461  is an implementa
+0000c570: 7469 6f6e 206f 6620 6d75 6c74 6968 6561  tion of multihea
+0000c580: 6420 6174 7465 6e74 696f 6e20 696e 2074  d attention in t
+0000c590: 6865 2070 6170 6572 2060 4174 7465 6e74  he paper `Attent
+0000c5a0: 696f 6e20 6973 2061 6c6c 2079 6f75 206e  ion is all you n
+0000c5b0: 6565 640a 2020 2020 2020 2020 3c68 7474  eed.        <htt
+0000c5c0: 7073 3a2f 2f61 7278 6976 2e6f 7267 2f70  ps://arxiv.org/p
+0000c5d0: 6466 2f31 3730 362e 3033 3736 3276 352e  df/1706.03762v5.
+0000c5e0: 7064 663e 605f 2e20 4769 7665 6e20 7468  pdf>`_. Given th
+0000c5f0: 6520 7175 6572 7920 7665 6374 6f72 2077  e query vector w
+0000c600: 6974 6820 736f 7572 6365 206c 656e 6774  ith source lengt
+0000c610: 682c 2061 6e64 2074 6865 0a20 2020 2020  h, and the.     
+0000c620: 2020 206b 6579 2061 6e64 2076 616c 7565     key and value
+0000c630: 2076 6563 746f 7220 7769 7468 2074 6172   vector with tar
+0000c640: 6765 7420 6c65 6e67 7468 2c20 7468 6520  get length, the 
+0000c650: 6174 7465 6e74 696f 6e20 7769 6c6c 2062  attention will b
+0000c660: 6520 7065 7266 6f72 6d65 6420 6173 2074  e performed as t
+0000c670: 6865 2066 6f6c 6c6f 7769 6e67 0a0a 2020  he following..  
+0000c680: 2020 2020 2020 2e2e 206d 6174 683a 3a0a        .. math::.
+0000c690: 2020 2020 2020 2020 2020 2020 2020 204d                 M
+0000c6a0: 756c 7469 4865 6164 4174 7465 6e74 696f  ultiHeadAttentio
+0000c6b0: 6e28 7175 6572 792c 206b 6579 2c20 7665  n(query, key, ve
+0000c6c0: 6374 6f72 2920 3d20 436f 6e63 6174 2868  ctor) = Concat(h
+0000c6d0: 6561 645f 312c 205c 646f 7473 2c20 6865  ead_1, \dots, he
+0000c6e0: 6164 5f68 2957 5e4f 0a0a 2020 2020 2020  ad_h)W^O..      
+0000c6f0: 2020 7768 6572 6520 3a6d 6174 683a 6068    where :math:`h
+0000c700: 6561 645f 6920 3d20 4174 7465 6e74 696f  ead_i = Attentio
+0000c710: 6e28 5157 5f69 5e51 2c20 4b57 5f69 5e4b  n(QW_i^Q, KW_i^K
+0000c720: 2c20 5657 5f69 5e56 2960 2e20 5468 6520  , VW_i^V)`. The 
+0000c730: 6465 6661 756c 7420 6973 2077 6974 6820  default is with 
+0000c740: 6120 6269 6173 2e0a 0a20 2020 2020 2020  a bias...       
+0000c750: 2069 6620 7175 6572 792c 206b 6579 2061   if query, key a
+0000c760: 6e64 2076 616c 7565 2074 656e 736f 7220  nd value tensor 
+0000c770: 6973 2073 616d 652c 2074 6865 6e20 6974  is same, then it
+0000c780: 2077 696c 6c20 6265 2073 656c 6620 6174   will be self at
+0000c790: 7465 6e74 696f 6e2e 0a0a 2020 2020 2020  tention...      
+0000c7a0: 2020 4172 6773 3a0a 2020 2020 2020 2020    Args:.        
+0000c7b0: 2020 2020 6261 7463 685f 7369 7a65 2869      batch_size(i
+0000c7c0: 6e74 293a 2054 6865 2062 6174 6368 2073  nt): The batch s
+0000c7d0: 697a 6520 6f66 2074 6865 2069 6e70 7574  ize of the input
+0000c7e0: 2074 656e 736f 7220 7768 656e 2064 6f20   tensor when do 
+0000c7f0: 696e 6372 656e 6d65 6e74 616c 2070 7265  increnmental pre
+0000c800: 6469 6374 696f 6e2e 2053 686f 756c 6420  diction. Should 
+0000c810: 6265 2061 2070 6f73 6974 6976 650a 2020  be a positive.  
+0000c820: 2020 2020 2020 2020 2020 2020 2020 7661                va
+0000c830: 6c75 652e 2057 6865 6e20 646f 2074 7261  lue. When do tra
+0000c840: 696e 696e 6720 6f72 2070 7265 6469 6374  ining or predict
+0000c850: 696f 6e2c 2074 6865 2061 7267 756d 656e  ion, the argumen
+0000c860: 7420 7769 6c6c 206e 6f74 2077 6f72 6b20  t will not work 
+0000c870: 616e 6420 7468 6520 7573 6572 2063 616e  and the user can
+0000c880: 206a 7573 7420 7061 7373 204e 6f6e 6520   just pass None 
+0000c890: 746f 0a20 2020 2020 2020 2020 2020 2020  to.             
+0000c8a0: 2020 2074 6865 2061 7267 756d 656e 742e     the argument.
+0000c8b0: 0a20 2020 2020 2020 2020 2020 2073 7263  .            src
+0000c8c0: 5f73 6571 5f6c 656e 6774 6828 696e 7429  _seq_length(int)
+0000c8d0: 3a20 5468 6520 7365 7175 656e 6365 206c  : The sequence l
+0000c8e0: 656e 6774 6820 6f66 2074 6865 2071 7565  ength of the que
+0000c8f0: 7279 2076 6563 746f 722e 0a20 2020 2020  ry vector..     
+0000c900: 2020 2020 2020 2074 6774 5f73 6571 5f6c         tgt_seq_l
+0000c910: 656e 6774 6828 696e 7429 3a20 5468 6520  ength(int): The 
+0000c920: 7365 7175 656e 6365 206c 656e 6774 6820  sequence length 
+0000c930: 6f66 2074 6865 206b 6579 2061 6e64 2076  of the key and v
+0000c940: 616c 7565 2076 6563 746f 722e 0a20 2020  alue vector..   
+0000c950: 2020 2020 2020 2020 2068 6964 6465 6e5f           hidden_
+0000c960: 7369 7a65 2869 6e74 293a 2054 6865 2068  size(int): The h
+0000c970: 6964 6465 6e20 7369 7a65 206f 6620 7468  idden size of th
+0000c980: 6520 696e 7075 742e 0a20 2020 2020 2020  e input..       
+0000c990: 2020 2020 206e 756d 5f68 6561 6473 2869       num_heads(i
+0000c9a0: 6e74 293a 2054 6865 206e 756d 6265 7220  nt): The number 
+0000c9b0: 6f66 2074 6865 2068 6561 6473 2e0a 2020  of the heads..  
+0000c9c0: 2020 2020 2020 2020 2020 6869 6464 656e            hidden
+0000c9d0: 5f64 726f 706f 7574 5f72 6174 6528 666c  _dropout_rate(fl
+0000c9e0: 6f61 7429 3a20 5468 6520 6472 6f70 6f75  oat): The dropou
+0000c9f0: 7420 7261 7465 206f 6620 7468 6520 6669  t rate of the fi
+0000ca00: 6e61 6c20 6f75 7470 7574 206f 6620 7468  nal output of th
+0000ca10: 6520 6c61 7965 722e 2044 6566 6175 6c74  e layer. Default
+0000ca20: 3a30 2e31 2e0a 2020 2020 2020 2020 2020  :0.1..          
+0000ca30: 2020 6174 7465 6e74 696f 6e5f 6472 6f70    attention_drop
+0000ca40: 6f75 745f 7261 7465 2866 6c6f 6174 293a  out_rate(float):
+0000ca50: 2054 6865 2064 726f 706f 7574 2072 6174   The dropout rat
+0000ca60: 6520 6f66 2074 6865 2061 7474 656e 7469  e of the attenti
+0000ca70: 6f6e 2073 636f 7265 732e 2044 6566 6175  on scores. Defau
+0000ca80: 6c74 3a30 2e31 2e0a 2020 2020 2020 2020  lt:0.1..        
+0000ca90: 2020 2020 636f 6d70 7574 655f 6474 7970      compute_dtyp
+0000caa0: 6528 6474 7970 652e 4e75 6d62 6572 293a  e(dtype.Number):
+0000cab0: 2054 6865 2063 6f6d 7075 7461 7469 6f6e   The computation
+0000cac0: 2074 7970 6520 6f66 2064 656e 7365 2e20   type of dense. 
+0000cad0: 4465 6661 756c 7420 6d73 7479 7065 2e66  Default mstype.f
+0000cae0: 6c6f 6174 3136 2e0a 2020 2020 2020 2020  loat16..        
+0000caf0: 2020 2020 2020 2020 5368 6f75 6c64 2062          Should b
+0000cb00: 6520 6d73 7479 7065 2e66 6c6f 6174 3332  e mstype.float32
+0000cb10: 206f 7220 6d73 7479 7065 2e66 6c6f 6174   or mstype.float
+0000cb20: 3136 2e0a 2020 2020 2020 2020 2020 2020  16..            
+0000cb30: 736f 6674 6d61 785f 636f 6d70 7574 655f  softmax_compute_
+0000cb40: 7479 7065 2864 7479 7065 2e4e 756d 6265  type(dtype.Numbe
+0000cb50: 7229 3a20 5468 6520 7479 7065 206f 6620  r): The type of 
+0000cb60: 736f 6674 6d61 7820 636f 6d70 7574 6174  softmax computat
+0000cb70: 696f 6e20 6d6f 6475 6c65 2e20 4465 6661  ion module. Defa
+0000cb80: 756c 7420 6d73 7479 7065 2e66 6c6f 6174  ult mstype.float
+0000cb90: 3332 2e0a 2020 2020 2020 2020 2020 2020  32..            
+0000cba0: 2020 2020 5368 6f75 6c64 2062 6520 6d73      Should be ms
+0000cbb0: 7479 7065 2e66 6c6f 6174 3332 206f 7220  type.float32 or 
+0000cbc0: 6d73 7479 7065 2e66 6c6f 6174 3136 2e0a  mstype.float16..
+0000cbd0: 2020 2020 2020 2020 2020 2020 7061 7261              para
+0000cbe0: 6d5f 696e 6974 5f74 7970 6528 6474 7970  m_init_type(dtyp
+0000cbf0: 652e 4e75 6d62 6572 293a 2054 6865 2070  e.Number): The p
+0000cc00: 6172 616d 6574 6572 2069 6e69 7469 616c  arameter initial
+0000cc10: 697a 6174 696f 6e20 7479 7065 206f 6620  ization type of 
+0000cc20: 7468 6520 6d6f 6475 6c65 2e20 4465 6661  the module. Defa
+0000cc30: 756c 7420 6d73 7479 7065 2e66 6c6f 6174  ult mstype.float
+0000cc40: 3332 2e0a 2020 2020 2020 2020 2020 2020  32..            
+0000cc50: 2020 2020 5368 6f75 6c64 2062 6520 6d73      Should be ms
+0000cc60: 7479 7065 2e66 6c6f 6174 3332 206f 7220  type.float32 or 
+0000cc70: 6d73 7479 7065 2e66 6c6f 6174 3136 2e0a  mstype.float16..
+0000cc80: 2020 2020 2020 2020 2020 2020 7573 655f              use_
+0000cc90: 7061 7374 2862 6f6f 6c29 3a20 5573 6520  past(bool): Use 
+0000cca0: 7468 6520 7061 7374 2073 7461 7465 2074  the past state t
+0000ccb0: 6f20 636f 6d70 7574 652c 2075 7365 6420  o compute, used 
+0000ccc0: 666f 7220 696e 6372 656d 656e 7461 6c20  for incremental 
+0000ccd0: 7072 6564 6963 7469 6f6e 2e20 466f 7220  prediction. For 
+0000cce0: 6578 616d 706c 652c 2069 6620 7765 2068  example, if we h
+0000ccf0: 6176 6520 7477 6f0a 2020 2020 2020 2020  ave two.        
+0000cd00: 2020 2020 2020 2020 776f 7264 7320 616e          words an
+0000cd10: 6420 7761 6e74 2074 6f20 6765 6e65 7261  d want to genera
+0000cd20: 7465 2074 6865 2074 656e 206d 6f72 6520  te the ten more 
+0000cd30: 776f 7264 732e 2057 6520 6a75 7374 206e  words. We just n
+0000cd40: 6565 6420 746f 2063 6f6d 7075 7465 2074  eed to compute t
+0000cd50: 6865 2074 776f 2077 6f72 6473 2720 7374  he two words' st
+0000cd60: 6174 6520 6f6e 6c79 206f 6e63 652c 0a20  ate only once,. 
+0000cd70: 2020 2020 2020 2020 2020 2020 2020 2061                 a
+0000cd80: 6e64 2067 656e 6572 6174 6520 7468 6520  nd generate the 
+0000cd90: 6e65 7874 2077 6f72 6420 6f6e 6520 6279  next word one by
+0000cda0: 206f 6e65 2e20 5768 656e 2075 7365 5f70   one. When use_p
+0000cdb0: 6173 7420 6973 2054 7275 652c 2074 6865  ast is True, the
+0000cdc0: 7265 2061 7265 2074 776f 2073 7465 7073  re are two steps
+0000cdd0: 2074 6f20 7275 6e20 7468 6520 7072 6564   to run the pred
+0000cde0: 6963 7469 6f6e 2e0a 2020 2020 2020 2020  iction..        
+0000cdf0: 2020 2020 2020 2020 496e 2074 6865 2066          In the f
+0000ce00: 6972 7374 2073 7465 702c 2073 6574 2074  irst step, set t
+0000ce10: 6865 2069 735f 6669 7273 745f 6974 6572  he is_first_iter
+0000ce20: 6174 696f 6e20 746f 2062 6520 5472 7565  ation to be True
+0000ce30: 2062 790a 2020 2020 2020 2020 2020 2020   by.            
+0000ce40: 2020 2020 606d 6f64 656c 2e61 6464 5f66      `model.add_f
+0000ce50: 6c61 6773 5f72 6563 7572 7369 7665 2869  lags_recursive(i
+0000ce60: 735f 6669 7273 745f 6974 6572 6174 696f  s_first_iteratio
+0000ce70: 6e3d 5472 7565 2960 2c20 616e 6420 7061  n=True)`, and pa
+0000ce80: 7373 2074 6865 2066 756c 6c20 696e 7075  ss the full inpu
+0000ce90: 7473 2e20 5468 656e 2c20 7365 7420 7468  ts. Then, set th
+0000cea0: 650a 2020 2020 2020 2020 2020 2020 2020  e.              
+0000ceb0: 2020 6973 5f66 6972 7374 5f69 7465 7261    is_first_itera
+0000cec0: 7469 6f6e 2074 6f20 6265 2046 616c 7365  tion to be False
+0000ced0: 2062 7920 606d 6f64 656c 2e61 6464 5f66   by `model.add_f
+0000cee0: 6c61 6773 5f72 6563 7572 7369 7665 2869  lags_recursive(i
+0000cef0: 735f 6669 7273 745f 6974 6572 6174 696f  s_first_iteratio
+0000cf00: 6e3d 4661 6c73 6529 602e 2041 7420 7468  n=False)`. At th
+0000cf10: 6973 206d 6f6d 656e 742c 0a20 2020 2020  is moment,.     
+0000cf20: 2020 2020 2020 2020 2020 2070 6173 7320             pass 
+0000cf30: 7468 6520 7369 6e67 6c65 2073 7465 7027  the single step'
+0000cf40: 7320 696e 7075 7420 7465 6e73 6f72 2c20  s input tensor, 
+0000cf50: 616e 6420 6c6f 6f70 2069 742e 2044 6566  and loop it. Def
+0000cf60: 6175 6c74 2046 616c 7365 2e0a 2020 2020  ault False..    
+0000cf70: 2020 2020 2020 2020 7061 7261 6c6c 656c          parallel
+0000cf80: 5f63 6f6e 6669 6728 4f70 5061 7261 6c6c  _config(OpParall
+0000cf90: 656c 436f 6e66 6967 293a 2054 6865 2070  elConfig): The p
+0000cfa0: 6172 616c 6c65 6c20 636f 6e66 6967 7572  arallel configur
+0000cfb0: 652e 2044 6566 6175 6c74 2060 6465 6661  e. Default `defa
+0000cfc0: 756c 745f 6470 6d70 5f63 6f6e 6669 6760  ult_dpmp_config`
+0000cfd0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+0000cfe0: 2020 616e 2069 6e73 7461 6e63 6520 6f66    an instance of
+0000cff0: 2060 4f70 5061 7261 6c6c 656c 436f 6e66   `OpParallelConf
+0000d000: 6967 6020 7769 7468 2064 6566 6175 6c74  ig` with default
+0000d010: 2061 7267 732e 0a0a 2020 2020 2020 2020   args...        
+0000d020: 496e 7075 7473 3a0a 2020 2020 2020 2020  Inputs:.        
+0000d030: 2020 2020 2d20 2a2a 7175 6572 795f 7465      - **query_te
+0000d040: 6e73 6f72 2a2a 2028 5465 6e73 6f72 2920  nsor** (Tensor) 
+0000d050: 2d20 5468 6520 7175 6572 7920 7665 6374  - The query vect
+0000d060: 6f72 2077 6974 6820 7368 6170 6520 2862  or with shape (b
+0000d070: 6174 6368 5f73 697a 652c 2073 7263 5f73  atch_size, src_s
+0000d080: 6571 5f6c 656e 6774 682c 2068 6964 6465  eq_length, hidde
+0000d090: 6e5f 7369 7a65 2920 6f72 0a20 2020 2020  n_size) or.     
+0000d0a0: 2020 2020 2020 2020 2028 6261 7463 685f           (batch_
+0000d0b0: 7369 7a65 202a 2073 7263 5f73 6571 5f6c  size * src_seq_l
+0000d0c0: 656e 6774 682c 2068 6964 6465 6e5f 7369  ength, hidden_si
+0000d0d0: 7a65 292c 2069 6620 7468 6520 7573 655f  ze), if the use_
+0000d0e0: 7061 7374 2069 7320 4661 6c73 6520 6f72  past is False or
+0000d0f0: 2069 735f 6669 7273 745f 6974 6572 6174   is_first_iterat
+0000d100: 696f 6e3d 5472 7565 2e0a 2020 2020 2020  ion=True..      
+0000d110: 2020 2020 2020 2020 4f74 6865 7277 6973          Otherwis
+0000d120: 652c 206d 7573 7420 6265 2028 6261 7463  e, must be (batc
+0000d130: 685f 7369 7a65 2c20 312c 2068 6964 6465  h_size, 1, hidde
+0000d140: 6e5f 7369 7a65 290a 2020 2020 2020 2020  n_size).        
+0000d150: 2020 2020 2d20 2a2a 6b65 795f 7465 6e73      - **key_tens
+0000d160: 6f72 2a2a 2028 5465 6e73 6f72 2920 2d20  or** (Tensor) - 
+0000d170: 5468 6520 6b65 7920 7665 6374 6f72 2077  The key vector w
+0000d180: 6974 6820 7368 6170 6520 2862 6174 6368  ith shape (batch
+0000d190: 5f73 697a 652c 2074 6774 5f73 6571 5f6c  _size, tgt_seq_l
+0000d1a0: 656e 6774 682c 2068 6964 6465 6e5f 7369  ength, hidden_si
+0000d1b0: 7a65 2920 6f72 0a20 2020 2020 2020 2020  ze) or.         
+0000d1c0: 2020 2020 2028 6261 7463 685f 7369 7a65       (batch_size
+0000d1d0: 202a 2074 6774 5f73 6571 5f6c 656e 6774   * tgt_seq_lengt
+0000d1e0: 682c 2068 6964 6465 6e5f 7369 7a65 292c  h, hidden_size),
+0000d1f0: 2069 6620 7468 6520 7573 655f 7061 7374   if the use_past
+0000d200: 2069 7320 4661 6c73 6520 6f72 2069 735f   is False or is_
+0000d210: 6669 7273 745f 6974 6572 6174 696f 6e3d  first_iteration=
+0000d220: 5472 7565 2e0a 2020 2020 2020 2020 2020  True..          
+0000d230: 2020 2020 4f74 6865 7277 6973 652c 206d      Otherwise, m
+0000d240: 7573 7420 6265 2028 6261 7463 685f 7369  ust be (batch_si
+0000d250: 7a65 2c20 312c 2068 6964 6465 6e5f 7369  ze, 1, hidden_si
+0000d260: 7a65 290a 2020 2020 2020 2020 2020 2020  ze).            
+0000d270: 2d20 2a2a 7661 6c75 655f 7465 6e73 6f72  - **value_tensor
+0000d280: 2a2a 2028 5465 6e73 6f72 2920 2d20 5468  ** (Tensor) - Th
+0000d290: 6520 7661 6c75 6520 7665 6374 6f72 2077  e value vector w
+0000d2a0: 6974 6820 7368 6170 6520 2862 6174 6368  ith shape (batch
+0000d2b0: 5f73 697a 652c 2074 6774 5f73 6571 5f6c  _size, tgt_seq_l
+0000d2c0: 656e 6774 682c 2068 6964 6465 6e5f 7369  ength, hidden_si
+0000d2d0: 7a65 2920 6f72 0a20 2020 2020 2020 2020  ze) or.         
+0000d2e0: 2020 2020 2028 6261 7463 685f 7369 7a65       (batch_size
+0000d2f0: 202a 2074 6774 5f73 6571 5f6c 656e 6774   * tgt_seq_lengt
+0000d300: 682c 2068 6964 6465 6e5f 7369 7a65 292c  h, hidden_size),
+0000d310: 2069 6620 7468 6520 7573 655f 7061 7374   if the use_past
+0000d320: 2069 7320 4661 6c73 6520 6f72 2069 735f   is False or is_
+0000d330: 6669 7273 745f 6974 6572 6174 696f 6e3d  first_iteration=
+0000d340: 5472 7565 2e0a 2020 2020 2020 2020 2020  True..          
+0000d350: 2020 2020 4f74 6865 7277 6973 652c 206d      Otherwise, m
+0000d360: 7573 7420 6265 2028 6261 7463 685f 7369  ust be (batch_si
+0000d370: 7a65 2c20 312c 2068 6964 6465 6e5f 7369  ze, 1, hidden_si
+0000d380: 7a65 290a 2020 2020 2020 2020 2020 2020  ze).            
+0000d390: 2d20 2a2a 6174 7465 6e74 696f 6e5f 6d61  - **attention_ma
+0000d3a0: 736b 2a2a 2028 5465 6e73 6f72 2920 2d20  sk** (Tensor) - 
+0000d3b0: 4966 2074 6865 2075 7365 5f70 6173 7420  If the use_past 
+0000d3c0: 6973 2046 616c 7365 206f 7220 6973 5f66  is False or is_f
+0000d3d0: 6972 7374 5f69 7465 7261 7469 6f6e 3d54  irst_iteration=T
+0000d3e0: 7275 652c 2074 6865 2061 7474 656e 7469  rue, the attenti
+0000d3f0: 6f6e 206d 6173 6b0a 2020 2020 2020 2020  on mask.        
+0000d400: 2020 2020 2020 6d61 7472 6978 2073 686f        matrix sho
+0000d410: 756c 6420 6261 2028 6261 7463 685f 7369  uld ba (batch_si
+0000d420: 7a65 2c20 7372 635f 7365 715f 6c65 6e67  ze, src_seq_leng
+0000d430: 7468 2c20 7467 745f 7365 715f 6c65 6e67  th, tgt_seq_leng
+0000d440: 7468 292c 206f 7220 4e6f 6e65 2e20 4e6f  th), or None. No
+0000d450: 6e65 206d 6561 6e73 2074 6865 7265 2077  ne means there w
+0000d460: 696c 6c20 6265 206e 6f20 6d61 736b 0a20  ill be no mask. 
+0000d470: 2020 2020 2020 2020 2020 2020 2069 6e20               in 
+0000d480: 736f 6674 6d61 7820 636f 6d70 7574 6174  softmax computat
+0000d490: 696f 6e2e 204f 7468 6572 7769 7365 2c20  ion. Otherwise, 
+0000d4a0: 7468 6520 6d61 736b 206d 7573 7420 6265  the mask must be
+0000d4b0: 2028 6261 7463 685f 7369 7a65 2c20 312c   (batch_size, 1,
+0000d4c0: 2074 6774 5f73 6571 5f6c 656e 6774 6829   tgt_seq_length)
+0000d4d0: 0a20 2020 2020 2020 2020 2020 202d 202a  .            - *
+0000d4e0: 2a6b 6579 5f70 6173 742a 2a20 2854 656e  *key_past** (Ten
+0000d4f0: 736f 7229 202d 2046 6c6f 6174 3136 2074  sor) - Float16 t
+0000d500: 656e 736f 7220 7769 7468 2073 6861 7065  ensor with shape
+0000d510: 2028 6261 7463 685f 7369 7a65 2c20 6e75   (batch_size, nu
+0000d520: 6d5f 6865 6164 732c 2073 697a 655f 7065  m_heads, size_pe
+0000d530: 725f 6865 6164 2c20 7467 745f 7365 715f  r_head, tgt_seq_
+0000d540: 6c65 6e67 7468 292e 0a20 2020 2020 2020  length)..       
+0000d550: 2020 2020 2020 2054 6865 2070 6173 7420         The past 
+0000d560: 6361 6c63 756c 6174 6564 206b 6579 2076  calculated key v
+0000d570: 6563 746f 722e 2055 7365 6420 666f 7220  ector. Used for 
+0000d580: 696e 6372 656d 656e 7461 6c20 7072 6564  incremental pred
+0000d590: 6963 7469 6f6e 2077 6865 6e20 7468 6520  iction when the 
+0000d5a0: 7573 655f 7061 7374 2069 7320 5472 7565  use_past is True
+0000d5b0: 2e0a 2020 2020 2020 2020 2020 2020 2020  ..              
+0000d5c0: 4465 6661 756c 7420 4e6f 6e65 2e0a 2020  Default None..  
+0000d5d0: 2020 2020 2020 2020 2020 2d20 2a2a 7661            - **va
+0000d5e0: 6c75 655f 7061 7374 2a2a 2028 5465 6e73  lue_past** (Tens
+0000d5f0: 6f72 2920 2d20 466c 6f61 7431 3620 7465  or) - Float16 te
+0000d600: 6e73 6f72 2077 6974 6820 7368 6170 650a  nsor with shape.
+0000d610: 2020 2020 2020 2020 2020 2020 2020 2862                (b
+0000d620: 6174 6368 5f73 697a 652c 206e 756d 5f68  atch_size, num_h
+0000d630: 6561 6473 2c20 7467 745f 7365 715f 6c65  eads, tgt_seq_le
+0000d640: 6e67 7468 2c20 7369 7a65 5f70 6572 5f68  ngth, size_per_h
+0000d650: 6561 6429 2e0a 2020 2020 2020 2020 2020  ead)..          
+0000d660: 2020 2020 5468 6520 7061 7374 2063 616c      The past cal
+0000d670: 6375 6c61 7465 6420 7661 6c75 6520 7665  culated value ve
+0000d680: 6374 6f72 2e20 5573 6564 2066 6f72 2069  ctor. Used for i
+0000d690: 6e63 7265 6d65 6e74 616c 2070 7265 6469  ncremental predi
+0000d6a0: 6374 696f 6e20 7768 656e 2074 6865 2075  ction when the u
+0000d6b0: 7365 5f70 6173 7420 6973 2054 7275 652e  se_past is True.
+0000d6c0: 0a20 2020 2020 2020 2020 2020 2020 2044  .              D
+0000d6d0: 6566 6175 6c74 204e 6f6e 652e 0a20 2020  efault None..   
+0000d6e0: 2020 2020 2020 2020 202d 202a 2a62 6174           - **bat
+0000d6f0: 6368 5f76 616c 6964 5f6c 656e 6774 682a  ch_valid_length*
+0000d700: 2a20 2854 656e 736f 7229 202d 2049 6e74  * (Tensor) - Int
+0000d710: 3332 2074 656e 736f 7220 7769 7468 2073  32 tensor with s
+0000d720: 6861 7065 2028 6261 7463 685f 7369 7a65  hape (batch_size
+0000d730: 2c29 2074 6865 2070 6173 7420 6361 6c63  ,) the past calc
+0000d740: 756c 6174 6564 2074 6865 2069 6e64 6578  ulated the index
+0000d750: 2e0a 2020 2020 2020 2020 2020 2020 2020  ..              
+0000d760: 5573 6564 2066 6f72 2069 6e63 7265 6d65  Used for increme
+0000d770: 6e74 616c 2070 7265 6469 6374 696f 6e20  ntal prediction 
+0000d780: 7768 656e 2074 6865 2075 7365 5f70 6173  when the use_pas
+0000d790: 7420 6973 2054 7275 652e 2044 6566 6175  t is True. Defau
+0000d7a0: 6c74 204e 6f6e 652e 0a0a 2020 2020 2020  lt None...      
+0000d7b0: 2020 4f75 7470 7574 733a 0a20 2020 2020    Outputs:.     
+0000d7c0: 2020 2020 2020 2054 7570 6c65 2c20 6120         Tuple, a 
+0000d7d0: 7475 706c 6520 636f 6e74 6169 6e73 2860  tuple contains(`
+0000d7e0: 6f75 7470 7574 602c 2060 6c61 7965 725f  output`, `layer_
+0000d7f0: 7072 6573 656e 7460 290a 0a20 2020 2020  present`)..     
+0000d800: 2020 2020 2020 202d 202a 2a6f 7574 7075         - **outpu
+0000d810: 742a 2a20 2854 656e 736f 7229 202d 2054  t** (Tensor) - T
+0000d820: 656e 736f 722c 2074 6865 2066 6c6f 6174  ensor, the float
+0000d830: 2074 656e 736f 7220 6f66 2074 6865 206f   tensor of the o
+0000d840: 7574 7075 7420 6f66 2074 6865 206c 6179  utput of the lay
+0000d850: 6572 2077 6974 680a 2020 2020 2020 2020  er with.        
+0000d860: 2020 2020 2020 7368 6170 6520 2862 6174        shape (bat
+0000d870: 6368 5f73 697a 652c 2073 7263 5f73 6571  ch_size, src_seq
+0000d880: 5f6c 656e 6774 682c 2068 6964 6465 6e5f  _length, hidden_
+0000d890: 7369 7a65 2920 6f72 2028 6261 7463 685f  size) or (batch_
+0000d8a0: 7369 7a65 202a 2073 7263 5f73 6571 5f6c  size * src_seq_l
+0000d8b0: 656e 6774 682c 2068 6964 6465 6e5f 7369  ength, hidden_si
+0000d8c0: 7a65 292c 0a20 2020 2020 2020 2020 2020  ze),.           
+0000d8d0: 2020 2069 6620 7468 6520 7573 655f 7061     if the use_pa
+0000d8e0: 7374 2069 7320 4661 6c73 6520 6f72 2069  st is False or i
+0000d8f0: 735f 6669 7273 745f 6974 6572 6174 696f  s_first_iteratio
+0000d900: 6e3d 5472 7565 2e20 4f74 6865 7277 6973  n=True. Otherwis
+0000d910: 652c 2069 7420 7769 6c6c 2062 6520 2862  e, it will be (b
+0000d920: 6174 6368 5f73 697a 652c 2031 2c20 6869  atch_size, 1, hi
+0000d930: 6464 656e 5f73 697a 6529 2e0a 0a20 2020  dden_size)...   
+0000d940: 2020 2020 2020 2020 202d 202a 2a6c 6179           - **lay
+0000d950: 6572 5f70 7265 7365 6e74 2a2a 2028 5475  er_present** (Tu
+0000d960: 706c 6529 202d 2041 2074 7570 6c65 206f  ple) - A tuple o
+0000d970: 6620 7468 6520 5465 6e73 6f72 206f 6620  f the Tensor of 
+0000d980: 7468 6520 7072 6f6a 6563 7465 6420 6b65  the projected ke
+0000d990: 7920 616e 6420 7661 6c75 6520 7665 6374  y and value vect
+0000d9a0: 6f72 2077 6974 680a 2020 2020 2020 2020  or with.        
+0000d9b0: 2020 2020 2020 2828 6261 7463 685f 7369        ((batch_si
+0000d9c0: 7a65 2c20 6e75 6d5f 6865 6164 732c 2073  ze, num_heads, s
+0000d9d0: 697a 655f 7065 725f 6865 6164 2c20 7467  ize_per_head, tg
+0000d9e0: 745f 7365 715f 6c65 6e67 7468 292c 0a20  t_seq_length),. 
+0000d9f0: 2020 2020 2020 2020 2020 2020 2028 6261               (ba
+0000da00: 7463 685f 7369 7a65 2c20 6e75 6d5f 6865  tch_size, num_he
+0000da10: 6164 732c 2074 6774 5f73 6571 5f6c 656e  ads, tgt_seq_len
+0000da20: 6774 682c 2073 697a 655f 7065 725f 6865  gth, size_per_he
+0000da30: 6164 2929 2e0a 0a20 2020 2020 2020 2053  ad))...        S
+0000da40: 7570 706f 7274 6564 2050 6c61 7466 6f72  upported Platfor
+0000da50: 6d73 3a0a 2020 2020 2020 2020 2020 2020  ms:.            
+0000da60: 6060 4173 6365 6e64 6060 2060 6047 5055  ``Ascend`` ``GPU
+0000da70: 6060 0a0a 2020 2020 2020 2020 4578 616d  ``..        Exam
+0000da80: 706c 6573 3a0a 2020 2020 2020 2020 2020  ples:.          
+0000da90: 2020 3e3e 3e20 696d 706f 7274 206e 756d    >>> import num
+0000daa0: 7079 2061 7320 6e70 0a20 2020 2020 2020  py as np.       
+0000dab0: 2020 2020 203e 3e3e 2066 726f 6d20 6d69       >>> from mi
+0000dac0: 6e64 666f 726d 6572 732e 6d6f 6475 6c65  ndformers.module
+0000dad0: 732e 7472 616e 7366 6f72 6d65 7220 696d  s.transformer im
+0000dae0: 706f 7274 204d 756c 7469 4865 6164 4174  port MultiHeadAt
+0000daf0: 7465 6e74 696f 6e0a 2020 2020 2020 2020  tention.        
+0000db00: 2020 2020 3e3e 3e20 6672 6f6d 206d 696e      >>> from min
+0000db10: 6473 706f 7265 2069 6d70 6f72 7420 6474  dspore import dt
+0000db20: 7970 6520 6173 206d 7374 7970 650a 2020  ype as mstype.  
+0000db30: 2020 2020 2020 2020 2020 3e3e 3e20 6672            >>> fr
+0000db40: 6f6d 206d 696e 6473 706f 7265 2069 6d70  om mindspore imp
+0000db50: 6f72 7420 5465 6e73 6f72 0a20 2020 2020  ort Tensor.     
+0000db60: 2020 2020 2020 203e 3e3e 206d 6f64 656c         >>> model
+0000db70: 203d 204d 756c 7469 4865 6164 4174 7465   = MultiHeadAtte
+0000db80: 6e74 696f 6e28 6261 7463 685f 7369 7a65  ntion(batch_size
+0000db90: 3d4e 6f6e 652c 2068 6964 6465 6e5f 7369  =None, hidden_si
+0000dba0: 7a65 3d31 352c 2073 7263 5f73 6571 5f6c  ze=15, src_seq_l
+0000dbb0: 656e 6774 683d 3230 2c20 7467 745f 7365  ength=20, tgt_se
+0000dbc0: 715f 6c65 6e67 7468 3d32 302c 0a20 2020  q_length=20,.   
+0000dbd0: 2020 2020 2020 2020 202e 2e2e 2020 2020           ...    
+0000dbe0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000dbf0: 2020 2020 2020 2020 6e75 6d5f 6865 6164          num_head
+0000dc00: 733d 3329 0a20 2020 2020 2020 2020 2020  s=3).           
+0000dc10: 203e 3e3e 2066 726f 6d5f 7465 6e73 6f72   >>> from_tensor
+0000dc20: 203d 2054 656e 736f 7228 6e70 2e6f 6e65   = Tensor(np.one
+0000dc30: 7328 2832 2c20 3230 2c20 3135 2929 2c20  s((2, 20, 15)), 
+0000dc40: 6d73 7479 7065 2e66 6c6f 6174 3332 290a  mstype.float32).
+0000dc50: 2020 2020 2020 2020 2020 2020 3e3e 3e20              >>> 
+0000dc60: 746f 5f74 656e 736f 7220 3d20 5465 6e73  to_tensor = Tens
+0000dc70: 6f72 286e 702e 6f6e 6573 2828 322c 2032  or(np.ones((2, 2
+0000dc80: 302c 2031 3529 292c 206d 7374 7970 652e  0, 15)), mstype.
+0000dc90: 666c 6f61 7431 3629 0a20 2020 2020 2020  float16).       
+0000dca0: 2020 2020 203e 3e3e 2061 7474 656e 7469       >>> attenti
+0000dcb0: 6f6e 5f6d 6173 6b20 3d20 5465 6e73 6f72  on_mask = Tensor
+0000dcc0: 286e 702e 6f6e 6573 2828 322c 2032 302c  (np.ones((2, 20,
+0000dcd0: 2032 3029 292c 206d 7374 7970 652e 666c   20)), mstype.fl
+0000dce0: 6f61 7431 3629 0a20 2020 2020 2020 2020  oat16).         
+0000dcf0: 2020 203e 3e3e 2061 7474 6e5f 6f75 742c     >>> attn_out,
+0000dd00: 2070 6173 7420 3d20 6d6f 6465 6c28 6672   past = model(fr
+0000dd10: 6f6d 5f74 656e 736f 722c 2074 6f5f 7465  om_tensor, to_te
+0000dd20: 6e73 6f72 2c20 746f 5f74 656e 736f 722c  nsor, to_tensor,
+0000dd30: 2061 7474 656e 7469 6f6e 5f6d 6173 6b29   attention_mask)
+0000dd40: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
+0000dd50: 2070 7269 6e74 2861 7474 6e5f 6f75 742e   print(attn_out.
+0000dd60: 7368 6170 6529 0a20 2020 2020 2020 2020  shape).         
+0000dd70: 2020 2028 322c 2032 302c 2031 3529 0a20     (2, 20, 15). 
+0000dd80: 2020 2020 2020 2020 2020 203e 3e3e 2070             >>> p
+0000dd90: 7269 6e74 2870 6173 745b 305d 2e73 6861  rint(past[0].sha
+0000dda0: 7065 290a 2020 2020 2020 2020 2020 2020  pe).            
+0000ddb0: 2832 2c20 332c 2035 2c20 3230 290a 2020  (2, 3, 5, 20).  
+0000ddc0: 2020 2020 2020 2020 2020 3e3e 3e20 7072            >>> pr
+0000ddd0: 696e 7428 7061 7374 5b31 5d2e 7368 6170  int(past[1].shap
+0000dde0: 6529 0a20 2020 2020 2020 2020 2020 2028  e).            (
+0000ddf0: 322c 2033 2c20 3230 2c20 3529 0a20 2020  2, 3, 20, 5).   
+0000de00: 2020 2020 2020 2020 203e 3e3e 2023 2057           >>> # W
+0000de10: 6865 6e20 7573 6520 7573 655f 7061 7374  hen use use_past
+0000de20: 3d54 7275 652c 2069 7420 696e 636c 7564  =True, it includ
+0000de30: 6573 2074 776f 2073 7465 7073 2074 6f20  es two steps to 
+0000de40: 696d 706c 656d 656e 7420 7468 6520 696e  implement the in
+0000de50: 6372 656d 656e 7461 6c20 7072 6564 6963  cremental predic
+0000de60: 7469 6f6e 2e0a 2020 2020 2020 2020 2020  tion..          
+0000de70: 2020 3e3e 3e20 2320 5374 6570 2031 3a20    >>> # Step 1: 
+0000de80: 7365 7420 6973 5f66 6972 7374 5f69 7465  set is_first_ite
+0000de90: 7261 7469 6f6e 3d54 7275 652c 2061 6e64  ration=True, and
+0000dea0: 2069 6e70 7574 2074 6865 2066 756c 6c20   input the full 
+0000deb0: 7365 7175 656e 6365 206c 656e 6774 6827  sequence length'
+0000dec0: 7320 7374 6174 652e 0a20 2020 2020 2020  s state..       
+0000ded0: 2020 2020 203e 3e3e 2023 2057 6520 6e65       >>> # We ne
+0000dee0: 6564 2074 6f20 7072 6570 6172 6520 7468  ed to prepare th
+0000def0: 6520 6d65 6d6f 7279 2070 6172 616d 6574  e memory paramet
+0000df00: 6572 7320 666f 7220 7361 7669 6e67 206b  ers for saving k
+0000df10: 6579 2061 6e64 2076 616c 7565 2073 7461  ey and value sta
+0000df20: 7465 7320 6669 7273 746c 792e 0a20 2020  tes firstly..   
+0000df30: 2020 2020 2020 2020 203e 3e3e 206d 6f64           >>> mod
+0000df40: 656c 203d 204d 756c 7469 4865 6164 4174  el = MultiHeadAt
+0000df50: 7465 6e74 696f 6e28 6261 7463 685f 7369  tention(batch_si
+0000df60: 7a65 3d32 2c20 6869 6464 656e 5f73 697a  ze=2, hidden_siz
+0000df70: 653d 3135 2c20 7372 635f 7365 715f 6c65  e=15, src_seq_le
+0000df80: 6e67 7468 3d32 302c 2074 6774 5f73 6571  ngth=20, tgt_seq
+0000df90: 5f6c 656e 6774 683d 3230 2c0a 2020 2020  _length=20,.    
+0000dfa0: 2020 2020 2020 2020 2e2e 2e20 2020 2020          ...     
+0000dfb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000dfc0: 2020 2020 2020 206e 756d 5f68 6561 6473         num_heads
+0000dfd0: 3d33 2c20 7573 655f 7061 7374 3d54 7275  =3, use_past=Tru
+0000dfe0: 6529 0a20 2020 2020 2020 2020 2020 203e  e).            >
+0000dff0: 3e3e 206b 6579 5f70 6173 7420 3d20 5465  >> key_past = Te
+0000e000: 6e73 6f72 286e 702e 7a65 726f 7328 7368  nsor(np.zeros(sh
+0000e010: 6170 653d 2832 2c20 332c 2035 2c20 3230  ape=(2, 3, 5, 20
+0000e020: 2929 2c20 6d73 7479 7065 2e66 6c6f 6174  )), mstype.float
+0000e030: 3136 290a 2020 2020 2020 2020 2020 2020  16).            
+0000e040: 3e3e 3e20 7661 6c75 655f 7061 7374 203d  >>> value_past =
+0000e050: 2054 656e 736f 7228 6e70 2e7a 6572 6f73   Tensor(np.zeros
+0000e060: 2873 6861 7065 3d28 322c 2033 2c20 3230  (shape=(2, 3, 20
+0000e070: 2c20 3529 292c 206d 7374 7970 652e 666c  , 5)), mstype.fl
+0000e080: 6f61 7431 3629 0a20 2020 2020 2020 2020  oat16).         
+0000e090: 2020 203e 3e3e 2062 6174 6368 5f76 616c     >>> batch_val
+0000e0a0: 6964 5f6c 656e 6774 6820 3d20 5465 6e73  id_length = Tens
+0000e0b0: 6f72 286e 702e 6f6e 6573 2828 322c 2929  or(np.ones((2,))
+0000e0c0: 2c20 6d73 7479 7065 2e69 6e74 3332 290a  , mstype.int32).
+0000e0d0: 2020 2020 2020 2020 2020 2020 3e3e 3e20              >>> 
+0000e0e0: 2320 5365 7420 6973 5f66 6972 7374 5f69  # Set is_first_i
+0000e0f0: 7465 7261 7469 6f6e 3d54 7275 6520 746f  teration=True to
+0000e100: 2067 656e 6572 6174 6520 7468 6520 6675   generate the fu
+0000e110: 6c6c 206d 656d 6f72 7920 7374 6174 6573  ll memory states
+0000e120: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
+0000e130: 206d 6f64 656c 2e61 6464 5f66 6c61 6773   model.add_flags
+0000e140: 5f72 6563 7572 7369 7665 2869 735f 6669  _recursive(is_fi
+0000e150: 7273 745f 6974 6572 6174 696f 6e3d 5472  rst_iteration=Tr
+0000e160: 7565 290a 2020 2020 2020 2020 2020 2020  ue).            
+0000e170: 3e3e 3e20 6174 746e 5f6f 7574 2c20 7061  >>> attn_out, pa
+0000e180: 7374 203d 206d 6f64 656c 2866 726f 6d5f  st = model(from_
+0000e190: 7465 6e73 6f72 2c20 746f 5f74 656e 736f  tensor, to_tenso
+0000e1a0: 722c 2074 6f5f 7465 6e73 6f72 2c20 6174  r, to_tensor, at
+0000e1b0: 7465 6e74 696f 6e5f 6d61 736b 2c20 6b65  tention_mask, ke
+0000e1c0: 795f 7061 7374 2c20 7661 6c75 655f 7061  y_past, value_pa
+0000e1d0: 7374 2c0a 2020 2020 2020 2020 2020 2020  st,.            
+0000e1e0: 2e2e 2e20 2020 2020 2020 2020 2020 2020  ...             
+0000e1f0: 2020 2020 2020 2020 2020 2062 6174 6368             batch
+0000e200: 5f76 616c 6964 5f6c 656e 6774 6829 0a20  _valid_length). 
+0000e210: 2020 2020 2020 2020 2020 203e 3e3e 2070             >>> p
+0000e220: 7269 6e74 2861 7474 6e5f 6f75 742e 7368  rint(attn_out.sh
+0000e230: 6170 6529 0a20 2020 2020 2020 2020 2020  ape).           
+0000e240: 2028 322c 2032 302c 2031 3529 0a20 2020   (2, 20, 15).   
+0000e250: 2020 2020 2020 2020 203e 3e3e 2070 7269           >>> pri
+0000e260: 6e74 2870 6173 745b 305d 2e73 6861 7065  nt(past[0].shape
+0000e270: 290a 2020 2020 2020 2020 2020 2020 2832  ).            (2
+0000e280: 2c20 332c 2035 2c20 3230 290a 2020 2020  , 3, 5, 20).    
+0000e290: 2020 2020 2020 2020 3e3e 3e20 7072 696e          >>> prin
+0000e2a0: 7428 7061 7374 5b31 5d2e 7368 6170 6529  t(past[1].shape)
+0000e2b0: 0a20 2020 2020 2020 2020 2020 2028 322c  .            (2,
+0000e2c0: 2033 2c20 3230 2c20 3529 0a20 2020 2020   3, 20, 5).     
+0000e2d0: 2020 2020 2020 203e 3e3e 2066 726f 6d5f         >>> from_
+0000e2e0: 7465 6e73 6f72 203d 2054 656e 736f 7228  tensor = Tensor(
+0000e2f0: 6e70 2e6f 6e65 7328 2832 2c20 312c 2031  np.ones((2, 1, 1
+0000e300: 3529 292c 206d 7374 7970 652e 666c 6f61  5)), mstype.floa
+0000e310: 7433 3229 0a20 2020 2020 2020 2020 2020  t32).           
+0000e320: 203e 3e3e 2074 6f5f 7465 6e73 6f72 203d   >>> to_tensor =
+0000e330: 2054 656e 736f 7228 6e70 2e6f 6e65 7328   Tensor(np.ones(
+0000e340: 2832 2c20 312c 2031 3529 292c 206d 7374  (2, 1, 15)), mst
+0000e350: 7970 652e 666c 6f61 7431 3629 0a20 2020  ype.float16).   
+0000e360: 2020 2020 2020 2020 203e 3e3e 2061 7474           >>> att
+0000e370: 656e 7469 6f6e 5f6d 6173 6b20 3d20 5465  ention_mask = Te
+0000e380: 6e73 6f72 286e 702e 6f6e 6573 2828 322c  nsor(np.ones((2,
+0000e390: 2031 2c20 3230 2929 2c20 6d73 7479 7065   1, 20)), mstype
+0000e3a0: 2e66 6c6f 6174 3136 290a 2020 2020 2020  .float16).      
+0000e3b0: 2020 2020 2020 3e3e 3e20 2320 5374 6570        >>> # Step
+0000e3c0: 2032 3a20 7365 7420 6973 5f66 6972 7374   2: set is_first
+0000e3d0: 5f69 7465 7261 7469 6f6e 3d46 616c 7365  _iteration=False
+0000e3e0: 2c20 616e 6420 7061 7373 2074 6865 2073  , and pass the s
+0000e3f0: 696e 676c 6520 776f 7264 2074 6f20 7275  ingle word to ru
+0000e400: 6e20 7468 6520 7072 6564 6963 7469 6f6e  n the prediction
+0000e410: 2072 6174 6865 7220 7468 616e 2074 6865   rather than the
+0000e420: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
+0000e430: 2023 2066 756c 6c20 7365 7175 656e 6365   # full sequence
+0000e440: 2e0a 2020 2020 2020 2020 2020 2020 3e3e  ..            >>
+0000e450: 3e20 6d6f 6465 6c2e 6164 645f 666c 6167  > model.add_flag
+0000e460: 735f 7265 6375 7273 6976 6528 6973 5f66  s_recursive(is_f
+0000e470: 6972 7374 5f69 7465 7261 7469 6f6e 3d46  irst_iteration=F
+0000e480: 616c 7365 290a 2020 2020 2020 2020 2020  alse).          
+0000e490: 2020 3e3e 3e20 6174 746e 5f6f 7574 2c20    >>> attn_out, 
+0000e4a0: 7061 7374 203d 206d 6f64 656c 2866 726f  past = model(fro
+0000e4b0: 6d5f 7465 6e73 6f72 2c20 746f 5f74 656e  m_tensor, to_ten
+0000e4c0: 736f 722c 2074 6f5f 7465 6e73 6f72 2c20  sor, to_tensor, 
+0000e4d0: 6174 7465 6e74 696f 6e5f 6d61 736b 2c20  attention_mask, 
+0000e4e0: 6b65 795f 7061 7374 2c20 7661 6c75 655f  key_past, value_
+0000e4f0: 7061 7374 2c0a 2020 2020 2020 2020 2020  past,.          
+0000e500: 2020 2e2e 2e20 2020 2020 2020 2020 2020    ...           
+0000e510: 2020 2020 2020 2020 2020 2020 2062 6174               bat
+0000e520: 6368 5f76 616c 6964 5f6c 656e 6774 6829  ch_valid_length)
+0000e530: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
+0000e540: 2070 7269 6e74 2861 7474 6e5f 6f75 742e   print(attn_out.
+0000e550: 7368 6170 6529 0a20 2020 2020 2020 2020  shape).         
+0000e560: 2020 2028 322c 2031 2c20 3135 290a 2020     (2, 1, 15).  
+0000e570: 2020 2020 2020 2020 2020 3e3e 3e20 7072            >>> pr
+0000e580: 696e 7428 7061 7374 5b30 5d2e 7368 6170  int(past[0].shap
+0000e590: 6529 0a20 2020 2020 2020 2020 2020 2028  e).            (
+0000e5a0: 322c 2033 2c20 352c 2032 3029 0a20 2020  2, 3, 5, 20).   
+0000e5b0: 2020 2020 2020 2020 203e 3e3e 2070 7269           >>> pri
+0000e5c0: 6e74 2870 6173 745b 315d 2e73 6861 7065  nt(past[1].shape
+0000e5d0: 290a 2020 2020 2020 2020 2020 2020 2832  ).            (2
+0000e5e0: 2c20 332c 2032 302c 2035 290a 2020 2020  , 3, 20, 5).    
+0000e5f0: 2222 220a 0a20 2020 2040 5f4c 6f67 4163  """..    @_LogAc
+0000e600: 7469 6f6e 4f6e 6365 286d 5f6c 6f67 6765  tionOnce(m_logge
+0000e610: 723d 6c6f 6767 6572 2c20 6b65 793d 274d  r=logger, key='M
+0000e620: 756c 7469 4865 6164 4174 7465 6e74 696f  ultiHeadAttentio
+0000e630: 6e27 2c0a 2020 2020 2020 2020 2020 2020  n',.            
+0000e640: 2020 2020 2020 2020 6e6f 5f77 6172 6e69          no_warni
+0000e650: 6e67 3d5f 6765 745f 7061 7261 6c6c 656c  ng=_get_parallel
+0000e660: 5f6d 6f64 6528 2920 696e 2028 5061 7261  _mode() in (Para
+0000e670: 6c6c 656c 4d6f 6465 2e53 5441 4e44 5f41  llelMode.STAND_A
+0000e680: 4c4f 4e45 2c29 290a 2020 2020 405f 6172  LONE,)).    @_ar
+0000e690: 6773 5f74 7970 655f 7661 6c69 6461 746f  gs_type_validato
+0000e6a0: 725f 6368 6563 6b28 6869 6464 656e 5f73  r_check(hidden_s
+0000e6b0: 697a 653d 5661 6c69 6461 746f 722e 6368  ize=Validator.ch
+0000e6c0: 6563 6b5f 706f 7369 7469 7665 5f69 6e74  eck_positive_int
+0000e6d0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+0000e6e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000e6f0: 2020 6e75 6d5f 6865 6164 733d 5661 6c69    num_heads=Vali
+0000e700: 6461 746f 722e 6368 6563 6b5f 706f 7369  dator.check_posi
+0000e710: 7469 7665 5f69 6e74 2c0a 2020 2020 2020  tive_int,.      
+0000e720: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000e730: 2020 2020 2020 2020 2020 7372 635f 7365            src_se
+0000e740: 715f 6c65 6e67 7468 3d56 616c 6964 6174  q_length=Validat
+0000e750: 6f72 2e63 6865 636b 5f70 6f73 6974 6976  or.check_positiv
+0000e760: 655f 696e 742c 0a20 2020 2020 2020 2020  e_int,.         
+0000e770: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000e780: 2020 2020 2020 2074 6774 5f73 6571 5f6c         tgt_seq_l
+0000e790: 656e 6774 683d 5661 6c69 6461 746f 722e  ength=Validator.
+0000e7a0: 6368 6563 6b5f 706f 7369 7469 7665 5f69  check_positive_i
+0000e7b0: 6e74 2c0a 2020 2020 2020 2020 2020 2020  nt,.            
+0000e7c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000e7d0: 2020 2020 6174 7465 6e74 696f 6e5f 6472      attention_dr
+0000e7e0: 6f70 6f75 745f 7261 7465 3d56 616c 6964  opout_rate=Valid
+0000e7f0: 6174 6f72 2e63 6865 636b 5f6e 6f6e 5f6e  ator.check_non_n
+0000e800: 6567 6174 6976 655f 666c 6f61 742c 0a20  egative_float,. 
+0000e810: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000e820: 2020 2020 2020 2020 2020 2020 2020 2068                 h
+0000e830: 6964 6465 6e5f 6472 6f70 6f75 745f 7261  idden_dropout_ra
+0000e840: 7465 3d56 616c 6964 6174 6f72 2e63 6865  te=Validator.che
+0000e850: 636b 5f6e 6f6e 5f6e 6567 6174 6976 655f  ck_non_negative_
+0000e860: 666c 6f61 742c 0a20 2020 2020 2020 2020  float,.         
+0000e870: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000e880: 2020 2020 2020 2063 6f6d 7075 7465 5f64         compute_d
+0000e890: 7479 7065 3d5f 7661 6c69 645f 7661 6c75  type=_valid_valu
+0000e8a0: 655f 6368 6563 6b73 285b 6d73 7479 7065  e_checks([mstype
+0000e8b0: 2e66 6c6f 6174 3332 2c20 6d73 7479 7065  .float32, mstype
+0000e8c0: 2e66 6c6f 6174 3136 2c20 6d73 7479 7065  .float16, mstype
+0000e8d0: 2e62 666c 6f61 7431 365d 2c0a 2020 2020  .bfloat16],.    
+0000e8e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000e8f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000e900: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000e910: 2020 2020 2020 2020 2020 2020 2020 224d                "M
+0000e920: 756c 7469 4865 6164 4174 7465 6e74 696f  ultiHeadAttentio
+0000e930: 6e22 292c 0a20 2020 2020 2020 2020 2020  n"),.           
+0000e940: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000e950: 2020 2020 2073 6f66 746d 6178 5f63 6f6d       softmax_com
+0000e960: 7075 7465 5f74 7970 653d 5f76 616c 6964  pute_type=_valid
+0000e970: 5f76 616c 7565 5f63 6865 636b 7328 5b6d  _value_checks([m
+0000e980: 7374 7970 652e 666c 6f61 7433 322c 0a20  stype.float32,. 
 0000e990: 2020 2020 2020 2020 2020 2020 2020 2020                  
 0000e9a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000e9b0: 6e75 6d5f 6865 6164 733d 5661 6c69 6461  num_heads=Valida
-0000e9c0: 746f 722e 6368 6563 6b5f 706f 7369 7469  tor.check_positi
-0000e9d0: 7665 5f69 6e74 2c0a 2020 2020 2020 2020  ve_int,.        
-0000e9e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000e9f0: 2020 2020 2020 2020 7372 635f 7365 715f          src_seq_
-0000ea00: 6c65 6e67 7468 3d56 616c 6964 6174 6f72  length=Validator
-0000ea10: 2e63 6865 636b 5f70 6f73 6974 6976 655f  .check_positive_
-0000ea20: 696e 742c 0a20 2020 2020 2020 2020 2020  int,.           
+0000e9b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000e9c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000e9d0: 2020 2020 2020 2020 206d 7374 7970 652e           mstype.
+0000e9e0: 666c 6f61 7431 362c 206d 7374 7970 652e  float16, mstype.
+0000e9f0: 6266 6c6f 6174 3136 5d2c 0a20 2020 2020  bfloat16],.     
+0000ea00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000ea10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000ea20: 2020 2020 2020 2020 2020 2020 2020 2020                  
 0000ea30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000ea40: 2020 2020 2074 6774 5f73 6571 5f6c 656e       tgt_seq_len
-0000ea50: 6774 683d 5661 6c69 6461 746f 722e 6368  gth=Validator.ch
-0000ea60: 6563 6b5f 706f 7369 7469 7665 5f69 6e74  eck_positive_int
-0000ea70: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-0000ea80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000ea90: 2020 6174 7465 6e74 696f 6e5f 6472 6f70    attention_drop
-0000eaa0: 6f75 745f 7261 7465 3d56 616c 6964 6174  out_rate=Validat
-0000eab0: 6f72 2e63 6865 636b 5f6e 6f6e 5f6e 6567  or.check_non_neg
-0000eac0: 6174 6976 655f 666c 6f61 742c 0a20 2020  ative_float,.   
-0000ead0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000eae0: 2020 2020 2020 2020 2020 2020 2068 6964               hid
-0000eaf0: 6465 6e5f 6472 6f70 6f75 745f 7261 7465  den_dropout_rate
-0000eb00: 3d56 616c 6964 6174 6f72 2e63 6865 636b  =Validator.check
-0000eb10: 5f6e 6f6e 5f6e 6567 6174 6976 655f 666c  _non_negative_fl
-0000eb20: 6f61 742c 0a20 2020 2020 2020 2020 2020  oat,.           
+0000ea40: 2020 2020 224d 756c 7469 4865 6164 4174      "MultiHeadAt
+0000ea50: 7465 6e74 696f 6e22 292c 0a20 2020 2020  tention"),.     
+0000ea60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000ea70: 2020 2020 2020 2020 2020 2070 6172 616d             param
+0000ea80: 5f69 6e69 745f 7479 7065 3d5f 7661 6c69  _init_type=_vali
+0000ea90: 645f 7661 6c75 655f 6368 6563 6b73 285b  d_value_checks([
+0000eaa0: 6d73 7479 7065 2e66 6c6f 6174 3332 2c20  mstype.float32, 
+0000eab0: 6d73 7479 7065 2e66 6c6f 6174 3136 2c20  mstype.float16, 
+0000eac0: 6d73 7479 7065 2e62 666c 6f61 7431 365d  mstype.bfloat16]
+0000ead0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+0000eae0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000eaf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000eb00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000eb10: 2020 2020 2020 224d 756c 7469 4865 6164        "MultiHead
+0000eb20: 4174 7465 6e74 696f 6e22 292c 0a20 2020  Attention"),.   
 0000eb30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000eb40: 2020 2020 2063 6f6d 7075 7465 5f64 7479       compute_dty
-0000eb50: 7065 3d5f 7661 6c69 645f 7661 6c75 655f  pe=_valid_value_
-0000eb60: 6368 6563 6b73 285b 6d73 7479 7065 2e66  checks([mstype.f
-0000eb70: 6c6f 6174 3332 2c20 6d73 7479 7065 2e66  loat32, mstype.f
-0000eb80: 6c6f 6174 3136 2c20 6d73 7479 7065 2e62  loat16, mstype.b
-0000eb90: 666c 6f61 7431 365d 2c0a 2020 2020 2020  float16],.      
+0000eb40: 2020 2020 2020 2020 2020 2020 2070 6172               par
+0000eb50: 616c 6c65 6c5f 636f 6e66 6967 3d5f 7661  allel_config=_va
+0000eb60: 6c69 645f 7479 7065 5f63 6865 636b 7328  lid_type_checks(
+0000eb70: 5b4f 7050 6172 616c 6c65 6c43 6f6e 6669  [OpParallelConfi
+0000eb80: 675d 2c0a 2020 2020 2020 2020 2020 2020  g],.            
+0000eb90: 2020 2020 2020 2020 2020 2020 2020 2020                  
 0000eba0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 0000ebb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000ebc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000ebd0: 2020 2020 2020 2020 2020 2020 224d 756c              "Mul
-0000ebe0: 7469 4865 6164 4174 7465 6e74 696f 6e22  tiHeadAttention"
-0000ebf0: 292c 0a20 2020 2020 2020 2020 2020 2020  ),.             
-0000ec00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000ec10: 2020 2073 6f66 746d 6178 5f63 6f6d 7075     softmax_compu
-0000ec20: 7465 5f74 7970 653d 5f76 616c 6964 5f76  te_type=_valid_v
-0000ec30: 616c 7565 5f63 6865 636b 7328 5b6d 7374  alue_checks([mst
-0000ec40: 7970 652e 666c 6f61 7433 322c 0a20 2020  ype.float32,.   
-0000ec50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000ec60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000ebc0: 2020 2020 2020 2022 4d75 6c74 6948 6561         "MultiHea
+0000ebd0: 6441 7474 656e 7469 6f6e 2229 2c0a 2020  dAttention"),.  
+0000ebe0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000ebf0: 2020 2020 2020 2020 2020 2020 2020 7573                us
+0000ec00: 655f 7061 7374 3d56 616c 6964 6174 6f72  e_past=Validator
+0000ec10: 2e63 6865 636b 5f62 6f6f 6c2c 0a20 2020  .check_bool,.   
+0000ec20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000ec30: 2020 2020 2020 2020 2020 2020 2075 7365               use
+0000ec40: 5f66 6c61 7368 5f61 7474 656e 7469 6f6e  _flash_attention
+0000ec50: 3d56 616c 6964 6174 6f72 2e63 6865 636b  =Validator.check
+0000ec60: 5f62 6f6f 6c2c 0a20 2020 2020 2020 2020  _bool,.         
 0000ec70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000ec80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000ec90: 2020 2020 2020 206d 7374 7970 652e 666c         mstype.fl
-0000eca0: 6f61 7431 362c 206d 7374 7970 652e 6266  oat16, mstype.bf
-0000ecb0: 6c6f 6174 3136 5d2c 0a20 2020 2020 2020  loat16],.       
+0000ec80: 2020 2020 2020 2075 7365 5f70 726f 6d70         use_promp
+0000ec90: 745f 666c 6173 685f 6174 7465 6e74 696f  t_flash_attentio
+0000eca0: 6e3d 5661 6c69 6461 746f 722e 6368 6563  n=Validator.chec
+0000ecb0: 6b5f 626f 6f6c 2c0a 2020 2020 2020 2020  k_bool,.        
 0000ecc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000ecd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000ece0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000ecf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000ed00: 2020 224d 756c 7469 4865 6164 4174 7465    "MultiHeadAtte
-0000ed10: 6e74 696f 6e22 292c 0a20 2020 2020 2020  ntion"),.       
-0000ed20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000ed30: 2020 2020 2020 2020 2070 6172 616d 5f69           param_i
-0000ed40: 6e69 745f 7479 7065 3d5f 7661 6c69 645f  nit_type=_valid_
-0000ed50: 7661 6c75 655f 6368 6563 6b73 285b 6d73  value_checks([ms
-0000ed60: 7479 7065 2e66 6c6f 6174 3332 2c20 6d73  type.float32, ms
-0000ed70: 7479 7065 2e66 6c6f 6174 3136 2c20 6d73  type.float16, ms
-0000ed80: 7479 7065 2e62 666c 6f61 7431 365d 2c0a  type.bfloat16],.
-0000ed90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000eda0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000edb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000edc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000edd0: 2020 2020 224d 756c 7469 4865 6164 4174      "MultiHeadAt
-0000ede0: 7465 6e74 696f 6e22 292c 0a20 2020 2020  tention"),.     
-0000edf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000ee00: 2020 2020 2020 2020 2020 2070 6172 616c             paral
-0000ee10: 6c65 6c5f 636f 6e66 6967 3d5f 7661 6c69  lel_config=_vali
-0000ee20: 645f 7479 7065 5f63 6865 636b 7328 5b4f  d_type_checks([O
-0000ee30: 7050 6172 616c 6c65 6c43 6f6e 6669 675d  pParallelConfig]
-0000ee40: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-0000ee50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000ee60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000ee70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000ee80: 2020 2020 2022 4d75 6c74 6948 6561 6441       "MultiHeadA
-0000ee90: 7474 656e 7469 6f6e 2229 2c0a 2020 2020  ttention"),.    
-0000eea0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000eeb0: 2020 2020 2020 2020 2020 2020 7573 655f              use_
-0000eec0: 7061 7374 3d56 616c 6964 6174 6f72 2e63  past=Validator.c
-0000eed0: 6865 636b 5f62 6f6f 6c2c 0a20 2020 2020  heck_bool,.     
-0000eee0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000eef0: 2020 2020 2020 2020 2020 2075 7365 5f66             use_f
-0000ef00: 6c61 7368 5f61 7474 656e 7469 6f6e 3d56  lash_attention=V
-0000ef10: 616c 6964 6174 6f72 2e63 6865 636b 5f62  alidator.check_b
-0000ef20: 6f6f 6c2c 0a20 2020 2020 2020 2020 2020  ool,.           
-0000ef30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000ef40: 2020 2020 2075 7365 5f70 726f 6d70 745f       use_prompt_
-0000ef50: 666c 6173 685f 6174 7465 6e74 696f 6e3d  flash_attention=
-0000ef60: 5661 6c69 6461 746f 722e 6368 6563 6b5f  Validator.check_
-0000ef70: 626f 6f6c 290a 2020 2020 6465 6620 5f5f  bool).    def __
-0000ef80: 696e 6974 5f5f 2873 656c 662c 2062 6174  init__(self, bat
-0000ef90: 6368 5f73 697a 652c 0a20 2020 2020 2020  ch_size,.       
-0000efa0: 2020 2020 2020 2020 2020 7372 635f 7365            src_se
-0000efb0: 715f 6c65 6e67 7468 2c0a 2020 2020 2020  q_length,.      
-0000efc0: 2020 2020 2020 2020 2020 2074 6774 5f73             tgt_s
-0000efd0: 6571 5f6c 656e 6774 682c 0a20 2020 2020  eq_length,.     
-0000efe0: 2020 2020 2020 2020 2020 2020 6869 6464              hidd
-0000eff0: 656e 5f73 697a 652c 0a20 2020 2020 2020  en_size,.       
-0000f000: 2020 2020 2020 2020 2020 6e75 6d5f 6865            num_he
-0000f010: 6164 732c 0a20 2020 2020 2020 2020 2020  ads,.           
-0000f020: 2020 2020 2020 6869 6464 656e 5f64 726f        hidden_dro
-0000f030: 706f 7574 5f72 6174 653d 302e 312c 0a20  pout_rate=0.1,. 
-0000f040: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000f050: 6174 7465 6e74 696f 6e5f 6472 6f70 6f75  attention_dropou
-0000f060: 745f 7261 7465 3d30 2e31 2c0a 2020 2020  t_rate=0.1,.    
-0000f070: 2020 2020 2020 2020 2020 2020 2063 6f6d               com
-0000f080: 7075 7465 5f64 7479 7065 3d6d 7374 7970  pute_dtype=mstyp
-0000f090: 652e 666c 6f61 7431 362c 0a20 2020 2020  e.float16,.     
-0000f0a0: 2020 2020 2020 2020 2020 2020 736f 6674              soft
-0000f0b0: 6d61 785f 636f 6d70 7574 655f 7479 7065  max_compute_type
-0000f0c0: 3d6d 7374 7970 652e 666c 6f61 7433 322c  =mstype.float32,
-0000f0d0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0000f0e0: 2020 7061 7261 6d5f 696e 6974 5f74 7970    param_init_typ
-0000f0f0: 653d 6d73 7479 7065 2e66 6c6f 6174 3332  e=mstype.float32
-0000f100: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-0000f110: 2020 2075 7365 5f70 6173 743d 4661 6c73     use_past=Fals
-0000f120: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
-0000f130: 2020 2020 7061 7261 6c6c 656c 5f63 6f6e      parallel_con
-0000f140: 6669 673d 6465 6661 756c 745f 6470 6d70  fig=default_dpmp
-0000f150: 5f63 6f6e 6669 672c 0a20 2020 2020 2020  _config,.       
-0000f160: 2020 2020 2020 2020 2020 7573 655f 666c            use_fl
-0000f170: 6173 685f 6174 7465 6e74 696f 6e3d 4661  ash_attention=Fa
-0000f180: 6c73 652c 0a20 2020 2020 2020 2020 2020  lse,.           
-0000f190: 2020 2020 2020 7573 655f 7072 6f6d 7074        use_prompt
-0000f1a0: 5f66 6c61 7368 5f61 7474 656e 7469 6f6e  _flash_attention
-0000f1b0: 3d46 616c 7365 293a 0a20 2020 2020 2020  =False):.       
-0000f1c0: 2073 7570 6572 284d 756c 7469 4865 6164   super(MultiHead
-0000f1d0: 4174 7465 6e74 696f 6e2c 2073 656c 6629  Attention, self)
-0000f1e0: 2e5f 5f69 6e69 745f 5f28 290a 2020 2020  .__init__().    
-0000f1f0: 2020 2020 7365 6c66 2e5f 6973 5f61 7363      self._is_asc
-0000f200: 656e 6420 3d20 636f 6e74 6578 742e 6765  end = context.ge
-0000f210: 745f 636f 6e74 6578 7428 2764 6576 6963  t_context('devic
-0000f220: 655f 7461 7267 6574 2729 2069 6e20 5b22  e_target') in ["
-0000f230: 4173 6365 6e64 225d 0a20 2020 2020 2020  Ascend"].       
-0000f240: 2073 656c 662e 6470 203d 2070 6172 616c   self.dp = paral
-0000f250: 6c65 6c5f 636f 6e66 6967 2e64 6174 615f  lel_config.data_
-0000f260: 7061 7261 6c6c 656c 0a20 2020 2020 2020  parallel.       
-0000f270: 2073 656c 662e 6973 5f70 6172 616c 6c65   self.is_paralle
-0000f280: 6c5f 6d6f 6465 203d 205f 6765 745f 7061  l_mode = _get_pa
-0000f290: 7261 6c6c 656c 5f6d 6f64 6528 2920 696e  rallel_mode() in
-0000f2a0: 2028 0a20 2020 2020 2020 2020 2020 2050   (.            P
-0000f2b0: 6172 616c 6c65 6c4d 6f64 652e 5345 4d49  arallelMode.SEMI
-0000f2c0: 5f41 5554 4f5f 5041 5241 4c4c 454c 2c20  _AUTO_PARALLEL, 
-0000f2d0: 5061 7261 6c6c 656c 4d6f 6465 2e41 5554  ParallelMode.AUT
-0000f2e0: 4f5f 5041 5241 4c4c 454c 290a 2020 2020  O_PARALLEL).    
-0000f2f0: 2020 2020 6966 2062 6174 6368 5f73 697a      if batch_siz
-0000f300: 653a 0a20 2020 2020 2020 2020 2020 2056  e:.            V
-0000f310: 616c 6964 6174 6f72 2e63 6865 636b 5f70  alidator.check_p
-0000f320: 6f73 6974 6976 655f 696e 7428 6261 7463  ositive_int(batc
-0000f330: 685f 7369 7a65 290a 2020 2020 2020 2020  h_size).        
-0000f340: 6966 205f 6765 745f 7061 7261 6c6c 656c  if _get_parallel
-0000f350: 5f6d 6f64 6528 2920 696e 2028 5061 7261  _mode() in (Para
-0000f360: 6c6c 656c 4d6f 6465 2e41 5554 4f5f 5041  llelMode.AUTO_PA
-0000f370: 5241 4c4c 454c 2c29 3a0a 2020 2020 2020  RALLEL,):.      
-0000f380: 2020 2020 2020 5f63 6865 636b 5f63 6f6e        _check_con
-0000f390: 6669 6728 7061 7261 6c6c 656c 5f63 6f6e  fig(parallel_con
-0000f3a0: 6669 6729 0a20 2020 2020 2020 2020 2020  fig).           
-0000f3b0: 2073 656c 662e 7372 635f 7365 715f 6c65   self.src_seq_le
-0000f3c0: 6e67 7468 203d 2073 7263 5f73 6571 5f6c  ngth = src_seq_l
-0000f3d0: 656e 6774 680a 2020 2020 2020 2020 2020  ength.          
-0000f3e0: 2020 7365 6c66 2e74 6774 5f73 6571 5f6c    self.tgt_seq_l
-0000f3f0: 656e 6774 6820 3d20 7467 745f 7365 715f  ength = tgt_seq_
-0000f400: 6c65 6e67 7468 0a20 2020 2020 2020 2020  length.         
-0000f410: 2020 2073 656c 662e 6869 6464 656e 5f73     self.hidden_s
-0000f420: 697a 6520 3d20 6869 6464 656e 5f73 697a  ize = hidden_siz
-0000f430: 650a 2020 2020 2020 2020 2020 2020 7365  e.            se
-0000f440: 6c66 2e62 6174 6368 5f73 697a 6520 3d20  lf.batch_size = 
-0000f450: 6261 7463 685f 7369 7a65 0a20 2020 2020  batch_size.     
-0000f460: 2020 2020 2020 2069 6620 6869 6464 656e         if hidden
-0000f470: 5f64 726f 706f 7574 5f72 6174 6520 3c20  _dropout_rate < 
-0000f480: 3020 6f72 2068 6964 6465 6e5f 6472 6f70  0 or hidden_drop
-0000f490: 6f75 745f 7261 7465 203e 3d20 313a 0a20  out_rate >= 1:. 
-0000f4a0: 2020 2020 2020 2020 2020 2020 2020 2072                 r
-0000f4b0: 6169 7365 2056 616c 7565 4572 726f 7228  aise ValueError(
-0000f4c0: 2246 6f72 2027 4d75 6c74 6948 6561 6441  "For 'MultiHeadA
-0000f4d0: 7474 656e 7469 6f6e 272c 2074 6865 2063  ttention', the c
-0000f4e0: 6c61 7373 2076 6172 6961 626c 6520 2768  lass variable 'h
-0000f4f0: 6964 6465 6e5f 6472 6f70 6f75 745f 7261  idden_dropout_ra
-0000f500: 7465 2720 6d75 7374 2062 6520 220a 2020  te' must be ".  
+0000ecd0: 2020 2020 2020 2020 7573 655f 696e 6372          use_incr
+0000ece0: 655f 666c 6173 685f 6174 7465 6e74 696f  e_flash_attentio
+0000ecf0: 6e3d 5661 6c69 6461 746f 722e 6368 6563  n=Validator.chec
+0000ed00: 6b5f 626f 6f6c 290a 2020 2020 6465 6620  k_bool).    def 
+0000ed10: 5f5f 696e 6974 5f5f 2873 656c 662c 2062  __init__(self, b
+0000ed20: 6174 6368 5f73 697a 652c 0a20 2020 2020  atch_size,.     
+0000ed30: 2020 2020 2020 2020 2020 2020 7372 635f              src_
+0000ed40: 7365 715f 6c65 6e67 7468 2c0a 2020 2020  seq_length,.    
+0000ed50: 2020 2020 2020 2020 2020 2020 2074 6774               tgt
+0000ed60: 5f73 6571 5f6c 656e 6774 682c 0a20 2020  _seq_length,.   
+0000ed70: 2020 2020 2020 2020 2020 2020 2020 6869                hi
+0000ed80: 6464 656e 5f73 697a 652c 0a20 2020 2020  dden_size,.     
+0000ed90: 2020 2020 2020 2020 2020 2020 6e75 6d5f              num_
+0000eda0: 6865 6164 732c 0a20 2020 2020 2020 2020  heads,.         
+0000edb0: 2020 2020 2020 2020 6869 6464 656e 5f64          hidden_d
+0000edc0: 726f 706f 7574 5f72 6174 653d 302e 312c  ropout_rate=0.1,
+0000edd0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0000ede0: 2020 6174 7465 6e74 696f 6e5f 6472 6f70    attention_drop
+0000edf0: 6f75 745f 7261 7465 3d30 2e31 2c0a 2020  out_rate=0.1,.  
+0000ee00: 2020 2020 2020 2020 2020 2020 2020 2063                 c
+0000ee10: 6f6d 7075 7465 5f64 7479 7065 3d6d 7374  ompute_dtype=mst
+0000ee20: 7970 652e 666c 6f61 7431 362c 0a20 2020  ype.float16,.   
+0000ee30: 2020 2020 2020 2020 2020 2020 2020 736f                so
+0000ee40: 6674 6d61 785f 636f 6d70 7574 655f 7479  ftmax_compute_ty
+0000ee50: 7065 3d6d 7374 7970 652e 666c 6f61 7433  pe=mstype.float3
+0000ee60: 322c 0a20 2020 2020 2020 2020 2020 2020  2,.             
+0000ee70: 2020 2020 7061 7261 6d5f 696e 6974 5f74      param_init_t
+0000ee80: 7970 653d 6d73 7479 7065 2e66 6c6f 6174  ype=mstype.float
+0000ee90: 3332 2c0a 2020 2020 2020 2020 2020 2020  32,.            
+0000eea0: 2020 2020 2075 7365 5f70 6173 743d 4661       use_past=Fa
+0000eeb0: 6c73 652c 0a20 2020 2020 2020 2020 2020  lse,.           
+0000eec0: 2020 2020 2020 7061 7261 6c6c 656c 5f63        parallel_c
+0000eed0: 6f6e 6669 673d 6465 6661 756c 745f 6470  onfig=default_dp
+0000eee0: 6d70 5f63 6f6e 6669 672c 0a20 2020 2020  mp_config,.     
+0000eef0: 2020 2020 2020 2020 2020 2020 7573 655f              use_
+0000ef00: 666c 6173 685f 6174 7465 6e74 696f 6e3d  flash_attention=
+0000ef10: 4661 6c73 652c 0a20 2020 2020 2020 2020  False,.         
+0000ef20: 2020 2020 2020 2020 7573 655f 7072 6f6d          use_prom
+0000ef30: 7074 5f66 6c61 7368 5f61 7474 656e 7469  pt_flash_attenti
+0000ef40: 6f6e 3d46 616c 7365 2c0a 2020 2020 2020  on=False,.      
+0000ef50: 2020 2020 2020 2020 2020 2075 7365 5f69             use_i
+0000ef60: 6e63 7265 5f66 6c61 7368 5f61 7474 656e  ncre_flash_atten
+0000ef70: 7469 6f6e 3d46 616c 7365 293a 0a20 2020  tion=False):.   
+0000ef80: 2020 2020 2073 7570 6572 284d 756c 7469       super(Multi
+0000ef90: 4865 6164 4174 7465 6e74 696f 6e2c 2073  HeadAttention, s
+0000efa0: 656c 6629 2e5f 5f69 6e69 745f 5f28 290a  elf).__init__().
+0000efb0: 2020 2020 2020 2020 7365 6c66 2e5f 6973          self._is
+0000efc0: 5f61 7363 656e 6420 3d20 636f 6e74 6578  _ascend = contex
+0000efd0: 742e 6765 745f 636f 6e74 6578 7428 2764  t.get_context('d
+0000efe0: 6576 6963 655f 7461 7267 6574 2729 2069  evice_target') i
+0000eff0: 6e20 5b22 4173 6365 6e64 225d 0a20 2020  n ["Ascend"].   
+0000f000: 2020 2020 2073 656c 662e 6470 203d 2070       self.dp = p
+0000f010: 6172 616c 6c65 6c5f 636f 6e66 6967 2e64  arallel_config.d
+0000f020: 6174 615f 7061 7261 6c6c 656c 0a20 2020  ata_parallel.   
+0000f030: 2020 2020 2073 656c 662e 6973 5f70 6172       self.is_par
+0000f040: 616c 6c65 6c5f 6d6f 6465 203d 205f 6765  allel_mode = _ge
+0000f050: 745f 7061 7261 6c6c 656c 5f6d 6f64 6528  t_parallel_mode(
+0000f060: 2920 696e 2028 0a20 2020 2020 2020 2020  ) in (.         
+0000f070: 2020 2050 6172 616c 6c65 6c4d 6f64 652e     ParallelMode.
+0000f080: 5345 4d49 5f41 5554 4f5f 5041 5241 4c4c  SEMI_AUTO_PARALL
+0000f090: 454c 2c20 5061 7261 6c6c 656c 4d6f 6465  EL, ParallelMode
+0000f0a0: 2e41 5554 4f5f 5041 5241 4c4c 454c 290a  .AUTO_PARALLEL).
+0000f0b0: 2020 2020 2020 2020 6966 2062 6174 6368          if batch
+0000f0c0: 5f73 697a 653a 0a20 2020 2020 2020 2020  _size:.         
+0000f0d0: 2020 2056 616c 6964 6174 6f72 2e63 6865     Validator.che
+0000f0e0: 636b 5f70 6f73 6974 6976 655f 696e 7428  ck_positive_int(
+0000f0f0: 6261 7463 685f 7369 7a65 290a 2020 2020  batch_size).    
+0000f100: 2020 2020 6966 205f 6765 745f 7061 7261      if _get_para
+0000f110: 6c6c 656c 5f6d 6f64 6528 2920 696e 2028  llel_mode() in (
+0000f120: 5061 7261 6c6c 656c 4d6f 6465 2e41 5554  ParallelMode.AUT
+0000f130: 4f5f 5041 5241 4c4c 454c 2c29 3a0a 2020  O_PARALLEL,):.  
+0000f140: 2020 2020 2020 2020 2020 5f63 6865 636b            _check
+0000f150: 5f63 6f6e 6669 6728 7061 7261 6c6c 656c  _config(parallel
+0000f160: 5f63 6f6e 6669 6729 0a20 2020 2020 2020  _config).       
+0000f170: 2020 2020 2073 656c 662e 7372 635f 7365       self.src_se
+0000f180: 715f 6c65 6e67 7468 203d 2073 7263 5f73  q_length = src_s
+0000f190: 6571 5f6c 656e 6774 680a 2020 2020 2020  eq_length.      
+0000f1a0: 2020 2020 2020 7365 6c66 2e74 6774 5f73        self.tgt_s
+0000f1b0: 6571 5f6c 656e 6774 6820 3d20 7467 745f  eq_length = tgt_
+0000f1c0: 7365 715f 6c65 6e67 7468 0a20 2020 2020  seq_length.     
+0000f1d0: 2020 2020 2020 2073 656c 662e 6869 6464         self.hidd
+0000f1e0: 656e 5f73 697a 6520 3d20 6869 6464 656e  en_size = hidden
+0000f1f0: 5f73 697a 650a 2020 2020 2020 2020 2020  _size.          
+0000f200: 2020 7365 6c66 2e62 6174 6368 5f73 697a    self.batch_siz
+0000f210: 6520 3d20 6261 7463 685f 7369 7a65 0a20  e = batch_size. 
+0000f220: 2020 2020 2020 2020 2020 2069 6620 6869             if hi
+0000f230: 6464 656e 5f64 726f 706f 7574 5f72 6174  dden_dropout_rat
+0000f240: 6520 3c20 3020 6f72 2068 6964 6465 6e5f  e < 0 or hidden_
+0000f250: 6472 6f70 6f75 745f 7261 7465 203e 3d20  dropout_rate >= 
+0000f260: 313a 0a20 2020 2020 2020 2020 2020 2020  1:.             
+0000f270: 2020 2072 6169 7365 2056 616c 7565 4572     raise ValueEr
+0000f280: 726f 7228 2246 6f72 2027 4d75 6c74 6948  ror("For 'MultiH
+0000f290: 6561 6441 7474 656e 7469 6f6e 272c 2074  eadAttention', t
+0000f2a0: 6865 2063 6c61 7373 2076 6172 6961 626c  he class variabl
+0000f2b0: 6520 2768 6964 6465 6e5f 6472 6f70 6f75  e 'hidden_dropou
+0000f2c0: 745f 7261 7465 2720 6d75 7374 2062 6520  t_rate' must be 
+0000f2d0: 220a 2020 2020 2020 2020 2020 2020 2020  ".              
+0000f2e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000f2f0: 2020 2022 696e 2072 616e 6765 205b 302c     "in range [0,
+0000f300: 2031 2e30 292c 2062 7574 2067 6f74 2074   1.0), but got t
+0000f310: 6865 2076 616c 7565 203a 207b 7d2e 222e  he value : {}.".
+0000f320: 666f 726d 6174 2868 6964 6465 6e5f 6472  format(hidden_dr
+0000f330: 6f70 6f75 745f 7261 7465 2929 0a20 2020  opout_rate)).   
+0000f340: 2020 2020 2020 2020 2069 6620 6174 7465           if atte
+0000f350: 6e74 696f 6e5f 6472 6f70 6f75 745f 7261  ntion_dropout_ra
+0000f360: 7465 203c 2030 206f 7220 6174 7465 6e74  te < 0 or attent
+0000f370: 696f 6e5f 6472 6f70 6f75 745f 7261 7465  ion_dropout_rate
+0000f380: 203e 3d20 313a 0a20 2020 2020 2020 2020   >= 1:.         
+0000f390: 2020 2020 2020 2072 6169 7365 2056 616c         raise Val
+0000f3a0: 7565 4572 726f 7228 2246 6f72 2027 4d75  ueError("For 'Mu
+0000f3b0: 6c74 6948 6561 6441 7474 656e 7469 6f6e  ltiHeadAttention
+0000f3c0: 272c 2074 6865 2063 6c61 7373 2076 6172  ', the class var
+0000f3d0: 6961 626c 6520 2761 7474 656e 7469 6f6e  iable 'attention
+0000f3e0: 5f64 726f 706f 7574 5f72 6174 6527 206d  _dropout_rate' m
+0000f3f0: 7573 7420 6265 2022 0a20 2020 2020 2020  ust be ".       
+0000f400: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000f410: 2020 2020 2020 2020 2020 2269 6e20 7261            "in ra
+0000f420: 6e67 6520 5b30 2c20 312e 3029 2c20 6275  nge [0, 1.0), bu
+0000f430: 7420 676f 7420 7468 6520 7661 6c75 6520  t got the value 
+0000f440: 3a20 7b7d 2e22 2e66 6f72 6d61 7428 6174  : {}.".format(at
+0000f450: 7465 6e74 696f 6e5f 6472 6f70 6f75 745f  tention_dropout_
+0000f460: 7261 7465 2929 0a20 2020 2020 2020 2020  rate)).         
+0000f470: 2020 2069 6620 6869 6464 656e 5f73 697a     if hidden_siz
+0000f480: 6520 2520 6e75 6d5f 6865 6164 7320 213d  e % num_heads !=
+0000f490: 2030 3a0a 2020 2020 2020 2020 2020 2020   0:.            
+0000f4a0: 2020 2020 7261 6973 6520 5661 6c75 6545      raise ValueE
+0000f4b0: 7272 6f72 2822 466f 7220 274d 756c 7469  rror("For 'Multi
+0000f4c0: 4865 6164 4174 7465 6e74 696f 6e27 2c20  HeadAttention', 
+0000f4d0: 7468 6520 636c 6173 7320 7661 7269 6162  the class variab
+0000f4e0: 6c65 2027 6869 6464 656e 5f73 697a 6527  le 'hidden_size'
+0000f4f0: 206d 7573 7420 6265 2061 206d 756c 7469   must be a multi
+0000f500: 706c 6520 220a 2020 2020 2020 2020 2020  ple ".          
 0000f510: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000f520: 2020 2020 2020 2020 2020 2020 2020 2022                 "
-0000f530: 696e 2072 616e 6765 205b 302c 2031 2e30  in range [0, 1.0
-0000f540: 292c 2062 7574 2067 6f74 2074 6865 2076  ), but got the v
-0000f550: 616c 7565 203a 207b 7d2e 222e 666f 726d  alue : {}.".form
-0000f560: 6174 2868 6964 6465 6e5f 6472 6f70 6f75  at(hidden_dropou
-0000f570: 745f 7261 7465 2929 0a20 2020 2020 2020  t_rate)).       
-0000f580: 2020 2020 2069 6620 6174 7465 6e74 696f       if attentio
-0000f590: 6e5f 6472 6f70 6f75 745f 7261 7465 203c  n_dropout_rate <
-0000f5a0: 2030 206f 7220 6174 7465 6e74 696f 6e5f   0 or attention_
-0000f5b0: 6472 6f70 6f75 745f 7261 7465 203e 3d20  dropout_rate >= 
-0000f5c0: 313a 0a20 2020 2020 2020 2020 2020 2020  1:.             
-0000f5d0: 2020 2072 6169 7365 2056 616c 7565 4572     raise ValueEr
-0000f5e0: 726f 7228 2246 6f72 2027 4d75 6c74 6948  ror("For 'MultiH
-0000f5f0: 6561 6441 7474 656e 7469 6f6e 272c 2074  eadAttention', t
-0000f600: 6865 2063 6c61 7373 2076 6172 6961 626c  he class variabl
-0000f610: 6520 2761 7474 656e 7469 6f6e 5f64 726f  e 'attention_dro
-0000f620: 706f 7574 5f72 6174 6527 206d 7573 7420  pout_rate' must 
-0000f630: 6265 2022 0a20 2020 2020 2020 2020 2020  be ".           
-0000f640: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000f650: 2020 2020 2020 2269 6e20 7261 6e67 6520        "in range 
-0000f660: 5b30 2c20 312e 3029 2c20 6275 7420 676f  [0, 1.0), but go
-0000f670: 7420 7468 6520 7661 6c75 6520 3a20 7b7d  t the value : {}
-0000f680: 2e22 2e66 6f72 6d61 7428 6174 7465 6e74  .".format(attent
-0000f690: 696f 6e5f 6472 6f70 6f75 745f 7261 7465  ion_dropout_rate
-0000f6a0: 2929 0a20 2020 2020 2020 2020 2020 2069  )).            i
-0000f6b0: 6620 6869 6464 656e 5f73 697a 6520 2520  f hidden_size % 
-0000f6c0: 6e75 6d5f 6865 6164 7320 213d 2030 3a0a  num_heads != 0:.
+0000f520: 2020 2020 2020 2022 6f66 2027 6e75 6d5f         "of 'num_
+0000f530: 6865 6164 7327 2c20 6275 7420 676f 7420  heads', but got 
+0000f540: 7468 6520 6869 6464 656e 5f73 697a 6520  the hidden_size 
+0000f550: 6973 207b 7d20 616e 6420 7468 6520 6e75  is {} and the nu
+0000f560: 6d5f 6865 6164 7320 6973 207b 7d2e 220a  m_heads is {}.".
+0000f570: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000f580: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000f590: 202e 666f 726d 6174 2868 6964 6465 6e5f   .format(hidden_
+0000f5a0: 7369 7a65 2c20 6e75 6d5f 6865 6164 7329  size, num_heads)
+0000f5b0: 290a 2020 2020 2020 2020 2020 2020 6966  ).            if
+0000f5c0: 206e 756d 5f68 6561 6473 2025 2070 6172   num_heads % par
+0000f5d0: 616c 6c65 6c5f 636f 6e66 6967 2e6d 6f64  allel_config.mod
+0000f5e0: 656c 5f70 6172 616c 6c65 6c20 213d 2030  el_parallel != 0
+0000f5f0: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
+0000f600: 2020 7261 6973 6520 5661 6c75 6545 7272    raise ValueErr
+0000f610: 6f72 2822 466f 7220 274d 756c 7469 4865  or("For 'MultiHe
+0000f620: 6164 4174 7465 6e74 696f 6e27 2c20 7468  adAttention', th
+0000f630: 6520 636c 6173 7320 7661 7269 6162 6c65  e class variable
+0000f640: 2027 6e75 6d5f 6865 6164 7327 206d 7573   'num_heads' mus
+0000f650: 7420 6265 2061 206d 756c 7469 706c 6520  t be a multiple 
+0000f660: 6f66 2022 0a20 2020 2020 2020 2020 2020  of ".           
+0000f670: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000f680: 2020 2020 2020 2227 7061 7261 6c6c 656c        "'parallel
+0000f690: 5f63 6f6e 6669 672e 6d6f 6465 6c5f 7061  _config.model_pa
+0000f6a0: 7261 6c6c 656c 272c 2062 7574 2067 6f74  rallel', but got
+0000f6b0: 2074 6865 206e 756d 5f68 6561 6473 2069   the num_heads i
+0000f6c0: 7320 7b7d 2022 0a20 2020 2020 2020 2020  s {} ".         
 0000f6d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000f6e0: 7261 6973 6520 5661 6c75 6545 7272 6f72  raise ValueError
-0000f6f0: 2822 466f 7220 274d 756c 7469 4865 6164  ("For 'MultiHead
-0000f700: 4174 7465 6e74 696f 6e27 2c20 7468 6520  Attention', the 
-0000f710: 636c 6173 7320 7661 7269 6162 6c65 2027  class variable '
-0000f720: 6869 6464 656e 5f73 697a 6527 206d 7573  hidden_size' mus
-0000f730: 7420 6265 2061 206d 756c 7469 706c 6520  t be a multiple 
-0000f740: 220a 2020 2020 2020 2020 2020 2020 2020  ".              
-0000f750: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000f760: 2020 2022 6f66 2027 6e75 6d5f 6865 6164     "of 'num_head
-0000f770: 7327 2c20 6275 7420 676f 7420 7468 6520  s', but got the 
-0000f780: 6869 6464 656e 5f73 697a 6520 6973 207b  hidden_size is {
-0000f790: 7d20 616e 6420 7468 6520 6e75 6d5f 6865  } and the num_he
-0000f7a0: 6164 7320 6973 207b 7d2e 220a 2020 2020  ads is {}.".    
-0000f7b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000f7c0: 2020 2020 2020 2020 2020 2020 202e 666f               .fo
-0000f7d0: 726d 6174 2868 6964 6465 6e5f 7369 7a65  rmat(hidden_size
-0000f7e0: 2c20 6e75 6d5f 6865 6164 7329 290a 2020  , num_heads)).  
-0000f7f0: 2020 2020 2020 2020 2020 6966 206e 756d            if num
-0000f800: 5f68 6561 6473 2025 2070 6172 616c 6c65  _heads % paralle
-0000f810: 6c5f 636f 6e66 6967 2e6d 6f64 656c 5f70  l_config.model_p
-0000f820: 6172 616c 6c65 6c20 213d 2030 3a0a 2020  arallel != 0:.  
-0000f830: 2020 2020 2020 2020 2020 2020 2020 7261                ra
-0000f840: 6973 6520 5661 6c75 6545 7272 6f72 2822  ise ValueError("
-0000f850: 466f 7220 274d 756c 7469 4865 6164 4174  For 'MultiHeadAt
-0000f860: 7465 6e74 696f 6e27 2c20 7468 6520 636c  tention', the cl
-0000f870: 6173 7320 7661 7269 6162 6c65 2027 6e75  ass variable 'nu
-0000f880: 6d5f 6865 6164 7327 206d 7573 7420 6265  m_heads' must be
-0000f890: 2061 206d 756c 7469 706c 6520 6f66 2022   a multiple of "
-0000f8a0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0000f6e0: 2020 2020 2020 2020 2261 6e64 2074 6865          "and the
+0000f6f0: 2070 6172 616c 6c65 6c5f 636f 6e66 6967   parallel_config
+0000f700: 2e6d 6f64 656c 5f70 6172 616c 6c65 6c20  .model_parallel 
+0000f710: 2069 7320 7b7d 2e22 0a20 2020 2020 2020   is {}.".       
+0000f720: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000f730: 2020 2020 2020 2020 2020 2e66 6f72 6d61            .forma
+0000f740: 7428 6e75 6d5f 6865 6164 732c 2070 6172  t(num_heads, par
+0000f750: 616c 6c65 6c5f 636f 6e66 6967 2e6d 6f64  allel_config.mod
+0000f760: 656c 5f70 6172 616c 6c65 6c29 290a 2020  el_parallel)).  
+0000f770: 2020 2020 2020 2020 2020 7365 6c66 2e69            self.i
+0000f780: 735f 6669 7273 745f 6974 6572 6174 696f  s_first_iteratio
+0000f790: 6e20 3d20 5472 7565 0a20 2020 2020 2020  n = True.       
+0000f7a0: 2020 2020 2023 204f 7574 7075 7420 6c61       # Output la
+0000f7b0: 7965 720a 2020 2020 2020 2020 2020 2020  yer.            
+0000f7c0: 7365 6c66 2e70 726f 6a65 6374 696f 6e20  self.projection 
+0000f7d0: 3d20 4c69 6e65 6172 2869 6e5f 6368 616e  = Linear(in_chan
+0000f7e0: 6e65 6c73 3d68 6964 6465 6e5f 7369 7a65  nels=hidden_size
+0000f7f0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+0000f800: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000f810: 2020 2020 2020 206f 7574 5f63 6861 6e6e         out_chann
+0000f820: 656c 733d 6869 6464 656e 5f73 697a 652c  els=hidden_size,
+0000f830: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0000f840: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000f850: 2020 2020 2020 7472 616e 7370 6f73 655f        transpose_
+0000f860: 623d 4661 6c73 652c 0a20 2020 2020 2020  b=False,.       
+0000f870: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000f880: 2020 2020 2020 2020 2020 2020 2020 636f                co
+0000f890: 6d70 7574 655f 6474 7970 653d 636f 6d70  mpute_dtype=comp
+0000f8a0: 7574 655f 6474 7970 652c 0a20 2020 2020  ute_dtype,.     
 0000f8b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000f8c0: 2020 2227 7061 7261 6c6c 656c 5f63 6f6e    "'parallel_con
-0000f8d0: 6669 672e 6d6f 6465 6c5f 7061 7261 6c6c  fig.model_parall
-0000f8e0: 656c 272c 2062 7574 2067 6f74 2074 6865  el', but got the
-0000f8f0: 206e 756d 5f68 6561 6473 2069 7320 7b7d   num_heads is {}
-0000f900: 2022 0a20 2020 2020 2020 2020 2020 2020   ".             
-0000f910: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000f920: 2020 2020 2261 6e64 2074 6865 2070 6172      "and the par
-0000f930: 616c 6c65 6c5f 636f 6e66 6967 2e6d 6f64  allel_config.mod
-0000f940: 656c 5f70 6172 616c 6c65 6c20 2069 7320  el_parallel  is 
-0000f950: 7b7d 2e22 0a20 2020 2020 2020 2020 2020  {}.".           
-0000f960: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000f970: 2020 2020 2020 2e66 6f72 6d61 7428 6e75        .format(nu
-0000f980: 6d5f 6865 6164 732c 2070 6172 616c 6c65  m_heads, paralle
-0000f990: 6c5f 636f 6e66 6967 2e6d 6f64 656c 5f70  l_config.model_p
-0000f9a0: 6172 616c 6c65 6c29 290a 2020 2020 2020  arallel)).      
-0000f9b0: 2020 2020 2020 7365 6c66 2e69 735f 6669        self.is_fi
-0000f9c0: 7273 745f 6974 6572 6174 696f 6e20 3d20  rst_iteration = 
-0000f9d0: 5472 7565 0a20 2020 2020 2020 2020 2020  True.           
-0000f9e0: 2023 204f 7574 7075 7420 6c61 7965 720a   # Output layer.
-0000f9f0: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-0000fa00: 2e70 726f 6a65 6374 696f 6e20 3d20 4c69  .projection = Li
-0000fa10: 6e65 6172 2869 6e5f 6368 616e 6e65 6c73  near(in_channels
-0000fa20: 3d68 6964 6465 6e5f 7369 7a65 2c0a 2020  =hidden_size,.  
-0000fa30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000fa40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000fa50: 2020 206f 7574 5f63 6861 6e6e 656c 733d     out_channels=
-0000fa60: 6869 6464 656e 5f73 697a 652c 0a20 2020  hidden_size,.   
-0000fa70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000fa80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000fa90: 2020 7472 616e 7370 6f73 655f 623d 4661    transpose_b=Fa
-0000faa0: 6c73 652c 0a20 2020 2020 2020 2020 2020  lse,.           
-0000fab0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000fac0: 2020 2020 2020 2020 2020 636f 6d70 7574            comput
-0000fad0: 655f 6474 7970 653d 636f 6d70 7574 655f  e_dtype=compute_
-0000fae0: 6474 7970 652c 0a20 2020 2020 2020 2020  dtype,.         
-0000faf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000fb00: 2020 2020 2020 2020 2020 2020 7061 7261              para
-0000fb10: 6d5f 696e 6974 5f74 7970 653d 7061 7261  m_init_type=para
-0000fb20: 6d5f 696e 6974 5f74 7970 6529 0a20 2020  m_init_type).   
-0000fb30: 2020 2020 2020 2020 2073 656c 662e 7072           self.pr
-0000fb40: 6f6a 6563 7469 6f6e 2e73 6861 7264 2873  ojection.shard(s
-0000fb50: 7472 6174 6567 795f 6269 6173 3d28 2870  trategy_bias=((p
-0000fb60: 6172 616c 6c65 6c5f 636f 6e66 6967 2e64  arallel_config.d
-0000fb70: 6174 615f 7061 7261 6c6c 656c 2c20 3129  ata_parallel, 1)
-0000fb80: 2c20 2831 2c29 292c 0a20 2020 2020 2020  , (1,)),.       
-0000fb90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000fba0: 2020 2020 2020 2020 2020 2073 7472 6174             strat
-0000fbb0: 6567 795f 6d61 746d 756c 3d28 2870 6172  egy_matmul=((par
-0000fbc0: 616c 6c65 6c5f 636f 6e66 6967 2e64 6174  allel_config.dat
-0000fbd0: 615f 7061 7261 6c6c 656c 2c20 7061 7261  a_parallel, para
-0000fbe0: 6c6c 656c 5f63 6f6e 6669 672e 6d6f 6465  llel_config.mode
-0000fbf0: 6c5f 7061 7261 6c6c 656c 292c 0a20 2020  l_parallel),.   
-0000fc00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000fc10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000fc20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000fc30: 2870 6172 616c 6c65 6c5f 636f 6e66 6967  (parallel_config
-0000fc40: 2e6d 6f64 656c 5f70 6172 616c 6c65 6c2c  .model_parallel,
-0000fc50: 2031 2929 290a 2020 2020 2020 2020 2020   1))).          
-0000fc60: 2020 7365 6c66 2e70 726f 6a65 6374 696f    self.projectio
-0000fc70: 6e2e 6269 6173 2e70 6172 616c 6c65 6c5f  n.bias.parallel_
-0000fc80: 6f70 7469 6d69 7a65 7220 3d20 4661 6c73  optimizer = Fals
-0000fc90: 650a 2020 2020 2020 2020 2020 2020 7365  e.            se
-0000fca0: 6c66 2e74 7261 6e73 706f 7365 203d 2050  lf.transpose = P
-0000fcb0: 2e54 7261 6e73 706f 7365 2829 0a20 2020  .Transpose().   
-0000fcc0: 2020 2020 2020 2020 2073 656c 662e 6d65           self.me
-0000fcd0: 7267 6572 5f68 6561 645f 7472 616e 7370  rger_head_transp
-0000fce0: 6f73 6520 3d20 502e 5472 616e 7370 6f73  ose = P.Transpos
-0000fcf0: 6528 290a 2020 2020 2020 2020 2020 2020  e().            
-0000fd00: 7365 6c66 2e72 6573 6861 7065 203d 2050  self.reshape = P
-0000fd10: 2e52 6573 6861 7065 2829 0a20 2020 2020  .Reshape().     
-0000fd20: 2020 2020 2020 2073 656c 662e 6e5f 6865         self.n_he
-0000fd30: 6164 203d 206e 756d 5f68 6561 6473 0a20  ad = num_heads. 
-0000fd40: 2020 2020 2020 2020 2020 2023 2065 6d62             # emb
-0000fd50: 6564 6469 6e67 2073 697a 6520 7065 7220  edding size per 
-0000fd60: 6865 6164 0a20 2020 2020 2020 2020 2020  head.           
-0000fd70: 2073 656c 662e 7369 7a65 5f70 6572 5f68   self.size_per_h
-0000fd80: 6561 6420 3d20 6869 6464 656e 5f73 697a  ead = hidden_siz
-0000fd90: 6520 2f2f 2073 656c 662e 6e5f 6865 6164  e // self.n_head
-0000fda0: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-0000fdb0: 662e 636f 6e63 6174 5f6b 203d 2050 2e43  f.concat_k = P.C
-0000fdc0: 6f6e 6361 7428 6178 6973 3d33 290a 2020  oncat(axis=3).  
-0000fdd0: 2020 2020 2020 2020 2020 7365 6c66 2e63            self.c
-0000fde0: 6f6e 6361 745f 7620 3d20 502e 436f 6e63  oncat_v = P.Conc
-0000fdf0: 6174 2861 7869 733d 3229 0a20 2020 2020  at(axis=2).     
-0000fe00: 2020 2020 2020 2073 656c 662e 6d75 6c74         self.mult
-0000fe10: 6970 6c79 5f64 6174 6120 3d20 5465 6e73  iply_data = Tens
-0000fe20: 6f72 285b 0a20 2020 2020 2020 2020 2020  or([.           
-0000fe30: 2020 2020 202d 3130 3030 302e 302c 0a20       -10000.0,. 
-0000fe40: 2020 2020 2020 2020 2020 205d 2c20 6474             ], dt
-0000fe50: 7970 653d 736f 6674 6d61 785f 636f 6d70  ype=softmax_comp
-0000fe60: 7574 655f 7479 7065 290a 2020 2020 2020  ute_type).      
-0000fe70: 2020 2020 2020 7365 6c66 2e62 6174 6368        self.batch
-0000fe80: 5f6d 6174 6d75 6c20 3d20 502e 4261 7463  _matmul = P.Batc
-0000fe90: 684d 6174 4d75 6c28 290a 2020 2020 2020  hMatMul().      
-0000fea0: 2020 2020 2020 7365 6c66 2e72 6561 6c5f        self.real_
-0000feb0: 6469 7620 3d20 502e 5265 616c 4469 7628  div = P.RealDiv(
-0000fec0: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
-0000fed0: 6c66 2e73 7562 203d 2050 2e53 7562 2829  lf.sub = P.Sub()
-0000fee0: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-0000fef0: 662e 7375 625f 7361 203d 2050 2e53 7562  f.sub_sa = P.Sub
-0000ff00: 2829 0a20 2020 2020 2020 2020 2020 2073  ().            s
-0000ff10: 656c 662e 6d75 6c20 3d20 502e 4d75 6c28  elf.mul = P.Mul(
-0000ff20: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
-0000ff30: 6c66 2e61 6464 203d 2050 2e41 6464 2829  lf.add = P.Add()
-0000ff40: 0a20 2020 2020 2020 2020 2020 2023 204e  .            # N
-0000ff50: 6f72 6d61 6c69 7a65 2066 6163 746f 7220  ormalize factor 
-0000ff60: 666f 7220 6174 7465 6e74 696f 6e2c 2073  for attention, s
-0000ff70: 7172 7428 646b 2920 6173 2077 6964 656c  qrt(dk) as widel
-0000ff80: 7920 7573 6564 0a20 2020 2020 2020 2020  y used.         
-0000ff90: 2020 2073 656c 662e 7363 616c 655f 6661     self.scale_fa
-0000ffa0: 6374 6f72 203d 2054 656e 736f 7228 6d61  ctor = Tensor(ma
-0000ffb0: 7468 2e73 7172 7428 6d61 7468 2e73 7172  th.sqrt(math.sqr
-0000ffc0: 7428 7365 6c66 2e73 697a 655f 7065 725f  t(self.size_per_
-0000ffd0: 6865 6164 2929 290a 2020 2020 2020 2020  head))).        
-0000ffe0: 2020 2020 7365 6c66 2e75 7365 5f70 6173      self.use_pas
-0000fff0: 7420 3d20 7573 655f 7061 7374 0a20 2020  t = use_past.   
-00010000: 2020 2020 2020 2020 2073 656c 662e 6472           self.dr
-00010010: 6f70 6f75 7420 3d20 6765 745f 6472 6f70  opout = get_drop
-00010020: 6f75 7428 6869 6464 656e 5f64 726f 706f  out(hidden_dropo
-00010030: 7574 5f72 6174 6529 0a20 2020 2020 2020  ut_rate).       
-00010040: 2020 2020 2073 656c 662e 7072 6f62 5f64       self.prob_d
-00010050: 726f 706f 7574 203d 2067 6574 5f64 726f  ropout = get_dro
-00010060: 706f 7574 2861 7474 656e 7469 6f6e 5f64  pout(attention_d
-00010070: 726f 706f 7574 5f72 6174 6529 0a20 2020  ropout_rate).   
-00010080: 2020 2020 2020 2020 2073 656c 662e 736f           self.so
-00010090: 6674 6d61 7820 3d20 6e6e 2e53 6f66 746d  ftmax = nn.Softm
-000100a0: 6178 2829 2e74 6f5f 666c 6f61 7428 736f  ax().to_float(so
-000100b0: 6674 6d61 785f 636f 6d70 7574 655f 7479  ftmax_compute_ty
-000100c0: 7065 290a 2020 2020 2020 2020 2020 2020  pe).            
-000100d0: 7365 6c66 2e73 6f66 746d 6178 5f33 6420  self.softmax_3d 
-000100e0: 3d20 6e6e 2e53 6f66 746d 6178 2829 2e74  = nn.Softmax().t
-000100f0: 6f5f 666c 6f61 7428 736f 6674 6d61 785f  o_float(softmax_
-00010100: 636f 6d70 7574 655f 7479 7065 290a 2020  compute_type).  
-00010110: 2020 2020 2020 2020 2020 7365 6c66 2e73            self.s
-00010120: 6f66 746d 6178 5f63 6173 7420 3d20 502e  oftmax_cast = P.
-00010130: 4361 7374 2829 0a20 2020 2020 2020 2020  Cast().         
-00010140: 2020 2073 656c 662e 736f 6674 6d61 785f     self.softmax_
-00010150: 7265 7368 6170 6520 3d20 502e 5265 7368  reshape = P.Resh
-00010160: 6170 6528 290a 2020 2020 2020 2020 2020  ape().          
-00010170: 2020 7365 6c66 2e65 7870 616e 645f 6469    self.expand_di
-00010180: 6d73 203d 2050 2e45 7870 616e 6444 696d  ms = P.ExpandDim
-00010190: 7328 290a 0a20 2020 2020 2020 2020 2020  s()..           
-000101a0: 2023 2051 7565 7279 0a20 2020 2020 2020   # Query.       
-000101b0: 2020 2020 2073 656c 662e 6465 6e73 6531       self.dense1
-000101c0: 203d 204c 696e 6561 7228 6869 6464 656e   = Linear(hidden
-000101d0: 5f73 697a 652c 0a20 2020 2020 2020 2020  _size,.         
-000101e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000101f0: 2020 2020 2020 2020 6869 6464 656e 5f73          hidden_s
-00010200: 697a 652c 0a20 2020 2020 2020 2020 2020  ize,.           
-00010210: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010220: 2020 2020 2020 636f 6d70 7574 655f 6474        compute_dt
-00010230: 7970 653d 636f 6d70 7574 655f 6474 7970  ype=compute_dtyp
-00010240: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
-00010250: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010260: 2020 2020 7061 7261 6d5f 696e 6974 5f74      param_init_t
-00010270: 7970 653d 7061 7261 6d5f 696e 6974 5f74  ype=param_init_t
-00010280: 7970 6529 0a20 2020 2020 2020 2020 2020  ype).           
-00010290: 2023 204b 6579 0a20 2020 2020 2020 2020   # Key.         
-000102a0: 2020 2073 656c 662e 6465 6e73 6532 203d     self.dense2 =
-000102b0: 204c 696e 6561 7228 6869 6464 656e 5f73   Linear(hidden_s
-000102c0: 697a 652c 0a20 2020 2020 2020 2020 2020  ize,.           
-000102d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000102e0: 2020 2020 2020 6869 6464 656e 5f73 697a        hidden_siz
-000102f0: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
-00010300: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010310: 2020 2020 636f 6d70 7574 655f 6474 7970      compute_dtyp
-00010320: 653d 636f 6d70 7574 655f 6474 7970 652c  e=compute_dtype,
-00010330: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00010340: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010350: 2020 7061 7261 6d5f 696e 6974 5f74 7970    param_init_typ
-00010360: 653d 7061 7261 6d5f 696e 6974 5f74 7970  e=param_init_typ
-00010370: 6529 0a20 2020 2020 2020 2020 2020 2023  e).            #
-00010380: 2056 616c 7565 0a20 2020 2020 2020 2020   Value.         
-00010390: 2020 2073 656c 662e 6465 6e73 6533 203d     self.dense3 =
-000103a0: 204c 696e 6561 7228 6869 6464 656e 5f73   Linear(hidden_s
-000103b0: 697a 652c 0a20 2020 2020 2020 2020 2020  ize,.           
-000103c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000103d0: 2020 2020 2020 6869 6464 656e 5f73 697a        hidden_siz
-000103e0: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
-000103f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010400: 2020 2020 636f 6d70 7574 655f 6474 7970      compute_dtyp
-00010410: 653d 636f 6d70 7574 655f 6474 7970 652c  e=compute_dtype,
-00010420: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00010430: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010440: 2020 7061 7261 6d5f 696e 6974 5f74 7970    param_init_typ
-00010450: 653d 7061 7261 6d5f 696e 6974 5f74 7970  e=param_init_typ
-00010460: 6529 0a0a 2020 2020 2020 2020 2020 2020  e)..            
-00010470: 7365 6c66 2e64 7479 7065 203d 2063 6f6d  self.dtype = com
-00010480: 7075 7465 5f64 7479 7065 0a20 2020 2020  pute_dtype.     
-00010490: 2020 2020 2020 2073 656c 662e 736f 6674         self.soft
-000104a0: 6d61 785f 6474 7970 6520 3d20 736f 6674  max_dtype = soft
-000104b0: 6d61 785f 636f 6d70 7574 655f 7479 7065  max_compute_type
-000104c0: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if 
-000104d0: 7365 6c66 2e75 7365 5f70 6173 743a 0a20  self.use_past:. 
-000104e0: 2020 2020 2020 2020 2020 2020 2020 2023                 #
-000104f0: 206f 7065 7261 746f 7273 2075 7365 6420   operators used 
-00010500: 666f 7220 7374 6174 6520 7265 7573 650a  for state reuse.
-00010510: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010520: 7365 715f 7261 6e67 6520 3d20 6e70 2e61  seq_range = np.a
-00010530: 7261 6e67 6528 7372 635f 7365 715f 6c65  range(src_seq_le
-00010540: 6e67 7468 292e 7265 7368 6170 6528 312c  ngth).reshape(1,
-00010550: 2031 2c20 2d31 290a 2020 2020 2020 2020   1, -1).        
-00010560: 2020 2020 2020 2020 7365 6c66 2e72 616e          self.ran
-00010570: 6765 203d 2054 656e 736f 7228 6e70 2e74  ge = Tensor(np.t
-00010580: 696c 6528 7365 715f 7261 6e67 652c 2028  ile(seq_range, (
-00010590: 6261 7463 685f 7369 7a65 2c20 312c 2031  batch_size, 1, 1
-000105a0: 2929 2c20 6d73 7479 7065 2e69 6e74 3332  )), mstype.int32
-000105b0: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
-000105c0: 2020 7365 6c66 2e73 6571 5f6c 656e 6774    self.seq_lengt
-000105d0: 6820 3d20 7372 635f 7365 715f 6c65 6e67  h = src_seq_leng
-000105e0: 7468 0a20 2020 2020 2020 2020 2020 2020  th.             
-000105f0: 2020 2073 656c 662e 6174 7465 6e74 696f     self.attentio
-00010600: 6e5f 6d61 736b 203d 2054 656e 736f 7228  n_mask = Tensor(
-00010610: 6e70 2e74 7269 6c28 6e70 2e6f 6e65 7328  np.tril(np.ones(
-00010620: 7368 6170 653d 2873 656c 662e 7365 715f  shape=(self.seq_
-00010630: 6c65 6e67 7468 2c20 7365 6c66 2e73 6571  length, self.seq
-00010640: 5f6c 656e 6774 6829 2929 2c20 6d73 7479  _length))), msty
-00010650: 7065 2e69 6e74 3332 290a 2020 2020 2020  pe.int32).      
-00010660: 2020 2020 2020 2020 2020 7365 6c66 2e73            self.s
-00010670: 6c69 6365 203d 2050 2e53 7472 6964 6564  lice = P.Strided
-00010680: 536c 6963 6528 292e 7368 6172 6428 2828  Slice().shard(((
-00010690: 312c 2031 2c20 312c 2031 292c 2929 0a20  1, 1, 1, 1),)). 
-000106a0: 2020 2020 2020 2020 2020 2020 2020 2073                 s
-000106b0: 656c 662e 6e6f 745f 6571 7561 6c20 3d20  elf.not_equal = 
-000106c0: 502e 4e6f 7445 7175 616c 2829 2e73 6861  P.NotEqual().sha
-000106d0: 7264 2828 2831 2c20 312c 2031 2c20 3129  rd(((1, 1, 1, 1)
-000106e0: 2c20 2829 2929 0a20 2020 2020 2020 2020  , ())).         
-000106f0: 2020 2020 2020 2073 656c 662e 7265 6475         self.redu
-00010700: 6365 7375 6d20 3d20 502e 5265 6475 6365  cesum = P.Reduce
-00010710: 5375 6d28 292e 7368 6172 6428 2828 312c  Sum().shard(((1,
-00010720: 2031 2c20 312c 2031 292c 2929 0a20 2020   1, 1, 1),)).   
-00010730: 2020 2020 2020 2020 2020 2020 2073 656c               sel
-00010740: 662e 6578 7061 6e64 5f64 696d 7320 3d20  f.expand_dims = 
-00010750: 502e 4578 7061 6e64 4469 6d73 2829 2e73  P.ExpandDims().s
-00010760: 6861 7264 2828 2831 2c20 312c 2031 292c  hard(((1, 1, 1),
-00010770: 2929 0a20 2020 2020 2020 2020 2020 2020  )).             
-00010780: 2020 2073 656c 662e 7465 6e73 6f72 5f6c     self.tensor_l
-00010790: 6520 3d20 502e 4c65 7373 4571 7561 6c28  e = P.LessEqual(
-000107a0: 292e 7368 6172 6428 2828 312c 2031 2c20  ).shard(((1, 1, 
-000107b0: 3129 2c20 2831 2c20 312c 2031 2929 290a  1), (1, 1, 1))).
-000107c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000107d0: 7365 6c66 2e61 6464 203d 2050 2e41 6464  self.add = P.Add
-000107e0: 2829 2e73 6861 7264 2828 2831 2c20 312c  ().shard(((1, 1,
-000107f0: 2031 2c20 3129 2c20 2831 2c20 312c 2031   1, 1), (1, 1, 1
-00010800: 2c20 3129 2929 0a20 2020 2020 2020 2020  , 1))).         
-00010810: 2020 2020 2020 2073 656c 662e 6571 7561         self.equa
-00010820: 6c20 3d20 502e 4571 7561 6c28 292e 7368  l = P.Equal().sh
-00010830: 6172 6428 2828 312c 2031 2c20 3129 2c20  ard(((1, 1, 1), 
-00010840: 2831 2c20 312c 2031 2929 290a 2020 2020  (1, 1, 1))).    
-00010850: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-00010860: 2e73 7562 3120 3d20 502e 5375 6228 292e  .sub1 = P.Sub().
-00010870: 7368 6172 6428 2828 312c 292c 2028 2929  shard(((1,), ())
-00010880: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
-00010890: 2020 7365 6c66 2e74 696c 6520 3d20 502e    self.tile = P.
-000108a0: 5469 6c65 2829 2e73 6861 7264 2828 2831  Tile().shard(((1
-000108b0: 2c20 312c 2031 2c20 3129 2c29 290a 2020  , 1, 1, 1),)).  
-000108c0: 2020 2020 2020 2020 2020 2020 2020 7365                se
-000108d0: 6c66 2e6c 6573 7320 3d20 502e 4c65 7373  lf.less = P.Less
-000108e0: 2829 2e73 6861 7264 2828 2831 2c20 312c  ().shard(((1, 1,
-000108f0: 2031 292c 2028 312c 2031 2c20 3129 2929   1), (1, 1, 1)))
-00010900: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00010910: 2073 656c 662e 6d75 6c31 203d 2050 2e4d   self.mul1 = P.M
-00010920: 756c 2829 2e73 6861 7264 2828 2831 2c20  ul().shard(((1, 
-00010930: 312c 2031 2c20 3129 2c20 2831 2c20 312c  1, 1, 1), (1, 1,
-00010940: 2031 2c20 3129 2929 0a20 2020 2020 2020   1, 1))).       
-00010950: 2065 6c73 653a 0a20 2020 2020 2020 2020   else:.         
-00010960: 2020 205f 6368 6563 6b5f 636f 6e66 6967     _check_config
-00010970: 2870 6172 616c 6c65 6c5f 636f 6e66 6967  (parallel_config
-00010980: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
-00010990: 6c66 2e73 7263 5f73 6571 5f6c 656e 6774  lf.src_seq_lengt
-000109a0: 6820 3d20 7372 635f 7365 715f 6c65 6e67  h = src_seq_leng
-000109b0: 7468 0a20 2020 2020 2020 2020 2020 2073  th.            s
-000109c0: 656c 662e 7467 745f 7365 715f 6c65 6e67  elf.tgt_seq_leng
-000109d0: 7468 203d 2074 6774 5f73 6571 5f6c 656e  th = tgt_seq_len
-000109e0: 6774 680a 2020 2020 2020 2020 2020 2020  gth.            
-000109f0: 7365 6c66 2e68 6964 6465 6e5f 7369 7a65  self.hidden_size
-00010a00: 203d 2068 6964 6465 6e5f 7369 7a65 0a20   = hidden_size. 
-00010a10: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-00010a20: 6261 7463 685f 7369 7a65 203d 2062 6174  batch_size = bat
-00010a30: 6368 5f73 697a 650a 2020 2020 2020 2020  ch_size.        
-00010a40: 2020 2020 6966 2068 6964 6465 6e5f 6472      if hidden_dr
-00010a50: 6f70 6f75 745f 7261 7465 203c 2030 206f  opout_rate < 0 o
-00010a60: 7220 6869 6464 656e 5f64 726f 706f 7574  r hidden_dropout
-00010a70: 5f72 6174 6520 3e3d 2031 3a0a 2020 2020  _rate >= 1:.    
-00010a80: 2020 2020 2020 2020 2020 2020 7261 6973              rais
-00010a90: 6520 5661 6c75 6545 7272 6f72 2822 466f  e ValueError("Fo
-00010aa0: 7220 274d 756c 7469 4865 6164 4174 7465  r 'MultiHeadAtte
-00010ab0: 6e74 696f 6e27 2c20 7468 6520 636c 6173  ntion', the clas
-00010ac0: 7320 7661 7269 6162 6c65 2027 6869 6464  s variable 'hidd
-00010ad0: 656e 5f64 726f 706f 7574 5f72 6174 6527  en_dropout_rate'
-00010ae0: 206d 7573 7420 6265 2022 0a20 2020 2020   must be ".     
+0000f8c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000f8d0: 7061 7261 6d5f 696e 6974 5f74 7970 653d  param_init_type=
+0000f8e0: 7061 7261 6d5f 696e 6974 5f74 7970 6529  param_init_type)
+0000f8f0: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
+0000f900: 662e 7072 6f6a 6563 7469 6f6e 2e73 6861  f.projection.sha
+0000f910: 7264 2873 7472 6174 6567 795f 6269 6173  rd(strategy_bias
+0000f920: 3d28 2870 6172 616c 6c65 6c5f 636f 6e66  =((parallel_conf
+0000f930: 6967 2e64 6174 615f 7061 7261 6c6c 656c  ig.data_parallel
+0000f940: 2c20 3129 2c20 2831 2c29 292c 0a20 2020  , 1), (1,)),.   
+0000f950: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000f960: 2020 2020 2020 2020 2020 2020 2020 2073                 s
+0000f970: 7472 6174 6567 795f 6d61 746d 756c 3d28  trategy_matmul=(
+0000f980: 2870 6172 616c 6c65 6c5f 636f 6e66 6967  (parallel_config
+0000f990: 2e64 6174 615f 7061 7261 6c6c 656c 2c20  .data_parallel, 
+0000f9a0: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
+0000f9b0: 6d6f 6465 6c5f 7061 7261 6c6c 656c 292c  model_parallel),
+0000f9c0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0000f9d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000f9e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000f9f0: 2020 2020 2870 6172 616c 6c65 6c5f 636f      (parallel_co
+0000fa00: 6e66 6967 2e6d 6f64 656c 5f70 6172 616c  nfig.model_paral
+0000fa10: 6c65 6c2c 2031 2929 290a 2020 2020 2020  lel, 1))).      
+0000fa20: 2020 2020 2020 7365 6c66 2e70 726f 6a65        self.proje
+0000fa30: 6374 696f 6e2e 6269 6173 2e70 6172 616c  ction.bias.paral
+0000fa40: 6c65 6c5f 6f70 7469 6d69 7a65 7220 3d20  lel_optimizer = 
+0000fa50: 4661 6c73 650a 2020 2020 2020 2020 2020  False.          
+0000fa60: 2020 7365 6c66 2e74 7261 6e73 706f 7365    self.transpose
+0000fa70: 203d 2050 2e54 7261 6e73 706f 7365 2829   = P.Transpose()
+0000fa80: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
+0000fa90: 662e 6d65 7267 6572 5f68 6561 645f 7472  f.merger_head_tr
+0000faa0: 616e 7370 6f73 6520 3d20 502e 5472 616e  anspose = P.Tran
+0000fab0: 7370 6f73 6528 290a 2020 2020 2020 2020  spose().        
+0000fac0: 2020 2020 7365 6c66 2e72 6573 6861 7065      self.reshape
+0000fad0: 203d 2050 2e52 6573 6861 7065 2829 0a20   = P.Reshape(). 
+0000fae0: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+0000faf0: 6e5f 6865 6164 203d 206e 756d 5f68 6561  n_head = num_hea
+0000fb00: 6473 0a20 2020 2020 2020 2020 2020 2023  ds.            #
+0000fb10: 2065 6d62 6564 6469 6e67 2073 697a 6520   embedding size 
+0000fb20: 7065 7220 6865 6164 0a20 2020 2020 2020  per head.       
+0000fb30: 2020 2020 2073 656c 662e 7369 7a65 5f70       self.size_p
+0000fb40: 6572 5f68 6561 6420 3d20 6869 6464 656e  er_head = hidden
+0000fb50: 5f73 697a 6520 2f2f 2073 656c 662e 6e5f  _size // self.n_
+0000fb60: 6865 6164 0a20 2020 2020 2020 2020 2020  head.           
+0000fb70: 2073 656c 662e 636f 6e63 6174 5f6b 203d   self.concat_k =
+0000fb80: 2050 2e43 6f6e 6361 7428 6178 6973 3d33   P.Concat(axis=3
+0000fb90: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
+0000fba0: 6c66 2e63 6f6e 6361 745f 7620 3d20 502e  lf.concat_v = P.
+0000fbb0: 436f 6e63 6174 2861 7869 733d 3229 0a20  Concat(axis=2). 
+0000fbc0: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+0000fbd0: 6d75 6c74 6970 6c79 5f64 6174 6120 3d20  multiply_data = 
+0000fbe0: 5465 6e73 6f72 285b 0a20 2020 2020 2020  Tensor([.       
+0000fbf0: 2020 2020 2020 2020 202d 3130 3030 302e           -10000.
+0000fc00: 302c 0a20 2020 2020 2020 2020 2020 205d  0,.            ]
+0000fc10: 2c20 6474 7970 653d 736f 6674 6d61 785f  , dtype=softmax_
+0000fc20: 636f 6d70 7574 655f 7479 7065 290a 2020  compute_type).  
+0000fc30: 2020 2020 2020 2020 2020 7365 6c66 2e62            self.b
+0000fc40: 6174 6368 5f6d 6174 6d75 6c20 3d20 502e  atch_matmul = P.
+0000fc50: 4261 7463 684d 6174 4d75 6c28 290a 2020  BatchMatMul().  
+0000fc60: 2020 2020 2020 2020 2020 7365 6c66 2e72            self.r
+0000fc70: 6561 6c5f 6469 7620 3d20 502e 5265 616c  eal_div = P.Real
+0000fc80: 4469 7628 290a 2020 2020 2020 2020 2020  Div().          
+0000fc90: 2020 7365 6c66 2e73 7562 203d 2050 2e53    self.sub = P.S
+0000fca0: 7562 2829 0a20 2020 2020 2020 2020 2020  ub().           
+0000fcb0: 2073 656c 662e 7375 625f 7361 203d 2050   self.sub_sa = P
+0000fcc0: 2e53 7562 2829 0a20 2020 2020 2020 2020  .Sub().         
+0000fcd0: 2020 2073 656c 662e 6d75 6c20 3d20 502e     self.mul = P.
+0000fce0: 4d75 6c28 290a 2020 2020 2020 2020 2020  Mul().          
+0000fcf0: 2020 7365 6c66 2e61 6464 203d 2050 2e41    self.add = P.A
+0000fd00: 6464 2829 0a20 2020 2020 2020 2020 2020  dd().           
+0000fd10: 2023 204e 6f72 6d61 6c69 7a65 2066 6163   # Normalize fac
+0000fd20: 746f 7220 666f 7220 6174 7465 6e74 696f  tor for attentio
+0000fd30: 6e2c 2073 7172 7428 646b 2920 6173 2077  n, sqrt(dk) as w
+0000fd40: 6964 656c 7920 7573 6564 0a20 2020 2020  idely used.     
+0000fd50: 2020 2020 2020 2073 656c 662e 7363 616c         self.scal
+0000fd60: 655f 6661 6374 6f72 203d 2054 656e 736f  e_factor = Tenso
+0000fd70: 7228 6d61 7468 2e73 7172 7428 6d61 7468  r(math.sqrt(math
+0000fd80: 2e73 7172 7428 7365 6c66 2e73 697a 655f  .sqrt(self.size_
+0000fd90: 7065 725f 6865 6164 2929 290a 2020 2020  per_head))).    
+0000fda0: 2020 2020 2020 2020 7365 6c66 2e75 7365          self.use
+0000fdb0: 5f70 6173 7420 3d20 7573 655f 7061 7374  _past = use_past
+0000fdc0: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
+0000fdd0: 662e 6472 6f70 6f75 7420 3d20 6765 745f  f.dropout = get_
+0000fde0: 6472 6f70 6f75 7428 6869 6464 656e 5f64  dropout(hidden_d
+0000fdf0: 726f 706f 7574 5f72 6174 6529 0a20 2020  ropout_rate).   
+0000fe00: 2020 2020 2020 2020 2073 656c 662e 7072           self.pr
+0000fe10: 6f62 5f64 726f 706f 7574 203d 2067 6574  ob_dropout = get
+0000fe20: 5f64 726f 706f 7574 2861 7474 656e 7469  _dropout(attenti
+0000fe30: 6f6e 5f64 726f 706f 7574 5f72 6174 6529  on_dropout_rate)
+0000fe40: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
+0000fe50: 662e 736f 6674 6d61 7820 3d20 6e6e 2e53  f.softmax = nn.S
+0000fe60: 6f66 746d 6178 2829 2e74 6f5f 666c 6f61  oftmax().to_floa
+0000fe70: 7428 736f 6674 6d61 785f 636f 6d70 7574  t(softmax_comput
+0000fe80: 655f 7479 7065 290a 2020 2020 2020 2020  e_type).        
+0000fe90: 2020 2020 7365 6c66 2e73 6f66 746d 6178      self.softmax
+0000fea0: 5f33 6420 3d20 6e6e 2e53 6f66 746d 6178  _3d = nn.Softmax
+0000feb0: 2829 2e74 6f5f 666c 6f61 7428 736f 6674  ().to_float(soft
+0000fec0: 6d61 785f 636f 6d70 7574 655f 7479 7065  max_compute_type
+0000fed0: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
+0000fee0: 6c66 2e73 6f66 746d 6178 5f63 6173 7420  lf.softmax_cast 
+0000fef0: 3d20 502e 4361 7374 2829 0a20 2020 2020  = P.Cast().     
+0000ff00: 2020 2020 2020 2073 656c 662e 736f 6674         self.soft
+0000ff10: 6d61 785f 7265 7368 6170 6520 3d20 502e  max_reshape = P.
+0000ff20: 5265 7368 6170 6528 290a 2020 2020 2020  Reshape().      
+0000ff30: 2020 2020 2020 7365 6c66 2e65 7870 616e        self.expan
+0000ff40: 645f 6469 6d73 203d 2050 2e45 7870 616e  d_dims = P.Expan
+0000ff50: 6444 696d 7328 290a 0a20 2020 2020 2020  dDims()..       
+0000ff60: 2020 2020 2023 2051 7565 7279 0a20 2020       # Query.   
+0000ff70: 2020 2020 2020 2020 2073 656c 662e 6465           self.de
+0000ff80: 6e73 6531 203d 204c 696e 6561 7228 6869  nse1 = Linear(hi
+0000ff90: 6464 656e 5f73 697a 652c 0a20 2020 2020  dden_size,.     
+0000ffa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000ffb0: 2020 2020 2020 2020 2020 2020 6869 6464              hidd
+0000ffc0: 656e 5f73 697a 652c 0a20 2020 2020 2020  en_size,.       
+0000ffd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000ffe0: 2020 2020 2020 2020 2020 636f 6d70 7574            comput
+0000fff0: 655f 6474 7970 653d 636f 6d70 7574 655f  e_dtype=compute_
+00010000: 6474 7970 652c 0a20 2020 2020 2020 2020  dtype,.         
+00010010: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010020: 2020 2020 2020 2020 7061 7261 6d5f 696e          param_in
+00010030: 6974 5f74 7970 653d 7061 7261 6d5f 696e  it_type=param_in
+00010040: 6974 5f74 7970 6529 0a20 2020 2020 2020  it_type).       
+00010050: 2020 2020 2023 204b 6579 0a20 2020 2020       # Key.     
+00010060: 2020 2020 2020 2073 656c 662e 6465 6e73         self.dens
+00010070: 6532 203d 204c 696e 6561 7228 6869 6464  e2 = Linear(hidd
+00010080: 656e 5f73 697a 652c 0a20 2020 2020 2020  en_size,.       
+00010090: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000100a0: 2020 2020 2020 2020 2020 6869 6464 656e            hidden
+000100b0: 5f73 697a 652c 0a20 2020 2020 2020 2020  _size,.         
+000100c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000100d0: 2020 2020 2020 2020 636f 6d70 7574 655f          compute_
+000100e0: 6474 7970 653d 636f 6d70 7574 655f 6474  dtype=compute_dt
+000100f0: 7970 652c 0a20 2020 2020 2020 2020 2020  ype,.           
+00010100: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010110: 2020 2020 2020 7061 7261 6d5f 696e 6974        param_init
+00010120: 5f74 7970 653d 7061 7261 6d5f 696e 6974  _type=param_init
+00010130: 5f74 7970 6529 0a20 2020 2020 2020 2020  _type).         
+00010140: 2020 2023 2056 616c 7565 0a20 2020 2020     # Value.     
+00010150: 2020 2020 2020 2073 656c 662e 6465 6e73         self.dens
+00010160: 6533 203d 204c 696e 6561 7228 6869 6464  e3 = Linear(hidd
+00010170: 656e 5f73 697a 652c 0a20 2020 2020 2020  en_size,.       
+00010180: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010190: 2020 2020 2020 2020 2020 6869 6464 656e            hidden
+000101a0: 5f73 697a 652c 0a20 2020 2020 2020 2020  _size,.         
+000101b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000101c0: 2020 2020 2020 2020 636f 6d70 7574 655f          compute_
+000101d0: 6474 7970 653d 636f 6d70 7574 655f 6474  dtype=compute_dt
+000101e0: 7970 652c 0a20 2020 2020 2020 2020 2020  ype,.           
+000101f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010200: 2020 2020 2020 7061 7261 6d5f 696e 6974        param_init
+00010210: 5f74 7970 653d 7061 7261 6d5f 696e 6974  _type=param_init
+00010220: 5f74 7970 6529 0a0a 2020 2020 2020 2020  _type)..        
+00010230: 2020 2020 7365 6c66 2e64 7479 7065 203d      self.dtype =
+00010240: 2063 6f6d 7075 7465 5f64 7479 7065 0a20   compute_dtype. 
+00010250: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+00010260: 736f 6674 6d61 785f 6474 7970 6520 3d20  softmax_dtype = 
+00010270: 736f 6674 6d61 785f 636f 6d70 7574 655f  softmax_compute_
+00010280: 7479 7065 0a20 2020 2020 2020 2020 2020  type.           
+00010290: 2069 6620 7365 6c66 2e75 7365 5f70 6173   if self.use_pas
+000102a0: 743a 0a20 2020 2020 2020 2020 2020 2020  t:.             
+000102b0: 2020 2023 206f 7065 7261 746f 7273 2075     # operators u
+000102c0: 7365 6420 666f 7220 7374 6174 6520 7265  sed for state re
+000102d0: 7573 650a 2020 2020 2020 2020 2020 2020  use.            
+000102e0: 2020 2020 7365 715f 7261 6e67 6520 3d20      seq_range = 
+000102f0: 6e70 2e61 7261 6e67 6528 7372 635f 7365  np.arange(src_se
+00010300: 715f 6c65 6e67 7468 292e 7265 7368 6170  q_length).reshap
+00010310: 6528 312c 2031 2c20 2d31 290a 2020 2020  e(1, 1, -1).    
+00010320: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+00010330: 2e72 616e 6765 203d 2054 656e 736f 7228  .range = Tensor(
+00010340: 6e70 2e74 696c 6528 7365 715f 7261 6e67  np.tile(seq_rang
+00010350: 652c 2028 6261 7463 685f 7369 7a65 2c20  e, (batch_size, 
+00010360: 312c 2031 2929 2c20 6d73 7479 7065 2e69  1, 1)), mstype.i
+00010370: 6e74 3332 290a 2020 2020 2020 2020 2020  nt32).          
+00010380: 2020 2020 2020 7365 6c66 2e73 6571 5f6c        self.seq_l
+00010390: 656e 6774 6820 3d20 7372 635f 7365 715f  ength = src_seq_
+000103a0: 6c65 6e67 7468 0a20 2020 2020 2020 2020  length.         
+000103b0: 2020 2020 2020 2073 656c 662e 6174 7465         self.atte
+000103c0: 6e74 696f 6e5f 6d61 736b 203d 2054 656e  ntion_mask = Ten
+000103d0: 736f 7228 6e70 2e74 7269 6c28 6e70 2e6f  sor(np.tril(np.o
+000103e0: 6e65 7328 7368 6170 653d 2873 656c 662e  nes(shape=(self.
+000103f0: 7365 715f 6c65 6e67 7468 2c20 7365 6c66  seq_length, self
+00010400: 2e73 6571 5f6c 656e 6774 6829 2929 2c20  .seq_length))), 
+00010410: 6d73 7479 7065 2e69 6e74 3332 290a 2020  mstype.int32).  
+00010420: 2020 2020 2020 2020 2020 2020 2020 7365                se
+00010430: 6c66 2e73 6c69 6365 203d 2050 2e53 7472  lf.slice = P.Str
+00010440: 6964 6564 536c 6963 6528 292e 7368 6172  idedSlice().shar
+00010450: 6428 2828 312c 2031 2c20 312c 2031 292c  d(((1, 1, 1, 1),
+00010460: 2929 0a20 2020 2020 2020 2020 2020 2020  )).             
+00010470: 2020 2073 656c 662e 6e6f 745f 6571 7561     self.not_equa
+00010480: 6c20 3d20 502e 4e6f 7445 7175 616c 2829  l = P.NotEqual()
+00010490: 2e73 6861 7264 2828 2831 2c20 312c 2031  .shard(((1, 1, 1
+000104a0: 2c20 3129 2c20 2829 2929 0a20 2020 2020  , 1), ())).     
+000104b0: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+000104c0: 7265 6475 6365 7375 6d20 3d20 502e 5265  reducesum = P.Re
+000104d0: 6475 6365 5375 6d28 292e 7368 6172 6428  duceSum().shard(
+000104e0: 2828 312c 2031 2c20 312c 2031 292c 2929  ((1, 1, 1, 1),))
+000104f0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00010500: 2073 656c 662e 6578 7061 6e64 5f64 696d   self.expand_dim
+00010510: 7320 3d20 502e 4578 7061 6e64 4469 6d73  s = P.ExpandDims
+00010520: 2829 2e73 6861 7264 2828 2831 2c20 312c  ().shard(((1, 1,
+00010530: 2031 292c 2929 0a20 2020 2020 2020 2020   1),)).         
+00010540: 2020 2020 2020 2073 656c 662e 7465 6e73         self.tens
+00010550: 6f72 5f6c 6520 3d20 502e 4c65 7373 4571  or_le = P.LessEq
+00010560: 7561 6c28 292e 7368 6172 6428 2828 312c  ual().shard(((1,
+00010570: 2031 2c20 3129 2c20 2831 2c20 312c 2031   1, 1), (1, 1, 1
+00010580: 2929 290a 2020 2020 2020 2020 2020 2020  ))).            
+00010590: 2020 2020 7365 6c66 2e61 6464 203d 2050      self.add = P
+000105a0: 2e41 6464 2829 2e73 6861 7264 2828 2831  .Add().shard(((1
+000105b0: 2c20 312c 2031 2c20 3129 2c20 2831 2c20  , 1, 1, 1), (1, 
+000105c0: 312c 2031 2c20 3129 2929 0a20 2020 2020  1, 1, 1))).     
+000105d0: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+000105e0: 6571 7561 6c20 3d20 502e 4571 7561 6c28  equal = P.Equal(
+000105f0: 292e 7368 6172 6428 2828 312c 2031 2c20  ).shard(((1, 1, 
+00010600: 3129 2c20 2831 2c20 312c 2031 2929 290a  1), (1, 1, 1))).
+00010610: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010620: 7365 6c66 2e73 7562 3120 3d20 502e 5375  self.sub1 = P.Su
+00010630: 6228 292e 7368 6172 6428 2828 312c 292c  b().shard(((1,),
+00010640: 2028 2929 290a 2020 2020 2020 2020 2020   ())).          
+00010650: 2020 2020 2020 7365 6c66 2e74 696c 6520        self.tile 
+00010660: 3d20 502e 5469 6c65 2829 2e73 6861 7264  = P.Tile().shard
+00010670: 2828 2831 2c20 312c 2031 2c20 3129 2c29  (((1, 1, 1, 1),)
+00010680: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
+00010690: 2020 7365 6c66 2e6c 6573 7320 3d20 502e    self.less = P.
+000106a0: 4c65 7373 2829 2e73 6861 7264 2828 2831  Less().shard(((1
+000106b0: 2c20 312c 2031 292c 2028 312c 2031 2c20  , 1, 1), (1, 1, 
+000106c0: 3129 2929 0a20 2020 2020 2020 2020 2020  1))).           
+000106d0: 2020 2020 2073 656c 662e 6d75 6c31 203d       self.mul1 =
+000106e0: 2050 2e4d 756c 2829 2e73 6861 7264 2828   P.Mul().shard((
+000106f0: 2831 2c20 312c 2031 2c20 3129 2c20 2831  (1, 1, 1, 1), (1
+00010700: 2c20 312c 2031 2c20 3129 2929 0a20 2020  , 1, 1, 1))).   
+00010710: 2020 2020 2065 6c73 653a 0a20 2020 2020       else:.     
+00010720: 2020 2020 2020 205f 6368 6563 6b5f 636f         _check_co
+00010730: 6e66 6967 2870 6172 616c 6c65 6c5f 636f  nfig(parallel_co
+00010740: 6e66 6967 290a 2020 2020 2020 2020 2020  nfig).          
+00010750: 2020 7365 6c66 2e73 7263 5f73 6571 5f6c    self.src_seq_l
+00010760: 656e 6774 6820 3d20 7372 635f 7365 715f  ength = src_seq_
+00010770: 6c65 6e67 7468 0a20 2020 2020 2020 2020  length.         
+00010780: 2020 2073 656c 662e 7467 745f 7365 715f     self.tgt_seq_
+00010790: 6c65 6e67 7468 203d 2074 6774 5f73 6571  length = tgt_seq
+000107a0: 5f6c 656e 6774 680a 2020 2020 2020 2020  _length.        
+000107b0: 2020 2020 7365 6c66 2e68 6964 6465 6e5f      self.hidden_
+000107c0: 7369 7a65 203d 2068 6964 6465 6e5f 7369  size = hidden_si
+000107d0: 7a65 0a20 2020 2020 2020 2020 2020 2073  ze.            s
+000107e0: 656c 662e 6261 7463 685f 7369 7a65 203d  elf.batch_size =
+000107f0: 2062 6174 6368 5f73 697a 650a 2020 2020   batch_size.    
+00010800: 2020 2020 2020 2020 6966 2068 6964 6465          if hidde
+00010810: 6e5f 6472 6f70 6f75 745f 7261 7465 203c  n_dropout_rate <
+00010820: 2030 206f 7220 6869 6464 656e 5f64 726f   0 or hidden_dro
+00010830: 706f 7574 5f72 6174 6520 3e3d 2031 3a0a  pout_rate >= 1:.
+00010840: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010850: 7261 6973 6520 5661 6c75 6545 7272 6f72  raise ValueError
+00010860: 2822 466f 7220 274d 756c 7469 4865 6164  ("For 'MultiHead
+00010870: 4174 7465 6e74 696f 6e27 2c20 7468 6520  Attention', the 
+00010880: 636c 6173 7320 7661 7269 6162 6c65 2027  class variable '
+00010890: 6869 6464 656e 5f64 726f 706f 7574 5f72  hidden_dropout_r
+000108a0: 6174 6527 206d 7573 7420 6265 2022 0a20  ate' must be ". 
+000108b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000108c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000108d0: 2269 6e20 7261 6e67 6520 5b30 2c20 312e  "in range [0, 1.
+000108e0: 3029 2c20 6275 7420 676f 7420 7468 6520  0), but got the 
+000108f0: 7661 6c75 6520 3a20 7b7d 2e22 2e66 6f72  value : {}.".for
+00010900: 6d61 7428 6869 6464 656e 5f64 726f 706f  mat(hidden_dropo
+00010910: 7574 5f72 6174 6529 290a 2020 2020 2020  ut_rate)).      
+00010920: 2020 2020 2020 6966 2061 7474 656e 7469        if attenti
+00010930: 6f6e 5f64 726f 706f 7574 5f72 6174 6520  on_dropout_rate 
+00010940: 3c20 3020 6f72 2061 7474 656e 7469 6f6e  < 0 or attention
+00010950: 5f64 726f 706f 7574 5f72 6174 6520 3e3d  _dropout_rate >=
+00010960: 2031 3a0a 2020 2020 2020 2020 2020 2020   1:.            
+00010970: 2020 2020 7261 6973 6520 5661 6c75 6545      raise ValueE
+00010980: 7272 6f72 2822 466f 7220 274d 756c 7469  rror("For 'Multi
+00010990: 4865 6164 4174 7465 6e74 696f 6e27 2c20  HeadAttention', 
+000109a0: 7468 6520 636c 6173 7320 7661 7269 6162  the class variab
+000109b0: 6c65 2027 6174 7465 6e74 696f 6e5f 6472  le 'attention_dr
+000109c0: 6f70 6f75 745f 7261 7465 2720 6d75 7374  opout_rate' must
+000109d0: 2062 6520 220a 2020 2020 2020 2020 2020   be ".          
+000109e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000109f0: 2020 2020 2020 2022 696e 2072 616e 6765         "in range
+00010a00: 205b 302c 2031 2e30 292c 2062 7574 2067   [0, 1.0), but g
+00010a10: 6f74 2074 6865 2076 616c 7565 203a 207b  ot the value : {
+00010a20: 7d2e 222e 666f 726d 6174 2861 7474 656e  }.".format(atten
+00010a30: 7469 6f6e 5f64 726f 706f 7574 5f72 6174  tion_dropout_rat
+00010a40: 6529 290a 2020 2020 2020 2020 2020 2020  e)).            
+00010a50: 6966 2068 6964 6465 6e5f 7369 7a65 2025  if hidden_size %
+00010a60: 206e 756d 5f68 6561 6473 2021 3d20 303a   num_heads != 0:
+00010a70: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00010a80: 2072 6169 7365 2056 616c 7565 4572 726f   raise ValueErro
+00010a90: 7228 2246 6f72 2027 4d75 6c74 6948 6561  r("For 'MultiHea
+00010aa0: 6441 7474 656e 7469 6f6e 272c 2074 6865  dAttention', the
+00010ab0: 2063 6c61 7373 2076 6172 6961 626c 6520   class variable 
+00010ac0: 2768 6964 6465 6e5f 7369 7a65 2720 6d75  'hidden_size' mu
+00010ad0: 7374 2062 6520 6120 6d75 6c74 6970 6c65  st be a multiple
+00010ae0: 2022 0a20 2020 2020 2020 2020 2020 2020   ".             
 00010af0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010b00: 2020 2020 2020 2020 2020 2020 2269 6e20              "in 
-00010b10: 7261 6e67 6520 5b30 2c20 312e 3029 2c20  range [0, 1.0), 
-00010b20: 6275 7420 676f 7420 7468 6520 7661 6c75  but got the valu
-00010b30: 6520 3a20 7b7d 2e22 2e66 6f72 6d61 7428  e : {}.".format(
-00010b40: 6869 6464 656e 5f64 726f 706f 7574 5f72  hidden_dropout_r
-00010b50: 6174 6529 290a 2020 2020 2020 2020 2020  ate)).          
-00010b60: 2020 6966 2061 7474 656e 7469 6f6e 5f64    if attention_d
-00010b70: 726f 706f 7574 5f72 6174 6520 3c20 3020  ropout_rate < 0 
-00010b80: 6f72 2061 7474 656e 7469 6f6e 5f64 726f  or attention_dro
-00010b90: 706f 7574 5f72 6174 6520 3e3d 2031 3a0a  pout_rate >= 1:.
-00010ba0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010bb0: 7261 6973 6520 5661 6c75 6545 7272 6f72  raise ValueError
-00010bc0: 2822 466f 7220 274d 756c 7469 4865 6164  ("For 'MultiHead
-00010bd0: 4174 7465 6e74 696f 6e27 2c20 7468 6520  Attention', the 
-00010be0: 636c 6173 7320 7661 7269 6162 6c65 2027  class variable '
-00010bf0: 6174 7465 6e74 696f 6e5f 6472 6f70 6f75  attention_dropou
-00010c00: 745f 7261 7465 2720 6d75 7374 2062 6520  t_rate' must be 
-00010c10: 220a 2020 2020 2020 2020 2020 2020 2020  ".              
-00010c20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010c30: 2020 2022 696e 2072 616e 6765 205b 302c     "in range [0,
-00010c40: 2031 2e30 292c 2062 7574 2067 6f74 2074   1.0), but got t
-00010c50: 6865 2076 616c 7565 203a 207b 7d2e 222e  he value : {}.".
-00010c60: 666f 726d 6174 2861 7474 656e 7469 6f6e  format(attention
-00010c70: 5f64 726f 706f 7574 5f72 6174 6529 290a  _dropout_rate)).
-00010c80: 2020 2020 2020 2020 2020 2020 6966 2068              if h
-00010c90: 6964 6465 6e5f 7369 7a65 2025 206e 756d  idden_size % num
-00010ca0: 5f68 6561 6473 2021 3d20 303a 0a20 2020  _heads != 0:.   
-00010cb0: 2020 2020 2020 2020 2020 2020 2072 6169               rai
-00010cc0: 7365 2056 616c 7565 4572 726f 7228 2246  se ValueError("F
-00010cd0: 6f72 2027 4d75 6c74 6948 6561 6441 7474  or 'MultiHeadAtt
-00010ce0: 656e 7469 6f6e 272c 2074 6865 2063 6c61  ention', the cla
-00010cf0: 7373 2076 6172 6961 626c 6520 2768 6964  ss variable 'hid
-00010d00: 6465 6e5f 7369 7a65 2720 6d75 7374 2062  den_size' must b
-00010d10: 6520 6120 6d75 6c74 6970 6c65 2022 0a20  e a multiple ". 
-00010d20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010d30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010d40: 226f 6620 276e 756d 5f68 6561 6473 272c  "of 'num_heads',
-00010d50: 2062 7574 2067 6f74 2074 6865 2068 6964   but got the hid
-00010d60: 6465 6e5f 7369 7a65 2069 7320 7b7d 2061  den_size is {} a
-00010d70: 6e64 2074 6865 206e 756d 5f68 6561 6473  nd the num_heads
-00010d80: 2069 7320 7b7d 2e22 0a20 2020 2020 2020   is {}.".       
-00010d90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010da0: 2020 2020 2020 2020 2020 2e66 6f72 6d61            .forma
-00010db0: 7428 6869 6464 656e 5f73 697a 652c 206e  t(hidden_size, n
-00010dc0: 756d 5f68 6561 6473 2929 0a20 2020 2020  um_heads)).     
-00010dd0: 2020 2020 2020 2069 6620 6e75 6d5f 6865         if num_he
-00010de0: 6164 7320 2520 7061 7261 6c6c 656c 5f63  ads % parallel_c
-00010df0: 6f6e 6669 672e 6d6f 6465 6c5f 7061 7261  onfig.model_para
-00010e00: 6c6c 656c 2021 3d20 303a 0a20 2020 2020  llel != 0:.     
-00010e10: 2020 2020 2020 2020 2020 2072 6169 7365             raise
-00010e20: 2056 616c 7565 4572 726f 7228 2246 6f72   ValueError("For
-00010e30: 2027 4d75 6c74 6948 6561 6441 7474 656e   'MultiHeadAtten
-00010e40: 7469 6f6e 272c 2074 6865 2063 6c61 7373  tion', the class
-00010e50: 2076 6172 6961 626c 6520 276e 756d 5f68   variable 'num_h
-00010e60: 6561 6473 2720 6d75 7374 2062 6520 6120  eads' must be a 
-00010e70: 6d75 6c74 6970 6c65 206f 6620 220a 2020  multiple of ".  
-00010e80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010e90: 2020 2020 2020 2020 2020 2020 2020 2022                 "
-00010ea0: 2770 6172 616c 6c65 6c5f 636f 6e66 6967  'parallel_config
-00010eb0: 2e6d 6f64 656c 5f70 6172 616c 6c65 6c27  .model_parallel'
-00010ec0: 2c20 6275 7420 676f 7420 7468 6520 6e75  , but got the nu
-00010ed0: 6d5f 6865 6164 7320 6973 207b 7d20 220a  m_heads is {} ".
-00010ee0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010ef0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010f00: 2022 616e 6420 7468 6520 7061 7261 6c6c   "and the parall
-00010f10: 656c 5f63 6f6e 6669 672e 6d6f 6465 6c5f  el_config.model_
-00010f20: 7061 7261 6c6c 656c 2020 6973 207b 7d2e  parallel  is {}.
-00010f30: 220a 2020 2020 2020 2020 2020 2020 2020  ".              
-00010f40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00010f50: 2020 202e 666f 726d 6174 286e 756d 5f68     .format(num_h
-00010f60: 6561 6473 2c20 7061 7261 6c6c 656c 5f63  eads, parallel_c
-00010f70: 6f6e 6669 672e 6d6f 6465 6c5f 7061 7261  onfig.model_para
-00010f80: 6c6c 656c 2929 0a20 2020 2020 2020 2020  llel)).         
-00010f90: 2020 2073 656c 662e 6973 5f66 6972 7374     self.is_first
-00010fa0: 5f69 7465 7261 7469 6f6e 203d 2054 7275  _iteration = Tru
-00010fb0: 650a 2020 2020 2020 2020 2020 2020 2320  e.            # 
-00010fc0: 4f75 7470 7574 206c 6179 6572 0a20 2020  Output layer.   
-00010fd0: 2020 2020 2020 2020 2073 656c 662e 7072           self.pr
-00010fe0: 6f6a 6563 7469 6f6e 203d 204c 696e 6561  ojection = Linea
-00010ff0: 7228 696e 5f63 6861 6e6e 656c 733d 6869  r(in_channels=hi
-00011000: 6464 656e 5f73 697a 652c 0a20 2020 2020  dden_size,.     
-00011010: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011020: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011030: 6f75 745f 6368 616e 6e65 6c73 3d68 6964  out_channels=hid
-00011040: 6465 6e5f 7369 7a65 2c0a 2020 2020 2020  den_size,.      
-00011050: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011060: 2020 2020 2020 2020 2020 2020 2020 2074                 t
-00011070: 7261 6e73 706f 7365 5f62 3d46 616c 7365  ranspose_b=False
-00011080: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00011090: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000110a0: 2020 2020 2020 2063 6f6d 7075 7465 5f64         compute_d
-000110b0: 7479 7065 3d63 6f6d 7075 7465 5f64 7479  type=compute_dty
-000110c0: 7065 2c0a 2020 2020 2020 2020 2020 2020  pe,.            
-000110d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000110e0: 2020 2020 2020 2020 2070 6172 616d 5f69           param_i
-000110f0: 6e69 745f 7479 7065 3d70 6172 616d 5f69  nit_type=param_i
-00011100: 6e69 745f 7479 7065 290a 2020 2020 2020  nit_type).      
-00011110: 2020 2020 2020 7365 6c66 2e70 726f 6a65        self.proje
-00011120: 6374 696f 6e2e 7368 6172 6428 7374 7261  ction.shard(stra
-00011130: 7465 6779 5f62 6961 733d 2828 7061 7261  tegy_bias=((para
-00011140: 6c6c 656c 5f63 6f6e 6669 672e 6461 7461  llel_config.data
-00011150: 5f70 6172 616c 6c65 6c2c 2031 292c 2028  _parallel, 1), (
-00011160: 312c 2929 2c0a 2020 2020 2020 2020 2020  1,)),.          
-00011170: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011180: 2020 2020 2020 2020 7374 7261 7465 6779          strategy
-00011190: 5f6d 6174 6d75 6c3d 2828 7061 7261 6c6c  _matmul=((parall
-000111a0: 656c 5f63 6f6e 6669 672e 6461 7461 5f70  el_config.data_p
-000111b0: 6172 616c 6c65 6c2c 2070 6172 616c 6c65  arallel, paralle
-000111c0: 6c5f 636f 6e66 6967 2e6d 6f64 656c 5f70  l_config.model_p
-000111d0: 6172 616c 6c65 6c29 2c0a 2020 2020 2020  arallel),.      
-000111e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000111f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011200: 2020 2020 2020 2020 2020 2020 2028 7061               (pa
-00011210: 7261 6c6c 656c 5f63 6f6e 6669 672e 6d6f  rallel_config.mo
-00011220: 6465 6c5f 7061 7261 6c6c 656c 2c20 3129  del_parallel, 1)
-00011230: 2929 0a20 2020 2020 2020 2020 2020 2073  )).            s
-00011240: 656c 662e 7072 6f6a 6563 7469 6f6e 2e62  elf.projection.b
-00011250: 6961 732e 7061 7261 6c6c 656c 5f6f 7074  ias.parallel_opt
-00011260: 696d 697a 6572 203d 2046 616c 7365 0a20  imizer = False. 
-00011270: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-00011280: 7472 616e 7370 6f73 6520 3d20 502e 5472  transpose = P.Tr
-00011290: 616e 7370 6f73 6528 292e 7368 6172 6428  anspose().shard(
-000112a0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-000112b0: 2028 2870 6172 616c 6c65 6c5f 636f 6e66   ((parallel_conf
-000112c0: 6967 2e64 6174 615f 7061 7261 6c6c 656c  ig.data_parallel
-000112d0: 2c20 312c 2070 6172 616c 6c65 6c5f 636f  , 1, parallel_co
-000112e0: 6e66 6967 2e6d 6f64 656c 5f70 6172 616c  nfig.model_paral
-000112f0: 6c65 6c2c 2031 292c 2929 0a20 2020 2020  lel, 1),)).     
-00011300: 2020 2020 2020 2073 656c 662e 6d65 7267         self.merg
-00011310: 6572 5f68 6561 645f 7472 616e 7370 6f73  er_head_transpos
-00011320: 6520 3d20 502e 5472 616e 7370 6f73 6528  e = P.Transpose(
-00011330: 292e 7368 6172 6428 0a20 2020 2020 2020  ).shard(.       
-00011340: 2020 2020 2020 2020 2028 2870 6172 616c           ((paral
-00011350: 6c65 6c5f 636f 6e66 6967 2e64 6174 615f  lel_config.data_
-00011360: 7061 7261 6c6c 656c 2c20 7061 7261 6c6c  parallel, parall
-00011370: 656c 5f63 6f6e 6669 672e 6d6f 6465 6c5f  el_config.model_
-00011380: 7061 7261 6c6c 656c 2c20 312c 2031 292c  parallel, 1, 1),
-00011390: 2929 0a20 2020 2020 2020 2020 2020 2073  )).            s
-000113a0: 656c 662e 7265 7368 6170 6520 3d20 502e  elf.reshape = P.
-000113b0: 5265 7368 6170 6528 290a 2020 2020 2020  Reshape().      
-000113c0: 2020 2020 2020 7365 6c66 2e6e 5f68 6561        self.n_hea
-000113d0: 6420 3d20 6e75 6d5f 6865 6164 730a 2020  d = num_heads.  
-000113e0: 2020 2020 2020 2020 2020 2320 656d 6265            # embe
-000113f0: 6464 696e 6720 7369 7a65 2070 6572 2068  dding size per h
-00011400: 6561 640a 2020 2020 2020 2020 2020 2020  ead.            
-00011410: 7365 6c66 2e73 697a 655f 7065 725f 6865  self.size_per_he
-00011420: 6164 203d 2068 6964 6465 6e5f 7369 7a65  ad = hidden_size
-00011430: 202f 2f20 7365 6c66 2e6e 5f68 6561 640a   // self.n_head.
-00011440: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-00011450: 2e63 6f6e 6361 745f 6b20 3d20 502e 436f  .concat_k = P.Co
-00011460: 6e63 6174 2861 7869 733d 3329 0a20 2020  ncat(axis=3).   
-00011470: 2020 2020 2020 2020 2073 656c 662e 636f           self.co
-00011480: 6e63 6174 5f76 203d 2050 2e43 6f6e 6361  ncat_v = P.Conca
-00011490: 7428 6178 6973 3d32 290a 2020 2020 2020  t(axis=2).      
-000114a0: 2020 2020 2020 7365 6c66 2e6d 756c 7469        self.multi
-000114b0: 706c 795f 6461 7461 203d 2054 656e 736f  ply_data = Tenso
-000114c0: 7228 5b0a 2020 2020 2020 2020 2020 2020  r([.            
-000114d0: 2020 2020 2d31 3030 3030 2e30 2c0a 2020      -10000.0,.  
-000114e0: 2020 2020 2020 2020 2020 5d2c 2064 7479            ], dty
-000114f0: 7065 3d73 6f66 746d 6178 5f63 6f6d 7075  pe=softmax_compu
-00011500: 7465 5f74 7970 6529 0a20 2020 2020 2020  te_type).       
-00011510: 2020 2020 2073 656c 662e 6261 7463 685f       self.batch_
-00011520: 6d61 746d 756c 203d 2050 2e42 6174 6368  matmul = P.Batch
-00011530: 4d61 744d 756c 2829 2e73 6861 7264 280a  MatMul().shard(.
-00011540: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011550: 2828 7061 7261 6c6c 656c 5f63 6f6e 6669  ((parallel_confi
-00011560: 672e 6461 7461 5f70 6172 616c 6c65 6c2c  g.data_parallel,
-00011570: 2070 6172 616c 6c65 6c5f 636f 6e66 6967   parallel_config
-00011580: 2e6d 6f64 656c 5f70 6172 616c 6c65 6c2c  .model_parallel,
-00011590: 2031 2c20 3129 2c0a 2020 2020 2020 2020   1, 1),.        
-000115a0: 2020 2020 2020 2020 2028 7061 7261 6c6c           (parall
-000115b0: 656c 5f63 6f6e 6669 672e 6461 7461 5f70  el_config.data_p
-000115c0: 6172 616c 6c65 6c2c 2070 6172 616c 6c65  arallel, paralle
-000115d0: 6c5f 636f 6e66 6967 2e6d 6f64 656c 5f70  l_config.model_p
-000115e0: 6172 616c 6c65 6c2c 2031 2c20 3129 2929  arallel, 1, 1)))
-000115f0: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-00011600: 662e 7265 616c 5f64 6976 203d 2050 2e52  f.real_div = P.R
-00011610: 6561 6c44 6976 2829 2e73 6861 7264 280a  ealDiv().shard(.
-00011620: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011630: 2828 7061 7261 6c6c 656c 5f63 6f6e 6669  ((parallel_confi
-00011640: 672e 6461 7461 5f70 6172 616c 6c65 6c2c  g.data_parallel,
-00011650: 2070 6172 616c 6c65 6c5f 636f 6e66 6967   parallel_config
-00011660: 2e6d 6f64 656c 5f70 6172 616c 6c65 6c2c  .model_parallel,
-00011670: 2031 2c20 3129 2c20 2829 2929 0a20 2020   1, 1), ())).   
-00011680: 2020 2020 2020 2020 2073 656c 662e 7375           self.su
-00011690: 625f 7361 203d 2050 2e53 7562 2829 2e73  b_sa = P.Sub().s
-000116a0: 6861 7264 280a 2020 2020 2020 2020 2020  hard(.          
-000116b0: 2020 2020 2020 2828 312c 292c 2028 7061        ((1,), (pa
-000116c0: 7261 6c6c 656c 5f63 6f6e 6669 672e 6461  rallel_config.da
-000116d0: 7461 5f70 6172 616c 6c65 6c2c 2031 2c20  ta_parallel, 1, 
-000116e0: 312c 2031 2929 290a 2020 2020 2020 2020  1, 1))).        
-000116f0: 2020 2020 7365 6c66 2e73 7562 203d 2050      self.sub = P
-00011700: 2e53 7562 2829 2e73 6861 7264 280a 2020  .Sub().shard(.  
-00011710: 2020 2020 2020 2020 2020 2020 2020 2828                ((
-00011720: 312c 292c 2028 7061 7261 6c6c 656c 5f63  1,), (parallel_c
-00011730: 6f6e 6669 672e 6461 7461 5f70 6172 616c  onfig.data_paral
-00011740: 6c65 6c2c 2031 2c20 312c 2031 2929 290a  lel, 1, 1, 1))).
-00011750: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-00011760: 2e6d 756c 203d 2050 2e4d 756c 2829 2e73  .mul = P.Mul().s
-00011770: 6861 7264 280a 2020 2020 2020 2020 2020  hard(.          
-00011780: 2020 2020 2020 2828 7061 7261 6c6c 656c        ((parallel
-00011790: 5f63 6f6e 6669 672e 6461 7461 5f70 6172  _config.data_par
-000117a0: 616c 6c65 6c2c 2031 2c20 312c 2031 292c  allel, 1, 1, 1),
-000117b0: 2028 312c 2929 290a 2020 2020 2020 2020   (1,))).        
-000117c0: 2020 2020 7365 6c66 2e61 6464 203d 2050      self.add = P
-000117d0: 2e41 6464 2829 2e73 6861 7264 280a 2020  .Add().shard(.  
-000117e0: 2020 2020 2020 2020 2020 2020 2020 2828                ((
-000117f0: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
-00011800: 6461 7461 5f70 6172 616c 6c65 6c2c 2031  data_parallel, 1
-00011810: 2c20 312c 2031 292c 0a20 2020 2020 2020  , 1, 1),.       
-00011820: 2020 2020 2020 2020 2020 2870 6172 616c            (paral
-00011830: 6c65 6c5f 636f 6e66 6967 2e64 6174 615f  lel_config.data_
-00011840: 7061 7261 6c6c 656c 2c20 7061 7261 6c6c  parallel, parall
-00011850: 656c 5f63 6f6e 6669 672e 6d6f 6465 6c5f  el_config.model_
-00011860: 7061 7261 6c6c 656c 2c20 312c 2031 2929  parallel, 1, 1))
-00011870: 290a 2020 2020 2020 2020 2020 2020 2320  ).            # 
-00011880: 4e6f 726d 616c 697a 6520 6661 6374 6f72  Normalize factor
-00011890: 2066 6f72 2061 7474 656e 7469 6f6e 2c20   for attention, 
-000118a0: 7371 7274 2864 6b29 2061 7320 7769 6465  sqrt(dk) as wide
-000118b0: 6c79 2075 7365 640a 2020 2020 2020 2020  ly used.        
-000118c0: 2020 2020 7365 6c66 2e73 6361 6c65 5f66      self.scale_f
-000118d0: 6163 746f 7220 3d20 5465 6e73 6f72 286d  actor = Tensor(m
-000118e0: 6174 682e 7371 7274 286d 6174 682e 7371  ath.sqrt(math.sq
-000118f0: 7274 2873 656c 662e 7369 7a65 5f70 6572  rt(self.size_per
-00011900: 5f68 6561 6429 2929 0a20 2020 2020 2020  _head))).       
-00011910: 2020 2020 2073 656c 662e 7573 655f 7061       self.use_pa
-00011920: 7374 203d 2075 7365 5f70 6173 740a 2020  st = use_past.  
-00011930: 2020 2020 2020 2020 2020 7365 6c66 2e64            self.d
-00011940: 726f 706f 7574 203d 2067 6574 5f64 726f  ropout = get_dro
-00011950: 706f 7574 2868 6964 6465 6e5f 6472 6f70  pout(hidden_drop
-00011960: 6f75 745f 7261 7465 290a 2020 2020 2020  out_rate).      
-00011970: 2020 2020 2020 7365 6c66 2e70 726f 625f        self.prob_
-00011980: 6472 6f70 6f75 7420 3d20 6765 745f 6472  dropout = get_dr
-00011990: 6f70 6f75 7428 6174 7465 6e74 696f 6e5f  opout(attention_
-000119a0: 6472 6f70 6f75 745f 7261 7465 290a 2020  dropout_rate).  
-000119b0: 2020 2020 2020 2020 2020 7365 6c66 2e64            self.d
-000119c0: 726f 706f 7574 2e64 726f 706f 7574 2e73  ropout.dropout.s
-000119d0: 6861 7264 2828 2870 6172 616c 6c65 6c5f  hard(((parallel_
-000119e0: 636f 6e66 6967 2e64 6174 615f 7061 7261  config.data_para
-000119f0: 6c6c 656c 2c20 3129 2c29 290a 2020 2020  llel, 1),)).    
-00011a00: 2020 2020 2020 2020 7365 6c66 2e70 726f          self.pro
-00011a10: 625f 6472 6f70 6f75 742e 6472 6f70 6f75  b_dropout.dropou
-00011a20: 742e 7368 6172 6428 0a20 2020 2020 2020  t.shard(.       
-00011a30: 2020 2020 2020 2020 2028 2870 6172 616c           ((paral
-00011a40: 6c65 6c5f 636f 6e66 6967 2e64 6174 615f  lel_config.data_
-00011a50: 7061 7261 6c6c 656c 2c20 7061 7261 6c6c  parallel, parall
-00011a60: 656c 5f63 6f6e 6669 672e 6d6f 6465 6c5f  el_config.model_
-00011a70: 7061 7261 6c6c 656c 2c20 312c 2031 292c  parallel, 1, 1),
-00011a80: 2929 0a20 2020 2020 2020 2020 2020 2073  )).            s
-00011a90: 656c 662e 736f 6674 6d61 7820 3d20 6e6e  elf.softmax = nn
-00011aa0: 2e53 6f66 746d 6178 2829 2e74 6f5f 666c  .Softmax().to_fl
-00011ab0: 6f61 7428 736f 6674 6d61 785f 636f 6d70  oat(softmax_comp
-00011ac0: 7574 655f 7479 7065 290a 2020 2020 2020  ute_type).      
-00011ad0: 2020 2020 2020 7365 6c66 2e73 6f66 746d        self.softm
-00011ae0: 6178 2e73 6f66 746d 6178 2e73 6861 7264  ax.softmax.shard
-00011af0: 2828 2870 6172 616c 6c65 6c5f 636f 6e66  (((parallel_conf
-00011b00: 6967 2e64 6174 615f 7061 7261 6c6c 656c  ig.data_parallel
-00011b10: 2c20 7061 7261 6c6c 656c 5f63 6f6e 6669  , parallel_confi
-00011b20: 672e 6d6f 6465 6c5f 7061 7261 6c6c 656c  g.model_parallel
-00011b30: 2c20 312c 2031 292c 2929 0a20 2020 2020  , 1, 1),)).     
-00011b40: 2020 2020 2020 2073 656c 662e 736f 6674         self.soft
-00011b50: 6d61 785f 3364 203d 206e 6e2e 536f 6674  max_3d = nn.Soft
-00011b60: 6d61 7828 292e 746f 5f66 6c6f 6174 2873  max().to_float(s
-00011b70: 6f66 746d 6178 5f63 6f6d 7075 7465 5f74  oftmax_compute_t
-00011b80: 7970 6529 0a20 2020 2020 2020 2020 2020  ype).           
-00011b90: 2073 656c 662e 736f 6674 6d61 785f 3364   self.softmax_3d
-00011ba0: 2e73 6f66 746d 6178 2e73 6861 7264 2828  .softmax.shard((
-00011bb0: 2870 6172 616c 6c65 6c5f 636f 6e66 6967  (parallel_config
-00011bc0: 2e64 6174 615f 7061 7261 6c6c 656c 2c20  .data_parallel, 
-00011bd0: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
-00011be0: 6d6f 6465 6c5f 7061 7261 6c6c 656c 2c20  model_parallel, 
-00011bf0: 3129 2c29 290a 2020 2020 2020 2020 2020  1),)).          
-00011c00: 2020 7365 6c66 2e73 6f66 746d 6178 5f63    self.softmax_c
-00011c10: 6173 7420 3d20 502e 4361 7374 2829 0a20  ast = P.Cast(). 
-00011c20: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-00011c30: 736f 6674 6d61 785f 7265 7368 6170 6520  softmax_reshape 
-00011c40: 3d20 502e 5265 7368 6170 6528 290a 2020  = P.Reshape().  
-00011c50: 2020 2020 2020 2020 2020 7365 6c66 2e65            self.e
-00011c60: 7870 616e 645f 6469 6d73 203d 2050 2e45  xpand_dims = P.E
-00011c70: 7870 616e 6444 696d 7328 292e 7368 6172  xpandDims().shar
-00011c80: 6428 2828 7061 7261 6c6c 656c 5f63 6f6e  d(((parallel_con
-00011c90: 6669 672e 6461 7461 5f70 6172 616c 6c65  fig.data_paralle
-00011ca0: 6c2c 2031 2c20 3129 2c29 290a 0a20 2020  l, 1, 1),))..   
-00011cb0: 2020 2020 2020 2020 2023 2051 7565 7279           # Query
-00011cc0: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-00011cd0: 662e 6465 6e73 6531 203d 204c 696e 6561  f.dense1 = Linea
-00011ce0: 7228 6869 6464 656e 5f73 697a 652c 0a20  r(hidden_size,. 
-00011cf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011d00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011d10: 6869 6464 656e 5f73 697a 652c 0a20 2020  hidden_size,.   
-00011d20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011d30: 2020 2020 2020 2020 2020 2020 2020 636f                co
-00011d40: 6d70 7574 655f 6474 7970 653d 636f 6d70  mpute_dtype=comp
-00011d50: 7574 655f 6474 7970 652c 0a20 2020 2020  ute_dtype,.     
-00011d60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011d70: 2020 2020 2020 2020 2020 2020 7061 7261              para
-00011d80: 6d5f 696e 6974 5f74 7970 653d 7061 7261  m_init_type=para
-00011d90: 6d5f 696e 6974 5f74 7970 6529 0a20 2020  m_init_type).   
-00011da0: 2020 2020 2020 2020 2073 656c 662e 6465           self.de
-00011db0: 6e73 6531 2e73 6861 7264 2873 7472 6174  nse1.shard(strat
-00011dc0: 6567 795f 6d61 746d 756c 3d28 2870 6172  egy_matmul=((par
-00011dd0: 616c 6c65 6c5f 636f 6e66 6967 2e64 6174  allel_config.dat
-00011de0: 615f 7061 7261 6c6c 656c 2c20 3129 2c0a  a_parallel, 1),.
-00011df0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011e00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011e10: 2020 2020 2020 2020 2020 2020 2020 2028                 (
-00011e20: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
-00011e30: 6d6f 6465 6c5f 7061 7261 6c6c 656c 2c20  model_parallel, 
-00011e40: 3129 292c 0a20 2020 2020 2020 2020 2020  1)),.           
-00011e50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011e60: 2020 2073 7472 6174 6567 795f 6269 6173     strategy_bias
-00011e70: 3d28 2870 6172 616c 6c65 6c5f 636f 6e66  =((parallel_conf
-00011e80: 6967 2e64 6174 615f 7061 7261 6c6c 656c  ig.data_parallel
-00011e90: 2c20 7061 7261 6c6c 656c 5f63 6f6e 6669  , parallel_confi
-00011ea0: 672e 6d6f 6465 6c5f 7061 7261 6c6c 656c  g.model_parallel
-00011eb0: 292c 0a20 2020 2020 2020 2020 2020 2020  ),.             
-00011ec0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010b00: 2020 2020 226f 6620 276e 756d 5f68 6561      "of 'num_hea
+00010b10: 6473 272c 2062 7574 2067 6f74 2074 6865  ds', but got the
+00010b20: 2068 6964 6465 6e5f 7369 7a65 2069 7320   hidden_size is 
+00010b30: 7b7d 2061 6e64 2074 6865 206e 756d 5f68  {} and the num_h
+00010b40: 6561 6473 2069 7320 7b7d 2e22 0a20 2020  eads is {}.".   
+00010b50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010b60: 2020 2020 2020 2020 2020 2020 2020 2e66                .f
+00010b70: 6f72 6d61 7428 6869 6464 656e 5f73 697a  ormat(hidden_siz
+00010b80: 652c 206e 756d 5f68 6561 6473 2929 0a20  e, num_heads)). 
+00010b90: 2020 2020 2020 2020 2020 2069 6620 6e75             if nu
+00010ba0: 6d5f 6865 6164 7320 2520 7061 7261 6c6c  m_heads % parall
+00010bb0: 656c 5f63 6f6e 6669 672e 6d6f 6465 6c5f  el_config.model_
+00010bc0: 7061 7261 6c6c 656c 2021 3d20 303a 0a20  parallel != 0:. 
+00010bd0: 2020 2020 2020 2020 2020 2020 2020 2072                 r
+00010be0: 6169 7365 2056 616c 7565 4572 726f 7228  aise ValueError(
+00010bf0: 2246 6f72 2027 4d75 6c74 6948 6561 6441  "For 'MultiHeadA
+00010c00: 7474 656e 7469 6f6e 272c 2074 6865 2063  ttention', the c
+00010c10: 6c61 7373 2076 6172 6961 626c 6520 276e  lass variable 'n
+00010c20: 756d 5f68 6561 6473 2720 6d75 7374 2062  um_heads' must b
+00010c30: 6520 6120 6d75 6c74 6970 6c65 206f 6620  e a multiple of 
+00010c40: 220a 2020 2020 2020 2020 2020 2020 2020  ".              
+00010c50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010c60: 2020 2022 2770 6172 616c 6c65 6c5f 636f     "'parallel_co
+00010c70: 6e66 6967 2e6d 6f64 656c 5f70 6172 616c  nfig.model_paral
+00010c80: 6c65 6c27 2c20 6275 7420 676f 7420 7468  lel', but got th
+00010c90: 6520 6e75 6d5f 6865 6164 7320 6973 207b  e num_heads is {
+00010ca0: 7d20 220a 2020 2020 2020 2020 2020 2020  } ".            
+00010cb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010cc0: 2020 2020 2022 616e 6420 7468 6520 7061       "and the pa
+00010cd0: 7261 6c6c 656c 5f63 6f6e 6669 672e 6d6f  rallel_config.mo
+00010ce0: 6465 6c5f 7061 7261 6c6c 656c 2020 6973  del_parallel  is
+00010cf0: 207b 7d2e 220a 2020 2020 2020 2020 2020   {}.".          
+00010d00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010d10: 2020 2020 2020 202e 666f 726d 6174 286e         .format(n
+00010d20: 756d 5f68 6561 6473 2c20 7061 7261 6c6c  um_heads, parall
+00010d30: 656c 5f63 6f6e 6669 672e 6d6f 6465 6c5f  el_config.model_
+00010d40: 7061 7261 6c6c 656c 2929 0a20 2020 2020  parallel)).     
+00010d50: 2020 2020 2020 2073 656c 662e 6973 5f66         self.is_f
+00010d60: 6972 7374 5f69 7465 7261 7469 6f6e 203d  irst_iteration =
+00010d70: 2054 7275 650a 2020 2020 2020 2020 2020   True.          
+00010d80: 2020 2320 4f75 7470 7574 206c 6179 6572    # Output layer
+00010d90: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
+00010da0: 662e 7072 6f6a 6563 7469 6f6e 203d 204c  f.projection = L
+00010db0: 696e 6561 7228 696e 5f63 6861 6e6e 656c  inear(in_channel
+00010dc0: 733d 6869 6464 656e 5f73 697a 652c 0a20  s=hidden_size,. 
+00010dd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010de0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010df0: 2020 2020 6f75 745f 6368 616e 6e65 6c73      out_channels
+00010e00: 3d68 6964 6465 6e5f 7369 7a65 2c0a 2020  =hidden_size,.  
+00010e10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010e20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010e30: 2020 2074 7261 6e73 706f 7365 5f62 3d46     transpose_b=F
+00010e40: 616c 7365 2c0a 2020 2020 2020 2020 2020  alse,.          
+00010e50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010e60: 2020 2020 2020 2020 2020 2063 6f6d 7075             compu
+00010e70: 7465 5f64 7479 7065 3d63 6f6d 7075 7465  te_dtype=compute
+00010e80: 5f64 7479 7065 2c0a 2020 2020 2020 2020  _dtype,.        
+00010e90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010ea0: 2020 2020 2020 2020 2020 2020 2070 6172               par
+00010eb0: 616d 5f69 6e69 745f 7479 7065 3d70 6172  am_init_type=par
+00010ec0: 616d 5f69 6e69 745f 7479 7065 290a 2020  am_init_type).  
+00010ed0: 2020 2020 2020 2020 2020 7365 6c66 2e70            self.p
+00010ee0: 726f 6a65 6374 696f 6e2e 7368 6172 6428  rojection.shard(
+00010ef0: 7374 7261 7465 6779 5f62 6961 733d 2828  strategy_bias=((
+00010f00: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
+00010f10: 6461 7461 5f70 6172 616c 6c65 6c2c 2031  data_parallel, 1
+00010f20: 292c 2028 312c 2929 2c0a 2020 2020 2020  ), (1,)),.      
+00010f30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010f40: 2020 2020 2020 2020 2020 2020 7374 7261              stra
+00010f50: 7465 6779 5f6d 6174 6d75 6c3d 2828 7061  tegy_matmul=((pa
+00010f60: 7261 6c6c 656c 5f63 6f6e 6669 672e 6461  rallel_config.da
+00010f70: 7461 5f70 6172 616c 6c65 6c2c 2070 6172  ta_parallel, par
+00010f80: 616c 6c65 6c5f 636f 6e66 6967 2e6d 6f64  allel_config.mod
+00010f90: 656c 5f70 6172 616c 6c65 6c29 2c0a 2020  el_parallel),.  
+00010fa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010fb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010fc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00010fd0: 2028 7061 7261 6c6c 656c 5f63 6f6e 6669   (parallel_confi
+00010fe0: 672e 6d6f 6465 6c5f 7061 7261 6c6c 656c  g.model_parallel
+00010ff0: 2c20 3129 2929 0a20 2020 2020 2020 2020  , 1))).         
+00011000: 2020 2073 656c 662e 7072 6f6a 6563 7469     self.projecti
+00011010: 6f6e 2e62 6961 732e 7061 7261 6c6c 656c  on.bias.parallel
+00011020: 5f6f 7074 696d 697a 6572 203d 2046 616c  _optimizer = Fal
+00011030: 7365 0a20 2020 2020 2020 2020 2020 2073  se.            s
+00011040: 656c 662e 7472 616e 7370 6f73 6520 3d20  elf.transpose = 
+00011050: 502e 5472 616e 7370 6f73 6528 292e 7368  P.Transpose().sh
+00011060: 6172 6428 0a20 2020 2020 2020 2020 2020  ard(.           
+00011070: 2020 2020 2028 2870 6172 616c 6c65 6c5f       ((parallel_
+00011080: 636f 6e66 6967 2e64 6174 615f 7061 7261  config.data_para
+00011090: 6c6c 656c 2c20 312c 2070 6172 616c 6c65  llel, 1, paralle
+000110a0: 6c5f 636f 6e66 6967 2e6d 6f64 656c 5f70  l_config.model_p
+000110b0: 6172 616c 6c65 6c2c 2031 292c 2929 0a20  arallel, 1),)). 
+000110c0: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+000110d0: 6d65 7267 6572 5f68 6561 645f 7472 616e  merger_head_tran
+000110e0: 7370 6f73 6520 3d20 502e 5472 616e 7370  spose = P.Transp
+000110f0: 6f73 6528 292e 7368 6172 6428 0a20 2020  ose().shard(.   
+00011100: 2020 2020 2020 2020 2020 2020 2028 2870               ((p
+00011110: 6172 616c 6c65 6c5f 636f 6e66 6967 2e64  arallel_config.d
+00011120: 6174 615f 7061 7261 6c6c 656c 2c20 7061  ata_parallel, pa
+00011130: 7261 6c6c 656c 5f63 6f6e 6669 672e 6d6f  rallel_config.mo
+00011140: 6465 6c5f 7061 7261 6c6c 656c 2c20 312c  del_parallel, 1,
+00011150: 2031 292c 2929 0a20 2020 2020 2020 2020   1),)).         
+00011160: 2020 2073 656c 662e 7265 7368 6170 6520     self.reshape 
+00011170: 3d20 502e 5265 7368 6170 6528 290a 2020  = P.Reshape().  
+00011180: 2020 2020 2020 2020 2020 7365 6c66 2e6e            self.n
+00011190: 5f68 6561 6420 3d20 6e75 6d5f 6865 6164  _head = num_head
+000111a0: 730a 2020 2020 2020 2020 2020 2020 2320  s.            # 
+000111b0: 656d 6265 6464 696e 6720 7369 7a65 2070  embedding size p
+000111c0: 6572 2068 6561 640a 2020 2020 2020 2020  er head.        
+000111d0: 2020 2020 7365 6c66 2e73 697a 655f 7065      self.size_pe
+000111e0: 725f 6865 6164 203d 2068 6964 6465 6e5f  r_head = hidden_
+000111f0: 7369 7a65 202f 2f20 7365 6c66 2e6e 5f68  size // self.n_h
+00011200: 6561 640a 2020 2020 2020 2020 2020 2020  ead.            
+00011210: 7365 6c66 2e63 6f6e 6361 745f 6b20 3d20  self.concat_k = 
+00011220: 502e 436f 6e63 6174 2861 7869 733d 3329  P.Concat(axis=3)
+00011230: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
+00011240: 662e 636f 6e63 6174 5f76 203d 2050 2e43  f.concat_v = P.C
+00011250: 6f6e 6361 7428 6178 6973 3d32 290a 2020  oncat(axis=2).  
+00011260: 2020 2020 2020 2020 2020 7365 6c66 2e6d            self.m
+00011270: 756c 7469 706c 795f 6461 7461 203d 2054  ultiply_data = T
+00011280: 656e 736f 7228 5b0a 2020 2020 2020 2020  ensor([.        
+00011290: 2020 2020 2020 2020 2d31 3030 3030 2e30          -10000.0
+000112a0: 2c0a 2020 2020 2020 2020 2020 2020 5d2c  ,.            ],
+000112b0: 2064 7479 7065 3d73 6f66 746d 6178 5f63   dtype=softmax_c
+000112c0: 6f6d 7075 7465 5f74 7970 6529 0a20 2020  ompute_type).   
+000112d0: 2020 2020 2020 2020 2073 656c 662e 6261           self.ba
+000112e0: 7463 685f 6d61 746d 756c 203d 2050 2e42  tch_matmul = P.B
+000112f0: 6174 6368 4d61 744d 756c 2829 2e73 6861  atchMatMul().sha
+00011300: 7264 280a 2020 2020 2020 2020 2020 2020  rd(.            
+00011310: 2020 2020 2828 7061 7261 6c6c 656c 5f63      ((parallel_c
+00011320: 6f6e 6669 672e 6461 7461 5f70 6172 616c  onfig.data_paral
+00011330: 6c65 6c2c 2070 6172 616c 6c65 6c5f 636f  lel, parallel_co
+00011340: 6e66 6967 2e6d 6f64 656c 5f70 6172 616c  nfig.model_paral
+00011350: 6c65 6c2c 2031 2c20 3129 2c0a 2020 2020  lel, 1, 1),.    
+00011360: 2020 2020 2020 2020 2020 2020 2028 7061               (pa
+00011370: 7261 6c6c 656c 5f63 6f6e 6669 672e 6461  rallel_config.da
+00011380: 7461 5f70 6172 616c 6c65 6c2c 2070 6172  ta_parallel, par
+00011390: 616c 6c65 6c5f 636f 6e66 6967 2e6d 6f64  allel_config.mod
+000113a0: 656c 5f70 6172 616c 6c65 6c2c 2031 2c20  el_parallel, 1, 
+000113b0: 3129 2929 0a20 2020 2020 2020 2020 2020  1))).           
+000113c0: 2073 656c 662e 7265 616c 5f64 6976 203d   self.real_div =
+000113d0: 2050 2e52 6561 6c44 6976 2829 2e73 6861   P.RealDiv().sha
+000113e0: 7264 280a 2020 2020 2020 2020 2020 2020  rd(.            
+000113f0: 2020 2020 2828 7061 7261 6c6c 656c 5f63      ((parallel_c
+00011400: 6f6e 6669 672e 6461 7461 5f70 6172 616c  onfig.data_paral
+00011410: 6c65 6c2c 2070 6172 616c 6c65 6c5f 636f  lel, parallel_co
+00011420: 6e66 6967 2e6d 6f64 656c 5f70 6172 616c  nfig.model_paral
+00011430: 6c65 6c2c 2031 2c20 3129 2c20 2829 2929  lel, 1, 1), ()))
+00011440: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
+00011450: 662e 7375 625f 7361 203d 2050 2e53 7562  f.sub_sa = P.Sub
+00011460: 2829 2e73 6861 7264 280a 2020 2020 2020  ().shard(.      
+00011470: 2020 2020 2020 2020 2020 2828 312c 292c            ((1,),
+00011480: 2028 7061 7261 6c6c 656c 5f63 6f6e 6669   (parallel_confi
+00011490: 672e 6461 7461 5f70 6172 616c 6c65 6c2c  g.data_parallel,
+000114a0: 2031 2c20 312c 2031 2929 290a 2020 2020   1, 1, 1))).    
+000114b0: 2020 2020 2020 2020 7365 6c66 2e73 7562          self.sub
+000114c0: 203d 2050 2e53 7562 2829 2e73 6861 7264   = P.Sub().shard
+000114d0: 280a 2020 2020 2020 2020 2020 2020 2020  (.              
+000114e0: 2020 2828 312c 292c 2028 7061 7261 6c6c    ((1,), (parall
+000114f0: 656c 5f63 6f6e 6669 672e 6461 7461 5f70  el_config.data_p
+00011500: 6172 616c 6c65 6c2c 2031 2c20 312c 2031  arallel, 1, 1, 1
+00011510: 2929 290a 2020 2020 2020 2020 2020 2020  ))).            
+00011520: 7365 6c66 2e6d 756c 203d 2050 2e4d 756c  self.mul = P.Mul
+00011530: 2829 2e73 6861 7264 280a 2020 2020 2020  ().shard(.      
+00011540: 2020 2020 2020 2020 2020 2828 7061 7261            ((para
+00011550: 6c6c 656c 5f63 6f6e 6669 672e 6461 7461  llel_config.data
+00011560: 5f70 6172 616c 6c65 6c2c 2031 2c20 312c  _parallel, 1, 1,
+00011570: 2031 292c 2028 312c 2929 290a 2020 2020   1), (1,))).    
+00011580: 2020 2020 2020 2020 7365 6c66 2e61 6464          self.add
+00011590: 203d 2050 2e41 6464 2829 2e73 6861 7264   = P.Add().shard
+000115a0: 280a 2020 2020 2020 2020 2020 2020 2020  (.              
+000115b0: 2020 2828 7061 7261 6c6c 656c 5f63 6f6e    ((parallel_con
+000115c0: 6669 672e 6461 7461 5f70 6172 616c 6c65  fig.data_paralle
+000115d0: 6c2c 2031 2c20 312c 2031 292c 0a20 2020  l, 1, 1, 1),.   
+000115e0: 2020 2020 2020 2020 2020 2020 2020 2870                (p
+000115f0: 6172 616c 6c65 6c5f 636f 6e66 6967 2e64  arallel_config.d
+00011600: 6174 615f 7061 7261 6c6c 656c 2c20 7061  ata_parallel, pa
+00011610: 7261 6c6c 656c 5f63 6f6e 6669 672e 6d6f  rallel_config.mo
+00011620: 6465 6c5f 7061 7261 6c6c 656c 2c20 312c  del_parallel, 1,
+00011630: 2031 2929 290a 2020 2020 2020 2020 2020   1))).          
+00011640: 2020 2320 4e6f 726d 616c 697a 6520 6661    # Normalize fa
+00011650: 6374 6f72 2066 6f72 2061 7474 656e 7469  ctor for attenti
+00011660: 6f6e 2c20 7371 7274 2864 6b29 2061 7320  on, sqrt(dk) as 
+00011670: 7769 6465 6c79 2075 7365 640a 2020 2020  widely used.    
+00011680: 2020 2020 2020 2020 7365 6c66 2e73 6361          self.sca
+00011690: 6c65 5f66 6163 746f 7220 3d20 5465 6e73  le_factor = Tens
+000116a0: 6f72 286d 6174 682e 7371 7274 286d 6174  or(math.sqrt(mat
+000116b0: 682e 7371 7274 2873 656c 662e 7369 7a65  h.sqrt(self.size
+000116c0: 5f70 6572 5f68 6561 6429 2929 0a20 2020  _per_head))).   
+000116d0: 2020 2020 2020 2020 2073 656c 662e 7573           self.us
+000116e0: 655f 7061 7374 203d 2075 7365 5f70 6173  e_past = use_pas
+000116f0: 740a 2020 2020 2020 2020 2020 2020 7365  t.            se
+00011700: 6c66 2e64 726f 706f 7574 203d 2067 6574  lf.dropout = get
+00011710: 5f64 726f 706f 7574 2868 6964 6465 6e5f  _dropout(hidden_
+00011720: 6472 6f70 6f75 745f 7261 7465 290a 2020  dropout_rate).  
+00011730: 2020 2020 2020 2020 2020 7365 6c66 2e70            self.p
+00011740: 726f 625f 6472 6f70 6f75 7420 3d20 6765  rob_dropout = ge
+00011750: 745f 6472 6f70 6f75 7428 6174 7465 6e74  t_dropout(attent
+00011760: 696f 6e5f 6472 6f70 6f75 745f 7261 7465  ion_dropout_rate
+00011770: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
+00011780: 6c66 2e64 726f 706f 7574 2e64 726f 706f  lf.dropout.dropo
+00011790: 7574 2e73 6861 7264 2828 2870 6172 616c  ut.shard(((paral
+000117a0: 6c65 6c5f 636f 6e66 6967 2e64 6174 615f  lel_config.data_
+000117b0: 7061 7261 6c6c 656c 2c20 3129 2c29 290a  parallel, 1),)).
+000117c0: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+000117d0: 2e70 726f 625f 6472 6f70 6f75 742e 6472  .prob_dropout.dr
+000117e0: 6f70 6f75 742e 7368 6172 6428 0a20 2020  opout.shard(.   
+000117f0: 2020 2020 2020 2020 2020 2020 2028 2870               ((p
+00011800: 6172 616c 6c65 6c5f 636f 6e66 6967 2e64  arallel_config.d
+00011810: 6174 615f 7061 7261 6c6c 656c 2c20 7061  ata_parallel, pa
+00011820: 7261 6c6c 656c 5f63 6f6e 6669 672e 6d6f  rallel_config.mo
+00011830: 6465 6c5f 7061 7261 6c6c 656c 2c20 312c  del_parallel, 1,
+00011840: 2031 292c 2929 0a20 2020 2020 2020 2020   1),)).         
+00011850: 2020 2073 656c 662e 736f 6674 6d61 7820     self.softmax 
+00011860: 3d20 6e6e 2e53 6f66 746d 6178 2829 2e74  = nn.Softmax().t
+00011870: 6f5f 666c 6f61 7428 736f 6674 6d61 785f  o_float(softmax_
+00011880: 636f 6d70 7574 655f 7479 7065 290a 2020  compute_type).  
+00011890: 2020 2020 2020 2020 2020 7365 6c66 2e73            self.s
+000118a0: 6f66 746d 6178 2e73 6f66 746d 6178 2e73  oftmax.softmax.s
+000118b0: 6861 7264 2828 2870 6172 616c 6c65 6c5f  hard(((parallel_
+000118c0: 636f 6e66 6967 2e64 6174 615f 7061 7261  config.data_para
+000118d0: 6c6c 656c 2c20 7061 7261 6c6c 656c 5f63  llel, parallel_c
+000118e0: 6f6e 6669 672e 6d6f 6465 6c5f 7061 7261  onfig.model_para
+000118f0: 6c6c 656c 2c20 312c 2031 292c 2929 0a20  llel, 1, 1),)). 
+00011900: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+00011910: 736f 6674 6d61 785f 3364 203d 206e 6e2e  softmax_3d = nn.
+00011920: 536f 6674 6d61 7828 292e 746f 5f66 6c6f  Softmax().to_flo
+00011930: 6174 2873 6f66 746d 6178 5f63 6f6d 7075  at(softmax_compu
+00011940: 7465 5f74 7970 6529 0a20 2020 2020 2020  te_type).       
+00011950: 2020 2020 2073 656c 662e 736f 6674 6d61       self.softma
+00011960: 785f 3364 2e73 6f66 746d 6178 2e73 6861  x_3d.softmax.sha
+00011970: 7264 2828 2870 6172 616c 6c65 6c5f 636f  rd(((parallel_co
+00011980: 6e66 6967 2e64 6174 615f 7061 7261 6c6c  nfig.data_parall
+00011990: 656c 2c20 7061 7261 6c6c 656c 5f63 6f6e  el, parallel_con
+000119a0: 6669 672e 6d6f 6465 6c5f 7061 7261 6c6c  fig.model_parall
+000119b0: 656c 2c20 3129 2c29 290a 2020 2020 2020  el, 1),)).      
+000119c0: 2020 2020 2020 7365 6c66 2e73 6f66 746d        self.softm
+000119d0: 6178 5f63 6173 7420 3d20 502e 4361 7374  ax_cast = P.Cast
+000119e0: 2829 0a20 2020 2020 2020 2020 2020 2073  ().            s
+000119f0: 656c 662e 736f 6674 6d61 785f 7265 7368  elf.softmax_resh
+00011a00: 6170 6520 3d20 502e 5265 7368 6170 6528  ape = P.Reshape(
+00011a10: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
+00011a20: 6c66 2e65 7870 616e 645f 6469 6d73 203d  lf.expand_dims =
+00011a30: 2050 2e45 7870 616e 6444 696d 7328 292e   P.ExpandDims().
+00011a40: 7368 6172 6428 2828 7061 7261 6c6c 656c  shard(((parallel
+00011a50: 5f63 6f6e 6669 672e 6461 7461 5f70 6172  _config.data_par
+00011a60: 616c 6c65 6c2c 2031 2c20 3129 2c29 290a  allel, 1, 1),)).
+00011a70: 0a20 2020 2020 2020 2020 2020 2023 2051  .            # Q
+00011a80: 7565 7279 0a20 2020 2020 2020 2020 2020  uery.           
+00011a90: 2073 656c 662e 6465 6e73 6531 203d 204c   self.dense1 = L
+00011aa0: 696e 6561 7228 6869 6464 656e 5f73 697a  inear(hidden_siz
+00011ab0: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
+00011ac0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011ad0: 2020 2020 6869 6464 656e 5f73 697a 652c      hidden_size,
+00011ae0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00011af0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011b00: 2020 636f 6d70 7574 655f 6474 7970 653d    compute_dtype=
+00011b10: 636f 6d70 7574 655f 6474 7970 652c 0a20  compute_dtype,. 
+00011b20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011b30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011b40: 7061 7261 6d5f 696e 6974 5f74 7970 653d  param_init_type=
+00011b50: 7061 7261 6d5f 696e 6974 5f74 7970 6529  param_init_type)
+00011b60: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
+00011b70: 662e 6465 6e73 6531 2e73 6861 7264 2873  f.dense1.shard(s
+00011b80: 7472 6174 6567 795f 6d61 746d 756c 3d28  trategy_matmul=(
+00011b90: 2870 6172 616c 6c65 6c5f 636f 6e66 6967  (parallel_config
+00011ba0: 2e64 6174 615f 7061 7261 6c6c 656c 2c20  .data_parallel, 
+00011bb0: 3129 2c0a 2020 2020 2020 2020 2020 2020  1),.            
+00011bc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011bd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011be0: 2020 2028 7061 7261 6c6c 656c 5f63 6f6e     (parallel_con
+00011bf0: 6669 672e 6d6f 6465 6c5f 7061 7261 6c6c  fig.model_parall
+00011c00: 656c 2c20 3129 292c 0a20 2020 2020 2020  el, 1)),.       
+00011c10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011c20: 2020 2020 2020 2073 7472 6174 6567 795f         strategy_
+00011c30: 6269 6173 3d28 2870 6172 616c 6c65 6c5f  bias=((parallel_
+00011c40: 636f 6e66 6967 2e64 6174 615f 7061 7261  config.data_para
+00011c50: 6c6c 656c 2c20 7061 7261 6c6c 656c 5f63  llel, parallel_c
+00011c60: 6f6e 6669 672e 6d6f 6465 6c5f 7061 7261  onfig.model_para
+00011c70: 6c6c 656c 292c 0a20 2020 2020 2020 2020  llel),.         
+00011c80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011c90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011ca0: 2020 2020 2870 6172 616c 6c65 6c5f 636f      (parallel_co
+00011cb0: 6e66 6967 2e6d 6f64 656c 5f70 6172 616c  nfig.model_paral
+00011cc0: 6c65 6c2c 2929 290a 2020 2020 2020 2020  lel,))).        
+00011cd0: 2020 2020 2320 4b65 790a 2020 2020 2020      # Key.      
+00011ce0: 2020 2020 2020 7365 6c66 2e64 656e 7365        self.dense
+00011cf0: 3220 3d20 4c69 6e65 6172 2868 6964 6465  2 = Linear(hidde
+00011d00: 6e5f 7369 7a65 2c0a 2020 2020 2020 2020  n_size,.        
+00011d10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011d20: 2020 2020 2020 2020 2068 6964 6465 6e5f           hidden_
+00011d30: 7369 7a65 2c0a 2020 2020 2020 2020 2020  size,.          
+00011d40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011d50: 2020 2020 2020 2063 6f6d 7075 7465 5f64         compute_d
+00011d60: 7479 7065 3d63 6f6d 7075 7465 5f64 7479  type=compute_dty
+00011d70: 7065 2c0a 2020 2020 2020 2020 2020 2020  pe,.            
+00011d80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011d90: 2020 2020 2070 6172 616d 5f69 6e69 745f       param_init_
+00011da0: 7479 7065 3d70 6172 616d 5f69 6e69 745f  type=param_init_
+00011db0: 7479 7065 290a 2020 2020 2020 2020 2020  type).          
+00011dc0: 2020 7365 6c66 2e64 656e 7365 322e 7368    self.dense2.sh
+00011dd0: 6172 6428 7374 7261 7465 6779 5f6d 6174  ard(strategy_mat
+00011de0: 6d75 6c3d 2828 7061 7261 6c6c 656c 5f63  mul=((parallel_c
+00011df0: 6f6e 6669 672e 6461 7461 5f70 6172 616c  onfig.data_paral
+00011e00: 6c65 6c2c 2031 292c 0a20 2020 2020 2020  lel, 1),.       
+00011e10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011e20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011e30: 2020 2020 2020 2020 2870 6172 616c 6c65          (paralle
+00011e40: 6c5f 636f 6e66 6967 2e6d 6f64 656c 5f70  l_config.model_p
+00011e50: 6172 616c 6c65 6c2c 2031 2929 2c0a 2020  arallel, 1)),.  
+00011e60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011e70: 2020 2020 2020 2020 2020 2020 7374 7261              stra
+00011e80: 7465 6779 5f62 6961 733d 2828 7061 7261  tegy_bias=((para
+00011e90: 6c6c 656c 5f63 6f6e 6669 672e 6461 7461  llel_config.data
+00011ea0: 5f70 6172 616c 6c65 6c2c 2070 6172 616c  _parallel, paral
+00011eb0: 6c65 6c5f 636f 6e66 6967 2e6d 6f64 656c  lel_config.model
+00011ec0: 5f70 6172 616c 6c65 6c29 2c0a 2020 2020  _parallel),.    
 00011ed0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011ee0: 2870 6172 616c 6c65 6c5f 636f 6e66 6967  (parallel_config
-00011ef0: 2e6d 6f64 656c 5f70 6172 616c 6c65 6c2c  .model_parallel,
-00011f00: 2929 290a 2020 2020 2020 2020 2020 2020  ))).            
-00011f10: 2320 4b65 790a 2020 2020 2020 2020 2020  # Key.          
-00011f20: 2020 7365 6c66 2e64 656e 7365 3220 3d20    self.dense2 = 
-00011f30: 4c69 6e65 6172 2868 6964 6465 6e5f 7369  Linear(hidden_si
-00011f40: 7a65 2c0a 2020 2020 2020 2020 2020 2020  ze,.            
-00011f50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011f60: 2020 2020 2068 6964 6465 6e5f 7369 7a65       hidden_size
-00011f70: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00011f80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011f90: 2020 2063 6f6d 7075 7465 5f64 7479 7065     compute_dtype
-00011fa0: 3d63 6f6d 7075 7465 5f64 7479 7065 2c0a  =compute_dtype,.
-00011fb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011fc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00011fd0: 2070 6172 616d 5f69 6e69 745f 7479 7065   param_init_type
-00011fe0: 3d70 6172 616d 5f69 6e69 745f 7479 7065  =param_init_type
-00011ff0: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
-00012000: 6c66 2e64 656e 7365 322e 7368 6172 6428  lf.dense2.shard(
-00012010: 7374 7261 7465 6779 5f6d 6174 6d75 6c3d  strategy_matmul=
-00012020: 2828 7061 7261 6c6c 656c 5f63 6f6e 6669  ((parallel_confi
-00012030: 672e 6461 7461 5f70 6172 616c 6c65 6c2c  g.data_parallel,
-00012040: 2031 292c 0a20 2020 2020 2020 2020 2020   1),.           
-00012050: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012060: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012070: 2020 2020 2870 6172 616c 6c65 6c5f 636f      (parallel_co
-00012080: 6e66 6967 2e6d 6f64 656c 5f70 6172 616c  nfig.model_paral
-00012090: 6c65 6c2c 2031 2929 2c0a 2020 2020 2020  lel, 1)),.      
-000120a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000120b0: 2020 2020 2020 2020 7374 7261 7465 6779          strategy
-000120c0: 5f62 6961 733d 2828 7061 7261 6c6c 656c  _bias=((parallel
-000120d0: 5f63 6f6e 6669 672e 6461 7461 5f70 6172  _config.data_par
-000120e0: 616c 6c65 6c2c 2070 6172 616c 6c65 6c5f  allel, parallel_
-000120f0: 636f 6e66 6967 2e6d 6f64 656c 5f70 6172  config.model_par
-00012100: 616c 6c65 6c29 2c0a 2020 2020 2020 2020  allel),.        
-00012110: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012120: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012130: 2020 2020 2028 7061 7261 6c6c 656c 5f63       (parallel_c
-00012140: 6f6e 6669 672e 6d6f 6465 6c5f 7061 7261  onfig.model_para
-00012150: 6c6c 656c 2c29 2929 0a0a 2020 2020 2020  llel,)))..      
-00012160: 2020 2020 2020 2320 5661 6c75 650a 2020        # Value.  
-00012170: 2020 2020 2020 2020 2020 7365 6c66 2e64            self.d
-00012180: 656e 7365 3320 3d20 4c69 6e65 6172 2868  ense3 = Linear(h
-00012190: 6964 6465 6e5f 7369 7a65 2c0a 2020 2020  idden_size,.    
-000121a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000121b0: 2020 2020 2020 2020 2020 2020 2068 6964               hid
-000121c0: 6465 6e5f 7369 7a65 2c0a 2020 2020 2020  den_size,.      
-000121d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000121e0: 2020 2020 2020 2020 2020 2063 6f6d 7075             compu
-000121f0: 7465 5f64 7479 7065 3d63 6f6d 7075 7465  te_dtype=compute
-00012200: 5f64 7479 7065 2c0a 2020 2020 2020 2020  _dtype,.        
-00012210: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012220: 2020 2020 2020 2020 2070 6172 616d 5f69           param_i
-00012230: 6e69 745f 7479 7065 3d70 6172 616d 5f69  nit_type=param_i
-00012240: 6e69 745f 7479 7065 290a 2020 2020 2020  nit_type).      
-00012250: 2020 2020 2020 7365 6c66 2e64 656e 7365        self.dense
-00012260: 332e 7368 6172 6428 7374 7261 7465 6779  3.shard(strategy
-00012270: 5f6d 6174 6d75 6c3d 2828 7061 7261 6c6c  _matmul=((parall
-00012280: 656c 5f63 6f6e 6669 672e 6461 7461 5f70  el_config.data_p
-00012290: 6172 616c 6c65 6c2c 2031 292c 0a20 2020  arallel, 1),.   
-000122a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000122b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000122c0: 2020 2020 2020 2020 2020 2020 2870 6172              (par
-000122d0: 616c 6c65 6c5f 636f 6e66 6967 2e6d 6f64  allel_config.mod
-000122e0: 656c 5f70 6172 616c 6c65 6c2c 2031 2929  el_parallel, 1))
-000122f0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00012300: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012310: 7374 7261 7465 6779 5f62 6961 733d 2828  strategy_bias=((
-00012320: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
-00012330: 6461 7461 5f70 6172 616c 6c65 6c2c 2070  data_parallel, p
-00012340: 6172 616c 6c65 6c5f 636f 6e66 6967 2e6d  arallel_config.m
-00012350: 6f64 656c 5f70 6172 616c 6c65 6c29 2c0a  odel_parallel),.
-00012360: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012370: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012380: 2020 2020 2020 2020 2020 2020 2028 7061               (pa
-00012390: 7261 6c6c 656c 5f63 6f6e 6669 672e 6d6f  rallel_config.mo
-000123a0: 6465 6c5f 7061 7261 6c6c 656c 2c29 2929  del_parallel,)))
-000123b0: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-000123c0: 662e 6474 7970 6520 3d20 636f 6d70 7574  f.dtype = comput
-000123d0: 655f 6474 7970 650a 2020 2020 2020 2020  e_dtype.        
-000123e0: 2020 2020 7365 6c66 2e73 6f66 746d 6178      self.softmax
-000123f0: 5f64 7479 7065 203d 2073 6f66 746d 6178  _dtype = softmax
-00012400: 5f63 6f6d 7075 7465 5f74 7970 650a 2020  _compute_type.  
-00012410: 2020 2020 2020 2020 2020 6966 2073 656c            if sel
-00012420: 662e 7573 655f 7061 7374 3a0a 2020 2020  f.use_past:.    
-00012430: 2020 2020 2020 2020 2020 2020 2320 6f70              # op
-00012440: 6572 6174 6f72 7320 7573 6564 2066 6f72  erators used for
-00012450: 2073 7461 7465 2072 6575 7365 0a20 2020   state reuse.   
-00012460: 2020 2020 2020 2020 2020 2020 2073 6571               seq
-00012470: 5f72 616e 6765 203d 206e 702e 6172 616e  _range = np.aran
-00012480: 6765 2873 7263 5f73 6571 5f6c 656e 6774  ge(src_seq_lengt
-00012490: 6829 2e72 6573 6861 7065 2831 2c20 312c  h).reshape(1, 1,
-000124a0: 202d 3129 0a20 2020 2020 2020 2020 2020   -1).           
-000124b0: 2020 2020 2073 656c 662e 7261 6e67 6520       self.range 
-000124c0: 3d20 5465 6e73 6f72 286e 702e 7469 6c65  = Tensor(np.tile
-000124d0: 2873 6571 5f72 616e 6765 2c20 2862 6174  (seq_range, (bat
-000124e0: 6368 5f73 697a 652c 2031 2c20 3129 292c  ch_size, 1, 1)),
-000124f0: 206d 7374 7970 652e 696e 7433 3229 0a20   mstype.int32). 
-00012500: 2020 2020 2020 2020 2020 2020 2020 2073                 s
-00012510: 656c 662e 7365 715f 6c65 6e67 7468 203d  elf.seq_length =
-00012520: 2073 7263 5f73 6571 5f6c 656e 6774 680a   src_seq_length.
-00012530: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012540: 7365 6c66 2e61 7474 656e 7469 6f6e 5f6d  self.attention_m
-00012550: 6173 6b20 3d20 5465 6e73 6f72 286e 702e  ask = Tensor(np.
-00012560: 7472 696c 286e 702e 6f6e 6573 2873 6861  tril(np.ones(sha
-00012570: 7065 3d28 7365 6c66 2e73 6571 5f6c 656e  pe=(self.seq_len
-00012580: 6774 682c 2073 656c 662e 7365 715f 6c65  gth, self.seq_le
-00012590: 6e67 7468 2929 292c 206d 7374 7970 652e  ngth))), mstype.
-000125a0: 696e 7433 3229 0a20 2020 2020 2020 2020  int32).         
-000125b0: 2020 2020 2020 2073 656c 662e 736c 6963         self.slic
-000125c0: 6520 3d20 502e 5374 7269 6465 6453 6c69  e = P.StridedSli
-000125d0: 6365 2829 2e73 6861 7264 2828 2831 2c20  ce().shard(((1, 
-000125e0: 312c 2031 2c20 3129 2c29 290a 2020 2020  1, 1, 1),)).    
-000125f0: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-00012600: 2e6e 6f74 5f65 7175 616c 203d 2050 2e4e  .not_equal = P.N
-00012610: 6f74 4571 7561 6c28 292e 7368 6172 6428  otEqual().shard(
-00012620: 2828 312c 2031 2c20 312c 2031 292c 2028  ((1, 1, 1, 1), (
-00012630: 2929 290a 2020 2020 2020 2020 2020 2020  ))).            
-00012640: 2020 2020 7365 6c66 2e72 6564 7563 6573      self.reduces
-00012650: 756d 203d 2050 2e52 6564 7563 6553 756d  um = P.ReduceSum
-00012660: 2829 2e73 6861 7264 2828 2831 2c20 312c  ().shard(((1, 1,
-00012670: 2031 2c20 3129 2c29 290a 2020 2020 2020   1, 1),)).      
-00012680: 2020 2020 2020 2020 2020 7365 6c66 2e65            self.e
-00012690: 7870 616e 645f 6469 6d73 203d 2050 2e45  xpand_dims = P.E
-000126a0: 7870 616e 6444 696d 7328 292e 7368 6172  xpandDims().shar
-000126b0: 6428 2828 312c 2031 2c20 3129 2c29 290a  d(((1, 1, 1),)).
-000126c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000126d0: 7365 6c66 2e74 656e 736f 725f 6c65 203d  self.tensor_le =
-000126e0: 2050 2e4c 6573 7345 7175 616c 2829 2e73   P.LessEqual().s
-000126f0: 6861 7264 2828 2831 2c20 312c 2031 292c  hard(((1, 1, 1),
-00012700: 2028 312c 2031 2c20 3129 2929 0a20 2020   (1, 1, 1))).   
-00012710: 2020 2020 2020 2020 2020 2020 2073 656c               sel
-00012720: 662e 6164 6420 3d20 502e 4164 6428 292e  f.add = P.Add().
-00012730: 7368 6172 6428 2828 312c 2031 2c20 312c  shard(((1, 1, 1,
-00012740: 2031 292c 2028 312c 2031 2c20 312c 2031   1), (1, 1, 1, 1
-00012750: 2929 290a 2020 2020 2020 2020 2020 2020  ))).            
-00012760: 2020 2020 7365 6c66 2e65 7175 616c 203d      self.equal =
-00012770: 2050 2e45 7175 616c 2829 2e73 6861 7264   P.Equal().shard
-00012780: 2828 2831 2c20 312c 2031 292c 2028 312c  (((1, 1, 1), (1,
-00012790: 2031 2c20 3129 2929 0a20 2020 2020 2020   1, 1))).       
-000127a0: 2020 2020 2020 2020 2073 656c 662e 7375           self.su
-000127b0: 6231 203d 2050 2e53 7562 2829 2e73 6861  b1 = P.Sub().sha
-000127c0: 7264 2828 2831 2c29 2c20 2829 2929 0a20  rd(((1,), ())). 
-000127d0: 2020 2020 2020 2020 2020 2020 2020 2073                 s
-000127e0: 656c 662e 7469 6c65 203d 2050 2e54 696c  elf.tile = P.Til
-000127f0: 6528 292e 7368 6172 6428 2828 312c 2031  e().shard(((1, 1
-00012800: 2c20 312c 2031 292c 2929 0a20 2020 2020  , 1, 1),)).     
-00012810: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-00012820: 6c65 7373 203d 2050 2e4c 6573 7328 292e  less = P.Less().
-00012830: 7368 6172 6428 2828 312c 2031 2c20 3129  shard(((1, 1, 1)
-00012840: 2c20 2831 2c20 312c 2031 2929 290a 2020  , (1, 1, 1))).  
-00012850: 2020 2020 2020 2020 2020 2020 2020 7365                se
-00012860: 6c66 2e6d 756c 3120 3d20 502e 4d75 6c28  lf.mul1 = P.Mul(
-00012870: 292e 7368 6172 6428 2828 312c 2031 2c20  ).shard(((1, 1, 
-00012880: 312c 2031 292c 2028 312c 2031 2c20 312c  1, 1), (1, 1, 1,
-00012890: 2031 2929 290a 0a20 2020 2020 2020 2020   1)))..         
-000128a0: 2020 2069 6620 7061 7261 6c6c 656c 5f63     if parallel_c
-000128b0: 6f6e 6669 672e 7573 655f 7365 715f 7061  onfig.use_seq_pa
-000128c0: 7261 6c6c 656c 3a0a 2020 2020 2020 2020  rallel:.        
-000128d0: 2020 2020 2020 2020 7365 6c66 2e64 726f          self.dro
-000128e0: 706f 7574 2e64 726f 706f 7574 2e73 6861  pout.dropout.sha
-000128f0: 7264 2828 2870 6172 616c 6c65 6c5f 636f  rd(((parallel_co
-00012900: 6e66 6967 2e64 6174 615f 7061 7261 6c6c  nfig.data_parall
-00012910: 656c 202a 2070 6172 616c 6c65 6c5f 636f  el * parallel_co
-00012920: 6e66 6967 2e6d 6f64 656c 5f70 6172 616c  nfig.model_paral
-00012930: 6c65 6c2c 2031 292c 2929 0a20 2020 2020  lel, 1),)).     
-00012940: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-00012950: 7072 6f6a 6563 7469 6f6e 2e73 6861 7264  projection.shard
-00012960: 280a 2020 2020 2020 2020 2020 2020 2020  (.              
-00012970: 2020 2020 2020 7374 7261 7465 6779 5f62        strategy_b
-00012980: 6961 733d 2828 7061 7261 6c6c 656c 5f63  ias=((parallel_c
-00012990: 6f6e 6669 672e 6461 7461 5f70 6172 616c  onfig.data_paral
-000129a0: 6c65 6c20 2a20 7061 7261 6c6c 656c 5f63  lel * parallel_c
-000129b0: 6f6e 6669 672e 6d6f 6465 6c5f 7061 7261  onfig.model_para
-000129c0: 6c6c 656c 2c20 3129 2c20 2831 2c29 292c  llel, 1), (1,)),
-000129d0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-000129e0: 2020 2020 2073 7472 6174 6567 795f 6d61       strategy_ma
-000129f0: 746d 756c 3d28 2870 6172 616c 6c65 6c5f  tmul=((parallel_
-00012a00: 636f 6e66 6967 2e64 6174 615f 7061 7261  config.data_para
-00012a10: 6c6c 656c 2c20 7061 7261 6c6c 656c 5f63  llel, parallel_c
-00012a20: 6f6e 6669 672e 6d6f 6465 6c5f 7061 7261  onfig.model_para
-00012a30: 6c6c 656c 292c 0a20 2020 2020 2020 2020  llel),.         
-00012a40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012a50: 2020 2020 2020 2020 2020 2020 2870 6172              (par
-00012a60: 616c 6c65 6c5f 636f 6e66 6967 2e6d 6f64  allel_config.mod
-00012a70: 656c 5f70 6172 616c 6c65 6c2c 2031 2929  el_parallel, 1))
-00012a80: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00012a90: 2020 2020 2020 6f75 745f 7374 7261 7465        out_strate
-00012aa0: 6779 5f6d 6174 6d75 6c3d 2828 7061 7261  gy_matmul=((para
-00012ab0: 6c6c 656c 5f63 6f6e 6669 672e 6461 7461  llel_config.data
-00012ac0: 5f70 6172 616c 6c65 6c20 2a20 7061 7261  _parallel * para
-00012ad0: 6c6c 656c 5f63 6f6e 6669 672e 6d6f 6465  llel_config.mode
-00012ae0: 6c5f 7061 7261 6c6c 656c 2c20 3129 2c29  l_parallel, 1),)
-00012af0: 290a 0a20 2020 2020 2020 2073 656c 662e  )..        self.
-00012b00: 7573 655f 666c 6173 685f 6174 7465 6e74  use_flash_attent
-00012b10: 696f 6e20 3d20 7573 655f 666c 6173 685f  ion = use_flash_
-00012b20: 6174 7465 6e74 696f 6e0a 2020 2020 2020  attention.      
-00012b30: 2020 7365 6c66 2e75 7365 5f70 726f 6d70    self.use_promp
-00012b40: 745f 666c 6173 685f 6174 7465 6e74 696f  t_flash_attentio
-00012b50: 6e20 3d20 7573 655f 7072 6f6d 7074 5f66  n = use_prompt_f
-00012b60: 6c61 7368 5f61 7474 656e 7469 6f6e 0a0a  lash_attention..
-00012b70: 2020 2020 2020 2020 6966 2073 656c 662e          if self.
-00012b80: 7573 655f 666c 6173 685f 6174 7465 6e74  use_flash_attent
-00012b90: 696f 6e3a 0a20 2020 2020 2020 2020 2020  ion:.           
-00012ba0: 2073 656c 662e 666c 6173 685f 6174 7465   self.flash_atte
-00012bb0: 6e74 696f 6e20 3d20 466c 6173 6841 7474  ntion = FlashAtt
-00012bc0: 656e 7469 6f6e 280a 2020 2020 2020 2020  ention(.        
-00012bd0: 2020 2020 2020 2020 6865 6164 5f6e 756d          head_num
-00012be0: 3d6e 756d 5f68 6561 6473 2c0a 2020 2020  =num_heads,.    
-00012bf0: 2020 2020 2020 2020 2020 2020 7072 655f              pre_
-00012c00: 746f 6b65 6e73 3d36 3535 3336 2c0a 2020  tokens=65536,.  
-00012c10: 2020 2020 2020 2020 2020 2020 2020 6e65                ne
-00012c20: 7874 5f74 6f6b 656e 733d 302c 0a20 2020  xt_tokens=0,.   
-00012c30: 2020 2020 2020 2020 2020 2020 206b 6565               kee
-00012c40: 705f 7072 6f62 3d31 2e20 2d20 6174 7465  p_prob=1. - atte
-00012c50: 6e74 696f 6e5f 6472 6f70 6f75 745f 7261  ntion_dropout_ra
-00012c60: 7465 2c0a 2020 2020 2020 2020 2020 2020  te,.            
-00012c70: 2020 2020 7363 616c 655f 7661 6c75 653d      scale_value=
-00012c80: 312e 202f 206d 6174 682e 7371 7274 2873  1. / math.sqrt(s
-00012c90: 656c 662e 7369 7a65 5f70 6572 5f68 6561  elf.size_per_hea
-00012ca0: 6429 2c0a 2020 2020 2020 2020 2020 2020  d),.            
-00012cb0: 2020 2020 696e 7075 745f 6c61 796f 7574      input_layout
-00012cc0: 3d22 424e 5344 222c 0a20 2020 2020 2020  ="BNSD",.       
-00012cd0: 2020 2020 2020 2020 2073 7061 7273 655f           sparse_
-00012ce0: 6d6f 6465 3d30 2c0a 2020 2020 2020 2020  mode=0,.        
-00012cf0: 2020 2020 2020 2020 7573 655f 6174 7465          use_atte
-00012d00: 6e74 696f 6e5f 6d61 736b 3d54 7275 652c  ntion_mask=True,
-00012d10: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00012d20: 2064 703d 7061 7261 6c6c 656c 5f63 6f6e   dp=parallel_con
-00012d30: 6669 672e 6461 7461 5f70 6172 616c 6c65  fig.data_paralle
-00012d40: 6c2c 0a20 2020 2020 2020 2020 2020 2020  l,.             
-00012d50: 2020 206d 703d 7061 7261 6c6c 656c 5f63     mp=parallel_c
-00012d60: 6f6e 6669 672e 6d6f 6465 6c5f 7061 7261  onfig.model_para
-00012d70: 6c6c 656c 0a20 2020 2020 2020 2020 2020  llel.           
-00012d80: 2029 0a20 2020 2020 2020 2020 2020 2073   ).            s
-00012d90: 656c 662e 7375 6220 3d20 502e 5375 6228  elf.sub = P.Sub(
-00012da0: 292e 7368 6172 6428 0a20 2020 2020 2020  ).shard(.       
-00012db0: 2020 2020 2020 2020 2028 2831 2c29 2c20           ((1,), 
-00012dc0: 2870 6172 616c 6c65 6c5f 636f 6e66 6967  (parallel_config
-00012dd0: 2e64 6174 615f 7061 7261 6c6c 656c 2c20  .data_parallel, 
-00012de0: 312c 2031 2c20 3129 2929 0a0a 2020 2020  1, 1, 1)))..    
-00012df0: 2020 2020 2020 2020 7365 6c66 2e6f 6e65          self.one
-00012e00: 203d 2054 656e 736f 7228 5b31 2e30 5d2c   = Tensor([1.0],
-00012e10: 2064 7479 7065 3d63 6f6d 7075 7465 5f64   dtype=compute_d
-00012e20: 7479 7065 290a 0a20 2020 2020 2020 2069  type)..        i
-00012e30: 6620 7365 6c66 2e75 7365 5f70 726f 6d70  f self.use_promp
-00012e40: 745f 666c 6173 685f 6174 7465 6e74 696f  t_flash_attentio
-00012e50: 6e3a 0a20 2020 2020 2020 2020 2020 2073  n:.            s
-00012e60: 656c 662e 7072 6f6d 7074 5f66 6c61 7368  elf.prompt_flash
-00012e70: 5f61 7474 656e 7469 6f6e 203d 2050 726f  _attention = Pro
-00012e80: 6d70 7446 6c61 7368 4174 7465 6e74 696f  mptFlashAttentio
-00012e90: 6e28 6e75 6d5f 6865 6164 733d 6e75 6d5f  n(num_heads=num_
-00012ea0: 6865 6164 732c 0a20 2020 2020 2020 2020  heads,.         
-00012eb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012ec0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012ed0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012ee0: 2020 2020 2020 7363 616c 655f 7661 6c75        scale_valu
-00012ef0: 653d 312e 3020 2f20 286d 6174 682e 7371  e=1.0 / (math.sq
-00012f00: 7274 2873 656c 662e 7369 7a65 5f70 6572  rt(self.size_per
-00012f10: 5f68 6561 6429 292c 0a20 2020 2020 2020  _head)),.       
-00012f20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012f30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012f40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012f50: 2020 2020 2020 2020 7072 655f 746f 6b65          pre_toke
-00012f60: 6e73 3d73 656c 662e 7372 635f 7365 715f  ns=self.src_seq_
-00012f70: 6c65 6e67 7468 2c0a 2020 2020 2020 2020  length,.        
+00011ee0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011ef0: 2020 2020 2020 2020 2028 7061 7261 6c6c           (parall
+00011f00: 656c 5f63 6f6e 6669 672e 6d6f 6465 6c5f  el_config.model_
+00011f10: 7061 7261 6c6c 656c 2c29 2929 0a0a 2020  parallel,)))..  
+00011f20: 2020 2020 2020 2020 2020 2320 5661 6c75            # Valu
+00011f30: 650a 2020 2020 2020 2020 2020 2020 7365  e.            se
+00011f40: 6c66 2e64 656e 7365 3320 3d20 4c69 6e65  lf.dense3 = Line
+00011f50: 6172 2868 6964 6465 6e5f 7369 7a65 2c0a  ar(hidden_size,.
+00011f60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011f70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011f80: 2068 6964 6465 6e5f 7369 7a65 2c0a 2020   hidden_size,.  
+00011f90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011fa0: 2020 2020 2020 2020 2020 2020 2020 2063                 c
+00011fb0: 6f6d 7075 7465 5f64 7479 7065 3d63 6f6d  ompute_dtype=com
+00011fc0: 7075 7465 5f64 7479 7065 2c0a 2020 2020  pute_dtype,.    
+00011fd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00011fe0: 2020 2020 2020 2020 2020 2020 2070 6172               par
+00011ff0: 616d 5f69 6e69 745f 7479 7065 3d70 6172  am_init_type=par
+00012000: 616d 5f69 6e69 745f 7479 7065 290a 2020  am_init_type).  
+00012010: 2020 2020 2020 2020 2020 7365 6c66 2e64            self.d
+00012020: 656e 7365 332e 7368 6172 6428 7374 7261  ense3.shard(stra
+00012030: 7465 6779 5f6d 6174 6d75 6c3d 2828 7061  tegy_matmul=((pa
+00012040: 7261 6c6c 656c 5f63 6f6e 6669 672e 6461  rallel_config.da
+00012050: 7461 5f70 6172 616c 6c65 6c2c 2031 292c  ta_parallel, 1),
+00012060: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00012070: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012080: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012090: 2870 6172 616c 6c65 6c5f 636f 6e66 6967  (parallel_config
+000120a0: 2e6d 6f64 656c 5f70 6172 616c 6c65 6c2c  .model_parallel,
+000120b0: 2031 2929 2c0a 2020 2020 2020 2020 2020   1)),.          
+000120c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000120d0: 2020 2020 7374 7261 7465 6779 5f62 6961      strategy_bia
+000120e0: 733d 2828 7061 7261 6c6c 656c 5f63 6f6e  s=((parallel_con
+000120f0: 6669 672e 6461 7461 5f70 6172 616c 6c65  fig.data_paralle
+00012100: 6c2c 2070 6172 616c 6c65 6c5f 636f 6e66  l, parallel_conf
+00012110: 6967 2e6d 6f64 656c 5f70 6172 616c 6c65  ig.model_paralle
+00012120: 6c29 2c0a 2020 2020 2020 2020 2020 2020  l),.            
+00012130: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012140: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012150: 2028 7061 7261 6c6c 656c 5f63 6f6e 6669   (parallel_confi
+00012160: 672e 6d6f 6465 6c5f 7061 7261 6c6c 656c  g.model_parallel
+00012170: 2c29 2929 0a20 2020 2020 2020 2020 2020  ,))).           
+00012180: 2073 656c 662e 6474 7970 6520 3d20 636f   self.dtype = co
+00012190: 6d70 7574 655f 6474 7970 650a 2020 2020  mpute_dtype.    
+000121a0: 2020 2020 2020 2020 7365 6c66 2e73 6f66          self.sof
+000121b0: 746d 6178 5f64 7479 7065 203d 2073 6f66  tmax_dtype = sof
+000121c0: 746d 6178 5f63 6f6d 7075 7465 5f74 7970  tmax_compute_typ
+000121d0: 650a 2020 2020 2020 2020 2020 2020 6966  e.            if
+000121e0: 2073 656c 662e 7573 655f 7061 7374 3a0a   self.use_past:.
+000121f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012200: 2320 6f70 6572 6174 6f72 7320 7573 6564  # operators used
+00012210: 2066 6f72 2073 7461 7465 2072 6575 7365   for state reuse
+00012220: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00012230: 2073 6571 5f72 616e 6765 203d 206e 702e   seq_range = np.
+00012240: 6172 616e 6765 2873 7263 5f73 6571 5f6c  arange(src_seq_l
+00012250: 656e 6774 6829 2e72 6573 6861 7065 2831  ength).reshape(1
+00012260: 2c20 312c 202d 3129 0a20 2020 2020 2020  , 1, -1).       
+00012270: 2020 2020 2020 2020 2073 656c 662e 7261           self.ra
+00012280: 6e67 6520 3d20 5465 6e73 6f72 286e 702e  nge = Tensor(np.
+00012290: 7469 6c65 2873 6571 5f72 616e 6765 2c20  tile(seq_range, 
+000122a0: 2862 6174 6368 5f73 697a 652c 2031 2c20  (batch_size, 1, 
+000122b0: 3129 292c 206d 7374 7970 652e 696e 7433  1)), mstype.int3
+000122c0: 3229 0a20 2020 2020 2020 2020 2020 2020  2).             
+000122d0: 2020 2073 656c 662e 7365 715f 6c65 6e67     self.seq_leng
+000122e0: 7468 203d 2073 7263 5f73 6571 5f6c 656e  th = src_seq_len
+000122f0: 6774 680a 2020 2020 2020 2020 2020 2020  gth.            
+00012300: 2020 2020 7365 6c66 2e61 7474 656e 7469      self.attenti
+00012310: 6f6e 5f6d 6173 6b20 3d20 5465 6e73 6f72  on_mask = Tensor
+00012320: 286e 702e 7472 696c 286e 702e 6f6e 6573  (np.tril(np.ones
+00012330: 2873 6861 7065 3d28 7365 6c66 2e73 6571  (shape=(self.seq
+00012340: 5f6c 656e 6774 682c 2073 656c 662e 7365  _length, self.se
+00012350: 715f 6c65 6e67 7468 2929 292c 206d 7374  q_length))), mst
+00012360: 7970 652e 696e 7433 3229 0a20 2020 2020  ype.int32).     
+00012370: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+00012380: 736c 6963 6520 3d20 502e 5374 7269 6465  slice = P.Stride
+00012390: 6453 6c69 6365 2829 2e73 6861 7264 2828  dSlice().shard((
+000123a0: 2831 2c20 312c 2031 2c20 3129 2c29 290a  (1, 1, 1, 1),)).
+000123b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000123c0: 7365 6c66 2e6e 6f74 5f65 7175 616c 203d  self.not_equal =
+000123d0: 2050 2e4e 6f74 4571 7561 6c28 292e 7368   P.NotEqual().sh
+000123e0: 6172 6428 2828 312c 2031 2c20 312c 2031  ard(((1, 1, 1, 1
+000123f0: 292c 2028 2929 290a 2020 2020 2020 2020  ), ())).        
+00012400: 2020 2020 2020 2020 7365 6c66 2e72 6564          self.red
+00012410: 7563 6573 756d 203d 2050 2e52 6564 7563  ucesum = P.Reduc
+00012420: 6553 756d 2829 2e73 6861 7264 2828 2831  eSum().shard(((1
+00012430: 2c20 312c 2031 2c20 3129 2c29 290a 2020  , 1, 1, 1),)).  
+00012440: 2020 2020 2020 2020 2020 2020 2020 7365                se
+00012450: 6c66 2e65 7870 616e 645f 6469 6d73 203d  lf.expand_dims =
+00012460: 2050 2e45 7870 616e 6444 696d 7328 292e   P.ExpandDims().
+00012470: 7368 6172 6428 2828 312c 2031 2c20 3129  shard(((1, 1, 1)
+00012480: 2c29 290a 2020 2020 2020 2020 2020 2020  ,)).            
+00012490: 2020 2020 7365 6c66 2e74 656e 736f 725f      self.tensor_
+000124a0: 6c65 203d 2050 2e4c 6573 7345 7175 616c  le = P.LessEqual
+000124b0: 2829 2e73 6861 7264 2828 2831 2c20 312c  ().shard(((1, 1,
+000124c0: 2031 292c 2028 312c 2031 2c20 3129 2929   1), (1, 1, 1)))
+000124d0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+000124e0: 2073 656c 662e 6164 6420 3d20 502e 4164   self.add = P.Ad
+000124f0: 6428 292e 7368 6172 6428 2828 312c 2031  d().shard(((1, 1
+00012500: 2c20 312c 2031 292c 2028 312c 2031 2c20  , 1, 1), (1, 1, 
+00012510: 312c 2031 2929 290a 2020 2020 2020 2020  1, 1))).        
+00012520: 2020 2020 2020 2020 7365 6c66 2e65 7175          self.equ
+00012530: 616c 203d 2050 2e45 7175 616c 2829 2e73  al = P.Equal().s
+00012540: 6861 7264 2828 2831 2c20 312c 2031 292c  hard(((1, 1, 1),
+00012550: 2028 312c 2031 2c20 3129 2929 0a20 2020   (1, 1, 1))).   
+00012560: 2020 2020 2020 2020 2020 2020 2073 656c               sel
+00012570: 662e 7375 6231 203d 2050 2e53 7562 2829  f.sub1 = P.Sub()
+00012580: 2e73 6861 7264 2828 2831 2c29 2c20 2829  .shard(((1,), ()
+00012590: 2929 0a20 2020 2020 2020 2020 2020 2020  )).             
+000125a0: 2020 2073 656c 662e 7469 6c65 203d 2050     self.tile = P
+000125b0: 2e54 696c 6528 292e 7368 6172 6428 2828  .Tile().shard(((
+000125c0: 312c 2031 2c20 312c 2031 292c 2929 0a20  1, 1, 1, 1),)). 
+000125d0: 2020 2020 2020 2020 2020 2020 2020 2073                 s
+000125e0: 656c 662e 6c65 7373 203d 2050 2e4c 6573  elf.less = P.Les
+000125f0: 7328 292e 7368 6172 6428 2828 312c 2031  s().shard(((1, 1
+00012600: 2c20 3129 2c20 2831 2c20 312c 2031 2929  , 1), (1, 1, 1))
+00012610: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
+00012620: 2020 7365 6c66 2e6d 756c 3120 3d20 502e    self.mul1 = P.
+00012630: 4d75 6c28 292e 7368 6172 6428 2828 312c  Mul().shard(((1,
+00012640: 2031 2c20 312c 2031 292c 2028 312c 2031   1, 1, 1), (1, 1
+00012650: 2c20 312c 2031 2929 290a 0a20 2020 2020  , 1, 1)))..     
+00012660: 2020 2020 2020 2069 6620 7061 7261 6c6c         if parall
+00012670: 656c 5f63 6f6e 6669 672e 7573 655f 7365  el_config.use_se
+00012680: 715f 7061 7261 6c6c 656c 3a0a 2020 2020  q_parallel:.    
+00012690: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+000126a0: 2e64 726f 706f 7574 2e64 726f 706f 7574  .dropout.dropout
+000126b0: 2e73 6861 7264 2828 2870 6172 616c 6c65  .shard(((paralle
+000126c0: 6c5f 636f 6e66 6967 2e64 6174 615f 7061  l_config.data_pa
+000126d0: 7261 6c6c 656c 202a 2070 6172 616c 6c65  rallel * paralle
+000126e0: 6c5f 636f 6e66 6967 2e6d 6f64 656c 5f70  l_config.model_p
+000126f0: 6172 616c 6c65 6c2c 2031 292c 2929 0a20  arallel, 1),)). 
+00012700: 2020 2020 2020 2020 2020 2020 2020 2073                 s
+00012710: 656c 662e 7072 6f6a 6563 7469 6f6e 2e73  elf.projection.s
+00012720: 6861 7264 280a 2020 2020 2020 2020 2020  hard(.          
+00012730: 2020 2020 2020 2020 2020 7374 7261 7465            strate
+00012740: 6779 5f62 6961 733d 2828 7061 7261 6c6c  gy_bias=((parall
+00012750: 656c 5f63 6f6e 6669 672e 6461 7461 5f70  el_config.data_p
+00012760: 6172 616c 6c65 6c20 2a20 7061 7261 6c6c  arallel * parall
+00012770: 656c 5f63 6f6e 6669 672e 6d6f 6465 6c5f  el_config.model_
+00012780: 7061 7261 6c6c 656c 2c20 3129 2c20 2831  parallel, 1), (1
+00012790: 2c29 292c 0a20 2020 2020 2020 2020 2020  ,)),.           
+000127a0: 2020 2020 2020 2020 2073 7472 6174 6567           strateg
+000127b0: 795f 6d61 746d 756c 3d28 2870 6172 616c  y_matmul=((paral
+000127c0: 6c65 6c5f 636f 6e66 6967 2e64 6174 615f  lel_config.data_
+000127d0: 7061 7261 6c6c 656c 2c20 7061 7261 6c6c  parallel, parall
+000127e0: 656c 5f63 6f6e 6669 672e 6d6f 6465 6c5f  el_config.model_
+000127f0: 7061 7261 6c6c 656c 292c 0a20 2020 2020  parallel),.     
+00012800: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012810: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012820: 2870 6172 616c 6c65 6c5f 636f 6e66 6967  (parallel_config
+00012830: 2e6d 6f64 656c 5f70 6172 616c 6c65 6c2c  .model_parallel,
+00012840: 2031 2929 2c0a 2020 2020 2020 2020 2020   1)),.          
+00012850: 2020 2020 2020 2020 2020 6f75 745f 7374            out_st
+00012860: 7261 7465 6779 5f6d 6174 6d75 6c3d 2828  rategy_matmul=((
+00012870: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
+00012880: 6461 7461 5f70 6172 616c 6c65 6c20 2a20  data_parallel * 
+00012890: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
+000128a0: 6d6f 6465 6c5f 7061 7261 6c6c 656c 2c20  model_parallel, 
+000128b0: 3129 2c29 290a 0a20 2020 2020 2020 2073  1),))..        s
+000128c0: 656c 662e 7573 655f 666c 6173 685f 6174  elf.use_flash_at
+000128d0: 7465 6e74 696f 6e20 3d20 7573 655f 666c  tention = use_fl
+000128e0: 6173 685f 6174 7465 6e74 696f 6e0a 2020  ash_attention.  
+000128f0: 2020 2020 2020 7365 6c66 2e75 7365 5f70        self.use_p
+00012900: 726f 6d70 745f 666c 6173 685f 6174 7465  rompt_flash_atte
+00012910: 6e74 696f 6e20 3d20 7573 655f 7072 6f6d  ntion = use_prom
+00012920: 7074 5f66 6c61 7368 5f61 7474 656e 7469  pt_flash_attenti
+00012930: 6f6e 0a20 2020 2020 2020 2073 656c 662e  on.        self.
+00012940: 7573 655f 696e 6372 655f 666c 6173 685f  use_incre_flash_
+00012950: 6174 7465 6e74 696f 6e20 3d20 7573 655f  attention = use_
+00012960: 696e 6372 655f 666c 6173 685f 6174 7465  incre_flash_atte
+00012970: 6e74 696f 6e0a 0a20 2020 2020 2020 2069  ntion..        i
+00012980: 6620 7365 6c66 2e75 7365 5f66 6c61 7368  f self.use_flash
+00012990: 5f61 7474 656e 7469 6f6e 3a0a 2020 2020  _attention:.    
+000129a0: 2020 2020 2020 2020 7365 6c66 2e66 6c61          self.fla
+000129b0: 7368 5f61 7474 656e 7469 6f6e 203d 2046  sh_attention = F
+000129c0: 6c61 7368 4174 7465 6e74 696f 6e28 0a20  lashAttention(. 
+000129d0: 2020 2020 2020 2020 2020 2020 2020 2068                 h
+000129e0: 6561 645f 6e75 6d3d 6e75 6d5f 6865 6164  ead_num=num_head
+000129f0: 732c 0a20 2020 2020 2020 2020 2020 2020  s,.             
+00012a00: 2020 2070 7265 5f74 6f6b 656e 733d 3635     pre_tokens=65
+00012a10: 3533 362c 0a20 2020 2020 2020 2020 2020  536,.           
+00012a20: 2020 2020 206e 6578 745f 746f 6b65 6e73       next_tokens
+00012a30: 3d30 2c0a 2020 2020 2020 2020 2020 2020  =0,.            
+00012a40: 2020 2020 6b65 6570 5f70 726f 623d 312e      keep_prob=1.
+00012a50: 202d 2061 7474 656e 7469 6f6e 5f64 726f   - attention_dro
+00012a60: 706f 7574 5f72 6174 652c 0a20 2020 2020  pout_rate,.     
+00012a70: 2020 2020 2020 2020 2020 2073 6361 6c65             scale
+00012a80: 5f76 616c 7565 3d31 2e20 2f20 6d61 7468  _value=1. / math
+00012a90: 2e73 7172 7428 7365 6c66 2e73 697a 655f  .sqrt(self.size_
+00012aa0: 7065 725f 6865 6164 292c 0a20 2020 2020  per_head),.     
+00012ab0: 2020 2020 2020 2020 2020 2069 6e70 7574             input
+00012ac0: 5f6c 6179 6f75 743d 2242 4e53 4422 2c0a  _layout="BNSD",.
+00012ad0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012ae0: 7370 6172 7365 5f6d 6f64 653d 302c 0a20  sparse_mode=0,. 
+00012af0: 2020 2020 2020 2020 2020 2020 2020 2075                 u
+00012b00: 7365 5f61 7474 656e 7469 6f6e 5f6d 6173  se_attention_mas
+00012b10: 6b3d 5472 7565 2c0a 2020 2020 2020 2020  k=True,.        
+00012b20: 2020 2020 2020 2020 6470 3d70 6172 616c          dp=paral
+00012b30: 6c65 6c5f 636f 6e66 6967 2e64 6174 615f  lel_config.data_
+00012b40: 7061 7261 6c6c 656c 2c0a 2020 2020 2020  parallel,.      
+00012b50: 2020 2020 2020 2020 2020 6d70 3d70 6172            mp=par
+00012b60: 616c 6c65 6c5f 636f 6e66 6967 2e6d 6f64  allel_config.mod
+00012b70: 656c 5f70 6172 616c 6c65 6c0a 2020 2020  el_parallel.    
+00012b80: 2020 2020 2020 2020 290a 2020 2020 2020          ).      
+00012b90: 2020 2020 2020 7365 6c66 2e73 7562 203d        self.sub =
+00012ba0: 2050 2e53 7562 2829 2e73 6861 7264 280a   P.Sub().shard(.
+00012bb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012bc0: 2828 312c 292c 2028 7061 7261 6c6c 656c  ((1,), (parallel
+00012bd0: 5f63 6f6e 6669 672e 6461 7461 5f70 6172  _config.data_par
+00012be0: 616c 6c65 6c2c 2031 2c20 312c 2031 2929  allel, 1, 1, 1))
+00012bf0: 290a 0a20 2020 2020 2020 2020 2020 2073  )..            s
+00012c00: 656c 662e 6f6e 6520 3d20 5465 6e73 6f72  elf.one = Tensor
+00012c10: 285b 312e 305d 2c20 6474 7970 653d 636f  ([1.0], dtype=co
+00012c20: 6d70 7574 655f 6474 7970 6529 0a0a 2020  mpute_dtype)..  
+00012c30: 2020 2020 2020 6966 2073 656c 662e 7573        if self.us
+00012c40: 655f 7072 6f6d 7074 5f66 6c61 7368 5f61  e_prompt_flash_a
+00012c50: 7474 656e 7469 6f6e 3a0a 2020 2020 2020  ttention:.      
+00012c60: 2020 2020 2020 7365 6c66 2e70 726f 6d70        self.promp
+00012c70: 745f 666c 6173 685f 6174 7465 6e74 696f  t_flash_attentio
+00012c80: 6e20 3d20 5072 6f6d 7074 466c 6173 6841  n = PromptFlashA
+00012c90: 7474 656e 7469 6f6e 286e 756d 5f68 6561  ttention(num_hea
+00012ca0: 6473 3d6e 756d 5f68 6561 6473 2c0a 2020  ds=num_heads,.  
+00012cb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012cc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012cd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012ce0: 2020 2020 2020 2020 2020 2020 2073 6361               sca
+00012cf0: 6c65 5f76 616c 7565 3d31 2e30 202f 2028  le_value=1.0 / (
+00012d00: 6d61 7468 2e73 7172 7428 7365 6c66 2e73  math.sqrt(self.s
+00012d10: 697a 655f 7065 725f 6865 6164 2929 2c0a  ize_per_head)),.
+00012d20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012d30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012d40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012d50: 2020 2020 2020 2020 2020 2020 2020 2070                 p
+00012d60: 7265 5f74 6f6b 656e 733d 7365 6c66 2e73  re_tokens=self.s
+00012d70: 7263 5f73 6571 5f6c 656e 6774 682c 0a20  rc_seq_length,. 
+00012d80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012d90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012da0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012db0: 2020 2020 2020 2020 2020 2020 2020 6e65                ne
+00012dc0: 7874 5f74 6f6b 656e 733d 302c 0a20 2020  xt_tokens=0,.   
+00012dd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012de0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012df0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012e00: 2020 2020 2020 2020 2020 2020 696e 7075              inpu
+00012e10: 745f 6c61 796f 7574 3d27 424e 5344 272c  t_layout='BNSD',
+00012e20: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00012e30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012e40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012e50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012e60: 6e75 6d5f 6b65 795f 7661 6c75 655f 6865  num_key_value_he
+00012e70: 6164 733d 3029 0a20 2020 2020 2020 2020  ads=0).         
+00012e80: 2020 2073 656c 662e 7072 6f6d 7074 5f66     self.prompt_f
+00012e90: 6c61 7368 5f61 7474 656e 7469 6f6e 2e73  lash_attention.s
+00012ea0: 6861 7264 2828 2870 6172 616c 6c65 6c5f  hard(((parallel_
+00012eb0: 636f 6e66 6967 2e64 6174 615f 7061 7261  config.data_para
+00012ec0: 6c6c 656c 2c20 7061 7261 6c6c 656c 5f63  llel, parallel_c
+00012ed0: 6f6e 6669 672e 6d6f 6465 6c5f 7061 7261  onfig.model_para
+00012ee0: 6c6c 656c 2c20 312c 2031 292c 0a20 2020  llel, 1, 1),.   
+00012ef0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012f00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012f10: 2020 2020 2020 2020 2020 2020 2870 6172              (par
+00012f20: 616c 6c65 6c5f 636f 6e66 6967 2e64 6174  allel_config.dat
+00012f30: 615f 7061 7261 6c6c 656c 2c20 7061 7261  a_parallel, para
+00012f40: 6c6c 656c 5f63 6f6e 6669 672e 6d6f 6465  llel_config.mode
+00012f50: 6c5f 7061 7261 6c6c 656c 2c20 312c 2031  l_parallel, 1, 1
+00012f60: 292c 0a20 2020 2020 2020 2020 2020 2020  ),.             
+00012f70: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00012f80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012f90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012fa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00012fb0: 2020 2020 2020 206e 6578 745f 746f 6b65         next_toke
-00012fc0: 6e73 3d30 2c0a 2020 2020 2020 2020 2020  ns=0,.          
-00012fd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00012f90: 2020 2870 6172 616c 6c65 6c5f 636f 6e66    (parallel_conf
+00012fa0: 6967 2e64 6174 615f 7061 7261 6c6c 656c  ig.data_parallel
+00012fb0: 2c20 7061 7261 6c6c 656c 5f63 6f6e 6669  , parallel_confi
+00012fc0: 672e 6d6f 6465 6c5f 7061 7261 6c6c 656c  g.model_parallel
+00012fd0: 2c20 312c 2031 292c 0a20 2020 2020 2020  , 1, 1),.       
 00012fe0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00012ff0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00013000: 2020 2020 2069 6e70 7574 5f6c 6179 6f75       input_layou
-00013010: 743d 2742 4e53 4427 2c0a 2020 2020 2020  t='BNSD',.      
-00013020: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00013030: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00013040: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00013050: 2020 2020 2020 2020 206e 756d 5f6b 6579           num_key
-00013060: 5f76 616c 7565 5f68 6561 6473 3d30 290a  _value_heads=0).
-00013070: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-00013080: 2e70 726f 6d70 745f 666c 6173 685f 6174  .prompt_flash_at
-00013090: 7465 6e74 696f 6e2e 7368 6172 6428 2828  tention.shard(((
-000130a0: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
-000130b0: 6461 7461 5f70 6172 616c 6c65 6c2c 2070  data_parallel, p
-000130c0: 6172 616c 6c65 6c5f 636f 6e66 6967 2e6d  arallel_config.m
-000130d0: 6f64 656c 5f70 6172 616c 6c65 6c2c 2031  odel_parallel, 1
-000130e0: 2c20 3129 2c0a 2020 2020 2020 2020 2020  , 1),.          
-000130f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00013100: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00013110: 2020 2020 2028 7061 7261 6c6c 656c 5f63       (parallel_c
-00013120: 6f6e 6669 672e 6461 7461 5f70 6172 616c  onfig.data_paral
-00013130: 6c65 6c2c 2070 6172 616c 6c65 6c5f 636f  lel, parallel_co
-00013140: 6e66 6967 2e6d 6f64 656c 5f70 6172 616c  nfig.model_paral
-00013150: 6c65 6c2c 2031 2c20 3129 2c0a 2020 2020  lel, 1, 1),.    
-00013160: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00013000: 2020 2020 2020 2020 2870 6172 616c 6c65          (paralle
+00013010: 6c5f 636f 6e66 6967 2e64 6174 615f 7061  l_config.data_pa
+00013020: 7261 6c6c 656c 2c20 312c 2031 2c20 3129  rallel, 1, 1, 1)
+00013030: 2929 0a20 2020 2020 2020 2020 2020 2073  )).            s
+00013040: 656c 662e 7375 625f 7066 6120 3d20 502e  elf.sub_pfa = P.
+00013050: 5375 6228 292e 7368 6172 6428 0a20 2020  Sub().shard(.   
+00013060: 2020 2020 2020 2020 2020 2020 2028 2831               ((1
+00013070: 2c29 2c20 2870 6172 616c 6c65 6c5f 636f  ,), (parallel_co
+00013080: 6e66 6967 2e64 6174 615f 7061 7261 6c6c  nfig.data_parall
+00013090: 656c 2c20 312c 2031 2c20 3129 2929 0a20  el, 1, 1, 1))). 
+000130a0: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+000130b0: 6f6e 6520 3d20 5465 6e73 6f72 285b 312e  one = Tensor([1.
+000130c0: 305d 2c20 6474 7970 653d 636f 6d70 7574  0], dtype=comput
+000130d0: 655f 6474 7970 6529 0a0a 2020 2020 2020  e_dtype)..      
+000130e0: 2020 6966 2073 656c 662e 7573 655f 696e    if self.use_in
+000130f0: 6372 655f 666c 6173 685f 6174 7465 6e74  cre_flash_attent
+00013100: 696f 6e20 616e 6420 7365 6c66 2e75 7365  ion and self.use
+00013110: 5f70 6173 743a 0a20 2020 2020 2020 2020  _past:.         
+00013120: 2020 2073 656c 662e 696e 6372 655f 666c     self.incre_fl
+00013130: 6173 685f 6174 7465 6e74 696f 6e20 3d20  ash_attention = 
+00013140: 496e 6372 6546 6c61 7368 4174 7465 6e74  IncreFlashAttent
+00013150: 696f 6e28 6e75 6d5f 6865 6164 733d 6e75  ion(num_heads=nu
+00013160: 6d5f 6865 6164 732c 0a20 2020 2020 2020  m_heads,.       
 00013170: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00013180: 2020 2020 2020 2020 2020 2028 7061 7261             (para
-00013190: 6c6c 656c 5f63 6f6e 6669 672e 6461 7461  llel_config.data
-000131a0: 5f70 6172 616c 6c65 6c2c 2070 6172 616c  _parallel, paral
-000131b0: 6c65 6c5f 636f 6e66 6967 2e6d 6f64 656c  lel_config.model
-000131c0: 5f70 6172 616c 6c65 6c2c 2031 2c20 3129  _parallel, 1, 1)
-000131d0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00013180: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00013190: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000131a0: 2020 2020 2020 7363 616c 655f 7661 6c75        scale_valu
+000131b0: 653d 312e 3020 2f20 286d 6174 682e 7371  e=1.0 / (math.sq
+000131c0: 7274 2873 656c 662e 7369 7a65 5f70 6572  rt(self.size_per
+000131d0: 5f68 6561 6429 292c 0a20 2020 2020 2020  _head)),.       
 000131e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 000131f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00013200: 2028 7061 7261 6c6c 656c 5f63 6f6e 6669   (parallel_confi
-00013210: 672e 6461 7461 5f70 6172 616c 6c65 6c2c  g.data_parallel,
-00013220: 2031 2c20 312c 2031 2929 290a 2020 2020   1, 1, 1))).    
-00013230: 2020 2020 2020 2020 7365 6c66 2e73 7562          self.sub
-00013240: 5f70 6661 203d 2050 2e53 7562 2829 2e73  _pfa = P.Sub().s
-00013250: 6861 7264 280a 2020 2020 2020 2020 2020  hard(.          
-00013260: 2020 2020 2020 2828 312c 292c 2028 7061        ((1,), (pa
-00013270: 7261 6c6c 656c 5f63 6f6e 6669 672e 6461  rallel_config.da
-00013280: 7461 5f70 6172 616c 6c65 6c2c 2031 2c20  ta_parallel, 1, 
-00013290: 312c 2031 2929 290a 2020 2020 2020 2020  1, 1))).        
-000132a0: 2020 2020 7365 6c66 2e6f 6e65 203d 2054      self.one = T
-000132b0: 656e 736f 7228 5b31 2e30 5d2c 2064 7479  ensor([1.0], dty
-000132c0: 7065 3d63 6f6d 7075 7465 5f64 7479 7065  pe=compute_dtype
-000132d0: 290a 0a20 2020 2020 2020 2069 6620 7061  )..        if pa
-000132e0: 7261 6c6c 656c 5f63 6f6e 6669 672e 7365  rallel_config.se
-000132f0: 6c65 6374 5f72 6563 6f6d 7075 7465 3a0a  lect_recompute:.
-00013300: 2020 2020 2020 2020 2020 2020 2320 7265              # re
-00013310: 636f 6d70 7574 6520 6973 2075 7365 6420  compute is used 
-00013320: 696e 205f 6174 746e 2c20 7573 7561 6c6c  in _attn, usuall
-00013330: 7920 7769 7468 2073 6571 7565 6e63 6520  y with sequence 
-00013340: 7061 7261 6c6c 656c 0a20 2020 2020 2020  parallel.       
-00013350: 2020 2020 2073 656c 662e 6261 7463 685f       self.batch_
-00013360: 6d61 746d 756c 2e72 6563 6f6d 7075 7465  matmul.recompute
-00013370: 2829 0a20 2020 2020 2020 2020 2020 2073  ().            s
-00013380: 656c 662e 7375 622e 7265 636f 6d70 7574  elf.sub.recomput
-00013390: 6528 290a 2020 2020 2020 2020 2020 2020  e().            
-000133a0: 7365 6c66 2e61 6464 2e72 6563 6f6d 7075  self.add.recompu
-000133b0: 7465 2829 0a20 2020 2020 2020 2020 2020  te().           
-000133c0: 2073 656c 662e 6d65 7267 6572 5f68 6561   self.merger_hea
-000133d0: 645f 7472 616e 7370 6f73 652e 7265 636f  d_transpose.reco
-000133e0: 6d70 7574 6528 290a 2020 2020 2020 2020  mpute().        
-000133f0: 2020 2020 7365 6c66 2e73 6f66 746d 6178      self.softmax
-00013400: 5f72 6573 6861 7065 2e72 6563 6f6d 7075  _reshape.recompu
-00013410: 7465 2829 0a20 2020 2020 2020 2020 2020  te().           
-00013420: 2073 656c 662e 7072 6f62 5f64 726f 706f   self.prob_dropo
-00013430: 7574 2e72 6563 6f6d 7075 7465 2829 0a20  ut.recompute(). 
-00013440: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-00013450: 736f 6674 6d61 785f 6361 7374 2e72 6563  softmax_cast.rec
-00013460: 6f6d 7075 7465 2829 0a20 2020 2020 2020  ompute().       
-00013470: 2020 2020 2073 656c 662e 736f 6674 6d61       self.softma
-00013480: 782e 736f 6674 6d61 782e 7265 636f 6d70  x.softmax.recomp
-00013490: 7574 6528 290a 2020 2020 2020 2020 2020  ute().          
-000134a0: 2020 7365 6c66 2e73 6f66 746d 6178 5f33    self.softmax_3
-000134b0: 642e 7265 636f 6d70 7574 6528 290a 0a20  d.recompute().. 
-000134c0: 2020 2064 6566 2063 6f6e 7374 7275 6374     def construct
-000134d0: 2873 656c 662c 2071 7565 7279 5f74 656e  (self, query_ten
-000134e0: 736f 722c 206b 6579 5f74 656e 736f 722c  sor, key_tensor,
-000134f0: 2076 616c 7565 5f74 656e 736f 722c 2061   value_tensor, a
-00013500: 7474 656e 7469 6f6e 5f6d 6173 6b2c 206b  ttention_mask, k
-00013510: 6579 5f70 6173 743d 4e6f 6e65 2c0a 2020  ey_past=None,.  
-00013520: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00013530: 7661 6c75 655f 7061 7374 3d4e 6f6e 652c  value_past=None,
-00013540: 2062 6174 6368 5f76 616c 6964 5f6c 656e   batch_valid_len
-00013550: 6774 683d 4e6f 6e65 293a 0a20 2020 2020  gth=None):.     
-00013560: 2020 2022 2222 466f 7277 6172 6420 7072     """Forward pr
-00013570: 6f63 6573 7320 6f66 2074 6865 204d 756c  ocess of the Mul
-00013580: 7469 4865 6164 4174 7465 6e74 696f 6e22  tiHeadAttention"
-00013590: 2222 0a20 2020 2020 2020 2073 656c 662e  "".        self.
-000135a0: 5f63 6865 636b 5f69 6e70 7574 7328 7175  _check_inputs(qu
-000135b0: 6572 795f 7465 6e73 6f72 2c20 6b65 795f  ery_tensor, key_
-000135c0: 7465 6e73 6f72 2c20 7661 6c75 655f 7465  tensor, value_te
-000135d0: 6e73 6f72 2c20 6174 7465 6e74 696f 6e5f  nsor, attention_
-000135e0: 6d61 736b 2c20 6b65 795f 7061 7374 2c0a  mask, key_past,.
-000135f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00013600: 2020 2020 2020 2020 2020 2076 616c 7565             value
-00013610: 5f70 6173 742c 2062 6174 6368 5f76 616c  _past, batch_val
-00013620: 6964 5f6c 656e 6774 6829 0a20 2020 2020  id_length).     
-00013630: 2020 206f 7269 5f73 6861 7065 203d 2046     ori_shape = F
-00013640: 2e73 6861 7065 2871 7565 7279 5f74 656e  .shape(query_ten
-00013650: 736f 7229 0a20 2020 2020 2020 2062 6174  sor).        bat
-00013660: 6368 5f73 697a 6520 3d20 7365 6c66 2e5f  ch_size = self._
-00013670: 6765 745f 6261 7463 685f 7369 7a65 5f66  get_batch_size_f
-00013680: 726f 6d5f 7175 6572 7928 7175 6572 795f  rom_query(query_
-00013690: 7465 6e73 6f72 290a 2020 2020 2020 2020  tensor).        
-000136a0: 7175 6572 795f 7465 6e73 6f72 2c20 6b65  query_tensor, ke
-000136b0: 795f 7465 6e73 6f72 2c20 7661 6c75 655f  y_tensor, value_
-000136c0: 7465 6e73 6f72 203d 2073 656c 662e 5f63  tensor = self._c
-000136d0: 6f6e 7665 7274 5f74 6f5f 3264 5f74 656e  onvert_to_2d_ten
-000136e0: 736f 7228 7175 6572 795f 7465 6e73 6f72  sor(query_tensor
-000136f0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00013700: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00013710: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00013720: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00013730: 2020 2020 2020 2020 2020 2020 2020 6b65                ke
-00013740: 795f 7465 6e73 6f72 2c0a 2020 2020 2020  y_tensor,.      
-00013750: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00013760: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00013770: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00013780: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00013790: 2020 2020 2020 7661 6c75 655f 7465 6e73        value_tens
-000137a0: 6f72 290a 2020 2020 2020 2020 6f72 695f  or).        ori_
-000137b0: 6474 7970 6520 3d20 462e 6474 7970 6528  dtype = F.dtype(
-000137c0: 7175 6572 795f 7465 6e73 6f72 290a 2020  query_tensor).  
-000137d0: 2020 2020 2020 7175 6572 795f 7465 6e73        query_tens
-000137e0: 6f72 203d 2046 2e63 6173 7428 7175 6572  or = F.cast(quer
-000137f0: 795f 7465 6e73 6f72 2c20 7365 6c66 2e64  y_tensor, self.d
-00013800: 7479 7065 290a 2020 2020 2020 2020 6b65  type).        ke
-00013810: 795f 7465 6e73 6f72 203d 2046 2e63 6173  y_tensor = F.cas
-00013820: 7428 6b65 795f 7465 6e73 6f72 2c20 7365  t(key_tensor, se
-00013830: 6c66 2e64 7479 7065 290a 2020 2020 2020  lf.dtype).      
-00013840: 2020 7661 6c75 655f 7465 6e73 6f72 203d    value_tensor =
-00013850: 2046 2e63 6173 7428 7661 6c75 655f 7465   F.cast(value_te
-00013860: 6e73 6f72 2c20 7365 6c66 2e64 7479 7065  nsor, self.dtype
-00013870: 290a 2020 2020 2020 2020 2320 6d75 6c74  ).        # mult
-00013880: 6920 6865 6164 2061 7474 656e 7469 6f6e  i head attention
-00013890: 3a20 7175 6572 792c 206b 6579 2c20 7661  : query, key, va
-000138a0: 6c75 6520 6172 6520 6465 7269 7665 6420  lue are derived 
-000138b0: 6672 6f6d 2074 6865 2073 616d 6520 696e  from the same in
-000138c0: 7075 7473 0a20 2020 2020 2020 2071 7565  puts.        que
-000138d0: 7279 203d 2073 656c 662e 6465 6e73 6531  ry = self.dense1
-000138e0: 2871 7565 7279 5f74 656e 736f 7229 0a20  (query_tensor). 
-000138f0: 2020 2020 2020 206b 6579 203d 2073 656c         key = sel
-00013900: 662e 6465 6e73 6532 286b 6579 5f74 656e  f.dense2(key_ten
-00013910: 736f 7229 0a20 2020 2020 2020 2076 616c  sor).        val
-00013920: 7565 203d 2073 656c 662e 6465 6e73 6533  ue = self.dense3
-00013930: 2876 616c 7565 5f74 656e 736f 7229 0a20  (value_tensor). 
-00013940: 2020 2020 2020 2023 2074 6865 2072 6574         # the ret
-00013950: 7572 6e65 6420 7368 6170 6520 6973 205b  urned shape is [
-00013960: 6273 2c20 6e75 6d5f 6865 6164 732c 2073  bs, num_heads, s
-00013970: 6571 5f6c 656e 6774 682c 2073 697a 655f  eq_length, size_
-00013980: 7065 725f 6865 6164 5d0a 2020 2020 2020  per_head].      
-00013990: 2020 7175 6572 7920 3d20 7365 6c66 2e74    query = self.t
-000139a0: 7261 6e73 706f 7365 280a 2020 2020 2020  ranspose(.      
-000139b0: 2020 2020 2020 462e 7265 7368 6170 6528        F.reshape(
-000139c0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-000139d0: 2071 7565 7279 2c0a 2020 2020 2020 2020   query,.        
-000139e0: 2020 2020 2020 2020 2862 6174 6368 5f73          (batch_s
-000139f0: 697a 652c 2073 656c 662e 5f67 6574 5f73  ize, self._get_s
-00013a00: 6571 5f6c 656e 6774 685f 756e 6465 725f  eq_length_under_
-00013a10: 696e 6372 656d 656e 7461 6c28 7365 6c66  incremental(self
-00013a20: 2e73 7263 5f73 6571 5f6c 656e 6774 6829  .src_seq_length)
-00013a30: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00013a40: 2020 2073 656c 662e 6e5f 6865 6164 2c20     self.n_head, 
-00013a50: 7365 6c66 2e73 697a 655f 7065 725f 6865  self.size_per_he
-00013a60: 6164 2929 2c0a 2020 2020 2020 2020 2020  ad)),.          
-00013a70: 2020 2830 2c20 322c 2031 2c20 3329 290a    (0, 2, 1, 3)).
-00013a80: 2020 2020 2020 2020 2320 4641 3a20 5b62          # FA: [b
-00013a90: 732c 206e 756d 5f68 6561 6473 2c20 7365  s, num_heads, se
-00013aa0: 715f 6c65 6e67 7468 2c20 7369 7a65 5f70  q_length, size_p
-00013ab0: 6572 5f68 6561 645d 206f 7220 5b62 732c  er_head] or [bs,
-00013ac0: 206e 756d 5f68 6561 6473 2c20 7369 7a65   num_heads, size
-00013ad0: 5f70 6572 5f68 6561 642c 2073 6571 5f6c  _per_head, seq_l
-00013ae0: 656e 6774 685d 0a20 2020 2020 2020 2069  ength].        i
-00013af0: 6620 6e6f 7420 7365 6c66 2e74 7261 696e  f not self.train
-00013b00: 696e 673a 0a20 2020 2020 2020 2020 2020  ing:.           
-00013b10: 2064 6f5f 6469 6666 6572 656e 745f 7368   do_different_sh
-00013b20: 6170 6520 3d20 2873 656c 662e 7573 655f  ape = (self.use_
-00013b30: 666c 6173 685f 6174 7465 6e74 696f 6e0a  flash_attention.
-00013b40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00013b50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00013b60: 2020 6f72 2073 656c 662e 7573 655f 7072    or self.use_pr
-00013b70: 6f6d 7074 5f66 6c61 7368 5f61 7474 656e  ompt_flash_atten
-00013b80: 7469 6f6e 290a 2020 2020 2020 2020 656c  tion).        el
-00013b90: 7365 3a0a 2020 2020 2020 2020 2020 2020  se:.            
-00013ba0: 646f 5f64 6966 6665 7265 6e74 5f73 6861  do_different_sha
-00013bb0: 7065 203d 2073 656c 662e 7573 655f 666c  pe = self.use_fl
-00013bc0: 6173 685f 6174 7465 6e74 696f 6e0a 0a20  ash_attention.. 
-00013bd0: 2020 2020 2020 206b 6579 5f74 7261 6e73         key_trans
-00013be0: 706f 7365 5f73 6861 7065 203d 2028 302c  pose_shape = (0,
-00013bf0: 2032 2c20 312c 2033 2920 6966 2064 6f5f   2, 1, 3) if do_
-00013c00: 6469 6666 6572 656e 745f 7368 6170 6520  different_shape 
-00013c10: 656c 7365 2028 302c 2032 2c20 332c 2031  else (0, 2, 3, 1
-00013c20: 290a 2020 2020 2020 2020 6b65 7920 3d20  ).        key = 
-00013c30: 7365 6c66 2e74 7261 6e73 706f 7365 280a  self.transpose(.
-00013c40: 2020 2020 2020 2020 2020 2020 462e 7265              F.re
-00013c50: 7368 6170 6528 0a20 2020 2020 2020 2020  shape(.         
-00013c60: 2020 2020 2020 206b 6579 2c20 2862 6174         key, (bat
-00013c70: 6368 5f73 697a 652c 2073 656c 662e 5f67  ch_size, self._g
-00013c80: 6574 5f73 6571 5f6c 656e 6774 685f 756e  et_seq_length_un
-00013c90: 6465 725f 696e 6372 656d 656e 7461 6c28  der_incremental(
-00013ca0: 7365 6c66 2e74 6774 5f73 6571 5f6c 656e  self.tgt_seq_len
-00013cb0: 6774 6829 2c0a 2020 2020 2020 2020 2020  gth),.          
-00013cc0: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-00013cd0: 2e6e 5f68 6561 642c 2073 656c 662e 7369  .n_head, self.si
-00013ce0: 7a65 5f70 6572 5f68 6561 6429 292c 0a20  ze_per_head)),. 
-00013cf0: 2020 2020 2020 2020 2020 206b 6579 5f74             key_t
-00013d00: 7261 6e73 706f 7365 5f73 6861 7065 290a  ranspose_shape).
-00013d10: 2020 2020 2020 2020 2320 7468 6520 7265          # the re
-00013d20: 7475 726e 6564 2073 6861 7065 2069 7320  turned shape is 
-00013d30: 5b62 732c 206e 756d 5f68 6561 6473 2c20  [bs, num_heads, 
-00013d40: 7365 715f 6c65 6e67 7468 2c20 7369 7a65  seq_length, size
-00013d50: 5f70 6572 5f68 6561 645d 0a20 2020 2020  _per_head].     
-00013d60: 2020 2076 616c 7565 203d 2073 656c 662e     value = self.
-00013d70: 7472 616e 7370 6f73 6528 0a20 2020 2020  transpose(.     
-00013d80: 2020 2020 2020 2046 2e72 6573 6861 7065         F.reshape
-00013d90: 280a 2020 2020 2020 2020 2020 2020 2020  (.              
-00013da0: 2020 7661 6c75 652c 0a20 2020 2020 2020    value,.       
-00013db0: 2020 2020 2020 2020 2028 6261 7463 685f           (batch_
-00013dc0: 7369 7a65 2c20 7365 6c66 2e5f 6765 745f  size, self._get_
-00013dd0: 7365 715f 6c65 6e67 7468 5f75 6e64 6572  seq_length_under
-00013de0: 5f69 6e63 7265 6d65 6e74 616c 2873 656c  _incremental(sel
-00013df0: 662e 7467 745f 7365 715f 6c65 6e67 7468  f.tgt_seq_length
-00013e00: 292c 0a20 2020 2020 2020 2020 2020 2020  ),.             
-00013e10: 2020 2020 7365 6c66 2e6e 5f68 6561 642c      self.n_head,
-00013e20: 2073 656c 662e 7369 7a65 5f70 6572 5f68   self.size_per_h
-00013e30: 6561 6429 292c 0a20 2020 2020 2020 2020  ead)),.         
-00013e40: 2020 2028 302c 2032 2c20 312c 2033 2929     (0, 2, 1, 3))
-00013e50: 0a20 2020 2020 2020 2023 2073 7570 706f  .        # suppo
-00013e60: 7274 2069 6e70 7574 2073 6861 7065 2069  rt input shape i
-00013e70: 7320 5b62 732c 2073 6571 2c20 7365 715d  s [bs, seq, seq]
-00013e80: 206f 7220 5b62 732c 2068 6561 6473 2c20   or [bs, heads, 
-00013e90: 7365 712c 2073 6571 5d0a 2020 2020 2020  seq, seq].      
-00013ea0: 2020 2320 7066 6120 616e 6420 6661 7320    # pfa and fas 
-00013eb0: 7573 6520 3464 206d 6173 6b0a 2020 2020  use 4d mask.    
-00013ec0: 2020 2020 6966 2061 7474 656e 7469 6f6e      if attention
-00013ed0: 5f6d 6173 6b20 6973 206e 6f74 204e 6f6e  _mask is not Non
-00013ee0: 6520 616e 6420 6c65 6e28 462e 7368 6170  e and len(F.shap
-00013ef0: 6528 6174 7465 6e74 696f 6e5f 6d61 736b  e(attention_mask
-00013f00: 2929 203d 3d20 3320 616e 6420 5c0a 2020  )) == 3 and \.  
-00013f10: 2020 2020 2020 2020 2020 2020 2020 2873                (s
-00013f20: 656c 662e 7573 655f 666c 6173 685f 6174  elf.use_flash_at
-00013f30: 7465 6e74 696f 6e20 6f72 206e 6f74 2073  tention or not s
-00013f40: 656c 662e 7573 655f 666c 6173 685f 6174  elf.use_flash_at
-00013f50: 7465 6e74 696f 6e20 6f72 0a20 2020 2020  tention or.     
-00013f60: 2020 2020 2020 2020 2020 2020 2828 6e6f              ((no
-00013f70: 7420 7365 6c66 2e74 7261 696e 696e 6729  t self.training)
-00013f80: 2061 6e64 2073 656c 662e 7573 655f 7072   and self.use_pr
-00013f90: 6f6d 7074 5f66 6c61 7368 5f61 7474 656e  ompt_flash_atten
-00013fa0: 7469 6f6e 2929 3a0a 2020 2020 2020 2020  tion)):.        
-00013fb0: 2020 2020 2320 6578 7061 6e64 2061 7474      # expand att
-00013fc0: 656e 7469 6f6e 206d 6173 6b20 6672 6f6d  ention mask from
-00013fd0: 205b 6273 2c20 7365 712c 2073 6571 5d20   [bs, seq, seq] 
-00013fe0: 2d3e 205b 6273 2c20 312c 2073 6571 2c20  -> [bs, 1, seq, 
-00013ff0: 7365 715d 0a20 2020 2020 2020 2020 2020  seq].           
-00014000: 2061 7474 656e 7469 6f6e 5f6d 6173 6b20   attention_mask 
-00014010: 3d20 7365 6c66 2e65 7870 616e 645f 6469  = self.expand_di
-00014020: 6d73 2861 7474 656e 7469 6f6e 5f6d 6173  ms(attention_mas
-00014030: 6b2c 2031 290a 2020 2020 2020 2020 2320  k, 1).        # 
-00014040: 6b65 7920 616e 6420 7661 6c75 6520 666f  key and value fo
-00014050: 7220 6375 7272 656e 7420 746f 6b65 6e28  r current token(
-00014060: 7329 0a20 2020 2020 2020 206b 6579 5f70  s).        key_p
-00014070: 7265 7365 6e74 203d 206b 6579 0a20 2020  resent = key.   
-00014080: 2020 2020 2076 616c 7565 5f70 7265 7365       value_prese
-00014090: 6e74 203d 2076 616c 7565 0a20 2020 2020  nt = value.     
-000140a0: 2020 2069 6620 7365 6c66 2e75 7365 5f70     if self.use_p
-000140b0: 6173 743a 0a20 2020 2020 2020 2020 2020  ast:.           
-000140c0: 2023 2054 6865 2066 6972 7374 2067 7261   # The first gra
-000140d0: 7068 2077 6974 6820 7468 6520 696e 7075  ph with the inpu
-000140e0: 7420 7369 7a65 206f 6620 2862 732c 2073  t size of (bs, s
-000140f0: 6571 5f6c 656e 6774 6829 0a20 2020 2020  eq_length).     
-00014100: 2020 2020 2020 2069 6620 7365 6c66 2e69         if self.i
-00014110: 735f 6669 7273 745f 6974 6572 6174 696f  s_first_iteratio
-00014120: 6e3a 0a20 2020 2020 2020 2020 2020 2020  n:.             
-00014130: 2020 2023 2047 6574 2074 6865 2076 616c     # Get the val
-00014140: 6964 2069 6e70 7574 206c 656e 6774 6820  id input length 
-00014150: 7769 7468 6f75 7420 7061 6464 696e 670a  without padding.
-00014160: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00014170: 7661 6c69 645f 6c65 6e67 7468 5f76 6563  valid_length_vec
-00014180: 746f 7220 3d20 462e 6361 7374 2873 656c  tor = F.cast(sel
-00014190: 662e 6c65 7373 2873 656c 662e 7261 6e67  f.less(self.rang
-000141a0: 652c 2062 6174 6368 5f76 616c 6964 5f6c  e, batch_valid_l
-000141b0: 656e 6774 682e 7669 6577 282d 312c 2031  ength.view(-1, 1
-000141c0: 2c20 3129 292c 2073 656c 662e 6474 7970  , 1)), self.dtyp
-000141d0: 6529 0a20 2020 2020 2020 2020 2020 2020  e).             
-000141e0: 2020 2023 2043 6f76 6572 2074 6865 206b     # Cover the k
-000141f0: 6579 2061 6e64 2076 616c 7565 206e 756d  ey and value num
-00014200: 6265 7273 2063 6f72 7265 7370 6f6e 6469  bers correspondi
-00014210: 6e67 2074 6f20 7468 6520 7061 6464 696e  ng to the paddin
-00014220: 6720 706f 7369 7469 6f6e 0a20 2020 2020  g position.     
-00014230: 2020 2020 2020 2020 2020 2065 7870 616e             expan
-00014240: 645f 6178 6973 203d 2033 2069 6620 646f  d_axis = 3 if do
-00014250: 5f64 6966 6665 7265 6e74 5f73 6861 7065  _different_shape
-00014260: 2065 6c73 6520 320a 2020 2020 2020 2020   else 2.        
-00014270: 2020 2020 2020 2020 6b65 795f 7072 6573          key_pres
-00014280: 656e 7420 3d20 7365 6c66 2e6d 756c 3128  ent = self.mul1(
-00014290: 6b65 792c 2073 656c 662e 6578 7061 6e64  key, self.expand
-000142a0: 5f64 696d 7328 7661 6c69 645f 6c65 6e67  _dims(valid_leng
-000142b0: 7468 5f76 6563 746f 722c 2065 7870 616e  th_vector, expan
-000142c0: 645f 6178 6973 2929 0a20 2020 2020 2020  d_axis)).       
-000142d0: 2020 2020 2020 2020 2076 616c 7565 5f70           value_p
-000142e0: 7265 7365 6e74 203d 2073 656c 662e 6d75  resent = self.mu
-000142f0: 6c31 2876 616c 7565 2c20 7365 6c66 2e65  l1(value, self.e
-00014300: 7870 616e 645f 6469 6d73 2876 616c 6964  xpand_dims(valid
-00014310: 5f6c 656e 6774 685f 7665 6374 6f72 2c20  _length_vector, 
-00014320: 3329 290a 2020 2020 2020 2020 2020 2020  3)).            
-00014330: 2320 5468 6520 7365 636f 6e64 2067 7261  # The second gra
-00014340: 7068 2077 6974 6820 7468 6520 696e 7075  ph with the inpu
-00014350: 7320 7369 7a65 206f 6620 2862 732c 2031  s size of (bs, 1
-00014360: 290a 2020 2020 2020 2020 2020 2020 2320  ).            # 
-00014370: 7468 6520 7368 6170 6520 6f66 2071 7565  the shape of que
-00014380: 7279 2069 7320 2862 732c 206e 756d 5f68  ry is (bs, num_h
-00014390: 6561 6473 2c20 312c 2073 697a 655f 7065  eads, 1, size_pe
-000143a0: 725f 6865 6164 290a 2020 2020 2020 2020  r_head).        
-000143b0: 2020 2020 2320 7468 6520 7368 6170 6520      # the shape 
-000143c0: 6f66 206b 6579 2069 7320 2020 2862 732c  of key is   (bs,
-000143d0: 206e 756d 5f68 6561 6473 2c20 7369 7a65   num_heads, size
-000143e0: 5f70 6572 5f68 6561 642c 2031 290a 2020  _per_head, 1).  
-000143f0: 2020 2020 2020 2020 2020 2320 7468 6520            # the 
-00014400: 7368 6170 6520 6f66 2076 616c 7565 2069  shape of value i
-00014410: 7320 2862 732c 206e 756d 5f68 6561 6473  s (bs, num_heads
-00014420: 2c20 312c 2073 697a 655f 7065 725f 6865  , 1, size_per_he
-00014430: 6164 290a 2020 2020 2020 2020 2020 2020  ad).            
-00014440: 656c 7365 3a0a 2020 2020 2020 2020 2020  else:.          
-00014450: 2020 2020 2020 2320 4765 7420 7468 6520        # Get the 
-00014460: 6375 7272 656e 7420 746f 6b65 6e20 706f  current token po
-00014470: 7369 7469 6f6e 2069 6e64 6578 0a20 2020  sition index.   
-00014480: 2020 2020 2020 2020 2020 2020 2076 616c               val
-00014490: 6964 5f6c 656e 6774 6820 3d20 6261 7463  id_length = batc
-000144a0: 685f 7661 6c69 645f 6c65 6e67 7468 202d  h_valid_length -
-000144b0: 2031 0a20 2020 2020 2020 2020 2020 2020   1.             
-000144c0: 2020 2076 616c 6964 5f6c 656e 6774 6820     valid_length 
-000144d0: 3d20 7365 6c66 2e72 6573 6861 7065 2876  = self.reshape(v
-000144e0: 616c 6964 5f6c 656e 6774 682c 2028 2d31  alid_length, (-1
-000144f0: 2c20 312c 2031 2929 0a20 2020 2020 2020  , 1, 1)).       
-00014500: 2020 2020 2020 2020 2076 616c 6964 5f6c           valid_l
-00014510: 656e 6774 685f 7665 6374 6f72 203d 2046  ength_vector = F
-00014520: 2e63 6173 7428 7365 6c66 2e65 7175 616c  .cast(self.equal
-00014530: 2876 616c 6964 5f6c 656e 6774 682c 2073  (valid_length, s
-00014540: 656c 662e 7261 6e67 6529 2c20 7365 6c66  elf.range), self
-00014550: 2e64 7479 7065 290a 2020 2020 2020 2020  .dtype).        
-00014560: 2020 2020 2020 2020 2320 5061 6420 7468          # Pad th
-00014570: 6520 6b65 7920 616e 6420 7661 6c75 6520  e key and value 
-00014580: 746f 2073 6571 5f6c 656e 6774 6820 7769  to seq_length wi
-00014590: 7468 206f 6e6c 7920 7468 6520 706f 7369  th only the posi
-000145a0: 7469 6f6e 2069 6e64 6578 206e 6f74 207a  tion index not z
-000145b0: 6572 6f0a 2020 2020 2020 2020 2020 2020  ero.            
-000145c0: 2020 2020 6966 2064 6f5f 6469 6666 6572      if do_differ
-000145d0: 656e 745f 7368 6170 653a 0a20 2020 2020  ent_shape:.     
-000145e0: 2020 2020 2020 2020 2020 2020 2020 206d                 m
-000145f0: 756c 7469 706c 6573 203d 2028 312c 2031  ultiples = (1, 1
-00014600: 2c20 7365 6c66 2e73 6571 5f6c 656e 6774  , self.seq_lengt
-00014610: 682c 2031 290a 2020 2020 2020 2020 2020  h, 1).          
-00014620: 2020 2020 2020 2020 2020 6578 7061 6e64            expand
-00014630: 5f61 7869 7320 3d20 330a 2020 2020 2020  _axis = 3.      
-00014640: 2020 2020 2020 2020 2020 656c 7365 3a0a            else:.
-00014650: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00014660: 2020 2020 6d75 6c74 6970 6c65 7320 3d20      multiples = 
-00014670: 2831 2c20 312c 2031 2c20 7365 6c66 2e73  (1, 1, 1, self.s
-00014680: 6571 5f6c 656e 6774 6829 0a20 2020 2020  eq_length).     
-00014690: 2020 2020 2020 2020 2020 2020 2020 2065                 e
-000146a0: 7870 616e 645f 6178 6973 203d 2032 0a20  xpand_axis = 2. 
-000146b0: 2020 2020 2020 2020 2020 2020 2020 2063                 c
-000146c0: 7572 7265 6e74 5f6b 6579 203d 2073 656c  urrent_key = sel
-000146d0: 662e 6d75 6c31 2873 656c 662e 7469 6c65  f.mul1(self.tile
-000146e0: 286b 6579 2c20 6d75 6c74 6970 6c65 7329  (key, multiples)
-000146f0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00014700: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00014710: 2020 2020 2020 2020 2020 7365 6c66 2e65            self.e
-00014720: 7870 616e 645f 6469 6d73 2876 616c 6964  xpand_dims(valid
-00014730: 5f6c 656e 6774 685f 7665 6374 6f72 2c20  _length_vector, 
-00014740: 6578 7061 6e64 5f61 7869 7329 290a 2020  expand_axis)).  
-00014750: 2020 2020 2020 2020 2020 2020 2020 6375                cu
-00014760: 7272 656e 745f 7661 6c75 6520 3d20 7365  rrent_value = se
-00014770: 6c66 2e6d 756c 3128 7365 6c66 2e74 696c  lf.mul1(self.til
-00014780: 6528 7661 6c75 652c 2028 312c 2031 2c20  e(value, (1, 1, 
-00014790: 7365 6c66 2e73 6571 5f6c 656e 6774 682c  self.seq_length,
-000147a0: 2031 2929 2c0a 2020 2020 2020 2020 2020   1)),.          
-000147b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000147c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000147d0: 7365 6c66 2e65 7870 616e 645f 6469 6d73  self.expand_dims
-000147e0: 2876 616c 6964 5f6c 656e 6774 685f 7665  (valid_length_ve
-000147f0: 6374 6f72 2c20 3329 290a 2020 2020 2020  ctor, 3)).      
-00014800: 2020 2020 2020 2020 2020 2320 436f 6e63            # Conc
-00014810: 6174 2074 6865 2070 7265 7669 6f75 7320  at the previous 
-00014820: 7361 7665 6420 7374 6174 6520 616e 6420  saved state and 
-00014830: 6375 7272 656e 7420 7374 6174 650a 2020  current state.  
-00014840: 2020 2020 2020 2020 2020 2020 2020 6b65                ke
-00014850: 7920 3d20 7365 6c66 2e61 6464 286b 6579  y = self.add(key
-00014860: 5f70 6173 742c 2063 7572 7265 6e74 5f6b  _past, current_k
-00014870: 6579 290a 2020 2020 2020 2020 2020 2020  ey).            
-00014880: 2020 2020 7661 6c75 6520 3d20 7365 6c66      value = self
-00014890: 2e61 6464 2876 616c 7565 5f70 6173 742c  .add(value_past,
-000148a0: 2063 7572 7265 6e74 5f76 616c 7565 290a   current_value).
-000148b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000148c0: 2320 5570 6461 7465 206b 6579 5f70 7265  # Update key_pre
-000148d0: 7365 6e74 2061 6e64 2076 616c 7565 5f70  sent and value_p
-000148e0: 7265 7365 6e74 2066 6f72 2073 7461 7465  resent for state
-000148f0: 2075 7064 6174 650a 2020 2020 2020 2020   update.        
-00014900: 2020 2020 2020 2020 6b65 795f 7072 6573          key_pres
-00014910: 656e 7420 3d20 6b65 790a 2020 2020 2020  ent = key.      
-00014920: 2020 2020 2020 2020 2020 7661 6c75 655f            value_
-00014930: 7072 6573 656e 7420 3d20 7661 6c75 650a  present = value.
-00014940: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00014950: 6174 7465 6e74 696f 6e5f 6d61 736b 203d  attention_mask =
-00014960: 2046 2e72 6573 6861 7065 2873 656c 662e   F.reshape(self.
-00014970: 6174 7465 6e74 696f 6e5f 6d61 736b 2c20  attention_mask, 
-00014980: 2873 656c 662e 7365 715f 6c65 6e67 7468  (self.seq_length
-00014990: 2c20 7365 6c66 2e73 6571 5f6c 656e 6774  , self.seq_lengt
-000149a0: 682c 2031 2c20 3129 290a 0a20 2020 2020  h, 1, 1))..     
-000149b0: 2020 206c 6179 6572 5f70 7265 7365 6e74     layer_present
-000149c0: 203d 2028 6b65 795f 7072 6573 656e 742c   = (key_present,
-000149d0: 2076 616c 7565 5f70 7265 7365 6e74 290a   value_present).
-000149e0: 2020 2020 2020 2020 2320 6d75 6c74 6920          # multi 
-000149f0: 6865 6164 2061 7474 656e 7469 6f6e 2063  head attention c
-00014a00: 6f6e 7369 6465 7269 6e67 2061 7474 656e  onsidering atten
-00014a10: 7469 6f6e 206d 6173 6b0a 2020 2020 2020  tion mask.      
-00014a20: 2020 2320 7468 6520 7265 7475 726e 2073    # the return s
-00014a30: 6861 7065 2069 7320 5b62 7320 2a20 7365  hape is [bs * se
-00014a40: 715f 6c65 6e67 7468 2c20 6869 6464 656e  q_length, hidden
-00014a50: 5f73 697a 655d 0a20 2020 2020 2020 2069  _size].        i
-00014a60: 6620 6e6f 7420 7365 6c66 2e74 7261 696e  f not self.train
-00014a70: 696e 6720 616e 6420 7365 6c66 2e75 7365  ing and self.use
-00014a80: 5f70 726f 6d70 745f 666c 6173 685f 6174  _prompt_flash_at
-00014a90: 7465 6e74 696f 6e3a 0a20 2020 2020 2020  tention:.       
-00014aa0: 2020 2020 2069 6620 7365 6c66 2e75 7365       if self.use
-00014ab0: 5f70 6173 7420 616e 6420 6e6f 7420 7365  _past and not se
-00014ac0: 6c66 2e69 735f 6669 7273 745f 6974 6572  lf.is_first_iter
-00014ad0: 6174 696f 6e3a 0a20 2020 2020 2020 2020  ation:.         
-00014ae0: 2020 2020 2020 206b 6579 203d 2073 656c         key = sel
-00014af0: 662e 7472 616e 7370 6f73 6528 6b65 792c  f.transpose(key,
-00014b00: 2028 302c 2031 2c20 332c 2032 2929 0a20   (0, 1, 3, 2)). 
-00014b10: 2020 2020 2020 2020 2020 2020 2020 2061                 a
-00014b20: 7474 656e 7469 6f6e 203d 2073 656c 662e  ttention = self.
-00014b30: 5f61 7474 6e28 7175 6572 792c 206b 6579  _attn(query, key
-00014b40: 2c20 7661 6c75 652c 2061 7474 656e 7469  , value, attenti
-00014b50: 6f6e 5f6d 6173 6b29 0a20 2020 2020 2020  on_mask).       
-00014b60: 2020 2020 2065 6c73 653a 0a20 2020 2020       else:.     
-00014b70: 2020 2020 2020 2020 2020 2071 7565 7279             query
-00014b80: 2c20 6b65 792c 2061 7474 656e 7469 6f6e  , key, attention
-00014b90: 5f6d 6173 6b20 3d20 7365 6c66 2e5f 7066  _mask = self._pf
-00014ba0: 615f 6966 615f 6461 7461 5f70 7265 7072  a_ifa_data_prepr
-00014bb0: 6f63 6573 7328 7175 6572 792c 206b 6579  ocess(query, key
-00014bc0: 2c20 6174 7465 6e74 696f 6e5f 6d61 736b  , attention_mask
-00014bd0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00014be0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00014bf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00014c00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00014c10: 2020 2020 2020 2020 2020 2020 2062 6174               bat
-00014c20: 6368 5f76 616c 6964 5f6c 656e 6774 6829  ch_valid_length)
-00014c30: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00014c40: 2061 7474 656e 7469 6f6e 203d 2073 656c   attention = sel
-00014c50: 662e 7072 6f6d 7074 5f66 6c61 7368 5f61  f.prompt_flash_a
-00014c60: 7474 656e 7469 6f6e 2871 7565 7279 2c20  ttention(query, 
-00014c70: 6b65 792c 2076 616c 7565 2c20 6174 7465  key, value, atte
-00014c80: 6e74 696f 6e5f 6d61 736b 2c0a 2020 2020  ntion_mask,.    
-00014c90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00014ca0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00014cb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00014cc0: 2020 2020 4e6f 6e65 2c20 4e6f 6e65 2c20      None, None, 
-00014cd0: 4e6f 6e65 2c20 4e6f 6e65 2c20 4e6f 6e65  None, None, None
-00014ce0: 2c20 4e6f 6e65 2c20 4e6f 6e65 2c20 4e6f  , None, None, No
-00014cf0: 6e65 290a 2020 2020 2020 2020 2020 2020  ne).            
-00014d00: 2020 2020 6174 7465 6e74 696f 6e20 3d20      attention = 
-00014d10: 7365 6c66 2e5f 6d65 7267 655f 6865 6164  self._merge_head
-00014d20: 7328 6174 7465 6e74 696f 6e29 0a20 2020  s(attention).   
-00014d30: 2020 2020 2065 6c69 6620 7365 6c66 2e75       elif self.u
-00014d40: 7365 5f66 6c61 7368 5f61 7474 656e 7469  se_flash_attenti
-00014d50: 6f6e 3a0a 2020 2020 2020 2020 2020 2020  on:.            
-00014d60: 6174 7465 6e74 696f 6e5f 6d61 736b 203d  attention_mask =
-00014d70: 2073 656c 662e 6361 7374 2861 7474 656e   self.cast(atten
-00014d80: 7469 6f6e 5f6d 6173 6b2c 206d 7374 7970  tion_mask, mstyp
-00014d90: 652e 7569 6e74 3829 0a20 2020 2020 2020  e.uint8).       
-00014da0: 2020 2020 2061 7474 656e 7469 6f6e 203d       attention =
-00014db0: 2073 656c 662e 5f66 6c61 7368 5f61 7474   self._flash_att
-00014dc0: 6e28 7175 6572 792c 206b 6579 2c20 7661  n(query, key, va
-00014dd0: 6c75 652c 2061 7474 656e 7469 6f6e 5f6d  lue, attention_m
-00014de0: 6173 6b29 0a20 2020 2020 2020 2065 6c73  ask).        els
-00014df0: 653a 0a20 2020 2020 2020 2020 2020 2061  e:.            a
-00014e00: 7474 656e 7469 6f6e 203d 2073 656c 662e  ttention = self.
-00014e10: 5f61 7474 6e28 7175 6572 792c 206b 6579  _attn(query, key
-00014e20: 2c20 7661 6c75 652c 2061 7474 656e 7469  , value, attenti
-00014e30: 6f6e 5f6d 6173 6b29 0a20 2020 2020 2020  on_mask).       
-00014e40: 2023 204f 7574 7075 740a 2020 2020 2020   # Output.      
-00014e50: 2020 6f75 7470 7574 203d 2073 656c 662e    output = self.
-00014e60: 7072 6f6a 6563 7469 6f6e 2861 7474 656e  projection(atten
-00014e70: 7469 6f6e 290a 2020 2020 2020 2020 6f75  tion).        ou
-00014e80: 7470 7574 203d 2073 656c 662e 6472 6f70  tput = self.drop
-00014e90: 6f75 7428 6f75 7470 7574 290a 2020 2020  out(output).    
-00014ea0: 2020 2020 6f75 7470 7574 203d 2046 2e72      output = F.r
-00014eb0: 6573 6861 7065 286f 7574 7075 742c 206f  eshape(output, o
-00014ec0: 7269 5f73 6861 7065 290a 2020 2020 2020  ri_shape).      
-00014ed0: 2020 6f75 7470 7574 203d 2046 2e63 6173    output = F.cas
-00014ee0: 7428 6f75 7470 7574 2c20 6f72 695f 6474  t(output, ori_dt
-00014ef0: 7970 6529 0a20 2020 2020 2020 2072 6574  ype).        ret
-00014f00: 7572 6e20 6f75 7470 7574 2c20 6c61 7965  urn output, laye
-00014f10: 725f 7072 6573 656e 740a 0a20 2020 2064  r_present..    d
-00014f20: 6566 205f 6765 745f 6261 7463 685f 7369  ef _get_batch_si
-00014f30: 7a65 5f66 726f 6d5f 7175 6572 7928 7365  ze_from_query(se
-00014f40: 6c66 2c20 7175 6572 7929 3a0a 2020 2020  lf, query):.    
-00014f50: 2020 2020 7222 2222 4765 7420 7468 6520      r"""Get the 
-00014f60: 6261 7463 6820 7369 7a65 2066 726f 6d20  batch size from 
-00014f70: 7175 6572 7920 7465 6e73 6f72 2222 220a  query tensor""".
-00014f80: 2020 2020 2020 2020 2320 466f 7220 7468          # For th
-00014f90: 6520 696e 6372 656d 656e 7461 6c20 7072  e incremental pr
-00014fa0: 6564 6963 7469 6f6e 2c20 7468 6520 7365  ediction, the se
-00014fb0: 7120 6c65 6e67 7468 2066 6f72 2074 6865  q length for the
-00014fc0: 2069 6e70 7574 2069 7320 312e 0a20 2020   input is 1..   
-00014fd0: 2020 2020 2069 6620 6c65 6e28 462e 7368       if len(F.sh
-00014fe0: 6170 6528 7175 6572 7929 2920 3d3d 2032  ape(query)) == 2
-00014ff0: 2061 6e64 2028 2873 656c 662e 7573 655f   and ((self.use_
-00015000: 7061 7374 2061 6e64 2073 656c 662e 6973  past and self.is
-00015010: 5f66 6972 7374 5f69 7465 7261 7469 6f6e  _first_iteration
-00015020: 2920 6f72 2028 6e6f 7420 7365 6c66 2e75  ) or (not self.u
-00015030: 7365 5f70 6173 7429 293a 0a20 2020 2020  se_past)):.     
-00015040: 2020 2020 2020 2072 6574 7572 6e20 462e         return F.
-00015050: 7368 6170 6528 7175 6572 7929 5b30 5d20  shape(query)[0] 
-00015060: 2f2f 2073 656c 662e 7372 635f 7365 715f  // self.src_seq_
-00015070: 6c65 6e67 7468 0a20 2020 2020 2020 2072  length.        r
-00015080: 6574 7572 6e20 462e 7368 6170 6528 7175  eturn F.shape(qu
-00015090: 6572 7929 5b30 5d0a 0a20 2020 2064 6566  ery)[0]..    def
-000150a0: 205f 6765 745f 7365 715f 6c65 6e67 7468   _get_seq_length
-000150b0: 5f75 6e64 6572 5f69 6e63 7265 6d65 6e74  _under_increment
-000150c0: 616c 2873 656c 662c 206c 656e 6774 6829  al(self, length)
-000150d0: 3a0a 2020 2020 2020 2020 7222 2222 5265  :.        r"""Re
-000150e0: 7475 726e 2074 6865 206c 656e 6774 6820  turn the length 
-000150f0: 6f66 2074 6865 2074 656e 736f 722e 0a20  of the tensor.. 
-00015100: 2020 2020 2020 2020 2020 2046 6f72 2074             For t
-00015110: 6865 2069 6e63 7265 6d65 6e74 616c 2070  he incremental p
-00015120: 7265 6469 6374 696f 6e2c 2074 6865 2073  rediction, the s
-00015130: 6571 206c 656e 6774 6820 666f 7220 7468  eq length for th
-00015140: 6520 696e 7075 7420 6973 2031 2e0a 2020  e input is 1..  
-00015150: 2020 2020 2020 2222 220a 2020 2020 2020        """.      
-00015160: 2020 6966 2073 656c 662e 7573 655f 7061    if self.use_pa
-00015170: 7374 2061 6e64 206e 6f74 2073 656c 662e  st and not self.
-00015180: 6973 5f66 6972 7374 5f69 7465 7261 7469  is_first_iterati
-00015190: 6f6e 3a0a 2020 2020 2020 2020 2020 2020  on:.            
-000151a0: 7265 7475 726e 2031 0a20 2020 2020 2020  return 1.       
-000151b0: 2072 6574 7572 6e20 6c65 6e67 7468 0a0a   return length..
-000151c0: 2020 2020 6465 6620 5f70 6661 5f69 6661      def _pfa_ifa
-000151d0: 5f64 6174 615f 7072 6570 726f 6365 7373  _data_preprocess
-000151e0: 2873 656c 662c 2071 7565 7279 2c20 6b65  (self, query, ke
-000151f0: 792c 2061 7474 656e 7469 6f6e 5f6d 6173  y, attention_mas
-00015200: 6b2c 2062 6174 6368 5f76 616c 6964 5f6c  k, batch_valid_l
-00015210: 656e 6774 6829 3a0a 2020 2020 2020 2020  ength):.        
-00015220: 7222 2222 5265 7475 726e 2070 726f 6365  r"""Return proce
-00015230: 7373 6564 2071 2c20 6b20 616e 6420 6174  ssed q, k and at
-00015240: 7465 6e74 696f 6e20 6d61 736b 2222 220a  tention mask""".
-00015250: 2020 2020 2020 2020 6966 2073 656c 662e          if self.
-00015260: 7573 655f 7061 7374 2061 6e64 206e 6f74  use_past and not
-00015270: 2073 656c 662e 6973 5f66 6972 7374 5f69   self.is_first_i
-00015280: 7465 7261 7469 6f6e 3a0a 2020 2020 2020  teration:.      
-00015290: 2020 2020 2020 2320 4765 7420 7468 6520        # Get the 
-000152a0: 7072 6563 6973 6520 706f 7369 7469 6f6e  precise position
-000152b0: 2069 6e64 6578 0a20 2020 2020 2020 2020   index.         
-000152c0: 2020 2063 7572 7265 6e74 5f69 6e64 6578     current_index
-000152d0: 203d 2062 6174 6368 5f76 616c 6964 5f6c   = batch_valid_l
-000152e0: 656e 6774 682e 7371 7565 657a 6528 3029  ength.squeeze(0)
-000152f0: 0a20 2020 2020 2020 2020 2020 2069 6e64  .            ind
-00015300: 6578 203d 2073 656c 662e 7375 6231 2846  ex = self.sub1(F
-00015310: 2e63 6173 7428 6375 7272 656e 745f 696e  .cast(current_in
-00015320: 6465 782c 206d 7374 7970 652e 696e 7433  dex, mstype.int3
-00015330: 3229 2c20 3129 0a20 2020 2020 2020 2020  2), 1).         
-00015340: 2020 2069 6e64 6578 203d 2046 2e72 6573     index = F.res
-00015350: 6861 7065 2869 6e64 6578 2c20 282d 312c  hape(index, (-1,
-00015360: 2031 2c20 3129 290a 2020 2020 2020 2020   1, 1)).        
-00015370: 2020 2020 2320 4361 6c63 756c 6174 6520      # Calculate 
-00015380: 7468 6520 6174 7465 6e74 696f 6e5f 6d61  the attention_ma
-00015390: 736b 206d 6174 7269 7820 7669 6120 7468  sk matrix via th
-000153a0: 6520 706f 7369 7469 6f6e 2069 6e64 6578  e position index
-000153b0: 0a20 2020 2020 2020 2020 2020 2061 7474  .            att
-000153c0: 656e 7469 6f6e 5f6d 6173 6b20 3d20 462e  ention_mask = F.
-000153d0: 6361 7374 2873 656c 662e 7465 6e73 6f72  cast(self.tensor
-000153e0: 5f6c 6528 7365 6c66 2e72 616e 6765 2c20  _le(self.range, 
-000153f0: 696e 6465 7829 2c20 502e 4454 7970 6528  index), P.DType(
-00015400: 2928 7175 6572 7929 290a 2020 2020 2020  )(query)).      
-00015410: 2020 2020 2020 6174 7465 6e74 696f 6e5f        attention_
-00015420: 6d61 736b 203d 2073 656c 662e 6578 7061  mask = self.expa
-00015430: 6e64 5f64 696d 7328 6174 7465 6e74 696f  nd_dims(attentio
-00015440: 6e5f 6d61 736b 2c20 3229 0a0a 2020 2020  n_mask, 2)..    
-00015450: 2020 2020 6174 7465 6e74 696f 6e5f 6d61      attention_ma
-00015460: 736b 203d 2073 656c 662e 7375 625f 7066  sk = self.sub_pf
-00015470: 6128 0a20 2020 2020 2020 2020 2020 2050  a(.            P
-00015480: 2e43 6173 7428 2928 7365 6c66 2e6f 6e65  .Cast()(self.one
-00015490: 2c20 502e 4454 7970 6528 2928 7175 6572  , P.DType()(quer
-000154a0: 7929 292c 0a20 2020 2020 2020 2020 2020  y)),.           
-000154b0: 2050 2e43 6173 7428 2928 6174 7465 6e74   P.Cast()(attent
-000154c0: 696f 6e5f 6d61 736b 2c20 502e 4454 7970  ion_mask, P.DTyp
-000154d0: 6528 2928 7175 6572 7929 2929 0a0a 2020  e()(query)))..  
-000154e0: 2020 2020 2020 7265 7475 726e 2071 7565        return que
-000154f0: 7279 2c20 6b65 792c 2061 7474 656e 7469  ry, key, attenti
-00015500: 6f6e 5f6d 6173 6b0a 0a20 2020 2064 6566  on_mask..    def
-00015510: 205f 6368 6563 6b5f 696e 7075 7473 2873   _check_inputs(s
-00015520: 656c 662c 2071 7565 7279 5f74 656e 736f  elf, query_tenso
-00015530: 722c 206b 6579 5f74 656e 736f 722c 2076  r, key_tensor, v
-00015540: 616c 7565 5f74 656e 736f 722c 2061 7474  alue_tensor, att
-00015550: 656e 7469 6f6e 5f6d 6173 6b2c 206b 6579  ention_mask, key
-00015560: 5f70 6173 743d 4e6f 6e65 2c0a 2020 2020  _past=None,.    
-00015570: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00015580: 2020 7661 6c75 655f 7061 7374 3d4e 6f6e    value_past=Non
-00015590: 652c 2062 6174 6368 5f76 616c 6964 5f6c  e, batch_valid_l
-000155a0: 656e 6774 683d 4e6f 6e65 293a 0a20 2020  ength=None):.   
-000155b0: 2020 2020 2072 2222 2243 6865 636b 2069       r"""Check i
-000155c0: 6e70 7574 7322 2222 0a20 2020 2020 2020  nputs""".       
-000155d0: 205f 6368 6563 6b5f 696e 7075 745f 6474   _check_input_dt
-000155e0: 7970 6528 462e 6474 7970 6528 7175 6572  ype(F.dtype(quer
-000155f0: 795f 7465 6e73 6f72 292c 2022 7175 6572  y_tensor), "quer
-00015600: 795f 7465 6e73 6f72 222c 0a20 2020 2020  y_tensor",.     
-00015610: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00015620: 2020 2020 2020 5b6d 7374 7970 652e 666c        [mstype.fl
-00015630: 6f61 7433 322c 206d 7374 7970 652e 666c  oat32, mstype.fl
-00015640: 6f61 7431 362c 206d 7374 7970 652e 6266  oat16, mstype.bf
-00015650: 6c6f 6174 3136 5d2c 2073 656c 662e 636c  loat16], self.cl
-00015660: 735f 6e61 6d65 290a 2020 2020 2020 2020  s_name).        
-00015670: 5f63 6865 636b 5f69 6e70 7574 5f64 7479  _check_input_dty
-00015680: 7065 2846 2e64 7479 7065 286b 6579 5f74  pe(F.dtype(key_t
-00015690: 656e 736f 7229 2c20 226b 6579 5f74 656e  ensor), "key_ten
-000156a0: 736f 7222 2c0a 2020 2020 2020 2020 2020  sor",.          
-000156b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000156c0: 205b 6d73 7479 7065 2e66 6c6f 6174 3332   [mstype.float32
-000156d0: 2c20 6d73 7479 7065 2e66 6c6f 6174 3136  , mstype.float16
-000156e0: 2c20 6d73 7479 7065 2e62 666c 6f61 7431  , mstype.bfloat1
-000156f0: 365d 2c20 7365 6c66 2e63 6c73 5f6e 616d  6], self.cls_nam
-00015700: 6529 0a20 2020 2020 2020 205f 6368 6563  e).        _chec
-00015710: 6b5f 696e 7075 745f 6474 7970 6528 462e  k_input_dtype(F.
-00015720: 6474 7970 6528 7661 6c75 655f 7465 6e73  dtype(value_tens
-00015730: 6f72 292c 2022 7661 6c75 655f 7465 6e73  or), "value_tens
-00015740: 6f72 222c 0a20 2020 2020 2020 2020 2020  or",.           
-00015750: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00015760: 5b6d 7374 7970 652e 666c 6f61 7433 322c  [mstype.float32,
-00015770: 206d 7374 7970 652e 666c 6f61 7431 362c   mstype.float16,
-00015780: 206d 7374 7970 652e 6266 6c6f 6174 3136   mstype.bfloat16
-00015790: 5d2c 2073 656c 662e 636c 735f 6e61 6d65  ], self.cls_name
-000157a0: 290a 2020 2020 2020 2020 6966 2061 7474  ).        if att
-000157b0: 656e 7469 6f6e 5f6d 6173 6b20 6973 206e  ention_mask is n
-000157c0: 6f74 204e 6f6e 653a 0a20 2020 2020 2020  ot None:.       
-000157d0: 2020 2020 205f 6368 6563 6b5f 696e 7075       _check_inpu
-000157e0: 745f 6474 7970 6528 462e 6474 7970 6528  t_dtype(F.dtype(
-000157f0: 6174 7465 6e74 696f 6e5f 6d61 736b 292c  attention_mask),
-00015800: 2022 6174 7465 6e74 696f 6e5f 6d61 736b   "attention_mask
-00015810: 222c 0a20 2020 2020 2020 2020 2020 2020  ",.             
-00015820: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00015830: 2020 5b6d 7374 7970 652e 666c 6f61 7433    [mstype.float3
-00015840: 322c 206d 7374 7970 652e 666c 6f61 7431  2, mstype.float1
-00015850: 362c 206d 7374 7970 652e 6266 6c6f 6174  6, mstype.bfloat
-00015860: 3136 5d2c 0a20 2020 2020 2020 2020 2020  16],.           
-00015870: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00015880: 2020 2020 7365 6c66 2e63 6c73 5f6e 616d      self.cls_nam
-00015890: 6529 0a0a 2020 2020 2020 2020 6261 7463  e)..        batc
-000158a0: 685f 7661 6c69 645f 6c65 6e67 7468 5f69  h_valid_length_i
-000158b0: 735f 7465 6e73 6f72 203d 2069 7369 6e73  s_tensor = isins
-000158c0: 7461 6e63 6528 6261 7463 685f 7661 6c69  tance(batch_vali
-000158d0: 645f 6c65 6e67 7468 2c20 5465 6e73 6f72  d_length, Tensor
-000158e0: 290a 2020 2020 2020 2020 6261 7463 685f  ).        batch_
-000158f0: 6973 5f64 6566 6175 6c74 203d 2062 6174  is_default = bat
-00015900: 6368 5f76 616c 6964 5f6c 656e 6774 6820  ch_valid_length 
-00015910: 6973 204e 6f6e 650a 2020 2020 2020 2020  is None.        
-00015920: 5f63 6865 636b 5f70 6173 745f 6e6f 6e65  _check_past_none
-00015930: 5f69 6e70 7574 5f6e 6f6e 6528 7365 6c66  _input_none(self
-00015940: 2e75 7365 5f70 6173 742c 2022 6261 7463  .use_past, "batc
-00015950: 685f 7661 6c69 645f 6c65 6e67 7468 222c  h_valid_length",
-00015960: 2073 656c 662e 636c 735f 6e61 6d65 2c20   self.cls_name, 
-00015970: 4e6f 6e65 2c0a 2020 2020 2020 2020 2020  None,.          
-00015980: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00015990: 2020 2020 2020 2020 2020 6261 7463 685f            batch_
-000159a0: 7661 6c69 645f 6c65 6e67 7468 5f69 735f  valid_length_is_
-000159b0: 7465 6e73 6f72 2c20 6261 7463 685f 6973  tensor, batch_is
-000159c0: 5f64 6566 6175 6c74 290a 2020 2020 2020  _default).      
-000159d0: 2020 6966 2073 656c 662e 7573 655f 7061    if self.use_pa
-000159e0: 7374 3a0a 2020 2020 2020 2020 2020 2020  st:.            
-000159f0: 5f63 6865 636b 5f69 6e70 7574 5f64 7479  _check_input_dty
-00015a00: 7065 2846 2e64 7479 7065 286b 6579 5f70  pe(F.dtype(key_p
-00015a10: 6173 7429 2c20 226b 6579 5f70 6173 7422  ast), "key_past"
-00015a20: 2c20 5b6d 7374 7970 652e 666c 6f61 7431  , [mstype.float1
-00015a30: 362c 206d 7374 7970 652e 6266 6c6f 6174  6, mstype.bfloat
-00015a40: 3136 5d2c 2073 656c 662e 636c 735f 6e61  16], self.cls_na
-00015a50: 6d65 290a 2020 2020 2020 2020 2020 2020  me).            
-00015a60: 5f63 6865 636b 5f69 6e70 7574 5f64 7479  _check_input_dty
-00015a70: 7065 2846 2e64 7479 7065 2876 616c 7565  pe(F.dtype(value
-00015a80: 5f70 6173 7429 2c20 2276 616c 7565 5f70  _past), "value_p
-00015a90: 6173 7422 2c20 5b6d 7374 7970 652e 666c  ast", [mstype.fl
-00015aa0: 6f61 7431 362c 206d 7374 7970 652e 6266  oat16, mstype.bf
-00015ab0: 6c6f 6174 3136 5d2c 2073 656c 662e 636c  loat16], self.cl
-00015ac0: 735f 6e61 6d65 290a 2020 2020 2020 2020  s_name).        
-00015ad0: 2020 2020 5f63 6865 636b 5f69 6e70 7574      _check_input
-00015ae0: 5f64 7479 7065 2846 2e64 7479 7065 2862  _dtype(F.dtype(b
-00015af0: 6174 6368 5f76 616c 6964 5f6c 656e 6774  atch_valid_lengt
-00015b00: 6829 2c20 2262 6174 6368 5f76 616c 6964  h), "batch_valid
-00015b10: 5f6c 656e 6774 6822 2c20 5b6d 7374 7970  _length", [mstyp
-00015b20: 652e 696e 7433 325d 2c20 7365 6c66 2e63  e.int32], self.c
-00015b30: 6c73 5f6e 616d 6529 0a20 2020 2020 2020  ls_name).       
-00015b40: 2072 6574 7572 6e20 5472 7565 0a0a 2020   return True..  
-00015b50: 2020 6465 6620 5f63 6f6e 7665 7274 5f74    def _convert_t
-00015b60: 6f5f 3264 5f74 656e 736f 7228 7365 6c66  o_2d_tensor(self
-00015b70: 2c20 7175 6572 795f 7465 6e73 6f72 2c20  , query_tensor, 
-00015b80: 6b65 795f 7465 6e73 6f72 2c20 7661 6c75  key_tensor, valu
-00015b90: 655f 7465 6e73 6f72 293a 0a20 2020 2020  e_tensor):.     
-00015ba0: 2020 2022 2222 636f 6e76 6572 7420 6120     """convert a 
-00015bb0: 6e64 2074 656e 736f 7220 746f 2061 2032  nd tensor to a 2
-00015bc0: 6420 7465 6e73 6f72 2222 220a 2020 2020  d tensor""".    
-00015bd0: 2020 2020 7175 6572 795f 7368 6170 6520      query_shape 
-00015be0: 3d20 462e 7368 6170 6528 7175 6572 795f  = F.shape(query_
-00015bf0: 7465 6e73 6f72 290a 2020 2020 2020 2020  tensor).        
-00015c00: 7175 6572 795f 7465 6e73 6f72 203d 2046  query_tensor = F
-00015c10: 2e72 6573 6861 7065 2871 7565 7279 5f74  .reshape(query_t
-00015c20: 656e 736f 722c 2028 2d31 2c20 7175 6572  ensor, (-1, quer
-00015c30: 795f 7368 6170 655b 2d31 5d29 290a 2020  y_shape[-1])).  
-00015c40: 2020 2020 2020 6b65 795f 7368 6170 6520        key_shape 
-00015c50: 3d20 462e 7368 6170 6528 6b65 795f 7465  = F.shape(key_te
-00015c60: 6e73 6f72 290a 2020 2020 2020 2020 6b65  nsor).        ke
-00015c70: 795f 7465 6e73 6f72 203d 2046 2e72 6573  y_tensor = F.res
-00015c80: 6861 7065 286b 6579 5f74 656e 736f 722c  hape(key_tensor,
-00015c90: 2028 2d31 2c20 6b65 795f 7368 6170 655b   (-1, key_shape[
-00015ca0: 2d31 5d29 290a 2020 2020 2020 2020 7661  -1])).        va
-00015cb0: 6c75 655f 7368 6170 6520 3d20 462e 7368  lue_shape = F.sh
-00015cc0: 6170 6528 7661 6c75 655f 7465 6e73 6f72  ape(value_tensor
-00015cd0: 290a 2020 2020 2020 2020 7661 6c75 655f  ).        value_
-00015ce0: 7465 6e73 6f72 203d 2046 2e72 6573 6861  tensor = F.resha
-00015cf0: 7065 2876 616c 7565 5f74 656e 736f 722c  pe(value_tensor,
-00015d00: 2028 2d31 2c20 7661 6c75 655f 7368 6170   (-1, value_shap
-00015d10: 655b 2d31 5d29 290a 0a20 2020 2020 2020  e[-1]))..       
-00015d20: 2072 6574 7572 6e20 7175 6572 795f 7465   return query_te
-00015d30: 6e73 6f72 2c20 6b65 795f 7465 6e73 6f72  nsor, key_tensor
-00015d40: 2c20 7661 6c75 655f 7465 6e73 6f72 0a0a  , value_tensor..
-00015d50: 2020 2020 6465 6620 5f6d 6572 6765 5f68      def _merge_h
-00015d60: 6561 6473 2873 656c 662c 2078 293a 0a20  eads(self, x):. 
-00015d70: 2020 2020 2020 2022 2222 0a20 2020 2020         """.     
-00015d80: 2020 2063 6f6e 7665 7274 2061 2034 6420     convert a 4d 
-00015d90: 696e 7075 7420 746f 2061 2032 6420 6f75  input to a 2d ou
-00015da0: 7470 7574 0a0a 2020 2020 2020 2020 496e  tput..        In
-00015db0: 7075 7473 3a0a 2020 2020 2020 2020 2020  puts:.          
-00015dc0: 2020 783a 2069 6e70 7574 2074 656e 736f    x: input tenso
-00015dd0: 720a 0a20 2020 2020 2020 204f 7574 7075  r..        Outpu
-00015de0: 743a 0a20 2020 2020 2020 2020 2020 2078  t:.            x
-00015df0: 5f6d 6572 6765 3a20 7468 6520 3264 206f  _merge: the 2d o
-00015e00: 7574 7075 740a 2020 2020 2020 2020 2222  utput.        ""
-00015e10: 220a 2020 2020 2020 2020 7820 3d20 7365  ".        x = se
-00015e20: 6c66 2e6d 6572 6765 725f 6865 6164 5f74  lf.merger_head_t
-00015e30: 7261 6e73 706f 7365 280a 2020 2020 2020  ranspose(.      
-00015e40: 2020 2020 2020 782c 2028 302c 2032 2c20        x, (0, 2, 
-00015e50: 312c 2033 2929 2020 2320 6273 2c20 7365  1, 3))  # bs, se
-00015e60: 715f 6c65 6e67 7468 2c20 6865 6164 2c20  q_length, head, 
-00015e70: 7369 7a65 5f70 6572 5f68 6561 640a 2020  size_per_head.  
-00015e80: 2020 2020 2020 785f 7368 6170 6520 3d20        x_shape = 
-00015e90: 502e 5368 6170 6528 2928 7829 0a20 2020  P.Shape()(x).   
-00015ea0: 2020 2020 206e 6577 5f73 6861 7065 203d       new_shape =
-00015eb0: 2028 2d31 2c20 785f 7368 6170 655b 2d32   (-1, x_shape[-2
-00015ec0: 5d20 2a20 785f 7368 6170 655b 2d31 5d29  ] * x_shape[-1])
-00015ed0: 0a20 2020 2020 2020 2078 5f6d 6572 6765  .        x_merge
-00015ee0: 203d 2073 656c 662e 7265 7368 6170 6528   = self.reshape(
-00015ef0: 782c 206e 6577 5f73 6861 7065 290a 2020  x, new_shape).  
-00015f00: 2020 2020 2020 7265 7475 726e 2078 5f6d        return x_m
-00015f10: 6572 6765 0a0a 2020 2020 6465 6620 5f73  erge..    def _s
-00015f20: 6f66 746d 6178 2873 656c 662c 2061 7474  oftmax(self, att
-00015f30: 656e 7469 6f6e 5f73 636f 7265 7329 3a0a  ention_scores):.
-00015f40: 2020 2020 2020 2020 2222 220a 2020 2020          """.    
-00015f50: 2020 2020 466f 7220 7468 6520 636f 6e73      For the cons
-00015f60: 6964 6572 6174 696f 6e20 6f66 2074 6865  ideration of the
-00015f70: 2070 6572 666f 726d 616e 6365 2c20 646f   performance, do
-00015f80: 2073 6f66 746d 6178 2061 6363 6f72 6469   softmax accordi
-00015f90: 6e67 2074 6f20 6469 6666 6572 656e 7420  ng to different 
-00015fa0: 7369 7475 6174 696f 6e73 0a20 2020 2020  situations.     
-00015fb0: 2020 203a 7061 7261 6d20 6174 7465 6e74     :param attent
-00015fc0: 696f 6e5f 7363 6f72 6573 3a20 6120 3364  ion_scores: a 3d
-00015fd0: 2074 656e 736f 7220 6265 666f 7265 2073   tensor before s
-00015fe0: 6f66 746d 6178 0a20 2020 2020 2020 203a  oftmax.        :
-00015ff0: 7265 7475 726e 3a20 7468 6520 6174 7465  return: the atte
-00016000: 6e74 696f 6e20 7363 6f72 6573 2e0a 2020  ntion scores..  
-00016010: 2020 2020 2020 2222 220a 0a20 2020 2020        """..     
-00016020: 2020 2069 6620 7365 6c66 2e5f 6973 5f61     if self._is_a
-00016030: 7363 656e 6420 616e 6420 7365 6c66 2e73  scend and self.s
-00016040: 6f66 746d 6178 5f64 7479 7065 203d 3d20  oftmax_dtype == 
-00016050: 6d73 7479 7065 2e66 6c6f 6174 3136 206f  mstype.float16 o
-00016060: 7220 6e6f 7420 7365 6c66 2e5f 6973 5f61  r not self._is_a
-00016070: 7363 656e 643a 0a20 2020 2020 2020 2020  scend:.         
-00016080: 2020 2061 7474 656e 7469 6f6e 5f70 726f     attention_pro
-00016090: 6273 203d 2073 656c 662e 736f 6674 6d61  bs = self.softma
-000160a0: 7828 6174 7465 6e74 696f 6e5f 7363 6f72  x(attention_scor
-000160b0: 6573 290a 2020 2020 2020 2020 656c 7365  es).        else
-000160c0: 3a0a 2020 2020 2020 2020 2020 2020 7368  :.            sh
-000160d0: 6170 6520 3d20 462e 7368 6170 6528 6174  ape = F.shape(at
-000160e0: 7465 6e74 696f 6e5f 7363 6f72 6573 290a  tention_scores).
-000160f0: 2020 2020 2020 2020 2020 2020 2320 6174              # at
-00016100: 7465 6e74 696f 6e20 7072 6f62 730a 2020  tention probs.  
-00016110: 2020 2020 2020 2020 2020 6174 7465 6e74            attent
-00016120: 696f 6e5f 7072 6f62 7320 3d20 7365 6c66  ion_probs = self
-00016130: 2e73 6f66 746d 6178 5f33 6428 0a20 2020  .softmax_3d(.   
-00016140: 2020 2020 2020 2020 2020 2020 2073 656c               sel
-00016150: 662e 736f 6674 6d61 785f 7265 7368 6170  f.softmax_reshap
-00016160: 6528 6174 7465 6e74 696f 6e5f 7363 6f72  e(attention_scor
-00016170: 6573 2c20 2873 6861 7065 5b30 5d2c 202d  es, (shape[0], -
-00016180: 312c 2073 6861 7065 5b2d 315d 2929 290a  1, shape[-1]))).
-00016190: 2020 2020 2020 2020 2020 2020 6174 7465              atte
-000161a0: 6e74 696f 6e5f 7072 6f62 7320 3d20 7365  ntion_probs = se
-000161b0: 6c66 2e73 6f66 746d 6178 5f72 6573 6861  lf.softmax_resha
-000161c0: 7065 2861 7474 656e 7469 6f6e 5f70 726f  pe(attention_pro
-000161d0: 6273 2c20 7368 6170 6529 0a20 2020 2020  bs, shape).     
-000161e0: 2020 2072 6574 7572 6e20 6174 7465 6e74     return attent
-000161f0: 696f 6e5f 7072 6f62 730a 0a20 2020 2064  ion_probs..    d
-00016200: 6566 205f 666c 6173 685f 6174 746e 2873  ef _flash_attn(s
-00016210: 656c 662c 2071 7565 7279 2c20 6b65 792c  elf, query, key,
-00016220: 2076 616c 7565 2c20 6174 7465 6e74 696f   value, attentio
-00016230: 6e5f 6d61 736b 293a 0a20 2020 2020 2020  n_mask):.       
-00016240: 2022 2222 0a20 2020 2020 2020 2066 6c61   """.        fla
-00016250: 7368 2061 7474 656e 7469 6f6e 0a20 2020  sh attention.   
-00016260: 2020 2020 2022 2222 0a20 2020 2020 2020       """.       
-00016270: 2069 6620 6174 7465 6e74 696f 6e5f 6d61   if attention_ma
-00016280: 736b 2069 7320 6e6f 7420 4e6f 6e65 3a0a  sk is not None:.
-00016290: 2020 2020 2020 2020 2020 2020 6174 7465              atte
-000162a0: 6e74 696f 6e5f 6d61 736b 5f64 7479 7065  ntion_mask_dtype
-000162b0: 203d 2063 686f 6f73 655f 666c 6173 685f   = choose_flash_
-000162c0: 6174 7465 6e74 696f 6e5f 6474 7970 6528  attention_dtype(
-000162d0: 290a 2020 2020 2020 2020 2020 2020 6174  ).            at
-000162e0: 7465 6e74 696f 6e5f 6d61 736b 203d 2073  tention_mask = s
-000162f0: 656c 662e 7375 6228 0a20 2020 2020 2020  elf.sub(.       
-00016300: 2020 2020 2020 2020 2050 2e43 6173 7428           P.Cast(
-00016310: 2928 7365 6c66 2e6f 6e65 2c20 6174 7465  )(self.one, atte
-00016320: 6e74 696f 6e5f 6d61 736b 5f64 7479 7065  ntion_mask_dtype
-00016330: 292c 0a20 2020 2020 2020 2020 2020 2020  ),.             
-00016340: 2020 2050 2e43 6173 7428 2928 6174 7465     P.Cast()(atte
-00016350: 6e74 696f 6e5f 6d61 736b 2c20 6174 7465  ntion_mask, atte
-00016360: 6e74 696f 6e5f 6d61 736b 5f64 7479 7065  ntion_mask_dtype
-00016370: 2929 0a0a 2020 2020 2020 2020 7765 6967  ))..        weig
-00016380: 6874 6564 5f76 616c 7565 7320 3d20 7365  hted_values = se
-00016390: 6c66 2e66 6c61 7368 5f61 7474 656e 7469  lf.flash_attenti
-000163a0: 6f6e 2871 7565 7279 2c20 6b65 792c 2076  on(query, key, v
-000163b0: 616c 7565 2c20 6174 7465 6e74 696f 6e5f  alue, attention_
-000163c0: 6d61 736b 290a 2020 2020 2020 2020 6174  mask).        at
-000163d0: 7465 6e74 696f 6e5f 6d65 7267 6520 3d20  tention_merge = 
-000163e0: 7365 6c66 2e5f 6d65 7267 655f 6865 6164  self._merge_head
-000163f0: 7328 7765 6967 6874 6564 5f76 616c 7565  s(weighted_value
-00016400: 7329 0a20 2020 2020 2020 2072 6574 7572  s).        retur
-00016410: 6e20 6174 7465 6e74 696f 6e5f 6d65 7267  n attention_merg
-00016420: 650a 0a20 2020 2064 6566 205f 6174 746e  e..    def _attn
-00016430: 2873 656c 662c 2071 7565 7279 2c20 6b65  (self, query, ke
-00016440: 792c 2076 616c 7565 2c20 6174 7465 6e74  y, value, attent
-00016450: 696f 6e5f 6d61 736b 293a 0a20 2020 2020  ion_mask):.     
-00016460: 2020 2022 2222 0a20 2020 2020 2020 2047     """.        G
-00016470: 6574 2074 6865 2077 6569 6768 7465 6420  et the weighted 
-00016480: 7363 6f72 6520 616c 6f6e 6720 7468 6520  score along the 
-00016490: 7365 715f 6c65 6e67 7468 0a0a 2020 2020  seq_length..    
-000164a0: 2020 2020 496e 7075 7473 3a0a 2020 2020      Inputs:.    
-000164b0: 2020 2020 2020 2020 7175 6572 793a 2074          query: t
-000164c0: 6865 2071 7565 7279 206d 6174 7269 780a  he query matrix.
-000164d0: 2020 2020 2020 2020 2020 2020 6b65 793a              key:
-000164e0: 2074 6865 206b 6579 206d 6174 7269 780a   the key matrix.
-000164f0: 2020 2020 2020 2020 2020 2020 7661 6c75              valu
-00016500: 653a 2074 6865 2076 616c 7565 206d 6174  e: the value mat
-00016510: 7269 780a 2020 2020 2020 2020 2020 2020  rix.            
-00016520: 6174 7465 6e74 696f 6e5f 6d61 736b 3a20  attention_mask: 
-00016530: 7468 6520 6174 7465 6e74 696f 6e20 6d61  the attention ma
-00016540: 736b 206d 6174 7269 7820 7769 7468 2073  sk matrix with s
-00016550: 6861 7065 2028 6261 7463 685f 7369 7a65  hape (batch_size
-00016560: 2c0a 2020 2020 2020 2020 2020 2020 312c  ,.            1,
-00016570: 2073 6571 5f6c 656e 6774 682c 2073 6571   seq_length, seq
-00016580: 5f6c 656e 6774 6829 0a20 2020 2020 2020  _length).       
-00016590: 204f 7574 7075 7473 3a0a 2020 2020 2020   Outputs:.      
-000165a0: 2020 2020 2020 7765 6967 6874 6564 5f76        weighted_v
-000165b0: 616c 7565 733a 2054 656e 736f 722c 2074  alues: Tensor, t
-000165c0: 6865 2077 6569 6768 7465 6420 7375 6d20  he weighted sum 
-000165d0: 7363 6f72 6573 0a20 2020 2020 2020 2022  scores.        "
-000165e0: 2222 0a20 2020 2020 2020 2023 204e 6f72  "".        # Nor
-000165f0: 6d61 6c69 7a65 2071 7565 7279 2061 6e64  malize query and
-00016600: 206b 6579 2062 6566 6f72 6520 4d61 744d   key before MatM
-00016610: 756c 2c20 6465 6661 756c 7420 6f66 660a  ul, default off.
-00016620: 2020 2020 2020 2020 2320 4174 7465 6e74          # Attent
-00016630: 696f 6e20 7363 6f72 6520 5b62 732c 206e  ion score [bs, n
-00016640: 756d 5f68 6561 6473 2c20 7365 715f 6c65  um_heads, seq_le
-00016650: 6e67 7468 2c20 7365 715f 6c65 6e67 7468  ngth, seq_length
-00016660: 5d0a 2020 2020 2020 2020 6661 6374 6f72  ].        factor
-00016670: 203d 2050 2e43 6173 7428 2928 7365 6c66   = P.Cast()(self
-00016680: 2e73 6361 6c65 5f66 6163 746f 722c 2050  .scale_factor, P
-00016690: 2e44 5479 7065 2829 2871 7565 7279 2929  .DType()(query))
-000166a0: 0a20 2020 2020 2020 2071 7565 7279 203d  .        query =
-000166b0: 2073 656c 662e 7265 616c 5f64 6976 2871   self.real_div(q
-000166c0: 7565 7279 2c20 6661 6374 6f72 290a 2020  uery, factor).  
-000166d0: 2020 2020 2020 6b65 7920 3d20 7365 6c66        key = self
-000166e0: 2e72 6561 6c5f 6469 7628 6b65 792c 2066  .real_div(key, f
-000166f0: 6163 746f 7229 0a20 2020 2020 2020 2073  actor).        s
-00016700: 636f 7265 203d 2073 656c 662e 6261 7463  core = self.batc
-00016710: 685f 6d61 746d 756c 2871 7565 7279 2c20  h_matmul(query, 
-00016720: 6b65 7929 0a0a 2020 2020 2020 2020 6f72  key)..        or
-00016730: 695f 6474 7970 6520 3d20 502e 4454 7970  i_dtype = P.DTyp
-00016740: 6528 2928 7363 6f72 6529 0a20 2020 2020  e()(score).     
-00016750: 2020 2061 7474 656e 7469 6f6e 5f73 636f     attention_sco
-00016760: 7265 7320 3d20 7365 6c66 2e73 6f66 746d  res = self.softm
-00016770: 6178 5f63 6173 7428 7363 6f72 652c 2073  ax_cast(score, s
-00016780: 656c 662e 736f 6674 6d61 785f 6474 7970  elf.softmax_dtyp
-00016790: 6529 0a0a 2020 2020 2020 2020 2320 666f  e)..        # fo
-000167a0: 7220 696e 7075 7420 7369 7a65 206f 6620  r input size of 
-000167b0: 2862 732c 2031 2920 6e61 6d65 6c79 2074  (bs, 1) namely t
-000167c0: 6865 2073 6563 6f6e 6420 6772 6170 682c  he second graph,
-000167d0: 0a20 2020 2020 2020 2023 2074 6865 2073  .        # the s
-000167e0: 6861 7065 206f 6620 6174 7465 6e74 696f  hape of attentio
-000167f0: 6e5f 6d61 736b 206d 6174 7269 7820 7368  n_mask matrix sh
-00016800: 6f75 6c64 2062 6520 2862 732c 2031 2c20  ould be (bs, 1, 
-00016810: 312c 2073 6571 5f6c 656e 6774 6829 0a20  1, seq_length). 
-00016820: 2020 2020 2020 2069 6620 6174 7465 6e74         if attent
-00016830: 696f 6e5f 6d61 736b 2069 7320 6e6f 7420  ion_mask is not 
-00016840: 4e6f 6e65 3a0a 2020 2020 2020 2020 2020  None:.          
-00016850: 2020 6966 2073 656c 662e 7573 655f 7061    if self.use_pa
-00016860: 7374 2061 6e64 206e 6f74 2073 656c 662e  st and not self.
-00016870: 6973 5f66 6972 7374 5f69 7465 7261 7469  is_first_iterati
-00016880: 6f6e 3a0a 2020 2020 2020 2020 2020 2020  on:.            
-00016890: 2020 2020 2320 4361 6c63 756c 6174 6520      # Calculate 
-000168a0: 7468 6520 6375 7272 656e 7420 746f 7461  the current tota
-000168b0: 6c20 746f 6b65 6e0a 2020 2020 2020 2020  l token.        
-000168c0: 2020 2020 2020 2020 6375 7272 656e 745f          current_
-000168d0: 696e 6465 7820 3d20 7365 6c66 2e72 6564  index = self.red
-000168e0: 7563 6573 756d 2846 2e63 6173 7428 7365  ucesum(F.cast(se
-000168f0: 6c66 2e6e 6f74 5f65 7175 616c 2873 656c  lf.not_equal(sel
-00016900: 662e 736c 6963 6528 6b65 792c 2028 302c  f.slice(key, (0,
-00016910: 2030 2c20 302c 2030 292c 0a20 2020 2020   0, 0, 0),.     
-00016920: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00016930: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00016940: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00016950: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00016960: 2020 2020 2020 2020 2020 2028 462e 7368             (F.sh
-00016970: 6170 6528 7175 6572 7929 5b30 5d2c 2031  ape(query)[0], 1
-00016980: 2c20 312c 0a20 2020 2020 2020 2020 2020  , 1,.           
-00016990: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000169a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000169b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000169c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000169d0: 2020 2020 2020 7365 6c66 2e73 6571 5f6c        self.seq_l
-000169e0: 656e 6774 6829 2c0a 2020 2020 2020 2020  ength),.        
-000169f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00016a00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00016a10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00016a20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00016a30: 2020 2020 2020 2020 2831 2c20 312c 2031          (1, 1, 1
-00016a40: 2c20 3129 292c 0a20 2020 2020 2020 2020  , 1)),.         
-00016a50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00016a60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00016a70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00016a80: 2020 2020 2020 2020 2020 2020 3029 2c20              0), 
-00016a90: 6d73 7479 7065 2e66 6c6f 6174 3332 292c  mstype.float32),
-00016aa0: 2028 312c 2032 2c20 3329 290a 2020 2020   (1, 2, 3)).    
-00016ab0: 2020 2020 2020 2020 2020 2020 2320 4765              # Ge
-00016ac0: 7420 7468 6520 7072 6563 6973 6520 706f  t the precise po
-00016ad0: 7369 7469 6f6e 2069 6e64 6578 0a20 2020  sition index.   
-00016ae0: 2020 2020 2020 2020 2020 2020 2069 6e64               ind
-00016af0: 6578 203d 2073 656c 662e 7375 6231 2846  ex = self.sub1(F
-00016b00: 2e63 6173 7428 6375 7272 656e 745f 696e  .cast(current_in
-00016b10: 6465 782c 206d 7374 7970 652e 696e 7433  dex, mstype.int3
-00016b20: 3229 2c20 3129 0a20 2020 2020 2020 2020  2), 1).         
-00016b30: 2020 2020 2020 2069 6e64 6578 203d 2046         index = F
-00016b40: 2e72 6573 6861 7065 2869 6e64 6578 2c20  .reshape(index, 
-00016b50: 282d 312c 2031 2c20 3129 290a 2020 2020  (-1, 1, 1)).    
-00016b60: 2020 2020 2020 2020 2020 2020 2320 4361              # Ca
-00016b70: 6c63 756c 6174 6520 7468 6520 6174 7465  lculate the atte
-00016b80: 6e74 696f 6e5f 6d61 736b 206d 6174 7269  ntion_mask matri
-00016b90: 7820 7669 6120 7468 6520 706f 7369 7469  x via the positi
-00016ba0: 6f6e 2069 6e64 6578 0a20 2020 2020 2020  on index.       
-00016bb0: 2020 2020 2020 2020 2061 7474 656e 7469           attenti
-00016bc0: 6f6e 5f6d 6173 6b20 3d20 462e 6361 7374  on_mask = F.cast
-00016bd0: 2873 656c 662e 7465 6e73 6f72 5f6c 6528  (self.tensor_le(
-00016be0: 7365 6c66 2e72 616e 6765 2c20 696e 6465  self.range, inde
-00016bf0: 7829 2c20 6d73 7479 7065 2e69 6e74 3332  x), mstype.int32
-00016c00: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
-00016c10: 2020 6174 7465 6e74 696f 6e5f 6d61 736b    attention_mask
-00016c20: 203d 2073 656c 662e 6578 7061 6e64 5f64   = self.expand_d
-00016c30: 696d 7328 6174 7465 6e74 696f 6e5f 6d61  ims(attention_ma
-00016c40: 736b 2c20 3229 0a20 2020 2020 2020 2020  sk, 2).         
-00016c50: 2020 2023 204d 696e 7573 2031 3030 3030     # Minus 10000
-00016c60: 2066 6f72 2074 6865 2070 6f73 6974 696f   for the positio
-00016c70: 6e20 7768 6572 6520 6d61 736b 6564 2074  n where masked t
-00016c80: 6f20 6578 636c 7564 6520 7468 656d 2066  o exclude them f
-00016c90: 726f 6d20 736f 6674 6d61 780a 2020 2020  rom softmax.    
-00016ca0: 2020 2020 2020 2020 6d75 6c74 6970 6c75          multiplu
-00016cb0: 5f6f 7574 203d 2073 656c 662e 7375 625f  _out = self.sub_
-00016cc0: 7361 280a 2020 2020 2020 2020 2020 2020  sa(.            
-00016cd0: 2020 2020 502e 4361 7374 2829 2846 2e74      P.Cast()(F.t
-00016ce0: 7570 6c65 5f74 6f5f 6172 7261 7928 2831  uple_to_array((1
-00016cf0: 2e30 2c29 292c 2050 2e44 5479 7065 2829  .0,)), P.DType()
-00016d00: 2861 7474 656e 7469 6f6e 5f73 636f 7265  (attention_score
-00016d10: 7329 292c 0a20 2020 2020 2020 2020 2020  s)),.           
-00016d20: 2020 2020 2050 2e43 6173 7428 2928 6174       P.Cast()(at
-00016d30: 7465 6e74 696f 6e5f 6d61 736b 2c20 502e  tention_mask, P.
-00016d40: 4454 7970 6528 2928 6174 7465 6e74 696f  DType()(attentio
-00016d50: 6e5f 7363 6f72 6573 2929 290a 0a20 2020  n_scores)))..   
-00016d60: 2020 2020 2020 2020 2061 6464 6572 203d           adder =
-00016d70: 2073 656c 662e 6d75 6c28 6d75 6c74 6970   self.mul(multip
-00016d80: 6c75 5f6f 7574 2c20 7365 6c66 2e6d 756c  lu_out, self.mul
-00016d90: 7469 706c 795f 6461 7461 290a 2020 2020  tiply_data).    
-00016da0: 2020 2020 2020 2020 6174 7465 6e74 696f          attentio
-00016db0: 6e5f 7363 6f72 6573 203d 2073 656c 662e  n_scores = self.
-00016dc0: 6164 6428 6164 6465 722c 2061 7474 656e  add(adder, atten
-00016dd0: 7469 6f6e 5f73 636f 7265 7329 0a0a 2020  tion_scores)..  
-00016de0: 2020 2020 2020 2320 6174 7465 6e74 696f        # attentio
-00016df0: 6e20 7072 6f62 730a 2020 2020 2020 2020  n probs.        
-00016e00: 6174 7465 6e74 696f 6e5f 7072 6f62 7320  attention_probs 
-00016e10: 3d20 7365 6c66 2e5f 736f 6674 6d61 7828  = self._softmax(
-00016e20: 6174 7465 6e74 696f 6e5f 7363 6f72 6573  attention_scores
-00016e30: 290a 2020 2020 2020 2020 6174 7465 6e74  ).        attent
-00016e40: 696f 6e5f 7072 6f62 7320 3d20 7365 6c66  ion_probs = self
-00016e50: 2e73 6f66 746d 6178 5f63 6173 7428 6174  .softmax_cast(at
-00016e60: 7465 6e74 696f 6e5f 7072 6f62 732c 206f  tention_probs, o
-00016e70: 7269 5f64 7479 7065 290a 0a20 2020 2020  ri_dtype)..     
-00016e80: 2020 2061 7474 656e 7469 6f6e 5f70 726f     attention_pro
-00016e90: 6273 203d 2073 656c 662e 7072 6f62 5f64  bs = self.prob_d
-00016ea0: 726f 706f 7574 2861 7474 656e 7469 6f6e  ropout(attention
-00016eb0: 5f70 726f 6273 290a 2020 2020 2020 2020  _probs).        
-00016ec0: 2320 5765 6967 6874 6564 2073 756d 206f  # Weighted sum o
-00016ed0: 7574 7075 7420 5b62 732c 206e 756d 5f68  utput [bs, num_h
-00016ee0: 6561 6473 2c20 7365 715f 6c65 6e67 7468  eads, seq_length
-00016ef0: 2c20 7369 7a65 5f70 6572 5f68 6561 645d  , size_per_head]
-00016f00: 0a20 2020 2020 2020 2077 6569 6768 7465  .        weighte
-00016f10: 645f 7661 6c75 6573 203d 2073 656c 662e  d_values = self.
-00016f20: 6261 7463 685f 6d61 746d 756c 2861 7474  batch_matmul(att
-00016f30: 656e 7469 6f6e 5f70 726f 6273 2c20 7661  ention_probs, va
-00016f40: 6c75 6529 0a20 2020 2020 2020 2061 7474  lue).        att
-00016f50: 656e 7469 6f6e 5f6d 6572 6765 203d 2073  ention_merge = s
-00016f60: 656c 662e 5f6d 6572 6765 5f68 6561 6473  elf._merge_heads
-00016f70: 2877 6569 6768 7465 645f 7661 6c75 6573  (weighted_values
-00016f80: 290a 2020 2020 2020 2020 7265 7475 726e  ).        return
-00016f90: 2061 7474 656e 7469 6f6e 5f6d 6572 6765   attention_merge
-00016fa0: 0a0a 0a63 6c61 7373 2054 7261 6e73 666f  ...class Transfo
-00016fb0: 726d 6572 456e 636f 6465 724c 6179 6572  rmerEncoderLayer
-00016fc0: 2843 656c 6c29 3a0a 2020 2020 7222 2222  (Cell):.    r"""
-00016fd0: 0a20 2020 2020 2020 2054 7261 6e73 666f  .        Transfo
-00016fe0: 726d 6572 2045 6e63 6f64 6572 204c 6179  rmer Encoder Lay
-00016ff0: 6572 2e20 5468 6973 2069 7320 616e 2069  er. This is an i
-00017000: 6d70 6c65 6d65 6e74 6174 696f 6e20 6f66  mplementation of
-00017010: 2074 6865 2073 696e 676c 6520 6c61 7965   the single laye
-00017020: 7220 6f66 2074 6865 2074 7261 6e73 666f  r of the transfo
-00017030: 726d 6572 0a20 2020 2020 2020 2065 6e63  rmer.        enc
-00017040: 6f64 6572 206c 6179 6572 2c20 696e 636c  oder layer, incl
-00017050: 7564 696e 6720 6d75 6c74 6968 6561 6420  uding multihead 
-00017060: 6174 7465 6e74 696f 6e20 616e 6420 6665  attention and fe
-00017070: 6564 7761 7264 206c 6179 6572 2e0a 0a20  edward layer... 
-00017080: 2020 2020 2020 2041 7267 733a 0a20 2020         Args:.   
-00017090: 2020 2020 2020 2020 2062 6174 6368 5f73           batch_s
-000170a0: 697a 6528 696e 7429 3a20 5468 6520 6261  ize(int): The ba
-000170b0: 7463 6820 7369 7a65 206f 6620 7468 6520  tch size of the 
-000170c0: 696e 7075 7420 7465 6e73 6f72 2077 6865  input tensor whe
-000170d0: 6e20 646f 2069 6e63 7265 6e6d 656e 7461  n do increnmenta
-000170e0: 6c20 7072 6564 6963 7469 6f6e 2e20 5368  l prediction. Sh
-000170f0: 6f75 6c64 2062 6520 6120 706f 7369 7469  ould be a positi
-00017100: 7665 0a20 2020 2020 2020 2020 2020 2020  ve.             
-00017110: 2020 2076 616c 7565 2e20 5768 656e 2064     value. When d
-00017120: 6f20 7472 6169 6e69 6e67 206f 7220 7072  o training or pr
-00017130: 6564 6963 7469 6f6e 2c20 7468 6520 6172  ediction, the ar
-00017140: 6775 6d65 6e74 2077 696c 6c20 6e6f 7420  gument will not 
-00017150: 776f 726b 2061 6e64 2074 6865 2075 7365  work and the use
-00017160: 7220 6361 6e20 6a75 7374 2070 6173 7320  r can just pass 
-00017170: 4e6f 6e65 2074 6f0a 2020 2020 2020 2020  None to.        
-00017180: 2020 2020 2020 2020 7468 6520 6172 6775          the argu
-00017190: 6d65 6e74 2e0a 2020 2020 2020 2020 2020  ment..          
-000171a0: 2020 6869 6464 656e 5f73 697a 6528 696e    hidden_size(in
-000171b0: 7429 3a20 5468 6520 6869 6464 656e 2073  t): The hidden s
-000171c0: 697a 6520 6f66 2074 6865 2069 6e70 7574  ize of the input
-000171d0: 2e0a 2020 2020 2020 2020 2020 2020 6666  ..            ff
-000171e0: 6e5f 6869 6464 656e 5f73 697a 6528 696e  n_hidden_size(in
-000171f0: 7429 3a20 5468 6520 6869 6464 656e 2073  t): The hidden s
-00017200: 697a 6520 6f66 2062 6f74 746c 656e 6563  ize of bottlenec
-00017210: 6b20 696e 2074 6865 2066 6565 6466 6f72  k in the feedfor
-00017220: 7761 7264 206c 6179 6572 2e0a 2020 2020  ward layer..    
-00017230: 2020 2020 2020 2020 6e75 6d5f 6865 6164          num_head
-00017240: 7328 696e 7429 3a20 5468 6520 6e75 6d62  s(int): The numb
-00017250: 6572 206f 6620 7468 6520 6865 6164 732e  er of the heads.
-00017260: 0a20 2020 2020 2020 2020 2020 2073 6571  .            seq
-00017270: 5f6c 656e 6774 6828 696e 7429 3a20 5468  _length(int): Th
-00017280: 6520 696e 7075 7420 7365 7175 656e 6365  e input sequence
-00017290: 206c 656e 6774 682e 0a20 2020 2020 2020   length..       
-000172a0: 2020 2020 2061 7474 656e 7469 6f6e 5f64       attention_d
-000172b0: 726f 706f 7574 5f72 6174 6528 666c 6f61  ropout_rate(floa
-000172c0: 7429 3a20 5468 6520 6472 6f70 6f75 7420  t): The dropout 
-000172d0: 7261 7465 206f 6620 7468 6520 6174 7465  rate of the atte
-000172e0: 6e74 696f 6e20 7363 6f72 6573 2e20 4465  ntion scores. De
-000172f0: 6661 756c 743a 302e 312e 0a20 2020 2020  fault:0.1..     
-00017300: 2020 2020 2020 2068 6964 6465 6e5f 6472         hidden_dr
-00017310: 6f70 6f75 745f 7261 7465 2866 6c6f 6174  opout_rate(float
-00017320: 293a 2054 6865 2064 726f 706f 7574 2072  ): The dropout r
-00017330: 6174 6520 6f66 2074 6865 2066 696e 616c  ate of the final
-00017340: 206f 7574 7075 7420 6f66 2074 6865 206c   output of the l
-00017350: 6179 6572 2e20 4465 6661 756c 743a 302e  ayer. Default:0.
-00017360: 312e 0a20 2020 2020 2020 2020 2020 2070  1..            p
-00017370: 6f73 745f 6c61 7965 726e 6f72 6d5f 7265  ost_layernorm_re
-00017380: 7369 6475 616c 2862 6f6f 6c29 3a20 446f  sidual(bool): Do
-00017390: 2072 6573 6964 7561 6c73 2061 6464 7320   residuals adds 
-000173a0: 6265 666f 7265 2074 6865 206c 6179 6572  before the layer
-000173b0: 6e6f 726d 2e20 4465 6661 756c 7420 4661  norm. Default Fa
-000173c0: 6c73 652e 0a20 2020 2020 2020 2020 2020  lse..           
-000173d0: 206c 6179 6572 6e6f 726d 5f63 6f6d 7075   layernorm_compu
-000173e0: 7465 5f74 7970 6528 6474 7970 652e 4e75  te_type(dtype.Nu
-000173f0: 6d62 6572 293a 2054 6865 2063 6f6d 7075  mber): The compu
-00017400: 7461 7469 6f6e 2074 7970 6520 6f66 2074  tation type of t
-00017410: 6865 206c 6179 6572 6e6f 726d 2e0a 2020  he layernorm..  
-00017420: 2020 2020 2020 2020 2020 2020 2020 5368                Sh
-00017430: 6f75 6c64 2062 6520 6d73 7479 7065 2e66  ould be mstype.f
-00017440: 6c6f 6174 3332 206f 7220 6d73 7479 7065  loat32 or mstype
-00017450: 2e66 6c6f 6174 3136 2e20 4465 6661 756c  .float16. Defaul
-00017460: 7420 6d73 7479 7065 2e66 6c6f 6174 3332  t mstype.float32
-00017470: 2e0a 2020 2020 2020 2020 2020 2020 736f  ..            so
-00017480: 6674 6d61 785f 636f 6d70 7574 655f 7479  ftmax_compute_ty
-00017490: 7065 2864 7479 7065 2e4e 756d 6265 7229  pe(dtype.Number)
-000174a0: 3a20 5468 6520 636f 6d70 7574 6174 696f  : The computatio
-000174b0: 6e20 7479 7065 206f 6620 7468 6520 736f  n type of the so
-000174c0: 6674 6d61 7820 696e 2074 6865 2061 7474  ftmax in the att
-000174d0: 656e 7469 6f6e 2e0a 2020 2020 2020 2020  ention..        
-000174e0: 2020 2020 2020 2020 5368 6f75 6c64 2062          Should b
-000174f0: 6520 6d73 7479 7065 2e66 6c6f 6174 3332  e mstype.float32
-00017500: 206f 7220 6d73 7479 7065 2e66 6c6f 6174   or mstype.float
-00017510: 3136 2e20 4465 6661 756c 7420 6d73 7479  16. Default msty
-00017520: 7065 2e66 6c6f 6174 3332 2e0a 2020 2020  pe.float32..    
-00017530: 2020 2020 2020 2020 7061 7261 6d5f 696e          param_in
-00017540: 6974 5f74 7970 6528 6474 7970 652e 4e75  it_type(dtype.Nu
-00017550: 6d62 6572 293a 2054 6865 2070 6172 616d  mber): The param
-00017560: 6574 6572 2069 6e69 7469 616c 697a 6174  eter initializat
-00017570: 696f 6e20 7479 7065 206f 6620 7468 6520  ion type of the 
-00017580: 6d6f 6475 6c65 2e0a 2020 2020 2020 2020  module..        
-00017590: 2020 2020 2020 2020 5368 6f75 6c64 2062          Should b
-000175a0: 6520 6d73 7479 7065 2e66 6c6f 6174 3332  e mstype.float32
-000175b0: 206f 7220 6d73 7479 7065 2e66 6c6f 6174   or mstype.float
-000175c0: 3136 2e20 4465 6661 756c 7420 6d73 7479  16. Default msty
-000175d0: 7065 2e66 6c6f 6174 3332 2e0a 2020 2020  pe.float32..    
-000175e0: 2020 2020 2020 2020 636f 6d70 7574 655f          compute_
-000175f0: 6474 7970 6528 6474 7970 652e 4e75 6d62  dtype(dtype.Numb
-00017600: 6572 293a 2054 6865 2063 6f6d 7075 7461  er): The computa
-00017610: 7469 6f6e 2074 7970 6520 6f66 2064 656e  tion type of den
-00017620: 7365 2e20 4465 6661 756c 7420 6d73 7479  se. Default msty
-00017630: 7065 2e66 6c6f 6174 3136 2e0a 2020 2020  pe.float16..    
-00017640: 2020 2020 2020 2020 2020 2020 5368 6f75              Shou
-00017650: 6c64 2062 6520 6d73 7479 7065 2e66 6c6f  ld be mstype.flo
-00017660: 6174 3332 206f 7220 6d73 7479 7065 2e66  at32 or mstype.f
-00017670: 6c6f 6174 3136 2e0a 2020 2020 2020 2020  loat16..        
-00017680: 2020 2020 6869 6464 656e 5f61 6374 2028      hidden_act (
-00017690: 7374 722c 206e 6e2e 4365 6c6c 293a 2054  str, nn.Cell): T
-000176a0: 6865 2061 6374 6976 6174 696f 6e20 6f66  he activation of
-000176b0: 2074 6865 2069 6e74 6572 6e61 6c20 6665   the internal fe
-000176c0: 6564 666f 7277 6172 6420 6c61 7965 722e  edforward layer.
-000176d0: 2053 7570 706f 7274 7320 2772 656c 7527   Supports 'relu'
-000176e0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-000176f0: 2020 2772 656c 7536 272c 2027 7461 6e68    'relu6', 'tanh
-00017700: 272c 2027 6765 6c75 272c 2027 6661 7374  ', 'gelu', 'fast
-00017710: 5f67 656c 7527 2c20 2765 6c75 272c 2027  _gelu', 'elu', '
-00017720: 7369 676d 6f69 6427 2c20 2770 7265 6c75  sigmoid', 'prelu
-00017730: 272c 2027 6c65 616b 7972 656c 7527 2c20  ', 'leakyrelu', 
-00017740: 2768 7377 6973 6827 2c0a 2020 2020 2020  'hswish',.      
-00017750: 2020 2020 2020 2020 2020 2768 7369 676d            'hsigm
-00017760: 6f69 6427 2c20 276c 6f67 7369 676d 6f69  oid', 'logsigmoi
-00017770: 6427 2061 6e64 2073 6f20 6f6e 2e20 5573  d' and so on. Us
-00017780: 6572 2063 616e 2070 726f 7669 6465 2063  er can provide c
-00017790: 7573 746f 6d20 6163 7469 7669 7469 6f6e  ustom activition
-000177a0: 2074 6f20 7468 6520 6172 6775 6d65 6e74   to the argument
-000177b0: 2e0a 2020 2020 2020 2020 2020 2020 2020  ..              
-000177c0: 2020 4966 2075 7365 7220 7761 6e74 7320    If user wants 
-000177d0: 746f 2072 756e 2074 6865 206e 6574 2069  to run the net i
-000177e0: 6e20 7468 6520 7061 7261 6c6c 656c 206d  n the parallel m
-000177f0: 6f64 652c 2074 6865 2063 7573 746f 6d20  ode, the custom 
-00017800: 6163 7469 7661 7469 6f6e 206d 7573 7420  activation must 
-00017810: 616c 736f 2070 726f 7669 6465 0a20 2020  also provide.   
-00017820: 2020 2020 2020 2020 2020 2020 2074 6865               the
-00017830: 2060 6163 7469 7661 7469 6f6e 5f73 6861   `activation_sha
-00017840: 7264 6020 6675 6e63 7469 6f6e 2e20 506c  rd` function. Pl
-00017850: 6561 7365 2073 6565 2074 6865 2065 7861  ease see the exa
-00017860: 6d70 6c65 7320 6f66 2074 6865 0a20 2020  mples of the.   
-00017870: 2020 2020 2020 2020 2020 2020 2063 6c61               cla
-00017880: 7373 3a60 6d69 6e64 666f 726d 6572 732e  ss:`mindformers.
-00017890: 6d6f 6475 6c65 732e 7472 616e 7366 6f72  modules.transfor
-000178a0: 6d65 722e 4665 6564 466f 7277 6172 6460  mer.FeedForward`
-000178b0: 2e20 4465 6661 756c 743a 2067 656c 752e  . Default: gelu.
-000178c0: 0a20 2020 2020 2020 2020 2020 2075 7365  .            use
-000178d0: 5f70 6173 7428 626f 6f6c 293a 2055 7365  _past(bool): Use
-000178e0: 2074 6865 2070 6173 7420 7374 6174 6520   the past state 
-000178f0: 746f 2063 6f6d 7075 7465 2c20 7573 6564  to compute, used
-00017900: 2066 6f72 2069 6e63 7265 6d65 6e74 616c   for incremental
-00017910: 2070 7265 6469 6374 696f 6e2e 2046 6f72   prediction. For
-00017920: 2065 7861 6d70 6c65 2c20 6966 2077 6520   example, if we 
-00017930: 6861 7665 2074 776f 0a20 2020 2020 2020  have two.       
-00017940: 2020 2020 2020 2020 2077 6f72 6473 2061           words a
-00017950: 6e64 2077 616e 7420 746f 2067 656e 6572  nd want to gener
-00017960: 6174 6520 7468 6520 7465 6e20 6d6f 7265  ate the ten more
-00017970: 2077 6f72 6473 2e20 5765 206a 7573 7420   words. We just 
-00017980: 6e65 6564 2074 6f20 636f 6d70 7574 6520  need to compute 
-00017990: 7468 6520 7477 6f20 776f 7264 7327 2073  the two words' s
-000179a0: 7461 7465 206f 6e6c 7920 6f6e 6365 2c0a  tate only once,.
-000179b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000179c0: 616e 6420 6765 6e65 7261 7465 2074 6865  and generate the
-000179d0: 206e 6578 7420 776f 7264 206f 6e65 2062   next word one b
-000179e0: 7920 6f6e 652e 2057 6865 6e20 7573 655f  y one. When use_
-000179f0: 7061 7374 2069 7320 5472 7565 2c20 7468  past is True, th
-00017a00: 6572 6520 6172 6520 7477 6f20 7374 6570  ere are two step
-00017a10: 7320 746f 2072 756e 2074 6865 2070 7265  s to run the pre
-00017a20: 6469 6374 696f 6e2e 0a20 2020 2020 2020  diction..       
-00017a30: 2020 2020 2020 2020 2049 6e20 7468 6520           In the 
-00017a40: 6669 7273 7420 7374 6570 2c20 7365 7420  first step, set 
-00017a50: 7468 6520 6973 5f66 6972 7374 5f69 7465  the is_first_ite
-00017a60: 7261 7469 6f6e 2074 6f20 6265 2054 7275  ration to be Tru
-00017a70: 6520 6279 0a20 2020 2020 2020 2020 2020  e by.           
-00017a80: 2020 2020 2060 6d6f 6465 6c2e 6164 645f       `model.add_
-00017a90: 666c 6167 735f 7265 6375 7273 6976 6528  flags_recursive(
-00017aa0: 6973 5f66 6972 7374 5f69 7465 7261 7469  is_first_iterati
-00017ab0: 6f6e 3d54 7275 6529 602c 2061 6e64 2070  on=True)`, and p
-00017ac0: 6173 7320 7468 6520 6675 6c6c 2069 6e70  ass the full inp
-00017ad0: 7574 732e 2054 6865 6e2c 2073 6574 2074  uts. Then, set t
-00017ae0: 6865 0a20 2020 2020 2020 2020 2020 2020  he.             
-00017af0: 2020 2069 735f 6669 7273 745f 6974 6572     is_first_iter
-00017b00: 6174 696f 6e20 746f 2062 6520 4661 6c73  ation to be Fals
-00017b10: 6520 6279 2060 6d6f 6465 6c2e 6164 645f  e by `model.add_
-00017b20: 666c 6167 735f 7265 6375 7273 6976 6528  flags_recursive(
-00017b30: 6973 5f66 6972 7374 5f69 7465 7261 7469  is_first_iterati
-00017b40: 6f6e 3d46 616c 7365 2960 2e0a 2020 2020  on=False)`..    
-00017b50: 2020 2020 2020 2020 2020 2020 4174 2074              At t
-00017b60: 6869 7320 6d6f 6d65 6e74 2c20 7061 7373  his moment, pass
-00017b70: 2074 6865 2073 696e 676c 6520 7374 6570   the single step
-00017b80: 2773 2069 6e70 7574 2074 656e 736f 722c  's input tensor,
-00017b90: 2061 6e64 206c 6f6f 7020 6974 2e20 4465   and loop it. De
-00017ba0: 6661 756c 7420 4661 6c73 652e 0a20 2020  fault False..   
-00017bb0: 2020 2020 2020 2020 206d 6f65 5f63 6f6e           moe_con
-00017bc0: 6669 6728 4d6f 4543 6f6e 6669 6729 3a20  fig(MoEConfig): 
-00017bd0: 5468 6520 636f 6e66 6967 7572 6174 696f  The configuratio
-00017be0: 6e20 6f66 204d 6f45 2028 4d69 7874 7572  n of MoE (Mixtur
-00017bf0: 6520 6f66 2045 7870 6572 7429 2e20 4465  e of Expert). De
-00017c00: 6661 756c 7420 6973 2061 6e20 696e 7374  fault is an inst
-00017c10: 616e 6365 206f 6620 4d6f 4543 6f6e 6669  ance of MoEConfi
-00017c20: 670a 2020 2020 2020 2020 2020 2020 2020  g.              
-00017c30: 2020 7769 7468 2064 6566 6175 6c74 2076    with default v
-00017c40: 616c 7565 732e 2050 6c65 6173 6520 7365  alues. Please se
-00017c50: 6520 604d 6f45 436f 6e66 6967 602e 0a20  e `MoEConfig`.. 
-00017c60: 2020 2020 2020 2020 2020 2070 6172 616c             paral
-00017c70: 6c65 6c5f 636f 6e66 6967 284f 7050 6172  lel_config(OpPar
-00017c80: 616c 6c65 6c43 6f6e 6669 672c 204d 6f45  allelConfig, MoE
-00017c90: 5061 7261 6c6c 656c 436f 6e66 6967 293a  ParallelConfig):
-00017ca0: 2054 6865 2070 6172 616c 6c65 6c20 636f   The parallel co
-00017cb0: 6e66 6967 7572 652e 2057 6865 6e20 4d6f  nfigure. When Mo
-00017cc0: 4520 6973 2061 7070 6c69 6564 2c0a 2020  E is applied,.  
-00017cd0: 2020 2020 2020 2020 2020 2020 2020 4d6f                Mo
-00017ce0: 4550 6172 616c 6c65 6c43 6f6e 6669 6720  EParallelConfig 
-00017cf0: 6973 2065 6666 6563 7469 7665 2c20 6f74  is effective, ot
-00017d00: 6865 7277 6973 6520 4f70 5061 7261 6c6c  herwise OpParall
-00017d10: 656c 436f 6e66 6967 2069 7320 6566 6665  elConfig is effe
-00017d20: 6374 6976 652e 2044 6566 6175 6c74 2060  ctive. Default `
-00017d30: 6465 6661 756c 745f 6470 6d70 5f63 6f6e  default_dpmp_con
-00017d40: 6669 6760 2c0a 2020 2020 2020 2020 2020  fig`,.          
-00017d50: 2020 2020 2020 616e 2069 6e73 7461 6e63        an instanc
-00017d60: 6520 6f66 2060 4f70 5061 7261 6c6c 656c  e of `OpParallel
-00017d70: 436f 6e66 6967 6020 7769 7468 2064 6566  Config` with def
-00017d80: 6175 6c74 2061 7267 732e 0a0a 2020 2020  ault args...    
-00017d90: 2020 2020 496e 7075 7473 3a0a 2020 2020      Inputs:.    
-00017da0: 2020 2020 2020 2020 2d20 2a2a 782a 2a20          - **x** 
-00017db0: 2854 656e 736f 7229 202d 2046 6c6f 6174  (Tensor) - Float
-00017dc0: 2054 656e 736f 722c 2073 6861 7065 2073   Tensor, shape s
-00017dd0: 686f 756c 6420 6265 205b 6261 7463 685f  hould be [batch_
-00017de0: 7369 7a65 2c20 7365 715f 6c65 6e67 7468  size, seq_length
-00017df0: 2c20 6869 6464 656e 5f73 697a 655d 206f  , hidden_size] o
-00017e00: 720a 2020 2020 2020 2020 2020 2020 2020  r.              
-00017e10: 5b62 6174 6368 5f73 697a 6520 2a20 7365  [batch_size * se
-00017e20: 715f 6c65 6e67 7468 2c20 6869 6464 656e  q_length, hidden
-00017e30: 5f73 697a 655d 2c20 6966 2074 6865 2075  _size], if the u
-00017e40: 7365 5f70 6173 7420 6973 2046 616c 7365  se_past is False
-00017e50: 206f 7220 6973 5f66 6972 7374 5f69 7465   or is_first_ite
-00017e60: 7261 7469 6f6e 3d54 7275 652e 204f 7468  ration=True. Oth
-00017e70: 6572 7769 7365 2c0a 2020 2020 2020 2020  erwise,.        
-00017e80: 2020 2020 2020 7368 6f75 6c64 2062 6520        should be 
-00017e90: 5b62 6174 6368 5f73 697a 652c 2031 2c20  [batch_size, 1, 
-00017ea0: 6869 6464 656e 5f73 697a 655d 0a20 2020  hidden_size].   
-00017eb0: 2020 2020 2020 2020 202d 202a 2a69 6e70           - **inp
-00017ec0: 7574 5f6d 6173 6b2a 2a20 2854 656e 736f  ut_mask** (Tenso
-00017ed0: 7229 202d 2046 6c6f 6174 2054 656e 736f  r) - Float Tenso
-00017ee0: 722c 2049 6620 7468 6520 7573 655f 7061  r, If the use_pa
-00017ef0: 7374 2069 7320 4661 6c73 6520 6f72 2069  st is False or i
-00017f00: 735f 6669 7273 745f 6974 6572 6174 696f  s_first_iteratio
-00017f10: 6e3d 5472 7565 2c0a 2020 2020 2020 2020  n=True,.        
-00017f20: 2020 2020 2020 7468 6520 6174 7465 6e74        the attent
-00017f30: 696f 6e20 6d61 736b 206d 6174 7269 7820  ion mask matrix 
-00017f40: 7368 6f75 6c64 2062 6120 5b62 6174 6368  should ba [batch
-00017f50: 5f73 697a 652c 2073 6571 5f6c 656e 6774  _size, seq_lengt
-00017f60: 682c 2073 6571 5f6c 656e 6774 685d 2c20  h, seq_length], 
-00017f70: 6f72 204e 6f6e 652e 204e 6f6e 6520 6d65  or None. None me
-00017f80: 616e 7320 7468 6572 6520 7769 6c6c 0a20  ans there will. 
-00017f90: 2020 2020 2020 2020 2020 2020 2062 6520               be 
-00017fa0: 6e6f 206d 6173 6b20 696e 2073 6f66 746d  no mask in softm
-00017fb0: 6178 2063 6f6d 7075 7461 7469 6f6e 2e20  ax computation. 
-00017fc0: 4f74 6865 7277 6973 652c 2073 686f 756c  Otherwise, shoul
-00017fd0: 6420 6265 205b 6261 7463 685f 7369 7a65  d be [batch_size
-00017fe0: 2c20 312c 2068 6964 6465 6e5f 7369 7a65  , 1, hidden_size
-00017ff0: 5d0a 2020 2020 2020 2020 2020 2020 2d20  ].            - 
-00018000: 2a2a 696e 6974 5f72 6573 6574 2a2a 2028  **init_reset** (
-00018010: 5465 6e73 6f72 2920 2d20 4120 626f 6f6c  Tensor) - A bool
-00018020: 2074 656e 736f 7220 7769 7468 2073 6861   tensor with sha
-00018030: 7065 205b 315d 2c20 7573 6564 2074 6f20  pe [1], used to 
-00018040: 636c 6561 7220 7468 6520 7061 7374 206b  clear the past k
-00018050: 6579 2070 6172 616d 6574 6572 2061 6e64  ey parameter and
-00018060: 0a20 2020 2020 2020 2020 2020 2020 2070  .              p
-00018070: 6173 7420 7661 6c75 6520 7061 7261 6d65  ast value parame
-00018080: 7465 7220 7573 6564 2069 6e20 7468 6520  ter used in the 
-00018090: 696e 6372 656d 656e 7461 6c20 7072 6564  incremental pred
-000180a0: 6963 7469 6f6e 2e20 4f6e 6c79 2076 616c  iction. Only val
-000180b0: 6964 2077 6865 6e20 7573 655f 7061 7374  id when use_past
-000180c0: 2069 7320 5472 7565 2e20 4465 6661 756c   is True. Defaul
-000180d0: 7420 5472 7565 2e0a 2020 2020 2020 2020  t True..        
-000180e0: 2020 2020 2d20 2a2a 6261 7463 685f 7661      - **batch_va
-000180f0: 6c69 645f 6c65 6e67 7468 2a2a 2028 5465  lid_length** (Te
-00018100: 6e73 6f72 2920 2d20 496e 7433 3220 7465  nsor) - Int32 te
-00018110: 6e73 6f72 2077 6974 6820 7368 6170 6520  nsor with shape 
-00018120: 5b62 6174 6368 5f73 697a 655d 2074 6865  [batch_size] the
-00018130: 2070 6173 7420 6361 6c63 756c 6174 6564   past calculated
-00018140: 2074 6865 2069 6e64 6578 2e0a 2020 2020   the index..    
-00018150: 2020 2020 2020 2020 2020 5573 6564 2066            Used f
-00018160: 6f72 2069 6e63 7265 6d65 6e74 616c 2070  or incremental p
-00018170: 7265 6469 6374 696f 6e20 7768 656e 2074  rediction when t
-00018180: 6865 2075 7365 5f70 6173 7420 6973 2054  he use_past is T
-00018190: 7275 652e 2044 6566 6175 6c74 204e 6f6e  rue. Default Non
-000181a0: 652e 0a0a 2020 2020 2020 2020 4f75 7470  e...        Outp
-000181b0: 7574 733a 0a20 2020 2020 2020 2020 2020  uts:.           
-000181c0: 2054 7570 6c65 2c20 6120 7475 706c 6520   Tuple, a tuple 
-000181d0: 636f 6e74 6169 6e73 2860 6f75 7470 7574  contains(`output
-000181e0: 602c 2060 6c61 7965 725f 7072 6573 656e  `, `layer_presen
-000181f0: 7460 292e 0a0a 2020 2020 2020 2020 2020  t`)...          
-00018200: 2020 2d20 2a2a 6f75 7470 7574 2a2a 2028    - **output** (
-00018210: 5465 6e73 6f72 2920 2d20 5468 6520 666c  Tensor) - The fl
-00018220: 6f61 7420 7465 6e73 6f72 206f 6620 7468  oat tensor of th
-00018230: 6520 6f75 7470 7574 206f 6620 7468 6520  e output of the 
-00018240: 6c61 7965 7220 7769 7468 0a20 2020 2020  layer with.     
-00018250: 2020 2020 2020 2020 2073 6861 7065 2028           shape (
-00018260: 6261 7463 685f 7369 7a65 2c20 7365 715f  batch_size, seq_
-00018270: 6c65 6e67 7468 2c20 6869 6464 656e 5f73  length, hidden_s
-00018280: 697a 6529 206f 7220 2862 6174 6368 5f73  ize) or (batch_s
-00018290: 697a 6520 2a20 7365 715f 6c65 6e67 7468  ize * seq_length
-000182a0: 2c20 6869 6464 656e 5f73 697a 6529 2c20  , hidden_size), 
-000182b0: 6966 2074 6865 2075 7365 5f70 6173 7420  if the use_past 
-000182c0: 6973 0a20 2020 2020 2020 2020 2020 2020  is.             
-000182d0: 2046 616c 7365 206f 7220 6973 5f66 6972   False or is_fir
-000182e0: 7374 5f69 7465 7261 7469 6f6e 3d54 7275  st_iteration=Tru
-000182f0: 652e 204f 7468 6572 7769 7365 2c20 6974  e. Otherwise, it
-00018300: 2077 696c 6c20 6265 2028 6261 7463 685f   will be (batch_
-00018310: 7369 7a65 2c20 312c 2068 6964 6465 6e5f  size, 1, hidden_
-00018320: 7369 7a65 290a 0a20 2020 2020 2020 2020  size)..         
-00018330: 2020 202d 202a 2a6c 6179 6572 5f70 7265     - **layer_pre
-00018340: 7365 6e74 2a2a 2028 5475 706c 6529 202d  sent** (Tuple) -
-00018350: 2041 2074 7570 6c65 206f 6620 7468 6520   A tuple of the 
-00018360: 5465 6e73 6f72 206f 6620 7468 6520 7072  Tensor of the pr
-00018370: 6f6a 6563 7465 6420 6b65 7920 616e 6420  ojected key and 
-00018380: 7661 6c75 6520 7665 6374 6f72 2077 6974  value vector wit
-00018390: 680a 2020 2020 2020 2020 2020 2020 2020  h.              
-000183a0: 2828 6261 7463 685f 7369 7a65 2c20 6e75  ((batch_size, nu
-000183b0: 6d5f 6865 6164 732c 2073 697a 655f 7065  m_heads, size_pe
-000183c0: 725f 6865 6164 2c20 7365 715f 6c65 6e67  r_head, seq_leng
-000183d0: 7468 292c 0a20 2020 2020 2020 2020 2020  th),.           
-000183e0: 2020 2028 6261 7463 685f 7369 7a65 2c20     (batch_size, 
-000183f0: 6e75 6d5f 6865 6164 732c 2073 6571 5f6c  num_heads, seq_l
-00018400: 656e 6774 682c 2073 697a 655f 7065 725f  ength, size_per_
-00018410: 6865 6164 2929 2e0a 0a20 2020 2020 2020  head))...       
-00018420: 2053 7570 706f 7274 6564 2050 6c61 7466   Supported Platf
-00018430: 6f72 6d73 3a0a 2020 2020 2020 2020 2020  orms:.          
-00018440: 2020 6060 4173 6365 6e64 6060 2060 6047    ``Ascend`` ``G
-00018450: 5055 6060 0a0a 2020 2020 2020 2020 4578  PU``..        Ex
-00018460: 616d 706c 6573 3a0a 2020 2020 2020 2020  amples:.        
-00018470: 2020 2020 3e3e 3e20 696d 706f 7274 206e      >>> import n
-00018480: 756d 7079 2061 7320 6e70 0a20 2020 2020  umpy as np.     
-00018490: 2020 2020 2020 203e 3e3e 2066 726f 6d20         >>> from 
-000184a0: 6d69 6e64 7370 6f72 6520 696d 706f 7274  mindspore import
-000184b0: 2064 7479 7065 2061 7320 6d73 7479 7065   dtype as mstype
-000184c0: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
-000184d0: 2066 726f 6d20 6d69 6e64 666f 726d 6572   from mindformer
-000184e0: 732e 6d6f 6475 6c65 732e 7472 616e 7366  s.modules.transf
-000184f0: 6f72 6d65 7220 696d 706f 7274 2054 7261  ormer import Tra
-00018500: 6e73 666f 726d 6572 456e 636f 6465 724c  nsformerEncoderL
-00018510: 6179 6572 0a20 2020 2020 2020 2020 2020  ayer.           
-00018520: 203e 3e3e 2066 726f 6d20 6d69 6e64 7370   >>> from mindsp
-00018530: 6f72 6520 696d 706f 7274 2054 656e 736f  ore import Tenso
-00018540: 720a 2020 2020 2020 2020 2020 2020 3e3e  r.            >>
-00018550: 3e20 6d6f 6465 6c20 3d20 5472 616e 7366  > model = Transf
-00018560: 6f72 6d65 7245 6e63 6f64 6572 4c61 7965  ormerEncoderLaye
-00018570: 7228 6261 7463 685f 7369 7a65 3d32 2c20  r(batch_size=2, 
-00018580: 6869 6464 656e 5f73 697a 653d 382c 2066  hidden_size=8, f
-00018590: 666e 5f68 6964 6465 6e5f 7369 7a65 3d36  fn_hidden_size=6
-000185a0: 342c 2073 6571 5f6c 656e 6774 683d 3136  4, seq_length=16
-000185b0: 2c0a 2020 2020 2020 2020 2020 2020 2e2e  ,.            ..
-000185c0: 2e20 2020 2020 2020 2020 2020 2020 2020  .               
-000185d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000185e0: 2020 6e75 6d5f 6865 6164 733d 3229 0a20    num_heads=2). 
-000185f0: 2020 2020 2020 2020 2020 203e 3e3e 2065             >>> e
-00018600: 6e63 6f64 6572 5f69 6e70 7574 5f76 616c  ncoder_input_val
-00018610: 7565 203d 2054 656e 736f 7228 6e70 2e6f  ue = Tensor(np.o
-00018620: 6e65 7328 2832 2c20 3136 2c20 3829 292c  nes((2, 16, 8)),
-00018630: 206d 7374 7970 652e 666c 6f61 7433 3229   mstype.float32)
-00018640: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
-00018650: 2065 6e63 6f64 6572 5f69 6e70 7574 5f6d   encoder_input_m
-00018660: 6173 6b20 3d20 5465 6e73 6f72 286e 702e  ask = Tensor(np.
-00018670: 6f6e 6573 2828 322c 2031 362c 2031 3629  ones((2, 16, 16)
-00018680: 292c 206d 7374 7970 652e 666c 6f61 7431  ), mstype.float1
-00018690: 3629 0a20 2020 2020 2020 2020 2020 203e  6).            >
-000186a0: 3e3e 206f 7574 7075 742c 2070 6173 7420  >> output, past 
-000186b0: 3d20 6d6f 6465 6c28 656e 636f 6465 725f  = model(encoder_
-000186c0: 696e 7075 745f 7661 6c75 652c 2065 6e63  input_value, enc
-000186d0: 6f64 6572 5f69 6e70 7574 5f6d 6173 6b29  oder_input_mask)
-000186e0: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
-000186f0: 2070 7269 6e74 286f 7574 7075 742e 7368   print(output.sh
-00018700: 6170 6529 0a20 2020 2020 2020 2020 2020  ape).           
-00018710: 2028 322c 2031 362c 2038 290a 2020 2020   (2, 16, 8).    
-00018720: 2020 2020 2020 2020 3e3e 3e20 7072 696e          >>> prin
-00018730: 7428 7061 7374 5b30 5d2e 7368 6170 6529  t(past[0].shape)
-00018740: 0a20 2020 2020 2020 2020 2020 2028 322c  .            (2,
-00018750: 2032 2c20 342c 2031 3629 0a20 2020 2020   2, 4, 16).     
-00018760: 2020 2020 2020 203e 3e3e 2070 7269 6e74         >>> print
-00018770: 2870 6173 745b 315d 2e73 6861 7065 290a  (past[1].shape).
-00018780: 2020 2020 2020 2020 2020 2020 2832 2c20              (2, 
-00018790: 322c 2031 362c 2034 290a 2020 2020 2020  2, 16, 4).      
-000187a0: 2020 2020 2020 3e3e 3e20 2320 5768 656e        >>> # When
-000187b0: 2075 7365 2075 7365 5f70 6173 743d 5472   use use_past=Tr
-000187c0: 7565 2c20 6974 2069 6e63 6c75 6465 7320  ue, it includes 
-000187d0: 7477 6f20 7374 6570 7320 746f 2069 6d70  two steps to imp
-000187e0: 6c65 6d65 6e74 2074 6865 2069 6e63 7265  lement the incre
-000187f0: 6d65 6e74 616c 2070 7265 6469 6374 696f  mental predictio
-00018800: 6e2e 0a20 2020 2020 2020 2020 2020 203e  n..            >
-00018810: 3e3e 2023 2053 7465 7020 313a 2073 6574  >> # Step 1: set
-00018820: 2069 735f 6669 7273 745f 6974 6572 6174   is_first_iterat
-00018830: 696f 6e3d 5472 7565 2c20 616e 6420 696e  ion=True, and in
-00018840: 7075 7420 7468 6520 6675 6c6c 2073 6571  put the full seq
-00018850: 7565 6e63 6520 6c65 6e67 7468 2773 2073  uence length's s
-00018860: 7461 7465 2e0a 2020 2020 2020 2020 2020  tate..          
-00018870: 2020 3e3e 3e20 6261 7463 685f 7661 6c69    >>> batch_vali
-00018880: 645f 6c65 6e67 7468 203d 2054 656e 736f  d_length = Tenso
-00018890: 7228 6e70 2e6f 6e65 7328 2832 2c29 292c  r(np.ones((2,)),
-000188a0: 206d 7374 7970 652e 696e 7433 3229 0a20   mstype.int32). 
-000188b0: 2020 2020 2020 2020 2020 203e 3e3e 2069             >>> i
-000188c0: 6e69 745f 7265 7365 7420 3d20 5465 6e73  nit_reset = Tens
-000188d0: 6f72 285b 5472 7565 5d2c 206d 7374 7970  or([True], mstyp
-000188e0: 652e 626f 6f6c 5f29 0a20 2020 2020 2020  e.bool_).       
-000188f0: 2020 2020 203e 3e3e 2023 2053 6574 2069       >>> # Set i
-00018900: 735f 6669 7273 745f 6974 6572 6174 696f  s_first_iteratio
-00018910: 6e3d 5472 7565 2074 6f20 6765 6e65 7261  n=True to genera
-00018920: 7465 2074 6865 2066 756c 6c20 6d65 6d6f  te the full memo
-00018930: 7279 2073 7461 7465 730a 2020 2020 2020  ry states.      
-00018940: 2020 2020 2020 3e3e 3e20 6d6f 6465 6c20        >>> model 
-00018950: 3d20 5472 616e 7366 6f72 6d65 7245 6e63  = TransformerEnc
-00018960: 6f64 6572 4c61 7965 7228 6261 7463 685f  oderLayer(batch_
-00018970: 7369 7a65 3d32 2c20 6869 6464 656e 5f73  size=2, hidden_s
-00018980: 697a 653d 382c 2066 666e 5f68 6964 6465  ize=8, ffn_hidde
-00018990: 6e5f 7369 7a65 3d36 342c 2073 6571 5f6c  n_size=64, seq_l
-000189a0: 656e 6774 683d 3136 2c0a 2020 2020 2020  ength=16,.      
-000189b0: 2020 2020 2020 2e2e 2e20 2020 2020 2020        ...       
-000189c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000189d0: 2020 2020 2020 2020 2020 6e75 6d5f 6865            num_he
-000189e0: 6164 733d 322c 2075 7365 5f70 6173 743d  ads=2, use_past=
-000189f0: 5472 7565 290a 2020 2020 2020 2020 2020  True).          
-00018a00: 2020 3e3e 3e20 6d6f 6465 6c2e 6164 645f    >>> model.add_
-00018a10: 666c 6167 735f 7265 6375 7273 6976 6528  flags_recursive(
-00018a20: 6973 5f66 6972 7374 5f69 7465 7261 7469  is_first_iterati
-00018a30: 6f6e 3d54 7275 6529 0a20 2020 2020 2020  on=True).       
-00018a40: 2020 2020 203e 3e3e 2068 6964 6465 6e2c       >>> hidden,
-00018a50: 2070 6173 7420 3d20 6d6f 6465 6c28 656e   past = model(en
-00018a60: 636f 6465 725f 696e 7075 745f 7661 6c75  coder_input_valu
-00018a70: 652c 2065 6e63 6f64 6572 5f69 6e70 7574  e, encoder_input
-00018a80: 5f6d 6173 6b2c 2069 6e69 745f 7265 7365  _mask, init_rese
-00018a90: 742c 2062 6174 6368 5f76 616c 6964 5f6c  t, batch_valid_l
-00018aa0: 656e 6774 6829 0a20 2020 2020 2020 2020  ength).         
-00018ab0: 2020 203e 3e3e 2070 7269 6e74 2868 6964     >>> print(hid
-00018ac0: 6465 6e2e 7368 6170 6529 0a20 2020 2020  den.shape).     
-00018ad0: 2020 2020 2020 2028 322c 2031 362c 2038         (2, 16, 8
-00018ae0: 290a 2020 2020 2020 2020 2020 2020 3e3e  ).            >>
-00018af0: 3e20 7072 696e 7428 7061 7374 5b30 5d2e  > print(past[0].
-00018b00: 7368 6170 6529 0a20 2020 2020 2020 2020  shape).         
-00018b10: 2020 2028 322c 2032 2c20 342c 2031 3629     (2, 2, 4, 16)
-00018b20: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
-00018b30: 2070 7269 6e74 2870 6173 745b 315d 2e73   print(past[1].s
-00018b40: 6861 7065 290a 2020 2020 2020 2020 2020  hape).          
-00018b50: 2020 2832 2c20 322c 2031 362c 2034 290a    (2, 2, 16, 4).
-00018b60: 2020 2020 2020 2020 2020 2020 3e3e 3e20              >>> 
-00018b70: 656e 636f 6465 725f 696e 7075 745f 7661  encoder_input_va
-00018b80: 6c75 6520 3d20 5465 6e73 6f72 286e 702e  lue = Tensor(np.
-00018b90: 6f6e 6573 2828 322c 2031 2c20 3829 292c  ones((2, 1, 8)),
-00018ba0: 206d 7374 7970 652e 666c 6f61 7433 3229   mstype.float32)
-00018bb0: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
-00018bc0: 2065 6e63 6f64 6572 5f69 6e70 7574 5f6d   encoder_input_m
-00018bd0: 6173 6b20 3d20 5465 6e73 6f72 286e 702e  ask = Tensor(np.
-00018be0: 6f6e 6573 2828 322c 2031 2c20 3136 2929  ones((2, 1, 16))
-00018bf0: 2c20 6d73 7479 7065 2e66 6c6f 6174 3136  , mstype.float16
-00018c00: 290a 2020 2020 2020 2020 2020 2020 3e3e  ).            >>
-00018c10: 3e20 696e 6974 5f72 6573 6574 203d 2054  > init_reset = T
-00018c20: 656e 736f 7228 5b46 616c 7365 5d2c 206d  ensor([False], m
-00018c30: 7374 7970 652e 626f 6f6c 5f29 0a20 2020  stype.bool_).   
-00018c40: 2020 2020 2020 2020 203e 3e3e 2023 2053           >>> # S
-00018c50: 7465 7020 323a 2073 6574 2069 735f 6669  tep 2: set is_fi
-00018c60: 7273 745f 6974 6572 6174 696f 6e3d 4661  rst_iteration=Fa
-00018c70: 6c73 652c 2061 6e64 2070 6173 7320 7468  lse, and pass th
-00018c80: 6520 7369 6e67 6c65 2077 6f72 6420 746f  e single word to
-00018c90: 2072 756e 2074 6865 2070 7265 6469 6374   run the predict
-00018ca0: 696f 6e20 7261 7468 6572 2074 6861 6e0a  ion rather than.
-00018cb0: 2020 2020 2020 2020 2020 2020 3e3e 3e20              >>> 
-00018cc0: 2320 7468 6520 6675 6c6c 2073 6571 7565  # the full seque
-00018cd0: 6e63 652e 0a20 2020 2020 2020 2020 2020  nce..           
-00018ce0: 203e 3e3e 206d 6f64 656c 2e61 6464 5f66   >>> model.add_f
-00018cf0: 6c61 6773 5f72 6563 7572 7369 7665 2869  lags_recursive(i
-00018d00: 735f 6669 7273 745f 6974 6572 6174 696f  s_first_iteratio
-00018d10: 6e3d 4661 6c73 6529 0a20 2020 2020 2020  n=False).       
-00018d20: 2020 2020 203e 3e3e 2068 6964 6465 6e2c       >>> hidden,
-00018d30: 2070 6173 7420 3d20 6d6f 6465 6c28 656e   past = model(en
-00018d40: 636f 6465 725f 696e 7075 745f 7661 6c75  coder_input_valu
-00018d50: 652c 2065 6e63 6f64 6572 5f69 6e70 7574  e, encoder_input
-00018d60: 5f6d 6173 6b2c 2069 6e69 745f 7265 7365  _mask, init_rese
-00018d70: 742c 2062 6174 6368 5f76 616c 6964 5f6c  t, batch_valid_l
-00018d80: 656e 6774 6829 0a20 2020 2020 2020 2020  ength).         
-00018d90: 2020 203e 3e3e 2070 7269 6e74 2868 6964     >>> print(hid
-00018da0: 6465 6e2e 7368 6170 6529 0a20 2020 2020  den.shape).     
-00018db0: 2020 2020 2020 2028 322c 2031 2c20 3829         (2, 1, 8)
-00018dc0: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
-00018dd0: 2070 7269 6e74 2870 6173 745b 305d 2e73   print(past[0].s
-00018de0: 6861 7065 290a 2020 2020 2020 2020 2020  hape).          
-00018df0: 2020 2832 2c20 322c 2034 2c20 3136 290a    (2, 2, 4, 16).
-00018e00: 2020 2020 2020 2020 2020 2020 3e3e 3e20              >>> 
-00018e10: 7072 696e 7428 7061 7374 5b31 5d2e 7368  print(past[1].sh
-00018e20: 6170 6529 0a20 2020 2020 2020 2020 2020  ape).           
-00018e30: 2028 322c 2032 2c20 3136 2c20 3429 0a20   (2, 2, 16, 4). 
-00018e40: 2020 2022 2222 0a0a 2020 2020 405f 4c6f     """..    @_Lo
-00018e50: 6741 6374 696f 6e4f 6e63 6528 6d5f 6c6f  gActionOnce(m_lo
-00018e60: 6767 6572 3d6c 6f67 6765 722c 206b 6579  gger=logger, key
-00018e70: 3d27 5472 616e 7366 6f72 6d65 7245 6e63  ='TransformerEnc
-00018e80: 6f64 6572 4c61 7965 7227 2c0a 2020 2020  oderLayer',.    
-00018e90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00018ea0: 6e6f 5f77 6172 6e69 6e67 3d5f 6765 745f  no_warning=_get_
-00018eb0: 7061 7261 6c6c 656c 5f6d 6f64 6528 2920  parallel_mode() 
-00018ec0: 696e 2028 5061 7261 6c6c 656c 4d6f 6465  in (ParallelMode
-00018ed0: 2e53 5441 4e44 5f41 4c4f 4e45 2c29 290a  .STAND_ALONE,)).
-00018ee0: 2020 2020 405f 6172 6773 5f74 7970 655f      @_args_type_
-00018ef0: 7661 6c69 6461 746f 725f 6368 6563 6b28  validator_check(
-00018f00: 6869 6464 656e 5f73 697a 653d 5661 6c69  hidden_size=Vali
-00018f10: 6461 746f 722e 6368 6563 6b5f 706f 7369  dator.check_posi
-00018f20: 7469 7665 5f69 6e74 2c0a 2020 2020 2020  tive_int,.      
-00018f30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00018f40: 2020 2020 2020 2020 2020 6e75 6d5f 6865            num_he
-00018f50: 6164 733d 5661 6c69 6461 746f 722e 6368  ads=Validator.ch
-00018f60: 6563 6b5f 706f 7369 7469 7665 5f69 6e74  eck_positive_int
-00018f70: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00018f80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00018f90: 2020 6666 6e5f 6869 6464 656e 5f73 697a    ffn_hidden_siz
-00018fa0: 653d 5661 6c69 6461 746f 722e 6368 6563  e=Validator.chec
-00018fb0: 6b5f 706f 7369 7469 7665 5f69 6e74 2c0a  k_positive_int,.
-00018fc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00018fd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00018fe0: 7365 715f 6c65 6e67 7468 3d56 616c 6964  seq_length=Valid
-00018ff0: 6174 6f72 2e63 6865 636b 5f70 6f73 6974  ator.check_posit
-00019000: 6976 655f 696e 742c 0a20 2020 2020 2020  ive_int,.       
-00019010: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019020: 2020 2020 2020 2020 2061 7474 656e 7469           attenti
-00019030: 6f6e 5f64 726f 706f 7574 5f72 6174 653d  on_dropout_rate=
-00019040: 5661 6c69 6461 746f 722e 6368 6563 6b5f  Validator.check_
-00019050: 6e6f 6e5f 6e65 6761 7469 7665 5f66 6c6f  non_negative_flo
-00019060: 6174 2c0a 2020 2020 2020 2020 2020 2020  at,.            
-00019070: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019080: 2020 2020 6869 6464 656e 5f64 726f 706f      hidden_dropo
-00019090: 7574 5f72 6174 653d 5661 6c69 6461 746f  ut_rate=Validato
-000190a0: 722e 6368 6563 6b5f 6e6f 6e5f 6e65 6761  r.check_non_nega
-000190b0: 7469 7665 5f66 6c6f 6174 2c0a 2020 2020  tive_float,.    
-000190c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000190d0: 2020 2020 2020 2020 2020 2020 706f 7374              post
-000190e0: 5f6c 6179 6572 6e6f 726d 5f72 6573 6964  _layernorm_resid
-000190f0: 7561 6c3d 5661 6c69 6461 746f 722e 6368  ual=Validator.ch
-00019100: 6563 6b5f 626f 6f6c 2c0a 2020 2020 2020  eck_bool,.      
-00019110: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019120: 2020 2020 2020 2020 2020 6c61 7965 726e            layern
-00019130: 6f72 6d5f 636f 6d70 7574 655f 7479 7065  orm_compute_type
-00019140: 3d5f 7661 6c69 645f 7661 6c75 655f 6368  =_valid_value_ch
-00019150: 6563 6b73 285b 6d73 7479 7065 2e66 6c6f  ecks([mstype.flo
-00019160: 6174 3332 2c0a 2020 2020 2020 2020 2020  at32,.          
-00019170: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019180: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019190: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000191a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000191b0: 2020 6d73 7479 7065 2e66 6c6f 6174 3136    mstype.float16
-000191c0: 2c20 6d73 7479 7065 2e62 666c 6f61 7431  , mstype.bfloat1
-000191d0: 365d 2c0a 2020 2020 2020 2020 2020 2020  6],.            
-000191e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000191f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019200: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019210: 2020 2020 2020 2020 2020 2020 2020 2022                 "
-00019220: 5472 616e 7366 6f72 6d65 7245 6e63 6f64  TransformerEncod
-00019230: 6572 4c61 7965 7222 292c 0a20 2020 2020  erLayer"),.     
-00019240: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019250: 2020 2020 2020 2020 2020 2073 6f66 746d             softm
-00019260: 6178 5f63 6f6d 7075 7465 5f74 7970 653d  ax_compute_type=
-00019270: 5f76 616c 6964 5f76 616c 7565 5f63 6865  _valid_value_che
-00019280: 636b 7328 5b6d 7374 7970 652e 666c 6f61  cks([mstype.floa
-00019290: 7433 322c 0a20 2020 2020 2020 2020 2020  t32,.           
+00013200: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00013210: 2020 2020 2020 696e 7075 745f 6c61 796f        input_layo
+00013220: 7574 3d27 424e 5344 272c 0a20 2020 2020  ut='BNSD',.     
+00013230: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00013240: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00013250: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00013260: 2020 2020 2020 2020 6e75 6d5f 6b65 795f          num_key_
+00013270: 7661 6c75 655f 6865 6164 733d 3029 0a20  value_heads=0). 
+00013280: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+00013290: 696e 6372 655f 666c 6173 685f 6174 7465  incre_flash_atte
+000132a0: 6e74 696f 6e2e 7368 6172 6428 2828 7061  ntion.shard(((pa
+000132b0: 7261 6c6c 656c 5f63 6f6e 6669 672e 6461  rallel_config.da
+000132c0: 7461 5f70 6172 616c 6c65 6c2c 2070 6172  ta_parallel, par
+000132d0: 616c 6c65 6c5f 636f 6e66 6967 2e6d 6f64  allel_config.mod
+000132e0: 656c 5f70 6172 616c 6c65 6c2c 2031 2c20  el_parallel, 1, 
+000132f0: 3129 2c0a 2020 2020 2020 2020 2020 2020  1),.            
+00013300: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00013310: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00013320: 2020 2870 6172 616c 6c65 6c5f 636f 6e66    (parallel_conf
+00013330: 6967 2e64 6174 615f 7061 7261 6c6c 656c  ig.data_parallel
+00013340: 2c20 7061 7261 6c6c 656c 5f63 6f6e 6669  , parallel_confi
+00013350: 672e 6d6f 6465 6c5f 7061 7261 6c6c 656c  g.model_parallel
+00013360: 2c20 312c 2031 292c 0a20 2020 2020 2020  , 1, 1),.       
+00013370: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00013380: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00013390: 2020 2020 2020 2028 7061 7261 6c6c 656c         (parallel
+000133a0: 5f63 6f6e 6669 672e 6461 7461 5f70 6172  _config.data_par
+000133b0: 616c 6c65 6c2c 2070 6172 616c 6c65 6c5f  allel, parallel_
+000133c0: 636f 6e66 6967 2e6d 6f64 656c 5f70 6172  config.model_par
+000133d0: 616c 6c65 6c2c 2031 2c20 3129 2c0a 2020  allel, 1, 1),.  
+000133e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000133f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00013400: 2020 2020 2020 2020 2020 2020 2870 6172              (par
+00013410: 616c 6c65 6c5f 636f 6e66 6967 2e64 6174  allel_config.dat
+00013420: 615f 7061 7261 6c6c 656c 2c20 312c 2031  a_parallel, 1, 1
+00013430: 2c20 3129 2929 0a0a 2020 2020 2020 2020  , 1)))..        
+00013440: 6966 2070 6172 616c 6c65 6c5f 636f 6e66  if parallel_conf
+00013450: 6967 2e73 656c 6563 745f 7265 636f 6d70  ig.select_recomp
+00013460: 7574 653a 0a20 2020 2020 2020 2020 2020  ute:.           
+00013470: 2023 205f 6174 746e e4b8 ade6 b689 e58f   # _attn........
+00013480: 8ae7 9a84 e585 b3e9 94ae e7ae 97e5 ad90  ................
+00013490: e4bd bfe7 94a8 e987 8de8 aea1 e7ae 97e9  ................
+000134a0: 80bb e8be 91ef bc8c e980 9ae5 b8b8 e985  ................
+000134b0: 8de5 9088 e5ba 8fe5 8897 e5b9 b6e8 a18c  ................
+000134c0: e4bd bfe7 94a8 efbc 8ce4 bc9a e69c 89e8  ................
+000134d0: be83 e5a5 bde7 9a84 e680 a7e8 83bd e68f  ................
+000134e0: 90e5 8d87 e695 88e6 9e9c 0a20 2020 2020  ...........     
+000134f0: 2020 2020 2020 2073 656c 662e 6261 7463         self.batc
+00013500: 685f 6d61 746d 756c 2e72 6563 6f6d 7075  h_matmul.recompu
+00013510: 7465 2829 0a20 2020 2020 2020 2020 2020  te().           
+00013520: 2073 656c 662e 7375 622e 7265 636f 6d70   self.sub.recomp
+00013530: 7574 6528 290a 2020 2020 2020 2020 2020  ute().          
+00013540: 2020 7365 6c66 2e61 6464 2e72 6563 6f6d    self.add.recom
+00013550: 7075 7465 2829 0a20 2020 2020 2020 2020  pute().         
+00013560: 2020 2073 656c 662e 6d65 7267 6572 5f68     self.merger_h
+00013570: 6561 645f 7472 616e 7370 6f73 652e 7265  ead_transpose.re
+00013580: 636f 6d70 7574 6528 290a 2020 2020 2020  compute().      
+00013590: 2020 2020 2020 7365 6c66 2e73 6f66 746d        self.softm
+000135a0: 6178 5f72 6573 6861 7065 2e72 6563 6f6d  ax_reshape.recom
+000135b0: 7075 7465 2829 0a20 2020 2020 2020 2020  pute().         
+000135c0: 2020 2073 656c 662e 7072 6f62 5f64 726f     self.prob_dro
+000135d0: 706f 7574 2e72 6563 6f6d 7075 7465 2829  pout.recompute()
+000135e0: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
+000135f0: 662e 736f 6674 6d61 785f 6361 7374 2e72  f.softmax_cast.r
+00013600: 6563 6f6d 7075 7465 2829 0a20 2020 2020  ecompute().     
+00013610: 2020 2020 2020 2073 656c 662e 736f 6674         self.soft
+00013620: 6d61 782e 736f 6674 6d61 782e 7265 636f  max.softmax.reco
+00013630: 6d70 7574 6528 290a 2020 2020 2020 2020  mpute().        
+00013640: 2020 2020 7365 6c66 2e73 6f66 746d 6178      self.softmax
+00013650: 5f33 642e 7265 636f 6d70 7574 6528 290a  _3d.recompute().
+00013660: 0a20 2020 2064 6566 2063 6f6e 7374 7275  .    def constru
+00013670: 6374 2873 656c 662c 2071 7565 7279 5f74  ct(self, query_t
+00013680: 656e 736f 722c 206b 6579 5f74 656e 736f  ensor, key_tenso
+00013690: 722c 2076 616c 7565 5f74 656e 736f 722c  r, value_tensor,
+000136a0: 2061 7474 656e 7469 6f6e 5f6d 6173 6b2c   attention_mask,
+000136b0: 206b 6579 5f70 6173 743d 4e6f 6e65 2c0a   key_past=None,.
+000136c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000136d0: 2020 7661 6c75 655f 7061 7374 3d4e 6f6e    value_past=Non
+000136e0: 652c 2062 6174 6368 5f76 616c 6964 5f6c  e, batch_valid_l
+000136f0: 656e 6774 683d 4e6f 6e65 293a 0a20 2020  ength=None):.   
+00013700: 2020 2020 2022 2222 466f 7277 6172 6420       """Forward 
+00013710: 7072 6f63 6573 7320 6f66 2074 6865 204d  process of the M
+00013720: 756c 7469 4865 6164 4174 7465 6e74 696f  ultiHeadAttentio
+00013730: 6e22 2222 0a20 2020 2020 2020 2073 656c  n""".        sel
+00013740: 662e 5f63 6865 636b 5f69 6e70 7574 7328  f._check_inputs(
+00013750: 7175 6572 795f 7465 6e73 6f72 2c20 6b65  query_tensor, ke
+00013760: 795f 7465 6e73 6f72 2c20 7661 6c75 655f  y_tensor, value_
+00013770: 7465 6e73 6f72 2c20 6174 7465 6e74 696f  tensor, attentio
+00013780: 6e5f 6d61 736b 2c20 6b65 795f 7061 7374  n_mask, key_past
+00013790: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+000137a0: 2020 2020 2020 2020 2020 2020 2076 616c               val
+000137b0: 7565 5f70 6173 742c 2062 6174 6368 5f76  ue_past, batch_v
+000137c0: 616c 6964 5f6c 656e 6774 6829 0a20 2020  alid_length).   
+000137d0: 2020 2020 206f 7269 5f73 6861 7065 203d       ori_shape =
+000137e0: 2046 2e73 6861 7065 2871 7565 7279 5f74   F.shape(query_t
+000137f0: 656e 736f 7229 0a20 2020 2020 2020 2062  ensor).        b
+00013800: 6174 6368 5f73 697a 6520 3d20 7365 6c66  atch_size = self
+00013810: 2e5f 6765 745f 6261 7463 685f 7369 7a65  ._get_batch_size
+00013820: 5f66 726f 6d5f 7175 6572 7928 7175 6572  _from_query(quer
+00013830: 795f 7465 6e73 6f72 290a 2020 2020 2020  y_tensor).      
+00013840: 2020 7175 6572 795f 7465 6e73 6f72 2c20    query_tensor, 
+00013850: 6b65 795f 7465 6e73 6f72 2c20 7661 6c75  key_tensor, valu
+00013860: 655f 7465 6e73 6f72 203d 2073 656c 662e  e_tensor = self.
+00013870: 5f63 6f6e 7665 7274 5f74 6f5f 3264 5f74  _convert_to_2d_t
+00013880: 656e 736f 7228 7175 6572 795f 7465 6e73  ensor(query_tens
+00013890: 6f72 2c0a 2020 2020 2020 2020 2020 2020  or,.            
+000138a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000138b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000138c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000138d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000138e0: 6b65 795f 7465 6e73 6f72 2c0a 2020 2020  key_tensor,.    
+000138f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00013900: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00013910: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00013920: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00013930: 2020 2020 2020 2020 7661 6c75 655f 7465          value_te
+00013940: 6e73 6f72 290a 2020 2020 2020 2020 6f72  nsor).        or
+00013950: 695f 6474 7970 6520 3d20 462e 6474 7970  i_dtype = F.dtyp
+00013960: 6528 7175 6572 795f 7465 6e73 6f72 290a  e(query_tensor).
+00013970: 2020 2020 2020 2020 7175 6572 795f 7465          query_te
+00013980: 6e73 6f72 203d 2046 2e63 6173 7428 7175  nsor = F.cast(qu
+00013990: 6572 795f 7465 6e73 6f72 2c20 7365 6c66  ery_tensor, self
+000139a0: 2e64 7479 7065 290a 2020 2020 2020 2020  .dtype).        
+000139b0: 6b65 795f 7465 6e73 6f72 203d 2046 2e63  key_tensor = F.c
+000139c0: 6173 7428 6b65 795f 7465 6e73 6f72 2c20  ast(key_tensor, 
+000139d0: 7365 6c66 2e64 7479 7065 290a 2020 2020  self.dtype).    
+000139e0: 2020 2020 7661 6c75 655f 7465 6e73 6f72      value_tensor
+000139f0: 203d 2046 2e63 6173 7428 7661 6c75 655f   = F.cast(value_
+00013a00: 7465 6e73 6f72 2c20 7365 6c66 2e64 7479  tensor, self.dty
+00013a10: 7065 290a 2020 2020 2020 2020 2320 6d75  pe).        # mu
+00013a20: 6c74 6920 6865 6164 2061 7474 656e 7469  lti head attenti
+00013a30: 6f6e 3a20 7175 6572 792c 206b 6579 2c20  on: query, key, 
+00013a40: 7661 6c75 6520 6172 6520 6465 7269 7665  value are derive
+00013a50: 6420 6672 6f6d 2074 6865 2073 616d 6520  d from the same 
+00013a60: 696e 7075 7473 0a20 2020 2020 2020 2071  inputs.        q
+00013a70: 7565 7279 203d 2073 656c 662e 6465 6e73  uery = self.dens
+00013a80: 6531 2871 7565 7279 5f74 656e 736f 7229  e1(query_tensor)
+00013a90: 0a20 2020 2020 2020 206b 6579 203d 2073  .        key = s
+00013aa0: 656c 662e 6465 6e73 6532 286b 6579 5f74  elf.dense2(key_t
+00013ab0: 656e 736f 7229 0a20 2020 2020 2020 2076  ensor).        v
+00013ac0: 616c 7565 203d 2073 656c 662e 6465 6e73  alue = self.dens
+00013ad0: 6533 2876 616c 7565 5f74 656e 736f 7229  e3(value_tensor)
+00013ae0: 0a20 2020 2020 2020 2023 2074 6865 2072  .        # the r
+00013af0: 6574 7572 6e65 6420 7368 6170 6520 6973  eturned shape is
+00013b00: 205b 6273 2c20 6e75 6d5f 6865 6164 732c   [bs, num_heads,
+00013b10: 2073 6571 5f6c 656e 6774 682c 2073 697a   seq_length, siz
+00013b20: 655f 7065 725f 6865 6164 5d0a 2020 2020  e_per_head].    
+00013b30: 2020 2020 7175 6572 7920 3d20 7365 6c66      query = self
+00013b40: 2e74 7261 6e73 706f 7365 280a 2020 2020  .transpose(.    
+00013b50: 2020 2020 2020 2020 462e 7265 7368 6170          F.reshap
+00013b60: 6528 0a20 2020 2020 2020 2020 2020 2020  e(.             
+00013b70: 2020 2071 7565 7279 2c0a 2020 2020 2020     query,.      
+00013b80: 2020 2020 2020 2020 2020 2862 6174 6368            (batch
+00013b90: 5f73 697a 652c 2073 656c 662e 5f67 6574  _size, self._get
+00013ba0: 5f73 6571 5f6c 656e 6774 685f 756e 6465  _seq_length_unde
+00013bb0: 725f 696e 6372 656d 656e 7461 6c28 7365  r_incremental(se
+00013bc0: 6c66 2e73 7263 5f73 6571 5f6c 656e 6774  lf.src_seq_lengt
+00013bd0: 6829 2c0a 2020 2020 2020 2020 2020 2020  h),.            
+00013be0: 2020 2020 2073 656c 662e 6e5f 6865 6164       self.n_head
+00013bf0: 2c20 7365 6c66 2e73 697a 655f 7065 725f  , self.size_per_
+00013c00: 6865 6164 2929 2c0a 2020 2020 2020 2020  head)),.        
+00013c10: 2020 2020 2830 2c20 322c 2031 2c20 3329      (0, 2, 1, 3)
+00013c20: 290a 2020 2020 2020 2020 2320 4641 3a20  ).        # FA: 
+00013c30: 5b62 732c 206e 756d 5f68 6561 6473 2c20  [bs, num_heads, 
+00013c40: 7365 715f 6c65 6e67 7468 2c20 7369 7a65  seq_length, size
+00013c50: 5f70 6572 5f68 6561 645d 206f 7220 5b62  _per_head] or [b
+00013c60: 732c 206e 756d 5f68 6561 6473 2c20 7369  s, num_heads, si
+00013c70: 7a65 5f70 6572 5f68 6561 642c 2073 6571  ze_per_head, seq
+00013c80: 5f6c 656e 6774 685d 0a20 2020 2020 2020  _length].       
+00013c90: 2069 6620 6e6f 7420 7365 6c66 2e74 7261   if not self.tra
+00013ca0: 696e 696e 673a 0a20 2020 2020 2020 2020  ining:.         
+00013cb0: 2020 2064 6f5f 6469 6666 6572 656e 745f     do_different_
+00013cc0: 7368 6170 6520 3d20 2873 656c 662e 7573  shape = (self.us
+00013cd0: 655f 666c 6173 685f 6174 7465 6e74 696f  e_flash_attentio
+00013ce0: 6e0a 2020 2020 2020 2020 2020 2020 2020  n.              
+00013cf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00013d00: 2020 2020 6f72 2073 656c 662e 7573 655f      or self.use_
+00013d10: 7072 6f6d 7074 5f66 6c61 7368 5f61 7474  prompt_flash_att
+00013d20: 656e 7469 6f6e 0a20 2020 2020 2020 2020  ention.         
+00013d30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00013d40: 2020 2020 2020 2020 206f 7220 7365 6c66           or self
+00013d50: 2e75 7365 5f69 6e63 7265 5f66 6c61 7368  .use_incre_flash
+00013d60: 5f61 7474 656e 7469 6f6e 290a 2020 2020  _attention).    
+00013d70: 2020 2020 656c 7365 3a0a 2020 2020 2020      else:.      
+00013d80: 2020 2020 2020 646f 5f64 6966 6665 7265        do_differe
+00013d90: 6e74 5f73 6861 7065 203d 2073 656c 662e  nt_shape = self.
+00013da0: 7573 655f 666c 6173 685f 6174 7465 6e74  use_flash_attent
+00013db0: 696f 6e0a 0a20 2020 2020 2020 206b 6579  ion..        key
+00013dc0: 5f74 7261 6e73 706f 7365 5f73 6861 7065  _transpose_shape
+00013dd0: 203d 2028 302c 2032 2c20 312c 2033 2920   = (0, 2, 1, 3) 
+00013de0: 6966 2064 6f5f 6469 6666 6572 656e 745f  if do_different_
+00013df0: 7368 6170 6520 656c 7365 2028 302c 2032  shape else (0, 2
+00013e00: 2c20 332c 2031 290a 2020 2020 2020 2020  , 3, 1).        
+00013e10: 6b65 7920 3d20 7365 6c66 2e74 7261 6e73  key = self.trans
+00013e20: 706f 7365 280a 2020 2020 2020 2020 2020  pose(.          
+00013e30: 2020 462e 7265 7368 6170 6528 0a20 2020    F.reshape(.   
+00013e40: 2020 2020 2020 2020 2020 2020 206b 6579               key
+00013e50: 2c20 2862 6174 6368 5f73 697a 652c 2073  , (batch_size, s
+00013e60: 656c 662e 5f67 6574 5f73 6571 5f6c 656e  elf._get_seq_len
+00013e70: 6774 685f 756e 6465 725f 696e 6372 656d  gth_under_increm
+00013e80: 656e 7461 6c28 7365 6c66 2e74 6774 5f73  ental(self.tgt_s
+00013e90: 6571 5f6c 656e 6774 6829 2c0a 2020 2020  eq_length),.    
+00013ea0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00013eb0: 2020 7365 6c66 2e6e 5f68 6561 642c 2073    self.n_head, s
+00013ec0: 656c 662e 7369 7a65 5f70 6572 5f68 6561  elf.size_per_hea
+00013ed0: 6429 292c 0a20 2020 2020 2020 2020 2020  d)),.           
+00013ee0: 206b 6579 5f74 7261 6e73 706f 7365 5f73   key_transpose_s
+00013ef0: 6861 7065 290a 2020 2020 2020 2020 2320  hape).        # 
+00013f00: 7468 6520 7265 7475 726e 6564 2073 6861  the returned sha
+00013f10: 7065 2069 7320 5b62 732c 206e 756d 5f68  pe is [bs, num_h
+00013f20: 6561 6473 2c20 7365 715f 6c65 6e67 7468  eads, seq_length
+00013f30: 2c20 7369 7a65 5f70 6572 5f68 6561 645d  , size_per_head]
+00013f40: 0a20 2020 2020 2020 2076 616c 7565 203d  .        value =
+00013f50: 2073 656c 662e 7472 616e 7370 6f73 6528   self.transpose(
+00013f60: 0a20 2020 2020 2020 2020 2020 2046 2e72  .            F.r
+00013f70: 6573 6861 7065 280a 2020 2020 2020 2020  eshape(.        
+00013f80: 2020 2020 2020 2020 7661 6c75 652c 0a20          value,. 
+00013f90: 2020 2020 2020 2020 2020 2020 2020 2028                 (
+00013fa0: 6261 7463 685f 7369 7a65 2c20 7365 6c66  batch_size, self
+00013fb0: 2e5f 6765 745f 7365 715f 6c65 6e67 7468  ._get_seq_length
+00013fc0: 5f75 6e64 6572 5f69 6e63 7265 6d65 6e74  _under_increment
+00013fd0: 616c 2873 656c 662e 7467 745f 7365 715f  al(self.tgt_seq_
+00013fe0: 6c65 6e67 7468 292c 0a20 2020 2020 2020  length),.       
+00013ff0: 2020 2020 2020 2020 2020 7365 6c66 2e6e            self.n
+00014000: 5f68 6561 642c 2073 656c 662e 7369 7a65  _head, self.size
+00014010: 5f70 6572 5f68 6561 6429 292c 0a20 2020  _per_head)),.   
+00014020: 2020 2020 2020 2020 2028 302c 2032 2c20           (0, 2, 
+00014030: 312c 2033 2929 0a20 2020 2020 2020 2023  1, 3)).        #
+00014040: 2073 7570 706f 7274 2069 6e70 7574 2073   support input s
+00014050: 6861 7065 2069 7320 5b62 732c 2073 6571  hape is [bs, seq
+00014060: 2c20 7365 715d 206f 7220 5b62 732c 2068  , seq] or [bs, h
+00014070: 6561 6473 2c20 7365 712c 2073 6571 5d0a  eads, seq, seq].
+00014080: 2020 2020 2020 2020 2320 7066 6120 616e          # pfa an
+00014090: 6420 6661 7320 7573 6520 3464 206d 6173  d fas use 4d mas
+000140a0: 6b0a 2020 2020 2020 2020 6966 2061 7474  k.        if att
+000140b0: 656e 7469 6f6e 5f6d 6173 6b20 6973 206e  ention_mask is n
+000140c0: 6f74 204e 6f6e 6520 616e 6420 6c65 6e28  ot None and len(
+000140d0: 462e 7368 6170 6528 6174 7465 6e74 696f  F.shape(attentio
+000140e0: 6e5f 6d61 736b 2929 203d 3d20 3320 616e  n_mask)) == 3 an
+000140f0: 6420 5c0a 2020 2020 2020 2020 2020 2020  d \.            
+00014100: 2020 2020 2873 656c 662e 7573 655f 666c      (self.use_fl
+00014110: 6173 685f 6174 7465 6e74 696f 6e20 6f72  ash_attention or
+00014120: 206e 6f74 2073 656c 662e 7573 655f 666c   not self.use_fl
+00014130: 6173 685f 6174 7465 6e74 696f 6e20 6f72  ash_attention or
+00014140: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00014150: 2020 2828 6e6f 7420 7365 6c66 2e74 7261    ((not self.tra
+00014160: 696e 696e 6729 2061 6e64 2073 656c 662e  ining) and self.
+00014170: 7573 655f 7072 6f6d 7074 5f66 6c61 7368  use_prompt_flash
+00014180: 5f61 7474 656e 7469 6f6e 2929 3a0a 2020  _attention)):.  
+00014190: 2020 2020 2020 2020 2020 2320 6578 7061            # expa
+000141a0: 6e64 2061 7474 656e 7469 6f6e 206d 6173  nd attention mas
+000141b0: 6b20 6672 6f6d 205b 6273 2c20 7365 712c  k from [bs, seq,
+000141c0: 2073 6571 5d20 2d3e 205b 6273 2c20 312c   seq] -> [bs, 1,
+000141d0: 2073 6571 2c20 7365 715d 0a20 2020 2020   seq, seq].     
+000141e0: 2020 2020 2020 2061 7474 656e 7469 6f6e         attention
+000141f0: 5f6d 6173 6b20 3d20 7365 6c66 2e65 7870  _mask = self.exp
+00014200: 616e 645f 6469 6d73 2861 7474 656e 7469  and_dims(attenti
+00014210: 6f6e 5f6d 6173 6b2c 2031 290a 2020 2020  on_mask, 1).    
+00014220: 2020 2020 2320 6b65 7920 616e 6420 7661      # key and va
+00014230: 6c75 6520 666f 7220 6375 7272 656e 7420  lue for current 
+00014240: 746f 6b65 6e28 7329 0a20 2020 2020 2020  token(s).       
+00014250: 206b 6579 5f70 7265 7365 6e74 203d 206b   key_present = k
+00014260: 6579 0a20 2020 2020 2020 2076 616c 7565  ey.        value
+00014270: 5f70 7265 7365 6e74 203d 2076 616c 7565  _present = value
+00014280: 0a20 2020 2020 2020 2069 6620 7365 6c66  .        if self
+00014290: 2e75 7365 5f70 6173 743a 0a20 2020 2020  .use_past:.     
+000142a0: 2020 2020 2020 2023 2054 6865 2066 6972         # The fir
+000142b0: 7374 2067 7261 7068 2077 6974 6820 7468  st graph with th
+000142c0: 6520 696e 7075 7420 7369 7a65 206f 6620  e input size of 
+000142d0: 2862 732c 2073 6571 5f6c 656e 6774 6829  (bs, seq_length)
+000142e0: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if 
+000142f0: 7365 6c66 2e69 735f 6669 7273 745f 6974  self.is_first_it
+00014300: 6572 6174 696f 6e3a 0a20 2020 2020 2020  eration:.       
+00014310: 2020 2020 2020 2020 2023 2047 6574 2074           # Get t
+00014320: 6865 2076 616c 6964 2069 6e70 7574 206c  he valid input l
+00014330: 656e 6774 6820 7769 7468 6f75 7420 7061  ength without pa
+00014340: 6464 696e 670a 2020 2020 2020 2020 2020  dding.          
+00014350: 2020 2020 2020 7661 6c69 645f 6c65 6e67        valid_leng
+00014360: 7468 5f76 6563 746f 7220 3d20 462e 6361  th_vector = F.ca
+00014370: 7374 2873 656c 662e 6c65 7373 2873 656c  st(self.less(sel
+00014380: 662e 7261 6e67 652c 2062 6174 6368 5f76  f.range, batch_v
+00014390: 616c 6964 5f6c 656e 6774 682e 7669 6577  alid_length.view
+000143a0: 282d 312c 2031 2c20 3129 292c 2073 656c  (-1, 1, 1)), sel
+000143b0: 662e 6474 7970 6529 0a20 2020 2020 2020  f.dtype).       
+000143c0: 2020 2020 2020 2020 2023 2043 6f76 6572           # Cover
+000143d0: 2074 6865 206b 6579 2061 6e64 2076 616c   the key and val
+000143e0: 7565 206e 756d 6265 7273 2063 6f72 7265  ue numbers corre
+000143f0: 7370 6f6e 6469 6e67 2074 6f20 7468 6520  sponding to the 
+00014400: 7061 6464 696e 6720 706f 7369 7469 6f6e  padding position
+00014410: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00014420: 2065 7870 616e 645f 6178 6973 203d 2033   expand_axis = 3
+00014430: 2069 6620 646f 5f64 6966 6665 7265 6e74   if do_different
+00014440: 5f73 6861 7065 2065 6c73 6520 320a 2020  _shape else 2.  
+00014450: 2020 2020 2020 2020 2020 2020 2020 6b65                ke
+00014460: 795f 7072 6573 656e 7420 3d20 7365 6c66  y_present = self
+00014470: 2e6d 756c 3128 6b65 792c 2073 656c 662e  .mul1(key, self.
+00014480: 6578 7061 6e64 5f64 696d 7328 7661 6c69  expand_dims(vali
+00014490: 645f 6c65 6e67 7468 5f76 6563 746f 722c  d_length_vector,
+000144a0: 2065 7870 616e 645f 6178 6973 2929 0a20   expand_axis)). 
+000144b0: 2020 2020 2020 2020 2020 2020 2020 2076                 v
+000144c0: 616c 7565 5f70 7265 7365 6e74 203d 2073  alue_present = s
+000144d0: 656c 662e 6d75 6c31 2876 616c 7565 2c20  elf.mul1(value, 
+000144e0: 7365 6c66 2e65 7870 616e 645f 6469 6d73  self.expand_dims
+000144f0: 2876 616c 6964 5f6c 656e 6774 685f 7665  (valid_length_ve
+00014500: 6374 6f72 2c20 3329 290a 2020 2020 2020  ctor, 3)).      
+00014510: 2020 2020 2020 2320 5468 6520 7365 636f        # The seco
+00014520: 6e64 2067 7261 7068 2077 6974 6820 7468  nd graph with th
+00014530: 6520 696e 7075 7320 7369 7a65 206f 6620  e inpus size of 
+00014540: 2862 732c 2031 290a 2020 2020 2020 2020  (bs, 1).        
+00014550: 2020 2020 2320 7468 6520 7368 6170 6520      # the shape 
+00014560: 6f66 2071 7565 7279 2069 7320 2862 732c  of query is (bs,
+00014570: 206e 756d 5f68 6561 6473 2c20 312c 2073   num_heads, 1, s
+00014580: 697a 655f 7065 725f 6865 6164 290a 2020  ize_per_head).  
+00014590: 2020 2020 2020 2020 2020 2320 7468 6520            # the 
+000145a0: 7368 6170 6520 6f66 206b 6579 2069 7320  shape of key is 
+000145b0: 2020 2862 732c 206e 756d 5f68 6561 6473    (bs, num_heads
+000145c0: 2c20 7369 7a65 5f70 6572 5f68 6561 642c  , size_per_head,
+000145d0: 2031 290a 2020 2020 2020 2020 2020 2020   1).            
+000145e0: 2320 7468 6520 7368 6170 6520 6f66 2076  # the shape of v
+000145f0: 616c 7565 2069 7320 2862 732c 206e 756d  alue is (bs, num
+00014600: 5f68 6561 6473 2c20 312c 2073 697a 655f  _heads, 1, size_
+00014610: 7065 725f 6865 6164 290a 2020 2020 2020  per_head).      
+00014620: 2020 2020 2020 656c 7365 3a0a 2020 2020        else:.    
+00014630: 2020 2020 2020 2020 2020 2020 2320 4765              # Ge
+00014640: 7420 7468 6520 6375 7272 656e 7420 746f  t the current to
+00014650: 6b65 6e20 706f 7369 7469 6f6e 2069 6e64  ken position ind
+00014660: 6578 0a20 2020 2020 2020 2020 2020 2020  ex.             
+00014670: 2020 2076 616c 6964 5f6c 656e 6774 6820     valid_length 
+00014680: 3d20 6261 7463 685f 7661 6c69 645f 6c65  = batch_valid_le
+00014690: 6e67 7468 202d 2031 0a20 2020 2020 2020  ngth - 1.       
+000146a0: 2020 2020 2020 2020 2076 616c 6964 5f6c           valid_l
+000146b0: 656e 6774 6820 3d20 7365 6c66 2e72 6573  ength = self.res
+000146c0: 6861 7065 2876 616c 6964 5f6c 656e 6774  hape(valid_lengt
+000146d0: 682c 2028 2d31 2c20 312c 2031 2929 0a20  h, (-1, 1, 1)). 
+000146e0: 2020 2020 2020 2020 2020 2020 2020 2076                 v
+000146f0: 616c 6964 5f6c 656e 6774 685f 7665 6374  alid_length_vect
+00014700: 6f72 203d 2046 2e63 6173 7428 7365 6c66  or = F.cast(self
+00014710: 2e65 7175 616c 2876 616c 6964 5f6c 656e  .equal(valid_len
+00014720: 6774 682c 2073 656c 662e 7261 6e67 6529  gth, self.range)
+00014730: 2c20 7365 6c66 2e64 7479 7065 290a 2020  , self.dtype).  
+00014740: 2020 2020 2020 2020 2020 2020 2020 2320                # 
+00014750: 5061 6420 7468 6520 6b65 7920 616e 6420  Pad the key and 
+00014760: 7661 6c75 6520 746f 2073 6571 5f6c 656e  value to seq_len
+00014770: 6774 6820 7769 7468 206f 6e6c 7920 7468  gth with only th
+00014780: 6520 706f 7369 7469 6f6e 2069 6e64 6578  e position index
+00014790: 206e 6f74 207a 6572 6f0a 2020 2020 2020   not zero.      
+000147a0: 2020 2020 2020 2020 2020 6966 2064 6f5f            if do_
+000147b0: 6469 6666 6572 656e 745f 7368 6170 653a  different_shape:
+000147c0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+000147d0: 2020 2020 206d 756c 7469 706c 6573 203d       multiples =
+000147e0: 2028 312c 2031 2c20 7365 6c66 2e73 6571   (1, 1, self.seq
+000147f0: 5f6c 656e 6774 682c 2031 290a 2020 2020  _length, 1).    
+00014800: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00014810: 6578 7061 6e64 5f61 7869 7320 3d20 330a  expand_axis = 3.
+00014820: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00014830: 656c 7365 3a0a 2020 2020 2020 2020 2020  else:.          
+00014840: 2020 2020 2020 2020 2020 6d75 6c74 6970            multip
+00014850: 6c65 7320 3d20 2831 2c20 312c 2031 2c20  les = (1, 1, 1, 
+00014860: 7365 6c66 2e73 6571 5f6c 656e 6774 6829  self.seq_length)
+00014870: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00014880: 2020 2020 2065 7870 616e 645f 6178 6973       expand_axis
+00014890: 203d 2032 0a20 2020 2020 2020 2020 2020   = 2.           
+000148a0: 2020 2020 2063 7572 7265 6e74 5f6b 6579       current_key
+000148b0: 203d 2073 656c 662e 6d75 6c31 2873 656c   = self.mul1(sel
+000148c0: 662e 7469 6c65 286b 6579 2c20 6d75 6c74  f.tile(key, mult
+000148d0: 6970 6c65 7329 2c0a 2020 2020 2020 2020  iples),.        
+000148e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000148f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00014900: 7365 6c66 2e65 7870 616e 645f 6469 6d73  self.expand_dims
+00014910: 2876 616c 6964 5f6c 656e 6774 685f 7665  (valid_length_ve
+00014920: 6374 6f72 2c20 6578 7061 6e64 5f61 7869  ctor, expand_axi
+00014930: 7329 290a 2020 2020 2020 2020 2020 2020  s)).            
+00014940: 2020 2020 6375 7272 656e 745f 7661 6c75      current_valu
+00014950: 6520 3d20 7365 6c66 2e6d 756c 3128 7365  e = self.mul1(se
+00014960: 6c66 2e74 696c 6528 7661 6c75 652c 2028  lf.tile(value, (
+00014970: 312c 2031 2c20 7365 6c66 2e73 6571 5f6c  1, 1, self.seq_l
+00014980: 656e 6774 682c 2031 2929 2c0a 2020 2020  ength, 1)),.    
+00014990: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000149a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000149b0: 2020 2020 2020 7365 6c66 2e65 7870 616e        self.expan
+000149c0: 645f 6469 6d73 2876 616c 6964 5f6c 656e  d_dims(valid_len
+000149d0: 6774 685f 7665 6374 6f72 2c20 3329 290a  gth_vector, 3)).
+000149e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000149f0: 2320 436f 6e63 6174 2074 6865 2070 7265  # Concat the pre
+00014a00: 7669 6f75 7320 7361 7665 6420 7374 6174  vious saved stat
+00014a10: 6520 616e 6420 6375 7272 656e 7420 7374  e and current st
+00014a20: 6174 650a 2020 2020 2020 2020 2020 2020  ate.            
+00014a30: 2020 2020 6b65 7920 3d20 7365 6c66 2e61      key = self.a
+00014a40: 6464 286b 6579 5f70 6173 742c 2063 7572  dd(key_past, cur
+00014a50: 7265 6e74 5f6b 6579 290a 2020 2020 2020  rent_key).      
+00014a60: 2020 2020 2020 2020 2020 7661 6c75 6520            value 
+00014a70: 3d20 7365 6c66 2e61 6464 2876 616c 7565  = self.add(value
+00014a80: 5f70 6173 742c 2063 7572 7265 6e74 5f76  _past, current_v
+00014a90: 616c 7565 290a 2020 2020 2020 2020 2020  alue).          
+00014aa0: 2020 2020 2020 2320 5570 6461 7465 206b        # Update k
+00014ab0: 6579 5f70 7265 7365 6e74 2061 6e64 2076  ey_present and v
+00014ac0: 616c 7565 5f70 7265 7365 6e74 2066 6f72  alue_present for
+00014ad0: 2073 7461 7465 2075 7064 6174 650a 2020   state update.  
+00014ae0: 2020 2020 2020 2020 2020 2020 2020 6b65                ke
+00014af0: 795f 7072 6573 656e 7420 3d20 6b65 790a  y_present = key.
+00014b00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00014b10: 7661 6c75 655f 7072 6573 656e 7420 3d20  value_present = 
+00014b20: 7661 6c75 650a 2020 2020 2020 2020 2020  value.          
+00014b30: 2020 2020 2020 6174 7465 6e74 696f 6e5f        attention_
+00014b40: 6d61 736b 203d 2046 2e72 6573 6861 7065  mask = F.reshape
+00014b50: 2873 656c 662e 6174 7465 6e74 696f 6e5f  (self.attention_
+00014b60: 6d61 736b 2c20 2873 656c 662e 7365 715f  mask, (self.seq_
+00014b70: 6c65 6e67 7468 2c20 7365 6c66 2e73 6571  length, self.seq
+00014b80: 5f6c 656e 6774 682c 2031 2c20 3129 290a  _length, 1, 1)).
+00014b90: 0a20 2020 2020 2020 206c 6179 6572 5f70  .        layer_p
+00014ba0: 7265 7365 6e74 203d 2028 6b65 795f 7072  resent = (key_pr
+00014bb0: 6573 656e 742c 2076 616c 7565 5f70 7265  esent, value_pre
+00014bc0: 7365 6e74 290a 2020 2020 2020 2020 2320  sent).        # 
+00014bd0: 6d75 6c74 6920 6865 6164 2061 7474 656e  multi head atten
+00014be0: 7469 6f6e 2063 6f6e 7369 6465 7269 6e67  tion considering
+00014bf0: 2061 7474 656e 7469 6f6e 206d 6173 6b0a   attention mask.
+00014c00: 2020 2020 2020 2020 2320 7468 6520 7265          # the re
+00014c10: 7475 726e 2073 6861 7065 2069 7320 5b62  turn shape is [b
+00014c20: 7320 2a20 7365 715f 6c65 6e67 7468 2c20  s * seq_length, 
+00014c30: 6869 6464 656e 5f73 697a 655d 0a20 2020  hidden_size].   
+00014c40: 2020 2020 2069 6620 6e6f 7420 7365 6c66       if not self
+00014c50: 2e74 7261 696e 696e 6720 616e 6420 7365  .training and se
+00014c60: 6c66 2e75 7365 5f70 726f 6d70 745f 666c  lf.use_prompt_fl
+00014c70: 6173 685f 6174 7465 6e74 696f 6e3a 0a20  ash_attention:. 
+00014c80: 2020 2020 2020 2020 2020 2069 6620 7365             if se
+00014c90: 6c66 2e75 7365 5f70 6173 7420 616e 6420  lf.use_past and 
+00014ca0: 6e6f 7420 7365 6c66 2e69 735f 6669 7273  not self.is_firs
+00014cb0: 745f 6974 6572 6174 696f 6e3a 0a20 2020  t_iteration:.   
+00014cc0: 2020 2020 2020 2020 2020 2020 2069 6620               if 
+00014cd0: 7365 6c66 2e75 7365 5f69 6e63 7265 5f66  self.use_incre_f
+00014ce0: 6c61 7368 5f61 7474 656e 7469 6f6e 3a0a  lash_attention:.
+00014cf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00014d00: 2020 2020 7175 6572 792c 206b 6579 2c20      query, key, 
+00014d10: 6174 7465 6e74 696f 6e5f 6d61 736b 203d  attention_mask =
+00014d20: 2073 656c 662e 5f70 6661 5f69 6661 5f64   self._pfa_ifa_d
+00014d30: 6174 615f 7072 6570 726f 6365 7373 2871  ata_preprocess(q
+00014d40: 7565 7279 2c20 6b65 792c 2061 7474 656e  uery, key, atten
+00014d50: 7469 6f6e 5f6d 6173 6b2c 0a20 2020 2020  tion_mask,.     
+00014d60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00014d70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00014d80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00014d90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00014da0: 2020 2020 2020 2020 2020 6261 7463 685f            batch_
+00014db0: 7661 6c69 645f 6c65 6e67 7468 290a 2020  valid_length).  
+00014dc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00014dd0: 2020 6174 7465 6e74 696f 6e20 3d20 7365    attention = se
+00014de0: 6c66 2e69 6e63 7265 5f66 6c61 7368 5f61  lf.incre_flash_a
+00014df0: 7474 656e 7469 6f6e 2871 7565 7279 2c20  ttention(query, 
+00014e00: 6b65 792c 2076 616c 7565 2c20 6174 7465  key, value, atte
+00014e10: 6e74 696f 6e5f 6d61 736b 2c0a 2020 2020  ntion_mask,.    
+00014e20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00014e30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00014e40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00014e50: 2020 2020 2020 204e 6f6e 652c 204e 6f6e         None, Non
+00014e60: 652c 204e 6f6e 652c 204e 6f6e 652c 204e  e, None, None, N
+00014e70: 6f6e 652c 204e 6f6e 652c 204e 6f6e 652c  one, None, None,
+00014e80: 204e 6f6e 6529 0a20 2020 2020 2020 2020   None).         
+00014e90: 2020 2020 2020 2020 2020 2061 7474 656e             atten
+00014ea0: 7469 6f6e 203d 2073 656c 662e 5f6d 6572  tion = self._mer
+00014eb0: 6765 5f68 6561 6473 2861 7474 656e 7469  ge_heads(attenti
+00014ec0: 6f6e 290a 2020 2020 2020 2020 2020 2020  on).            
+00014ed0: 2020 2020 656c 7365 3a0a 2020 2020 2020      else:.      
+00014ee0: 2020 2020 2020 2020 2020 2020 2020 6b65                ke
+00014ef0: 7920 3d20 7365 6c66 2e74 7261 6e73 706f  y = self.transpo
+00014f00: 7365 286b 6579 2c20 2830 2c20 312c 2033  se(key, (0, 1, 3
+00014f10: 2c20 3229 290a 2020 2020 2020 2020 2020  , 2)).          
+00014f20: 2020 2020 2020 2020 2020 6174 7465 6e74            attent
+00014f30: 696f 6e20 3d20 7365 6c66 2e5f 6174 746e  ion = self._attn
+00014f40: 2871 7565 7279 2c20 6b65 792c 2076 616c  (query, key, val
+00014f50: 7565 2c20 6174 7465 6e74 696f 6e5f 6d61  ue, attention_ma
+00014f60: 736b 290a 2020 2020 2020 2020 2020 2020  sk).            
+00014f70: 656c 7365 3a0a 2020 2020 2020 2020 2020  else:.          
+00014f80: 2020 2020 2020 7175 6572 792c 206b 6579        query, key
+00014f90: 2c20 6174 7465 6e74 696f 6e5f 6d61 736b  , attention_mask
+00014fa0: 203d 2073 656c 662e 5f70 6661 5f69 6661   = self._pfa_ifa
+00014fb0: 5f64 6174 615f 7072 6570 726f 6365 7373  _data_preprocess
+00014fc0: 2871 7565 7279 2c20 6b65 792c 2061 7474  (query, key, att
+00014fd0: 656e 7469 6f6e 5f6d 6173 6b2c 0a20 2020  ention_mask,.   
+00014fe0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00014ff0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00015000: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00015010: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00015020: 2020 2020 2020 2020 6261 7463 685f 7661          batch_va
+00015030: 6c69 645f 6c65 6e67 7468 290a 2020 2020  lid_length).    
+00015040: 2020 2020 2020 2020 2020 2020 6174 7465              atte
+00015050: 6e74 696f 6e20 3d20 7365 6c66 2e70 726f  ntion = self.pro
+00015060: 6d70 745f 666c 6173 685f 6174 7465 6e74  mpt_flash_attent
+00015070: 696f 6e28 7175 6572 792c 206b 6579 2c20  ion(query, key, 
+00015080: 7661 6c75 652c 2061 7474 656e 7469 6f6e  value, attention
+00015090: 5f6d 6173 6b2c 0a20 2020 2020 2020 2020  _mask,.         
+000150a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000150b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000150c0: 2020 2020 2020 2020 2020 2020 2020 204e                 N
+000150d0: 6f6e 652c 204e 6f6e 652c 204e 6f6e 652c  one, None, None,
+000150e0: 204e 6f6e 652c 204e 6f6e 652c 204e 6f6e   None, None, Non
+000150f0: 652c 204e 6f6e 652c 204e 6f6e 6529 0a20  e, None, None). 
+00015100: 2020 2020 2020 2020 2020 2020 2020 2061                 a
+00015110: 7474 656e 7469 6f6e 203d 2073 656c 662e  ttention = self.
+00015120: 5f6d 6572 6765 5f68 6561 6473 2861 7474  _merge_heads(att
+00015130: 656e 7469 6f6e 290a 2020 2020 2020 2020  ention).        
+00015140: 656c 6966 2073 656c 662e 7573 655f 666c  elif self.use_fl
+00015150: 6173 685f 6174 7465 6e74 696f 6e3a 0a20  ash_attention:. 
+00015160: 2020 2020 2020 2020 2020 2061 7474 656e             atten
+00015170: 7469 6f6e 5f6d 6173 6b20 3d20 7365 6c66  tion_mask = self
+00015180: 2e63 6173 7428 6174 7465 6e74 696f 6e5f  .cast(attention_
+00015190: 6d61 736b 2c20 6d73 7479 7065 2e75 696e  mask, mstype.uin
+000151a0: 7438 290a 2020 2020 2020 2020 2020 2020  t8).            
+000151b0: 6174 7465 6e74 696f 6e20 3d20 7365 6c66  attention = self
+000151c0: 2e5f 666c 6173 685f 6174 746e 2871 7565  ._flash_attn(que
+000151d0: 7279 2c20 6b65 792c 2076 616c 7565 2c20  ry, key, value, 
+000151e0: 6174 7465 6e74 696f 6e5f 6d61 736b 290a  attention_mask).
+000151f0: 2020 2020 2020 2020 656c 7365 3a0a 2020          else:.  
+00015200: 2020 2020 2020 2020 2020 6174 7465 6e74            attent
+00015210: 696f 6e20 3d20 7365 6c66 2e5f 6174 746e  ion = self._attn
+00015220: 2871 7565 7279 2c20 6b65 792c 2076 616c  (query, key, val
+00015230: 7565 2c20 6174 7465 6e74 696f 6e5f 6d61  ue, attention_ma
+00015240: 736b 290a 2020 2020 2020 2020 2320 4f75  sk).        # Ou
+00015250: 7470 7574 0a20 2020 2020 2020 206f 7574  tput.        out
+00015260: 7075 7420 3d20 7365 6c66 2e70 726f 6a65  put = self.proje
+00015270: 6374 696f 6e28 6174 7465 6e74 696f 6e29  ction(attention)
+00015280: 0a20 2020 2020 2020 206f 7574 7075 7420  .        output 
+00015290: 3d20 7365 6c66 2e64 726f 706f 7574 286f  = self.dropout(o
+000152a0: 7574 7075 7429 0a20 2020 2020 2020 206f  utput).        o
+000152b0: 7574 7075 7420 3d20 462e 7265 7368 6170  utput = F.reshap
+000152c0: 6528 6f75 7470 7574 2c20 6f72 695f 7368  e(output, ori_sh
+000152d0: 6170 6529 0a20 2020 2020 2020 206f 7574  ape).        out
+000152e0: 7075 7420 3d20 462e 6361 7374 286f 7574  put = F.cast(out
+000152f0: 7075 742c 206f 7269 5f64 7479 7065 290a  put, ori_dtype).
+00015300: 2020 2020 2020 2020 7265 7475 726e 206f          return o
+00015310: 7574 7075 742c 206c 6179 6572 5f70 7265  utput, layer_pre
+00015320: 7365 6e74 0a0a 2020 2020 6465 6620 5f67  sent..    def _g
+00015330: 6574 5f62 6174 6368 5f73 697a 655f 6672  et_batch_size_fr
+00015340: 6f6d 5f71 7565 7279 2873 656c 662c 2071  om_query(self, q
+00015350: 7565 7279 293a 0a20 2020 2020 2020 2072  uery):.        r
+00015360: 2222 2247 6574 2074 6865 2062 6174 6368  """Get the batch
+00015370: 2073 697a 6520 6672 6f6d 2071 7565 7279   size from query
+00015380: 2074 656e 736f 7222 2222 0a20 2020 2020   tensor""".     
+00015390: 2020 2023 2046 6f72 2074 6865 2069 6e63     # For the inc
+000153a0: 7265 6d65 6e74 616c 2070 7265 6469 6374  remental predict
+000153b0: 696f 6e2c 2074 6865 2073 6571 206c 656e  ion, the seq len
+000153c0: 6774 6820 666f 7220 7468 6520 696e 7075  gth for the inpu
+000153d0: 7420 6973 2031 2e0a 2020 2020 2020 2020  t is 1..        
+000153e0: 6966 206c 656e 2846 2e73 6861 7065 2871  if len(F.shape(q
+000153f0: 7565 7279 2929 203d 3d20 3220 616e 6420  uery)) == 2 and 
+00015400: 2828 7365 6c66 2e75 7365 5f70 6173 7420  ((self.use_past 
+00015410: 616e 6420 7365 6c66 2e69 735f 6669 7273  and self.is_firs
+00015420: 745f 6974 6572 6174 696f 6e29 206f 7220  t_iteration) or 
+00015430: 286e 6f74 2073 656c 662e 7573 655f 7061  (not self.use_pa
+00015440: 7374 2929 3a0a 2020 2020 2020 2020 2020  st)):.          
+00015450: 2020 7265 7475 726e 2046 2e73 6861 7065    return F.shape
+00015460: 2871 7565 7279 295b 305d 202f 2f20 7365  (query)[0] // se
+00015470: 6c66 2e73 7263 5f73 6571 5f6c 656e 6774  lf.src_seq_lengt
+00015480: 680a 2020 2020 2020 2020 7265 7475 726e  h.        return
+00015490: 2046 2e73 6861 7065 2871 7565 7279 295b   F.shape(query)[
+000154a0: 305d 0a0a 2020 2020 6465 6620 5f67 6574  0]..    def _get
+000154b0: 5f73 6571 5f6c 656e 6774 685f 756e 6465  _seq_length_unde
+000154c0: 725f 696e 6372 656d 656e 7461 6c28 7365  r_incremental(se
+000154d0: 6c66 2c20 6c65 6e67 7468 293a 0a20 2020  lf, length):.   
+000154e0: 2020 2020 2072 2222 2252 6574 7572 6e20       r"""Return 
+000154f0: 7468 6520 6c65 6e67 7468 206f 6620 7468  the length of th
+00015500: 6520 7465 6e73 6f72 2e0a 2020 2020 2020  e tensor..      
+00015510: 2020 2020 2020 466f 7220 7468 6520 696e        For the in
+00015520: 6372 656d 656e 7461 6c20 7072 6564 6963  cremental predic
+00015530: 7469 6f6e 2c20 7468 6520 7365 7120 6c65  tion, the seq le
+00015540: 6e67 7468 2066 6f72 2074 6865 2069 6e70  ngth for the inp
+00015550: 7574 2069 7320 312e 0a20 2020 2020 2020  ut is 1..       
+00015560: 2022 2222 0a20 2020 2020 2020 2069 6620   """.        if 
+00015570: 7365 6c66 2e75 7365 5f70 6173 7420 616e  self.use_past an
+00015580: 6420 6e6f 7420 7365 6c66 2e69 735f 6669  d not self.is_fi
+00015590: 7273 745f 6974 6572 6174 696f 6e3a 0a20  rst_iteration:. 
+000155a0: 2020 2020 2020 2020 2020 2072 6574 7572             retur
+000155b0: 6e20 310a 2020 2020 2020 2020 7265 7475  n 1.        retu
+000155c0: 726e 206c 656e 6774 680a 0a20 2020 2064  rn length..    d
+000155d0: 6566 205f 7066 615f 6966 615f 6461 7461  ef _pfa_ifa_data
+000155e0: 5f70 7265 7072 6f63 6573 7328 7365 6c66  _preprocess(self
+000155f0: 2c20 7175 6572 792c 206b 6579 2c20 6174  , query, key, at
+00015600: 7465 6e74 696f 6e5f 6d61 736b 2c20 6261  tention_mask, ba
+00015610: 7463 685f 7661 6c69 645f 6c65 6e67 7468  tch_valid_length
+00015620: 293a 0a20 2020 2020 2020 2072 2222 2252  ):.        r"""R
+00015630: 6574 7572 6e20 7072 6f63 6573 7365 6420  eturn processed 
+00015640: 712c 206b 2061 6e64 2061 7474 656e 7469  q, k and attenti
+00015650: 6f6e 206d 6173 6b22 2222 0a20 2020 2020  on mask""".     
+00015660: 2020 2069 6620 7365 6c66 2e75 7365 5f70     if self.use_p
+00015670: 6173 7420 616e 6420 6e6f 7420 7365 6c66  ast and not self
+00015680: 2e69 735f 6669 7273 745f 6974 6572 6174  .is_first_iterat
+00015690: 696f 6e3a 0a20 2020 2020 2020 2020 2020  ion:.           
+000156a0: 2023 2047 6574 2074 6865 2070 7265 6369   # Get the preci
+000156b0: 7365 2070 6f73 6974 696f 6e20 696e 6465  se position inde
+000156c0: 780a 2020 2020 2020 2020 2020 2020 6375  x.            cu
+000156d0: 7272 656e 745f 696e 6465 7820 3d20 6261  rrent_index = ba
+000156e0: 7463 685f 7661 6c69 645f 6c65 6e67 7468  tch_valid_length
+000156f0: 2e73 7175 6565 7a65 2830 290a 2020 2020  .squeeze(0).    
+00015700: 2020 2020 2020 2020 696e 6465 7820 3d20          index = 
+00015710: 7365 6c66 2e73 7562 3128 462e 6361 7374  self.sub1(F.cast
+00015720: 2863 7572 7265 6e74 5f69 6e64 6578 2c20  (current_index, 
+00015730: 6d73 7479 7065 2e69 6e74 3332 292c 2031  mstype.int32), 1
+00015740: 290a 2020 2020 2020 2020 2020 2020 696e  ).            in
+00015750: 6465 7820 3d20 462e 7265 7368 6170 6528  dex = F.reshape(
+00015760: 696e 6465 782c 2028 2d31 2c20 312c 2031  index, (-1, 1, 1
+00015770: 2929 0a20 2020 2020 2020 2020 2020 2023  )).            #
+00015780: 2043 616c 6375 6c61 7465 2074 6865 2061   Calculate the a
+00015790: 7474 656e 7469 6f6e 5f6d 6173 6b20 6d61  ttention_mask ma
+000157a0: 7472 6978 2076 6961 2074 6865 2070 6f73  trix via the pos
+000157b0: 6974 696f 6e20 696e 6465 780a 2020 2020  ition index.    
+000157c0: 2020 2020 2020 2020 6174 7465 6e74 696f          attentio
+000157d0: 6e5f 6d61 736b 203d 2046 2e63 6173 7428  n_mask = F.cast(
+000157e0: 7365 6c66 2e74 656e 736f 725f 6c65 2873  self.tensor_le(s
+000157f0: 656c 662e 7261 6e67 652c 2069 6e64 6578  elf.range, index
+00015800: 292c 2050 2e44 5479 7065 2829 2871 7565  ), P.DType()(que
+00015810: 7279 2929 0a20 2020 2020 2020 2020 2020  ry)).           
+00015820: 2061 7474 656e 7469 6f6e 5f6d 6173 6b20   attention_mask 
+00015830: 3d20 7365 6c66 2e65 7870 616e 645f 6469  = self.expand_di
+00015840: 6d73 2861 7474 656e 7469 6f6e 5f6d 6173  ms(attention_mas
+00015850: 6b2c 2032 290a 0a20 2020 2020 2020 2061  k, 2)..        a
+00015860: 7474 656e 7469 6f6e 5f6d 6173 6b20 3d20  ttention_mask = 
+00015870: 7365 6c66 2e73 7562 5f70 6661 280a 2020  self.sub_pfa(.  
+00015880: 2020 2020 2020 2020 2020 502e 4361 7374            P.Cast
+00015890: 2829 2873 656c 662e 6f6e 652c 2050 2e44  ()(self.one, P.D
+000158a0: 5479 7065 2829 2871 7565 7279 2929 2c0a  Type()(query)),.
+000158b0: 2020 2020 2020 2020 2020 2020 502e 4361              P.Ca
+000158c0: 7374 2829 2861 7474 656e 7469 6f6e 5f6d  st()(attention_m
+000158d0: 6173 6b2c 2050 2e44 5479 7065 2829 2871  ask, P.DType()(q
+000158e0: 7565 7279 2929 290a 0a20 2020 2020 2020  uery)))..       
+000158f0: 2072 6574 7572 6e20 7175 6572 792c 206b   return query, k
+00015900: 6579 2c20 6174 7465 6e74 696f 6e5f 6d61  ey, attention_ma
+00015910: 736b 0a0a 2020 2020 6465 6620 5f63 6865  sk..    def _che
+00015920: 636b 5f69 6e70 7574 7328 7365 6c66 2c20  ck_inputs(self, 
+00015930: 7175 6572 795f 7465 6e73 6f72 2c20 6b65  query_tensor, ke
+00015940: 795f 7465 6e73 6f72 2c20 7661 6c75 655f  y_tensor, value_
+00015950: 7465 6e73 6f72 2c20 6174 7465 6e74 696f  tensor, attentio
+00015960: 6e5f 6d61 736b 2c20 6b65 795f 7061 7374  n_mask, key_past
+00015970: 3d4e 6f6e 652c 0a20 2020 2020 2020 2020  =None,.         
+00015980: 2020 2020 2020 2020 2020 2020 2076 616c               val
+00015990: 7565 5f70 6173 743d 4e6f 6e65 2c20 6261  ue_past=None, ba
+000159a0: 7463 685f 7661 6c69 645f 6c65 6e67 7468  tch_valid_length
+000159b0: 3d4e 6f6e 6529 3a0a 2020 2020 2020 2020  =None):.        
+000159c0: 7222 2222 4368 6563 6b20 696e 7075 7473  r"""Check inputs
+000159d0: 2222 220a 2020 2020 2020 2020 5f63 6865  """.        _che
+000159e0: 636b 5f69 6e70 7574 5f64 7479 7065 2846  ck_input_dtype(F
+000159f0: 2e64 7479 7065 2871 7565 7279 5f74 656e  .dtype(query_ten
+00015a00: 736f 7229 2c20 2271 7565 7279 5f74 656e  sor), "query_ten
+00015a10: 736f 7222 2c0a 2020 2020 2020 2020 2020  sor",.          
+00015a20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00015a30: 205b 6d73 7479 7065 2e66 6c6f 6174 3332   [mstype.float32
+00015a40: 2c20 6d73 7479 7065 2e66 6c6f 6174 3136  , mstype.float16
+00015a50: 2c20 6d73 7479 7065 2e62 666c 6f61 7431  , mstype.bfloat1
+00015a60: 365d 2c20 7365 6c66 2e63 6c73 5f6e 616d  6], self.cls_nam
+00015a70: 6529 0a20 2020 2020 2020 205f 6368 6563  e).        _chec
+00015a80: 6b5f 696e 7075 745f 6474 7970 6528 462e  k_input_dtype(F.
+00015a90: 6474 7970 6528 6b65 795f 7465 6e73 6f72  dtype(key_tensor
+00015aa0: 292c 2022 6b65 795f 7465 6e73 6f72 222c  ), "key_tensor",
+00015ab0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00015ac0: 2020 2020 2020 2020 2020 2020 5b6d 7374              [mst
+00015ad0: 7970 652e 666c 6f61 7433 322c 206d 7374  ype.float32, mst
+00015ae0: 7970 652e 666c 6f61 7431 362c 206d 7374  ype.float16, mst
+00015af0: 7970 652e 6266 6c6f 6174 3136 5d2c 2073  ype.bfloat16], s
+00015b00: 656c 662e 636c 735f 6e61 6d65 290a 2020  elf.cls_name).  
+00015b10: 2020 2020 2020 5f63 6865 636b 5f69 6e70        _check_inp
+00015b20: 7574 5f64 7479 7065 2846 2e64 7479 7065  ut_dtype(F.dtype
+00015b30: 2876 616c 7565 5f74 656e 736f 7229 2c20  (value_tensor), 
+00015b40: 2276 616c 7565 5f74 656e 736f 7222 2c0a  "value_tensor",.
+00015b50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00015b60: 2020 2020 2020 2020 2020 205b 6d73 7479             [msty
+00015b70: 7065 2e66 6c6f 6174 3332 2c20 6d73 7479  pe.float32, msty
+00015b80: 7065 2e66 6c6f 6174 3136 2c20 6d73 7479  pe.float16, msty
+00015b90: 7065 2e62 666c 6f61 7431 365d 2c20 7365  pe.bfloat16], se
+00015ba0: 6c66 2e63 6c73 5f6e 616d 6529 0a20 2020  lf.cls_name).   
+00015bb0: 2020 2020 2069 6620 6174 7465 6e74 696f       if attentio
+00015bc0: 6e5f 6d61 736b 2069 7320 6e6f 7420 4e6f  n_mask is not No
+00015bd0: 6e65 3a0a 2020 2020 2020 2020 2020 2020  ne:.            
+00015be0: 5f63 6865 636b 5f69 6e70 7574 5f64 7479  _check_input_dty
+00015bf0: 7065 2846 2e64 7479 7065 2861 7474 656e  pe(F.dtype(atten
+00015c00: 7469 6f6e 5f6d 6173 6b29 2c20 2261 7474  tion_mask), "att
+00015c10: 656e 7469 6f6e 5f6d 6173 6b22 2c0a 2020  ention_mask",.  
+00015c20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00015c30: 2020 2020 2020 2020 2020 2020 205b 6d73               [ms
+00015c40: 7479 7065 2e66 6c6f 6174 3332 2c20 6d73  type.float32, ms
+00015c50: 7479 7065 2e66 6c6f 6174 3136 2c20 6d73  type.float16, ms
+00015c60: 7479 7065 2e62 666c 6f61 7431 365d 2c0a  type.bfloat16],.
+00015c70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00015c80: 2020 2020 2020 2020 2020 2020 2020 2073                 s
+00015c90: 656c 662e 636c 735f 6e61 6d65 290a 0a20  elf.cls_name).. 
+00015ca0: 2020 2020 2020 2062 6174 6368 5f76 616c         batch_val
+00015cb0: 6964 5f6c 656e 6774 685f 6973 5f74 656e  id_length_is_ten
+00015cc0: 736f 7220 3d20 6973 696e 7374 616e 6365  sor = isinstance
+00015cd0: 2862 6174 6368 5f76 616c 6964 5f6c 656e  (batch_valid_len
+00015ce0: 6774 682c 2054 656e 736f 7229 0a20 2020  gth, Tensor).   
+00015cf0: 2020 2020 2062 6174 6368 5f69 735f 6465       batch_is_de
+00015d00: 6661 756c 7420 3d20 6261 7463 685f 7661  fault = batch_va
+00015d10: 6c69 645f 6c65 6e67 7468 2069 7320 4e6f  lid_length is No
+00015d20: 6e65 0a20 2020 2020 2020 205f 6368 6563  ne.        _chec
+00015d30: 6b5f 7061 7374 5f6e 6f6e 655f 696e 7075  k_past_none_inpu
+00015d40: 745f 6e6f 6e65 2873 656c 662e 7573 655f  t_none(self.use_
+00015d50: 7061 7374 2c20 2262 6174 6368 5f76 616c  past, "batch_val
+00015d60: 6964 5f6c 656e 6774 6822 2c20 7365 6c66  id_length", self
+00015d70: 2e63 6c73 5f6e 616d 652c 204e 6f6e 652c  .cls_name, None,
+00015d80: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00015d90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00015da0: 2020 2020 2062 6174 6368 5f76 616c 6964       batch_valid
+00015db0: 5f6c 656e 6774 685f 6973 5f74 656e 736f  _length_is_tenso
+00015dc0: 722c 2062 6174 6368 5f69 735f 6465 6661  r, batch_is_defa
+00015dd0: 756c 7429 0a20 2020 2020 2020 2069 6620  ult).        if 
+00015de0: 7365 6c66 2e75 7365 5f70 6173 743a 0a20  self.use_past:. 
+00015df0: 2020 2020 2020 2020 2020 205f 6368 6563             _chec
+00015e00: 6b5f 696e 7075 745f 6474 7970 6528 462e  k_input_dtype(F.
+00015e10: 6474 7970 6528 6b65 795f 7061 7374 292c  dtype(key_past),
+00015e20: 2022 6b65 795f 7061 7374 222c 205b 6d73   "key_past", [ms
+00015e30: 7479 7065 2e66 6c6f 6174 3136 2c20 6d73  type.float16, ms
+00015e40: 7479 7065 2e62 666c 6f61 7431 365d 2c20  type.bfloat16], 
+00015e50: 7365 6c66 2e63 6c73 5f6e 616d 6529 0a20  self.cls_name). 
+00015e60: 2020 2020 2020 2020 2020 205f 6368 6563             _chec
+00015e70: 6b5f 696e 7075 745f 6474 7970 6528 462e  k_input_dtype(F.
+00015e80: 6474 7970 6528 7661 6c75 655f 7061 7374  dtype(value_past
+00015e90: 292c 2022 7661 6c75 655f 7061 7374 222c  ), "value_past",
+00015ea0: 205b 6d73 7479 7065 2e66 6c6f 6174 3136   [mstype.float16
+00015eb0: 2c20 6d73 7479 7065 2e62 666c 6f61 7431  , mstype.bfloat1
+00015ec0: 365d 2c20 7365 6c66 2e63 6c73 5f6e 616d  6], self.cls_nam
+00015ed0: 6529 0a20 2020 2020 2020 2020 2020 205f  e).            _
+00015ee0: 6368 6563 6b5f 696e 7075 745f 6474 7970  check_input_dtyp
+00015ef0: 6528 462e 6474 7970 6528 6261 7463 685f  e(F.dtype(batch_
+00015f00: 7661 6c69 645f 6c65 6e67 7468 292c 2022  valid_length), "
+00015f10: 6261 7463 685f 7661 6c69 645f 6c65 6e67  batch_valid_leng
+00015f20: 7468 222c 205b 6d73 7479 7065 2e69 6e74  th", [mstype.int
+00015f30: 3332 5d2c 2073 656c 662e 636c 735f 6e61  32], self.cls_na
+00015f40: 6d65 290a 2020 2020 2020 2020 7265 7475  me).        retu
+00015f50: 726e 2054 7275 650a 0a20 2020 2064 6566  rn True..    def
+00015f60: 205f 636f 6e76 6572 745f 746f 5f32 645f   _convert_to_2d_
+00015f70: 7465 6e73 6f72 2873 656c 662c 2071 7565  tensor(self, que
+00015f80: 7279 5f74 656e 736f 722c 206b 6579 5f74  ry_tensor, key_t
+00015f90: 656e 736f 722c 2076 616c 7565 5f74 656e  ensor, value_ten
+00015fa0: 736f 7229 3a0a 2020 2020 2020 2020 2222  sor):.        ""
+00015fb0: 2263 6f6e 7665 7274 2061 206e 6420 7465  "convert a nd te
+00015fc0: 6e73 6f72 2074 6f20 6120 3264 2074 656e  nsor to a 2d ten
+00015fd0: 736f 7222 2222 0a20 2020 2020 2020 2071  sor""".        q
+00015fe0: 7565 7279 5f73 6861 7065 203d 2046 2e73  uery_shape = F.s
+00015ff0: 6861 7065 2871 7565 7279 5f74 656e 736f  hape(query_tenso
+00016000: 7229 0a20 2020 2020 2020 2071 7565 7279  r).        query
+00016010: 5f74 656e 736f 7220 3d20 462e 7265 7368  _tensor = F.resh
+00016020: 6170 6528 7175 6572 795f 7465 6e73 6f72  ape(query_tensor
+00016030: 2c20 282d 312c 2071 7565 7279 5f73 6861  , (-1, query_sha
+00016040: 7065 5b2d 315d 2929 0a20 2020 2020 2020  pe[-1])).       
+00016050: 206b 6579 5f73 6861 7065 203d 2046 2e73   key_shape = F.s
+00016060: 6861 7065 286b 6579 5f74 656e 736f 7229  hape(key_tensor)
+00016070: 0a20 2020 2020 2020 206b 6579 5f74 656e  .        key_ten
+00016080: 736f 7220 3d20 462e 7265 7368 6170 6528  sor = F.reshape(
+00016090: 6b65 795f 7465 6e73 6f72 2c20 282d 312c  key_tensor, (-1,
+000160a0: 206b 6579 5f73 6861 7065 5b2d 315d 2929   key_shape[-1]))
+000160b0: 0a20 2020 2020 2020 2076 616c 7565 5f73  .        value_s
+000160c0: 6861 7065 203d 2046 2e73 6861 7065 2876  hape = F.shape(v
+000160d0: 616c 7565 5f74 656e 736f 7229 0a20 2020  alue_tensor).   
+000160e0: 2020 2020 2076 616c 7565 5f74 656e 736f       value_tenso
+000160f0: 7220 3d20 462e 7265 7368 6170 6528 7661  r = F.reshape(va
+00016100: 6c75 655f 7465 6e73 6f72 2c20 282d 312c  lue_tensor, (-1,
+00016110: 2076 616c 7565 5f73 6861 7065 5b2d 315d   value_shape[-1]
+00016120: 2929 0a0a 2020 2020 2020 2020 7265 7475  ))..        retu
+00016130: 726e 2071 7565 7279 5f74 656e 736f 722c  rn query_tensor,
+00016140: 206b 6579 5f74 656e 736f 722c 2076 616c   key_tensor, val
+00016150: 7565 5f74 656e 736f 720a 0a20 2020 2064  ue_tensor..    d
+00016160: 6566 205f 6d65 7267 655f 6865 6164 7328  ef _merge_heads(
+00016170: 7365 6c66 2c20 7829 3a0a 2020 2020 2020  self, x):.      
+00016180: 2020 2222 220a 2020 2020 2020 2020 636f    """.        co
+00016190: 6e76 6572 7420 6120 3464 2069 6e70 7574  nvert a 4d input
+000161a0: 2074 6f20 6120 3264 206f 7574 7075 740a   to a 2d output.
+000161b0: 0a20 2020 2020 2020 2049 6e70 7574 733a  .        Inputs:
+000161c0: 0a20 2020 2020 2020 2020 2020 2078 3a20  .            x: 
+000161d0: 696e 7075 7420 7465 6e73 6f72 0a0a 2020  input tensor..  
+000161e0: 2020 2020 2020 4f75 7470 7574 3a0a 2020        Output:.  
+000161f0: 2020 2020 2020 2020 2020 785f 6d65 7267            x_merg
+00016200: 653a 2074 6865 2032 6420 6f75 7470 7574  e: the 2d output
+00016210: 0a20 2020 2020 2020 2022 2222 0a20 2020  .        """.   
+00016220: 2020 2020 2078 203d 2073 656c 662e 6d65       x = self.me
+00016230: 7267 6572 5f68 6561 645f 7472 616e 7370  rger_head_transp
+00016240: 6f73 6528 0a20 2020 2020 2020 2020 2020  ose(.           
+00016250: 2078 2c20 2830 2c20 322c 2031 2c20 3329   x, (0, 2, 1, 3)
+00016260: 2920 2023 2062 732c 2073 6571 5f6c 656e  )  # bs, seq_len
+00016270: 6774 682c 2068 6561 642c 2073 697a 655f  gth, head, size_
+00016280: 7065 725f 6865 6164 0a20 2020 2020 2020  per_head.       
+00016290: 2078 5f73 6861 7065 203d 2050 2e53 6861   x_shape = P.Sha
+000162a0: 7065 2829 2878 290a 2020 2020 2020 2020  pe()(x).        
+000162b0: 6e65 775f 7368 6170 6520 3d20 282d 312c  new_shape = (-1,
+000162c0: 2078 5f73 6861 7065 5b2d 325d 202a 2078   x_shape[-2] * x
+000162d0: 5f73 6861 7065 5b2d 315d 290a 2020 2020  _shape[-1]).    
+000162e0: 2020 2020 785f 6d65 7267 6520 3d20 7365      x_merge = se
+000162f0: 6c66 2e72 6573 6861 7065 2878 2c20 6e65  lf.reshape(x, ne
+00016300: 775f 7368 6170 6529 0a20 2020 2020 2020  w_shape).       
+00016310: 2072 6574 7572 6e20 785f 6d65 7267 650a   return x_merge.
+00016320: 0a20 2020 2064 6566 205f 736f 6674 6d61  .    def _softma
+00016330: 7828 7365 6c66 2c20 6174 7465 6e74 696f  x(self, attentio
+00016340: 6e5f 7363 6f72 6573 293a 0a20 2020 2020  n_scores):.     
+00016350: 2020 2022 2222 0a20 2020 2020 2020 2046     """.        F
+00016360: 6f72 2074 6865 2063 6f6e 7369 6465 7261  or the considera
+00016370: 7469 6f6e 206f 6620 7468 6520 7065 7266  tion of the perf
+00016380: 6f72 6d61 6e63 652c 2064 6f20 736f 6674  ormance, do soft
+00016390: 6d61 7820 6163 636f 7264 696e 6720 746f  max according to
+000163a0: 2064 6966 6665 7265 6e74 2073 6974 7561   different situa
+000163b0: 7469 6f6e 730a 2020 2020 2020 2020 3a70  tions.        :p
+000163c0: 6172 616d 2061 7474 656e 7469 6f6e 5f73  aram attention_s
+000163d0: 636f 7265 733a 2061 2033 6420 7465 6e73  cores: a 3d tens
+000163e0: 6f72 2062 6566 6f72 6520 736f 6674 6d61  or before softma
+000163f0: 780a 2020 2020 2020 2020 3a72 6574 7572  x.        :retur
+00016400: 6e3a 2074 6865 2061 7474 656e 7469 6f6e  n: the attention
+00016410: 2073 636f 7265 732e 0a20 2020 2020 2020   scores..       
+00016420: 2022 2222 0a0a 2020 2020 2020 2020 6966   """..        if
+00016430: 2073 656c 662e 5f69 735f 6173 6365 6e64   self._is_ascend
+00016440: 2061 6e64 2073 656c 662e 736f 6674 6d61   and self.softma
+00016450: 785f 6474 7970 6520 3d3d 206d 7374 7970  x_dtype == mstyp
+00016460: 652e 666c 6f61 7431 3620 6f72 206e 6f74  e.float16 or not
+00016470: 2073 656c 662e 5f69 735f 6173 6365 6e64   self._is_ascend
+00016480: 3a0a 2020 2020 2020 2020 2020 2020 6174  :.            at
+00016490: 7465 6e74 696f 6e5f 7072 6f62 7320 3d20  tention_probs = 
+000164a0: 7365 6c66 2e73 6f66 746d 6178 2861 7474  self.softmax(att
+000164b0: 656e 7469 6f6e 5f73 636f 7265 7329 0a20  ention_scores). 
+000164c0: 2020 2020 2020 2065 6c73 653a 0a20 2020         else:.   
+000164d0: 2020 2020 2020 2020 2073 6861 7065 203d           shape =
+000164e0: 2046 2e73 6861 7065 2861 7474 656e 7469   F.shape(attenti
+000164f0: 6f6e 5f73 636f 7265 7329 0a20 2020 2020  on_scores).     
+00016500: 2020 2020 2020 2023 2061 7474 656e 7469         # attenti
+00016510: 6f6e 2070 726f 6273 0a20 2020 2020 2020  on probs.       
+00016520: 2020 2020 2061 7474 656e 7469 6f6e 5f70       attention_p
+00016530: 726f 6273 203d 2073 656c 662e 736f 6674  robs = self.soft
+00016540: 6d61 785f 3364 280a 2020 2020 2020 2020  max_3d(.        
+00016550: 2020 2020 2020 2020 7365 6c66 2e73 6f66          self.sof
+00016560: 746d 6178 5f72 6573 6861 7065 2861 7474  tmax_reshape(att
+00016570: 656e 7469 6f6e 5f73 636f 7265 732c 2028  ention_scores, (
+00016580: 7368 6170 655b 305d 2c20 2d31 2c20 7368  shape[0], -1, sh
+00016590: 6170 655b 2d31 5d29 2929 0a20 2020 2020  ape[-1]))).     
+000165a0: 2020 2020 2020 2061 7474 656e 7469 6f6e         attention
+000165b0: 5f70 726f 6273 203d 2073 656c 662e 736f  _probs = self.so
+000165c0: 6674 6d61 785f 7265 7368 6170 6528 6174  ftmax_reshape(at
+000165d0: 7465 6e74 696f 6e5f 7072 6f62 732c 2073  tention_probs, s
+000165e0: 6861 7065 290a 2020 2020 2020 2020 7265  hape).        re
+000165f0: 7475 726e 2061 7474 656e 7469 6f6e 5f70  turn attention_p
+00016600: 726f 6273 0a0a 2020 2020 6465 6620 5f66  robs..    def _f
+00016610: 6c61 7368 5f61 7474 6e28 7365 6c66 2c20  lash_attn(self, 
+00016620: 7175 6572 792c 206b 6579 2c20 7661 6c75  query, key, valu
+00016630: 652c 2061 7474 656e 7469 6f6e 5f6d 6173  e, attention_mas
+00016640: 6b29 3a0a 2020 2020 2020 2020 2222 220a  k):.        """.
+00016650: 2020 2020 2020 2020 666c 6173 6820 6174          flash at
+00016660: 7465 6e74 696f 6e0a 2020 2020 2020 2020  tention.        
+00016670: 2222 220a 2020 2020 2020 2020 6966 2061  """.        if a
+00016680: 7474 656e 7469 6f6e 5f6d 6173 6b20 6973  ttention_mask is
+00016690: 206e 6f74 204e 6f6e 653a 0a20 2020 2020   not None:.     
+000166a0: 2020 2020 2020 2061 7474 656e 7469 6f6e         attention
+000166b0: 5f6d 6173 6b5f 6474 7970 6520 3d20 6368  _mask_dtype = ch
+000166c0: 6f6f 7365 5f66 6c61 7368 5f61 7474 656e  oose_flash_atten
+000166d0: 7469 6f6e 5f64 7479 7065 2829 0a20 2020  tion_dtype().   
+000166e0: 2020 2020 2020 2020 2061 7474 656e 7469           attenti
+000166f0: 6f6e 5f6d 6173 6b20 3d20 7365 6c66 2e73  on_mask = self.s
+00016700: 7562 280a 2020 2020 2020 2020 2020 2020  ub(.            
+00016710: 2020 2020 502e 4361 7374 2829 2873 656c      P.Cast()(sel
+00016720: 662e 6f6e 652c 2061 7474 656e 7469 6f6e  f.one, attention
+00016730: 5f6d 6173 6b5f 6474 7970 6529 2c0a 2020  _mask_dtype),.  
+00016740: 2020 2020 2020 2020 2020 2020 2020 502e                P.
+00016750: 4361 7374 2829 2861 7474 656e 7469 6f6e  Cast()(attention
+00016760: 5f6d 6173 6b2c 2061 7474 656e 7469 6f6e  _mask, attention
+00016770: 5f6d 6173 6b5f 6474 7970 6529 290a 0a20  _mask_dtype)).. 
+00016780: 2020 2020 2020 2077 6569 6768 7465 645f         weighted_
+00016790: 7661 6c75 6573 203d 2073 656c 662e 666c  values = self.fl
+000167a0: 6173 685f 6174 7465 6e74 696f 6e28 7175  ash_attention(qu
+000167b0: 6572 792c 206b 6579 2c20 7661 6c75 652c  ery, key, value,
+000167c0: 2061 7474 656e 7469 6f6e 5f6d 6173 6b29   attention_mask)
+000167d0: 0a20 2020 2020 2020 2061 7474 656e 7469  .        attenti
+000167e0: 6f6e 5f6d 6572 6765 203d 2073 656c 662e  on_merge = self.
+000167f0: 5f6d 6572 6765 5f68 6561 6473 2877 6569  _merge_heads(wei
+00016800: 6768 7465 645f 7661 6c75 6573 290a 2020  ghted_values).  
+00016810: 2020 2020 2020 7265 7475 726e 2061 7474        return att
+00016820: 656e 7469 6f6e 5f6d 6572 6765 0a0a 2020  ention_merge..  
+00016830: 2020 6465 6620 5f61 7474 6e28 7365 6c66    def _attn(self
+00016840: 2c20 7175 6572 792c 206b 6579 2c20 7661  , query, key, va
+00016850: 6c75 652c 2061 7474 656e 7469 6f6e 5f6d  lue, attention_m
+00016860: 6173 6b29 3a0a 2020 2020 2020 2020 2222  ask):.        ""
+00016870: 220a 2020 2020 2020 2020 4765 7420 7468  ".        Get th
+00016880: 6520 7765 6967 6874 6564 2073 636f 7265  e weighted score
+00016890: 2061 6c6f 6e67 2074 6865 2073 6571 5f6c   along the seq_l
+000168a0: 656e 6774 680a 0a20 2020 2020 2020 2049  ength..        I
+000168b0: 6e70 7574 733a 0a20 2020 2020 2020 2020  nputs:.         
+000168c0: 2020 2071 7565 7279 3a20 7468 6520 7175     query: the qu
+000168d0: 6572 7920 6d61 7472 6978 0a20 2020 2020  ery matrix.     
+000168e0: 2020 2020 2020 206b 6579 3a20 7468 6520         key: the 
+000168f0: 6b65 7920 6d61 7472 6978 0a20 2020 2020  key matrix.     
+00016900: 2020 2020 2020 2076 616c 7565 3a20 7468         value: th
+00016910: 6520 7661 6c75 6520 6d61 7472 6978 0a20  e value matrix. 
+00016920: 2020 2020 2020 2020 2020 2061 7474 656e             atten
+00016930: 7469 6f6e 5f6d 6173 6b3a 2074 6865 2061  tion_mask: the a
+00016940: 7474 656e 7469 6f6e 206d 6173 6b20 6d61  ttention mask ma
+00016950: 7472 6978 2077 6974 6820 7368 6170 6520  trix with shape 
+00016960: 2862 6174 6368 5f73 697a 652c 0a20 2020  (batch_size,.   
+00016970: 2020 2020 2020 2020 2031 2c20 7365 715f           1, seq_
+00016980: 6c65 6e67 7468 2c20 7365 715f 6c65 6e67  length, seq_leng
+00016990: 7468 290a 2020 2020 2020 2020 4f75 7470  th).        Outp
+000169a0: 7574 733a 0a20 2020 2020 2020 2020 2020  uts:.           
+000169b0: 2077 6569 6768 7465 645f 7661 6c75 6573   weighted_values
+000169c0: 3a20 5465 6e73 6f72 2c20 7468 6520 7765  : Tensor, the we
+000169d0: 6967 6874 6564 2073 756d 2073 636f 7265  ighted sum score
+000169e0: 730a 2020 2020 2020 2020 2222 220a 2020  s.        """.  
+000169f0: 2020 2020 2020 2320 4e6f 726d 616c 697a        # Normaliz
+00016a00: 6520 7175 6572 7920 616e 6420 6b65 7920  e query and key 
+00016a10: 6265 666f 7265 204d 6174 4d75 6c2c 2064  before MatMul, d
+00016a20: 6566 6175 6c74 206f 6666 0a20 2020 2020  efault off.     
+00016a30: 2020 2023 2041 7474 656e 7469 6f6e 2073     # Attention s
+00016a40: 636f 7265 205b 6273 2c20 6e75 6d5f 6865  core [bs, num_he
+00016a50: 6164 732c 2073 6571 5f6c 656e 6774 682c  ads, seq_length,
+00016a60: 2073 6571 5f6c 656e 6774 685d 0a20 2020   seq_length].   
+00016a70: 2020 2020 2066 6163 746f 7220 3d20 502e       factor = P.
+00016a80: 4361 7374 2829 2873 656c 662e 7363 616c  Cast()(self.scal
+00016a90: 655f 6661 6374 6f72 2c20 502e 4454 7970  e_factor, P.DTyp
+00016aa0: 6528 2928 7175 6572 7929 290a 2020 2020  e()(query)).    
+00016ab0: 2020 2020 7175 6572 7920 3d20 7365 6c66      query = self
+00016ac0: 2e72 6561 6c5f 6469 7628 7175 6572 792c  .real_div(query,
+00016ad0: 2066 6163 746f 7229 0a20 2020 2020 2020   factor).       
+00016ae0: 206b 6579 203d 2073 656c 662e 7265 616c   key = self.real
+00016af0: 5f64 6976 286b 6579 2c20 6661 6374 6f72  _div(key, factor
+00016b00: 290a 2020 2020 2020 2020 7363 6f72 6520  ).        score 
+00016b10: 3d20 7365 6c66 2e62 6174 6368 5f6d 6174  = self.batch_mat
+00016b20: 6d75 6c28 7175 6572 792c 206b 6579 290a  mul(query, key).
+00016b30: 0a20 2020 2020 2020 206f 7269 5f64 7479  .        ori_dty
+00016b40: 7065 203d 2050 2e44 5479 7065 2829 2873  pe = P.DType()(s
+00016b50: 636f 7265 290a 2020 2020 2020 2020 6174  core).        at
+00016b60: 7465 6e74 696f 6e5f 7363 6f72 6573 203d  tention_scores =
+00016b70: 2073 656c 662e 736f 6674 6d61 785f 6361   self.softmax_ca
+00016b80: 7374 2873 636f 7265 2c20 7365 6c66 2e73  st(score, self.s
+00016b90: 6f66 746d 6178 5f64 7479 7065 290a 0a20  oftmax_dtype).. 
+00016ba0: 2020 2020 2020 2023 2066 6f72 2069 6e70         # for inp
+00016bb0: 7574 2073 697a 6520 6f66 2028 6273 2c20  ut size of (bs, 
+00016bc0: 3129 206e 616d 656c 7920 7468 6520 7365  1) namely the se
+00016bd0: 636f 6e64 2067 7261 7068 2c0a 2020 2020  cond graph,.    
+00016be0: 2020 2020 2320 7468 6520 7368 6170 6520      # the shape 
+00016bf0: 6f66 2061 7474 656e 7469 6f6e 5f6d 6173  of attention_mas
+00016c00: 6b20 6d61 7472 6978 2073 686f 756c 6420  k matrix should 
+00016c10: 6265 2028 6273 2c20 312c 2031 2c20 7365  be (bs, 1, 1, se
+00016c20: 715f 6c65 6e67 7468 290a 2020 2020 2020  q_length).      
+00016c30: 2020 6966 2061 7474 656e 7469 6f6e 5f6d    if attention_m
+00016c40: 6173 6b20 6973 206e 6f74 204e 6f6e 653a  ask is not None:
+00016c50: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if 
+00016c60: 7365 6c66 2e75 7365 5f70 6173 7420 616e  self.use_past an
+00016c70: 6420 6e6f 7420 7365 6c66 2e69 735f 6669  d not self.is_fi
+00016c80: 7273 745f 6974 6572 6174 696f 6e3a 0a20  rst_iteration:. 
+00016c90: 2020 2020 2020 2020 2020 2020 2020 2023                 #
+00016ca0: 2043 616c 6375 6c61 7465 2074 6865 2063   Calculate the c
+00016cb0: 7572 7265 6e74 2074 6f74 616c 2074 6f6b  urrent total tok
+00016cc0: 656e 0a20 2020 2020 2020 2020 2020 2020  en.             
+00016cd0: 2020 2063 7572 7265 6e74 5f69 6e64 6578     current_index
+00016ce0: 203d 2073 656c 662e 7265 6475 6365 7375   = self.reducesu
+00016cf0: 6d28 462e 6361 7374 2873 656c 662e 6e6f  m(F.cast(self.no
+00016d00: 745f 6571 7561 6c28 7365 6c66 2e73 6c69  t_equal(self.sli
+00016d10: 6365 286b 6579 2c20 2830 2c20 302c 2030  ce(key, (0, 0, 0
+00016d20: 2c20 3029 2c0a 2020 2020 2020 2020 2020  , 0),.          
+00016d30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00016d40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00016d50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00016d60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00016d70: 2020 2020 2020 2846 2e73 6861 7065 2871        (F.shape(q
+00016d80: 7565 7279 295b 305d 2c20 312c 2031 2c0a  uery)[0], 1, 1,.
+00016d90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00016da0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00016db0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00016dc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00016dd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00016de0: 2073 656c 662e 7365 715f 6c65 6e67 7468   self.seq_length
+00016df0: 292c 0a20 2020 2020 2020 2020 2020 2020  ),.             
+00016e00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00016e10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00016e20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00016e30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00016e40: 2020 2028 312c 2031 2c20 312c 2031 2929     (1, 1, 1, 1))
+00016e50: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00016e60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00016e70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00016e80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00016e90: 2020 2020 2020 2030 292c 206d 7374 7970         0), mstyp
+00016ea0: 652e 666c 6f61 7433 3229 2c20 2831 2c20  e.float32), (1, 
+00016eb0: 322c 2033 2929 0a20 2020 2020 2020 2020  2, 3)).         
+00016ec0: 2020 2020 2020 2023 2047 6574 2074 6865         # Get the
+00016ed0: 2070 7265 6369 7365 2070 6f73 6974 696f   precise positio
+00016ee0: 6e20 696e 6465 780a 2020 2020 2020 2020  n index.        
+00016ef0: 2020 2020 2020 2020 696e 6465 7820 3d20          index = 
+00016f00: 7365 6c66 2e73 7562 3128 462e 6361 7374  self.sub1(F.cast
+00016f10: 2863 7572 7265 6e74 5f69 6e64 6578 2c20  (current_index, 
+00016f20: 6d73 7479 7065 2e69 6e74 3332 292c 2031  mstype.int32), 1
+00016f30: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
+00016f40: 2020 696e 6465 7820 3d20 462e 7265 7368    index = F.resh
+00016f50: 6170 6528 696e 6465 782c 2028 2d31 2c20  ape(index, (-1, 
+00016f60: 312c 2031 2929 0a20 2020 2020 2020 2020  1, 1)).         
+00016f70: 2020 2020 2020 2023 2043 616c 6375 6c61         # Calcula
+00016f80: 7465 2074 6865 2061 7474 656e 7469 6f6e  te the attention
+00016f90: 5f6d 6173 6b20 6d61 7472 6978 2076 6961  _mask matrix via
+00016fa0: 2074 6865 2070 6f73 6974 696f 6e20 696e   the position in
+00016fb0: 6465 780a 2020 2020 2020 2020 2020 2020  dex.            
+00016fc0: 2020 2020 6174 7465 6e74 696f 6e5f 6d61      attention_ma
+00016fd0: 736b 203d 2046 2e63 6173 7428 7365 6c66  sk = F.cast(self
+00016fe0: 2e74 656e 736f 725f 6c65 2873 656c 662e  .tensor_le(self.
+00016ff0: 7261 6e67 652c 2069 6e64 6578 292c 206d  range, index), m
+00017000: 7374 7970 652e 696e 7433 3229 0a20 2020  stype.int32).   
+00017010: 2020 2020 2020 2020 2020 2020 2061 7474               att
+00017020: 656e 7469 6f6e 5f6d 6173 6b20 3d20 7365  ention_mask = se
+00017030: 6c66 2e65 7870 616e 645f 6469 6d73 2861  lf.expand_dims(a
+00017040: 7474 656e 7469 6f6e 5f6d 6173 6b2c 2032  ttention_mask, 2
+00017050: 290a 2020 2020 2020 2020 2020 2020 2320  ).            # 
+00017060: 4d69 6e75 7320 3130 3030 3020 666f 7220  Minus 10000 for 
+00017070: 7468 6520 706f 7369 7469 6f6e 2077 6865  the position whe
+00017080: 7265 206d 6173 6b65 6420 746f 2065 7863  re masked to exc
+00017090: 6c75 6465 2074 6865 6d20 6672 6f6d 2073  lude them from s
+000170a0: 6f66 746d 6178 0a20 2020 2020 2020 2020  oftmax.         
+000170b0: 2020 206d 756c 7469 706c 755f 6f75 7420     multiplu_out 
+000170c0: 3d20 7365 6c66 2e73 7562 5f73 6128 0a20  = self.sub_sa(. 
+000170d0: 2020 2020 2020 2020 2020 2020 2020 2050                 P
+000170e0: 2e43 6173 7428 2928 462e 7475 706c 655f  .Cast()(F.tuple_
+000170f0: 746f 5f61 7272 6179 2828 312e 302c 2929  to_array((1.0,))
+00017100: 2c20 502e 4454 7970 6528 2928 6174 7465  , P.DType()(atte
+00017110: 6e74 696f 6e5f 7363 6f72 6573 2929 2c0a  ntion_scores)),.
+00017120: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017130: 502e 4361 7374 2829 2861 7474 656e 7469  P.Cast()(attenti
+00017140: 6f6e 5f6d 6173 6b2c 2050 2e44 5479 7065  on_mask, P.DType
+00017150: 2829 2861 7474 656e 7469 6f6e 5f73 636f  ()(attention_sco
+00017160: 7265 7329 2929 0a0a 2020 2020 2020 2020  res)))..        
+00017170: 2020 2020 6164 6465 7220 3d20 7365 6c66      adder = self
+00017180: 2e6d 756c 286d 756c 7469 706c 755f 6f75  .mul(multiplu_ou
+00017190: 742c 2073 656c 662e 6d75 6c74 6970 6c79  t, self.multiply
+000171a0: 5f64 6174 6129 0a20 2020 2020 2020 2020  _data).         
+000171b0: 2020 2061 7474 656e 7469 6f6e 5f73 636f     attention_sco
+000171c0: 7265 7320 3d20 7365 6c66 2e61 6464 2861  res = self.add(a
+000171d0: 6464 6572 2c20 6174 7465 6e74 696f 6e5f  dder, attention_
+000171e0: 7363 6f72 6573 290a 0a20 2020 2020 2020  scores)..       
+000171f0: 2023 2061 7474 656e 7469 6f6e 2070 726f   # attention pro
+00017200: 6273 0a20 2020 2020 2020 2061 7474 656e  bs.        atten
+00017210: 7469 6f6e 5f70 726f 6273 203d 2073 656c  tion_probs = sel
+00017220: 662e 5f73 6f66 746d 6178 2861 7474 656e  f._softmax(atten
+00017230: 7469 6f6e 5f73 636f 7265 7329 0a20 2020  tion_scores).   
+00017240: 2020 2020 2061 7474 656e 7469 6f6e 5f70       attention_p
+00017250: 726f 6273 203d 2073 656c 662e 736f 6674  robs = self.soft
+00017260: 6d61 785f 6361 7374 2861 7474 656e 7469  max_cast(attenti
+00017270: 6f6e 5f70 726f 6273 2c20 6f72 695f 6474  on_probs, ori_dt
+00017280: 7970 6529 0a0a 2020 2020 2020 2020 6174  ype)..        at
+00017290: 7465 6e74 696f 6e5f 7072 6f62 7320 3d20  tention_probs = 
+000172a0: 7365 6c66 2e70 726f 625f 6472 6f70 6f75  self.prob_dropou
+000172b0: 7428 6174 7465 6e74 696f 6e5f 7072 6f62  t(attention_prob
+000172c0: 7329 0a20 2020 2020 2020 2023 2057 6569  s).        # Wei
+000172d0: 6768 7465 6420 7375 6d20 6f75 7470 7574  ghted sum output
+000172e0: 205b 6273 2c20 6e75 6d5f 6865 6164 732c   [bs, num_heads,
+000172f0: 2073 6571 5f6c 656e 6774 682c 2073 697a   seq_length, siz
+00017300: 655f 7065 725f 6865 6164 5d0a 2020 2020  e_per_head].    
+00017310: 2020 2020 7765 6967 6874 6564 5f76 616c      weighted_val
+00017320: 7565 7320 3d20 7365 6c66 2e62 6174 6368  ues = self.batch
+00017330: 5f6d 6174 6d75 6c28 6174 7465 6e74 696f  _matmul(attentio
+00017340: 6e5f 7072 6f62 732c 2076 616c 7565 290a  n_probs, value).
+00017350: 2020 2020 2020 2020 6174 7465 6e74 696f          attentio
+00017360: 6e5f 6d65 7267 6520 3d20 7365 6c66 2e5f  n_merge = self._
+00017370: 6d65 7267 655f 6865 6164 7328 7765 6967  merge_heads(weig
+00017380: 6874 6564 5f76 616c 7565 7329 0a20 2020  hted_values).   
+00017390: 2020 2020 2072 6574 7572 6e20 6174 7465       return atte
+000173a0: 6e74 696f 6e5f 6d65 7267 650a 0a0a 636c  ntion_merge...cl
+000173b0: 6173 7320 5472 616e 7366 6f72 6d65 7245  ass TransformerE
+000173c0: 6e63 6f64 6572 4c61 7965 7228 4365 6c6c  ncoderLayer(Cell
+000173d0: 293a 0a20 2020 2072 2222 220a 2020 2020  ):.    r""".    
+000173e0: 2020 2020 5472 616e 7366 6f72 6d65 7220      Transformer 
+000173f0: 456e 636f 6465 7220 4c61 7965 722e 2054  Encoder Layer. T
+00017400: 6869 7320 6973 2061 6e20 696d 706c 656d  his is an implem
+00017410: 656e 7461 7469 6f6e 206f 6620 7468 6520  entation of the 
+00017420: 7369 6e67 6c65 206c 6179 6572 206f 6620  single layer of 
+00017430: 7468 6520 7472 616e 7366 6f72 6d65 720a  the transformer.
+00017440: 2020 2020 2020 2020 656e 636f 6465 7220          encoder 
+00017450: 6c61 7965 722c 2069 6e63 6c75 6469 6e67  layer, including
+00017460: 206d 756c 7469 6865 6164 2061 7474 656e   multihead atten
+00017470: 7469 6f6e 2061 6e64 2066 6565 6477 6172  tion and feedwar
+00017480: 6420 6c61 7965 722e 0a0a 2020 2020 2020  d layer...      
+00017490: 2020 4172 6773 3a0a 2020 2020 2020 2020    Args:.        
+000174a0: 2020 2020 6261 7463 685f 7369 7a65 2869      batch_size(i
+000174b0: 6e74 293a 2054 6865 2062 6174 6368 2073  nt): The batch s
+000174c0: 697a 6520 6f66 2074 6865 2069 6e70 7574  ize of the input
+000174d0: 2074 656e 736f 7220 7768 656e 2064 6f20   tensor when do 
+000174e0: 696e 6372 656e 6d65 6e74 616c 2070 7265  increnmental pre
+000174f0: 6469 6374 696f 6e2e 2053 686f 756c 6420  diction. Should 
+00017500: 6265 2061 2070 6f73 6974 6976 650a 2020  be a positive.  
+00017510: 2020 2020 2020 2020 2020 2020 2020 7661                va
+00017520: 6c75 652e 2057 6865 6e20 646f 2074 7261  lue. When do tra
+00017530: 696e 696e 6720 6f72 2070 7265 6469 6374  ining or predict
+00017540: 696f 6e2c 2074 6865 2061 7267 756d 656e  ion, the argumen
+00017550: 7420 7769 6c6c 206e 6f74 2077 6f72 6b20  t will not work 
+00017560: 616e 6420 7468 6520 7573 6572 2063 616e  and the user can
+00017570: 206a 7573 7420 7061 7373 204e 6f6e 6520   just pass None 
+00017580: 746f 0a20 2020 2020 2020 2020 2020 2020  to.             
+00017590: 2020 2074 6865 2061 7267 756d 656e 742e     the argument.
+000175a0: 0a20 2020 2020 2020 2020 2020 2068 6964  .            hid
+000175b0: 6465 6e5f 7369 7a65 2869 6e74 293a 2054  den_size(int): T
+000175c0: 6865 2068 6964 6465 6e20 7369 7a65 206f  he hidden size o
+000175d0: 6620 7468 6520 696e 7075 742e 0a20 2020  f the input..   
+000175e0: 2020 2020 2020 2020 2066 666e 5f68 6964           ffn_hid
+000175f0: 6465 6e5f 7369 7a65 2869 6e74 293a 2054  den_size(int): T
+00017600: 6865 2068 6964 6465 6e20 7369 7a65 206f  he hidden size o
+00017610: 6620 626f 7474 6c65 6e65 636b 2069 6e20  f bottleneck in 
+00017620: 7468 6520 6665 6564 666f 7277 6172 6420  the feedforward 
+00017630: 6c61 7965 722e 0a20 2020 2020 2020 2020  layer..         
+00017640: 2020 206e 756d 5f68 6561 6473 2869 6e74     num_heads(int
+00017650: 293a 2054 6865 206e 756d 6265 7220 6f66  ): The number of
+00017660: 2074 6865 2068 6561 6473 2e0a 2020 2020   the heads..    
+00017670: 2020 2020 2020 2020 7365 715f 6c65 6e67          seq_leng
+00017680: 7468 2869 6e74 293a 2054 6865 2069 6e70  th(int): The inp
+00017690: 7574 2073 6571 7565 6e63 6520 6c65 6e67  ut sequence leng
+000176a0: 7468 2e0a 2020 2020 2020 2020 2020 2020  th..            
+000176b0: 6174 7465 6e74 696f 6e5f 6472 6f70 6f75  attention_dropou
+000176c0: 745f 7261 7465 2866 6c6f 6174 293a 2054  t_rate(float): T
+000176d0: 6865 2064 726f 706f 7574 2072 6174 6520  he dropout rate 
+000176e0: 6f66 2074 6865 2061 7474 656e 7469 6f6e  of the attention
+000176f0: 2073 636f 7265 732e 2044 6566 6175 6c74   scores. Default
+00017700: 3a30 2e31 2e0a 2020 2020 2020 2020 2020  :0.1..          
+00017710: 2020 6869 6464 656e 5f64 726f 706f 7574    hidden_dropout
+00017720: 5f72 6174 6528 666c 6f61 7429 3a20 5468  _rate(float): Th
+00017730: 6520 6472 6f70 6f75 7420 7261 7465 206f  e dropout rate o
+00017740: 6620 7468 6520 6669 6e61 6c20 6f75 7470  f the final outp
+00017750: 7574 206f 6620 7468 6520 6c61 7965 722e  ut of the layer.
+00017760: 2044 6566 6175 6c74 3a30 2e31 2e0a 2020   Default:0.1..  
+00017770: 2020 2020 2020 2020 2020 706f 7374 5f6c            post_l
+00017780: 6179 6572 6e6f 726d 5f72 6573 6964 7561  ayernorm_residua
+00017790: 6c28 626f 6f6c 293a 2044 6f20 7265 7369  l(bool): Do resi
+000177a0: 6475 616c 7320 6164 6473 2062 6566 6f72  duals adds befor
+000177b0: 6520 7468 6520 6c61 7965 726e 6f72 6d2e  e the layernorm.
+000177c0: 2044 6566 6175 6c74 2046 616c 7365 2e0a   Default False..
+000177d0: 2020 2020 2020 2020 2020 2020 6c61 7965              laye
+000177e0: 726e 6f72 6d5f 636f 6d70 7574 655f 7479  rnorm_compute_ty
+000177f0: 7065 2864 7479 7065 2e4e 756d 6265 7229  pe(dtype.Number)
+00017800: 3a20 5468 6520 636f 6d70 7574 6174 696f  : The computatio
+00017810: 6e20 7479 7065 206f 6620 7468 6520 6c61  n type of the la
+00017820: 7965 726e 6f72 6d2e 0a20 2020 2020 2020  yernorm..       
+00017830: 2020 2020 2020 2020 2053 686f 756c 6420           Should 
+00017840: 6265 206d 7374 7970 652e 666c 6f61 7433  be mstype.float3
+00017850: 3220 6f72 206d 7374 7970 652e 666c 6f61  2 or mstype.floa
+00017860: 7431 362e 2044 6566 6175 6c74 206d 7374  t16. Default mst
+00017870: 7970 652e 666c 6f61 7433 322e 0a20 2020  ype.float32..   
+00017880: 2020 2020 2020 2020 2073 6f66 746d 6178           softmax
+00017890: 5f63 6f6d 7075 7465 5f74 7970 6528 6474  _compute_type(dt
+000178a0: 7970 652e 4e75 6d62 6572 293a 2054 6865  ype.Number): The
+000178b0: 2063 6f6d 7075 7461 7469 6f6e 2074 7970   computation typ
+000178c0: 6520 6f66 2074 6865 2073 6f66 746d 6178  e of the softmax
+000178d0: 2069 6e20 7468 6520 6174 7465 6e74 696f   in the attentio
+000178e0: 6e2e 0a20 2020 2020 2020 2020 2020 2020  n..             
+000178f0: 2020 2053 686f 756c 6420 6265 206d 7374     Should be mst
+00017900: 7970 652e 666c 6f61 7433 3220 6f72 206d  ype.float32 or m
+00017910: 7374 7970 652e 666c 6f61 7431 362e 2044  stype.float16. D
+00017920: 6566 6175 6c74 206d 7374 7970 652e 666c  efault mstype.fl
+00017930: 6f61 7433 322e 0a20 2020 2020 2020 2020  oat32..         
+00017940: 2020 2070 6172 616d 5f69 6e69 745f 7479     param_init_ty
+00017950: 7065 2864 7479 7065 2e4e 756d 6265 7229  pe(dtype.Number)
+00017960: 3a20 5468 6520 7061 7261 6d65 7465 7220  : The parameter 
+00017970: 696e 6974 6961 6c69 7a61 7469 6f6e 2074  initialization t
+00017980: 7970 6520 6f66 2074 6865 206d 6f64 756c  ype of the modul
+00017990: 652e 0a20 2020 2020 2020 2020 2020 2020  e..             
+000179a0: 2020 2053 686f 756c 6420 6265 206d 7374     Should be mst
+000179b0: 7970 652e 666c 6f61 7433 3220 6f72 206d  ype.float32 or m
+000179c0: 7374 7970 652e 666c 6f61 7431 362e 2044  stype.float16. D
+000179d0: 6566 6175 6c74 206d 7374 7970 652e 666c  efault mstype.fl
+000179e0: 6f61 7433 322e 0a20 2020 2020 2020 2020  oat32..         
+000179f0: 2020 2068 6964 6465 6e5f 6163 7420 2873     hidden_act (s
+00017a00: 7472 2c20 6e6e 2e43 656c 6c29 3a20 5468  tr, nn.Cell): Th
+00017a10: 6520 6163 7469 7661 7469 6f6e 206f 6620  e activation of 
+00017a20: 7468 6520 696e 7465 726e 616c 2066 6565  the internal fee
+00017a30: 6466 6f72 7761 7264 206c 6179 6572 2e20  dforward layer. 
+00017a40: 5375 7070 6f72 7473 2027 7265 6c75 272c  Supports 'relu',
+00017a50: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00017a60: 2027 7265 6c75 3627 2c20 2774 616e 6827   'relu6', 'tanh'
+00017a70: 2c20 2767 656c 7527 2c20 2766 6173 745f  , 'gelu', 'fast_
+00017a80: 6765 6c75 272c 2027 656c 7527 2c20 2773  gelu', 'elu', 's
+00017a90: 6967 6d6f 6964 272c 2027 7072 656c 7527  igmoid', 'prelu'
+00017aa0: 2c20 276c 6561 6b79 7265 6c75 272c 2027  , 'leakyrelu', '
+00017ab0: 6873 7769 7368 272c 0a20 2020 2020 2020  hswish',.       
+00017ac0: 2020 2020 2020 2020 2027 6873 6967 6d6f           'hsigmo
+00017ad0: 6964 272c 2027 6c6f 6773 6967 6d6f 6964  id', 'logsigmoid
+00017ae0: 2720 616e 6420 736f 206f 6e2e 2055 7365  ' and so on. Use
+00017af0: 7220 6361 6e20 7072 6f76 6964 6520 6375  r can provide cu
+00017b00: 7374 6f6d 2061 6374 6976 6974 696f 6e20  stom activition 
+00017b10: 746f 2074 6865 2061 7267 756d 656e 742e  to the argument.
+00017b20: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00017b30: 2049 6620 7573 6572 2077 616e 7473 2074   If user wants t
+00017b40: 6f20 7275 6e20 7468 6520 6e65 7420 696e  o run the net in
+00017b50: 2074 6865 2070 6172 616c 6c65 6c20 6d6f   the parallel mo
+00017b60: 6465 2c20 7468 6520 6375 7374 6f6d 2061  de, the custom a
+00017b70: 6374 6976 6174 696f 6e20 6d75 7374 2061  ctivation must a
+00017b80: 6c73 6f20 7072 6f76 6964 650a 2020 2020  lso provide.    
+00017b90: 2020 2020 2020 2020 2020 2020 7468 6520              the 
+00017ba0: 6061 6374 6976 6174 696f 6e5f 7368 6172  `activation_shar
+00017bb0: 6460 2066 756e 6374 696f 6e2e 2050 6c65  d` function. Ple
+00017bc0: 6173 6520 7365 6520 7468 6520 6578 616d  ase see the exam
+00017bd0: 706c 6573 206f 6620 7468 650a 2020 2020  ples of the.    
+00017be0: 2020 2020 2020 2020 2020 2020 636c 6173              clas
+00017bf0: 733a 606d 696e 6466 6f72 6d65 7273 2e6d  s:`mindformers.m
+00017c00: 6f64 756c 6573 2e74 7261 6e73 666f 726d  odules.transform
+00017c10: 6572 2e46 6565 6446 6f72 7761 7264 602e  er.FeedForward`.
+00017c20: 2044 6566 6175 6c74 3a20 6765 6c75 2e0a   Default: gelu..
+00017c30: 2020 2020 2020 2020 2020 2020 7573 655f              use_
+00017c40: 7061 7374 2862 6f6f 6c29 3a20 5573 6520  past(bool): Use 
+00017c50: 7468 6520 7061 7374 2073 7461 7465 2074  the past state t
+00017c60: 6f20 636f 6d70 7574 652c 2075 7365 6420  o compute, used 
+00017c70: 666f 7220 696e 6372 656d 656e 7461 6c20  for incremental 
+00017c80: 7072 6564 6963 7469 6f6e 2e20 466f 7220  prediction. For 
+00017c90: 6578 616d 706c 652c 2069 6620 7765 2068  example, if we h
+00017ca0: 6176 6520 7477 6f0a 2020 2020 2020 2020  ave two.        
+00017cb0: 2020 2020 2020 2020 776f 7264 7320 616e          words an
+00017cc0: 6420 7761 6e74 2074 6f20 6765 6e65 7261  d want to genera
+00017cd0: 7465 2074 6865 2074 656e 206d 6f72 6520  te the ten more 
+00017ce0: 776f 7264 732e 2057 6520 6a75 7374 206e  words. We just n
+00017cf0: 6565 6420 746f 2063 6f6d 7075 7465 2074  eed to compute t
+00017d00: 6865 2074 776f 2077 6f72 6473 2720 7374  he two words' st
+00017d10: 6174 6520 6f6e 6c79 206f 6e63 652c 0a20  ate only once,. 
+00017d20: 2020 2020 2020 2020 2020 2020 2020 2061                 a
+00017d30: 6e64 2067 656e 6572 6174 6520 7468 6520  nd generate the 
+00017d40: 6e65 7874 2077 6f72 6420 6f6e 6520 6279  next word one by
+00017d50: 206f 6e65 2e20 5768 656e 2075 7365 5f70   one. When use_p
+00017d60: 6173 7420 6973 2054 7275 652c 2074 6865  ast is True, the
+00017d70: 7265 2061 7265 2074 776f 2073 7465 7073  re are two steps
+00017d80: 2074 6f20 7275 6e20 7468 6520 7072 6564   to run the pred
+00017d90: 6963 7469 6f6e 2e0a 2020 2020 2020 2020  iction..        
+00017da0: 2020 2020 2020 2020 496e 2074 6865 2066          In the f
+00017db0: 6972 7374 2073 7465 702c 2073 6574 2074  irst step, set t
+00017dc0: 6865 2069 735f 6669 7273 745f 6974 6572  he is_first_iter
+00017dd0: 6174 696f 6e20 746f 2062 6520 5472 7565  ation to be True
+00017de0: 2062 790a 2020 2020 2020 2020 2020 2020   by.            
+00017df0: 2020 2020 606d 6f64 656c 2e61 6464 5f66      `model.add_f
+00017e00: 6c61 6773 5f72 6563 7572 7369 7665 2869  lags_recursive(i
+00017e10: 735f 6669 7273 745f 6974 6572 6174 696f  s_first_iteratio
+00017e20: 6e3d 5472 7565 2960 2c20 616e 6420 7061  n=True)`, and pa
+00017e30: 7373 2074 6865 2066 756c 6c20 696e 7075  ss the full inpu
+00017e40: 7473 2e20 5468 656e 2c20 7365 7420 7468  ts. Then, set th
+00017e50: 650a 2020 2020 2020 2020 2020 2020 2020  e.              
+00017e60: 2020 6973 5f66 6972 7374 5f69 7465 7261    is_first_itera
+00017e70: 7469 6f6e 2074 6f20 6265 2046 616c 7365  tion to be False
+00017e80: 2062 7920 606d 6f64 656c 2e61 6464 5f66   by `model.add_f
+00017e90: 6c61 6773 5f72 6563 7572 7369 7665 2869  lags_recursive(i
+00017ea0: 735f 6669 7273 745f 6974 6572 6174 696f  s_first_iteratio
+00017eb0: 6e3d 4661 6c73 6529 602e 0a20 2020 2020  n=False)`..     
+00017ec0: 2020 2020 2020 2020 2020 2041 7420 7468             At th
+00017ed0: 6973 206d 6f6d 656e 742c 2070 6173 7320  is moment, pass 
+00017ee0: 7468 6520 7369 6e67 6c65 2073 7465 7027  the single step'
+00017ef0: 7320 696e 7075 7420 7465 6e73 6f72 2c20  s input tensor, 
+00017f00: 616e 6420 6c6f 6f70 2069 742e 2044 6566  and loop it. Def
+00017f10: 6175 6c74 2046 616c 7365 2e0a 2020 2020  ault False..    
+00017f20: 2020 2020 2020 2020 6d6f 655f 636f 6e66          moe_conf
+00017f30: 6967 284d 6f45 436f 6e66 6967 293a 2054  ig(MoEConfig): T
+00017f40: 6865 2063 6f6e 6669 6775 7261 7469 6f6e  he configuration
+00017f50: 206f 6620 4d6f 4520 284d 6978 7475 7265   of MoE (Mixture
+00017f60: 206f 6620 4578 7065 7274 292e 2044 6566   of Expert). Def
+00017f70: 6175 6c74 2069 7320 616e 2069 6e73 7461  ault is an insta
+00017f80: 6e63 6520 6f66 204d 6f45 436f 6e66 6967  nce of MoEConfig
+00017f90: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00017fa0: 2077 6974 6820 6465 6661 756c 7420 7661   with default va
+00017fb0: 6c75 6573 2e20 506c 6561 7365 2073 6565  lues. Please see
+00017fc0: 2060 4d6f 4543 6f6e 6669 6760 2e0a 2020   `MoEConfig`..  
+00017fd0: 2020 2020 2020 2020 2020 7061 7261 6c6c            parall
+00017fe0: 656c 5f63 6f6e 6669 6728 4f70 5061 7261  el_config(OpPara
+00017ff0: 6c6c 656c 436f 6e66 6967 2c20 4d6f 4550  llelConfig, MoEP
+00018000: 6172 616c 6c65 6c43 6f6e 6669 6729 3a20  arallelConfig): 
+00018010: 5468 6520 7061 7261 6c6c 656c 2063 6f6e  The parallel con
+00018020: 6669 6775 7265 2e20 5768 656e 204d 6f45  figure. When MoE
+00018030: 2069 7320 6170 706c 6965 642c 0a20 2020   is applied,.   
+00018040: 2020 2020 2020 2020 2020 2020 204d 6f45               MoE
+00018050: 5061 7261 6c6c 656c 436f 6e66 6967 2069  ParallelConfig i
+00018060: 7320 6566 6665 6374 6976 652c 206f 7468  s effective, oth
+00018070: 6572 7769 7365 204f 7050 6172 616c 6c65  erwise OpParalle
+00018080: 6c43 6f6e 6669 6720 6973 2065 6666 6563  lConfig is effec
+00018090: 7469 7665 2e20 4465 6661 756c 7420 6064  tive. Default `d
+000180a0: 6566 6175 6c74 5f64 706d 705f 636f 6e66  efault_dpmp_conf
+000180b0: 6967 602c 0a20 2020 2020 2020 2020 2020  ig`,.           
+000180c0: 2020 2020 2061 6e20 696e 7374 616e 6365       an instance
+000180d0: 206f 6620 604f 7050 6172 616c 6c65 6c43   of `OpParallelC
+000180e0: 6f6e 6669 6760 2077 6974 6820 6465 6661  onfig` with defa
+000180f0: 756c 7420 6172 6773 2e0a 0a20 2020 2020  ult args...     
+00018100: 2020 2049 6e70 7574 733a 0a20 2020 2020     Inputs:.     
+00018110: 2020 2020 2020 202d 202a 2a78 2a2a 2028         - **x** (
+00018120: 5465 6e73 6f72 2920 2d20 466c 6f61 7420  Tensor) - Float 
+00018130: 5465 6e73 6f72 2c20 7368 6170 6520 7368  Tensor, shape sh
+00018140: 6f75 6c64 2062 6520 5b62 6174 6368 5f73  ould be [batch_s
+00018150: 697a 652c 2073 6571 5f6c 656e 6774 682c  ize, seq_length,
+00018160: 2068 6964 6465 6e5f 7369 7a65 5d20 6f72   hidden_size] or
+00018170: 0a20 2020 2020 2020 2020 2020 2020 205b  .              [
+00018180: 6261 7463 685f 7369 7a65 202a 2073 6571  batch_size * seq
+00018190: 5f6c 656e 6774 682c 2068 6964 6465 6e5f  _length, hidden_
+000181a0: 7369 7a65 5d2c 2069 6620 7468 6520 7573  size], if the us
+000181b0: 655f 7061 7374 2069 7320 4661 6c73 6520  e_past is False 
+000181c0: 6f72 2069 735f 6669 7273 745f 6974 6572  or is_first_iter
+000181d0: 6174 696f 6e3d 5472 7565 2e20 4f74 6865  ation=True. Othe
+000181e0: 7277 6973 652c 0a20 2020 2020 2020 2020  rwise,.         
+000181f0: 2020 2020 2073 686f 756c 6420 6265 205b       should be [
+00018200: 6261 7463 685f 7369 7a65 2c20 312c 2068  batch_size, 1, h
+00018210: 6964 6465 6e5f 7369 7a65 5d0a 2020 2020  idden_size].    
+00018220: 2020 2020 2020 2020 2d20 2a2a 696e 7075          - **inpu
+00018230: 745f 6d61 736b 2a2a 2028 5465 6e73 6f72  t_mask** (Tensor
+00018240: 2920 2d20 466c 6f61 7420 5465 6e73 6f72  ) - Float Tensor
+00018250: 2c20 4966 2074 6865 2075 7365 5f70 6173  , If the use_pas
+00018260: 7420 6973 2046 616c 7365 206f 7220 6973  t is False or is
+00018270: 5f66 6972 7374 5f69 7465 7261 7469 6f6e  _first_iteration
+00018280: 3d54 7275 652c 0a20 2020 2020 2020 2020  =True,.         
+00018290: 2020 2020 2074 6865 2061 7474 656e 7469       the attenti
+000182a0: 6f6e 206d 6173 6b20 6d61 7472 6978 2073  on mask matrix s
+000182b0: 686f 756c 6420 6261 205b 6261 7463 685f  hould ba [batch_
+000182c0: 7369 7a65 2c20 7365 715f 6c65 6e67 7468  size, seq_length
+000182d0: 2c20 7365 715f 6c65 6e67 7468 5d2c 206f  , seq_length], o
+000182e0: 7220 4e6f 6e65 2e20 4e6f 6e65 206d 6561  r None. None mea
+000182f0: 6e73 2074 6865 7265 2077 696c 6c0a 2020  ns there will.  
+00018300: 2020 2020 2020 2020 2020 2020 6265 206e              be n
+00018310: 6f20 6d61 736b 2069 6e20 736f 6674 6d61  o mask in softma
+00018320: 7820 636f 6d70 7574 6174 696f 6e2e 204f  x computation. O
+00018330: 7468 6572 7769 7365 2c20 7368 6f75 6c64  therwise, should
+00018340: 2062 6520 5b62 6174 6368 5f73 697a 652c   be [batch_size,
+00018350: 2031 2c20 6869 6464 656e 5f73 697a 655d   1, hidden_size]
+00018360: 0a20 2020 2020 2020 2020 2020 202d 202a  .            - *
+00018370: 2a69 6e69 745f 7265 7365 742a 2a20 2854  *init_reset** (T
+00018380: 656e 736f 7229 202d 2041 2062 6f6f 6c20  ensor) - A bool 
+00018390: 7465 6e73 6f72 2077 6974 6820 7368 6170  tensor with shap
+000183a0: 6520 5b31 5d2c 2075 7365 6420 746f 2063  e [1], used to c
+000183b0: 6c65 6172 2074 6865 2070 6173 7420 6b65  lear the past ke
+000183c0: 7920 7061 7261 6d65 7465 7220 616e 640a  y parameter and.
+000183d0: 2020 2020 2020 2020 2020 2020 2020 7061                pa
+000183e0: 7374 2076 616c 7565 2070 6172 616d 6574  st value paramet
+000183f0: 6572 2075 7365 6420 696e 2074 6865 2069  er used in the i
+00018400: 6e63 7265 6d65 6e74 616c 2070 7265 6469  ncremental predi
+00018410: 6374 696f 6e2e 204f 6e6c 7920 7661 6c69  ction. Only vali
+00018420: 6420 7768 656e 2075 7365 5f70 6173 7420  d when use_past 
+00018430: 6973 2054 7275 652e 2044 6566 6175 6c74  is True. Default
+00018440: 2054 7275 652e 0a20 2020 2020 2020 2020   True..         
+00018450: 2020 202d 202a 2a62 6174 6368 5f76 616c     - **batch_val
+00018460: 6964 5f6c 656e 6774 682a 2a20 2854 656e  id_length** (Ten
+00018470: 736f 7229 202d 2049 6e74 3332 2074 656e  sor) - Int32 ten
+00018480: 736f 7220 7769 7468 2073 6861 7065 205b  sor with shape [
+00018490: 6261 7463 685f 7369 7a65 5d20 7468 6520  batch_size] the 
+000184a0: 7061 7374 2063 616c 6375 6c61 7465 6420  past calculated 
+000184b0: 7468 6520 696e 6465 782e 0a20 2020 2020  the index..     
+000184c0: 2020 2020 2020 2020 2055 7365 6420 666f           Used fo
+000184d0: 7220 696e 6372 656d 656e 7461 6c20 7072  r incremental pr
+000184e0: 6564 6963 7469 6f6e 2077 6865 6e20 7468  ediction when th
+000184f0: 6520 7573 655f 7061 7374 2069 7320 5472  e use_past is Tr
+00018500: 7565 2e20 4465 6661 756c 7420 4e6f 6e65  ue. Default None
+00018510: 2e0a 0a20 2020 2020 2020 204f 7574 7075  ...        Outpu
+00018520: 7473 3a0a 2020 2020 2020 2020 2020 2020  ts:.            
+00018530: 5475 706c 652c 2061 2074 7570 6c65 2063  Tuple, a tuple c
+00018540: 6f6e 7461 696e 7328 606f 7574 7075 7460  ontains(`output`
+00018550: 2c20 606c 6179 6572 5f70 7265 7365 6e74  , `layer_present
+00018560: 6029 2e0a 0a20 2020 2020 2020 2020 2020  `)...           
+00018570: 202d 202a 2a6f 7574 7075 742a 2a20 2854   - **output** (T
+00018580: 656e 736f 7229 202d 2054 6865 2066 6c6f  ensor) - The flo
+00018590: 6174 2074 656e 736f 7220 6f66 2074 6865  at tensor of the
+000185a0: 206f 7574 7075 7420 6f66 2074 6865 206c   output of the l
+000185b0: 6179 6572 2077 6974 680a 2020 2020 2020  ayer with.      
+000185c0: 2020 2020 2020 2020 7368 6170 6520 2862          shape (b
+000185d0: 6174 6368 5f73 697a 652c 2073 6571 5f6c  atch_size, seq_l
+000185e0: 656e 6774 682c 2068 6964 6465 6e5f 7369  ength, hidden_si
+000185f0: 7a65 2920 6f72 2028 6261 7463 685f 7369  ze) or (batch_si
+00018600: 7a65 202a 2073 6571 5f6c 656e 6774 682c  ze * seq_length,
+00018610: 2068 6964 6465 6e5f 7369 7a65 292c 2069   hidden_size), i
+00018620: 6620 7468 6520 7573 655f 7061 7374 2069  f the use_past i
+00018630: 730a 2020 2020 2020 2020 2020 2020 2020  s.              
+00018640: 4661 6c73 6520 6f72 2069 735f 6669 7273  False or is_firs
+00018650: 745f 6974 6572 6174 696f 6e3d 5472 7565  t_iteration=True
+00018660: 2e20 4f74 6865 7277 6973 652c 2069 7420  . Otherwise, it 
+00018670: 7769 6c6c 2062 6520 2862 6174 6368 5f73  will be (batch_s
+00018680: 697a 652c 2031 2c20 6869 6464 656e 5f73  ize, 1, hidden_s
+00018690: 697a 6529 0a0a 2020 2020 2020 2020 2020  ize)..          
+000186a0: 2020 2d20 2a2a 6c61 7965 725f 7072 6573    - **layer_pres
+000186b0: 656e 742a 2a20 2854 7570 6c65 2920 2d20  ent** (Tuple) - 
+000186c0: 4120 7475 706c 6520 6f66 2074 6865 2054  A tuple of the T
+000186d0: 656e 736f 7220 6f66 2074 6865 2070 726f  ensor of the pro
+000186e0: 6a65 6374 6564 206b 6579 2061 6e64 2076  jected key and v
+000186f0: 616c 7565 2076 6563 746f 7220 7769 7468  alue vector with
+00018700: 0a20 2020 2020 2020 2020 2020 2020 2028  .              (
+00018710: 2862 6174 6368 5f73 697a 652c 206e 756d  (batch_size, num
+00018720: 5f68 6561 6473 2c20 7369 7a65 5f70 6572  _heads, size_per
+00018730: 5f68 6561 642c 2073 6571 5f6c 656e 6774  _head, seq_lengt
+00018740: 6829 2c0a 2020 2020 2020 2020 2020 2020  h),.            
+00018750: 2020 2862 6174 6368 5f73 697a 652c 206e    (batch_size, n
+00018760: 756d 5f68 6561 6473 2c20 7365 715f 6c65  um_heads, seq_le
+00018770: 6e67 7468 2c20 7369 7a65 5f70 6572 5f68  ngth, size_per_h
+00018780: 6561 6429 292e 0a0a 2020 2020 2020 2020  ead))...        
+00018790: 5375 7070 6f72 7465 6420 506c 6174 666f  Supported Platfo
+000187a0: 726d 733a 0a20 2020 2020 2020 2020 2020  rms:.           
+000187b0: 2060 6041 7363 656e 6460 6020 6060 4750   ``Ascend`` ``GP
+000187c0: 5560 600a 0a20 2020 2020 2020 2045 7861  U``..        Exa
+000187d0: 6d70 6c65 733a 0a20 2020 2020 2020 2020  mples:.         
+000187e0: 2020 203e 3e3e 2069 6d70 6f72 7420 6e75     >>> import nu
+000187f0: 6d70 7920 6173 206e 700a 2020 2020 2020  mpy as np.      
+00018800: 2020 2020 2020 3e3e 3e20 6672 6f6d 206d        >>> from m
+00018810: 696e 6473 706f 7265 2069 6d70 6f72 7420  indspore import 
+00018820: 6474 7970 6520 6173 206d 7374 7970 650a  dtype as mstype.
+00018830: 2020 2020 2020 2020 2020 2020 3e3e 3e20              >>> 
+00018840: 6672 6f6d 206d 696e 6466 6f72 6d65 7273  from mindformers
+00018850: 2e6d 6f64 756c 6573 2e74 7261 6e73 666f  .modules.transfo
+00018860: 726d 6572 2069 6d70 6f72 7420 5472 616e  rmer import Tran
+00018870: 7366 6f72 6d65 7245 6e63 6f64 6572 4c61  sformerEncoderLa
+00018880: 7965 720a 2020 2020 2020 2020 2020 2020  yer.            
+00018890: 3e3e 3e20 6672 6f6d 206d 696e 6473 706f  >>> from mindspo
+000188a0: 7265 2069 6d70 6f72 7420 5465 6e73 6f72  re import Tensor
+000188b0: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
+000188c0: 206d 6f64 656c 203d 2054 7261 6e73 666f   model = Transfo
+000188d0: 726d 6572 456e 636f 6465 724c 6179 6572  rmerEncoderLayer
+000188e0: 2862 6174 6368 5f73 697a 653d 322c 2068  (batch_size=2, h
+000188f0: 6964 6465 6e5f 7369 7a65 3d38 2c20 6666  idden_size=8, ff
+00018900: 6e5f 6869 6464 656e 5f73 697a 653d 3634  n_hidden_size=64
+00018910: 2c20 7365 715f 6c65 6e67 7468 3d31 362c  , seq_length=16,
+00018920: 0a20 2020 2020 2020 2020 2020 202e 2e2e  .            ...
+00018930: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00018940: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00018950: 206e 756d 5f68 6561 6473 3d32 290a 2020   num_heads=2).  
+00018960: 2020 2020 2020 2020 2020 3e3e 3e20 656e            >>> en
+00018970: 636f 6465 725f 696e 7075 745f 7661 6c75  coder_input_valu
+00018980: 6520 3d20 5465 6e73 6f72 286e 702e 6f6e  e = Tensor(np.on
+00018990: 6573 2828 322c 2031 362c 2038 2929 2c20  es((2, 16, 8)), 
+000189a0: 6d73 7479 7065 2e66 6c6f 6174 3332 290a  mstype.float32).
+000189b0: 2020 2020 2020 2020 2020 2020 3e3e 3e20              >>> 
+000189c0: 656e 636f 6465 725f 696e 7075 745f 6d61  encoder_input_ma
+000189d0: 736b 203d 2054 656e 736f 7228 6e70 2e6f  sk = Tensor(np.o
+000189e0: 6e65 7328 2832 2c20 3136 2c20 3136 2929  nes((2, 16, 16))
+000189f0: 2c20 6d73 7479 7065 2e66 6c6f 6174 3136  , mstype.float16
+00018a00: 290a 2020 2020 2020 2020 2020 2020 3e3e  ).            >>
+00018a10: 3e20 6f75 7470 7574 2c20 7061 7374 203d  > output, past =
+00018a20: 206d 6f64 656c 2865 6e63 6f64 6572 5f69   model(encoder_i
+00018a30: 6e70 7574 5f76 616c 7565 2c20 656e 636f  nput_value, enco
+00018a40: 6465 725f 696e 7075 745f 6d61 736b 290a  der_input_mask).
+00018a50: 2020 2020 2020 2020 2020 2020 3e3e 3e20              >>> 
+00018a60: 7072 696e 7428 6f75 7470 7574 2e73 6861  print(output.sha
+00018a70: 7065 290a 2020 2020 2020 2020 2020 2020  pe).            
+00018a80: 2832 2c20 3136 2c20 3829 0a20 2020 2020  (2, 16, 8).     
+00018a90: 2020 2020 2020 203e 3e3e 2070 7269 6e74         >>> print
+00018aa0: 2870 6173 745b 305d 2e73 6861 7065 290a  (past[0].shape).
+00018ab0: 2020 2020 2020 2020 2020 2020 2832 2c20              (2, 
+00018ac0: 322c 2034 2c20 3136 290a 2020 2020 2020  2, 4, 16).      
+00018ad0: 2020 2020 2020 3e3e 3e20 7072 696e 7428        >>> print(
+00018ae0: 7061 7374 5b31 5d2e 7368 6170 6529 0a20  past[1].shape). 
+00018af0: 2020 2020 2020 2020 2020 2028 322c 2032             (2, 2
+00018b00: 2c20 3136 2c20 3429 0a20 2020 2020 2020  , 16, 4).       
+00018b10: 2020 2020 203e 3e3e 2023 2057 6865 6e20       >>> # When 
+00018b20: 7573 6520 7573 655f 7061 7374 3d54 7275  use use_past=Tru
+00018b30: 652c 2069 7420 696e 636c 7564 6573 2074  e, it includes t
+00018b40: 776f 2073 7465 7073 2074 6f20 696d 706c  wo steps to impl
+00018b50: 656d 656e 7420 7468 6520 696e 6372 656d  ement the increm
+00018b60: 656e 7461 6c20 7072 6564 6963 7469 6f6e  ental prediction
+00018b70: 2e0a 2020 2020 2020 2020 2020 2020 3e3e  ..            >>
+00018b80: 3e20 2320 5374 6570 2031 3a20 7365 7420  > # Step 1: set 
+00018b90: 6973 5f66 6972 7374 5f69 7465 7261 7469  is_first_iterati
+00018ba0: 6f6e 3d54 7275 652c 2061 6e64 2069 6e70  on=True, and inp
+00018bb0: 7574 2074 6865 2066 756c 6c20 7365 7175  ut the full sequ
+00018bc0: 656e 6365 206c 656e 6774 6827 7320 7374  ence length's st
+00018bd0: 6174 652e 0a20 2020 2020 2020 2020 2020  ate..           
+00018be0: 203e 3e3e 2062 6174 6368 5f76 616c 6964   >>> batch_valid
+00018bf0: 5f6c 656e 6774 6820 3d20 5465 6e73 6f72  _length = Tensor
+00018c00: 286e 702e 6f6e 6573 2828 322c 2929 2c20  (np.ones((2,)), 
+00018c10: 6d73 7479 7065 2e69 6e74 3332 290a 2020  mstype.int32).  
+00018c20: 2020 2020 2020 2020 2020 3e3e 3e20 696e            >>> in
+00018c30: 6974 5f72 6573 6574 203d 2054 656e 736f  it_reset = Tenso
+00018c40: 7228 5b54 7275 655d 2c20 6d73 7479 7065  r([True], mstype
+00018c50: 2e62 6f6f 6c5f 290a 2020 2020 2020 2020  .bool_).        
+00018c60: 2020 2020 3e3e 3e20 2320 5365 7420 6973      >>> # Set is
+00018c70: 5f66 6972 7374 5f69 7465 7261 7469 6f6e  _first_iteration
+00018c80: 3d54 7275 6520 746f 2067 656e 6572 6174  =True to generat
+00018c90: 6520 7468 6520 6675 6c6c 206d 656d 6f72  e the full memor
+00018ca0: 7920 7374 6174 6573 0a20 2020 2020 2020  y states.       
+00018cb0: 2020 2020 203e 3e3e 206d 6f64 656c 203d       >>> model =
+00018cc0: 2054 7261 6e73 666f 726d 6572 456e 636f   TransformerEnco
+00018cd0: 6465 724c 6179 6572 2862 6174 6368 5f73  derLayer(batch_s
+00018ce0: 697a 653d 322c 2068 6964 6465 6e5f 7369  ize=2, hidden_si
+00018cf0: 7a65 3d38 2c20 6666 6e5f 6869 6464 656e  ze=8, ffn_hidden
+00018d00: 5f73 697a 653d 3634 2c20 7365 715f 6c65  _size=64, seq_le
+00018d10: 6e67 7468 3d31 362c 0a20 2020 2020 2020  ngth=16,.       
+00018d20: 2020 2020 202e 2e2e 2020 2020 2020 2020       ...        
+00018d30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00018d40: 2020 2020 2020 2020 206e 756d 5f68 6561           num_hea
+00018d50: 6473 3d32 2c20 7573 655f 7061 7374 3d54  ds=2, use_past=T
+00018d60: 7275 6529 0a20 2020 2020 2020 2020 2020  rue).           
+00018d70: 203e 3e3e 206d 6f64 656c 2e61 6464 5f66   >>> model.add_f
+00018d80: 6c61 6773 5f72 6563 7572 7369 7665 2869  lags_recursive(i
+00018d90: 735f 6669 7273 745f 6974 6572 6174 696f  s_first_iteratio
+00018da0: 6e3d 5472 7565 290a 2020 2020 2020 2020  n=True).        
+00018db0: 2020 2020 3e3e 3e20 6869 6464 656e 2c20      >>> hidden, 
+00018dc0: 7061 7374 203d 206d 6f64 656c 2865 6e63  past = model(enc
+00018dd0: 6f64 6572 5f69 6e70 7574 5f76 616c 7565  oder_input_value
+00018de0: 2c20 656e 636f 6465 725f 696e 7075 745f  , encoder_input_
+00018df0: 6d61 736b 2c20 696e 6974 5f72 6573 6574  mask, init_reset
+00018e00: 2c20 6261 7463 685f 7661 6c69 645f 6c65  , batch_valid_le
+00018e10: 6e67 7468 290a 2020 2020 2020 2020 2020  ngth).          
+00018e20: 2020 3e3e 3e20 7072 696e 7428 6869 6464    >>> print(hidd
+00018e30: 656e 2e73 6861 7065 290a 2020 2020 2020  en.shape).      
+00018e40: 2020 2020 2020 2832 2c20 3136 2c20 3829        (2, 16, 8)
+00018e50: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
+00018e60: 2070 7269 6e74 2870 6173 745b 305d 2e73   print(past[0].s
+00018e70: 6861 7065 290a 2020 2020 2020 2020 2020  hape).          
+00018e80: 2020 2832 2c20 322c 2034 2c20 3136 290a    (2, 2, 4, 16).
+00018e90: 2020 2020 2020 2020 2020 2020 3e3e 3e20              >>> 
+00018ea0: 7072 696e 7428 7061 7374 5b31 5d2e 7368  print(past[1].sh
+00018eb0: 6170 6529 0a20 2020 2020 2020 2020 2020  ape).           
+00018ec0: 2028 322c 2032 2c20 3136 2c20 3429 0a20   (2, 2, 16, 4). 
+00018ed0: 2020 2020 2020 2020 2020 203e 3e3e 2065             >>> e
+00018ee0: 6e63 6f64 6572 5f69 6e70 7574 5f76 616c  ncoder_input_val
+00018ef0: 7565 203d 2054 656e 736f 7228 6e70 2e6f  ue = Tensor(np.o
+00018f00: 6e65 7328 2832 2c20 312c 2038 2929 2c20  nes((2, 1, 8)), 
+00018f10: 6d73 7479 7065 2e66 6c6f 6174 3332 290a  mstype.float32).
+00018f20: 2020 2020 2020 2020 2020 2020 3e3e 3e20              >>> 
+00018f30: 656e 636f 6465 725f 696e 7075 745f 6d61  encoder_input_ma
+00018f40: 736b 203d 2054 656e 736f 7228 6e70 2e6f  sk = Tensor(np.o
+00018f50: 6e65 7328 2832 2c20 312c 2031 3629 292c  nes((2, 1, 16)),
+00018f60: 206d 7374 7970 652e 666c 6f61 7431 3629   mstype.float16)
+00018f70: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
+00018f80: 2069 6e69 745f 7265 7365 7420 3d20 5465   init_reset = Te
+00018f90: 6e73 6f72 285b 4661 6c73 655d 2c20 6d73  nsor([False], ms
+00018fa0: 7479 7065 2e62 6f6f 6c5f 290a 2020 2020  type.bool_).    
+00018fb0: 2020 2020 2020 2020 3e3e 3e20 2320 5374          >>> # St
+00018fc0: 6570 2032 3a20 7365 7420 6973 5f66 6972  ep 2: set is_fir
+00018fd0: 7374 5f69 7465 7261 7469 6f6e 3d46 616c  st_iteration=Fal
+00018fe0: 7365 2c20 616e 6420 7061 7373 2074 6865  se, and pass the
+00018ff0: 2073 696e 676c 6520 776f 7264 2074 6f20   single word to 
+00019000: 7275 6e20 7468 6520 7072 6564 6963 7469  run the predicti
+00019010: 6f6e 2072 6174 6865 7220 7468 616e 0a20  on rather than. 
+00019020: 2020 2020 2020 2020 2020 203e 3e3e 2023             >>> #
+00019030: 2074 6865 2066 756c 6c20 7365 7175 656e   the full sequen
+00019040: 6365 2e0a 2020 2020 2020 2020 2020 2020  ce..            
+00019050: 3e3e 3e20 6d6f 6465 6c2e 6164 645f 666c  >>> model.add_fl
+00019060: 6167 735f 7265 6375 7273 6976 6528 6973  ags_recursive(is
+00019070: 5f66 6972 7374 5f69 7465 7261 7469 6f6e  _first_iteration
+00019080: 3d46 616c 7365 290a 2020 2020 2020 2020  =False).        
+00019090: 2020 2020 3e3e 3e20 6869 6464 656e 2c20      >>> hidden, 
+000190a0: 7061 7374 203d 206d 6f64 656c 2865 6e63  past = model(enc
+000190b0: 6f64 6572 5f69 6e70 7574 5f76 616c 7565  oder_input_value
+000190c0: 2c20 656e 636f 6465 725f 696e 7075 745f  , encoder_input_
+000190d0: 6d61 736b 2c20 696e 6974 5f72 6573 6574  mask, init_reset
+000190e0: 2c20 6261 7463 685f 7661 6c69 645f 6c65  , batch_valid_le
+000190f0: 6e67 7468 290a 2020 2020 2020 2020 2020  ngth).          
+00019100: 2020 3e3e 3e20 7072 696e 7428 6869 6464    >>> print(hidd
+00019110: 656e 2e73 6861 7065 290a 2020 2020 2020  en.shape).      
+00019120: 2020 2020 2020 2832 2c20 312c 2038 290a        (2, 1, 8).
+00019130: 2020 2020 2020 2020 2020 2020 3e3e 3e20              >>> 
+00019140: 7072 696e 7428 7061 7374 5b30 5d2e 7368  print(past[0].sh
+00019150: 6170 6529 0a20 2020 2020 2020 2020 2020  ape).           
+00019160: 2028 322c 2032 2c20 342c 2031 3629 0a20   (2, 2, 4, 16). 
+00019170: 2020 2020 2020 2020 2020 203e 3e3e 2070             >>> p
+00019180: 7269 6e74 2870 6173 745b 315d 2e73 6861  rint(past[1].sha
+00019190: 7065 290a 2020 2020 2020 2020 2020 2020  pe).            
+000191a0: 2832 2c20 322c 2031 362c 2034 290a 2020  (2, 2, 16, 4).  
+000191b0: 2020 2222 220a 0a20 2020 2040 5f4c 6f67    """..    @_Log
+000191c0: 4163 7469 6f6e 4f6e 6365 286d 5f6c 6f67  ActionOnce(m_log
+000191d0: 6765 723d 6c6f 6767 6572 2c20 6b65 793d  ger=logger, key=
+000191e0: 2754 7261 6e73 666f 726d 6572 456e 636f  'TransformerEnco
+000191f0: 6465 724c 6179 6572 272c 0a20 2020 2020  derLayer',.     
+00019200: 2020 2020 2020 2020 2020 2020 2020 206e                 n
+00019210: 6f5f 7761 726e 696e 673d 5f67 6574 5f70  o_warning=_get_p
+00019220: 6172 616c 6c65 6c5f 6d6f 6465 2829 2069  arallel_mode() i
+00019230: 6e20 2850 6172 616c 6c65 6c4d 6f64 652e  n (ParallelMode.
+00019240: 5354 414e 445f 414c 4f4e 452c 2929 0a20  STAND_ALONE,)). 
+00019250: 2020 2040 5f61 7267 735f 7479 7065 5f76     @_args_type_v
+00019260: 616c 6964 6174 6f72 5f63 6865 636b 2868  alidator_check(h
+00019270: 6964 6465 6e5f 7369 7a65 3d56 616c 6964  idden_size=Valid
+00019280: 6174 6f72 2e63 6865 636b 5f70 6f73 6974  ator.check_posit
+00019290: 6976 655f 696e 742c 0a20 2020 2020 2020  ive_int,.       
 000192a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000192b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000192c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000192d0: 2020 2020 2020 2020 2020 2020 2020 206d                 m
-000192e0: 7374 7970 652e 666c 6f61 7431 362c 206d  stype.float16, m
-000192f0: 7374 7970 652e 6266 6c6f 6174 3136 5d2c  stype.bfloat16],
-00019300: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00019310: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019320: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000192b0: 2020 2020 2020 2020 206e 756d 5f68 6561           num_hea
+000192c0: 6473 3d56 616c 6964 6174 6f72 2e63 6865  ds=Validator.che
+000192d0: 636b 5f70 6f73 6974 6976 655f 696e 742c  ck_positive_int,
+000192e0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+000192f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019300: 2066 666e 5f68 6964 6465 6e5f 7369 7a65   ffn_hidden_size
+00019310: 3d56 616c 6964 6174 6f72 2e63 6865 636b  =Validator.check
+00019320: 5f70 6f73 6974 6976 655f 696e 742c 0a20  _positive_int,. 
 00019330: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019340: 2020 2020 2020 2020 2020 2254 7261 6e73            "Trans
-00019350: 666f 726d 6572 456e 636f 6465 724c 6179  formerEncoderLay
-00019360: 6572 2229 2c0a 2020 2020 2020 2020 2020  er"),.          
-00019370: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019380: 2020 2020 2020 7061 7261 6d5f 696e 6974        param_init
-00019390: 5f74 7970 653d 5f76 616c 6964 5f76 616c  _type=_valid_val
-000193a0: 7565 5f63 6865 636b 7328 5b6d 7374 7970  ue_checks([mstyp
-000193b0: 652e 666c 6f61 7433 322c 206d 7374 7970  e.float32, mstyp
-000193c0: 652e 666c 6f61 7431 362c 206d 7374 7970  e.float16, mstyp
-000193d0: 652e 6266 6c6f 6174 3136 5d2c 0a20 2020  e.bfloat16],.   
+00019340: 2020 2020 2020 2020 2020 2020 2020 2073                 s
+00019350: 6571 5f6c 656e 6774 683d 5661 6c69 6461  eq_length=Valida
+00019360: 746f 722e 6368 6563 6b5f 706f 7369 7469  tor.check_positi
+00019370: 7665 5f69 6e74 2c0a 2020 2020 2020 2020  ve_int,.        
+00019380: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019390: 2020 2020 2020 2020 6174 7465 6e74 696f          attentio
+000193a0: 6e5f 6472 6f70 6f75 745f 7261 7465 3d56  n_dropout_rate=V
+000193b0: 616c 6964 6174 6f72 2e63 6865 636b 5f6e  alidator.check_n
+000193c0: 6f6e 5f6e 6567 6174 6976 655f 666c 6f61  on_negative_floa
+000193d0: 742c 0a20 2020 2020 2020 2020 2020 2020  t,.             
 000193e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000193f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019400: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019410: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019420: 2022 5472 616e 7366 6f72 6d65 7245 6e63   "TransformerEnc
-00019430: 6f64 6572 4c61 7965 7222 292c 0a20 2020  oderLayer"),.   
-00019440: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019450: 2020 2020 2020 2020 2020 2020 2063 6f6d               com
-00019460: 7075 7465 5f64 7479 7065 3d5f 7661 6c69  pute_dtype=_vali
-00019470: 645f 7661 6c75 655f 6368 6563 6b73 285b  d_value_checks([
-00019480: 6d73 7479 7065 2e66 6c6f 6174 3332 2c20  mstype.float32, 
-00019490: 6d73 7479 7065 2e66 6c6f 6174 3136 2c20  mstype.float16, 
-000194a0: 6d73 7479 7065 2e62 666c 6f61 7431 365d  mstype.bfloat16]
-000194b0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-000194c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000194d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000193f0: 2020 2068 6964 6465 6e5f 6472 6f70 6f75     hidden_dropou
+00019400: 745f 7261 7465 3d56 616c 6964 6174 6f72  t_rate=Validator
+00019410: 2e63 6865 636b 5f6e 6f6e 5f6e 6567 6174  .check_non_negat
+00019420: 6976 655f 666c 6f61 742c 0a20 2020 2020  ive_float,.     
+00019430: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019440: 2020 2020 2020 2020 2020 2070 6f73 745f             post_
+00019450: 6c61 7965 726e 6f72 6d5f 7265 7369 6475  layernorm_residu
+00019460: 616c 3d56 616c 6964 6174 6f72 2e63 6865  al=Validator.che
+00019470: 636b 5f62 6f6f 6c2c 0a20 2020 2020 2020  ck_bool,.       
+00019480: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019490: 2020 2020 2020 2020 206c 6179 6572 6e6f           layerno
+000194a0: 726d 5f63 6f6d 7075 7465 5f74 7970 653d  rm_compute_type=
+000194b0: 5f76 616c 6964 5f76 616c 7565 5f63 6865  _valid_value_che
+000194c0: 636b 7328 5b6d 7374 7970 652e 666c 6f61  cks([mstype.floa
+000194d0: 7433 322c 0a20 2020 2020 2020 2020 2020  t32,.           
 000194e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000194f0: 2020 2020 2254 7261 6e73 666f 726d 6572      "Transformer
-00019500: 456e 636f 6465 724c 6179 6572 2229 2c0a  EncoderLayer"),.
+000194f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019500: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00019510: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019520: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019530: 7061 7261 6c6c 656c 5f63 6f6e 6669 673d  parallel_config=
-00019540: 5f76 616c 6964 5f74 7970 655f 6368 6563  _valid_type_chec
-00019550: 6b73 285b 4f70 5061 7261 6c6c 656c 436f  ks([OpParallelCo
-00019560: 6e66 6967 2c20 4d6f 4550 6172 616c 6c65  nfig, MoEParalle
-00019570: 6c43 6f6e 6669 675d 2c0a 2020 2020 2020  lConfig],.      
-00019580: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019590: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000195a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000195b0: 2020 2020 2020 2020 2020 2020 2022 5472               "Tr
-000195c0: 616e 7366 6f72 6d65 7245 6e63 6f64 6572  ansformerEncoder
-000195d0: 4c61 7965 7222 292c 0a20 2020 2020 2020  Layer"),.       
-000195e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000195f0: 2020 2020 2020 2020 2075 7365 5f70 6173           use_pas
-00019600: 743d 5661 6c69 6461 746f 722e 6368 6563  t=Validator.chec
-00019610: 6b5f 626f 6f6c 2c0a 2020 2020 2020 2020  k_bool,.        
+00019520: 206d 7374 7970 652e 666c 6f61 7431 362c   mstype.float16,
+00019530: 206d 7374 7970 652e 6266 6c6f 6174 3136   mstype.bfloat16
+00019540: 5d2c 0a20 2020 2020 2020 2020 2020 2020  ],.             
+00019550: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019560: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019570: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019580: 2020 2020 2020 2020 2020 2020 2020 2254                "T
+00019590: 7261 6e73 666f 726d 6572 456e 636f 6465  ransformerEncode
+000195a0: 724c 6179 6572 2229 2c0a 2020 2020 2020  rLayer"),.      
+000195b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000195c0: 2020 2020 2020 2020 2020 736f 6674 6d61            softma
+000195d0: 785f 636f 6d70 7574 655f 7479 7065 3d5f  x_compute_type=_
+000195e0: 7661 6c69 645f 7661 6c75 655f 6368 6563  valid_value_chec
+000195f0: 6b73 285b 6d73 7479 7065 2e66 6c6f 6174  ks([mstype.float
+00019600: 3332 2c0a 2020 2020 2020 2020 2020 2020  32,.            
+00019610: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00019620: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019630: 2020 2020 2020 2020 7573 655f 666c 6173          use_flas
-00019640: 685f 6174 7465 6e74 696f 6e3d 5661 6c69  h_attention=Vali
-00019650: 6461 746f 722e 6368 6563 6b5f 626f 6f6c  dator.check_bool
-00019660: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00019630: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019640: 2020 2020 2020 2020 2020 2020 2020 6d73                ms
+00019650: 7479 7065 2e66 6c6f 6174 3136 2c20 6d73  type.float16, ms
+00019660: 7479 7065 2e62 666c 6f61 7431 365d 2c0a  type.bfloat16],.
 00019670: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019680: 2020 7573 655f 7072 6f6d 7074 5f66 6c61    use_prompt_fla
-00019690: 7368 5f61 7474 656e 7469 6f6e 3d56 616c  sh_attention=Val
-000196a0: 6964 6174 6f72 2e63 6865 636b 5f62 6f6f  idator.check_boo
-000196b0: 6c29 0a20 2020 2064 6566 205f 5f69 6e69  l).    def __ini
-000196c0: 745f 5f28 7365 6c66 2c0a 2020 2020 2020  t__(self,.      
-000196d0: 2020 2020 2020 2020 2020 2062 6174 6368             batch
-000196e0: 5f73 697a 652c 0a20 2020 2020 2020 2020  _size,.         
-000196f0: 2020 2020 2020 2020 6869 6464 656e 5f73          hidden_s
-00019700: 697a 652c 0a20 2020 2020 2020 2020 2020  ize,.           
-00019710: 2020 2020 2020 6666 6e5f 6869 6464 656e        ffn_hidden
-00019720: 5f73 697a 652c 0a20 2020 2020 2020 2020  _size,.         
-00019730: 2020 2020 2020 2020 6e75 6d5f 6865 6164          num_head
-00019740: 732c 0a20 2020 2020 2020 2020 2020 2020  s,.             
-00019750: 2020 2020 7365 715f 6c65 6e67 7468 2c0a      seq_length,.
+00019680: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019690: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000196a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000196b0: 2020 2020 2020 2020 2022 5472 616e 7366           "Transf
+000196c0: 6f72 6d65 7245 6e63 6f64 6572 4c61 7965  ormerEncoderLaye
+000196d0: 7222 292c 0a20 2020 2020 2020 2020 2020  r"),.           
+000196e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000196f0: 2020 2020 2070 6172 616d 5f69 6e69 745f       param_init_
+00019700: 7479 7065 3d5f 7661 6c69 645f 7661 6c75  type=_valid_valu
+00019710: 655f 6368 6563 6b73 285b 6d73 7479 7065  e_checks([mstype
+00019720: 2e66 6c6f 6174 3332 2c20 6d73 7479 7065  .float32, mstype
+00019730: 2e66 6c6f 6174 3136 2c20 6d73 7479 7065  .float16, mstype
+00019740: 2e62 666c 6f61 7431 365d 2c0a 2020 2020  .bfloat16],.    
+00019750: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00019760: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019770: 2061 7474 656e 7469 6f6e 5f64 726f 706f   attention_dropo
-00019780: 7574 5f72 6174 653d 302e 312c 0a20 2020  ut_rate=0.1,.   
-00019790: 2020 2020 2020 2020 2020 2020 2020 6869                hi
-000197a0: 6464 656e 5f64 726f 706f 7574 5f72 6174  dden_dropout_rat
-000197b0: 653d 302e 312c 0a20 2020 2020 2020 2020  e=0.1,.         
-000197c0: 2020 2020 2020 2020 706f 7374 5f6c 6179          post_lay
-000197d0: 6572 6e6f 726d 5f72 6573 6964 7561 6c3d  ernorm_residual=
-000197e0: 4661 6c73 652c 0a20 2020 2020 2020 2020  False,.         
-000197f0: 2020 2020 2020 2020 6c61 7965 726e 6f72          layernor
-00019800: 6d5f 636f 6d70 7574 655f 7479 7065 3d6d  m_compute_type=m
-00019810: 7374 7970 652e 666c 6f61 7433 322c 0a20  stype.float32,. 
+00019770: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019780: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019790: 2254 7261 6e73 666f 726d 6572 456e 636f  "TransformerEnco
+000197a0: 6465 724c 6179 6572 2229 2c0a 2020 2020  derLayer"),.    
+000197b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000197c0: 2020 2020 2020 2020 2020 2020 7061 7261              para
+000197d0: 6c6c 656c 5f63 6f6e 6669 673d 5f76 616c  llel_config=_val
+000197e0: 6964 5f74 7970 655f 6368 6563 6b73 285b  id_type_checks([
+000197f0: 4f70 5061 7261 6c6c 656c 436f 6e66 6967  OpParallelConfig
+00019800: 2c20 4d6f 4550 6172 616c 6c65 6c43 6f6e  , MoEParallelCon
+00019810: 6669 675d 2c0a 2020 2020 2020 2020 2020  fig],.          
 00019820: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019830: 736f 6674 6d61 785f 636f 6d70 7574 655f  softmax_compute_
-00019840: 7479 7065 3d6d 7374 7970 652e 666c 6f61  type=mstype.floa
-00019850: 7433 322c 0a20 2020 2020 2020 2020 2020  t32,.           
-00019860: 2020 2020 2020 7061 7261 6d5f 696e 6974        param_init
-00019870: 5f74 7970 653d 6d73 7479 7065 2e66 6c6f  _type=mstype.flo
-00019880: 6174 3332 2c0a 2020 2020 2020 2020 2020  at32,.          
-00019890: 2020 2020 2020 2068 6964 6465 6e5f 6163         hidden_ac
-000198a0: 743d 2767 656c 7527 2c0a 2020 2020 2020  t='gelu',.      
-000198b0: 2020 2020 2020 2020 2020 2075 7365 5f70             use_p
-000198c0: 6173 743d 4661 6c73 652c 0a20 2020 2020  ast=False,.     
-000198d0: 2020 2020 2020 2020 2020 2020 6d6f 655f              moe_
-000198e0: 636f 6e66 6967 3d64 6566 6175 6c74 5f6d  config=default_m
-000198f0: 6f65 5f63 6f6e 6669 672c 0a20 2020 2020  oe_config,.     
-00019900: 2020 2020 2020 2020 2020 2020 7061 7261              para
-00019910: 6c6c 656c 5f63 6f6e 6669 673d 6465 6661  llel_config=defa
-00019920: 756c 745f 6470 6d70 5f63 6f6e 6669 672c  ult_dpmp_config,
-00019930: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00019940: 2020 7573 655f 666c 6173 685f 6174 7465    use_flash_atte
-00019950: 6e74 696f 6e3d 4661 6c73 652c 0a20 2020  ntion=False,.   
-00019960: 2020 2020 2020 2020 2020 2020 2020 7573                us
-00019970: 655f 7072 6f6d 7074 5f66 6c61 7368 5f61  e_prompt_flash_a
-00019980: 7474 656e 7469 6f6e 3d46 616c 7365 2c0a  ttention=False,.
-00019990: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000199a0: 2063 6f6d 7075 7465 5f64 7479 7065 3d6d   compute_dtype=m
-000199b0: 7374 7970 652e 666c 6f61 7431 3629 3a0a  stype.float16):.
-000199c0: 2020 2020 2020 2020 7375 7065 7228 5472          super(Tr
-000199d0: 616e 7366 6f72 6d65 7245 6e63 6f64 6572  ansformerEncoder
-000199e0: 4c61 7965 722c 2073 656c 6629 2e5f 5f69  Layer, self).__i
-000199f0: 6e69 745f 5f28 290a 2020 2020 2020 2020  nit__().        
-00019a00: 6966 2062 6174 6368 5f73 697a 6520 6f72  if batch_size or
-00019a10: 2075 7365 5f70 6173 743a 0a20 2020 2020   use_past:.     
-00019a20: 2020 2020 2020 2056 616c 6964 6174 6f72         Validator
-00019a30: 2e63 6865 636b 5f70 6f73 6974 6976 655f  .check_positive_
-00019a40: 696e 7428 6261 7463 685f 7369 7a65 290a  int(batch_size).
-00019a50: 2020 2020 2020 2020 7365 6c66 2e62 6174          self.bat
-00019a60: 6368 5f73 697a 6520 3d20 6261 7463 685f  ch_size = batch_
-00019a70: 7369 7a65 0a20 2020 2020 2020 2069 6620  size.        if 
-00019a80: 5f67 6574 5f70 6172 616c 6c65 6c5f 6d6f  _get_parallel_mo
-00019a90: 6465 2829 2069 6e20 2850 6172 616c 6c65  de() in (Paralle
-00019aa0: 6c4d 6f64 652e 4155 544f 5f50 4152 414c  lMode.AUTO_PARAL
-00019ab0: 4c45 4c2c 293a 0a20 2020 2020 2020 2020  LEL,):.         
-00019ac0: 2020 205f 6368 6563 6b5f 636f 6e66 6967     _check_config
-00019ad0: 2870 6172 616c 6c65 6c5f 636f 6e66 6967  (parallel_config
-00019ae0: 290a 2020 2020 2020 2020 2020 2020 6966  ).            if
-00019af0: 206e 756d 5f68 6561 6473 2025 2070 6172   num_heads % par
-00019b00: 616c 6c65 6c5f 636f 6e66 6967 2e6d 6f64  allel_config.mod
-00019b10: 656c 5f70 6172 616c 6c65 6c20 213d 2030  el_parallel != 0
-00019b20: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
-00019b30: 2020 7261 6973 6520 5661 6c75 6545 7272    raise ValueErr
-00019b40: 6f72 280a 2020 2020 2020 2020 2020 2020  or(.            
-00019b50: 2020 2020 2020 2020 2246 6f72 2027 5472          "For 'Tr
-00019b60: 616e 7366 6f72 6d65 7245 6e63 6f64 6572  ansformerEncoder
-00019b70: 4c61 7965 7227 2c20 7468 6520 636c 6173  Layer', the clas
-00019b80: 7320 7661 7269 6162 6c65 2027 6e75 6d5f  s variable 'num_
-00019b90: 6865 6164 7327 206d 7573 7420 6265 2064  heads' must be d
-00019ba0: 6976 6973 6962 6c65 6420 6279 2074 6865  ivisibled by the
-00019bb0: 2022 0a20 2020 2020 2020 2020 2020 2020   ".             
-00019bc0: 2020 2020 2020 2022 2770 6172 616c 6c65         "'paralle
-00019bd0: 6c5f 636f 6e66 6967 2e6d 6f64 656c 5f70  l_config.model_p
-00019be0: 6172 616c 6c65 6c27 2c20 6275 7420 676f  arallel', but go
-00019bf0: 7420 7468 6520 6e75 6d5f 6865 6164 7320  t the num_heads 
-00019c00: 6973 207b 7d20 616e 6420 220a 2020 2020  is {} and ".    
-00019c10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019c20: 2270 6172 616c 6c65 6c5f 636f 6e66 6967  "parallel_config
-00019c30: 2e6d 6f64 656c 5f70 6172 616c 6c65 6c20  .model_parallel 
-00019c40: 6973 207b 7d2e 222e 666f 726d 6174 286e  is {}.".format(n
-00019c50: 756d 5f68 6561 6473 2c20 7061 7261 6c6c  um_heads, parall
-00019c60: 656c 5f63 6f6e 6669 672e 6d6f 6465 6c5f  el_config.model_
-00019c70: 7061 7261 6c6c 656c 2929 0a20 2020 2020  parallel)).     
-00019c80: 2020 2020 2020 2069 6620 6869 6464 656e         if hidden
-00019c90: 5f73 697a 6520 2520 7061 7261 6c6c 656c  _size % parallel
-00019ca0: 5f63 6f6e 6669 672e 6d6f 6465 6c5f 7061  _config.model_pa
-00019cb0: 7261 6c6c 656c 2021 3d20 303a 0a20 2020  rallel != 0:.   
-00019cc0: 2020 2020 2020 2020 2020 2020 2072 6169               rai
-00019cd0: 7365 2056 616c 7565 4572 726f 7228 0a20  se ValueError(. 
-00019ce0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019cf0: 2020 2022 466f 7220 2754 7261 6e73 666f     "For 'Transfo
-00019d00: 726d 6572 456e 636f 6465 724c 6179 6572  rmerEncoderLayer
-00019d10: 272c 2074 6865 2063 6c61 7373 2076 6172  ', the class var
-00019d20: 6961 626c 6520 2768 6964 6465 6e5f 7369  iable 'hidden_si
-00019d30: 7a65 2720 6d75 7374 2062 6520 6469 7669  ze' must be divi
-00019d40: 7369 626c 6564 2062 7920 220a 2020 2020  sibled by ".    
-00019d50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00019d60: 2274 6865 2027 7061 7261 6c6c 656c 5f63  "the 'parallel_c
-00019d70: 6f6e 6669 672e 6d6f 6465 6c5f 7061 7261  onfig.model_para
-00019d80: 6c6c 656c 272c 2062 7574 2067 6f74 2074  llel', but got t
-00019d90: 6865 2068 6964 6465 6e5f 7369 7a65 2069  he hidden_size i
-00019da0: 7320 7b7d 2061 6e64 2070 6172 616c 6c65  s {} and paralle
-00019db0: 6c5f 636f 6e66 6967 2e22 0a20 2020 2020  l_config.".     
-00019dc0: 2020 2020 2020 2020 2020 2020 2020 2022                 "
-00019dd0: 206d 6f64 656c 5f70 6172 616c 6c65 6c20   model_parallel 
-00019de0: 6973 207b 7d2e 222e 666f 726d 6174 2868  is {}.".format(h
-00019df0: 6964 6465 6e5f 7369 7a65 2c20 7061 7261  idden_size, para
-00019e00: 6c6c 656c 5f63 6f6e 6669 672e 6d6f 6465  llel_config.mode
-00019e10: 6c5f 7061 7261 6c6c 656c 2929 0a20 2020  l_parallel)).   
-00019e20: 2020 2020 2020 2020 2069 6620 6666 6e5f           if ffn_
-00019e30: 6869 6464 656e 5f73 697a 6520 2520 7061  hidden_size % pa
-00019e40: 7261 6c6c 656c 5f63 6f6e 6669 672e 6d6f  rallel_config.mo
-00019e50: 6465 6c5f 7061 7261 6c6c 656c 2021 3d20  del_parallel != 
-00019e60: 303a 0a20 2020 2020 2020 2020 2020 2020  0:.             
-00019e70: 2020 2072 6169 7365 2056 616c 7565 4572     raise ValueEr
-00019e80: 726f 7228 0a20 2020 2020 2020 2020 2020  ror(.           
-00019e90: 2020 2020 2020 2020 2022 466f 7220 2754           "For 'T
-00019ea0: 7261 6e73 666f 726d 6572 456e 636f 6465  ransformerEncode
-00019eb0: 724c 6179 6572 272c 2074 6865 2063 6c61  rLayer', the cla
-00019ec0: 7373 2076 6172 6961 626c 6520 2766 666e  ss variable 'ffn
-00019ed0: 5f68 6964 6465 6e5f 7369 7a65 2720 6d75  _hidden_size' mu
-00019ee0: 7374 2062 6520 6469 7669 7369 626c 6564  st be divisibled
-00019ef0: 2022 0a20 2020 2020 2020 2020 2020 2020   ".             
-00019f00: 2020 2020 2020 2022 6279 2074 6865 2027         "by the '
+00019830: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019840: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019850: 2020 2020 2020 2020 2022 5472 616e 7366           "Transf
+00019860: 6f72 6d65 7245 6e63 6f64 6572 4c61 7965  ormerEncoderLaye
+00019870: 7222 292c 0a20 2020 2020 2020 2020 2020  r"),.           
+00019880: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019890: 2020 2020 2075 7365 5f70 6173 743d 5661       use_past=Va
+000198a0: 6c69 6461 746f 722e 6368 6563 6b5f 626f  lidator.check_bo
+000198b0: 6f6c 2c0a 2020 2020 2020 2020 2020 2020  ol,.            
+000198c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000198d0: 2020 2020 7573 655f 666c 6173 685f 6174      use_flash_at
+000198e0: 7465 6e74 696f 6e3d 5661 6c69 6461 746f  tention=Validato
+000198f0: 722e 6368 6563 6b5f 626f 6f6c 2c0a 2020  r.check_bool,.  
+00019900: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019910: 2020 2020 2020 2020 2020 2020 2020 7573                us
+00019920: 655f 7072 6f6d 7074 5f66 6c61 7368 5f61  e_prompt_flash_a
+00019930: 7474 656e 7469 6f6e 3d56 616c 6964 6174  ttention=Validat
+00019940: 6f72 2e63 6865 636b 5f62 6f6f 6c2c 0a20  or.check_bool,. 
+00019950: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019960: 2020 2020 2020 2020 2020 2020 2020 2075                 u
+00019970: 7365 5f69 6e63 7265 5f66 6c61 7368 5f61  se_incre_flash_a
+00019980: 7474 656e 7469 6f6e 3d56 616c 6964 6174  ttention=Validat
+00019990: 6f72 2e63 6865 636b 5f62 6f6f 6c29 0a20  or.check_bool). 
+000199a0: 2020 2064 6566 205f 5f69 6e69 745f 5f28     def __init__(
+000199b0: 7365 6c66 2c0a 2020 2020 2020 2020 2020  self,.          
+000199c0: 2020 2020 2020 2062 6174 6368 5f73 697a         batch_siz
+000199d0: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
+000199e0: 2020 2020 6869 6464 656e 5f73 697a 652c      hidden_size,
+000199f0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00019a00: 2020 6666 6e5f 6869 6464 656e 5f73 697a    ffn_hidden_siz
+00019a10: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
+00019a20: 2020 2020 6e75 6d5f 6865 6164 732c 0a20      num_heads,. 
+00019a30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019a40: 7365 715f 6c65 6e67 7468 2c0a 2020 2020  seq_length,.    
+00019a50: 2020 2020 2020 2020 2020 2020 2061 7474               att
+00019a60: 656e 7469 6f6e 5f64 726f 706f 7574 5f72  ention_dropout_r
+00019a70: 6174 653d 302e 312c 0a20 2020 2020 2020  ate=0.1,.       
+00019a80: 2020 2020 2020 2020 2020 6869 6464 656e            hidden
+00019a90: 5f64 726f 706f 7574 5f72 6174 653d 302e  _dropout_rate=0.
+00019aa0: 312c 0a20 2020 2020 2020 2020 2020 2020  1,.             
+00019ab0: 2020 2020 706f 7374 5f6c 6179 6572 6e6f      post_layerno
+00019ac0: 726d 5f72 6573 6964 7561 6c3d 4661 6c73  rm_residual=Fals
+00019ad0: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
+00019ae0: 2020 2020 6c61 7965 726e 6f72 6d5f 636f      layernorm_co
+00019af0: 6d70 7574 655f 7479 7065 3d6d 7374 7970  mpute_type=mstyp
+00019b00: 652e 666c 6f61 7433 322c 0a20 2020 2020  e.float32,.     
+00019b10: 2020 2020 2020 2020 2020 2020 736f 6674              soft
+00019b20: 6d61 785f 636f 6d70 7574 655f 7479 7065  max_compute_type
+00019b30: 3d6d 7374 7970 652e 666c 6f61 7433 322c  =mstype.float32,
+00019b40: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00019b50: 2020 7061 7261 6d5f 696e 6974 5f74 7970    param_init_typ
+00019b60: 653d 6d73 7479 7065 2e66 6c6f 6174 3332  e=mstype.float32
+00019b70: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00019b80: 2020 2068 6964 6465 6e5f 6163 743d 2767     hidden_act='g
+00019b90: 656c 7527 2c0a 2020 2020 2020 2020 2020  elu',.          
+00019ba0: 2020 2020 2020 2075 7365 5f70 6173 743d         use_past=
+00019bb0: 4661 6c73 652c 0a20 2020 2020 2020 2020  False,.         
+00019bc0: 2020 2020 2020 2020 6d6f 655f 636f 6e66          moe_conf
+00019bd0: 6967 3d64 6566 6175 6c74 5f6d 6f65 5f63  ig=default_moe_c
+00019be0: 6f6e 6669 672c 0a20 2020 2020 2020 2020  onfig,.         
+00019bf0: 2020 2020 2020 2020 7061 7261 6c6c 656c          parallel
+00019c00: 5f63 6f6e 6669 673d 6465 6661 756c 745f  _config=default_
+00019c10: 6470 6d70 5f63 6f6e 6669 672c 0a20 2020  dpmp_config,.   
+00019c20: 2020 2020 2020 2020 2020 2020 2020 7573                us
+00019c30: 655f 666c 6173 685f 6174 7465 6e74 696f  e_flash_attentio
+00019c40: 6e3d 4661 6c73 652c 0a20 2020 2020 2020  n=False,.       
+00019c50: 2020 2020 2020 2020 2020 7573 655f 7072            use_pr
+00019c60: 6f6d 7074 5f66 6c61 7368 5f61 7474 656e  ompt_flash_atten
+00019c70: 7469 6f6e 3d46 616c 7365 2c0a 2020 2020  tion=False,.    
+00019c80: 2020 2020 2020 2020 2020 2020 2075 7365               use
+00019c90: 5f69 6e63 7265 5f66 6c61 7368 5f61 7474  _incre_flash_att
+00019ca0: 656e 7469 6f6e 3d46 616c 7365 293a 0a20  ention=False):. 
+00019cb0: 2020 2020 2020 2073 7570 6572 2854 7261         super(Tra
+00019cc0: 6e73 666f 726d 6572 456e 636f 6465 724c  nsformerEncoderL
+00019cd0: 6179 6572 2c20 7365 6c66 292e 5f5f 696e  ayer, self).__in
+00019ce0: 6974 5f5f 2829 0a20 2020 2020 2020 2069  it__().        i
+00019cf0: 6620 6261 7463 685f 7369 7a65 206f 7220  f batch_size or 
+00019d00: 7573 655f 7061 7374 3a0a 2020 2020 2020  use_past:.      
+00019d10: 2020 2020 2020 5661 6c69 6461 746f 722e        Validator.
+00019d20: 6368 6563 6b5f 706f 7369 7469 7665 5f69  check_positive_i
+00019d30: 6e74 2862 6174 6368 5f73 697a 6529 0a20  nt(batch_size). 
+00019d40: 2020 2020 2020 2073 656c 662e 6261 7463         self.batc
+00019d50: 685f 7369 7a65 203d 2062 6174 6368 5f73  h_size = batch_s
+00019d60: 697a 650a 2020 2020 2020 2020 6966 205f  ize.        if _
+00019d70: 6765 745f 7061 7261 6c6c 656c 5f6d 6f64  get_parallel_mod
+00019d80: 6528 2920 696e 2028 5061 7261 6c6c 656c  e() in (Parallel
+00019d90: 4d6f 6465 2e41 5554 4f5f 5041 5241 4c4c  Mode.AUTO_PARALL
+00019da0: 454c 2c29 3a0a 2020 2020 2020 2020 2020  EL,):.          
+00019db0: 2020 5f63 6865 636b 5f63 6f6e 6669 6728    _check_config(
+00019dc0: 7061 7261 6c6c 656c 5f63 6f6e 6669 6729  parallel_config)
+00019dd0: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if 
+00019de0: 6e75 6d5f 6865 6164 7320 2520 7061 7261  num_heads % para
+00019df0: 6c6c 656c 5f63 6f6e 6669 672e 6d6f 6465  llel_config.mode
+00019e00: 6c5f 7061 7261 6c6c 656c 2021 3d20 303a  l_parallel != 0:
+00019e10: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00019e20: 2072 6169 7365 2056 616c 7565 4572 726f   raise ValueErro
+00019e30: 7228 0a20 2020 2020 2020 2020 2020 2020  r(.             
+00019e40: 2020 2020 2020 2022 466f 7220 2754 7261         "For 'Tra
+00019e50: 6e73 666f 726d 6572 456e 636f 6465 724c  nsformerEncoderL
+00019e60: 6179 6572 272c 2074 6865 2063 6c61 7373  ayer', the class
+00019e70: 2076 6172 6961 626c 6520 276e 756d 5f68   variable 'num_h
+00019e80: 6561 6473 2720 6d75 7374 2062 6520 6469  eads' must be di
+00019e90: 7669 7369 626c 6564 2062 7920 7468 6520  visibled by the 
+00019ea0: 220a 2020 2020 2020 2020 2020 2020 2020  ".              
+00019eb0: 2020 2020 2020 2227 7061 7261 6c6c 656c        "'parallel
+00019ec0: 5f63 6f6e 6669 672e 6d6f 6465 6c5f 7061  _config.model_pa
+00019ed0: 7261 6c6c 656c 272c 2062 7574 2067 6f74  rallel', but got
+00019ee0: 2074 6865 206e 756d 5f68 6561 6473 2069   the num_heads i
+00019ef0: 7320 7b7d 2061 6e64 2022 0a20 2020 2020  s {} and ".     
+00019f00: 2020 2020 2020 2020 2020 2020 2020 2022                 "
 00019f10: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
-00019f20: 6d6f 6465 6c5f 7061 7261 6c6c 656c 272c  model_parallel',
-00019f30: 2062 7574 2067 6f74 2074 6865 2066 666e   but got the ffn
-00019f40: 5f68 6964 6465 6e5f 7369 7a65 2069 7320  _hidden_size is 
-00019f50: 7b7d 2022 0a20 2020 2020 2020 2020 2020  {} ".           
-00019f60: 2020 2020 2020 2020 2022 616e 6420 7061           "and pa
-00019f70: 7261 6c6c 656c 5f63 6f6e 6669 672e 206d  rallel_config. m
-00019f80: 6f64 656c 5f70 6172 616c 6c65 6c20 6973  odel_parallel is
-00019f90: 207b 7d2e 220a 2020 2020 2020 2020 2020   {}.".          
-00019fa0: 2020 2020 2020 2020 2020 2e66 6f72 6d61            .forma
-00019fb0: 7428 6666 6e5f 6869 6464 656e 5f73 697a  t(ffn_hidden_siz
-00019fc0: 652c 2070 6172 616c 6c65 6c5f 636f 6e66  e, parallel_conf
-00019fd0: 6967 2e6d 6f64 656c 5f70 6172 616c 6c65  ig.model_paralle
-00019fe0: 6c29 290a 2020 2020 2020 2020 2020 2020  l)).            
-00019ff0: 5f63 6865 636b 5f6d 6f65 5f63 6f6e 6669  _check_moe_confi
-0001a000: 6728 6d6f 655f 636f 6e66 6967 2c20 7061  g(moe_config, pa
-0001a010: 7261 6c6c 656c 5f63 6f6e 6669 6729 0a20  rallel_config). 
-0001a020: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-0001a030: 7573 655f 6d6f 6520 3d20 286d 6f65 5f63  use_moe = (moe_c
-0001a040: 6f6e 6669 672e 6578 7065 7274 5f6e 756d  onfig.expert_num
-0001a050: 203e 2031 290a 2020 2020 2020 2020 2020   > 1).          
-0001a060: 2020 7365 6c66 2e75 7365 5f70 6173 7420    self.use_past 
-0001a070: 3d20 7573 655f 7061 7374 0a20 2020 2020  = use_past.     
-0001a080: 2020 2020 2020 2073 656c 662e 7365 715f         self.seq_
-0001a090: 6c65 6e67 7468 203d 2073 6571 5f6c 656e  length = seq_len
-0001a0a0: 6774 680a 2020 2020 2020 2020 2020 2020  gth.            
-0001a0b0: 7365 6c66 2e68 6964 6465 6e5f 7369 7a65  self.hidden_size
-0001a0c0: 203d 2068 6964 6465 6e5f 7369 7a65 0a20   = hidden_size. 
-0001a0d0: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-0001a0e0: 6c61 7965 726e 6f72 6d31 203d 204c 6179  layernorm1 = Lay
-0001a0f0: 6572 4e6f 726d 2828 6869 6464 656e 5f73  erNorm((hidden_s
-0001a100: 697a 652c 2929 2e74 6f5f 666c 6f61 7428  ize,)).to_float(
-0001a110: 6c61 7965 726e 6f72 6d5f 636f 6d70 7574  layernorm_comput
-0001a120: 655f 7479 7065 290a 2020 2020 2020 2020  e_type).        
-0001a130: 2020 2020 7365 6c66 2e6c 6179 6572 6e6f      self.layerno
-0001a140: 726d 3220 3d20 4c61 7965 724e 6f72 6d28  rm2 = LayerNorm(
-0001a150: 2868 6964 6465 6e5f 7369 7a65 2c29 292e  (hidden_size,)).
-0001a160: 746f 5f66 6c6f 6174 286c 6179 6572 6e6f  to_float(layerno
-0001a170: 726d 5f63 6f6d 7075 7465 5f74 7970 6529  rm_compute_type)
-0001a180: 0a0a 2020 2020 2020 2020 2020 2020 6174  ..            at
-0001a190: 7465 6e74 696f 6e5f 7061 7261 6c6c 656c  tention_parallel
-0001a1a0: 5f63 6f6e 6669 6720 3d20 7061 7261 6c6c  _config = parall
-0001a1b0: 656c 5f63 6f6e 6669 672e 6470 6d70 2069  el_config.dpmp i
-0001a1c0: 6620 7365 6c66 2e75 7365 5f6d 6f65 2065  f self.use_moe e
-0001a1d0: 6c73 6520 7061 7261 6c6c 656c 5f63 6f6e  lse parallel_con
-0001a1e0: 6669 670a 2020 2020 2020 2020 2020 2020  fig.            
-0001a1f0: 7365 6c66 2e61 7474 656e 7469 6f6e 203d  self.attention =
-0001a200: 204d 756c 7469 4865 6164 4174 7465 6e74   MultiHeadAttent
-0001a210: 696f 6e28 6261 7463 685f 7369 7a65 3d62  ion(batch_size=b
-0001a220: 6174 6368 5f73 697a 652c 0a20 2020 2020  atch_size,.     
-0001a230: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001a240: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001a250: 2020 2020 2020 2020 2020 2073 7263 5f73             src_s
-0001a260: 6571 5f6c 656e 6774 683d 7365 715f 6c65  eq_length=seq_le
-0001a270: 6e67 7468 2c0a 2020 2020 2020 2020 2020  ngth,.          
-0001a280: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001a290: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001a2a0: 2020 2020 2020 7467 745f 7365 715f 6c65        tgt_seq_le
-0001a2b0: 6e67 7468 3d73 6571 5f6c 656e 6774 682c  ngth=seq_length,
-0001a2c0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0001a2d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001a2e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001a2f0: 2068 6964 6465 6e5f 7369 7a65 3d68 6964   hidden_size=hid
-0001a300: 6465 6e5f 7369 7a65 2c0a 2020 2020 2020  den_size,.      
-0001a310: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001a320: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001a330: 2020 2020 2020 2020 2020 6e75 6d5f 6865            num_he
-0001a340: 6164 733d 6e75 6d5f 6865 6164 732c 0a20  ads=num_heads,. 
-0001a350: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001a360: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001a370: 2020 2020 2020 2020 2020 2020 2020 2068                 h
-0001a380: 6964 6465 6e5f 6472 6f70 6f75 745f 7261  idden_dropout_ra
-0001a390: 7465 3d68 6964 6465 6e5f 6472 6f70 6f75  te=hidden_dropou
-0001a3a0: 745f 7261 7465 2c0a 2020 2020 2020 2020  t_rate,.        
-0001a3b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001a3c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001a3d0: 2020 2020 2020 2020 6174 7465 6e74 696f          attentio
-0001a3e0: 6e5f 6472 6f70 6f75 745f 7261 7465 3d61  n_dropout_rate=a
-0001a3f0: 7474 656e 7469 6f6e 5f64 726f 706f 7574  ttention_dropout
-0001a400: 5f72 6174 652c 0a20 2020 2020 2020 2020  _rate,.         
-0001a410: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001a420: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001a430: 2020 2020 2020 2073 6f66 746d 6178 5f63         softmax_c
-0001a440: 6f6d 7075 7465 5f74 7970 653d 736f 6674  ompute_type=soft
-0001a450: 6d61 785f 636f 6d70 7574 655f 7479 7065  max_compute_type
-0001a460: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-0001a470: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001a480: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001a490: 2020 7061 7261 6d5f 696e 6974 5f74 7970    param_init_typ
-0001a4a0: 653d 7061 7261 6d5f 696e 6974 5f74 7970  e=param_init_typ
-0001a4b0: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
-0001a4c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001a4d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001a4e0: 2020 2075 7365 5f70 6173 743d 7573 655f     use_past=use_
-0001a4f0: 7061 7374 2c0a 2020 2020 2020 2020 2020  past,.          
-0001a500: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001a510: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001a520: 2020 2020 2020 7061 7261 6c6c 656c 5f63        parallel_c
-0001a530: 6f6e 6669 673d 6174 7465 6e74 696f 6e5f  onfig=attention_
-0001a540: 7061 7261 6c6c 656c 5f63 6f6e 6669 672c  parallel_config,
-0001a550: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0001a560: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019f20: 6d6f 6465 6c5f 7061 7261 6c6c 656c 2069  model_parallel i
+00019f30: 7320 7b7d 2e22 2e66 6f72 6d61 7428 6e75  s {}.".format(nu
+00019f40: 6d5f 6865 6164 732c 2070 6172 616c 6c65  m_heads, paralle
+00019f50: 6c5f 636f 6e66 6967 2e6d 6f64 656c 5f70  l_config.model_p
+00019f60: 6172 616c 6c65 6c29 290a 2020 2020 2020  arallel)).      
+00019f70: 2020 2020 2020 6966 2068 6964 6465 6e5f        if hidden_
+00019f80: 7369 7a65 2025 2070 6172 616c 6c65 6c5f  size % parallel_
+00019f90: 636f 6e66 6967 2e6d 6f64 656c 5f70 6172  config.model_par
+00019fa0: 616c 6c65 6c20 213d 2030 3a0a 2020 2020  allel != 0:.    
+00019fb0: 2020 2020 2020 2020 2020 2020 7261 6973              rais
+00019fc0: 6520 5661 6c75 6545 7272 6f72 280a 2020  e ValueError(.  
+00019fd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00019fe0: 2020 2246 6f72 2027 5472 616e 7366 6f72    "For 'Transfor
+00019ff0: 6d65 7245 6e63 6f64 6572 4c61 7965 7227  merEncoderLayer'
+0001a000: 2c20 7468 6520 636c 6173 7320 7661 7269  , the class vari
+0001a010: 6162 6c65 2027 6869 6464 656e 5f73 697a  able 'hidden_siz
+0001a020: 6527 206d 7573 7420 6265 2064 6976 6973  e' must be divis
+0001a030: 6962 6c65 6420 6279 2022 0a20 2020 2020  ibled by ".     
+0001a040: 2020 2020 2020 2020 2020 2020 2020 2022                 "
+0001a050: 7468 6520 2770 6172 616c 6c65 6c5f 636f  the 'parallel_co
+0001a060: 6e66 6967 2e6d 6f64 656c 5f70 6172 616c  nfig.model_paral
+0001a070: 6c65 6c27 2c20 6275 7420 676f 7420 7468  lel', but got th
+0001a080: 6520 6869 6464 656e 5f73 697a 6520 6973  e hidden_size is
+0001a090: 207b 7d20 616e 6420 7061 7261 6c6c 656c   {} and parallel
+0001a0a0: 5f63 6f6e 6669 672e 220a 2020 2020 2020  _config.".      
+0001a0b0: 2020 2020 2020 2020 2020 2020 2020 2220                " 
+0001a0c0: 6d6f 6465 6c5f 7061 7261 6c6c 656c 2069  model_parallel i
+0001a0d0: 7320 7b7d 2e22 2e66 6f72 6d61 7428 6869  s {}.".format(hi
+0001a0e0: 6464 656e 5f73 697a 652c 2070 6172 616c  dden_size, paral
+0001a0f0: 6c65 6c5f 636f 6e66 6967 2e6d 6f64 656c  lel_config.model
+0001a100: 5f70 6172 616c 6c65 6c29 290a 2020 2020  _parallel)).    
+0001a110: 2020 2020 2020 2020 6966 2066 666e 5f68          if ffn_h
+0001a120: 6964 6465 6e5f 7369 7a65 2025 2070 6172  idden_size % par
+0001a130: 616c 6c65 6c5f 636f 6e66 6967 2e6d 6f64  allel_config.mod
+0001a140: 656c 5f70 6172 616c 6c65 6c20 213d 2030  el_parallel != 0
+0001a150: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
+0001a160: 2020 7261 6973 6520 5661 6c75 6545 7272    raise ValueErr
+0001a170: 6f72 280a 2020 2020 2020 2020 2020 2020  or(.            
+0001a180: 2020 2020 2020 2020 2246 6f72 2027 5472          "For 'Tr
+0001a190: 616e 7366 6f72 6d65 7245 6e63 6f64 6572  ansformerEncoder
+0001a1a0: 4c61 7965 7227 2c20 7468 6520 636c 6173  Layer', the clas
+0001a1b0: 7320 7661 7269 6162 6c65 2027 6666 6e5f  s variable 'ffn_
+0001a1c0: 6869 6464 656e 5f73 697a 6527 206d 7573  hidden_size' mus
+0001a1d0: 7420 6265 2064 6976 6973 6962 6c65 6420  t be divisibled 
+0001a1e0: 220a 2020 2020 2020 2020 2020 2020 2020  ".              
+0001a1f0: 2020 2020 2020 2262 7920 7468 6520 2770        "by the 'p
+0001a200: 6172 616c 6c65 6c5f 636f 6e66 6967 2e6d  arallel_config.m
+0001a210: 6f64 656c 5f70 6172 616c 6c65 6c27 2c20  odel_parallel', 
+0001a220: 6275 7420 676f 7420 7468 6520 6666 6e5f  but got the ffn_
+0001a230: 6869 6464 656e 5f73 697a 6520 6973 207b  hidden_size is {
+0001a240: 7d20 220a 2020 2020 2020 2020 2020 2020  } ".            
+0001a250: 2020 2020 2020 2020 2261 6e64 2070 6172          "and par
+0001a260: 616c 6c65 6c5f 636f 6e66 6967 2e20 6d6f  allel_config. mo
+0001a270: 6465 6c5f 7061 7261 6c6c 656c 2069 7320  del_parallel is 
+0001a280: 7b7d 2e22 0a20 2020 2020 2020 2020 2020  {}.".           
+0001a290: 2020 2020 2020 2020 202e 666f 726d 6174           .format
+0001a2a0: 2866 666e 5f68 6964 6465 6e5f 7369 7a65  (ffn_hidden_size
+0001a2b0: 2c20 7061 7261 6c6c 656c 5f63 6f6e 6669  , parallel_confi
+0001a2c0: 672e 6d6f 6465 6c5f 7061 7261 6c6c 656c  g.model_parallel
+0001a2d0: 2929 0a20 2020 2020 2020 2020 2020 205f  )).            _
+0001a2e0: 6368 6563 6b5f 6d6f 655f 636f 6e66 6967  check_moe_config
+0001a2f0: 286d 6f65 5f63 6f6e 6669 672c 2070 6172  (moe_config, par
+0001a300: 616c 6c65 6c5f 636f 6e66 6967 290a 2020  allel_config).  
+0001a310: 2020 2020 2020 2020 2020 7365 6c66 2e75            self.u
+0001a320: 7365 5f6d 6f65 203d 2028 6d6f 655f 636f  se_moe = (moe_co
+0001a330: 6e66 6967 2e65 7870 6572 745f 6e75 6d20  nfig.expert_num 
+0001a340: 3e20 3129 0a20 2020 2020 2020 2020 2020  > 1).           
+0001a350: 2073 656c 662e 7573 655f 7061 7374 203d   self.use_past =
+0001a360: 2075 7365 5f70 6173 740a 2020 2020 2020   use_past.      
+0001a370: 2020 2020 2020 7365 6c66 2e73 6571 5f6c        self.seq_l
+0001a380: 656e 6774 6820 3d20 7365 715f 6c65 6e67  ength = seq_leng
+0001a390: 7468 0a20 2020 2020 2020 2020 2020 2073  th.            s
+0001a3a0: 656c 662e 6869 6464 656e 5f73 697a 6520  elf.hidden_size 
+0001a3b0: 3d20 6869 6464 656e 5f73 697a 650a 2020  = hidden_size.  
+0001a3c0: 2020 2020 2020 2020 2020 7365 6c66 2e6c            self.l
+0001a3d0: 6179 6572 6e6f 726d 3120 3d20 4c61 7965  ayernorm1 = Laye
+0001a3e0: 724e 6f72 6d28 2868 6964 6465 6e5f 7369  rNorm((hidden_si
+0001a3f0: 7a65 2c29 292e 746f 5f66 6c6f 6174 286c  ze,)).to_float(l
+0001a400: 6179 6572 6e6f 726d 5f63 6f6d 7075 7465  ayernorm_compute
+0001a410: 5f74 7970 6529 0a20 2020 2020 2020 2020  _type).         
+0001a420: 2020 2073 656c 662e 6c61 7965 726e 6f72     self.layernor
+0001a430: 6d32 203d 204c 6179 6572 4e6f 726d 2828  m2 = LayerNorm((
+0001a440: 6869 6464 656e 5f73 697a 652c 2929 2e74  hidden_size,)).t
+0001a450: 6f5f 666c 6f61 7428 6c61 7965 726e 6f72  o_float(layernor
+0001a460: 6d5f 636f 6d70 7574 655f 7479 7065 290a  m_compute_type).
+0001a470: 0a20 2020 2020 2020 2020 2020 2061 7474  .            att
+0001a480: 656e 7469 6f6e 5f70 6172 616c 6c65 6c5f  ention_parallel_
+0001a490: 636f 6e66 6967 203d 2070 6172 616c 6c65  config = paralle
+0001a4a0: 6c5f 636f 6e66 6967 2e64 706d 7020 6966  l_config.dpmp if
+0001a4b0: 2073 656c 662e 7573 655f 6d6f 6520 656c   self.use_moe el
+0001a4c0: 7365 2070 6172 616c 6c65 6c5f 636f 6e66  se parallel_conf
+0001a4d0: 6967 0a20 2020 2020 2020 2020 2020 2073  ig.            s
+0001a4e0: 656c 662e 6174 7465 6e74 696f 6e20 3d20  elf.attention = 
+0001a4f0: 4d75 6c74 6948 6561 6441 7474 656e 7469  MultiHeadAttenti
+0001a500: 6f6e 2862 6174 6368 5f73 697a 653d 6261  on(batch_size=ba
+0001a510: 7463 685f 7369 7a65 2c0a 2020 2020 2020  tch_size,.      
+0001a520: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001a530: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001a540: 2020 2020 2020 2020 2020 7372 635f 7365            src_se
+0001a550: 715f 6c65 6e67 7468 3d73 6571 5f6c 656e  q_length=seq_len
+0001a560: 6774 682c 0a20 2020 2020 2020 2020 2020  gth,.           
 0001a570: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001a580: 2075 7365 5f66 6c61 7368 5f61 7474 656e   use_flash_atten
-0001a590: 7469 6f6e 3d75 7365 5f66 6c61 7368 5f61  tion=use_flash_a
-0001a5a0: 7474 656e 7469 6f6e 2c0a 2020 2020 2020  ttention,.      
+0001a580: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001a590: 2020 2020 2074 6774 5f73 6571 5f6c 656e       tgt_seq_len
+0001a5a0: 6774 683d 7365 715f 6c65 6e67 7468 2c0a  gth=seq_length,.
 0001a5b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 0001a5c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001a5d0: 2020 2020 2020 2020 2020 7573 655f 7072            use_pr
-0001a5e0: 6f6d 7074 5f66 6c61 7368 5f61 7474 656e  ompt_flash_atten
-0001a5f0: 7469 6f6e 3d75 7365 5f70 726f 6d70 745f  tion=use_prompt_
-0001a600: 666c 6173 685f 6174 7465 6e74 696f 6e29  flash_attention)
-0001a610: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if 
-0001a620: 7365 6c66 2e75 7365 5f6d 6f65 3a0a 2020  self.use_moe:.  
-0001a630: 2020 2020 2020 2020 2020 2020 2020 7365                se
-0001a640: 6c66 2e6f 7574 7075 7420 3d20 4d6f 4528  lf.output = MoE(
-0001a650: 6869 6464 656e 5f73 697a 653d 6869 6464  hidden_size=hidd
-0001a660: 656e 5f73 697a 652c 0a20 2020 2020 2020  en_size,.       
-0001a670: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001a680: 2020 2020 2020 2020 2020 2064 726f 706f             dropo
-0001a690: 7574 5f72 6174 653d 6869 6464 656e 5f64  ut_rate=hidden_d
-0001a6a0: 726f 706f 7574 5f72 6174 652c 0a20 2020  ropout_rate,.   
+0001a5d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001a5e0: 6869 6464 656e 5f73 697a 653d 6869 6464  hidden_size=hidd
+0001a5f0: 656e 5f73 697a 652c 0a20 2020 2020 2020  en_size,.       
+0001a600: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001a610: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001a620: 2020 2020 2020 2020 206e 756d 5f68 6561           num_hea
+0001a630: 6473 3d6e 756d 5f68 6561 6473 2c0a 2020  ds=num_heads,.  
+0001a640: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001a650: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001a660: 2020 2020 2020 2020 2020 2020 2020 6869                hi
+0001a670: 6464 656e 5f64 726f 706f 7574 5f72 6174  dden_dropout_rat
+0001a680: 653d 6869 6464 656e 5f64 726f 706f 7574  e=hidden_dropout
+0001a690: 5f72 6174 652c 0a20 2020 2020 2020 2020  _rate,.         
+0001a6a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 0001a6b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001a6c0: 2020 2020 2020 2020 2020 2020 2020 2066                 f
-0001a6d0: 666e 5f68 6964 6465 6e5f 7369 7a65 3d66  fn_hidden_size=f
-0001a6e0: 666e 5f68 6964 6465 6e5f 7369 7a65 2c0a  fn_hidden_size,.
-0001a6f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001a6c0: 2020 2020 2020 2061 7474 656e 7469 6f6e         attention
+0001a6d0: 5f64 726f 706f 7574 5f72 6174 653d 6174  _dropout_rate=at
+0001a6e0: 7465 6e74 696f 6e5f 6472 6f70 6f75 745f  tention_dropout_
+0001a6f0: 7261 7465 2c0a 2020 2020 2020 2020 2020  rate,.          
 0001a700: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001a710: 2020 7061 7261 6d5f 696e 6974 5f74 7970    param_init_typ
-0001a720: 653d 7061 7261 6d5f 696e 6974 5f74 7970  e=param_init_typ
-0001a730: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
-0001a740: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001a750: 2020 2020 2068 6964 6465 6e5f 6163 743d       hidden_act=
-0001a760: 6869 6464 656e 5f61 6374 2c0a 2020 2020  hidden_act,.    
+0001a710: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001a720: 2020 2020 2020 736f 6674 6d61 785f 636f        softmax_co
+0001a730: 6d70 7574 655f 7479 7065 3d73 6f66 746d  mpute_type=softm
+0001a740: 6178 5f63 6f6d 7075 7465 5f74 7970 652c  ax_compute_type,
+0001a750: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0001a760: 2020 2020 2020 2020 2020 2020 2020 2020                  
 0001a770: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001a780: 2020 2020 2020 2020 2020 2020 2020 6d6f                mo
-0001a790: 655f 636f 6e66 6967 3d6d 6f65 5f63 6f6e  e_config=moe_con
-0001a7a0: 6669 672c 0a20 2020 2020 2020 2020 2020  fig,.           
+0001a780: 2070 6172 616d 5f69 6e69 745f 7479 7065   param_init_type
+0001a790: 3d70 6172 616d 5f69 6e69 745f 7479 7065  =param_init_type
+0001a7a0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
 0001a7b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001a7c0: 2020 2020 2020 2070 6172 616c 6c65 6c5f         parallel_
-0001a7d0: 636f 6e66 6967 3d70 6172 616c 6c65 6c5f  config=parallel_
-0001a7e0: 636f 6e66 6967 290a 2020 2020 2020 2020  config).        
-0001a7f0: 2020 2020 656c 7365 3a0a 2020 2020 2020      else:.      
-0001a800: 2020 2020 2020 2020 2020 2320 4665 6564            # Feed
-0001a810: 2046 6f72 7761 7264 204e 6574 776f 726b   Forward Network
-0001a820: 2c20 4646 4e0a 2020 2020 2020 2020 2020  , FFN.          
-0001a830: 2020 2020 2020 7365 6c66 2e6f 7574 7075        self.outpu
-0001a840: 7420 3d20 4665 6564 466f 7277 6172 6428  t = FeedForward(
-0001a850: 6869 6464 656e 5f73 697a 653d 6869 6464  hidden_size=hidd
-0001a860: 656e 5f73 697a 652c 0a20 2020 2020 2020  en_size,.       
-0001a870: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001a880: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001a890: 2020 2064 726f 706f 7574 5f72 6174 653d     dropout_rate=
-0001a8a0: 6869 6464 656e 5f64 726f 706f 7574 5f72  hidden_dropout_r
-0001a8b0: 6174 652c 0a20 2020 2020 2020 2020 2020  ate,.           
-0001a8c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001a8d0: 2020 2020 2020 2020 2020 2020 2020 2066                 f
-0001a8e0: 666e 5f68 6964 6465 6e5f 7369 7a65 3d66  fn_hidden_size=f
-0001a8f0: 666e 5f68 6964 6465 6e5f 7369 7a65 2c0a  fn_hidden_size,.
+0001a7c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001a7d0: 2020 7573 655f 7061 7374 3d75 7365 5f70    use_past=use_p
+0001a7e0: 6173 742c 0a20 2020 2020 2020 2020 2020  ast,.           
+0001a7f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001a800: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001a810: 2020 2020 2070 6172 616c 6c65 6c5f 636f       parallel_co
+0001a820: 6e66 6967 3d61 7474 656e 7469 6f6e 5f70  nfig=attention_p
+0001a830: 6172 616c 6c65 6c5f 636f 6e66 6967 2c0a  arallel_config,.
+0001a840: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001a850: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001a860: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001a870: 7573 655f 666c 6173 685f 6174 7465 6e74  use_flash_attent
+0001a880: 696f 6e3d 7573 655f 666c 6173 685f 6174  ion=use_flash_at
+0001a890: 7465 6e74 696f 6e2c 0a20 2020 2020 2020  tention,.       
+0001a8a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001a8b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001a8c0: 2020 2020 2020 2020 2075 7365 5f70 726f           use_pro
+0001a8d0: 6d70 745f 666c 6173 685f 6174 7465 6e74  mpt_flash_attent
+0001a8e0: 696f 6e3d 7573 655f 7072 6f6d 7074 5f66  ion=use_prompt_f
+0001a8f0: 6c61 7368 5f61 7474 656e 7469 6f6e 2c0a  lash_attention,.
 0001a900: 2020 2020 2020 2020 2020 2020 2020 2020                  
 0001a910: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001a920: 2020 2020 2020 2020 2020 7061 7261 6d5f            param_
-0001a930: 696e 6974 5f74 7970 653d 7061 7261 6d5f  init_type=param_
-0001a940: 696e 6974 5f74 7970 652c 0a20 2020 2020  init_type,.     
-0001a950: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001a960: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001a970: 2020 2020 2063 6f6d 7075 7465 5f64 7479       compute_dty
-0001a980: 7065 3d63 6f6d 7075 7465 5f64 7479 7065  pe=compute_dtype
-0001a990: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-0001a9a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001a9b0: 2020 2020 2020 2020 2020 2020 6869 6464              hidd
-0001a9c0: 656e 5f61 6374 3d68 6964 6465 6e5f 6163  en_act=hidden_ac
-0001a9d0: 742c 0a20 2020 2020 2020 2020 2020 2020  t,.             
-0001a9e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001a9f0: 2020 2020 2020 2020 2020 2020 2070 6172               par
-0001aa00: 616c 6c65 6c5f 636f 6e66 6967 3d70 6172  allel_config=par
-0001aa10: 616c 6c65 6c5f 636f 6e66 6967 290a 2020  allel_config).  
-0001aa20: 2020 2020 2020 2020 2020 7365 6c66 2e70            self.p
-0001aa30: 6f73 745f 6c61 7965 726e 6f72 6d5f 7265  ost_layernorm_re
-0001aa40: 7369 6475 616c 203d 2070 6f73 745f 6c61  sidual = post_la
-0001aa50: 7965 726e 6f72 6d5f 7265 7369 6475 616c  yernorm_residual
-0001aa60: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-0001aa70: 662e 6164 6420 3d20 502e 4164 6428 292e  f.add = P.Add().
-0001aa80: 7368 6172 6428 2828 7061 7261 6c6c 656c  shard(((parallel
-0001aa90: 5f63 6f6e 6669 672e 6461 7461 5f70 6172  _config.data_par
-0001aaa0: 616c 6c65 6c2c 2031 292c 2028 7061 7261  allel, 1), (para
-0001aab0: 6c6c 656c 5f63 6f6e 6669 672e 6461 7461  llel_config.data
-0001aac0: 5f70 6172 616c 6c65 6c2c 2031 2929 290a  _parallel, 1))).
-0001aad0: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-0001aae0: 2e61 6464 5f33 6420 3d20 502e 4164 6428  .add_3d = P.Add(
-0001aaf0: 292e 7368 6172 6428 2828 7061 7261 6c6c  ).shard(((parall
-0001ab00: 656c 5f63 6f6e 6669 672e 6461 7461 5f70  el_config.data_p
-0001ab10: 6172 616c 6c65 6c2c 2031 2c20 3129 2c20  arallel, 1, 1), 
-0001ab20: 2870 6172 616c 6c65 6c5f 636f 6e66 6967  (parallel_config
-0001ab30: 2e64 6174 615f 7061 7261 6c6c 656c 2c20  .data_parallel, 
-0001ab40: 312c 2031 2929 290a 2020 2020 2020 2020  1, 1))).        
-0001ab50: 2020 2020 7365 6c66 2e64 7479 7065 203d      self.dtype =
-0001ab60: 206d 7374 7970 652e 666c 6f61 7431 360a   mstype.float16.
-0001ab70: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-0001ab80: 2e6b 6579 5f70 6173 7420 3d20 4e6f 6e65  .key_past = None
-0001ab90: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-0001aba0: 662e 7661 6c75 655f 7061 7374 203d 204e  f.value_past = N
-0001abb0: 6f6e 650a 0a20 2020 2020 2020 2020 2020  one..           
-0001abc0: 2069 6620 7365 6c66 2e75 7365 5f70 6173   if self.use_pas
-0001abd0: 743a 0a20 2020 2020 2020 2020 2020 2020  t:.             
-0001abe0: 2020 2023 206f 7065 7261 746f 7220 7573     # operator us
-0001abf0: 6564 2066 6f72 2073 7461 7465 2072 6575  ed for state reu
-0001ac00: 7365 0a20 2020 2020 2020 2020 2020 2020  se.             
-0001ac10: 2020 2073 656c 662e 7265 6475 6365 7375     self.reducesu
-0001ac20: 6d20 3d20 502e 5265 6475 6365 5375 6d28  m = P.ReduceSum(
-0001ac30: 292e 7368 6172 6428 2828 312c 2031 2c20  ).shard(((1, 1, 
-0001ac40: 312c 2031 292c 2929 0a20 2020 2020 2020  1, 1),)).       
-0001ac50: 2020 2020 2020 2020 2073 656c 662e 6e6f           self.no
-0001ac60: 745f 6571 7561 6c20 3d20 502e 4e6f 7445  t_equal = P.NotE
-0001ac70: 7175 616c 2829 2e73 6861 7264 2828 2831  qual().shard(((1
-0001ac80: 2c20 312c 2031 2c20 3129 2c20 2829 2929  , 1, 1, 1), ()))
-0001ac90: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0001aca0: 2073 656c 662e 736c 6963 6520 3d20 502e   self.slice = P.
-0001acb0: 5374 7269 6465 6453 6c69 6365 2829 2e73  StridedSlice().s
-0001acc0: 6861 7264 2828 2831 2c20 312c 2031 2c20  hard(((1, 1, 1, 
-0001acd0: 3129 2c29 290a 2020 2020 2020 2020 2020  1),)).          
-0001ace0: 2020 2020 2020 7369 7a65 5f70 6572 5f68        size_per_h
-0001acf0: 6561 6420 3d20 6869 6464 656e 5f73 697a  ead = hidden_siz
-0001ad00: 6520 2f2f 206e 756d 5f68 6561 6473 0a20  e // num_heads. 
-0001ad10: 2020 2020 2020 2020 2020 2020 2020 2073                 s
-0001ad20: 656c 662e 6b65 795f 7368 6170 6520 3d20  elf.key_shape = 
-0001ad30: 2862 6174 6368 5f73 697a 652c 206e 756d  (batch_size, num
-0001ad40: 5f68 6561 6473 2c20 7369 7a65 5f70 6572  _heads, size_per
-0001ad50: 5f68 6561 642c 2073 6571 5f6c 656e 6774  _head, seq_lengt
-0001ad60: 6829 0a20 2020 2020 2020 2020 2020 2020  h).             
-0001ad70: 2020 2073 656c 662e 7661 6c75 655f 7368     self.value_sh
-0001ad80: 6170 6520 3d20 2862 6174 6368 5f73 697a  ape = (batch_siz
-0001ad90: 652c 206e 756d 5f68 6561 6473 2c20 7365  e, num_heads, se
-0001ada0: 715f 6c65 6e67 7468 2c20 7369 7a65 5f70  q_length, size_p
-0001adb0: 6572 5f68 6561 6429 0a20 2020 2020 2020  er_head).       
-0001adc0: 2020 2020 2020 2020 2023 2070 6172 616d           # param
-0001add0: 6574 6572 7320 7361 7669 6e67 206b 6579  eters saving key
-0001ade0: 2061 6e64 2076 616c 7565 2073 7461 7465   and value state
-0001adf0: 730a 2020 2020 2020 2020 2020 2020 2020  s.              
-0001ae00: 2020 7365 6c66 2e6b 6579 5f70 6173 7420    self.key_past 
-0001ae10: 3d20 5061 7261 6d65 7465 7228 5465 6e73  = Parameter(Tens
-0001ae20: 6f72 286e 702e 7a65 726f 7328 7368 6170  or(np.zeros(shap
-0001ae30: 653d 7365 6c66 2e6b 6579 5f73 6861 7065  e=self.key_shape
-0001ae40: 292c 2073 656c 662e 6474 7970 6529 2c20  ), self.dtype), 
-0001ae50: 6e61 6d65 3d22 6b65 795f 7061 7374 2229  name="key_past")
-0001ae60: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0001ae70: 2073 656c 662e 7661 6c75 655f 7061 7374   self.value_past
-0001ae80: 203d 2050 6172 616d 6574 6572 2854 656e   = Parameter(Ten
-0001ae90: 736f 7228 6e70 2e7a 6572 6f73 2873 6861  sor(np.zeros(sha
-0001aea0: 7065 3d73 656c 662e 7661 6c75 655f 7368  pe=self.value_sh
-0001aeb0: 6170 6529 2c20 7365 6c66 2e64 7479 7065  ape), self.dtype
-0001aec0: 292c 206e 616d 653d 2276 616c 7565 5f70  ), name="value_p
-0001aed0: 6173 7422 290a 2020 2020 2020 2020 2020  ast").          
-0001aee0: 2020 2020 2020 7365 6c66 2e74 696c 6520        self.tile 
-0001aef0: 3d20 502e 5469 6c65 2829 2e73 6861 7264  = P.Tile().shard
-0001af00: 2828 2831 2c20 3129 2c29 290a 2020 2020  (((1, 1),)).    
-0001af10: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-0001af20: 2e6d 756c 203d 2050 2e4d 756c 2829 2e73  .mul = P.Mul().s
-0001af30: 6861 7264 2828 2831 2c20 312c 2031 2c20  hard(((1, 1, 1, 
-0001af40: 3129 2c20 2831 2c29 2929 0a20 2020 2020  1), (1,))).     
-0001af50: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-0001af60: 6173 7369 676e 203d 2050 2e41 7373 6967  assign = P.Assig
-0001af70: 6e28 292e 7368 6172 6428 2828 312c 2031  n().shard(((1, 1
-0001af80: 2c20 312c 2031 292c 2028 312c 2031 2c20  , 1, 1), (1, 1, 
-0001af90: 312c 2031 2929 290a 2020 2020 2020 2020  1, 1))).        
-0001afa0: 656c 6966 205f 6765 745f 7061 7261 6c6c  elif _get_parall
-0001afb0: 656c 5f6d 6f64 6528 2920 6e6f 7420 696e  el_mode() not in
-0001afc0: 2028 5061 7261 6c6c 656c 4d6f 6465 2e41   (ParallelMode.A
-0001afd0: 5554 4f5f 5041 5241 4c4c 454c 2c29 3a0a  UTO_PARALLEL,):.
-0001afe0: 2020 2020 2020 2020 2020 2020 5f63 6865              _che
-0001aff0: 636b 5f63 6f6e 6669 6728 7061 7261 6c6c  ck_config(parall
-0001b000: 656c 5f63 6f6e 6669 6729 0a20 2020 2020  el_config).     
-0001b010: 2020 2020 2020 2069 6620 6e75 6d5f 6865         if num_he
-0001b020: 6164 7320 2520 7061 7261 6c6c 656c 5f63  ads % parallel_c
-0001b030: 6f6e 6669 672e 6d6f 6465 6c5f 7061 7261  onfig.model_para
-0001b040: 6c6c 656c 2021 3d20 303a 0a20 2020 2020  llel != 0:.     
-0001b050: 2020 2020 2020 2020 2020 2072 6169 7365             raise
-0001b060: 2056 616c 7565 4572 726f 7228 0a20 2020   ValueError(.   
+0001a920: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001a930: 7573 655f 696e 6372 655f 666c 6173 685f  use_incre_flash_
+0001a940: 6174 7465 6e74 696f 6e3d 7573 655f 696e  attention=use_in
+0001a950: 6372 655f 666c 6173 685f 6174 7465 6e74  cre_flash_attent
+0001a960: 696f 6e29 0a20 2020 2020 2020 2020 2020  ion).           
+0001a970: 2069 6620 7365 6c66 2e75 7365 5f6d 6f65   if self.use_moe
+0001a980: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
+0001a990: 2020 7365 6c66 2e6f 7574 7075 7420 3d20    self.output = 
+0001a9a0: 4d6f 4528 6869 6464 656e 5f73 697a 653d  MoE(hidden_size=
+0001a9b0: 6869 6464 656e 5f73 697a 652c 0a20 2020  hidden_size,.   
+0001a9c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001a9d0: 2020 2020 2020 2020 2020 2020 2020 2064                 d
+0001a9e0: 726f 706f 7574 5f72 6174 653d 6869 6464  ropout_rate=hidd
+0001a9f0: 656e 5f64 726f 706f 7574 5f72 6174 652c  en_dropout_rate,
+0001aa00: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0001aa10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001aa20: 2020 2066 666e 5f68 6964 6465 6e5f 7369     ffn_hidden_si
+0001aa30: 7a65 3d66 666e 5f68 6964 6465 6e5f 7369  ze=ffn_hidden_si
+0001aa40: 7a65 2c0a 2020 2020 2020 2020 2020 2020  ze,.            
+0001aa50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001aa60: 2020 2020 2020 7061 7261 6d5f 696e 6974        param_init
+0001aa70: 5f74 7970 653d 7061 7261 6d5f 696e 6974  _type=param_init
+0001aa80: 5f74 7970 652c 0a20 2020 2020 2020 2020  _type,.         
+0001aa90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001aaa0: 2020 2020 2020 2020 2068 6964 6465 6e5f           hidden_
+0001aab0: 6163 743d 6869 6464 656e 5f61 6374 2c0a  act=hidden_act,.
+0001aac0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001aad0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001aae0: 2020 6d6f 655f 636f 6e66 6967 3d6d 6f65    moe_config=moe
+0001aaf0: 5f63 6f6e 6669 672c 0a20 2020 2020 2020  _config,.       
+0001ab00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001ab10: 2020 2020 2020 2020 2020 2070 6172 616c             paral
+0001ab20: 6c65 6c5f 636f 6e66 6967 3d70 6172 616c  lel_config=paral
+0001ab30: 6c65 6c5f 636f 6e66 6967 290a 2020 2020  lel_config).    
+0001ab40: 2020 2020 2020 2020 656c 7365 3a0a 2020          else:.  
+0001ab50: 2020 2020 2020 2020 2020 2020 2020 2320                # 
+0001ab60: 4665 6564 2046 6f72 7761 7264 204e 6574  Feed Forward Net
+0001ab70: 776f 726b 2c20 4646 4e0a 2020 2020 2020  work, FFN.      
+0001ab80: 2020 2020 2020 2020 2020 7365 6c66 2e6f            self.o
+0001ab90: 7574 7075 7420 3d20 4665 6564 466f 7277  utput = FeedForw
+0001aba0: 6172 6428 6869 6464 656e 5f73 697a 653d  ard(hidden_size=
+0001abb0: 6869 6464 656e 5f73 697a 652c 0a20 2020  hidden_size,.   
+0001abc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001abd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001abe0: 2020 2020 2020 2064 726f 706f 7574 5f72         dropout_r
+0001abf0: 6174 653d 6869 6464 656e 5f64 726f 706f  ate=hidden_dropo
+0001ac00: 7574 5f72 6174 652c 0a20 2020 2020 2020  ut_rate,.       
+0001ac10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001ac20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001ac30: 2020 2066 666e 5f68 6964 6465 6e5f 7369     ffn_hidden_si
+0001ac40: 7a65 3d66 666e 5f68 6964 6465 6e5f 7369  ze=ffn_hidden_si
+0001ac50: 7a65 2c0a 2020 2020 2020 2020 2020 2020  ze,.            
+0001ac60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001ac70: 2020 2020 2020 2020 2020 2020 2020 7061                pa
+0001ac80: 7261 6d5f 696e 6974 5f74 7970 653d 7061  ram_init_type=pa
+0001ac90: 7261 6d5f 696e 6974 5f74 7970 652c 0a20  ram_init_type,. 
+0001aca0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001acb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001acc0: 2020 2020 2020 2020 2068 6964 6465 6e5f           hidden_
+0001acd0: 6163 743d 6869 6464 656e 5f61 6374 2c0a  act=hidden_act,.
+0001ace0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001acf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001ad00: 2020 2020 2020 2020 2020 7061 7261 6c6c            parall
+0001ad10: 656c 5f63 6f6e 6669 673d 7061 7261 6c6c  el_config=parall
+0001ad20: 656c 5f63 6f6e 6669 6729 0a20 2020 2020  el_config).     
+0001ad30: 2020 2020 2020 2073 656c 662e 706f 7374         self.post
+0001ad40: 5f6c 6179 6572 6e6f 726d 5f72 6573 6964  _layernorm_resid
+0001ad50: 7561 6c20 3d20 706f 7374 5f6c 6179 6572  ual = post_layer
+0001ad60: 6e6f 726d 5f72 6573 6964 7561 6c0a 2020  norm_residual.  
+0001ad70: 2020 2020 2020 2020 2020 7365 6c66 2e61            self.a
+0001ad80: 6464 203d 2050 2e41 6464 2829 2e73 6861  dd = P.Add().sha
+0001ad90: 7264 2828 2870 6172 616c 6c65 6c5f 636f  rd(((parallel_co
+0001ada0: 6e66 6967 2e64 6174 615f 7061 7261 6c6c  nfig.data_parall
+0001adb0: 656c 2c20 3129 2c20 2870 6172 616c 6c65  el, 1), (paralle
+0001adc0: 6c5f 636f 6e66 6967 2e64 6174 615f 7061  l_config.data_pa
+0001add0: 7261 6c6c 656c 2c20 3129 2929 0a20 2020  rallel, 1))).   
+0001ade0: 2020 2020 2020 2020 2073 656c 662e 6164           self.ad
+0001adf0: 645f 3364 203d 2050 2e41 6464 2829 2e73  d_3d = P.Add().s
+0001ae00: 6861 7264 2828 2870 6172 616c 6c65 6c5f  hard(((parallel_
+0001ae10: 636f 6e66 6967 2e64 6174 615f 7061 7261  config.data_para
+0001ae20: 6c6c 656c 2c20 312c 2031 292c 2028 7061  llel, 1, 1), (pa
+0001ae30: 7261 6c6c 656c 5f63 6f6e 6669 672e 6461  rallel_config.da
+0001ae40: 7461 5f70 6172 616c 6c65 6c2c 2031 2c20  ta_parallel, 1, 
+0001ae50: 3129 2929 0a20 2020 2020 2020 2020 2020  1))).           
+0001ae60: 2073 656c 662e 6474 7970 6520 3d20 6d73   self.dtype = ms
+0001ae70: 7479 7065 2e66 6c6f 6174 3136 0a20 2020  type.float16.   
+0001ae80: 2020 2020 2020 2020 2073 656c 662e 6b65           self.ke
+0001ae90: 795f 7061 7374 203d 204e 6f6e 650a 2020  y_past = None.  
+0001aea0: 2020 2020 2020 2020 2020 7365 6c66 2e76            self.v
+0001aeb0: 616c 7565 5f70 6173 7420 3d20 4e6f 6e65  alue_past = None
+0001aec0: 0a0a 2020 2020 2020 2020 2020 2020 6966  ..            if
+0001aed0: 2073 656c 662e 7573 655f 7061 7374 3a0a   self.use_past:.
+0001aee0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001aef0: 2320 6f70 6572 6174 6f72 2075 7365 6420  # operator used 
+0001af00: 666f 7220 7374 6174 6520 7265 7573 650a  for state reuse.
+0001af10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001af20: 7365 6c66 2e72 6564 7563 6573 756d 203d  self.reducesum =
+0001af30: 2050 2e52 6564 7563 6553 756d 2829 2e73   P.ReduceSum().s
+0001af40: 6861 7264 2828 2831 2c20 312c 2031 2c20  hard(((1, 1, 1, 
+0001af50: 3129 2c29 290a 2020 2020 2020 2020 2020  1),)).          
+0001af60: 2020 2020 2020 7365 6c66 2e6e 6f74 5f65        self.not_e
+0001af70: 7175 616c 203d 2050 2e4e 6f74 4571 7561  qual = P.NotEqua
+0001af80: 6c28 292e 7368 6172 6428 2828 312c 2031  l().shard(((1, 1
+0001af90: 2c20 312c 2031 292c 2028 2929 290a 2020  , 1, 1), ())).  
+0001afa0: 2020 2020 2020 2020 2020 2020 2020 7365                se
+0001afb0: 6c66 2e73 6c69 6365 203d 2050 2e53 7472  lf.slice = P.Str
+0001afc0: 6964 6564 536c 6963 6528 292e 7368 6172  idedSlice().shar
+0001afd0: 6428 2828 312c 2031 2c20 312c 2031 292c  d(((1, 1, 1, 1),
+0001afe0: 2929 0a20 2020 2020 2020 2020 2020 2020  )).             
+0001aff0: 2020 2073 697a 655f 7065 725f 6865 6164     size_per_head
+0001b000: 203d 2068 6964 6465 6e5f 7369 7a65 202f   = hidden_size /
+0001b010: 2f20 6e75 6d5f 6865 6164 730a 2020 2020  / num_heads.    
+0001b020: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+0001b030: 2e6b 6579 5f73 6861 7065 203d 2028 6261  .key_shape = (ba
+0001b040: 7463 685f 7369 7a65 2c20 6e75 6d5f 6865  tch_size, num_he
+0001b050: 6164 732c 2073 697a 655f 7065 725f 6865  ads, size_per_he
+0001b060: 6164 2c20 7365 715f 6c65 6e67 7468 290a  ad, seq_length).
 0001b070: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001b080: 2022 466f 7220 2754 7261 6e73 666f 726d   "For 'Transform
-0001b090: 6572 456e 636f 6465 724c 6179 6572 272c  erEncoderLayer',
-0001b0a0: 2074 6865 2063 6c61 7373 2076 6172 6961   the class varia
-0001b0b0: 626c 6520 276e 756d 5f68 6561 6473 2720  ble 'num_heads' 
-0001b0c0: 6d75 7374 2062 6520 6469 7669 7369 626c  must be divisibl
-0001b0d0: 6564 2062 7920 7468 6520 220a 2020 2020  ed by the ".    
-0001b0e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001b0f0: 2227 7061 7261 6c6c 656c 5f63 6f6e 6669  "'parallel_confi
-0001b100: 672e 6d6f 6465 6c5f 7061 7261 6c6c 656c  g.model_parallel
-0001b110: 272c 2062 7574 2067 6f74 2074 6865 206e  ', but got the n
-0001b120: 756d 5f68 6561 6473 2069 7320 7b7d 2061  um_heads is {} a
-0001b130: 6e64 2022 0a20 2020 2020 2020 2020 2020  nd ".           
-0001b140: 2020 2020 2020 2020 2022 7061 7261 6c6c           "parall
-0001b150: 656c 5f63 6f6e 6669 672e 6d6f 6465 6c5f  el_config.model_
-0001b160: 7061 7261 6c6c 656c 2069 7320 7b7d 2e22  parallel is {}."
-0001b170: 2e66 6f72 6d61 7428 6e75 6d5f 6865 6164  .format(num_head
-0001b180: 732c 2070 6172 616c 6c65 6c5f 636f 6e66  s, parallel_conf
-0001b190: 6967 2e6d 6f64 656c 5f70 6172 616c 6c65  ig.model_paralle
-0001b1a0: 6c29 290a 2020 2020 2020 2020 2020 2020  l)).            
-0001b1b0: 6966 2068 6964 6465 6e5f 7369 7a65 2025  if hidden_size %
-0001b1c0: 2070 6172 616c 6c65 6c5f 636f 6e66 6967   parallel_config
-0001b1d0: 2e6d 6f64 656c 5f70 6172 616c 6c65 6c20  .model_parallel 
-0001b1e0: 213d 2030 3a0a 2020 2020 2020 2020 2020  != 0:.          
-0001b1f0: 2020 2020 2020 7261 6973 6520 5661 6c75        raise Valu
-0001b200: 6545 7272 6f72 280a 2020 2020 2020 2020  eError(.        
-0001b210: 2020 2020 2020 2020 2020 2020 2246 6f72              "For
-0001b220: 2027 5472 616e 7366 6f72 6d65 7245 6e63   'TransformerEnc
-0001b230: 6f64 6572 4c61 7965 7227 2c20 7468 6520  oderLayer', the 
-0001b240: 636c 6173 7320 7661 7269 6162 6c65 2027  class variable '
-0001b250: 6869 6464 656e 5f73 697a 6527 206d 7573  hidden_size' mus
-0001b260: 7420 6265 2064 6976 6973 6962 6c65 6420  t be divisibled 
-0001b270: 6279 2022 0a20 2020 2020 2020 2020 2020  by ".           
-0001b280: 2020 2020 2020 2020 2022 7468 6520 2770           "the 'p
-0001b290: 6172 616c 6c65 6c5f 636f 6e66 6967 2e6d  arallel_config.m
-0001b2a0: 6f64 656c 5f70 6172 616c 6c65 6c27 2c20  odel_parallel', 
-0001b2b0: 6275 7420 676f 7420 7468 6520 6869 6464  but got the hidd
-0001b2c0: 656e 5f73 697a 6520 6973 207b 7d20 616e  en_size is {} an
-0001b2d0: 6420 7061 7261 6c6c 656c 5f63 6f6e 6669  d parallel_confi
-0001b2e0: 672e 220a 2020 2020 2020 2020 2020 2020  g.".            
-0001b2f0: 2020 2020 2020 2020 2220 6d6f 6465 6c5f          " model_
-0001b300: 7061 7261 6c6c 656c 2069 7320 7b7d 2e22  parallel is {}."
-0001b310: 2e66 6f72 6d61 7428 6869 6464 656e 5f73  .format(hidden_s
-0001b320: 697a 652c 2070 6172 616c 6c65 6c5f 636f  ize, parallel_co
-0001b330: 6e66 6967 2e6d 6f64 656c 5f70 6172 616c  nfig.model_paral
-0001b340: 6c65 6c29 290a 2020 2020 2020 2020 2020  lel)).          
-0001b350: 2020 6966 2066 666e 5f68 6964 6465 6e5f    if ffn_hidden_
-0001b360: 7369 7a65 2025 2070 6172 616c 6c65 6c5f  size % parallel_
-0001b370: 636f 6e66 6967 2e6d 6f64 656c 5f70 6172  config.model_par
-0001b380: 616c 6c65 6c20 213d 2030 3a0a 2020 2020  allel != 0:.    
-0001b390: 2020 2020 2020 2020 2020 2020 7261 6973              rais
-0001b3a0: 6520 5661 6c75 6545 7272 6f72 280a 2020  e ValueError(.  
-0001b3b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001b3c0: 2020 2246 6f72 2027 5472 616e 7366 6f72    "For 'Transfor
-0001b3d0: 6d65 7245 6e63 6f64 6572 4c61 7965 7227  merEncoderLayer'
-0001b3e0: 2c20 7468 6520 636c 6173 7320 7661 7269  , the class vari
-0001b3f0: 6162 6c65 2027 6666 6e5f 6869 6464 656e  able 'ffn_hidden
-0001b400: 5f73 697a 6527 206d 7573 7420 6265 2064  _size' must be d
-0001b410: 6976 6973 6962 6c65 6420 220a 2020 2020  ivisibled ".    
-0001b420: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001b430: 2262 7920 7468 6520 2770 6172 616c 6c65  "by the 'paralle
-0001b440: 6c5f 636f 6e66 6967 2e6d 6f64 656c 5f70  l_config.model_p
-0001b450: 6172 616c 6c65 6c27 2c20 6275 7420 676f  arallel', but go
-0001b460: 7420 7468 6520 6666 6e5f 6869 6464 656e  t the ffn_hidden
-0001b470: 5f73 697a 6520 6973 207b 7d20 220a 2020  _size is {} ".  
-0001b480: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001b490: 2020 2261 6e64 2070 6172 616c 6c65 6c5f    "and parallel_
-0001b4a0: 636f 6e66 6967 2e20 6d6f 6465 6c5f 7061  config. model_pa
-0001b4b0: 7261 6c6c 656c 2069 7320 7b7d 2e22 0a20  rallel is {}.". 
-0001b4c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001b4d0: 2020 202e 666f 726d 6174 2866 666e 5f68     .format(ffn_h
-0001b4e0: 6964 6465 6e5f 7369 7a65 2c20 7061 7261  idden_size, para
-0001b4f0: 6c6c 656c 5f63 6f6e 6669 672e 6d6f 6465  llel_config.mode
-0001b500: 6c5f 7061 7261 6c6c 656c 2929 0a20 2020  l_parallel)).   
-0001b510: 2020 2020 2020 2020 2023 2066 6c61 7368           # flash
-0001b520: 2061 7474 656e 7469 6f6e 202f 2070 726f   attention / pro
-0001b530: 6d70 7420 666c 6173 6820 6174 7465 6e74  mpt flash attent
-0001b540: 696f 6e20 2f20 696e 6372 6520 666c 6173  ion / incre flas
-0001b550: 6820 6174 7465 6e74 696f 6e20 7665 7273  h attention vers
-0001b560: 696f 6e20 7661 6c69 6461 7469 6f6e 0a20  ion validation. 
-0001b570: 2020 2020 2020 2020 2020 2069 6620 7573             if us
-0001b580: 655f 666c 6173 685f 6174 7465 6e74 696f  e_flash_attentio
-0001b590: 6e20 616e 6420 6e6f 7420 6368 6563 6b5f  n and not check_
-0001b5a0: 7661 6c69 645f 666c 6173 685f 6174 7465  valid_flash_atte
-0001b5b0: 6e74 696f 6e28 6661 5f74 7970 653d 2246  ntion(fa_type="F
-0001b5c0: 6c61 7368 4174 7465 6e74 696f 6e22 293a  lashAttention"):
-0001b5d0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0001b5e0: 2075 7365 5f66 6c61 7368 5f61 7474 656e   use_flash_atten
-0001b5f0: 7469 6f6e 203d 2046 616c 7365 0a20 2020  tion = False.   
-0001b600: 2020 2020 2020 2020 2020 2020 206c 6f67               log
-0001b610: 2e69 6e66 6f28 2243 7572 7265 6e74 204d  .info("Current M
-0001b620: 696e 6453 706f 7265 2064 6f20 6e6f 7420  indSpore do not 
-0001b630: 7375 7070 6f72 7420 666c 6173 6820 6174  support flash at
-0001b640: 7465 6e74 696f 6e2c 2070 6c65 6173 6520  tention, please 
-0001b650: 7570 6772 6164 6520 746f 2032 2e32 2e30  upgrade to 2.2.0
-0001b660: 206f 7220 6869 6768 6572 2229 0a20 2020   or higher").   
-0001b670: 2020 2020 2020 2020 2069 6620 7573 655f           if use_
-0001b680: 7072 6f6d 7074 5f66 6c61 7368 5f61 7474  prompt_flash_att
-0001b690: 656e 7469 6f6e 2061 6e64 205c 0a20 2020  ention and \.   
-0001b6a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001b6b0: 206e 6f74 2063 6865 636b 5f76 616c 6964   not check_valid
-0001b6c0: 5f66 6c61 7368 5f61 7474 656e 7469 6f6e  _flash_attention
-0001b6d0: 2850 524f 4d50 5446 4c41 5348 4154 5445  (PROMPTFLASHATTE
-0001b6e0: 4e54 494f 4e5f 5641 4c49 442c 2022 5072  NTION_VALID, "Pr
-0001b6f0: 6f6d 7074 466c 6173 6841 7474 656e 7469  omptFlashAttenti
-0001b700: 6f6e 2229 3a0a 2020 2020 2020 2020 2020  on"):.          
-0001b710: 2020 2020 2020 7573 655f 7072 6f6d 7074        use_prompt
-0001b720: 5f66 6c61 7368 5f61 7474 656e 7469 6f6e  _flash_attention
-0001b730: 203d 2046 616c 7365 0a20 2020 2020 2020   = False.       
-0001b740: 2020 2020 2020 2020 206c 6f67 2e69 6e66           log.inf
-0001b750: 6f28 2243 7572 7265 6e74 204d 696e 6453  o("Current MindS
-0001b760: 706f 7265 206f 7220 6465 7669 6365 2064  pore or device d
-0001b770: 6f20 6e6f 7420 7375 7070 6f72 7420 7072  o not support pr
-0001b780: 6f6d 7074 2066 6c61 7368 2061 7474 656e  ompt flash atten
-0001b790: 7469 6f6e 2c20 220a 2020 2020 2020 2020  tion, ".        
-0001b7a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001b7b0: 2022 706c 6561 7365 2075 7067 7261 6465   "please upgrade
-0001b7c0: 2074 6f20 322e 322e 3020 6f72 2068 6967   to 2.2.0 or hig
-0001b7d0: 6865 7220 6f72 2075 7365 2039 3130 4220  her or use 910B 
-0001b7e0: 746f 2072 756e 2070 6661 2229 0a0a 2020  to run pfa")..  
-0001b7f0: 2020 2020 2020 2020 2020 5f63 6865 636b            _check
-0001b800: 5f6d 6f65 5f63 6f6e 6669 6728 6d6f 655f  _moe_config(moe_
-0001b810: 636f 6e66 6967 2c20 7061 7261 6c6c 656c  config, parallel
-0001b820: 5f63 6f6e 6669 6729 0a20 2020 2020 2020  _config).       
-0001b830: 2020 2020 2073 656c 662e 7573 655f 6d6f       self.use_mo
-0001b840: 6520 3d20 286d 6f65 5f63 6f6e 6669 672e  e = (moe_config.
-0001b850: 6578 7065 7274 5f6e 756d 203e 2031 290a  expert_num > 1).
-0001b860: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-0001b870: 2e75 7365 5f70 6173 7420 3d20 7573 655f  .use_past = use_
-0001b880: 7061 7374 0a20 2020 2020 2020 2020 2020  past.           
-0001b890: 2073 656c 662e 7365 715f 6c65 6e67 7468   self.seq_length
-0001b8a0: 203d 2073 6571 5f6c 656e 6774 680a 2020   = seq_length.  
-0001b8b0: 2020 2020 2020 2020 2020 7365 6c66 2e68            self.h
-0001b8c0: 6964 6465 6e5f 7369 7a65 203d 2068 6964  idden_size = hid
-0001b8d0: 6465 6e5f 7369 7a65 0a20 2020 2020 2020  den_size.       
-0001b8e0: 2020 2020 2073 656c 662e 6c61 7965 726e       self.layern
-0001b8f0: 6f72 6d31 203d 204c 6179 6572 4e6f 726d  orm1 = LayerNorm
-0001b900: 2828 6869 6464 656e 5f73 697a 652c 2929  ((hidden_size,))
-0001b910: 2e74 6f5f 666c 6f61 7428 6c61 7965 726e  .to_float(layern
-0001b920: 6f72 6d5f 636f 6d70 7574 655f 7479 7065  orm_compute_type
-0001b930: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
-0001b940: 6c66 2e6c 6179 6572 6e6f 726d 312e 7368  lf.layernorm1.sh
-0001b950: 6172 6428 2828 7061 7261 6c6c 656c 5f63  ard(((parallel_c
-0001b960: 6f6e 6669 672e 6461 7461 5f70 6172 616c  onfig.data_paral
-0001b970: 6c65 6c2c 2031 292c 2929 0a20 2020 2020  lel, 1),)).     
-0001b980: 2020 2020 2020 2073 656c 662e 6c61 7965         self.laye
-0001b990: 726e 6f72 6d32 203d 204c 6179 6572 4e6f  rnorm2 = LayerNo
-0001b9a0: 726d 2828 6869 6464 656e 5f73 697a 652c  rm((hidden_size,
-0001b9b0: 2929 2e74 6f5f 666c 6f61 7428 6c61 7965  )).to_float(laye
-0001b9c0: 726e 6f72 6d5f 636f 6d70 7574 655f 7479  rnorm_compute_ty
-0001b9d0: 7065 290a 2020 2020 2020 2020 2020 2020  pe).            
-0001b9e0: 7365 6c66 2e6c 6179 6572 6e6f 726d 322e  self.layernorm2.
-0001b9f0: 7368 6172 6428 2828 7061 7261 6c6c 656c  shard(((parallel
-0001ba00: 5f63 6f6e 6669 672e 6461 7461 5f70 6172  _config.data_par
-0001ba10: 616c 6c65 6c2c 2031 292c 2929 0a0a 2020  allel, 1),))..  
-0001ba20: 2020 2020 2020 2020 2020 6174 7465 6e74            attent
-0001ba30: 696f 6e5f 7061 7261 6c6c 656c 5f63 6f6e  ion_parallel_con
-0001ba40: 6669 6720 3d20 7061 7261 6c6c 656c 5f63  fig = parallel_c
-0001ba50: 6f6e 6669 672e 6470 6d70 2069 6620 7365  onfig.dpmp if se
-0001ba60: 6c66 2e75 7365 5f6d 6f65 2065 6c73 6520  lf.use_moe else 
-0001ba70: 7061 7261 6c6c 656c 5f63 6f6e 6669 670a  parallel_config.
-0001ba80: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-0001ba90: 2e61 7474 656e 7469 6f6e 203d 204d 756c  .attention = Mul
-0001baa0: 7469 4865 6164 4174 7465 6e74 696f 6e28  tiHeadAttention(
-0001bab0: 6261 7463 685f 7369 7a65 3d62 6174 6368  batch_size=batch
-0001bac0: 5f73 697a 652c 0a20 2020 2020 2020 2020  _size,.         
-0001bad0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001bae0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001baf0: 2020 2020 2020 2073 7263 5f73 6571 5f6c         src_seq_l
-0001bb00: 656e 6774 683d 7365 715f 6c65 6e67 7468  ength=seq_length
-0001bb10: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-0001bb20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001bb30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001bb40: 2020 7467 745f 7365 715f 6c65 6e67 7468    tgt_seq_length
-0001bb50: 3d73 6571 5f6c 656e 6774 682c 0a20 2020  =seq_length,.   
-0001bb60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001bb70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001bb80: 2020 2020 2020 2020 2020 2020 2068 6964               hid
-0001bb90: 6465 6e5f 7369 7a65 3d68 6964 6465 6e5f  den_size=hidden_
-0001bba0: 7369 7a65 2c0a 2020 2020 2020 2020 2020  size,.          
-0001bbb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001bbc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001bbd0: 2020 2020 2020 6e75 6d5f 6865 6164 733d        num_heads=
-0001bbe0: 6e75 6d5f 6865 6164 732c 0a20 2020 2020  num_heads,.     
-0001bbf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001bc00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001bc10: 2020 2020 2020 2020 2020 2068 6964 6465             hidde
-0001bc20: 6e5f 6472 6f70 6f75 745f 7261 7465 3d68  n_dropout_rate=h
-0001bc30: 6964 6465 6e5f 6472 6f70 6f75 745f 7261  idden_dropout_ra
-0001bc40: 7465 2c0a 2020 2020 2020 2020 2020 2020  te,.            
-0001bc50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001bc60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001bc70: 2020 2020 6174 7465 6e74 696f 6e5f 6472      attention_dr
-0001bc80: 6f70 6f75 745f 7261 7465 3d61 7474 656e  opout_rate=atten
-0001bc90: 7469 6f6e 5f64 726f 706f 7574 5f72 6174  tion_dropout_rat
-0001bca0: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
-0001bcb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001bcc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001bcd0: 2020 2073 6f66 746d 6178 5f63 6f6d 7075     softmax_compu
-0001bce0: 7465 5f74 7970 653d 736f 6674 6d61 785f  te_type=softmax_
-0001bcf0: 636f 6d70 7574 655f 7479 7065 2c0a 2020  compute_type,.  
-0001bd00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001bd10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001bd20: 2020 2020 2020 2020 2020 2020 2020 7061                pa
-0001bd30: 7261 6d5f 696e 6974 5f74 7970 653d 7061  ram_init_type=pa
-0001bd40: 7261 6d5f 696e 6974 5f74 7970 652c 0a20  ram_init_type,. 
-0001bd50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001bd60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001bd70: 2020 2020 2020 2020 2020 2020 2020 2075                 u
-0001bd80: 7365 5f70 6173 743d 7573 655f 7061 7374  se_past=use_past
-0001bd90: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-0001bda0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001bdb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001bdc0: 2020 7061 7261 6c6c 656c 5f63 6f6e 6669    parallel_confi
-0001bdd0: 673d 6174 7465 6e74 696f 6e5f 7061 7261  g=attention_para
-0001bde0: 6c6c 656c 5f63 6f6e 6669 672c 0a20 2020  llel_config,.   
-0001bdf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001be00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001be10: 2020 2020 2020 2020 2020 2020 2075 7365               use
-0001be20: 5f66 6c61 7368 5f61 7474 656e 7469 6f6e  _flash_attention
-0001be30: 3d75 7365 5f66 6c61 7368 5f61 7474 656e  =use_flash_atten
-0001be40: 7469 6f6e 2c0a 2020 2020 2020 2020 2020  tion,.          
-0001be50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001be60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001be70: 2020 2020 2020 7573 655f 7072 6f6d 7074        use_prompt
-0001be80: 5f66 6c61 7368 5f61 7474 656e 7469 6f6e  _flash_attention
-0001be90: 3d75 7365 5f70 726f 6d70 745f 666c 6173  =use_prompt_flas
-0001bea0: 685f 6174 7465 6e74 696f 6e29 0a20 2020  h_attention).   
-0001beb0: 2020 2020 2020 2020 2069 6620 7365 6c66           if self
-0001bec0: 2e75 7365 5f6d 6f65 3a0a 2020 2020 2020  .use_moe:.      
-0001bed0: 2020 2020 2020 2020 2020 7365 6c66 2e6f            self.o
-0001bee0: 7574 7075 7420 3d20 4d6f 4528 6869 6464  utput = MoE(hidd
-0001bef0: 656e 5f73 697a 653d 6869 6464 656e 5f73  en_size=hidden_s
-0001bf00: 697a 652c 0a20 2020 2020 2020 2020 2020  ize,.           
-0001bf10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001bf20: 2020 2020 2020 2064 726f 706f 7574 5f72         dropout_r
-0001bf30: 6174 653d 6869 6464 656e 5f64 726f 706f  ate=hidden_dropo
-0001bf40: 7574 5f72 6174 652c 0a20 2020 2020 2020  ut_rate,.       
+0001b080: 7365 6c66 2e76 616c 7565 5f73 6861 7065  self.value_shape
+0001b090: 203d 2028 6261 7463 685f 7369 7a65 2c20   = (batch_size, 
+0001b0a0: 6e75 6d5f 6865 6164 732c 2073 6571 5f6c  num_heads, seq_l
+0001b0b0: 656e 6774 682c 2073 697a 655f 7065 725f  ength, size_per_
+0001b0c0: 6865 6164 290a 2020 2020 2020 2020 2020  head).          
+0001b0d0: 2020 2020 2020 2320 7061 7261 6d65 7465        # paramete
+0001b0e0: 7273 2073 6176 696e 6720 6b65 7920 616e  rs saving key an
+0001b0f0: 6420 7661 6c75 6520 7374 6174 6573 0a20  d value states. 
+0001b100: 2020 2020 2020 2020 2020 2020 2020 2073                 s
+0001b110: 656c 662e 6b65 795f 7061 7374 203d 2050  elf.key_past = P
+0001b120: 6172 616d 6574 6572 2854 656e 736f 7228  arameter(Tensor(
+0001b130: 6e70 2e7a 6572 6f73 2873 6861 7065 3d73  np.zeros(shape=s
+0001b140: 656c 662e 6b65 795f 7368 6170 6529 2c20  elf.key_shape), 
+0001b150: 7365 6c66 2e64 7479 7065 292c 206e 616d  self.dtype), nam
+0001b160: 653d 226b 6579 5f70 6173 7422 290a 2020  e="key_past").  
+0001b170: 2020 2020 2020 2020 2020 2020 2020 7365                se
+0001b180: 6c66 2e76 616c 7565 5f70 6173 7420 3d20  lf.value_past = 
+0001b190: 5061 7261 6d65 7465 7228 5465 6e73 6f72  Parameter(Tensor
+0001b1a0: 286e 702e 7a65 726f 7328 7368 6170 653d  (np.zeros(shape=
+0001b1b0: 7365 6c66 2e76 616c 7565 5f73 6861 7065  self.value_shape
+0001b1c0: 292c 2073 656c 662e 6474 7970 6529 2c20  ), self.dtype), 
+0001b1d0: 6e61 6d65 3d22 7661 6c75 655f 7061 7374  name="value_past
+0001b1e0: 2229 0a20 2020 2020 2020 2020 2020 2020  ").             
+0001b1f0: 2020 2073 656c 662e 7469 6c65 203d 2050     self.tile = P
+0001b200: 2e54 696c 6528 292e 7368 6172 6428 2828  .Tile().shard(((
+0001b210: 312c 2031 292c 2929 0a20 2020 2020 2020  1, 1),)).       
+0001b220: 2020 2020 2020 2020 2073 656c 662e 6d75           self.mu
+0001b230: 6c20 3d20 502e 4d75 6c28 292e 7368 6172  l = P.Mul().shar
+0001b240: 6428 2828 312c 2031 2c20 312c 2031 292c  d(((1, 1, 1, 1),
+0001b250: 2028 312c 2929 290a 2020 2020 2020 2020   (1,))).        
+0001b260: 2020 2020 2020 2020 7365 6c66 2e61 7373          self.ass
+0001b270: 6967 6e20 3d20 502e 4173 7369 676e 2829  ign = P.Assign()
+0001b280: 2e73 6861 7264 2828 2831 2c20 312c 2031  .shard(((1, 1, 1
+0001b290: 2c20 3129 2c20 2831 2c20 312c 2031 2c20  , 1), (1, 1, 1, 
+0001b2a0: 3129 2929 0a20 2020 2020 2020 2065 6c69  1))).        eli
+0001b2b0: 6620 5f67 6574 5f70 6172 616c 6c65 6c5f  f _get_parallel_
+0001b2c0: 6d6f 6465 2829 206e 6f74 2069 6e20 2850  mode() not in (P
+0001b2d0: 6172 616c 6c65 6c4d 6f64 652e 4155 544f  arallelMode.AUTO
+0001b2e0: 5f50 4152 414c 4c45 4c2c 293a 0a20 2020  _PARALLEL,):.   
+0001b2f0: 2020 2020 2020 2020 205f 6368 6563 6b5f           _check_
+0001b300: 636f 6e66 6967 2870 6172 616c 6c65 6c5f  config(parallel_
+0001b310: 636f 6e66 6967 290a 2020 2020 2020 2020  config).        
+0001b320: 2020 2020 6966 206e 756d 5f68 6561 6473      if num_heads
+0001b330: 2025 2070 6172 616c 6c65 6c5f 636f 6e66   % parallel_conf
+0001b340: 6967 2e6d 6f64 656c 5f70 6172 616c 6c65  ig.model_paralle
+0001b350: 6c20 213d 2030 3a0a 2020 2020 2020 2020  l != 0:.        
+0001b360: 2020 2020 2020 2020 7261 6973 6520 5661          raise Va
+0001b370: 6c75 6545 7272 6f72 280a 2020 2020 2020  lueError(.      
+0001b380: 2020 2020 2020 2020 2020 2020 2020 2246                "F
+0001b390: 6f72 2027 5472 616e 7366 6f72 6d65 7245  or 'TransformerE
+0001b3a0: 6e63 6f64 6572 4c61 7965 7227 2c20 7468  ncoderLayer', th
+0001b3b0: 6520 636c 6173 7320 7661 7269 6162 6c65  e class variable
+0001b3c0: 2027 6e75 6d5f 6865 6164 7327 206d 7573   'num_heads' mus
+0001b3d0: 7420 6265 2064 6976 6973 6962 6c65 6420  t be divisibled 
+0001b3e0: 6279 2074 6865 2022 0a20 2020 2020 2020  by the ".       
+0001b3f0: 2020 2020 2020 2020 2020 2020 2022 2770               "'p
+0001b400: 6172 616c 6c65 6c5f 636f 6e66 6967 2e6d  arallel_config.m
+0001b410: 6f64 656c 5f70 6172 616c 6c65 6c27 2c20  odel_parallel', 
+0001b420: 6275 7420 676f 7420 7468 6520 6e75 6d5f  but got the num_
+0001b430: 6865 6164 7320 6973 207b 7d20 616e 6420  heads is {} and 
+0001b440: 220a 2020 2020 2020 2020 2020 2020 2020  ".              
+0001b450: 2020 2020 2020 2270 6172 616c 6c65 6c5f        "parallel_
+0001b460: 636f 6e66 6967 2e6d 6f64 656c 5f70 6172  config.model_par
+0001b470: 616c 6c65 6c20 6973 207b 7d2e 222e 666f  allel is {}.".fo
+0001b480: 726d 6174 286e 756d 5f68 6561 6473 2c20  rmat(num_heads, 
+0001b490: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
+0001b4a0: 6d6f 6465 6c5f 7061 7261 6c6c 656c 2929  model_parallel))
+0001b4b0: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if 
+0001b4c0: 6869 6464 656e 5f73 697a 6520 2520 7061  hidden_size % pa
+0001b4d0: 7261 6c6c 656c 5f63 6f6e 6669 672e 6d6f  rallel_config.mo
+0001b4e0: 6465 6c5f 7061 7261 6c6c 656c 2021 3d20  del_parallel != 
+0001b4f0: 303a 0a20 2020 2020 2020 2020 2020 2020  0:.             
+0001b500: 2020 2072 6169 7365 2056 616c 7565 4572     raise ValueEr
+0001b510: 726f 7228 0a20 2020 2020 2020 2020 2020  ror(.           
+0001b520: 2020 2020 2020 2020 2022 466f 7220 2754           "For 'T
+0001b530: 7261 6e73 666f 726d 6572 456e 636f 6465  ransformerEncode
+0001b540: 724c 6179 6572 272c 2074 6865 2063 6c61  rLayer', the cla
+0001b550: 7373 2076 6172 6961 626c 6520 2768 6964  ss variable 'hid
+0001b560: 6465 6e5f 7369 7a65 2720 6d75 7374 2062  den_size' must b
+0001b570: 6520 6469 7669 7369 626c 6564 2062 7920  e divisibled by 
+0001b580: 220a 2020 2020 2020 2020 2020 2020 2020  ".              
+0001b590: 2020 2020 2020 2274 6865 2027 7061 7261        "the 'para
+0001b5a0: 6c6c 656c 5f63 6f6e 6669 672e 6d6f 6465  llel_config.mode
+0001b5b0: 6c5f 7061 7261 6c6c 656c 272c 2062 7574  l_parallel', but
+0001b5c0: 2067 6f74 2074 6865 2068 6964 6465 6e5f   got the hidden_
+0001b5d0: 7369 7a65 2069 7320 7b7d 2061 6e64 2070  size is {} and p
+0001b5e0: 6172 616c 6c65 6c5f 636f 6e66 6967 2e22  arallel_config."
+0001b5f0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0001b600: 2020 2020 2022 206d 6f64 656c 5f70 6172       " model_par
+0001b610: 616c 6c65 6c20 6973 207b 7d2e 222e 666f  allel is {}.".fo
+0001b620: 726d 6174 2868 6964 6465 6e5f 7369 7a65  rmat(hidden_size
+0001b630: 2c20 7061 7261 6c6c 656c 5f63 6f6e 6669  , parallel_confi
+0001b640: 672e 6d6f 6465 6c5f 7061 7261 6c6c 656c  g.model_parallel
+0001b650: 2929 0a20 2020 2020 2020 2020 2020 2069  )).            i
+0001b660: 6620 6666 6e5f 6869 6464 656e 5f73 697a  f ffn_hidden_siz
+0001b670: 6520 2520 7061 7261 6c6c 656c 5f63 6f6e  e % parallel_con
+0001b680: 6669 672e 6d6f 6465 6c5f 7061 7261 6c6c  fig.model_parall
+0001b690: 656c 2021 3d20 303a 0a20 2020 2020 2020  el != 0:.       
+0001b6a0: 2020 2020 2020 2020 2072 6169 7365 2056           raise V
+0001b6b0: 616c 7565 4572 726f 7228 0a20 2020 2020  alueError(.     
+0001b6c0: 2020 2020 2020 2020 2020 2020 2020 2022                 "
+0001b6d0: 466f 7220 2754 7261 6e73 666f 726d 6572  For 'Transformer
+0001b6e0: 456e 636f 6465 724c 6179 6572 272c 2074  EncoderLayer', t
+0001b6f0: 6865 2063 6c61 7373 2076 6172 6961 626c  he class variabl
+0001b700: 6520 2766 666e 5f68 6964 6465 6e5f 7369  e 'ffn_hidden_si
+0001b710: 7a65 2720 6d75 7374 2062 6520 6469 7669  ze' must be divi
+0001b720: 7369 626c 6564 2022 0a20 2020 2020 2020  sibled ".       
+0001b730: 2020 2020 2020 2020 2020 2020 2022 6279               "by
+0001b740: 2074 6865 2027 7061 7261 6c6c 656c 5f63   the 'parallel_c
+0001b750: 6f6e 6669 672e 6d6f 6465 6c5f 7061 7261  onfig.model_para
+0001b760: 6c6c 656c 272c 2062 7574 2067 6f74 2074  llel', but got t
+0001b770: 6865 2066 666e 5f68 6964 6465 6e5f 7369  he ffn_hidden_si
+0001b780: 7a65 2069 7320 7b7d 2022 0a20 2020 2020  ze is {} ".     
+0001b790: 2020 2020 2020 2020 2020 2020 2020 2022                 "
+0001b7a0: 616e 6420 7061 7261 6c6c 656c 5f63 6f6e  and parallel_con
+0001b7b0: 6669 672e 206d 6f64 656c 5f70 6172 616c  fig. model_paral
+0001b7c0: 6c65 6c20 6973 207b 7d2e 220a 2020 2020  lel is {}.".    
+0001b7d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001b7e0: 2e66 6f72 6d61 7428 6666 6e5f 6869 6464  .format(ffn_hidd
+0001b7f0: 656e 5f73 697a 652c 2070 6172 616c 6c65  en_size, paralle
+0001b800: 6c5f 636f 6e66 6967 2e6d 6f64 656c 5f70  l_config.model_p
+0001b810: 6172 616c 6c65 6c29 290a 2020 2020 2020  arallel)).      
+0001b820: 2020 2020 2020 2320 666c 6173 6820 6174        # flash at
+0001b830: 7465 6e74 696f 6e20 2f20 7072 6f6d 7074  tention / prompt
+0001b840: 2066 6c61 7368 2061 7474 656e 7469 6f6e   flash attention
+0001b850: 202f 2069 6e63 7265 2066 6c61 7368 2061   / incre flash a
+0001b860: 7474 656e 7469 6f6e 2076 6572 7369 6f6e  ttention version
+0001b870: 2076 616c 6964 6174 696f 6e0a 2020 2020   validation.    
+0001b880: 2020 2020 2020 2020 6966 2075 7365 5f66          if use_f
+0001b890: 6c61 7368 5f61 7474 656e 7469 6f6e 2061  lash_attention a
+0001b8a0: 6e64 206e 6f74 2063 6865 636b 5f76 616c  nd not check_val
+0001b8b0: 6964 5f66 6c61 7368 5f61 7474 656e 7469  id_flash_attenti
+0001b8c0: 6f6e 2866 615f 7479 7065 3d22 466c 6173  on(fa_type="Flas
+0001b8d0: 6841 7474 656e 7469 6f6e 2229 3a0a 2020  hAttention"):.  
+0001b8e0: 2020 2020 2020 2020 2020 2020 2020 7573                us
+0001b8f0: 655f 666c 6173 685f 6174 7465 6e74 696f  e_flash_attentio
+0001b900: 6e20 3d20 4661 6c73 650a 2020 2020 2020  n = False.      
+0001b910: 2020 2020 2020 2020 2020 6c6f 672e 696e            log.in
+0001b920: 666f 2822 4375 7272 656e 7420 4d69 6e64  fo("Current Mind
+0001b930: 5370 6f72 6520 646f 206e 6f74 2073 7570  Spore do not sup
+0001b940: 706f 7274 2066 6c61 7368 2061 7474 656e  port flash atten
+0001b950: 7469 6f6e 2c20 706c 6561 7365 2075 7067  tion, please upg
+0001b960: 7261 6465 2074 6f20 322e 322e 3020 6f72  rade to 2.2.0 or
+0001b970: 2068 6967 6865 7222 290a 2020 2020 2020   higher").      
+0001b980: 2020 2020 2020 6966 2075 7365 5f70 726f        if use_pro
+0001b990: 6d70 745f 666c 6173 685f 6174 7465 6e74  mpt_flash_attent
+0001b9a0: 696f 6e20 616e 6420 5c0a 2020 2020 2020  ion and \.      
+0001b9b0: 2020 2020 2020 2020 2020 2020 2020 6e6f                no
+0001b9c0: 7420 6368 6563 6b5f 7661 6c69 645f 666c  t check_valid_fl
+0001b9d0: 6173 685f 6174 7465 6e74 696f 6e28 5052  ash_attention(PR
+0001b9e0: 4f4d 5054 464c 4153 4841 5454 454e 5449  OMPTFLASHATTENTI
+0001b9f0: 4f4e 5f56 414c 4944 2c20 2250 726f 6d70  ON_VALID, "Promp
+0001ba00: 7446 6c61 7368 4174 7465 6e74 696f 6e22  tFlashAttention"
+0001ba10: 293a 0a20 2020 2020 2020 2020 2020 2020  ):.             
+0001ba20: 2020 2075 7365 5f70 726f 6d70 745f 666c     use_prompt_fl
+0001ba30: 6173 685f 6174 7465 6e74 696f 6e20 3d20  ash_attention = 
+0001ba40: 4661 6c73 650a 2020 2020 2020 2020 2020  False.          
+0001ba50: 2020 2020 2020 6c6f 672e 696e 666f 2822        log.info("
+0001ba60: 4375 7272 656e 7420 4d69 6e64 5370 6f72  Current MindSpor
+0001ba70: 6520 6f72 2064 6576 6963 6520 646f 206e  e or device do n
+0001ba80: 6f74 2073 7570 706f 7274 2070 726f 6d70  ot support promp
+0001ba90: 7420 666c 6173 6820 6174 7465 6e74 696f  t flash attentio
+0001baa0: 6e2c 2022 0a20 2020 2020 2020 2020 2020  n, ".           
+0001bab0: 2020 2020 2020 2020 2020 2020 2020 2270                "p
+0001bac0: 6c65 6173 6520 7570 6772 6164 6520 746f  lease upgrade to
+0001bad0: 2032 2e32 2e30 206f 7220 6869 6768 6572   2.2.0 or higher
+0001bae0: 206f 7220 7573 6520 3931 3042 2074 6f20   or use 910B to 
+0001baf0: 7275 6e20 7066 6122 290a 2020 2020 2020  run pfa").      
+0001bb00: 2020 2020 2020 6966 2075 7365 5f69 6e63        if use_inc
+0001bb10: 7265 5f66 6c61 7368 5f61 7474 656e 7469  re_flash_attenti
+0001bb20: 6f6e 2061 6e64 205c 0a20 2020 2020 2020  on and \.       
+0001bb30: 2020 2020 2020 2020 2020 2020 206e 6f74               not
+0001bb40: 2063 6865 636b 5f76 616c 6964 5f66 6c61   check_valid_fla
+0001bb50: 7368 5f61 7474 656e 7469 6f6e 2849 4e43  sh_attention(INC
+0001bb60: 5245 464c 4153 4841 5454 454e 5449 4f4e  REFLASHATTENTION
+0001bb70: 5f56 414c 4944 2c20 2249 6e63 7265 466c  _VALID, "IncreFl
+0001bb80: 6173 6841 7474 656e 7469 6f6e 2229 3a0a  ashAttention"):.
+0001bb90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001bba0: 7573 655f 696e 6372 655f 666c 6173 685f  use_incre_flash_
+0001bbb0: 6174 7465 6e74 696f 6e20 3d20 4661 6c73  attention = Fals
+0001bbc0: 650a 2020 2020 2020 2020 2020 2020 2020  e.              
+0001bbd0: 2020 6c6f 672e 696e 666f 2822 4375 7272    log.info("Curr
+0001bbe0: 656e 7420 4d69 6e64 5370 6f72 6520 6f72  ent MindSpore or
+0001bbf0: 2064 6576 6963 6520 646f 206e 6f74 2073   device do not s
+0001bc00: 7570 706f 7274 2070 726f 6d70 7420 666c  upport prompt fl
+0001bc10: 6173 6820 6174 7465 6e74 696f 6e2c 2022  ash attention, "
+0001bc20: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0001bc30: 2020 2020 2020 2020 2020 2270 6c65 6173            "pleas
+0001bc40: 6520 7570 6772 6164 6520 746f 2032 2e32  e upgrade to 2.2
+0001bc50: 2e30 206f 7220 6869 6768 6572 206f 7220  .0 or higher or 
+0001bc60: 7573 6520 3931 3042 2074 6f20 7275 6e20  use 910B to run 
+0001bc70: 6966 6122 290a 0a20 2020 2020 2020 2020  ifa")..         
+0001bc80: 2020 205f 6368 6563 6b5f 6d6f 655f 636f     _check_moe_co
+0001bc90: 6e66 6967 286d 6f65 5f63 6f6e 6669 672c  nfig(moe_config,
+0001bca0: 2070 6172 616c 6c65 6c5f 636f 6e66 6967   parallel_config
+0001bcb0: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
+0001bcc0: 6c66 2e75 7365 5f6d 6f65 203d 2028 6d6f  lf.use_moe = (mo
+0001bcd0: 655f 636f 6e66 6967 2e65 7870 6572 745f  e_config.expert_
+0001bce0: 6e75 6d20 3e20 3129 0a20 2020 2020 2020  num > 1).       
+0001bcf0: 2020 2020 2073 656c 662e 7573 655f 7061       self.use_pa
+0001bd00: 7374 203d 2075 7365 5f70 6173 740a 2020  st = use_past.  
+0001bd10: 2020 2020 2020 2020 2020 7365 6c66 2e73            self.s
+0001bd20: 6571 5f6c 656e 6774 6820 3d20 7365 715f  eq_length = seq_
+0001bd30: 6c65 6e67 7468 0a20 2020 2020 2020 2020  length.         
+0001bd40: 2020 2073 656c 662e 6869 6464 656e 5f73     self.hidden_s
+0001bd50: 697a 6520 3d20 6869 6464 656e 5f73 697a  ize = hidden_siz
+0001bd60: 650a 2020 2020 2020 2020 2020 2020 7365  e.            se
+0001bd70: 6c66 2e6c 6179 6572 6e6f 726d 3120 3d20  lf.layernorm1 = 
+0001bd80: 4c61 7965 724e 6f72 6d28 2868 6964 6465  LayerNorm((hidde
+0001bd90: 6e5f 7369 7a65 2c29 292e 746f 5f66 6c6f  n_size,)).to_flo
+0001bda0: 6174 286c 6179 6572 6e6f 726d 5f63 6f6d  at(layernorm_com
+0001bdb0: 7075 7465 5f74 7970 6529 0a20 2020 2020  pute_type).     
+0001bdc0: 2020 2020 2020 2073 656c 662e 6c61 7965         self.laye
+0001bdd0: 726e 6f72 6d31 2e73 6861 7264 2828 2870  rnorm1.shard(((p
+0001bde0: 6172 616c 6c65 6c5f 636f 6e66 6967 2e64  arallel_config.d
+0001bdf0: 6174 615f 7061 7261 6c6c 656c 2c20 3129  ata_parallel, 1)
+0001be00: 2c29 290a 2020 2020 2020 2020 2020 2020  ,)).            
+0001be10: 7365 6c66 2e6c 6179 6572 6e6f 726d 3220  self.layernorm2 
+0001be20: 3d20 4c61 7965 724e 6f72 6d28 2868 6964  = LayerNorm((hid
+0001be30: 6465 6e5f 7369 7a65 2c29 292e 746f 5f66  den_size,)).to_f
+0001be40: 6c6f 6174 286c 6179 6572 6e6f 726d 5f63  loat(layernorm_c
+0001be50: 6f6d 7075 7465 5f74 7970 6529 0a20 2020  ompute_type).   
+0001be60: 2020 2020 2020 2020 2073 656c 662e 6c61           self.la
+0001be70: 7965 726e 6f72 6d32 2e73 6861 7264 2828  yernorm2.shard((
+0001be80: 2870 6172 616c 6c65 6c5f 636f 6e66 6967  (parallel_config
+0001be90: 2e64 6174 615f 7061 7261 6c6c 656c 2c20  .data_parallel, 
+0001bea0: 3129 2c29 290a 0a20 2020 2020 2020 2020  1),))..         
+0001beb0: 2020 2061 7474 656e 7469 6f6e 5f70 6172     attention_par
+0001bec0: 616c 6c65 6c5f 636f 6e66 6967 203d 2070  allel_config = p
+0001bed0: 6172 616c 6c65 6c5f 636f 6e66 6967 2e64  arallel_config.d
+0001bee0: 706d 7020 6966 2073 656c 662e 7573 655f  pmp if self.use_
+0001bef0: 6d6f 6520 656c 7365 2070 6172 616c 6c65  moe else paralle
+0001bf00: 6c5f 636f 6e66 6967 0a20 2020 2020 2020  l_config.       
+0001bf10: 2020 2020 2073 656c 662e 6174 7465 6e74       self.attent
+0001bf20: 696f 6e20 3d20 4d75 6c74 6948 6561 6441  ion = MultiHeadA
+0001bf30: 7474 656e 7469 6f6e 2862 6174 6368 5f73  ttention(batch_s
+0001bf40: 697a 653d 6261 7463 685f 7369 7a65 2c0a  ize=batch_size,.
 0001bf50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001bf60: 2020 2020 2020 2020 2020 2066 666e 5f68             ffn_h
-0001bf70: 6964 6465 6e5f 7369 7a65 3d66 666e 5f68  idden_size=ffn_h
-0001bf80: 6964 6465 6e5f 7369 7a65 2c0a 2020 2020  idden_size,.    
-0001bf90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001bfa0: 2020 2020 2020 2020 2020 2020 2020 7061                pa
-0001bfb0: 7261 6d5f 696e 6974 5f74 7970 653d 7061  ram_init_type=pa
-0001bfc0: 7261 6d5f 696e 6974 5f74 7970 652c 0a20  ram_init_type,. 
-0001bfd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001bfe0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001bff0: 2068 6964 6465 6e5f 6163 743d 6869 6464   hidden_act=hidd
-0001c000: 656e 5f61 6374 2c0a 2020 2020 2020 2020  en_act,.        
-0001c010: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001c020: 2020 2020 2020 2020 2020 6d6f 655f 636f            moe_co
-0001c030: 6e66 6967 3d6d 6f65 5f63 6f6e 6669 672c  nfig=moe_config,
-0001c040: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0001c050: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001c060: 2020 2070 6172 616c 6c65 6c5f 636f 6e66     parallel_conf
-0001c070: 6967 3d70 6172 616c 6c65 6c5f 636f 6e66  ig=parallel_conf
-0001c080: 6967 290a 2020 2020 2020 2020 2020 2020  ig).            
-0001c090: 656c 7365 3a0a 2020 2020 2020 2020 2020  else:.          
-0001c0a0: 2020 2020 2020 2320 4665 6564 2046 6f72        # Feed For
-0001c0b0: 7761 7264 204e 6574 776f 726b 2c20 4646  ward Network, FF
-0001c0c0: 4e0a 2020 2020 2020 2020 2020 2020 2020  N.              
-0001c0d0: 2020 7365 6c66 2e6f 7574 7075 7420 3d20    self.output = 
-0001c0e0: 4665 6564 466f 7277 6172 6428 6869 6464  FeedForward(hidd
-0001c0f0: 656e 5f73 697a 653d 6869 6464 656e 5f73  en_size=hidden_s
-0001c100: 697a 652c 0a20 2020 2020 2020 2020 2020  ize,.           
-0001c110: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001c120: 2020 2020 2020 2020 2020 2020 2020 2064                 d
-0001c130: 726f 706f 7574 5f72 6174 653d 6869 6464  ropout_rate=hidd
-0001c140: 656e 5f64 726f 706f 7574 5f72 6174 652c  en_dropout_rate,
-0001c150: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0001c160: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001c170: 2020 2020 2020 2020 2020 2066 666e 5f68             ffn_h
-0001c180: 6964 6465 6e5f 7369 7a65 3d66 666e 5f68  idden_size=ffn_h
-0001c190: 6964 6465 6e5f 7369 7a65 2c0a 2020 2020  idden_size,.    
+0001bf60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001bf70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001bf80: 7372 635f 7365 715f 6c65 6e67 7468 3d73  src_seq_length=s
+0001bf90: 6571 5f6c 656e 6774 682c 0a20 2020 2020  eq_length,.     
+0001bfa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001bfb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001bfc0: 2020 2020 2020 2020 2020 2074 6774 5f73             tgt_s
+0001bfd0: 6571 5f6c 656e 6774 683d 7365 715f 6c65  eq_length=seq_le
+0001bfe0: 6e67 7468 2c0a 2020 2020 2020 2020 2020  ngth,.          
+0001bff0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001c000: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001c010: 2020 2020 2020 6869 6464 656e 5f73 697a        hidden_siz
+0001c020: 653d 6869 6464 656e 5f73 697a 652c 0a20  e=hidden_size,. 
+0001c030: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001c040: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001c050: 2020 2020 2020 2020 2020 2020 2020 206e                 n
+0001c060: 756d 5f68 6561 6473 3d6e 756d 5f68 6561  um_heads=num_hea
+0001c070: 6473 2c0a 2020 2020 2020 2020 2020 2020  ds,.            
+0001c080: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001c090: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001c0a0: 2020 2020 6869 6464 656e 5f64 726f 706f      hidden_dropo
+0001c0b0: 7574 5f72 6174 653d 6869 6464 656e 5f64  ut_rate=hidden_d
+0001c0c0: 726f 706f 7574 5f72 6174 652c 0a20 2020  ropout_rate,.   
+0001c0d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001c0e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001c0f0: 2020 2020 2020 2020 2020 2020 2061 7474               att
+0001c100: 656e 7469 6f6e 5f64 726f 706f 7574 5f72  ention_dropout_r
+0001c110: 6174 653d 6174 7465 6e74 696f 6e5f 6472  ate=attention_dr
+0001c120: 6f70 6f75 745f 7261 7465 2c0a 2020 2020  opout_rate,.    
+0001c130: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001c140: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001c150: 2020 2020 2020 2020 2020 2020 736f 6674              soft
+0001c160: 6d61 785f 636f 6d70 7574 655f 7479 7065  max_compute_type
+0001c170: 3d73 6f66 746d 6178 5f63 6f6d 7075 7465  =softmax_compute
+0001c180: 5f74 7970 652c 0a20 2020 2020 2020 2020  _type,.         
+0001c190: 2020 2020 2020 2020 2020 2020 2020 2020                  
 0001c1a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001c1b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001c1c0: 2020 2020 2020 7061 7261 6d5f 696e 6974        param_init
-0001c1d0: 5f74 7970 653d 7061 7261 6d5f 696e 6974  _type=param_init
-0001c1e0: 5f74 7970 652c 0a20 2020 2020 2020 2020  _type,.         
+0001c1b0: 2020 2020 2020 2070 6172 616d 5f69 6e69         param_ini
+0001c1c0: 745f 7479 7065 3d70 6172 616d 5f69 6e69  t_type=param_ini
+0001c1d0: 745f 7479 7065 2c0a 2020 2020 2020 2020  t_type,.        
+0001c1e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 0001c1f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001c200: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001c210: 2063 6f6d 7075 7465 5f64 7479 7065 3d63   compute_dtype=c
-0001c220: 6f6d 7075 7465 5f64 7479 7065 2c0a 2020  ompute_dtype,.  
+0001c200: 2020 2020 2020 2020 7573 655f 7061 7374          use_past
+0001c210: 3d75 7365 5f70 6173 742c 0a20 2020 2020  =use_past,.     
+0001c220: 2020 2020 2020 2020 2020 2020 2020 2020                  
 0001c230: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001c240: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001c250: 2020 2020 2020 2020 6869 6464 656e 5f61          hidden_a
-0001c260: 6374 3d68 6964 6465 6e5f 6163 742c 0a20  ct=hidden_act,. 
-0001c270: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001c240: 2020 2020 2020 2020 2020 2070 6172 616c             paral
+0001c250: 6c65 6c5f 636f 6e66 6967 3d61 7474 656e  lel_config=atten
+0001c260: 7469 6f6e 5f70 6172 616c 6c65 6c5f 636f  tion_parallel_co
+0001c270: 6e66 6967 2c0a 2020 2020 2020 2020 2020  nfig,.          
 0001c280: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001c290: 2020 2020 2020 2020 2070 6172 616c 6c65           paralle
-0001c2a0: 6c5f 636f 6e66 6967 3d70 6172 616c 6c65  l_config=paralle
-0001c2b0: 6c5f 636f 6e66 6967 290a 2020 2020 2020  l_config).      
-0001c2c0: 2020 2020 2020 7365 6c66 2e70 6f73 745f        self.post_
-0001c2d0: 6c61 7965 726e 6f72 6d5f 7265 7369 6475  layernorm_residu
-0001c2e0: 616c 203d 2070 6f73 745f 6c61 7965 726e  al = post_layern
-0001c2f0: 6f72 6d5f 7265 7369 6475 616c 0a20 2020  orm_residual.   
-0001c300: 2020 2020 2020 2020 2073 656c 662e 6164           self.ad
-0001c310: 6420 3d20 502e 4164 6428 292e 7368 6172  d = P.Add().shar
-0001c320: 6428 2828 7061 7261 6c6c 656c 5f63 6f6e  d(((parallel_con
-0001c330: 6669 672e 6461 7461 5f70 6172 616c 6c65  fig.data_paralle
-0001c340: 6c2c 2031 292c 2028 7061 7261 6c6c 656c  l, 1), (parallel
-0001c350: 5f63 6f6e 6669 672e 6461 7461 5f70 6172  _config.data_par
-0001c360: 616c 6c65 6c2c 2031 2929 290a 2020 2020  allel, 1))).    
-0001c370: 2020 2020 2020 2020 7365 6c66 2e61 6464          self.add
-0001c380: 5f33 6420 3d20 502e 4164 6428 292e 7368  _3d = P.Add().sh
-0001c390: 6172 6428 2828 7061 7261 6c6c 656c 5f63  ard(((parallel_c
-0001c3a0: 6f6e 6669 672e 6461 7461 5f70 6172 616c  onfig.data_paral
-0001c3b0: 6c65 6c2c 2031 2c20 3129 2c20 2870 6172  lel, 1, 1), (par
-0001c3c0: 616c 6c65 6c5f 636f 6e66 6967 2e64 6174  allel_config.dat
-0001c3d0: 615f 7061 7261 6c6c 656c 2c20 312c 2031  a_parallel, 1, 1
-0001c3e0: 2929 290a 2020 2020 2020 2020 2020 2020  ))).            
-0001c3f0: 7365 6c66 2e64 7479 7065 203d 206d 7374  self.dtype = mst
-0001c400: 7970 652e 666c 6f61 7431 360a 2020 2020  ype.float16.    
-0001c410: 2020 2020 2020 2020 7365 6c66 2e6b 6579          self.key
-0001c420: 5f70 6173 7420 3d20 4e6f 6e65 0a20 2020  _past = None.   
-0001c430: 2020 2020 2020 2020 2073 656c 662e 7661           self.va
-0001c440: 6c75 655f 7061 7374 203d 204e 6f6e 650a  lue_past = None.
-0001c450: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if 
-0001c460: 7365 6c66 2e75 7365 5f70 6173 743a 0a20  self.use_past:. 
-0001c470: 2020 2020 2020 2020 2020 2020 2020 2023                 #
-0001c480: 206f 7065 7261 746f 7220 7573 6564 2066   operator used f
-0001c490: 6f72 2073 7461 7465 2072 6575 7365 0a20  or state reuse. 
-0001c4a0: 2020 2020 2020 2020 2020 2020 2020 2073                 s
-0001c4b0: 656c 662e 7265 6475 6365 7375 6d20 3d20  elf.reducesum = 
-0001c4c0: 502e 5265 6475 6365 5375 6d28 292e 7368  P.ReduceSum().sh
-0001c4d0: 6172 6428 2828 312c 2031 2c20 312c 2031  ard(((1, 1, 1, 1
-0001c4e0: 292c 2929 0a20 2020 2020 2020 2020 2020  ),)).           
-0001c4f0: 2020 2020 2073 656c 662e 6e6f 745f 6571       self.not_eq
-0001c500: 7561 6c20 3d20 502e 4e6f 7445 7175 616c  ual = P.NotEqual
-0001c510: 2829 2e73 6861 7264 2828 2831 2c20 312c  ().shard(((1, 1,
-0001c520: 2031 2c20 3129 2c20 2829 2929 0a20 2020   1, 1), ())).   
-0001c530: 2020 2020 2020 2020 2020 2020 2073 656c               sel
-0001c540: 662e 736c 6963 6520 3d20 502e 5374 7269  f.slice = P.Stri
-0001c550: 6465 6453 6c69 6365 2829 2e73 6861 7264  dedSlice().shard
-0001c560: 2828 2831 2c20 312c 2031 2c20 3129 2c29  (((1, 1, 1, 1),)
-0001c570: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
-0001c580: 2020 7369 7a65 5f70 6572 5f68 6561 6420    size_per_head 
-0001c590: 3d20 6869 6464 656e 5f73 697a 6520 2f2f  = hidden_size //
-0001c5a0: 206e 756d 5f68 6561 6473 0a20 2020 2020   num_heads.     
-0001c5b0: 2020 2020 2020 2020 2020 2069 6620 7573             if us
-0001c5c0: 655f 7072 6f6d 7074 5f66 6c61 7368 5f61  e_prompt_flash_a
-0001c5d0: 7474 656e 7469 6f6e 206f 7220 7573 655f  ttention or use_
-0001c5e0: 666c 6173 685f 6174 7465 6e74 696f 6e3a  flash_attention:
-0001c5f0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0001c600: 2020 2020 2073 656c 662e 6b65 795f 7368       self.key_sh
-0001c610: 6170 6520 3d20 2862 6174 6368 5f73 697a  ape = (batch_siz
-0001c620: 652c 206e 756d 5f68 6561 6473 2c20 7365  e, num_heads, se
-0001c630: 715f 6c65 6e67 7468 2c20 7369 7a65 5f70  q_length, size_p
-0001c640: 6572 5f68 6561 6429 0a20 2020 2020 2020  er_head).       
-0001c650: 2020 2020 2020 2020 2065 6c73 653a 0a20           else:. 
-0001c660: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001c670: 2020 2073 656c 662e 6b65 795f 7368 6170     self.key_shap
-0001c680: 6520 3d20 2862 6174 6368 5f73 697a 652c  e = (batch_size,
-0001c690: 206e 756d 5f68 6561 6473 2c20 7369 7a65   num_heads, size
-0001c6a0: 5f70 6572 5f68 6561 642c 2073 6571 5f6c  _per_head, seq_l
-0001c6b0: 656e 6774 6829 0a20 2020 2020 2020 2020  ength).         
-0001c6c0: 2020 2020 2020 2073 656c 662e 7661 6c75         self.valu
-0001c6d0: 655f 7368 6170 6520 3d20 2862 6174 6368  e_shape = (batch
-0001c6e0: 5f73 697a 652c 206e 756d 5f68 6561 6473  _size, num_heads
-0001c6f0: 2c20 7365 715f 6c65 6e67 7468 2c20 7369  , seq_length, si
-0001c700: 7a65 5f70 6572 5f68 6561 6429 0a20 2020  ze_per_head).   
-0001c710: 2020 2020 2020 2020 2020 2020 2023 2070               # p
-0001c720: 6172 616d 6574 6572 7320 7361 7669 6e67  arameters saving
-0001c730: 206b 6579 2061 6e64 2076 616c 7565 2073   key and value s
-0001c740: 7461 7465 730a 2020 2020 2020 2020 2020  tates.          
-0001c750: 2020 2020 2020 7365 6c66 2e6b 6579 5f70        self.key_p
-0001c760: 6173 7420 3d20 5061 7261 6d65 7465 7228  ast = Parameter(
-0001c770: 5465 6e73 6f72 286e 702e 7a65 726f 7328  Tensor(np.zeros(
-0001c780: 7368 6170 653d 7365 6c66 2e6b 6579 5f73  shape=self.key_s
-0001c790: 6861 7065 292c 2073 656c 662e 6474 7970  hape), self.dtyp
-0001c7a0: 6529 2c20 6e61 6d65 3d22 6b65 795f 7061  e), name="key_pa
-0001c7b0: 7374 2229 0a20 2020 2020 2020 2020 2020  st").           
-0001c7c0: 2020 2020 2073 656c 662e 7661 6c75 655f       self.value_
-0001c7d0: 7061 7374 203d 2050 6172 616d 6574 6572  past = Parameter
-0001c7e0: 2854 656e 736f 7228 6e70 2e7a 6572 6f73  (Tensor(np.zeros
-0001c7f0: 2873 6861 7065 3d73 656c 662e 7661 6c75  (shape=self.valu
-0001c800: 655f 7368 6170 6529 2c20 7365 6c66 2e64  e_shape), self.d
-0001c810: 7479 7065 292c 206e 616d 653d 2276 616c  type), name="val
-0001c820: 7565 5f70 6173 7422 290a 2020 2020 2020  ue_past").      
-0001c830: 2020 2020 2020 2020 2020 7365 6c66 2e74            self.t
-0001c840: 696c 6520 3d20 502e 5469 6c65 2829 2e73  ile = P.Tile().s
-0001c850: 6861 7264 2828 2831 2c20 3129 2c29 290a  hard(((1, 1),)).
-0001c860: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001c870: 7365 6c66 2e6d 756c 203d 2050 2e4d 756c  self.mul = P.Mul
-0001c880: 2829 2e73 6861 7264 2828 2831 2c20 312c  ().shard(((1, 1,
-0001c890: 2031 2c20 3129 2c20 2831 2c29 2929 0a20   1, 1), (1,))). 
-0001c8a0: 2020 2020 2020 2020 2020 2020 2020 2073                 s
-0001c8b0: 656c 662e 6173 7369 676e 203d 2050 2e41  elf.assign = P.A
-0001c8c0: 7373 6967 6e28 292e 7368 6172 6428 2828  ssign().shard(((
-0001c8d0: 312c 2031 2c20 312c 2031 292c 2028 312c  1, 1, 1, 1), (1,
-0001c8e0: 2031 2c20 312c 2031 2929 290a 0a20 2020   1, 1, 1)))..   
-0001c8f0: 2020 2020 2020 2020 2069 6620 7061 7261           if para
-0001c900: 6c6c 656c 5f63 6f6e 6669 672e 7573 655f  llel_config.use_
-0001c910: 7365 715f 7061 7261 6c6c 656c 3a0a 2020  seq_parallel:.  
-0001c920: 2020 2020 2020 2020 2020 2020 2020 7365                se
-0001c930: 6c66 2e61 6464 2e73 6861 7264 2828 2870  lf.add.shard(((p
-0001c940: 6172 616c 6c65 6c5f 636f 6e66 6967 2e64  arallel_config.d
-0001c950: 6174 615f 7061 7261 6c6c 656c 202a 2070  ata_parallel * p
-0001c960: 6172 616c 6c65 6c5f 636f 6e66 6967 2e6d  arallel_config.m
-0001c970: 6f64 656c 5f70 6172 616c 6c65 6c2c 2031  odel_parallel, 1
-0001c980: 292c 0a20 2020 2020 2020 2020 2020 2020  ),.             
-0001c990: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001c9a0: 2020 2028 7061 7261 6c6c 656c 5f63 6f6e     (parallel_con
-0001c9b0: 6669 672e 6461 7461 5f70 6172 616c 6c65  fig.data_paralle
-0001c9c0: 6c20 2a20 7061 7261 6c6c 656c 5f63 6f6e  l * parallel_con
-0001c9d0: 6669 672e 6d6f 6465 6c5f 7061 7261 6c6c  fig.model_parall
-0001c9e0: 656c 2c20 3129 2929 0a20 2020 2020 2020  el, 1))).       
-0001c9f0: 2020 2020 2020 2020 2073 656c 662e 6c61           self.la
-0001ca00: 7965 726e 6f72 6d31 2e73 6861 7264 2828  yernorm1.shard((
-0001ca10: 2870 6172 616c 6c65 6c5f 636f 6e66 6967  (parallel_config
-0001ca20: 2e64 6174 615f 7061 7261 6c6c 656c 202a  .data_parallel *
-0001ca30: 2070 6172 616c 6c65 6c5f 636f 6e66 6967   parallel_config
-0001ca40: 2e6d 6f64 656c 5f70 6172 616c 6c65 6c2c  .model_parallel,
-0001ca50: 2031 292c 2929 0a20 2020 2020 2020 2020   1),)).         
-0001ca60: 2020 2020 2020 2073 656c 662e 6c61 7965         self.laye
-0001ca70: 726e 6f72 6d32 2e73 6861 7264 2828 2870  rnorm2.shard(((p
-0001ca80: 6172 616c 6c65 6c5f 636f 6e66 6967 2e64  arallel_config.d
-0001ca90: 6174 615f 7061 7261 6c6c 656c 202a 2070  ata_parallel * p
-0001caa0: 6172 616c 6c65 6c5f 636f 6e66 6967 2e6d  arallel_config.m
-0001cab0: 6f64 656c 5f70 6172 616c 6c65 6c2c 2031  odel_parallel, 1
-0001cac0: 292c 2929 0a20 2020 2020 2020 2020 2020  ),)).           
-0001cad0: 2020 2020 2069 6620 7061 7261 6c6c 656c       if parallel
-0001cae0: 5f63 6f6e 6669 672e 7265 636f 6d70 7574  _config.recomput
-0001caf0: 652e 7365 6c65 6374 5f72 6563 6f6d 7075  e.select_recompu
-0001cb00: 7465 3a0a 2020 2020 2020 2020 2020 2020  te:.            
-0001cb10: 2020 2020 2020 2020 2320 e6ad a4e5 a484          # ......
-0001cb20: e4bc 9ae6 b688 e880 97e8 be83 e5a4 a7e5  ................
-0001cb30: 8685 e5ad 98ef bc8c e5bc 80e5 90af e590  ................
-0001cb40: 8ee4 bc9a e68d 9fe5 a4b1 e4b8 80e9 83a8  ................
-0001cb50: e588 86e8 aea1 e7ae 97e6 80a7 e883 bd0a  ................
-0001cb60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001cb70: 2020 2020 7365 6c66 2e6c 6179 6572 6e6f      self.layerno
-0001cb80: 726d 322e 6c61 7965 725f 6e6f 726d 2e72  rm2.layer_norm.r
-0001cb90: 6563 6f6d 7075 7465 2829 0a20 2020 2020  ecompute().     
-0001cba0: 2020 2020 2020 2020 2020 2069 6620 6e6f             if no
-0001cbb0: 7420 7365 6c66 2e75 7365 5f6d 6f65 3a0a  t self.use_moe:.
-0001cbc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001cbd0: 2020 2020 7365 6c66 2e6f 7574 7075 742e      self.output.
-0001cbe0: 7072 6f6a 6563 7469 6f6e 2e73 6861 7264  projection.shard
-0001cbf0: 280a 2020 2020 2020 2020 2020 2020 2020  (.              
-0001cc00: 2020 2020 2020 2020 2020 7374 7261 7465            strate
-0001cc10: 6779 5f62 6961 733d 2828 7061 7261 6c6c  gy_bias=((parall
-0001cc20: 656c 5f63 6f6e 6669 672e 6461 7461 5f70  el_config.data_p
-0001cc30: 6172 616c 6c65 6c20 2a20 7061 7261 6c6c  arallel * parall
-0001cc40: 656c 5f63 6f6e 6669 672e 6d6f 6465 6c5f  el_config.model_
-0001cc50: 7061 7261 6c6c 656c 2c20 3129 2c20 2831  parallel, 1), (1
-0001cc60: 2c29 292c 0a20 2020 2020 2020 2020 2020  ,)),.           
-0001cc70: 2020 2020 2020 2020 2020 2020 2073 7472               str
-0001cc80: 6174 6567 795f 6d61 746d 756c 3d28 2870  ategy_matmul=((p
-0001cc90: 6172 616c 6c65 6c5f 636f 6e66 6967 2e64  arallel_config.d
-0001cca0: 6174 615f 7061 7261 6c6c 656c 2c20 7061  ata_parallel, pa
-0001ccb0: 7261 6c6c 656c 5f63 6f6e 6669 672e 6d6f  rallel_config.mo
-0001ccc0: 6465 6c5f 7061 7261 6c6c 656c 292c 0a20  del_parallel),. 
-0001ccd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001cce0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001ccf0: 2020 2020 2020 2020 2870 6172 616c 6c65          (paralle
-0001cd00: 6c5f 636f 6e66 6967 2e6d 6f64 656c 5f70  l_config.model_p
-0001cd10: 6172 616c 6c65 6c2c 2031 2929 2c0a 2020  arallel, 1)),.  
-0001cd20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001cd30: 2020 2020 2020 6f75 745f 7374 7261 7465        out_strate
-0001cd40: 6779 5f6d 6174 6d75 6c3d 2828 7061 7261  gy_matmul=((para
-0001cd50: 6c6c 656c 5f63 6f6e 6669 672e 6461 7461  llel_config.data
-0001cd60: 5f70 6172 616c 6c65 6c20 2a20 7061 7261  _parallel * para
-0001cd70: 6c6c 656c 5f63 6f6e 6669 672e 6d6f 6465  llel_config.mode
-0001cd80: 6c5f 7061 7261 6c6c 656c 2c20 3129 2c29  l_parallel, 1),)
-0001cd90: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
-0001cda0: 2020 2020 2020 7365 6c66 2e6f 7574 7075        self.outpu
-0001cdb0: 742e 6472 6f70 6f75 742e 6472 6f70 6f75  t.dropout.dropou
-0001cdc0: 742e 7368 6172 6428 0a20 2020 2020 2020  t.shard(.       
-0001cdd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001cde0: 2028 2870 6172 616c 6c65 6c5f 636f 6e66   ((parallel_conf
-0001cdf0: 6967 2e64 6174 615f 7061 7261 6c6c 656c  ig.data_parallel
-0001ce00: 202a 2070 6172 616c 6c65 6c5f 636f 6e66   * parallel_conf
-0001ce10: 6967 2e6d 6f64 656c 5f70 6172 616c 6c65  ig.model_paralle
-0001ce20: 6c2c 2031 292c 2929 0a20 2020 2020 2020  l, 1),)).       
-0001ce30: 2065 6c73 653a 0a20 2020 2020 2020 2020   else:.         
-0001ce40: 2020 2072 6169 7365 2052 756e 7469 6d65     raise Runtime
-0001ce50: 4572 726f 7228 6622 5468 6520 7b73 656c  Error(f"The {sel
-0001ce60: 662e 636c 735f 6e61 6d65 7d20 6f6e 6c79  f.cls_name} only
-0001ce70: 2073 7570 706f 7274 2073 6861 7264 696e   support shardin
-0001ce80: 6720 7072 6f70 6167 6174 696f 6e20 6f72  g propagation or
-0001ce90: 2022 0a20 2020 2020 2020 2020 2020 2020   ".             
-0001cea0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001ceb0: 2020 6622 7365 6d69 2d61 7574 6f20 7061    f"semi-auto pa
-0001cec0: 7261 6c6c 656c 206d 6f64 6520 6e6f 772e  rallel mode now.
-0001ced0: 2229 0a0a 2020 2020 6465 6620 636f 6e73  ")..    def cons
-0001cee0: 7472 7563 7428 7365 6c66 2c20 782c 2069  truct(self, x, i
-0001cef0: 6e70 7574 5f6d 6173 6b3d 4e6f 6e65 2c20  nput_mask=None, 
-0001cf00: 696e 6974 5f72 6573 6574 3d54 7275 652c  init_reset=True,
-0001cf10: 2062 6174 6368 5f76 616c 6964 5f6c 656e   batch_valid_len
-0001cf20: 6774 683d 4e6f 6e65 293a 0a20 2020 2020  gth=None):.     
-0001cf30: 2020 2022 2222 666f 7277 6172 6420 7072     """forward pr
-0001cf40: 6f63 6573 7322 2222 0a20 2020 2020 2020  ocess""".       
-0001cf50: 2073 656c 662e 5f63 6865 636b 5f69 6e70   self._check_inp
-0001cf60: 7574 2878 2c20 696e 7075 745f 6d61 736b  ut(x, input_mask
-0001cf70: 2c20 696e 6974 5f72 6573 6574 2c20 6261  , init_reset, ba
-0001cf80: 7463 685f 7661 6c69 645f 6c65 6e67 7468  tch_valid_length
-0001cf90: 290a 2020 2020 2020 2020 785f 7368 6170  ).        x_shap
-0001cfa0: 6520 3d20 462e 7368 6170 6528 7829 0a20  e = F.shape(x). 
-0001cfb0: 2020 2020 2020 2078 203d 2046 2e72 6573         x = F.res
-0001cfc0: 6861 7065 2878 2c20 282d 312c 2078 5f73  hape(x, (-1, x_s
-0001cfd0: 6861 7065 5b2d 315d 2929 0a20 2020 2020  hape[-1])).     
-0001cfe0: 2020 2069 6620 7365 6c66 2e70 6f73 745f     if self.post_
-0001cff0: 6c61 7965 726e 6f72 6d5f 7265 7369 6475  layernorm_residu
-0001d000: 616c 3a0a 2020 2020 2020 2020 2020 2020  al:.            
-0001d010: 696e 7075 745f 7820 3d20 780a 2020 2020  input_x = x.    
-0001d020: 2020 2020 656c 7365 3a0a 2020 2020 2020      else:.      
-0001d030: 2020 2020 2020 696e 7075 745f 7820 3d20        input_x = 
-0001d040: 7365 6c66 2e6c 6179 6572 6e6f 726d 3128  self.layernorm1(
-0001d050: 7829 0a20 2020 2020 2020 2069 6e70 7574  x).        input
-0001d060: 5f78 203d 2046 2e63 6173 7428 696e 7075  _x = F.cast(inpu
-0001d070: 745f 782c 2073 656c 662e 6474 7970 6529  t_x, self.dtype)
-0001d080: 0a0a 2020 2020 2020 2020 2320 696e 6469  ..        # indi
-0001d090: 6361 7465 2077 6865 7468 6572 2072 6573  cate whether res
-0001d0a0: 6574 2073 6176 6564 2073 7461 7465 730a  et saved states.
-0001d0b0: 2020 2020 2020 2020 6b65 795f 7265 7365          key_rese
-0001d0c0: 7420 3d20 4e6f 6e65 0a20 2020 2020 2020  t = None.       
-0001d0d0: 2076 616c 7565 5f72 6573 6574 203d 204e   value_reset = N
-0001d0e0: 6f6e 650a 0a20 2020 2020 2020 2069 6620  one..        if 
-0001d0f0: 7365 6c66 2e75 7365 5f70 6173 743a 0a20  self.use_past:. 
-0001d100: 2020 2020 2020 2020 2020 2023 2072 6573             # res
-0001d110: 6574 2073 7461 7465 732c 2069 6e69 745f  et states, init_
-0001d120: 7265 7365 7420 5472 7565 2066 6f72 2072  reset True for r
-0001d130: 6575 7365 2061 6e64 2046 616c 7365 2066  euse and False f
-0001d140: 6f72 2072 6573 6574 0a20 2020 2020 2020  or reset.       
-0001d150: 2020 2020 2073 656c 662e 6173 7369 676e       self.assign
-0001d160: 2873 656c 662e 6b65 795f 7061 7374 2c20  (self.key_past, 
-0001d170: 7365 6c66 2e6d 756c 2873 656c 662e 6b65  self.mul(self.ke
-0001d180: 795f 7061 7374 2c20 462e 6361 7374 2869  y_past, F.cast(i
-0001d190: 6e69 745f 7265 7365 742c 2073 656c 662e  nit_reset, self.
-0001d1a0: 6474 7970 6529 2929 0a20 2020 2020 2020  dtype))).       
-0001d1b0: 2020 2020 206b 6579 5f72 6573 6574 203d       key_reset =
-0001d1c0: 2073 656c 662e 6b65 795f 7061 7374 0a20   self.key_past. 
-0001d1d0: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-0001d1e0: 6173 7369 676e 2873 656c 662e 7661 6c75  assign(self.valu
-0001d1f0: 655f 7061 7374 2c20 7365 6c66 2e6d 756c  e_past, self.mul
-0001d200: 2873 656c 662e 7661 6c75 655f 7061 7374  (self.value_past
-0001d210: 2c20 462e 6361 7374 2869 6e69 745f 7265  , F.cast(init_re
-0001d220: 7365 742c 2073 656c 662e 6474 7970 6529  set, self.dtype)
-0001d230: 2929 0a20 2020 2020 2020 2020 2020 2076  )).            v
-0001d240: 616c 7565 5f72 6573 6574 203d 2073 656c  alue_reset = sel
-0001d250: 662e 7661 6c75 655f 7061 7374 0a20 2020  f.value_past.   
-0001d260: 2020 2020 2020 2020 2023 2061 6464 2064           # add d
-0001d270: 6570 656e 6465 6e63 7920 666f 7220 6465  ependency for de
-0001d280: 7369 7265 6420 6578 6563 7574 696f 6e20  sired execution 
-0001d290: 6f72 6465 720a 2020 2020 2020 2020 2020  order.          
-0001d2a0: 2020 696e 7075 745f 7820 3d20 462e 6465    input_x = F.de
-0001d2b0: 7065 6e64 2869 6e70 7574 5f78 2c20 6b65  pend(input_x, ke
-0001d2c0: 795f 7265 7365 7429 0a20 2020 2020 2020  y_reset).       
-0001d2d0: 2020 2020 2069 6e70 7574 5f78 203d 2046       input_x = F
-0001d2e0: 2e64 6570 656e 6428 696e 7075 745f 782c  .depend(input_x,
-0001d2f0: 2076 616c 7565 5f72 6573 6574 290a 0a20   value_reset).. 
-0001d300: 2020 2020 2020 2061 7474 656e 7469 6f6e         attention
-0001d310: 2c20 6c61 7965 725f 7072 6573 656e 7420  , layer_present 
-0001d320: 3d20 7365 6c66 2e61 7474 656e 7469 6f6e  = self.attention
-0001d330: 2869 6e70 7574 5f78 2c20 696e 7075 745f  (input_x, input_
-0001d340: 782c 2069 6e70 7574 5f78 2c20 696e 7075  x, input_x, inpu
-0001d350: 745f 6d61 736b 2c0a 2020 2020 2020 2020  t_mask,.        
-0001d360: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001d370: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001d380: 2020 2020 2020 2020 2020 7365 6c66 2e6b            self.k
-0001d390: 6579 5f70 6173 742c 2073 656c 662e 7661  ey_past, self.va
-0001d3a0: 6c75 655f 7061 7374 2c20 6261 7463 685f  lue_past, batch_
-0001d3b0: 7661 6c69 645f 6c65 6e67 7468 290a 2020  valid_length).  
-0001d3c0: 2020 2020 2020 2320 466f 7220 706f 7374        # For post
-0001d3d0: 2d6c 6179 6572 6e6f 726d 2074 6865 2069  -layernorm the i
-0001d3e0: 6e70 7574 7320 666f 7220 7265 7369 6475  nputs for residu
-0001d3f0: 616c 2070 6174 6820 6172 6520 6f75 7470  al path are outp
-0001d400: 7574 206f 6620 7365 6c66 2d61 7474 656e  ut of self-atten
-0001d410: 7469 6f6e 2061 6e64 206f 7574 7075 7420  tion and output 
-0001d420: 6f66 206c 6179 6572 6e6f 726d 0a20 2020  of layernorm.   
-0001d430: 2020 2020 2069 6620 7365 6c66 2e70 6f73       if self.pos
-0001d440: 745f 6c61 7965 726e 6f72 6d5f 7265 7369  t_layernorm_resi
-0001d450: 6475 616c 3a0a 2020 2020 2020 2020 2020  dual:.          
-0001d460: 2020 7820 3d20 7365 6c66 2e61 6464 2869    x = self.add(i
-0001d470: 6e70 7574 5f78 2c20 6174 7465 6e74 696f  nput_x, attentio
-0001d480: 6e29 0a20 2020 2020 2020 2023 2046 6f72  n).        # For
-0001d490: 2070 7265 2d6c 6179 6572 6e6f 726d 2074   pre-layernorm t
-0001d4a0: 6865 2069 6e70 7574 7320 666f 7220 7265  he inputs for re
-0001d4b0: 7369 6475 616c 2070 6174 6820 6172 6520  sidual path are 
-0001d4c0: 6f75 7470 7574 206f 6620 7365 6c66 2d61  output of self-a
-0001d4d0: 7474 656e 7469 6f6e 2061 6e64 2069 6e70  ttention and inp
-0001d4e0: 7574 206f 6620 7468 6973 206c 6179 6572  ut of this layer
-0001d4f0: 0a20 2020 2020 2020 2065 6c73 653a 0a20  .        else:. 
-0001d500: 2020 2020 2020 2020 2020 2078 203d 2073             x = s
-0001d510: 656c 662e 6164 6428 782c 2061 7474 656e  elf.add(x, atten
-0001d520: 7469 6f6e 290a 0a20 2020 2020 2020 206f  tion)..        o
-0001d530: 7574 7075 745f 7820 3d20 7365 6c66 2e6c  utput_x = self.l
-0001d540: 6179 6572 6e6f 726d 3228 7829 0a20 2020  ayernorm2(x).   
-0001d550: 2020 2020 206f 7574 7075 745f 7820 3d20       output_x = 
-0001d560: 462e 6361 7374 286f 7574 7075 745f 782c  F.cast(output_x,
-0001d570: 2073 656c 662e 6474 7970 6529 0a20 2020   self.dtype).   
-0001d580: 2020 2020 2061 7578 5f6c 6f73 7320 3d20       aux_loss = 
-0001d590: 4e6f 6e65 0a20 2020 2020 2020 2069 6620  None.        if 
-0001d5a0: 7365 6c66 2e75 7365 5f6d 6f65 3a0a 2020  self.use_moe:.  
-0001d5b0: 2020 2020 2020 2020 2020 6d6c 705f 6c6f            mlp_lo
-0001d5c0: 6769 742c 2061 7578 5f6c 6f73 7320 3d20  git, aux_loss = 
-0001d5d0: 7365 6c66 2e6f 7574 7075 7428 6f75 7470  self.output(outp
-0001d5e0: 7574 5f78 290a 2020 2020 2020 2020 656c  ut_x).        el
-0001d5f0: 7365 3a0a 2020 2020 2020 2020 2020 2020  se:.            
-0001d600: 6d6c 705f 6c6f 6769 7420 3d20 7365 6c66  mlp_logit = self
-0001d610: 2e6f 7574 7075 7428 6f75 7470 7574 5f78  .output(output_x
-0001d620: 290a 0a20 2020 2020 2020 2076 616c 7565  )..        value
-0001d630: 5f75 7064 6174 6520 3d20 4e6f 6e65 0a20  _update = None. 
-0001d640: 2020 2020 2020 206b 6579 5f75 7064 6174         key_updat
-0001d650: 6520 3d20 4e6f 6e65 0a20 2020 2020 2020  e = None.       
-0001d660: 2069 6620 7365 6c66 2e75 7365 5f70 6173   if self.use_pas
-0001d670: 743a 0a20 2020 2020 2020 2020 2020 2023  t:.            #
-0001d680: 2063 7572 7265 6e74 206b 6579 2061 6e64   current key and
-0001d690: 2076 616c 7565 0a20 2020 2020 2020 2020   value.         
-0001d6a0: 2020 206b 6579 5f70 7265 7365 6e74 2c20     key_present, 
-0001d6b0: 7661 6c75 655f 7072 6573 656e 7420 3d20  value_present = 
-0001d6c0: 6c61 7965 725f 7072 6573 656e 740a 2020  layer_present.  
-0001d6d0: 2020 2020 2020 2020 2020 2320 7570 6461            # upda
-0001d6e0: 7465 206b 6579 2061 6e64 2076 616c 7565  te key and value
-0001d6f0: 2063 616c 6375 6c61 7465 6420 7468 6973   calculated this
-0001d700: 2073 7465 700a 2020 2020 2020 2020 2020   step.          
-0001d710: 2020 7365 6c66 2e61 7373 6967 6e28 7365    self.assign(se
-0001d720: 6c66 2e6b 6579 5f70 6173 742c 206b 6579  lf.key_past, key
-0001d730: 5f70 7265 7365 6e74 290a 2020 2020 2020  _present).      
-0001d740: 2020 2020 2020 6b65 795f 7570 6461 7465        key_update
-0001d750: 203d 2073 656c 662e 6b65 795f 7061 7374   = self.key_past
-0001d760: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-0001d770: 662e 6173 7369 676e 2873 656c 662e 7661  f.assign(self.va
-0001d780: 6c75 655f 7061 7374 2c20 7661 6c75 655f  lue_past, value_
-0001d790: 7072 6573 656e 7429 0a20 2020 2020 2020  present).       
-0001d7a0: 2020 2020 2076 616c 7565 5f75 7064 6174       value_updat
-0001d7b0: 6520 3d20 7365 6c66 2e76 616c 7565 5f70  e = self.value_p
-0001d7c0: 6173 740a 2020 2020 2020 2020 2020 2020  ast.            
-0001d7d0: 2320 6164 6420 6465 7065 6e64 656e 6379  # add dependency
-0001d7e0: 2066 6f72 2064 6573 6972 6564 2065 7865   for desired exe
-0001d7f0: 6375 7469 6f6e 206f 7264 6572 0a20 2020  cution order.   
-0001d800: 2020 2020 2020 2020 206b 6579 5f75 7064           key_upd
-0001d810: 6174 6520 3d20 462e 6465 7065 6e64 286b  ate = F.depend(k
-0001d820: 6579 5f75 7064 6174 652c 206b 6579 5f72  ey_update, key_r
-0001d830: 6573 6574 290a 2020 2020 2020 2020 2020  eset).          
-0001d840: 2020 7661 6c75 655f 7570 6461 7465 203d    value_update =
-0001d850: 2046 2e64 6570 656e 6428 7661 6c75 655f   F.depend(value_
-0001d860: 7570 6461 7465 2c20 7661 6c75 655f 7265  update, value_re
-0001d870: 7365 7429 0a0a 2020 2020 2020 2020 2320  set)..        # 
-0001d880: 6164 6420 6465 7065 6e64 656e 6379 2066  add dependency f
-0001d890: 6f72 2064 6573 6972 6564 2065 7865 6375  or desired execu
-0001d8a0: 7469 6f6e 206f 7264 6572 0a20 2020 2020  tion order.     
-0001d8b0: 2020 206d 6c70 5f6c 6f67 6974 203d 2046     mlp_logit = F
-0001d8c0: 2e64 6570 656e 6428 6d6c 705f 6c6f 6769  .depend(mlp_logi
-0001d8d0: 742c 2076 616c 7565 5f75 7064 6174 6529  t, value_update)
-0001d8e0: 0a20 2020 2020 2020 206d 6c70 5f6c 6f67  .        mlp_log
-0001d8f0: 6974 203d 2046 2e64 6570 656e 6428 6d6c  it = F.depend(ml
-0001d900: 705f 6c6f 6769 742c 206b 6579 5f75 7064  p_logit, key_upd
-0001d910: 6174 6529 0a0a 2020 2020 2020 2020 2320  ate)..        # 
-0001d920: 6966 2073 6861 7065 2069 7320 3364 2c20  if shape is 3d, 
-0001d930: 7765 2072 6573 6861 7065 2074 6865 2069  we reshape the i
-0001d940: 6e70 7574 7320 6f66 2074 6865 2061 6464  nputs of the add
-0001d950: 0a20 2020 2020 2020 2069 6620 6c65 6e28  .        if len(
-0001d960: 785f 7368 6170 6529 203d 3d20 333a 0a20  x_shape) == 3:. 
-0001d970: 2020 2020 2020 2020 2020 206f 7574 7075             outpu
-0001d980: 745f 7820 3d20 502e 5265 7368 6170 6528  t_x = P.Reshape(
-0001d990: 2928 6f75 7470 7574 5f78 2c20 785f 7368  )(output_x, x_sh
-0001d9a0: 6170 6529 0a20 2020 2020 2020 2020 2020  ape).           
-0001d9b0: 206d 6c70 5f6c 6f67 6974 203d 2050 2e52   mlp_logit = P.R
-0001d9c0: 6573 6861 7065 2829 286d 6c70 5f6c 6f67  eshape()(mlp_log
-0001d9d0: 6974 2c20 785f 7368 6170 6529 0a20 2020  it, x_shape).   
-0001d9e0: 2020 2020 2020 2020 2078 203d 2050 2e52           x = P.R
-0001d9f0: 6573 6861 7065 2829 2878 2c20 785f 7368  eshape()(x, x_sh
-0001da00: 6170 6529 0a0a 2020 2020 2020 2020 2020  ape)..          
-0001da10: 2020 6966 2073 656c 662e 706f 7374 5f6c    if self.post_l
-0001da20: 6179 6572 6e6f 726d 5f72 6573 6964 7561  ayernorm_residua
-0001da30: 6c3a 0a20 2020 2020 2020 2020 2020 2020  l:.             
-0001da40: 2020 206f 7574 7075 7420 3d20 7365 6c66     output = self
-0001da50: 2e61 6464 5f33 6428 6f75 7470 7574 5f78  .add_3d(output_x
-0001da60: 2c20 6d6c 705f 6c6f 6769 7429 0a20 2020  , mlp_logit).   
-0001da70: 2020 2020 2020 2020 2020 2020 206f 7574               out
-0001da80: 7075 7420 3d20 462e 7265 7368 6170 6528  put = F.reshape(
-0001da90: 6f75 7470 7574 2c20 282d 312c 2078 5f73  output, (-1, x_s
-0001daa0: 6861 7065 5b2d 315d 2929 0a20 2020 2020  hape[-1])).     
-0001dab0: 2020 2020 2020 2020 2020 206f 7574 7075             outpu
-0001dac0: 7420 3d20 7365 6c66 2e6c 6179 6572 6e6f  t = self.layerno
-0001dad0: 726d 3128 6f75 7470 7574 290a 2020 2020  rm1(output).    
-0001dae0: 2020 2020 2020 2020 2020 2020 6f75 7470              outp
-0001daf0: 7574 203d 2046 2e72 6573 6861 7065 286f  ut = F.reshape(o
-0001db00: 7574 7075 742c 2078 5f73 6861 7065 290a  utput, x_shape).
-0001db10: 2020 2020 2020 2020 2020 2020 656c 7365              else
-0001db20: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
-0001db30: 2020 6f75 7470 7574 203d 2073 656c 662e    output = self.
-0001db40: 6164 645f 3364 2878 2c20 6d6c 705f 6c6f  add_3d(x, mlp_lo
-0001db50: 6769 7429 0a20 2020 2020 2020 2065 6c73  git).        els
-0001db60: 653a 0a20 2020 2020 2020 2020 2020 2069  e:.            i
-0001db70: 6620 7365 6c66 2e70 6f73 745f 6c61 7965  f self.post_laye
-0001db80: 726e 6f72 6d5f 7265 7369 6475 616c 3a0a  rnorm_residual:.
-0001db90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001dba0: 6f75 7470 7574 203d 2073 656c 662e 6164  output = self.ad
-0001dbb0: 6428 6f75 7470 7574 5f78 2c20 6d6c 705f  d(output_x, mlp_
-0001dbc0: 6c6f 6769 7429 0a20 2020 2020 2020 2020  logit).         
-0001dbd0: 2020 2020 2020 206f 7574 7075 7420 3d20         output = 
-0001dbe0: 7365 6c66 2e6c 6179 6572 6e6f 726d 3128  self.layernorm1(
-0001dbf0: 6f75 7470 7574 290a 2020 2020 2020 2020  output).        
-0001dc00: 2020 2020 656c 7365 3a0a 2020 2020 2020      else:.      
-0001dc10: 2020 2020 2020 2020 2020 6f75 7470 7574            output
-0001dc20: 203d 2073 656c 662e 6164 6428 782c 206d   = self.add(x, m
-0001dc30: 6c70 5f6c 6f67 6974 290a 2020 2020 2020  lp_logit).      
-0001dc40: 2020 2020 2020 6f75 7470 7574 203d 2046        output = F
-0001dc50: 2e72 6573 6861 7065 286f 7574 7075 742c  .reshape(output,
-0001dc60: 2078 5f73 6861 7065 290a 0a20 2020 2020   x_shape)..     
-0001dc70: 2020 2069 6620 7365 6c66 2e75 7365 5f6d     if self.use_m
-0001dc80: 6f65 3a0a 2020 2020 2020 2020 2020 2020  oe:.            
-0001dc90: 7265 7475 726e 206f 7574 7075 742c 206c  return output, l
-0001dca0: 6179 6572 5f70 7265 7365 6e74 2c20 6175  ayer_present, au
-0001dcb0: 785f 6c6f 7373 0a20 2020 2020 2020 2072  x_loss.        r
-0001dcc0: 6574 7572 6e20 6f75 7470 7574 2c20 6c61  eturn output, la
-0001dcd0: 7965 725f 7072 6573 656e 740a 0a20 2020  yer_present..   
-0001dce0: 2064 6566 205f 6368 6563 6b5f 696e 7075   def _check_inpu
-0001dcf0: 7428 7365 6c66 2c20 782c 2069 6e70 7574  t(self, x, input
-0001dd00: 5f6d 6173 6b2c 2069 6e69 745f 7265 7365  _mask, init_rese
-0001dd10: 742c 2062 6174 6368 5f76 616c 6964 5f6c  t, batch_valid_l
-0001dd20: 656e 6774 6829 3a0a 2020 2020 2020 2020  ength):.        
-0001dd30: 7222 2222 4368 6563 6b20 696e 7075 7473  r"""Check inputs
-0001dd40: 2222 220a 2020 2020 2020 2020 5f63 6865  """.        _che
-0001dd50: 636b 5f69 6e70 7574 5f64 7479 7065 2846  ck_input_dtype(F
-0001dd60: 2e64 7479 7065 2878 292c 2022 7822 2c20  .dtype(x), "x", 
-0001dd70: 5b6d 7374 7970 652e 666c 6f61 7433 322c  [mstype.float32,
-0001dd80: 206d 7374 7970 652e 666c 6f61 7431 362c   mstype.float16,
-0001dd90: 206d 7374 7970 652e 6266 6c6f 6174 3136   mstype.bfloat16
-0001dda0: 5d2c 2073 656c 662e 636c 735f 6e61 6d65  ], self.cls_name
-0001ddb0: 290a 2020 2020 2020 2020 6966 2069 6e70  ).        if inp
-0001ddc0: 7574 5f6d 6173 6b20 6973 206e 6f74 204e  ut_mask is not N
-0001ddd0: 6f6e 653a 0a20 2020 2020 2020 2020 2020  one:.           
-0001dde0: 205f 6368 6563 6b5f 696e 7075 745f 6474   _check_input_dt
-0001ddf0: 7970 6528 462e 6474 7970 6528 696e 7075  ype(F.dtype(inpu
-0001de00: 745f 6d61 736b 292c 2022 696e 7075 745f  t_mask), "input_
-0001de10: 6d61 736b 222c 0a20 2020 2020 2020 2020  mask",.         
-0001de20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001de30: 2020 2020 2020 5b6d 7374 7970 652e 666c        [mstype.fl
-0001de40: 6f61 7433 322c 206d 7374 7970 652e 666c  oat32, mstype.fl
-0001de50: 6f61 7431 362c 206d 7374 7970 652e 6266  oat16, mstype.bf
-0001de60: 6c6f 6174 3136 5d2c 2073 656c 662e 636c  loat16], self.cl
-0001de70: 735f 6e61 6d65 290a 0a20 2020 2020 2020  s_name)..       
-0001de80: 2069 6e69 745f 7265 7365 745f 6973 5f74   init_reset_is_t
-0001de90: 656e 736f 7220 3d20 6973 696e 7374 616e  ensor = isinstan
-0001dea0: 6365 2869 6e69 745f 7265 7365 742c 2054  ce(init_reset, T
-0001deb0: 656e 736f 7229 0a20 2020 2020 2020 2069  ensor).        i
-0001dec0: 6e69 745f 7265 7365 745f 6973 5f64 6566  nit_reset_is_def
-0001ded0: 6175 6c74 203d 2069 6e69 745f 7265 7365  ault = init_rese
-0001dee0: 7420 6973 2054 7275 650a 2020 2020 2020  t is True.      
-0001def0: 2020 6261 7463 685f 7661 6c69 645f 6c65    batch_valid_le
-0001df00: 6e67 7468 5f69 735f 7465 6e73 6f72 203d  ngth_is_tensor =
-0001df10: 2069 7369 6e73 7461 6e63 6528 6261 7463   isinstance(batc
-0001df20: 685f 7661 6c69 645f 6c65 6e67 7468 2c20  h_valid_length, 
-0001df30: 5465 6e73 6f72 290a 2020 2020 2020 2020  Tensor).        
-0001df40: 6261 7463 685f 6973 5f64 6566 6175 6c74  batch_is_default
-0001df50: 203d 2062 6174 6368 5f76 616c 6964 5f6c   = batch_valid_l
-0001df60: 656e 6774 6820 6973 204e 6f6e 650a 2020  ength is None.  
-0001df70: 2020 2020 2020 5f63 6865 636b 5f70 6173        _check_pas
-0001df80: 745f 6e6f 6e65 5f69 6e70 7574 5f6e 6f6e  t_none_input_non
-0001df90: 6528 7365 6c66 2e75 7365 5f70 6173 742c  e(self.use_past,
-0001dfa0: 2022 696e 6974 5f72 6573 6574 222c 2073   "init_reset", s
-0001dfb0: 656c 662e 636c 735f 6e61 6d65 2c20 5472  elf.cls_name, Tr
-0001dfc0: 7565 2c20 696e 6974 5f72 6573 6574 5f69  ue, init_reset_i
-0001dfd0: 735f 7465 6e73 6f72 2c0a 2020 2020 2020  s_tensor,.      
-0001dfe0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001dff0: 2020 2020 2020 2020 2020 2020 2020 696e                in
-0001e000: 6974 5f72 6573 6574 5f69 735f 6465 6661  it_reset_is_defa
-0001e010: 756c 7429 0a20 2020 2020 2020 205f 6368  ult).        _ch
-0001e020: 6563 6b5f 7061 7374 5f6e 6f6e 655f 696e  eck_past_none_in
-0001e030: 7075 745f 6e6f 6e65 2873 656c 662e 7573  put_none(self.us
-0001e040: 655f 7061 7374 2c20 2262 6174 6368 5f76  e_past, "batch_v
-0001e050: 616c 6964 5f6c 656e 6774 6822 2c20 7365  alid_length", se
-0001e060: 6c66 2e63 6c73 5f6e 616d 652c 204e 6f6e  lf.cls_name, Non
-0001e070: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
-0001e080: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001e090: 2020 2020 2020 2062 6174 6368 5f76 616c         batch_val
-0001e0a0: 6964 5f6c 656e 6774 685f 6973 5f74 656e  id_length_is_ten
-0001e0b0: 736f 722c 2062 6174 6368 5f69 735f 6465  sor, batch_is_de
-0001e0c0: 6661 756c 7429 0a0a 2020 2020 2020 2020  fault)..        
-0001e0d0: 6966 2073 656c 662e 7573 655f 7061 7374  if self.use_past
-0001e0e0: 3a0a 2020 2020 2020 2020 2020 2020 5f63  :.            _c
-0001e0f0: 6865 636b 5f69 6e70 7574 5f64 7479 7065  heck_input_dtype
-0001e100: 2846 2e64 7479 7065 2862 6174 6368 5f76  (F.dtype(batch_v
-0001e110: 616c 6964 5f6c 656e 6774 6829 2c20 2262  alid_length), "b
-0001e120: 6174 6368 5f76 616c 6964 5f6c 656e 6774  atch_valid_lengt
-0001e130: 6822 2c20 5b6d 7374 7970 652e 696e 7433  h", [mstype.int3
-0001e140: 325d 2c20 7365 6c66 2e63 6c73 5f6e 616d  2], self.cls_nam
-0001e150: 6529 0a20 2020 2020 2020 2072 6574 7572  e).        retur
-0001e160: 6e20 5472 7565 0a0a 0a63 6c61 7373 2054  n True...class T
-0001e170: 7261 6e73 666f 726d 6572 4465 636f 6465  ransformerDecode
-0001e180: 724c 6179 6572 2843 656c 6c29 3a0a 2020  rLayer(Cell):.  
-0001e190: 2020 7222 2222 0a20 2020 2020 2020 2054    r""".        T
-0001e1a0: 7261 6e73 666f 726d 6572 2044 6563 6f64  ransformer Decod
-0001e1b0: 6572 204c 6179 6572 2e20 5468 6973 2069  er Layer. This i
-0001e1c0: 7320 616e 2069 6d70 6c65 6d65 6e74 6174  s an implementat
-0001e1d0: 696f 6e20 6f66 2074 6865 2073 696e 676c  ion of the singl
-0001e1e0: 6520 6c61 7965 7220 6f66 2074 6865 2074  e layer of the t
-0001e1f0: 7261 6e73 666f 726d 6572 0a20 2020 2020  ransformer.     
-0001e200: 2020 2064 6563 6f64 6572 206c 6179 6572     decoder layer
-0001e210: 2c20 696e 636c 7564 696e 6720 7365 6c66  , including self
-0001e220: 2d61 7474 656e 7469 6f6e 2c20 6372 6f73  -attention, cros
-0001e230: 7320 6174 7465 6e74 696f 6e20 616e 6420  s attention and 
-0001e240: 6665 6564 7761 7264 206c 6179 6572 2e20  feedward layer. 
-0001e250: 5768 656e 2074 6865 2065 6e63 6f64 6572  When the encoder
-0001e260: 5f6f 7574 7075 7420 6973 204e 6f6e 652c  _output is None,
-0001e270: 0a20 2020 2020 2020 2074 6865 2063 726f  .        the cro
-0001e280: 7373 2061 7474 656e 7469 6f6e 2077 696c  ss attention wil
-0001e290: 6c20 6e6f 7420 6265 2065 6666 6563 7469  l not be effecti
-0001e2a0: 7665 2e0a 0a20 2020 2020 2020 2041 7267  ve...        Arg
-0001e2b0: 733a 0a20 2020 2020 2020 2020 2020 2068  s:.            h
-0001e2c0: 6964 6465 6e5f 7369 7a65 2869 6e74 293a  idden_size(int):
-0001e2d0: 2054 6865 2068 6964 6465 6e20 7369 7a65   The hidden size
-0001e2e0: 206f 6620 7468 6520 696e 7075 742e 0a20   of the input.. 
-0001e2f0: 2020 2020 2020 2020 2020 2066 666e 5f68             ffn_h
-0001e300: 6964 6465 6e5f 7369 7a65 2869 6e74 293a  idden_size(int):
-0001e310: 2054 6865 2068 6964 6465 6e20 7369 7a65   The hidden size
-0001e320: 206f 6620 626f 7474 6c65 6e65 636b 2069   of bottleneck i
-0001e330: 6e20 7468 6520 6665 6564 666f 7277 6172  n the feedforwar
-0001e340: 6420 6c61 7965 722e 0a20 2020 2020 2020  d layer..       
-0001e350: 2020 2020 206e 756d 5f68 6561 6473 2869       num_heads(i
-0001e360: 6e74 293a 2054 6865 206e 756d 6265 7220  nt): The number 
-0001e370: 6f66 2074 6865 2068 6561 6473 2e0a 2020  of the heads..  
-0001e380: 2020 2020 2020 2020 2020 6261 7463 685f            batch_
-0001e390: 7369 7a65 2869 6e74 293a 2054 6865 2062  size(int): The b
-0001e3a0: 6174 6368 2073 697a 6520 6f66 2074 6865  atch size of the
-0001e3b0: 2069 6e70 7574 2074 656e 736f 7220 7768   input tensor wh
-0001e3c0: 656e 2064 6f20 696e 6372 656e 6d65 6e74  en do increnment
-0001e3d0: 616c 2070 7265 6469 6374 696f 6e2e 2053  al prediction. S
-0001e3e0: 686f 756c 6420 6265 2061 2070 6f73 6974  hould be a posit
-0001e3f0: 6976 650a 2020 2020 2020 2020 2020 2020  ive.            
-0001e400: 2020 2020 7661 6c75 652e 2057 6865 6e20      value. When 
-0001e410: 646f 2074 7261 696e 696e 6720 6f72 2070  do training or p
-0001e420: 7265 6469 6374 696f 6e2c 2074 6865 2061  rediction, the a
-0001e430: 7267 756d 656e 7420 7769 6c6c 206e 6f74  rgument will not
-0001e440: 2077 6f72 6b20 616e 6420 7468 6520 7573   work and the us
-0001e450: 6572 2063 616e 206a 7573 7420 7061 7373  er can just pass
-0001e460: 204e 6f6e 6520 746f 0a20 2020 2020 2020   None to.       
-0001e470: 2020 2020 2020 2020 2074 6865 2061 7267           the arg
-0001e480: 756d 656e 742e 0a20 2020 2020 2020 2020  ument..         
-0001e490: 2020 2073 7263 5f73 6571 5f6c 656e 6774     src_seq_lengt
-0001e4a0: 6828 696e 7429 3a20 5468 6520 696e 7075  h(int): The inpu
-0001e4b0: 7420 736f 7572 6365 2073 6571 7565 6e63  t source sequenc
-0001e4c0: 6520 6c65 6e67 7468 2e0a 2020 2020 2020  e length..      
-0001e4d0: 2020 2020 2020 7467 745f 7365 715f 6c65        tgt_seq_le
-0001e4e0: 6e67 7468 2869 6e74 293a 2054 6865 2069  ngth(int): The i
-0001e4f0: 6e70 7574 2074 6172 6765 7420 7365 7175  nput target sequ
-0001e500: 656e 6365 206c 656e 6774 682e 0a20 2020  ence length..   
-0001e510: 2020 2020 2020 2020 2061 7474 656e 7469           attenti
-0001e520: 6f6e 5f64 726f 706f 7574 5f72 6174 6528  on_dropout_rate(
-0001e530: 666c 6f61 7429 3a20 5468 6520 6472 6f70  float): The drop
-0001e540: 6f75 7420 7261 7465 206f 6620 7468 6520  out rate of the 
-0001e550: 6174 7465 6e74 696f 6e20 7363 6f72 6573  attention scores
-0001e560: 2e20 4465 6661 756c 743a 302e 312e 0a20  . Default:0.1.. 
-0001e570: 2020 2020 2020 2020 2020 2068 6964 6465             hidde
-0001e580: 6e5f 6472 6f70 6f75 745f 7261 7465 2866  n_dropout_rate(f
-0001e590: 6c6f 6174 293a 2054 6865 2064 726f 706f  loat): The dropo
-0001e5a0: 7574 2072 6174 6520 6f66 2074 6865 2066  ut rate of the f
-0001e5b0: 696e 616c 206f 7574 7075 7420 6f66 2074  inal output of t
-0001e5c0: 6865 206c 6179 6572 2e20 4465 6661 756c  he layer. Defaul
-0001e5d0: 743a 302e 312e 0a20 2020 2020 2020 2020  t:0.1..         
-0001e5e0: 2020 2070 6f73 745f 6c61 7965 726e 6f72     post_layernor
-0001e5f0: 6d5f 7265 7369 6475 616c 2862 6f6f 6c29  m_residual(bool)
-0001e600: 3a20 446f 2072 6573 6964 7561 6c73 2061  : Do residuals a
-0001e610: 6464 7320 6265 666f 7265 2074 6865 206c  dds before the l
-0001e620: 6179 6572 6e6f 726d 2e20 4465 6661 756c  ayernorm. Defaul
-0001e630: 7420 4661 6c73 652e 0a20 2020 2020 2020  t False..       
-0001e640: 2020 2020 2075 7365 5f70 6173 7428 626f       use_past(bo
-0001e650: 6f6c 293a 2055 7365 2074 6865 2070 6173  ol): Use the pas
-0001e660: 7420 7374 6174 6520 746f 2063 6f6d 7075  t state to compu
-0001e670: 7465 2c20 7573 6564 2066 6f72 2069 6e63  te, used for inc
-0001e680: 7265 6d65 6e74 616c 2070 7265 6469 6374  remental predict
-0001e690: 696f 6e2e 2044 6566 6175 6c74 2046 616c  ion. Default Fal
-0001e6a0: 7365 2e0a 2020 2020 2020 2020 2020 2020  se..            
-0001e6b0: 6c61 7965 726e 6f72 6d5f 636f 6d70 7574  layernorm_comput
-0001e6c0: 655f 7479 7065 2864 7479 7065 2e4e 756d  e_type(dtype.Num
-0001e6d0: 6265 7229 3a20 5468 6520 636f 6d70 7574  ber): The comput
-0001e6e0: 6174 696f 6e20 7479 7065 206f 6620 7468  ation type of th
-0001e6f0: 6520 6c61 7965 726e 6f72 6d2e 0a20 2020  e layernorm..   
-0001e700: 2020 2020 2020 2020 2020 2020 2053 686f               Sho
-0001e710: 756c 6420 6265 2064 7479 7065 2e66 6c6f  uld be dtype.flo
-0001e720: 6174 3332 206f 7220 6474 7970 652e 666c  at32 or dtype.fl
-0001e730: 6f61 7431 362e 2044 6566 6175 6c74 2064  oat16. Default d
-0001e740: 7479 7065 2e66 6c6f 6174 3332 2e0a 2020  type.float32..  
-0001e750: 2020 2020 2020 2020 2020 736f 6674 6d61            softma
-0001e760: 785f 636f 6d70 7574 655f 7479 7065 2864  x_compute_type(d
-0001e770: 7479 7065 2e4e 756d 6265 7229 3a20 5468  type.Number): Th
-0001e780: 6520 636f 6d70 7574 6174 696f 6e20 7479  e computation ty
-0001e790: 7065 206f 6620 7468 6520 736f 6674 6d61  pe of the softma
-0001e7a0: 7820 696e 2074 6865 2061 7474 656e 7469  x in the attenti
-0001e7b0: 6f6e 2e0a 2020 2020 2020 2020 2020 2020  on..            
-0001e7c0: 2020 2020 5368 6f75 6c64 2062 6520 6474      Should be dt
-0001e7d0: 7970 652e 666c 6f61 7433 3220 6f72 2064  ype.float32 or d
-0001e7e0: 7479 7065 2e66 6c6f 6174 3136 2e20 4465  type.float16. De
-0001e7f0: 6661 756c 7420 6d73 7479 7065 2e66 6c6f  fault mstype.flo
-0001e800: 6174 3332 2e0a 2020 2020 2020 2020 2020  at32..          
-0001e810: 2020 7061 7261 6d5f 696e 6974 5f74 7970    param_init_typ
-0001e820: 6528 6474 7970 652e 4e75 6d62 6572 293a  e(dtype.Number):
-0001e830: 2054 6865 2070 6172 616d 6574 6572 2069   The parameter i
-0001e840: 6e69 7469 616c 697a 6174 696f 6e20 7479  nitialization ty
-0001e850: 7065 206f 6620 7468 6520 6d6f 6475 6c65  pe of the module
-0001e860: 2e0a 2020 2020 2020 2020 2020 2020 2020  ..              
-0001e870: 2020 5368 6f75 6c64 2062 6520 6474 7970    Should be dtyp
-0001e880: 652e 666c 6f61 7433 3220 6f72 2064 7479  e.float32 or dty
-0001e890: 7065 2e66 6c6f 6174 3136 2e20 4465 6661  pe.float16. Defa
-0001e8a0: 756c 7420 6474 7970 652e 666c 6f61 7433  ult dtype.float3
-0001e8b0: 322e 0a20 2020 2020 2020 2020 2020 2068  2..            h
-0001e8c0: 6964 6465 6e5f 6163 7420 2873 7472 2c20  idden_act (str, 
-0001e8d0: 6e6e 2e43 656c 6c29 3a20 5468 6520 6163  nn.Cell): The ac
-0001e8e0: 7469 7661 7469 6f6e 206f 6620 7468 6520  tivation of the 
-0001e8f0: 696e 7465 726e 616c 2066 6565 6466 6f72  internal feedfor
-0001e900: 7761 7264 206c 6179 6572 2e20 5375 7070  ward layer. Supp
-0001e910: 6f72 7473 2027 7265 6c75 272c 0a20 2020  orts 'relu',.   
-0001e920: 2020 2020 2020 2020 2020 2020 2027 7265               're
-0001e930: 6c75 3627 2c20 2774 616e 6827 2c20 2767  lu6', 'tanh', 'g
-0001e940: 656c 7527 2c20 2766 6173 745f 6765 6c75  elu', 'fast_gelu
-0001e950: 272c 2027 656c 7527 2c20 2773 6967 6d6f  ', 'elu', 'sigmo
-0001e960: 6964 272c 2027 7072 656c 7527 2c20 276c  id', 'prelu', 'l
-0001e970: 6561 6b79 7265 6c75 272c 2027 6873 7769  eakyrelu', 'hswi
-0001e980: 7368 272c 0a20 2020 2020 2020 2020 2020  sh',.           
-0001e990: 2020 2020 2027 6873 6967 6d6f 6964 272c       'hsigmoid',
-0001e9a0: 2027 6c6f 6773 6967 6d6f 6964 2720 616e   'logsigmoid' an
-0001e9b0: 6420 736f 206f 6e2e 2055 7365 7220 6361  d so on. User ca
-0001e9c0: 6e20 7072 6f76 6964 6520 6375 7374 6f6d  n provide custom
-0001e9d0: 2061 6374 6976 6974 696f 6e20 746f 2074   activition to t
-0001e9e0: 6865 2061 7267 756d 656e 742e 0a20 2020  he argument..   
-0001e9f0: 2020 2020 2020 2020 2020 2020 2049 6620               If 
-0001ea00: 7573 6572 2077 616e 7473 2074 6f20 7275  user wants to ru
-0001ea10: 6e20 7468 6520 6e65 7420 696e 2074 6865  n the net in the
-0001ea20: 2070 6172 616c 6c65 6c20 6d6f 6465 2c20   parallel mode, 
-0001ea30: 7468 6520 6375 7374 6f6d 2061 6374 6976  the custom activ
-0001ea40: 6174 696f 6e20 6d75 7374 2061 6c73 6f20  ation must also 
-0001ea50: 7072 6f76 6964 650a 2020 2020 2020 2020  provide.        
-0001ea60: 2020 2020 2020 2020 7468 6520 6061 6374          the `act
-0001ea70: 6976 6174 696f 6e5f 7368 6172 6460 2066  ivation_shard` f
-0001ea80: 756e 6374 696f 6e2e 2050 6c65 6173 6520  unction. Please 
-0001ea90: 7365 6520 7468 6520 6578 616d 706c 6573  see the examples
-0001eaa0: 206f 6620 7468 650a 2020 2020 2020 2020   of the.        
-0001eab0: 2020 2020 2020 2020 636c 6173 733a 606d          class:`m
-0001eac0: 696e 6466 6f72 6d65 7273 2e6d 6f64 756c  indformers.modul
-0001ead0: 6573 2e74 7261 6e73 666f 726d 6572 2e46  es.transformer.F
-0001eae0: 6565 6446 6f72 7761 7264 602e 2044 6566  eedForward`. Def
-0001eaf0: 6175 6c74 3a20 6765 6c75 2e0a 2020 2020  ault: gelu..    
-0001eb00: 2020 2020 2020 2020 6d6f 655f 636f 6e66          moe_conf
-0001eb10: 6967 284d 6f45 436f 6e66 6967 293a 2054  ig(MoEConfig): T
-0001eb20: 6865 2063 6f6e 6669 6775 7261 7469 6f6e  he configuration
-0001eb30: 206f 6620 4d6f 4520 284d 6978 7475 7265   of MoE (Mixture
-0001eb40: 206f 6620 4578 7065 7274 292e 2044 6566   of Expert). Def
-0001eb50: 6175 6c74 2069 7320 616e 2069 6e73 7461  ault is an insta
-0001eb60: 6e63 6520 6f66 204d 6f45 436f 6e66 6967  nce of MoEConfig
-0001eb70: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0001eb80: 2077 6974 6820 6465 6661 756c 7420 7661   with default va
-0001eb90: 6c75 6573 2e20 506c 6561 7365 2073 6565  lues. Please see
-0001eba0: 2060 4d6f 4543 6f6e 6669 6760 2e0a 2020   `MoEConfig`..  
-0001ebb0: 2020 2020 2020 2020 2020 7061 7261 6c6c            parall
-0001ebc0: 656c 5f63 6f6e 6669 6728 4f70 5061 7261  el_config(OpPara
-0001ebd0: 6c6c 656c 436f 6e66 6967 2c20 4d6f 4550  llelConfig, MoEP
-0001ebe0: 6172 616c 6c65 6c43 6f6e 6669 6729 3a20  arallelConfig): 
-0001ebf0: 5468 6520 7061 7261 6c6c 656c 2063 6f6e  The parallel con
-0001ec00: 6669 6775 7265 2e20 5768 656e 204d 6f45  figure. When MoE
-0001ec10: 2069 7320 6170 706c 6965 642c 0a20 2020   is applied,.   
-0001ec20: 2020 2020 2020 2020 2020 2020 204d 6f45               MoE
-0001ec30: 5061 7261 6c6c 656c 436f 6e66 6967 2069  ParallelConfig i
-0001ec40: 7320 6566 6665 6374 6976 652c 206f 7468  s effective, oth
-0001ec50: 6572 7769 7365 204f 7050 6172 616c 6c65  erwise OpParalle
-0001ec60: 6c43 6f6e 6669 6720 6973 2065 6666 6563  lConfig is effec
-0001ec70: 7469 7665 2e20 4465 6661 756c 7420 6064  tive. Default `d
-0001ec80: 6566 6175 6c74 5f64 706d 705f 636f 6e66  efault_dpmp_conf
-0001ec90: 6967 602c 0a20 2020 2020 2020 2020 2020  ig`,.           
-0001eca0: 2020 2020 2061 6e20 696e 7374 616e 6365       an instance
-0001ecb0: 206f 6620 604f 7050 6172 616c 6c65 6c43   of `OpParallelC
-0001ecc0: 6f6e 6669 6760 2077 6974 6820 6465 6661  onfig` with defa
-0001ecd0: 756c 7420 6172 6773 2e0a 0a20 2020 2020  ult args...     
-0001ece0: 2020 2049 6e70 7574 733a 0a20 2020 2020     Inputs:.     
-0001ecf0: 2020 2020 2020 202d 202a 2a68 6964 6465         - **hidde
-0001ed00: 6e5f 7374 6174 732a 2a20 2854 656e 736f  n_stats** (Tenso
-0001ed10: 7229 202d 2054 6865 2069 6e70 7574 2074  r) - The input t
-0001ed20: 656e 736f 7220 7769 7468 2073 6861 7065  ensor with shape
-0001ed30: 205b 6261 7463 685f 7369 7a65 2c20 7467   [batch_size, tg
-0001ed40: 745f 7365 715f 6c65 6e67 7468 2c20 6869  t_seq_length, hi
-0001ed50: 6464 656e 5f73 697a 655d 206f 720a 2020  dden_size] or.  
-0001ed60: 2020 2020 2020 2020 2020 2020 5b62 6174              [bat
-0001ed70: 6368 5f73 697a 6520 2a20 7467 745f 7365  ch_size * tgt_se
-0001ed80: 715f 6c65 6e67 7468 2c20 6869 6464 656e  q_length, hidden
-0001ed90: 5f73 697a 655d 2e0a 2020 2020 2020 2020  _size]..        
-0001eda0: 2020 2020 2d20 2a2a 6465 636f 6465 725f      - **decoder_
-0001edb0: 6d61 736b 2a2a 2028 5465 6e73 6f72 2920  mask** (Tensor) 
-0001edc0: 2d20 5468 6520 6174 7465 6e74 696f 6e20  - The attention 
-0001edd0: 6d61 736b 2066 6f72 2064 6563 6f64 6572  mask for decoder
-0001ede0: 2077 6974 6820 7368 6170 6520 5b62 6174   with shape [bat
-0001edf0: 6368 5f73 697a 652c 2073 7263 5f73 6571  ch_size, src_seq
-0001ee00: 5f6c 656e 6774 682c 0a20 2020 2020 2020  _length,.       
-0001ee10: 2020 2020 2020 2073 6571 5f6c 656e 6774         seq_lengt
-0001ee20: 685d 206f 7220 4e6f 6e65 2e20 4e6f 6e65  h] or None. None
-0001ee30: 206d 6561 6e73 2074 6865 7265 2077 696c   means there wil
-0001ee40: 6c20 6265 206e 6f20 6d61 736b 2069 6e20  l be no mask in 
-0001ee50: 736f 6674 6d61 7820 636f 6d70 7574 6174  softmax computat
-0001ee60: 696f 6e20 696e 2073 656c 6620 6174 7465  ion in self atte
-0001ee70: 6e74 696f 6e2e 0a20 2020 2020 2020 2020  ntion..         
-0001ee80: 2020 202d 202a 2a65 6e63 6f64 6572 5f6f     - **encoder_o
-0001ee90: 7574 7075 742a 2a20 2854 656e 736f 7229  utput** (Tensor)
-0001eea0: 202d 2054 6865 206f 7574 7075 7420 6f66   - The output of
-0001eeb0: 2074 6865 2065 6e63 6f64 6572 2077 6974   the encoder wit
-0001eec0: 6820 7368 6170 6520 5b62 6174 6368 5f73  h shape [batch_s
-0001eed0: 697a 652c 2073 6571 5f6c 656e 6774 682c  ize, seq_length,
-0001eee0: 2068 6964 6465 6e5f 7369 7a65 5d0a 2020   hidden_size].  
-0001eef0: 2020 2020 2020 2020 2020 2020 6f72 205b              or [
-0001ef00: 6261 7463 685f 7369 7a65 202a 2073 6571  batch_size * seq
-0001ef10: 5f6c 656e 6774 682c 2068 6964 6465 6e5f  _length, hidden_
-0001ef20: 7369 7a65 5d2e 0a20 2020 2020 2020 2020  size]..         
-0001ef30: 2020 2020 204e 6f74 6520 7468 6973 2061       Note this a
-0001ef40: 7267 7320 6361 6e20 6e6f 7420 6265 2070  rgs can not be p
-0001ef50: 6173 7365 6420 6279 204e 6f6e 6520 7768  assed by None wh
-0001ef60: 656e 2074 6865 206e 6574 2069 7320 696e  en the net is in
-0001ef70: 206f 7574 6572 6d6f 7374 206c 6179 6572   outermost layer
-0001ef80: 2e20 4465 6661 756c 7420 4e6f 6e65 2e0a  . Default None..
-0001ef90: 2020 2020 2020 2020 2020 2020 2d20 2a2a              - **
-0001efa0: 6d65 6d6f 7279 5f6d 6173 6b2a 2a20 2854  memory_mask** (T
-0001efb0: 656e 736f 7229 202d 2054 6865 206d 656d  ensor) - The mem
-0001efc0: 6f72 7920 6d61 736b 206f 6620 7468 6520  ory mask of the 
-0001efd0: 6372 6f73 7320 6174 7465 6e74 696f 6e20  cross attention 
-0001efe0: 7769 7468 2073 6861 7065 205b 6261 7463  with shape [batc
-0001eff0: 682c 2074 6774 5f73 6571 5f6c 656e 6774  h, tgt_seq_lengt
-0001f000: 682c 0a20 2020 2020 2020 2020 2020 2020  h,.             
-0001f010: 2073 7263 5f73 6571 5f6c 656e 6774 685d   src_seq_length]
-0001f020: 2077 6865 7265 2074 6774 5f73 6571 5f6c   where tgt_seq_l
-0001f030: 656e 6774 6820 6973 2074 6865 206c 656e  ength is the len
-0001f040: 6774 6820 6f66 2074 6865 2064 6563 6f64  gth of the decod
-0001f050: 6572 2e20 5468 6520 7573 6572 2063 616e  er. The user can
-0001f060: 2061 6c73 6f20 7061 7373 204e 6f6e 652e   also pass None.
-0001f070: 204e 6f6e 650a 2020 2020 2020 2020 2020   None.          
-0001f080: 2020 2020 6d65 616e 7320 7468 6572 6520      means there 
-0001f090: 7769 6c6c 2062 6520 6e6f 206d 6173 6b20  will be no mask 
-0001f0a0: 696e 2073 6f66 746d 6178 2063 6f6d 7075  in softmax compu
-0001f0b0: 7461 7469 6f6e 2069 6e20 6372 6f73 7320  tation in cross 
-0001f0c0: 6174 7465 6e74 696f 6e2e 2044 6566 6175  attention. Defau
-0001f0d0: 6c74 204e 6f6e 652e 0a20 2020 2020 2020  lt None..       
-0001f0e0: 2020 2020 202d 202a 2a69 6e69 745f 7265       - **init_re
-0001f0f0: 7365 742a 2a20 2854 656e 736f 7229 202d  set** (Tensor) -
-0001f100: 2041 2062 6f6f 6c20 7465 6e73 6f72 2077   A bool tensor w
-0001f110: 6974 6820 7368 6170 6520 5b31 5d2c 2075  ith shape [1], u
-0001f120: 7365 6420 746f 2063 6c65 6172 2074 6865  sed to clear the
-0001f130: 2070 6173 7420 6b65 7920 7061 7261 6d65   past key parame
-0001f140: 7465 7220 616e 640a 2020 2020 2020 2020  ter and.        
-0001f150: 2020 2020 2020 7061 7374 2076 616c 7565        past value
-0001f160: 2070 6172 616d 6574 6572 2075 7365 6420   parameter used 
-0001f170: 696e 2074 6865 2069 6e63 7265 6d65 6e74  in the increment
-0001f180: 616c 2070 7265 6469 6374 696f 6e2e 204f  al prediction. O
-0001f190: 6e6c 7920 7661 6c69 6420 7768 656e 2075  nly valid when u
-0001f1a0: 7365 5f70 6173 7420 6973 2054 7275 652e  se_past is True.
-0001f1b0: 2044 6566 6175 6c74 2054 7275 652e 0a20   Default True.. 
-0001f1c0: 2020 2020 2020 2020 2020 202d 202a 2a62             - **b
-0001f1d0: 6174 6368 5f76 616c 6964 5f6c 656e 6774  atch_valid_lengt
-0001f1e0: 682a 2a20 2854 656e 736f 7229 202d 2049  h** (Tensor) - I
-0001f1f0: 6e74 3332 2074 656e 736f 7220 7769 7468  nt32 tensor with
-0001f200: 2073 6861 7065 205b 6261 7463 685f 7369   shape [batch_si
-0001f210: 7a65 5d20 7468 6520 7061 7374 2063 616c  ze] the past cal
-0001f220: 6375 6c61 7465 6420 7468 6520 696e 6465  culated the inde
-0001f230: 782e 0a20 2020 2020 2020 2020 2020 2020  x..             
-0001f240: 2055 7365 6420 666f 7220 696e 6372 656d   Used for increm
-0001f250: 656e 7461 6c20 7072 6564 6963 7469 6f6e  ental prediction
-0001f260: 2077 6865 6e20 7468 6520 7573 655f 7061   when the use_pa
-0001f270: 7374 2069 7320 5472 7565 2e20 4465 6661  st is True. Defa
-0001f280: 756c 7420 4e6f 6e65 2e0a 0a20 2020 2020  ult None...     
-0001f290: 2020 204f 7574 7075 7473 3a0a 2020 2020     Outputs:.    
-0001f2a0: 2020 2020 2020 2020 5475 706c 652c 2061          Tuple, a
-0001f2b0: 2074 7570 6c65 2063 6f6e 7461 696e 7328   tuple contains(
-0001f2c0: 606f 7574 7075 7460 2c20 606c 6179 6572  `output`, `layer
-0001f2d0: 5f70 7265 7365 6e74 6029 0a0a 2020 2020  _present`)..    
-0001f2e0: 2020 2020 2020 2020 2d20 2a2a 6f75 7470          - **outp
-0001f2f0: 7574 2a2a 2028 5465 6e73 6f72 2920 2d20  ut** (Tensor) - 
-0001f300: 5468 6520 6f75 7470 7574 206c 6f67 6974  The output logit
-0001f310: 206f 6620 7468 6973 206c 6179 6572 2e20   of this layer. 
-0001f320: 5468 6520 7368 6170 6520 6973 205b 6261  The shape is [ba
-0001f330: 7463 682c 2073 6571 5f6c 656e 6774 682c  tch, seq_length,
-0001f340: 2068 6964 6465 6e5f 7369 7a65 5d20 6f72   hidden_size] or
-0001f350: 0a20 2020 2020 2020 2020 2020 2020 205b  .              [
-0001f360: 6261 7463 6820 2a20 7365 715f 6c65 6e67  batch * seq_leng
-0001f370: 7468 2c20 6869 6464 656e 5f73 697a 655d  th, hidden_size]
-0001f380: 2e0a 2020 2020 2020 2020 2020 2020 2d20  ..            - 
-0001f390: 2a2a 6c61 7965 725f 7072 6573 656e 742a  **layer_present*
-0001f3a0: 2a20 2854 7570 6c65 2920 2d20 4120 7475  * (Tuple) - A tu
-0001f3b0: 706c 652c 2077 6865 7265 2065 6163 6820  ple, where each 
-0001f3c0: 7475 706c 6520 6973 2074 6865 2074 656e  tuple is the ten
-0001f3d0: 736f 7220 6f66 2074 6865 2070 726f 6a65  sor of the proje
-0001f3e0: 6374 6564 206b 6579 2061 6e64 2076 616c  cted key and val
-0001f3f0: 7565 0a20 2020 2020 2020 2020 2020 2020  ue.             
-0001f400: 2076 6563 746f 7220 696e 2073 656c 6620   vector in self 
-0001f410: 6174 7465 6e74 696f 6e20 7769 7468 2073  attention with s
-0001f420: 6861 7065 2028 2862 6174 6368 5f73 697a  hape ((batch_siz
-0001f430: 652c 206e 756d 5f68 6561 6473 2c20 7369  e, num_heads, si
-0001f440: 7a65 5f70 6572 5f68 6561 642c 2074 6774  ze_per_head, tgt
-0001f450: 5f73 6571 5f6c 656e 6774 6829 2c0a 2020  _seq_length),.  
-0001f460: 2020 2020 2020 2020 2020 2020 2862 6174              (bat
-0001f470: 6368 5f73 697a 652c 206e 756d 5f68 6561  ch_size, num_hea
-0001f480: 6473 2c20 7467 745f 7365 715f 6c65 6e67  ds, tgt_seq_leng
-0001f490: 7468 2c20 7369 7a65 5f70 6572 5f68 6561  th, size_per_hea
-0001f4a0: 6429 2c20 616e 6420 6f66 2074 6865 2070  d), and of the p
-0001f4b0: 726f 6a65 6374 6564 206b 6579 2061 6e64  rojected key and
-0001f4c0: 2076 616c 7565 2076 6563 746f 720a 2020   value vector.  
-0001f4d0: 2020 2020 2020 2020 2020 2020 696e 2063              in c
-0001f4e0: 726f 7373 2061 7474 656e 7469 6f6e 2077  ross attention w
-0001f4f0: 6974 6820 7368 6170 6520 2028 6261 7463  ith shape  (batc
-0001f500: 685f 7369 7a65 2c20 6e75 6d5f 6865 6164  h_size, num_head
-0001f510: 732c 2073 697a 655f 7065 725f 6865 6164  s, size_per_head
-0001f520: 2c20 7372 635f 7365 715f 6c65 6e67 7468  , src_seq_length
-0001f530: 292c 0a20 2020 2020 2020 2020 2020 2020  ),.             
-0001f540: 2028 6261 7463 685f 7369 7a65 2c20 6e75   (batch_size, nu
-0001f550: 6d5f 6865 6164 732c 2073 7263 5f73 6571  m_heads, src_seq
-0001f560: 5f6c 656e 6774 682c 2073 697a 655f 7065  _length, size_pe
-0001f570: 725f 6865 6164 2929 2e0a 0a20 2020 2020  r_head))...     
-0001f580: 2020 2053 7570 706f 7274 6564 2050 6c61     Supported Pla
-0001f590: 7466 6f72 6d73 3a0a 2020 2020 2020 2020  tforms:.        
-0001f5a0: 2020 2020 6060 4173 6365 6e64 6060 2060      ``Ascend`` `
-0001f5b0: 6047 5055 6060 0a0a 2020 2020 2020 2020  `GPU``..        
-0001f5c0: 4578 616d 706c 6573 3a0a 2020 2020 2020  Examples:.      
-0001f5d0: 2020 2020 2020 3e3e 3e20 696d 706f 7274        >>> import
-0001f5e0: 206e 756d 7079 2061 7320 6e70 0a20 2020   numpy as np.   
-0001f5f0: 2020 2020 2020 2020 203e 3e3e 2066 726f           >>> fro
-0001f600: 6d20 6d69 6e64 7370 6f72 6520 696d 706f  m mindspore impo
-0001f610: 7274 2064 7479 7065 2061 7320 6d73 7479  rt dtype as msty
-0001f620: 7065 0a20 2020 2020 2020 2020 2020 203e  pe.            >
-0001f630: 3e3e 2066 726f 6d20 6d69 6e64 666f 726d  >> from mindform
-0001f640: 6572 732e 6d6f 6475 6c65 732e 7472 616e  ers.modules.tran
-0001f650: 7366 6f72 6d65 7220 696d 706f 7274 2054  sformer import T
-0001f660: 7261 6e73 666f 726d 6572 4465 636f 6465  ransformerDecode
-0001f670: 724c 6179 6572 0a20 2020 2020 2020 2020  rLayer.         
-0001f680: 2020 203e 3e3e 2066 726f 6d20 6d69 6e64     >>> from mind
-0001f690: 7370 6f72 6520 696d 706f 7274 2054 656e  spore import Ten
-0001f6a0: 736f 720a 2020 2020 2020 2020 2020 2020  sor.            
-0001f6b0: 3e3e 3e20 6d6f 6465 6c20 3d20 5472 616e  >>> model = Tran
-0001f6c0: 7366 6f72 6d65 7244 6563 6f64 6572 4c61  sformerDecoderLa
-0001f6d0: 7965 7228 6261 7463 685f 7369 7a65 3d32  yer(batch_size=2
-0001f6e0: 2c20 6869 6464 656e 5f73 697a 653d 3634  , hidden_size=64
-0001f6f0: 2c20 6666 6e5f 6869 6464 656e 5f73 697a  , ffn_hidden_siz
-0001f700: 653d 3634 2c20 6e75 6d5f 6865 6164 733d  e=64, num_heads=
-0001f710: 322c 0a20 2020 2020 2020 2020 2020 202e  2,.            .
-0001f720: 2e2e 2020 2020 2020 2020 2020 2020 2020  ..              
-0001f730: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001f740: 2020 2073 7263 5f73 6571 5f6c 656e 6774     src_seq_lengt
-0001f750: 683d 3230 2c20 7467 745f 7365 715f 6c65  h=20, tgt_seq_le
-0001f760: 6e67 7468 3d31 3029 0a20 2020 2020 2020  ngth=10).       
-0001f770: 2020 2020 203e 3e3e 2065 6e63 6f64 6572       >>> encoder
-0001f780: 5f69 6e70 7574 5f76 616c 7565 203d 2054  _input_value = T
-0001f790: 656e 736f 7228 6e70 2e6f 6e65 7328 2832  ensor(np.ones((2
-0001f7a0: 2c20 3230 2c20 3634 2929 2c20 6d73 7479  , 20, 64)), msty
-0001f7b0: 7065 2e66 6c6f 6174 3332 290a 2020 2020  pe.float32).    
-0001f7c0: 2020 2020 2020 2020 3e3e 3e20 6465 636f          >>> deco
-0001f7d0: 6465 725f 696e 7075 745f 7661 6c75 6520  der_input_value 
-0001f7e0: 3d20 5465 6e73 6f72 286e 702e 6f6e 6573  = Tensor(np.ones
-0001f7f0: 2828 322c 2031 302c 2036 3429 292c 206d  ((2, 10, 64)), m
-0001f800: 7374 7970 652e 666c 6f61 7433 3229 0a20  stype.float32). 
-0001f810: 2020 2020 2020 2020 2020 203e 3e3e 2064             >>> d
-0001f820: 6563 6f64 6572 5f69 6e70 7574 5f6d 6173  ecoder_input_mas
-0001f830: 6b20 3d20 5465 6e73 6f72 286e 702e 6f6e  k = Tensor(np.on
-0001f840: 6573 2828 322c 2031 302c 2031 3029 292c  es((2, 10, 10)),
-0001f850: 206d 7374 7970 652e 666c 6f61 7431 3629   mstype.float16)
-0001f860: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
-0001f870: 206d 656d 6f72 795f 6d61 736b 203d 2054   memory_mask = T
-0001f880: 656e 736f 7228 6e70 2e6f 6e65 7328 2832  ensor(np.ones((2
-0001f890: 2c20 3130 2c20 3230 2929 2c20 6d73 7479  , 10, 20)), msty
-0001f8a0: 7065 2e66 6c6f 6174 3136 290a 2020 2020  pe.float16).    
-0001f8b0: 2020 2020 2020 2020 3e3e 3e20 6f75 7470          >>> outp
-0001f8c0: 7574 2c20 7061 7374 203d 206d 6f64 656c  ut, past = model
-0001f8d0: 2864 6563 6f64 6572 5f69 6e70 7574 5f76  (decoder_input_v
-0001f8e0: 616c 7565 2c20 6465 636f 6465 725f 696e  alue, decoder_in
-0001f8f0: 7075 745f 6d61 736b 2c20 656e 636f 6465  put_mask, encode
-0001f900: 725f 696e 7075 745f 7661 6c75 652c 206d  r_input_value, m
-0001f910: 656d 6f72 795f 6d61 736b 290a 2020 2020  emory_mask).    
-0001f920: 2020 2020 2020 2020 3e3e 3e20 7072 696e          >>> prin
-0001f930: 7428 6f75 7470 7574 2e73 6861 7065 290a  t(output.shape).
-0001f940: 2020 2020 2020 2020 2020 2020 2832 2c20              (2, 
-0001f950: 3130 2c20 3634 290a 2020 2020 2020 2020  10, 64).        
-0001f960: 2020 2020 3e3e 3e20 7072 696e 7428 7061      >>> print(pa
-0001f970: 7374 5b30 5d2e 7368 6170 6529 0a20 2020  st[0].shape).   
-0001f980: 2020 2020 2020 2020 2028 322c 2032 2c20           (2, 2, 
-0001f990: 3332 2c20 3130 290a 2020 2020 2020 2020  32, 10).        
-0001f9a0: 2020 2020 3e3e 3e20 7072 696e 7428 7061      >>> print(pa
-0001f9b0: 7374 5b31 5d2e 7368 6170 6529 0a20 2020  st[1].shape).   
-0001f9c0: 2020 2020 2020 2020 2028 322c 2032 2c20           (2, 2, 
-0001f9d0: 3130 2c20 3332 290a 2020 2020 2020 2020  10, 32).        
-0001f9e0: 2020 2020 3e3e 3e20 7072 696e 7428 7061      >>> print(pa
-0001f9f0: 7374 5b32 5d2e 7368 6170 6529 0a20 2020  st[2].shape).   
-0001fa00: 2020 2020 2020 2020 2028 322c 2032 2c20           (2, 2, 
-0001fa10: 3332 2c20 3230 290a 2020 2020 2020 2020  32, 20).        
-0001fa20: 2020 2020 3e3e 3e20 7072 696e 7428 7061      >>> print(pa
-0001fa30: 7374 5b33 5d2e 7368 6170 6529 0a20 2020  st[3].shape).   
-0001fa40: 2020 2020 2020 2020 2028 322c 2032 2c20           (2, 2, 
-0001fa50: 3230 2c20 3332 290a 2020 2020 2222 220a  20, 32).    """.
-0001fa60: 0a20 2020 2040 5f4c 6f67 4163 7469 6f6e  .    @_LogAction
-0001fa70: 4f6e 6365 286d 5f6c 6f67 6765 723d 6c6f  Once(m_logger=lo
-0001fa80: 6767 6572 2c20 6b65 793d 2754 7261 6e73  gger, key='Trans
-0001fa90: 666f 726d 6572 4465 636f 6465 724c 6179  formerDecoderLay
-0001faa0: 6572 272c 0a20 2020 2020 2020 2020 2020  er',.           
-0001fab0: 2020 2020 2020 2020 206e 6f5f 7761 726e           no_warn
-0001fac0: 696e 673d 5f67 6574 5f70 6172 616c 6c65  ing=_get_paralle
-0001fad0: 6c5f 6d6f 6465 2829 2069 6e20 2850 6172  l_mode() in (Par
-0001fae0: 616c 6c65 6c4d 6f64 652e 5354 414e 445f  allelMode.STAND_
-0001faf0: 414c 4f4e 452c 2929 0a20 2020 2040 5f61  ALONE,)).    @_a
-0001fb00: 7267 735f 7479 7065 5f76 616c 6964 6174  rgs_type_validat
-0001fb10: 6f72 5f63 6865 636b 2868 6964 6465 6e5f  or_check(hidden_
-0001fb20: 7369 7a65 3d56 616c 6964 6174 6f72 2e63  size=Validator.c
-0001fb30: 6865 636b 5f70 6f73 6974 6976 655f 696e  heck_positive_in
-0001fb40: 742c 0a20 2020 2020 2020 2020 2020 2020  t,.             
-0001fb50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001fb60: 2020 206e 756d 5f68 6561 6473 3d56 616c     num_heads=Val
-0001fb70: 6964 6174 6f72 2e63 6865 636b 5f70 6f73  idator.check_pos
-0001fb80: 6974 6976 655f 696e 742c 0a20 2020 2020  itive_int,.     
-0001fb90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001fba0: 2020 2020 2020 2020 2020 2066 666e 5f68             ffn_h
-0001fbb0: 6964 6465 6e5f 7369 7a65 3d56 616c 6964  idden_size=Valid
-0001fbc0: 6174 6f72 2e63 6865 636b 5f70 6f73 6974  ator.check_posit
-0001fbd0: 6976 655f 696e 742c 0a20 2020 2020 2020  ive_int,.       
-0001fbe0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001fbf0: 2020 2020 2020 2020 2073 7263 5f73 6571           src_seq
-0001fc00: 5f6c 656e 6774 683d 5661 6c69 6461 746f  _length=Validato
-0001fc10: 722e 6368 6563 6b5f 706f 7369 7469 7665  r.check_positive
-0001fc20: 5f69 6e74 2c0a 2020 2020 2020 2020 2020  _int,.          
-0001fc30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001fc40: 2020 2020 2020 7467 745f 7365 715f 6c65        tgt_seq_le
-0001fc50: 6e67 7468 3d56 616c 6964 6174 6f72 2e63  ngth=Validator.c
-0001fc60: 6865 636b 5f70 6f73 6974 6976 655f 696e  heck_positive_in
-0001fc70: 742c 0a20 2020 2020 2020 2020 2020 2020  t,.             
-0001fc80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001fc90: 2020 2061 7474 656e 7469 6f6e 5f64 726f     attention_dro
-0001fca0: 706f 7574 5f72 6174 653d 5661 6c69 6461  pout_rate=Valida
-0001fcb0: 746f 722e 6368 6563 6b5f 6e6f 6e5f 6e65  tor.check_non_ne
-0001fcc0: 6761 7469 7665 5f66 6c6f 6174 2c0a 2020  gative_float,.  
-0001fcd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001fce0: 2020 2020 2020 2020 2020 2020 2020 6869                hi
-0001fcf0: 6464 656e 5f64 726f 706f 7574 5f72 6174  dden_dropout_rat
-0001fd00: 653d 5661 6c69 6461 746f 722e 6368 6563  e=Validator.chec
-0001fd10: 6b5f 6e6f 6e5f 6e65 6761 7469 7665 5f66  k_non_negative_f
-0001fd20: 6c6f 6174 2c0a 2020 2020 2020 2020 2020  loat,.          
-0001fd30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001fd40: 2020 2020 2020 706f 7374 5f6c 6179 6572        post_layer
-0001fd50: 6e6f 726d 5f72 6573 6964 7561 6c3d 5661  norm_residual=Va
-0001fd60: 6c69 6461 746f 722e 6368 6563 6b5f 626f  lidator.check_bo
-0001fd70: 6f6c 2c0a 2020 2020 2020 2020 2020 2020  ol,.            
-0001fd80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001fd90: 2020 2020 6c61 7965 726e 6f72 6d5f 636f      layernorm_co
-0001fda0: 6d70 7574 655f 7479 7065 3d5f 7661 6c69  mpute_type=_vali
-0001fdb0: 645f 7661 6c75 655f 6368 6563 6b73 285b  d_value_checks([
-0001fdc0: 6d73 7479 7065 2e66 6c6f 6174 3332 2c0a  mstype.float32,.
-0001fdd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001fde0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001fdf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001fe00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001fe10: 2020 2020 2020 2020 2020 2020 6d73 7479              msty
-0001fe20: 7065 2e66 6c6f 6174 3136 2c20 6d73 7479  pe.float16, msty
-0001fe30: 7065 2e62 666c 6f61 7431 365d 2c0a 2020  pe.bfloat16],.  
-0001fe40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001fe50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001fe60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001fe70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001fe80: 2020 2020 2020 2020 2022 5472 616e 7366           "Transf
-0001fe90: 6f72 6d65 7244 6563 6f64 6572 4c61 7965  ormerDecoderLaye
-0001fea0: 7222 292c 0a20 2020 2020 2020 2020 2020  r"),.           
-0001feb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001fec0: 2020 2020 2073 6f66 746d 6178 5f63 6f6d       softmax_com
-0001fed0: 7075 7465 5f74 7970 653d 5f76 616c 6964  pute_type=_valid
-0001fee0: 5f76 616c 7565 5f63 6865 636b 7328 5b6d  _value_checks([m
-0001fef0: 7374 7970 652e 666c 6f61 7433 322c 0a20  stype.float32,. 
-0001ff00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001ff10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001ff20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001ff30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001ff40: 2020 2020 2020 2020 206d 7374 7970 652e           mstype.
-0001ff50: 666c 6f61 7431 362c 206d 7374 7970 652e  float16, mstype.
-0001ff60: 6266 6c6f 6174 3136 5d2c 0a20 2020 2020  bfloat16],.     
-0001ff70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001ff80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001ff90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001ffa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001ffb0: 2020 2020 2254 7261 6e73 666f 726d 6572      "Transformer
-0001ffc0: 4465 636f 6465 724c 6179 6572 2229 2c0a  DecoderLayer"),.
-0001ffd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001ffe0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001fff0: 7061 7261 6d5f 696e 6974 5f74 7970 653d  param_init_type=
-00020000: 5f76 616c 6964 5f76 616c 7565 5f63 6865  _valid_value_che
-00020010: 636b 7328 5b6d 7374 7970 652e 666c 6f61  cks([mstype.floa
-00020020: 7433 322c 206d 7374 7970 652e 666c 6f61  t32, mstype.floa
-00020030: 7431 362c 206d 7374 7970 652e 6266 6c6f  t16, mstype.bflo
-00020040: 6174 3136 5d2c 0a20 2020 2020 2020 2020  at16],.         
-00020050: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020060: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020070: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020080: 2020 2020 2020 2020 2020 2022 5472 616e             "Tran
-00020090: 7366 6f72 6d65 7244 6563 6f64 6572 4c61  sformerDecoderLa
-000200a0: 7965 7222 292c 0a20 2020 2020 2020 2020  yer"),.         
-000200b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000200c0: 2020 2020 2020 2070 6172 616c 6c65 6c5f         parallel_
-000200d0: 636f 6e66 6967 3d5f 7661 6c69 645f 7479  config=_valid_ty
-000200e0: 7065 5f63 6865 636b 7328 5b4f 7050 6172  pe_checks([OpPar
-000200f0: 616c 6c65 6c43 6f6e 6669 672c 204d 6f45  allelConfig, MoE
-00020100: 5061 7261 6c6c 656c 436f 6e66 6967 5d2c  ParallelConfig],
-00020110: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0001c290: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001c2a0: 2020 2020 2020 7573 655f 666c 6173 685f        use_flash_
+0001c2b0: 6174 7465 6e74 696f 6e3d 7573 655f 666c  attention=use_fl
+0001c2c0: 6173 685f 6174 7465 6e74 696f 6e2c 0a20  ash_attention,. 
+0001c2d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001c2e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001c2f0: 2020 2020 2020 2020 2020 2020 2020 2075                 u
+0001c300: 7365 5f70 726f 6d70 745f 666c 6173 685f  se_prompt_flash_
+0001c310: 6174 7465 6e74 696f 6e3d 7573 655f 7072  attention=use_pr
+0001c320: 6f6d 7074 5f66 6c61 7368 5f61 7474 656e  ompt_flash_atten
+0001c330: 7469 6f6e 2c0a 2020 2020 2020 2020 2020  tion,.          
+0001c340: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001c350: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001c360: 2020 2020 2020 7573 655f 696e 6372 655f        use_incre_
+0001c370: 666c 6173 685f 6174 7465 6e74 696f 6e3d  flash_attention=
+0001c380: 7573 655f 696e 6372 655f 666c 6173 685f  use_incre_flash_
+0001c390: 6174 7465 6e74 696f 6e29 0a20 2020 2020  attention).     
+0001c3a0: 2020 2020 2020 2069 6620 7365 6c66 2e75         if self.u
+0001c3b0: 7365 5f6d 6f65 3a0a 2020 2020 2020 2020  se_moe:.        
+0001c3c0: 2020 2020 2020 2020 7365 6c66 2e6f 7574          self.out
+0001c3d0: 7075 7420 3d20 4d6f 4528 6869 6464 656e  put = MoE(hidden
+0001c3e0: 5f73 697a 653d 6869 6464 656e 5f73 697a  _size=hidden_siz
+0001c3f0: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
+0001c400: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001c410: 2020 2020 2064 726f 706f 7574 5f72 6174       dropout_rat
+0001c420: 653d 6869 6464 656e 5f64 726f 706f 7574  e=hidden_dropout
+0001c430: 5f72 6174 652c 0a20 2020 2020 2020 2020  _rate,.         
+0001c440: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001c450: 2020 2020 2020 2020 2066 666e 5f68 6964           ffn_hid
+0001c460: 6465 6e5f 7369 7a65 3d66 666e 5f68 6964  den_size=ffn_hid
+0001c470: 6465 6e5f 7369 7a65 2c0a 2020 2020 2020  den_size,.      
+0001c480: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001c490: 2020 2020 2020 2020 2020 2020 7061 7261              para
+0001c4a0: 6d5f 696e 6974 5f74 7970 653d 7061 7261  m_init_type=para
+0001c4b0: 6d5f 696e 6974 5f74 7970 652c 0a20 2020  m_init_type,.   
+0001c4c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001c4d0: 2020 2020 2020 2020 2020 2020 2020 2068                 h
+0001c4e0: 6964 6465 6e5f 6163 743d 6869 6464 656e  idden_act=hidden
+0001c4f0: 5f61 6374 2c0a 2020 2020 2020 2020 2020  _act,.          
+0001c500: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001c510: 2020 2020 2020 2020 6d6f 655f 636f 6e66          moe_conf
+0001c520: 6967 3d6d 6f65 5f63 6f6e 6669 672c 0a20  ig=moe_config,. 
+0001c530: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001c540: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001c550: 2070 6172 616c 6c65 6c5f 636f 6e66 6967   parallel_config
+0001c560: 3d70 6172 616c 6c65 6c5f 636f 6e66 6967  =parallel_config
+0001c570: 290a 2020 2020 2020 2020 2020 2020 656c  ).            el
+0001c580: 7365 3a0a 2020 2020 2020 2020 2020 2020  se:.            
+0001c590: 2020 2020 2320 4665 6564 2046 6f72 7761      # Feed Forwa
+0001c5a0: 7264 204e 6574 776f 726b 2c20 4646 4e0a  rd Network, FFN.
+0001c5b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001c5c0: 7365 6c66 2e6f 7574 7075 7420 3d20 4665  self.output = Fe
+0001c5d0: 6564 466f 7277 6172 6428 6869 6464 656e  edForward(hidden
+0001c5e0: 5f73 697a 653d 6869 6464 656e 5f73 697a  _size=hidden_siz
+0001c5f0: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
+0001c600: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001c610: 2020 2020 2020 2020 2020 2020 2064 726f               dro
+0001c620: 706f 7574 5f72 6174 653d 6869 6464 656e  pout_rate=hidden
+0001c630: 5f64 726f 706f 7574 5f72 6174 652c 0a20  _dropout_rate,. 
+0001c640: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001c650: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001c660: 2020 2020 2020 2020 2066 666e 5f68 6964           ffn_hid
+0001c670: 6465 6e5f 7369 7a65 3d66 666e 5f68 6964  den_size=ffn_hid
+0001c680: 6465 6e5f 7369 7a65 2c0a 2020 2020 2020  den_size,.      
+0001c690: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001c6a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001c6b0: 2020 2020 7061 7261 6d5f 696e 6974 5f74      param_init_t
+0001c6c0: 7970 653d 7061 7261 6d5f 696e 6974 5f74  ype=param_init_t
+0001c6d0: 7970 652c 0a20 2020 2020 2020 2020 2020  ype,.           
+0001c6e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001c6f0: 2020 2020 2020 2020 2020 2020 2020 2068                 h
+0001c700: 6964 6465 6e5f 6163 743d 6869 6464 656e  idden_act=hidden
+0001c710: 5f61 6374 2c0a 2020 2020 2020 2020 2020  _act,.          
+0001c720: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001c730: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001c740: 7061 7261 6c6c 656c 5f63 6f6e 6669 673d  parallel_config=
+0001c750: 7061 7261 6c6c 656c 5f63 6f6e 6669 6729  parallel_config)
+0001c760: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
+0001c770: 662e 706f 7374 5f6c 6179 6572 6e6f 726d  f.post_layernorm
+0001c780: 5f72 6573 6964 7561 6c20 3d20 706f 7374  _residual = post
+0001c790: 5f6c 6179 6572 6e6f 726d 5f72 6573 6964  _layernorm_resid
+0001c7a0: 7561 6c0a 2020 2020 2020 2020 2020 2020  ual.            
+0001c7b0: 7365 6c66 2e61 6464 203d 2050 2e41 6464  self.add = P.Add
+0001c7c0: 2829 2e73 6861 7264 2828 2870 6172 616c  ().shard(((paral
+0001c7d0: 6c65 6c5f 636f 6e66 6967 2e64 6174 615f  lel_config.data_
+0001c7e0: 7061 7261 6c6c 656c 2c20 3129 2c20 2870  parallel, 1), (p
+0001c7f0: 6172 616c 6c65 6c5f 636f 6e66 6967 2e64  arallel_config.d
+0001c800: 6174 615f 7061 7261 6c6c 656c 2c20 3129  ata_parallel, 1)
+0001c810: 2929 0a20 2020 2020 2020 2020 2020 2073  )).            s
+0001c820: 656c 662e 6164 645f 3364 203d 2050 2e41  elf.add_3d = P.A
+0001c830: 6464 2829 2e73 6861 7264 2828 2870 6172  dd().shard(((par
+0001c840: 616c 6c65 6c5f 636f 6e66 6967 2e64 6174  allel_config.dat
+0001c850: 615f 7061 7261 6c6c 656c 2c20 312c 2031  a_parallel, 1, 1
+0001c860: 292c 2028 7061 7261 6c6c 656c 5f63 6f6e  ), (parallel_con
+0001c870: 6669 672e 6461 7461 5f70 6172 616c 6c65  fig.data_paralle
+0001c880: 6c2c 2031 2c20 3129 2929 0a20 2020 2020  l, 1, 1))).     
+0001c890: 2020 2020 2020 2073 656c 662e 6474 7970         self.dtyp
+0001c8a0: 6520 3d20 6d73 7479 7065 2e66 6c6f 6174  e = mstype.float
+0001c8b0: 3136 0a20 2020 2020 2020 2020 2020 2073  16.            s
+0001c8c0: 656c 662e 6b65 795f 7061 7374 203d 204e  elf.key_past = N
+0001c8d0: 6f6e 650a 2020 2020 2020 2020 2020 2020  one.            
+0001c8e0: 7365 6c66 2e76 616c 7565 5f70 6173 7420  self.value_past 
+0001c8f0: 3d20 4e6f 6e65 0a0a 2020 2020 2020 2020  = None..        
+0001c900: 2020 2020 6966 2073 656c 662e 7573 655f      if self.use_
+0001c910: 7061 7374 3a0a 2020 2020 2020 2020 2020  past:.          
+0001c920: 2020 2020 2020 2320 6f70 6572 6174 6f72        # operator
+0001c930: 2075 7365 6420 666f 7220 7374 6174 6520   used for state 
+0001c940: 7265 7573 650a 2020 2020 2020 2020 2020  reuse.          
+0001c950: 2020 2020 2020 7365 6c66 2e72 6564 7563        self.reduc
+0001c960: 6573 756d 203d 2050 2e52 6564 7563 6553  esum = P.ReduceS
+0001c970: 756d 2829 2e73 6861 7264 2828 2831 2c20  um().shard(((1, 
+0001c980: 312c 2031 2c20 3129 2c29 290a 2020 2020  1, 1, 1),)).    
+0001c990: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+0001c9a0: 2e6e 6f74 5f65 7175 616c 203d 2050 2e4e  .not_equal = P.N
+0001c9b0: 6f74 4571 7561 6c28 292e 7368 6172 6428  otEqual().shard(
+0001c9c0: 2828 312c 2031 2c20 312c 2031 292c 2028  ((1, 1, 1, 1), (
+0001c9d0: 2929 290a 2020 2020 2020 2020 2020 2020  ))).            
+0001c9e0: 2020 2020 7365 6c66 2e73 6c69 6365 203d      self.slice =
+0001c9f0: 2050 2e53 7472 6964 6564 536c 6963 6528   P.StridedSlice(
+0001ca00: 292e 7368 6172 6428 2828 312c 2031 2c20  ).shard(((1, 1, 
+0001ca10: 312c 2031 292c 2929 0a20 2020 2020 2020  1, 1),)).       
+0001ca20: 2020 2020 2020 2020 2073 697a 655f 7065           size_pe
+0001ca30: 725f 6865 6164 203d 2068 6964 6465 6e5f  r_head = hidden_
+0001ca40: 7369 7a65 202f 2f20 6e75 6d5f 6865 6164  size // num_head
+0001ca50: 730a 2020 2020 2020 2020 2020 2020 2020  s.              
+0001ca60: 2020 6966 2075 7365 5f70 726f 6d70 745f    if use_prompt_
+0001ca70: 666c 6173 685f 6174 7465 6e74 696f 6e20  flash_attention 
+0001ca80: 6f72 2075 7365 5f66 6c61 7368 5f61 7474  or use_flash_att
+0001ca90: 656e 7469 6f6e 3a0a 2020 2020 2020 2020  ention:.        
+0001caa0: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+0001cab0: 2e6b 6579 5f73 6861 7065 203d 2028 6261  .key_shape = (ba
+0001cac0: 7463 685f 7369 7a65 2c20 6e75 6d5f 6865  tch_size, num_he
+0001cad0: 6164 732c 2073 6571 5f6c 656e 6774 682c  ads, seq_length,
+0001cae0: 2073 697a 655f 7065 725f 6865 6164 290a   size_per_head).
+0001caf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001cb00: 656c 7365 3a0a 2020 2020 2020 2020 2020  else:.          
+0001cb10: 2020 2020 2020 2020 2020 7365 6c66 2e6b            self.k
+0001cb20: 6579 5f73 6861 7065 203d 2028 6261 7463  ey_shape = (batc
+0001cb30: 685f 7369 7a65 2c20 6e75 6d5f 6865 6164  h_size, num_head
+0001cb40: 732c 2073 697a 655f 7065 725f 6865 6164  s, size_per_head
+0001cb50: 2c20 7365 715f 6c65 6e67 7468 290a 2020  , seq_length).  
+0001cb60: 2020 2020 2020 2020 2020 2020 2020 7365                se
+0001cb70: 6c66 2e76 616c 7565 5f73 6861 7065 203d  lf.value_shape =
+0001cb80: 2028 6261 7463 685f 7369 7a65 2c20 6e75   (batch_size, nu
+0001cb90: 6d5f 6865 6164 732c 2073 6571 5f6c 656e  m_heads, seq_len
+0001cba0: 6774 682c 2073 697a 655f 7065 725f 6865  gth, size_per_he
+0001cbb0: 6164 290a 2020 2020 2020 2020 2020 2020  ad).            
+0001cbc0: 2020 2020 2320 7061 7261 6d65 7465 7273      # parameters
+0001cbd0: 2073 6176 696e 6720 6b65 7920 616e 6420   saving key and 
+0001cbe0: 7661 6c75 6520 7374 6174 6573 0a20 2020  value states.   
+0001cbf0: 2020 2020 2020 2020 2020 2020 2073 656c               sel
+0001cc00: 662e 6b65 795f 7061 7374 203d 2050 6172  f.key_past = Par
+0001cc10: 616d 6574 6572 2854 656e 736f 7228 6e70  ameter(Tensor(np
+0001cc20: 2e7a 6572 6f73 2873 6861 7065 3d73 656c  .zeros(shape=sel
+0001cc30: 662e 6b65 795f 7368 6170 6529 2c20 7365  f.key_shape), se
+0001cc40: 6c66 2e64 7479 7065 292c 206e 616d 653d  lf.dtype), name=
+0001cc50: 226b 6579 5f70 6173 7422 290a 2020 2020  "key_past").    
+0001cc60: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+0001cc70: 2e76 616c 7565 5f70 6173 7420 3d20 5061  .value_past = Pa
+0001cc80: 7261 6d65 7465 7228 5465 6e73 6f72 286e  rameter(Tensor(n
+0001cc90: 702e 7a65 726f 7328 7368 6170 653d 7365  p.zeros(shape=se
+0001cca0: 6c66 2e76 616c 7565 5f73 6861 7065 292c  lf.value_shape),
+0001ccb0: 2073 656c 662e 6474 7970 6529 2c20 6e61   self.dtype), na
+0001ccc0: 6d65 3d22 7661 6c75 655f 7061 7374 2229  me="value_past")
+0001ccd0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0001cce0: 2073 656c 662e 7469 6c65 203d 2050 2e54   self.tile = P.T
+0001ccf0: 696c 6528 292e 7368 6172 6428 2828 312c  ile().shard(((1,
+0001cd00: 2031 292c 2929 0a20 2020 2020 2020 2020   1),)).         
+0001cd10: 2020 2020 2020 2073 656c 662e 6d75 6c20         self.mul 
+0001cd20: 3d20 502e 4d75 6c28 292e 7368 6172 6428  = P.Mul().shard(
+0001cd30: 2828 312c 2031 2c20 312c 2031 292c 2028  ((1, 1, 1, 1), (
+0001cd40: 312c 2929 290a 2020 2020 2020 2020 2020  1,))).          
+0001cd50: 2020 2020 2020 7365 6c66 2e61 7373 6967        self.assig
+0001cd60: 6e20 3d20 502e 4173 7369 676e 2829 2e73  n = P.Assign().s
+0001cd70: 6861 7264 2828 2831 2c20 312c 2031 2c20  hard(((1, 1, 1, 
+0001cd80: 3129 2c20 2831 2c20 312c 2031 2c20 3129  1), (1, 1, 1, 1)
+0001cd90: 2929 0a0a 2020 2020 2020 2020 2020 2020  ))..            
+0001cda0: 6966 2070 6172 616c 6c65 6c5f 636f 6e66  if parallel_conf
+0001cdb0: 6967 2e75 7365 5f73 6571 5f70 6172 616c  ig.use_seq_paral
+0001cdc0: 6c65 6c3a 0a20 2020 2020 2020 2020 2020  lel:.           
+0001cdd0: 2020 2020 2073 656c 662e 6164 642e 7368       self.add.sh
+0001cde0: 6172 6428 2828 7061 7261 6c6c 656c 5f63  ard(((parallel_c
+0001cdf0: 6f6e 6669 672e 6461 7461 5f70 6172 616c  onfig.data_paral
+0001ce00: 6c65 6c20 2a20 7061 7261 6c6c 656c 5f63  lel * parallel_c
+0001ce10: 6f6e 6669 672e 6d6f 6465 6c5f 7061 7261  onfig.model_para
+0001ce20: 6c6c 656c 2c20 3129 2c0a 2020 2020 2020  llel, 1),.      
+0001ce30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001ce40: 2020 2020 2020 2020 2020 2870 6172 616c            (paral
+0001ce50: 6c65 6c5f 636f 6e66 6967 2e64 6174 615f  lel_config.data_
+0001ce60: 7061 7261 6c6c 656c 202a 2070 6172 616c  parallel * paral
+0001ce70: 6c65 6c5f 636f 6e66 6967 2e6d 6f64 656c  lel_config.model
+0001ce80: 5f70 6172 616c 6c65 6c2c 2031 2929 290a  _parallel, 1))).
+0001ce90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001cea0: 7365 6c66 2e6c 6179 6572 6e6f 726d 312e  self.layernorm1.
+0001ceb0: 7368 6172 6428 2828 7061 7261 6c6c 656c  shard(((parallel
+0001cec0: 5f63 6f6e 6669 672e 6461 7461 5f70 6172  _config.data_par
+0001ced0: 616c 6c65 6c20 2a20 7061 7261 6c6c 656c  allel * parallel
+0001cee0: 5f63 6f6e 6669 672e 6d6f 6465 6c5f 7061  _config.model_pa
+0001cef0: 7261 6c6c 656c 2c20 3129 2c29 290a 2020  rallel, 1),)).  
+0001cf00: 2020 2020 2020 2020 2020 2020 2020 7365                se
+0001cf10: 6c66 2e6c 6179 6572 6e6f 726d 322e 7368  lf.layernorm2.sh
+0001cf20: 6172 6428 2828 7061 7261 6c6c 656c 5f63  ard(((parallel_c
+0001cf30: 6f6e 6669 672e 6461 7461 5f70 6172 616c  onfig.data_paral
+0001cf40: 6c65 6c20 2a20 7061 7261 6c6c 656c 5f63  lel * parallel_c
+0001cf50: 6f6e 6669 672e 6d6f 6465 6c5f 7061 7261  onfig.model_para
+0001cf60: 6c6c 656c 2c20 3129 2c29 290a 2020 2020  llel, 1),)).    
+0001cf70: 2020 2020 2020 2020 2020 2020 6966 2070              if p
+0001cf80: 6172 616c 6c65 6c5f 636f 6e66 6967 2e72  arallel_config.r
+0001cf90: 6563 6f6d 7075 7465 2e73 656c 6563 745f  ecompute.select_
+0001cfa0: 7265 636f 6d70 7574 653a 0a20 2020 2020  recompute:.     
+0001cfb0: 2020 2020 2020 2020 2020 2020 2020 2023                 #
+0001cfc0: 20e6 ada4 e5a4 84e4 bc9a e6b6 88e8 8097   ...............
+0001cfd0: e8be 83e5 a4a7 e586 85e5 ad98 efbc 8ce5  ................
+0001cfe0: bc80 e590 afe5 908e e4bc 9ae6 8d9f e5a4  ................
+0001cff0: b1e4 b880 e983 a8e5 8886 e8ae a1e7 ae97  ................
+0001d000: e680 a7e8 83bd 0a20 2020 2020 2020 2020  .......         
+0001d010: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+0001d020: 6c61 7965 726e 6f72 6d32 2e6c 6179 6572  layernorm2.layer
+0001d030: 5f6e 6f72 6d2e 7265 636f 6d70 7574 6528  _norm.recompute(
+0001d040: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
+0001d050: 2020 6966 206e 6f74 2073 656c 662e 7573    if not self.us
+0001d060: 655f 6d6f 653a 0a20 2020 2020 2020 2020  e_moe:.         
+0001d070: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+0001d080: 6f75 7470 7574 2e70 726f 6a65 6374 696f  output.projectio
+0001d090: 6e2e 7368 6172 6428 0a20 2020 2020 2020  n.shard(.       
+0001d0a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d0b0: 2073 7472 6174 6567 795f 6269 6173 3d28   strategy_bias=(
+0001d0c0: 2870 6172 616c 6c65 6c5f 636f 6e66 6967  (parallel_config
+0001d0d0: 2e64 6174 615f 7061 7261 6c6c 656c 202a  .data_parallel *
+0001d0e0: 2070 6172 616c 6c65 6c5f 636f 6e66 6967   parallel_config
+0001d0f0: 2e6d 6f64 656c 5f70 6172 616c 6c65 6c2c  .model_parallel,
+0001d100: 2031 292c 2028 312c 2929 2c0a 2020 2020   1), (1,)),.    
+0001d110: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d120: 2020 2020 7374 7261 7465 6779 5f6d 6174      strategy_mat
+0001d130: 6d75 6c3d 2828 7061 7261 6c6c 656c 5f63  mul=((parallel_c
+0001d140: 6f6e 6669 672e 6461 7461 5f70 6172 616c  onfig.data_paral
+0001d150: 6c65 6c2c 2070 6172 616c 6c65 6c5f 636f  lel, parallel_co
+0001d160: 6e66 6967 2e6d 6f64 656c 5f70 6172 616c  nfig.model_paral
+0001d170: 6c65 6c29 2c0a 2020 2020 2020 2020 2020  lel),.          
+0001d180: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d190: 2020 2020 2020 2020 2020 2020 2020 2028                 (
+0001d1a0: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
+0001d1b0: 6d6f 6465 6c5f 7061 7261 6c6c 656c 2c20  model_parallel, 
+0001d1c0: 3129 292c 0a20 2020 2020 2020 2020 2020  1)),.           
+0001d1d0: 2020 2020 2020 2020 2020 2020 206f 7574               out
+0001d1e0: 5f73 7472 6174 6567 795f 6d61 746d 756c  _strategy_matmul
+0001d1f0: 3d28 2870 6172 616c 6c65 6c5f 636f 6e66  =((parallel_conf
+0001d200: 6967 2e64 6174 615f 7061 7261 6c6c 656c  ig.data_parallel
+0001d210: 202a 2070 6172 616c 6c65 6c5f 636f 6e66   * parallel_conf
+0001d220: 6967 2e6d 6f64 656c 5f70 6172 616c 6c65  ig.model_paralle
+0001d230: 6c2c 2031 292c 2929 0a20 2020 2020 2020  l, 1),)).       
+0001d240: 2020 2020 2020 2020 2020 2020 2073 656c               sel
+0001d250: 662e 6f75 7470 7574 2e64 726f 706f 7574  f.output.dropout
+0001d260: 2e64 726f 706f 7574 2e73 6861 7264 280a  .dropout.shard(.
+0001d270: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d280: 2020 2020 2020 2020 2828 7061 7261 6c6c          ((parall
+0001d290: 656c 5f63 6f6e 6669 672e 6461 7461 5f70  el_config.data_p
+0001d2a0: 6172 616c 6c65 6c20 2a20 7061 7261 6c6c  arallel * parall
+0001d2b0: 656c 5f63 6f6e 6669 672e 6d6f 6465 6c5f  el_config.model_
+0001d2c0: 7061 7261 6c6c 656c 2c20 3129 2c29 290a  parallel, 1),)).
+0001d2d0: 2020 2020 2020 2020 656c 7365 3a0a 2020          else:.  
+0001d2e0: 2020 2020 2020 2020 2020 7261 6973 6520            raise 
+0001d2f0: 5275 6e74 696d 6545 7272 6f72 2866 2254  RuntimeError(f"T
+0001d300: 6865 207b 7365 6c66 2e63 6c73 5f6e 616d  he {self.cls_nam
+0001d310: 657d 206f 6e6c 7920 7375 7070 6f72 7420  e} only support 
+0001d320: 7368 6172 6469 6e67 2070 726f 7061 6761  sharding propaga
+0001d330: 7469 6f6e 206f 7220 220a 2020 2020 2020  tion or ".      
+0001d340: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d350: 2020 2020 2020 2020 2066 2273 656d 692d           f"semi-
+0001d360: 6175 746f 2070 6172 616c 6c65 6c20 6d6f  auto parallel mo
+0001d370: 6465 206e 6f77 2e22 290a 0a20 2020 2064  de now.")..    d
+0001d380: 6566 2063 6f6e 7374 7275 6374 2873 656c  ef construct(sel
+0001d390: 662c 2078 2c20 696e 7075 745f 6d61 736b  f, x, input_mask
+0001d3a0: 3d4e 6f6e 652c 2069 6e69 745f 7265 7365  =None, init_rese
+0001d3b0: 743d 5472 7565 2c20 6261 7463 685f 7661  t=True, batch_va
+0001d3c0: 6c69 645f 6c65 6e67 7468 3d4e 6f6e 6529  lid_length=None)
+0001d3d0: 3a0a 2020 2020 2020 2020 2222 2266 6f72  :.        """for
+0001d3e0: 7761 7264 2070 726f 6365 7373 2222 220a  ward process""".
+0001d3f0: 2020 2020 2020 2020 7365 6c66 2e5f 6368          self._ch
+0001d400: 6563 6b5f 696e 7075 7428 782c 2069 6e70  eck_input(x, inp
+0001d410: 7574 5f6d 6173 6b2c 2069 6e69 745f 7265  ut_mask, init_re
+0001d420: 7365 742c 2062 6174 6368 5f76 616c 6964  set, batch_valid
+0001d430: 5f6c 656e 6774 6829 0a20 2020 2020 2020  _length).       
+0001d440: 2078 5f73 6861 7065 203d 2046 2e73 6861   x_shape = F.sha
+0001d450: 7065 2878 290a 2020 2020 2020 2020 7820  pe(x).        x 
+0001d460: 3d20 462e 7265 7368 6170 6528 782c 2028  = F.reshape(x, (
+0001d470: 2d31 2c20 785f 7368 6170 655b 2d31 5d29  -1, x_shape[-1])
+0001d480: 290a 2020 2020 2020 2020 6966 2073 656c  ).        if sel
+0001d490: 662e 706f 7374 5f6c 6179 6572 6e6f 726d  f.post_layernorm
+0001d4a0: 5f72 6573 6964 7561 6c3a 0a20 2020 2020  _residual:.     
+0001d4b0: 2020 2020 2020 2069 6e70 7574 5f78 203d         input_x =
+0001d4c0: 2078 0a20 2020 2020 2020 2065 6c73 653a   x.        else:
+0001d4d0: 0a20 2020 2020 2020 2020 2020 2069 6e70  .            inp
+0001d4e0: 7574 5f78 203d 2073 656c 662e 6c61 7965  ut_x = self.laye
+0001d4f0: 726e 6f72 6d31 2878 290a 2020 2020 2020  rnorm1(x).      
+0001d500: 2020 696e 7075 745f 7820 3d20 462e 6361    input_x = F.ca
+0001d510: 7374 2869 6e70 7574 5f78 2c20 7365 6c66  st(input_x, self
+0001d520: 2e64 7479 7065 290a 0a20 2020 2020 2020  .dtype)..       
+0001d530: 2023 2069 6e64 6963 6174 6520 7768 6574   # indicate whet
+0001d540: 6865 7220 7265 7365 7420 7361 7665 6420  her reset saved 
+0001d550: 7374 6174 6573 0a20 2020 2020 2020 206b  states.        k
+0001d560: 6579 5f72 6573 6574 203d 204e 6f6e 650a  ey_reset = None.
+0001d570: 2020 2020 2020 2020 7661 6c75 655f 7265          value_re
+0001d580: 7365 7420 3d20 4e6f 6e65 0a0a 2020 2020  set = None..    
+0001d590: 2020 2020 6966 2073 656c 662e 7573 655f      if self.use_
+0001d5a0: 7061 7374 3a0a 2020 2020 2020 2020 2020  past:.          
+0001d5b0: 2020 2320 7265 7365 7420 7374 6174 6573    # reset states
+0001d5c0: 2c20 696e 6974 5f72 6573 6574 2054 7275  , init_reset Tru
+0001d5d0: 6520 666f 7220 7265 7573 6520 616e 6420  e for reuse and 
+0001d5e0: 4661 6c73 6520 666f 7220 7265 7365 740a  False for reset.
+0001d5f0: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+0001d600: 2e61 7373 6967 6e28 7365 6c66 2e6b 6579  .assign(self.key
+0001d610: 5f70 6173 742c 2073 656c 662e 6d75 6c28  _past, self.mul(
+0001d620: 7365 6c66 2e6b 6579 5f70 6173 742c 2046  self.key_past, F
+0001d630: 2e63 6173 7428 696e 6974 5f72 6573 6574  .cast(init_reset
+0001d640: 2c20 7365 6c66 2e64 7479 7065 2929 290a  , self.dtype))).
+0001d650: 2020 2020 2020 2020 2020 2020 6b65 795f              key_
+0001d660: 7265 7365 7420 3d20 7365 6c66 2e6b 6579  reset = self.key
+0001d670: 5f70 6173 740a 2020 2020 2020 2020 2020  _past.          
+0001d680: 2020 7365 6c66 2e61 7373 6967 6e28 7365    self.assign(se
+0001d690: 6c66 2e76 616c 7565 5f70 6173 742c 2073  lf.value_past, s
+0001d6a0: 656c 662e 6d75 6c28 7365 6c66 2e76 616c  elf.mul(self.val
+0001d6b0: 7565 5f70 6173 742c 2046 2e63 6173 7428  ue_past, F.cast(
+0001d6c0: 696e 6974 5f72 6573 6574 2c20 7365 6c66  init_reset, self
+0001d6d0: 2e64 7479 7065 2929 290a 2020 2020 2020  .dtype))).      
+0001d6e0: 2020 2020 2020 7661 6c75 655f 7265 7365        value_rese
+0001d6f0: 7420 3d20 7365 6c66 2e76 616c 7565 5f70  t = self.value_p
+0001d700: 6173 740a 2020 2020 2020 2020 2020 2020  ast.            
+0001d710: 2320 6164 6420 6465 7065 6e64 656e 6379  # add dependency
+0001d720: 2066 6f72 2064 6573 6972 6564 2065 7865   for desired exe
+0001d730: 6375 7469 6f6e 206f 7264 6572 0a20 2020  cution order.   
+0001d740: 2020 2020 2020 2020 2069 6e70 7574 5f78           input_x
+0001d750: 203d 2046 2e64 6570 656e 6428 696e 7075   = F.depend(inpu
+0001d760: 745f 782c 206b 6579 5f72 6573 6574 290a  t_x, key_reset).
+0001d770: 2020 2020 2020 2020 2020 2020 696e 7075              inpu
+0001d780: 745f 7820 3d20 462e 6465 7065 6e64 2869  t_x = F.depend(i
+0001d790: 6e70 7574 5f78 2c20 7661 6c75 655f 7265  nput_x, value_re
+0001d7a0: 7365 7429 0a0a 2020 2020 2020 2020 6174  set)..        at
+0001d7b0: 7465 6e74 696f 6e2c 206c 6179 6572 5f70  tention, layer_p
+0001d7c0: 7265 7365 6e74 203d 2073 656c 662e 6174  resent = self.at
+0001d7d0: 7465 6e74 696f 6e28 696e 7075 745f 782c  tention(input_x,
+0001d7e0: 2069 6e70 7574 5f78 2c20 696e 7075 745f   input_x, input_
+0001d7f0: 782c 2069 6e70 7574 5f6d 6173 6b2c 0a20  x, input_mask,. 
+0001d800: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d810: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d820: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001d830: 2073 656c 662e 6b65 795f 7061 7374 2c20   self.key_past, 
+0001d840: 7365 6c66 2e76 616c 7565 5f70 6173 742c  self.value_past,
+0001d850: 2062 6174 6368 5f76 616c 6964 5f6c 656e   batch_valid_len
+0001d860: 6774 6829 0a20 2020 2020 2020 2023 2046  gth).        # F
+0001d870: 6f72 2070 6f73 742d 6c61 7965 726e 6f72  or post-layernor
+0001d880: 6d20 7468 6520 696e 7075 7473 2066 6f72  m the inputs for
+0001d890: 2072 6573 6964 7561 6c20 7061 7468 2061   residual path a
+0001d8a0: 7265 206f 7574 7075 7420 6f66 2073 656c  re output of sel
+0001d8b0: 662d 6174 7465 6e74 696f 6e20 616e 6420  f-attention and 
+0001d8c0: 6f75 7470 7574 206f 6620 6c61 7965 726e  output of layern
+0001d8d0: 6f72 6d0a 2020 2020 2020 2020 6966 2073  orm.        if s
+0001d8e0: 656c 662e 706f 7374 5f6c 6179 6572 6e6f  elf.post_layerno
+0001d8f0: 726d 5f72 6573 6964 7561 6c3a 0a20 2020  rm_residual:.   
+0001d900: 2020 2020 2020 2020 2078 203d 2073 656c           x = sel
+0001d910: 662e 6164 6428 696e 7075 745f 782c 2061  f.add(input_x, a
+0001d920: 7474 656e 7469 6f6e 290a 2020 2020 2020  ttention).      
+0001d930: 2020 2320 466f 7220 7072 652d 6c61 7965    # For pre-laye
+0001d940: 726e 6f72 6d20 7468 6520 696e 7075 7473  rnorm the inputs
+0001d950: 2066 6f72 2072 6573 6964 7561 6c20 7061   for residual pa
+0001d960: 7468 2061 7265 206f 7574 7075 7420 6f66  th are output of
+0001d970: 2073 656c 662d 6174 7465 6e74 696f 6e20   self-attention 
+0001d980: 616e 6420 696e 7075 7420 6f66 2074 6869  and input of thi
+0001d990: 7320 6c61 7965 720a 2020 2020 2020 2020  s layer.        
+0001d9a0: 656c 7365 3a0a 2020 2020 2020 2020 2020  else:.          
+0001d9b0: 2020 7820 3d20 7365 6c66 2e61 6464 2878    x = self.add(x
+0001d9c0: 2c20 6174 7465 6e74 696f 6e29 0a0a 2020  , attention)..  
+0001d9d0: 2020 2020 2020 6f75 7470 7574 5f78 203d        output_x =
+0001d9e0: 2073 656c 662e 6c61 7965 726e 6f72 6d32   self.layernorm2
+0001d9f0: 2878 290a 2020 2020 2020 2020 6f75 7470  (x).        outp
+0001da00: 7574 5f78 203d 2046 2e63 6173 7428 6f75  ut_x = F.cast(ou
+0001da10: 7470 7574 5f78 2c20 7365 6c66 2e64 7479  tput_x, self.dty
+0001da20: 7065 290a 2020 2020 2020 2020 6175 785f  pe).        aux_
+0001da30: 6c6f 7373 203d 204e 6f6e 650a 2020 2020  loss = None.    
+0001da40: 2020 2020 6966 2073 656c 662e 7573 655f      if self.use_
+0001da50: 6d6f 653a 0a20 2020 2020 2020 2020 2020  moe:.           
+0001da60: 206d 6c70 5f6c 6f67 6974 2c20 6175 785f   mlp_logit, aux_
+0001da70: 6c6f 7373 203d 2073 656c 662e 6f75 7470  loss = self.outp
+0001da80: 7574 286f 7574 7075 745f 7829 0a20 2020  ut(output_x).   
+0001da90: 2020 2020 2065 6c73 653a 0a20 2020 2020       else:.     
+0001daa0: 2020 2020 2020 206d 6c70 5f6c 6f67 6974         mlp_logit
+0001dab0: 203d 2073 656c 662e 6f75 7470 7574 286f   = self.output(o
+0001dac0: 7574 7075 745f 7829 0a0a 2020 2020 2020  utput_x)..      
+0001dad0: 2020 7661 6c75 655f 7570 6461 7465 203d    value_update =
+0001dae0: 204e 6f6e 650a 2020 2020 2020 2020 6b65   None.        ke
+0001daf0: 795f 7570 6461 7465 203d 204e 6f6e 650a  y_update = None.
+0001db00: 2020 2020 2020 2020 6966 2073 656c 662e          if self.
+0001db10: 7573 655f 7061 7374 3a0a 2020 2020 2020  use_past:.      
+0001db20: 2020 2020 2020 2320 6375 7272 656e 7420        # current 
+0001db30: 6b65 7920 616e 6420 7661 6c75 650a 2020  key and value.  
+0001db40: 2020 2020 2020 2020 2020 6b65 795f 7072            key_pr
+0001db50: 6573 656e 742c 2076 616c 7565 5f70 7265  esent, value_pre
+0001db60: 7365 6e74 203d 206c 6179 6572 5f70 7265  sent = layer_pre
+0001db70: 7365 6e74 0a20 2020 2020 2020 2020 2020  sent.           
+0001db80: 2023 2075 7064 6174 6520 6b65 7920 616e   # update key an
+0001db90: 6420 7661 6c75 6520 6361 6c63 756c 6174  d value calculat
+0001dba0: 6564 2074 6869 7320 7374 6570 0a20 2020  ed this step.   
+0001dbb0: 2020 2020 2020 2020 2073 656c 662e 6173           self.as
+0001dbc0: 7369 676e 2873 656c 662e 6b65 795f 7061  sign(self.key_pa
+0001dbd0: 7374 2c20 6b65 795f 7072 6573 656e 7429  st, key_present)
+0001dbe0: 0a20 2020 2020 2020 2020 2020 206b 6579  .            key
+0001dbf0: 5f75 7064 6174 6520 3d20 7365 6c66 2e6b  _update = self.k
+0001dc00: 6579 5f70 6173 740a 2020 2020 2020 2020  ey_past.        
+0001dc10: 2020 2020 7365 6c66 2e61 7373 6967 6e28      self.assign(
+0001dc20: 7365 6c66 2e76 616c 7565 5f70 6173 742c  self.value_past,
+0001dc30: 2076 616c 7565 5f70 7265 7365 6e74 290a   value_present).
+0001dc40: 2020 2020 2020 2020 2020 2020 7661 6c75              valu
+0001dc50: 655f 7570 6461 7465 203d 2073 656c 662e  e_update = self.
+0001dc60: 7661 6c75 655f 7061 7374 0a20 2020 2020  value_past.     
+0001dc70: 2020 2020 2020 2023 2061 6464 2064 6570         # add dep
+0001dc80: 656e 6465 6e63 7920 666f 7220 6465 7369  endency for desi
+0001dc90: 7265 6420 6578 6563 7574 696f 6e20 6f72  red execution or
+0001dca0: 6465 720a 2020 2020 2020 2020 2020 2020  der.            
+0001dcb0: 6b65 795f 7570 6461 7465 203d 2046 2e64  key_update = F.d
+0001dcc0: 6570 656e 6428 6b65 795f 7570 6461 7465  epend(key_update
+0001dcd0: 2c20 6b65 795f 7265 7365 7429 0a20 2020  , key_reset).   
+0001dce0: 2020 2020 2020 2020 2076 616c 7565 5f75           value_u
+0001dcf0: 7064 6174 6520 3d20 462e 6465 7065 6e64  pdate = F.depend
+0001dd00: 2876 616c 7565 5f75 7064 6174 652c 2076  (value_update, v
+0001dd10: 616c 7565 5f72 6573 6574 290a 0a20 2020  alue_reset)..   
+0001dd20: 2020 2020 2023 2061 6464 2064 6570 656e       # add depen
+0001dd30: 6465 6e63 7920 666f 7220 6465 7369 7265  dency for desire
+0001dd40: 6420 6578 6563 7574 696f 6e20 6f72 6465  d execution orde
+0001dd50: 720a 2020 2020 2020 2020 6d6c 705f 6c6f  r.        mlp_lo
+0001dd60: 6769 7420 3d20 462e 6465 7065 6e64 286d  git = F.depend(m
+0001dd70: 6c70 5f6c 6f67 6974 2c20 7661 6c75 655f  lp_logit, value_
+0001dd80: 7570 6461 7465 290a 2020 2020 2020 2020  update).        
+0001dd90: 6d6c 705f 6c6f 6769 7420 3d20 462e 6465  mlp_logit = F.de
+0001dda0: 7065 6e64 286d 6c70 5f6c 6f67 6974 2c20  pend(mlp_logit, 
+0001ddb0: 6b65 795f 7570 6461 7465 290a 0a20 2020  key_update)..   
+0001ddc0: 2020 2020 2023 2069 6620 7368 6170 6520       # if shape 
+0001ddd0: 6973 2033 642c 2077 6520 7265 7368 6170  is 3d, we reshap
+0001dde0: 6520 7468 6520 696e 7075 7473 206f 6620  e the inputs of 
+0001ddf0: 7468 6520 6164 640a 2020 2020 2020 2020  the add.        
+0001de00: 6966 206c 656e 2878 5f73 6861 7065 2920  if len(x_shape) 
+0001de10: 3d3d 2033 3a0a 2020 2020 2020 2020 2020  == 3:.          
+0001de20: 2020 6f75 7470 7574 5f78 203d 2050 2e52    output_x = P.R
+0001de30: 6573 6861 7065 2829 286f 7574 7075 745f  eshape()(output_
+0001de40: 782c 2078 5f73 6861 7065 290a 2020 2020  x, x_shape).    
+0001de50: 2020 2020 2020 2020 6d6c 705f 6c6f 6769          mlp_logi
+0001de60: 7420 3d20 502e 5265 7368 6170 6528 2928  t = P.Reshape()(
+0001de70: 6d6c 705f 6c6f 6769 742c 2078 5f73 6861  mlp_logit, x_sha
+0001de80: 7065 290a 2020 2020 2020 2020 2020 2020  pe).            
+0001de90: 7820 3d20 502e 5265 7368 6170 6528 2928  x = P.Reshape()(
+0001dea0: 782c 2078 5f73 6861 7065 290a 0a20 2020  x, x_shape)..   
+0001deb0: 2020 2020 2020 2020 2069 6620 7365 6c66           if self
+0001dec0: 2e70 6f73 745f 6c61 7965 726e 6f72 6d5f  .post_layernorm_
+0001ded0: 7265 7369 6475 616c 3a0a 2020 2020 2020  residual:.      
+0001dee0: 2020 2020 2020 2020 2020 6f75 7470 7574            output
+0001def0: 203d 2073 656c 662e 6164 645f 3364 286f   = self.add_3d(o
+0001df00: 7574 7075 745f 782c 206d 6c70 5f6c 6f67  utput_x, mlp_log
+0001df10: 6974 290a 2020 2020 2020 2020 2020 2020  it).            
+0001df20: 2020 2020 6f75 7470 7574 203d 2046 2e72      output = F.r
+0001df30: 6573 6861 7065 286f 7574 7075 742c 2028  eshape(output, (
+0001df40: 2d31 2c20 785f 7368 6170 655b 2d31 5d29  -1, x_shape[-1])
+0001df50: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
+0001df60: 2020 6f75 7470 7574 203d 2073 656c 662e    output = self.
+0001df70: 6c61 7965 726e 6f72 6d31 286f 7574 7075  layernorm1(outpu
+0001df80: 7429 0a20 2020 2020 2020 2020 2020 2020  t).             
+0001df90: 2020 206f 7574 7075 7420 3d20 462e 7265     output = F.re
+0001dfa0: 7368 6170 6528 6f75 7470 7574 2c20 785f  shape(output, x_
+0001dfb0: 7368 6170 6529 0a20 2020 2020 2020 2020  shape).         
+0001dfc0: 2020 2065 6c73 653a 0a20 2020 2020 2020     else:.       
+0001dfd0: 2020 2020 2020 2020 206f 7574 7075 7420           output 
+0001dfe0: 3d20 7365 6c66 2e61 6464 5f33 6428 782c  = self.add_3d(x,
+0001dff0: 206d 6c70 5f6c 6f67 6974 290a 2020 2020   mlp_logit).    
+0001e000: 2020 2020 656c 7365 3a0a 2020 2020 2020      else:.      
+0001e010: 2020 2020 2020 6966 2073 656c 662e 706f        if self.po
+0001e020: 7374 5f6c 6179 6572 6e6f 726d 5f72 6573  st_layernorm_res
+0001e030: 6964 7561 6c3a 0a20 2020 2020 2020 2020  idual:.         
+0001e040: 2020 2020 2020 206f 7574 7075 7420 3d20         output = 
+0001e050: 7365 6c66 2e61 6464 286f 7574 7075 745f  self.add(output_
+0001e060: 782c 206d 6c70 5f6c 6f67 6974 290a 2020  x, mlp_logit).  
+0001e070: 2020 2020 2020 2020 2020 2020 2020 6f75                ou
+0001e080: 7470 7574 203d 2073 656c 662e 6c61 7965  tput = self.laye
+0001e090: 726e 6f72 6d31 286f 7574 7075 7429 0a20  rnorm1(output). 
+0001e0a0: 2020 2020 2020 2020 2020 2065 6c73 653a             else:
+0001e0b0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0001e0c0: 206f 7574 7075 7420 3d20 7365 6c66 2e61   output = self.a
+0001e0d0: 6464 2878 2c20 6d6c 705f 6c6f 6769 7429  dd(x, mlp_logit)
+0001e0e0: 0a20 2020 2020 2020 2020 2020 206f 7574  .            out
+0001e0f0: 7075 7420 3d20 462e 7265 7368 6170 6528  put = F.reshape(
+0001e100: 6f75 7470 7574 2c20 785f 7368 6170 6529  output, x_shape)
+0001e110: 0a0a 2020 2020 2020 2020 6966 2073 656c  ..        if sel
+0001e120: 662e 7573 655f 6d6f 653a 0a20 2020 2020  f.use_moe:.     
+0001e130: 2020 2020 2020 2072 6574 7572 6e20 6f75         return ou
+0001e140: 7470 7574 2c20 6c61 7965 725f 7072 6573  tput, layer_pres
+0001e150: 656e 742c 2061 7578 5f6c 6f73 730a 2020  ent, aux_loss.  
+0001e160: 2020 2020 2020 7265 7475 726e 206f 7574        return out
+0001e170: 7075 742c 206c 6179 6572 5f70 7265 7365  put, layer_prese
+0001e180: 6e74 0a0a 2020 2020 6465 6620 5f63 6865  nt..    def _che
+0001e190: 636b 5f69 6e70 7574 2873 656c 662c 2078  ck_input(self, x
+0001e1a0: 2c20 696e 7075 745f 6d61 736b 2c20 696e  , input_mask, in
+0001e1b0: 6974 5f72 6573 6574 2c20 6261 7463 685f  it_reset, batch_
+0001e1c0: 7661 6c69 645f 6c65 6e67 7468 293a 0a20  valid_length):. 
+0001e1d0: 2020 2020 2020 2072 2222 2243 6865 636b         r"""Check
+0001e1e0: 2069 6e70 7574 7322 2222 0a20 2020 2020   inputs""".     
+0001e1f0: 2020 205f 6368 6563 6b5f 696e 7075 745f     _check_input_
+0001e200: 6474 7970 6528 462e 6474 7970 6528 7829  dtype(F.dtype(x)
+0001e210: 2c20 2278 222c 205b 6d73 7479 7065 2e66  , "x", [mstype.f
+0001e220: 6c6f 6174 3332 2c20 6d73 7479 7065 2e66  loat32, mstype.f
+0001e230: 6c6f 6174 3136 2c20 6d73 7479 7065 2e62  loat16, mstype.b
+0001e240: 666c 6f61 7431 365d 2c20 7365 6c66 2e63  float16], self.c
+0001e250: 6c73 5f6e 616d 6529 0a20 2020 2020 2020  ls_name).       
+0001e260: 2069 6620 696e 7075 745f 6d61 736b 2069   if input_mask i
+0001e270: 7320 6e6f 7420 4e6f 6e65 3a0a 2020 2020  s not None:.    
+0001e280: 2020 2020 2020 2020 5f63 6865 636b 5f69          _check_i
+0001e290: 6e70 7574 5f64 7479 7065 2846 2e64 7479  nput_dtype(F.dty
+0001e2a0: 7065 2869 6e70 7574 5f6d 6173 6b29 2c20  pe(input_mask), 
+0001e2b0: 2269 6e70 7574 5f6d 6173 6b22 2c0a 2020  "input_mask",.  
+0001e2c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e2d0: 2020 2020 2020 2020 2020 2020 205b 6d73               [ms
+0001e2e0: 7479 7065 2e66 6c6f 6174 3332 2c20 6d73  type.float32, ms
+0001e2f0: 7479 7065 2e66 6c6f 6174 3136 2c20 6d73  type.float16, ms
+0001e300: 7479 7065 2e62 666c 6f61 7431 365d 2c20  type.bfloat16], 
+0001e310: 7365 6c66 2e63 6c73 5f6e 616d 6529 0a0a  self.cls_name)..
+0001e320: 2020 2020 2020 2020 696e 6974 5f72 6573          init_res
+0001e330: 6574 5f69 735f 7465 6e73 6f72 203d 2069  et_is_tensor = i
+0001e340: 7369 6e73 7461 6e63 6528 696e 6974 5f72  sinstance(init_r
+0001e350: 6573 6574 2c20 5465 6e73 6f72 290a 2020  eset, Tensor).  
+0001e360: 2020 2020 2020 696e 6974 5f72 6573 6574        init_reset
+0001e370: 5f69 735f 6465 6661 756c 7420 3d20 696e  _is_default = in
+0001e380: 6974 5f72 6573 6574 2069 7320 5472 7565  it_reset is True
+0001e390: 0a20 2020 2020 2020 2062 6174 6368 5f76  .        batch_v
+0001e3a0: 616c 6964 5f6c 656e 6774 685f 6973 5f74  alid_length_is_t
+0001e3b0: 656e 736f 7220 3d20 6973 696e 7374 616e  ensor = isinstan
+0001e3c0: 6365 2862 6174 6368 5f76 616c 6964 5f6c  ce(batch_valid_l
+0001e3d0: 656e 6774 682c 2054 656e 736f 7229 0a20  ength, Tensor). 
+0001e3e0: 2020 2020 2020 2062 6174 6368 5f69 735f         batch_is_
+0001e3f0: 6465 6661 756c 7420 3d20 6261 7463 685f  default = batch_
+0001e400: 7661 6c69 645f 6c65 6e67 7468 2069 7320  valid_length is 
+0001e410: 4e6f 6e65 0a20 2020 2020 2020 205f 6368  None.        _ch
+0001e420: 6563 6b5f 7061 7374 5f6e 6f6e 655f 696e  eck_past_none_in
+0001e430: 7075 745f 6e6f 6e65 2873 656c 662e 7573  put_none(self.us
+0001e440: 655f 7061 7374 2c20 2269 6e69 745f 7265  e_past, "init_re
+0001e450: 7365 7422 2c20 7365 6c66 2e63 6c73 5f6e  set", self.cls_n
+0001e460: 616d 652c 2054 7275 652c 2069 6e69 745f  ame, True, init_
+0001e470: 7265 7365 745f 6973 5f74 656e 736f 722c  reset_is_tensor,
+0001e480: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0001e490: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e4a0: 2020 2020 2069 6e69 745f 7265 7365 745f       init_reset_
+0001e4b0: 6973 5f64 6566 6175 6c74 290a 2020 2020  is_default).    
+0001e4c0: 2020 2020 5f63 6865 636b 5f70 6173 745f      _check_past_
+0001e4d0: 6e6f 6e65 5f69 6e70 7574 5f6e 6f6e 6528  none_input_none(
+0001e4e0: 7365 6c66 2e75 7365 5f70 6173 742c 2022  self.use_past, "
+0001e4f0: 6261 7463 685f 7661 6c69 645f 6c65 6e67  batch_valid_leng
+0001e500: 7468 222c 2073 656c 662e 636c 735f 6e61  th", self.cls_na
+0001e510: 6d65 2c20 4e6f 6e65 2c0a 2020 2020 2020  me, None,.      
+0001e520: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e530: 2020 2020 2020 2020 2020 2020 2020 6261                ba
+0001e540: 7463 685f 7661 6c69 645f 6c65 6e67 7468  tch_valid_length
+0001e550: 5f69 735f 7465 6e73 6f72 2c20 6261 7463  _is_tensor, batc
+0001e560: 685f 6973 5f64 6566 6175 6c74 290a 0a20  h_is_default).. 
+0001e570: 2020 2020 2020 2069 6620 7365 6c66 2e75         if self.u
+0001e580: 7365 5f70 6173 743a 0a20 2020 2020 2020  se_past:.       
+0001e590: 2020 2020 205f 6368 6563 6b5f 696e 7075       _check_inpu
+0001e5a0: 745f 6474 7970 6528 462e 6474 7970 6528  t_dtype(F.dtype(
+0001e5b0: 6261 7463 685f 7661 6c69 645f 6c65 6e67  batch_valid_leng
+0001e5c0: 7468 292c 2022 6261 7463 685f 7661 6c69  th), "batch_vali
+0001e5d0: 645f 6c65 6e67 7468 222c 205b 6d73 7479  d_length", [msty
+0001e5e0: 7065 2e69 6e74 3332 5d2c 2073 656c 662e  pe.int32], self.
+0001e5f0: 636c 735f 6e61 6d65 290a 2020 2020 2020  cls_name).      
+0001e600: 2020 7265 7475 726e 2054 7275 650a 0a0a    return True...
+0001e610: 636c 6173 7320 5472 616e 7366 6f72 6d65  class Transforme
+0001e620: 7244 6563 6f64 6572 4c61 7965 7228 4365  rDecoderLayer(Ce
+0001e630: 6c6c 293a 0a20 2020 2072 2222 220a 2020  ll):.    r""".  
+0001e640: 2020 2020 2020 5472 616e 7366 6f72 6d65        Transforme
+0001e650: 7220 4465 636f 6465 7220 4c61 7965 722e  r Decoder Layer.
+0001e660: 2054 6869 7320 6973 2061 6e20 696d 706c   This is an impl
+0001e670: 656d 656e 7461 7469 6f6e 206f 6620 7468  ementation of th
+0001e680: 6520 7369 6e67 6c65 206c 6179 6572 206f  e single layer o
+0001e690: 6620 7468 6520 7472 616e 7366 6f72 6d65  f the transforme
+0001e6a0: 720a 2020 2020 2020 2020 6465 636f 6465  r.        decode
+0001e6b0: 7220 6c61 7965 722c 2069 6e63 6c75 6469  r layer, includi
+0001e6c0: 6e67 2073 656c 662d 6174 7465 6e74 696f  ng self-attentio
+0001e6d0: 6e2c 2063 726f 7373 2061 7474 656e 7469  n, cross attenti
+0001e6e0: 6f6e 2061 6e64 2066 6565 6477 6172 6420  on and feedward 
+0001e6f0: 6c61 7965 722e 2057 6865 6e20 7468 6520  layer. When the 
+0001e700: 656e 636f 6465 725f 6f75 7470 7574 2069  encoder_output i
+0001e710: 7320 4e6f 6e65 2c0a 2020 2020 2020 2020  s None,.        
+0001e720: 7468 6520 6372 6f73 7320 6174 7465 6e74  the cross attent
+0001e730: 696f 6e20 7769 6c6c 206e 6f74 2062 6520  ion will not be 
+0001e740: 6566 6665 6374 6976 652e 0a0a 2020 2020  effective...    
+0001e750: 2020 2020 4172 6773 3a0a 2020 2020 2020      Args:.      
+0001e760: 2020 2020 2020 6869 6464 656e 5f73 697a        hidden_siz
+0001e770: 6528 696e 7429 3a20 5468 6520 6869 6464  e(int): The hidd
+0001e780: 656e 2073 697a 6520 6f66 2074 6865 2069  en size of the i
+0001e790: 6e70 7574 2e0a 2020 2020 2020 2020 2020  nput..          
+0001e7a0: 2020 6666 6e5f 6869 6464 656e 5f73 697a    ffn_hidden_siz
+0001e7b0: 6528 696e 7429 3a20 5468 6520 6869 6464  e(int): The hidd
+0001e7c0: 656e 2073 697a 6520 6f66 2062 6f74 746c  en size of bottl
+0001e7d0: 656e 6563 6b20 696e 2074 6865 2066 6565  eneck in the fee
+0001e7e0: 6466 6f72 7761 7264 206c 6179 6572 2e0a  dforward layer..
+0001e7f0: 2020 2020 2020 2020 2020 2020 6e75 6d5f              num_
+0001e800: 6865 6164 7328 696e 7429 3a20 5468 6520  heads(int): The 
+0001e810: 6e75 6d62 6572 206f 6620 7468 6520 6865  number of the he
+0001e820: 6164 732e 0a20 2020 2020 2020 2020 2020  ads..           
+0001e830: 2062 6174 6368 5f73 697a 6528 696e 7429   batch_size(int)
+0001e840: 3a20 5468 6520 6261 7463 6820 7369 7a65  : The batch size
+0001e850: 206f 6620 7468 6520 696e 7075 7420 7465   of the input te
+0001e860: 6e73 6f72 2077 6865 6e20 646f 2069 6e63  nsor when do inc
+0001e870: 7265 6e6d 656e 7461 6c20 7072 6564 6963  renmental predic
+0001e880: 7469 6f6e 2e20 5368 6f75 6c64 2062 6520  tion. Should be 
+0001e890: 6120 706f 7369 7469 7665 0a20 2020 2020  a positive.     
+0001e8a0: 2020 2020 2020 2020 2020 2076 616c 7565             value
+0001e8b0: 2e20 5768 656e 2064 6f20 7472 6169 6e69  . When do traini
+0001e8c0: 6e67 206f 7220 7072 6564 6963 7469 6f6e  ng or prediction
+0001e8d0: 2c20 7468 6520 6172 6775 6d65 6e74 2077  , the argument w
+0001e8e0: 696c 6c20 6e6f 7420 776f 726b 2061 6e64  ill not work and
+0001e8f0: 2074 6865 2075 7365 7220 6361 6e20 6a75   the user can ju
+0001e900: 7374 2070 6173 7320 4e6f 6e65 2074 6f0a  st pass None to.
+0001e910: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001e920: 7468 6520 6172 6775 6d65 6e74 2e0a 2020  the argument..  
+0001e930: 2020 2020 2020 2020 2020 7372 635f 7365            src_se
+0001e940: 715f 6c65 6e67 7468 2869 6e74 293a 2054  q_length(int): T
+0001e950: 6865 2069 6e70 7574 2073 6f75 7263 6520  he input source 
+0001e960: 7365 7175 656e 6365 206c 656e 6774 682e  sequence length.
+0001e970: 0a20 2020 2020 2020 2020 2020 2074 6774  .            tgt
+0001e980: 5f73 6571 5f6c 656e 6774 6828 696e 7429  _seq_length(int)
+0001e990: 3a20 5468 6520 696e 7075 7420 7461 7267  : The input targ
+0001e9a0: 6574 2073 6571 7565 6e63 6520 6c65 6e67  et sequence leng
+0001e9b0: 7468 2e0a 2020 2020 2020 2020 2020 2020  th..            
+0001e9c0: 6174 7465 6e74 696f 6e5f 6472 6f70 6f75  attention_dropou
+0001e9d0: 745f 7261 7465 2866 6c6f 6174 293a 2054  t_rate(float): T
+0001e9e0: 6865 2064 726f 706f 7574 2072 6174 6520  he dropout rate 
+0001e9f0: 6f66 2074 6865 2061 7474 656e 7469 6f6e  of the attention
+0001ea00: 2073 636f 7265 732e 2044 6566 6175 6c74   scores. Default
+0001ea10: 3a30 2e31 2e0a 2020 2020 2020 2020 2020  :0.1..          
+0001ea20: 2020 6869 6464 656e 5f64 726f 706f 7574    hidden_dropout
+0001ea30: 5f72 6174 6528 666c 6f61 7429 3a20 5468  _rate(float): Th
+0001ea40: 6520 6472 6f70 6f75 7420 7261 7465 206f  e dropout rate o
+0001ea50: 6620 7468 6520 6669 6e61 6c20 6f75 7470  f the final outp
+0001ea60: 7574 206f 6620 7468 6520 6c61 7965 722e  ut of the layer.
+0001ea70: 2044 6566 6175 6c74 3a30 2e31 2e0a 2020   Default:0.1..  
+0001ea80: 2020 2020 2020 2020 2020 706f 7374 5f6c            post_l
+0001ea90: 6179 6572 6e6f 726d 5f72 6573 6964 7561  ayernorm_residua
+0001eaa0: 6c28 626f 6f6c 293a 2044 6f20 7265 7369  l(bool): Do resi
+0001eab0: 6475 616c 7320 6164 6473 2062 6566 6f72  duals adds befor
+0001eac0: 6520 7468 6520 6c61 7965 726e 6f72 6d2e  e the layernorm.
+0001ead0: 2044 6566 6175 6c74 2046 616c 7365 2e0a   Default False..
+0001eae0: 2020 2020 2020 2020 2020 2020 7573 655f              use_
+0001eaf0: 7061 7374 2862 6f6f 6c29 3a20 5573 6520  past(bool): Use 
+0001eb00: 7468 6520 7061 7374 2073 7461 7465 2074  the past state t
+0001eb10: 6f20 636f 6d70 7574 652c 2075 7365 6420  o compute, used 
+0001eb20: 666f 7220 696e 6372 656d 656e 7461 6c20  for incremental 
+0001eb30: 7072 6564 6963 7469 6f6e 2e20 4465 6661  prediction. Defa
+0001eb40: 756c 7420 4661 6c73 652e 0a20 2020 2020  ult False..     
+0001eb50: 2020 2020 2020 206c 6179 6572 6e6f 726d         layernorm
+0001eb60: 5f63 6f6d 7075 7465 5f74 7970 6528 6474  _compute_type(dt
+0001eb70: 7970 652e 4e75 6d62 6572 293a 2054 6865  ype.Number): The
+0001eb80: 2063 6f6d 7075 7461 7469 6f6e 2074 7970   computation typ
+0001eb90: 6520 6f66 2074 6865 206c 6179 6572 6e6f  e of the layerno
+0001eba0: 726d 2e0a 2020 2020 2020 2020 2020 2020  rm..            
+0001ebb0: 2020 2020 5368 6f75 6c64 2062 6520 6474      Should be dt
+0001ebc0: 7970 652e 666c 6f61 7433 3220 6f72 2064  ype.float32 or d
+0001ebd0: 7479 7065 2e66 6c6f 6174 3136 2e20 4465  type.float16. De
+0001ebe0: 6661 756c 7420 6474 7970 652e 666c 6f61  fault dtype.floa
+0001ebf0: 7433 322e 0a20 2020 2020 2020 2020 2020  t32..           
+0001ec00: 2073 6f66 746d 6178 5f63 6f6d 7075 7465   softmax_compute
+0001ec10: 5f74 7970 6528 6474 7970 652e 4e75 6d62  _type(dtype.Numb
+0001ec20: 6572 293a 2054 6865 2063 6f6d 7075 7461  er): The computa
+0001ec30: 7469 6f6e 2074 7970 6520 6f66 2074 6865  tion type of the
+0001ec40: 2073 6f66 746d 6178 2069 6e20 7468 6520   softmax in the 
+0001ec50: 6174 7465 6e74 696f 6e2e 0a20 2020 2020  attention..     
+0001ec60: 2020 2020 2020 2020 2020 2053 686f 756c             Shoul
+0001ec70: 6420 6265 2064 7479 7065 2e66 6c6f 6174  d be dtype.float
+0001ec80: 3332 206f 7220 6474 7970 652e 666c 6f61  32 or dtype.floa
+0001ec90: 7431 362e 2044 6566 6175 6c74 206d 7374  t16. Default mst
+0001eca0: 7970 652e 666c 6f61 7433 322e 0a20 2020  ype.float32..   
+0001ecb0: 2020 2020 2020 2020 2070 6172 616d 5f69           param_i
+0001ecc0: 6e69 745f 7479 7065 2864 7479 7065 2e4e  nit_type(dtype.N
+0001ecd0: 756d 6265 7229 3a20 5468 6520 7061 7261  umber): The para
+0001ece0: 6d65 7465 7220 696e 6974 6961 6c69 7a61  meter initializa
+0001ecf0: 7469 6f6e 2074 7970 6520 6f66 2074 6865  tion type of the
+0001ed00: 206d 6f64 756c 652e 0a20 2020 2020 2020   module..       
+0001ed10: 2020 2020 2020 2020 2053 686f 756c 6420           Should 
+0001ed20: 6265 2064 7479 7065 2e66 6c6f 6174 3332  be dtype.float32
+0001ed30: 206f 7220 6474 7970 652e 666c 6f61 7431   or dtype.float1
+0001ed40: 362e 2044 6566 6175 6c74 2064 7479 7065  6. Default dtype
+0001ed50: 2e66 6c6f 6174 3332 2e0a 2020 2020 2020  .float32..      
+0001ed60: 2020 2020 2020 6869 6464 656e 5f61 6374        hidden_act
+0001ed70: 2028 7374 722c 206e 6e2e 4365 6c6c 293a   (str, nn.Cell):
+0001ed80: 2054 6865 2061 6374 6976 6174 696f 6e20   The activation 
+0001ed90: 6f66 2074 6865 2069 6e74 6572 6e61 6c20  of the internal 
+0001eda0: 6665 6564 666f 7277 6172 6420 6c61 7965  feedforward laye
+0001edb0: 722e 2053 7570 706f 7274 7320 2772 656c  r. Supports 'rel
+0001edc0: 7527 2c0a 2020 2020 2020 2020 2020 2020  u',.            
+0001edd0: 2020 2020 2772 656c 7536 272c 2027 7461      'relu6', 'ta
+0001ede0: 6e68 272c 2027 6765 6c75 272c 2027 6661  nh', 'gelu', 'fa
+0001edf0: 7374 5f67 656c 7527 2c20 2765 6c75 272c  st_gelu', 'elu',
+0001ee00: 2027 7369 676d 6f69 6427 2c20 2770 7265   'sigmoid', 'pre
+0001ee10: 6c75 272c 2027 6c65 616b 7972 656c 7527  lu', 'leakyrelu'
+0001ee20: 2c20 2768 7377 6973 6827 2c0a 2020 2020  , 'hswish',.    
+0001ee30: 2020 2020 2020 2020 2020 2020 2768 7369              'hsi
+0001ee40: 676d 6f69 6427 2c20 276c 6f67 7369 676d  gmoid', 'logsigm
+0001ee50: 6f69 6427 2061 6e64 2073 6f20 6f6e 2e20  oid' and so on. 
+0001ee60: 5573 6572 2063 616e 2070 726f 7669 6465  User can provide
+0001ee70: 2063 7573 746f 6d20 6163 7469 7669 7469   custom activiti
+0001ee80: 6f6e 2074 6f20 7468 6520 6172 6775 6d65  on to the argume
+0001ee90: 6e74 2e0a 2020 2020 2020 2020 2020 2020  nt..            
+0001eea0: 2020 2020 4966 2075 7365 7220 7761 6e74      If user want
+0001eeb0: 7320 746f 2072 756e 2074 6865 206e 6574  s to run the net
+0001eec0: 2069 6e20 7468 6520 7061 7261 6c6c 656c   in the parallel
+0001eed0: 206d 6f64 652c 2074 6865 2063 7573 746f   mode, the custo
+0001eee0: 6d20 6163 7469 7661 7469 6f6e 206d 7573  m activation mus
+0001eef0: 7420 616c 736f 2070 726f 7669 6465 0a20  t also provide. 
+0001ef00: 2020 2020 2020 2020 2020 2020 2020 2074                 t
+0001ef10: 6865 2060 6163 7469 7661 7469 6f6e 5f73  he `activation_s
+0001ef20: 6861 7264 6020 6675 6e63 7469 6f6e 2e20  hard` function. 
+0001ef30: 506c 6561 7365 2073 6565 2074 6865 2065  Please see the e
+0001ef40: 7861 6d70 6c65 7320 6f66 2074 6865 0a20  xamples of the. 
+0001ef50: 2020 2020 2020 2020 2020 2020 2020 2063                 c
+0001ef60: 6c61 7373 3a60 6d69 6e64 666f 726d 6572  lass:`mindformer
+0001ef70: 732e 6d6f 6475 6c65 732e 7472 616e 7366  s.modules.transf
+0001ef80: 6f72 6d65 722e 4665 6564 466f 7277 6172  ormer.FeedForwar
+0001ef90: 6460 2e20 4465 6661 756c 743a 2067 656c  d`. Default: gel
+0001efa0: 752e 0a20 2020 2020 2020 2020 2020 206d  u..            m
+0001efb0: 6f65 5f63 6f6e 6669 6728 4d6f 4543 6f6e  oe_config(MoECon
+0001efc0: 6669 6729 3a20 5468 6520 636f 6e66 6967  fig): The config
+0001efd0: 7572 6174 696f 6e20 6f66 204d 6f45 2028  uration of MoE (
+0001efe0: 4d69 7874 7572 6520 6f66 2045 7870 6572  Mixture of Exper
+0001eff0: 7429 2e20 4465 6661 756c 7420 6973 2061  t). Default is a
+0001f000: 6e20 696e 7374 616e 6365 206f 6620 4d6f  n instance of Mo
+0001f010: 4543 6f6e 6669 670a 2020 2020 2020 2020  EConfig.        
+0001f020: 2020 2020 2020 2020 7769 7468 2064 6566          with def
+0001f030: 6175 6c74 2076 616c 7565 732e 2050 6c65  ault values. Ple
+0001f040: 6173 6520 7365 6520 604d 6f45 436f 6e66  ase see `MoEConf
+0001f050: 6967 602e 0a20 2020 2020 2020 2020 2020  ig`..           
+0001f060: 2070 6172 616c 6c65 6c5f 636f 6e66 6967   parallel_config
+0001f070: 284f 7050 6172 616c 6c65 6c43 6f6e 6669  (OpParallelConfi
+0001f080: 672c 204d 6f45 5061 7261 6c6c 656c 436f  g, MoEParallelCo
+0001f090: 6e66 6967 293a 2054 6865 2070 6172 616c  nfig): The paral
+0001f0a0: 6c65 6c20 636f 6e66 6967 7572 652e 2057  lel configure. W
+0001f0b0: 6865 6e20 4d6f 4520 6973 2061 7070 6c69  hen MoE is appli
+0001f0c0: 6564 2c0a 2020 2020 2020 2020 2020 2020  ed,.            
+0001f0d0: 2020 2020 4d6f 4550 6172 616c 6c65 6c43      MoEParallelC
+0001f0e0: 6f6e 6669 6720 6973 2065 6666 6563 7469  onfig is effecti
+0001f0f0: 7665 2c20 6f74 6865 7277 6973 6520 4f70  ve, otherwise Op
+0001f100: 5061 7261 6c6c 656c 436f 6e66 6967 2069  ParallelConfig i
+0001f110: 7320 6566 6665 6374 6976 652e 2044 6566  s effective. Def
+0001f120: 6175 6c74 2060 6465 6661 756c 745f 6470  ault `default_dp
+0001f130: 6d70 5f63 6f6e 6669 6760 2c0a 2020 2020  mp_config`,.    
+0001f140: 2020 2020 2020 2020 2020 2020 616e 2069              an i
+0001f150: 6e73 7461 6e63 6520 6f66 2060 4f70 5061  nstance of `OpPa
+0001f160: 7261 6c6c 656c 436f 6e66 6967 6020 7769  rallelConfig` wi
+0001f170: 7468 2064 6566 6175 6c74 2061 7267 732e  th default args.
+0001f180: 0a0a 2020 2020 2020 2020 496e 7075 7473  ..        Inputs
+0001f190: 3a0a 2020 2020 2020 2020 2020 2020 2d20  :.            - 
+0001f1a0: 2a2a 6869 6464 656e 5f73 7461 7473 2a2a  **hidden_stats**
+0001f1b0: 2028 5465 6e73 6f72 2920 2d20 5468 6520   (Tensor) - The 
+0001f1c0: 696e 7075 7420 7465 6e73 6f72 2077 6974  input tensor wit
+0001f1d0: 6820 7368 6170 6520 5b62 6174 6368 5f73  h shape [batch_s
+0001f1e0: 697a 652c 2074 6774 5f73 6571 5f6c 656e  ize, tgt_seq_len
+0001f1f0: 6774 682c 2068 6964 6465 6e5f 7369 7a65  gth, hidden_size
+0001f200: 5d20 6f72 0a20 2020 2020 2020 2020 2020  ] or.           
+0001f210: 2020 205b 6261 7463 685f 7369 7a65 202a     [batch_size *
+0001f220: 2074 6774 5f73 6571 5f6c 656e 6774 682c   tgt_seq_length,
+0001f230: 2068 6964 6465 6e5f 7369 7a65 5d2e 0a20   hidden_size].. 
+0001f240: 2020 2020 2020 2020 2020 202d 202a 2a64             - **d
+0001f250: 6563 6f64 6572 5f6d 6173 6b2a 2a20 2854  ecoder_mask** (T
+0001f260: 656e 736f 7229 202d 2054 6865 2061 7474  ensor) - The att
+0001f270: 656e 7469 6f6e 206d 6173 6b20 666f 7220  ention mask for 
+0001f280: 6465 636f 6465 7220 7769 7468 2073 6861  decoder with sha
+0001f290: 7065 205b 6261 7463 685f 7369 7a65 2c20  pe [batch_size, 
+0001f2a0: 7372 635f 7365 715f 6c65 6e67 7468 2c0a  src_seq_length,.
+0001f2b0: 2020 2020 2020 2020 2020 2020 2020 7365                se
+0001f2c0: 715f 6c65 6e67 7468 5d20 6f72 204e 6f6e  q_length] or Non
+0001f2d0: 652e 204e 6f6e 6520 6d65 616e 7320 7468  e. None means th
+0001f2e0: 6572 6520 7769 6c6c 2062 6520 6e6f 206d  ere will be no m
+0001f2f0: 6173 6b20 696e 2073 6f66 746d 6178 2063  ask in softmax c
+0001f300: 6f6d 7075 7461 7469 6f6e 2069 6e20 7365  omputation in se
+0001f310: 6c66 2061 7474 656e 7469 6f6e 2e0a 2020  lf attention..  
+0001f320: 2020 2020 2020 2020 2020 2d20 2a2a 656e            - **en
+0001f330: 636f 6465 725f 6f75 7470 7574 2a2a 2028  coder_output** (
+0001f340: 5465 6e73 6f72 2920 2d20 5468 6520 6f75  Tensor) - The ou
+0001f350: 7470 7574 206f 6620 7468 6520 656e 636f  tput of the enco
+0001f360: 6465 7220 7769 7468 2073 6861 7065 205b  der with shape [
+0001f370: 6261 7463 685f 7369 7a65 2c20 7365 715f  batch_size, seq_
+0001f380: 6c65 6e67 7468 2c20 6869 6464 656e 5f73  length, hidden_s
+0001f390: 697a 655d 0a20 2020 2020 2020 2020 2020  ize].           
+0001f3a0: 2020 206f 7220 5b62 6174 6368 5f73 697a     or [batch_siz
+0001f3b0: 6520 2a20 7365 715f 6c65 6e67 7468 2c20  e * seq_length, 
+0001f3c0: 6869 6464 656e 5f73 697a 655d 2e0a 2020  hidden_size]..  
+0001f3d0: 2020 2020 2020 2020 2020 2020 4e6f 7465              Note
+0001f3e0: 2074 6869 7320 6172 6773 2063 616e 206e   this args can n
+0001f3f0: 6f74 2062 6520 7061 7373 6564 2062 7920  ot be passed by 
+0001f400: 4e6f 6e65 2077 6865 6e20 7468 6520 6e65  None when the ne
+0001f410: 7420 6973 2069 6e20 6f75 7465 726d 6f73  t is in outermos
+0001f420: 7420 6c61 7965 722e 2044 6566 6175 6c74  t layer. Default
+0001f430: 204e 6f6e 652e 0a20 2020 2020 2020 2020   None..         
+0001f440: 2020 202d 202a 2a6d 656d 6f72 795f 6d61     - **memory_ma
+0001f450: 736b 2a2a 2028 5465 6e73 6f72 2920 2d20  sk** (Tensor) - 
+0001f460: 5468 6520 6d65 6d6f 7279 206d 6173 6b20  The memory mask 
+0001f470: 6f66 2074 6865 2063 726f 7373 2061 7474  of the cross att
+0001f480: 656e 7469 6f6e 2077 6974 6820 7368 6170  ention with shap
+0001f490: 6520 5b62 6174 6368 2c20 7467 745f 7365  e [batch, tgt_se
+0001f4a0: 715f 6c65 6e67 7468 2c0a 2020 2020 2020  q_length,.      
+0001f4b0: 2020 2020 2020 2020 7372 635f 7365 715f          src_seq_
+0001f4c0: 6c65 6e67 7468 5d20 7768 6572 6520 7467  length] where tg
+0001f4d0: 745f 7365 715f 6c65 6e67 7468 2069 7320  t_seq_length is 
+0001f4e0: 7468 6520 6c65 6e67 7468 206f 6620 7468  the length of th
+0001f4f0: 6520 6465 636f 6465 722e 2054 6865 2075  e decoder. The u
+0001f500: 7365 7220 6361 6e20 616c 736f 2070 6173  ser can also pas
+0001f510: 7320 4e6f 6e65 2e20 4e6f 6e65 0a20 2020  s None. None.   
+0001f520: 2020 2020 2020 2020 2020 206d 6561 6e73             means
+0001f530: 2074 6865 7265 2077 696c 6c20 6265 206e   there will be n
+0001f540: 6f20 6d61 736b 2069 6e20 736f 6674 6d61  o mask in softma
+0001f550: 7820 636f 6d70 7574 6174 696f 6e20 696e  x computation in
+0001f560: 2063 726f 7373 2061 7474 656e 7469 6f6e   cross attention
+0001f570: 2e20 4465 6661 756c 7420 4e6f 6e65 2e0a  . Default None..
+0001f580: 2020 2020 2020 2020 2020 2020 2d20 2a2a              - **
+0001f590: 696e 6974 5f72 6573 6574 2a2a 2028 5465  init_reset** (Te
+0001f5a0: 6e73 6f72 2920 2d20 4120 626f 6f6c 2074  nsor) - A bool t
+0001f5b0: 656e 736f 7220 7769 7468 2073 6861 7065  ensor with shape
+0001f5c0: 205b 315d 2c20 7573 6564 2074 6f20 636c   [1], used to cl
+0001f5d0: 6561 7220 7468 6520 7061 7374 206b 6579  ear the past key
+0001f5e0: 2070 6172 616d 6574 6572 2061 6e64 0a20   parameter and. 
+0001f5f0: 2020 2020 2020 2020 2020 2020 2070 6173               pas
+0001f600: 7420 7661 6c75 6520 7061 7261 6d65 7465  t value paramete
+0001f610: 7220 7573 6564 2069 6e20 7468 6520 696e  r used in the in
+0001f620: 6372 656d 656e 7461 6c20 7072 6564 6963  cremental predic
+0001f630: 7469 6f6e 2e20 4f6e 6c79 2076 616c 6964  tion. Only valid
+0001f640: 2077 6865 6e20 7573 655f 7061 7374 2069   when use_past i
+0001f650: 7320 5472 7565 2e20 4465 6661 756c 7420  s True. Default 
+0001f660: 5472 7565 2e0a 2020 2020 2020 2020 2020  True..          
+0001f670: 2020 2d20 2a2a 6261 7463 685f 7661 6c69    - **batch_vali
+0001f680: 645f 6c65 6e67 7468 2a2a 2028 5465 6e73  d_length** (Tens
+0001f690: 6f72 2920 2d20 496e 7433 3220 7465 6e73  or) - Int32 tens
+0001f6a0: 6f72 2077 6974 6820 7368 6170 6520 5b62  or with shape [b
+0001f6b0: 6174 6368 5f73 697a 655d 2074 6865 2070  atch_size] the p
+0001f6c0: 6173 7420 6361 6c63 756c 6174 6564 2074  ast calculated t
+0001f6d0: 6865 2069 6e64 6578 2e0a 2020 2020 2020  he index..      
+0001f6e0: 2020 2020 2020 2020 5573 6564 2066 6f72          Used for
+0001f6f0: 2069 6e63 7265 6d65 6e74 616c 2070 7265   incremental pre
+0001f700: 6469 6374 696f 6e20 7768 656e 2074 6865  diction when the
+0001f710: 2075 7365 5f70 6173 7420 6973 2054 7275   use_past is Tru
+0001f720: 652e 2044 6566 6175 6c74 204e 6f6e 652e  e. Default None.
+0001f730: 0a0a 2020 2020 2020 2020 4f75 7470 7574  ..        Output
+0001f740: 733a 0a20 2020 2020 2020 2020 2020 2054  s:.            T
+0001f750: 7570 6c65 2c20 6120 7475 706c 6520 636f  uple, a tuple co
+0001f760: 6e74 6169 6e73 2860 6f75 7470 7574 602c  ntains(`output`,
+0001f770: 2060 6c61 7965 725f 7072 6573 656e 7460   `layer_present`
+0001f780: 290a 0a20 2020 2020 2020 2020 2020 202d  )..            -
+0001f790: 202a 2a6f 7574 7075 742a 2a20 2854 656e   **output** (Ten
+0001f7a0: 736f 7229 202d 2054 6865 206f 7574 7075  sor) - The outpu
+0001f7b0: 7420 6c6f 6769 7420 6f66 2074 6869 7320  t logit of this 
+0001f7c0: 6c61 7965 722e 2054 6865 2073 6861 7065  layer. The shape
+0001f7d0: 2069 7320 5b62 6174 6368 2c20 7365 715f   is [batch, seq_
+0001f7e0: 6c65 6e67 7468 2c20 6869 6464 656e 5f73  length, hidden_s
+0001f7f0: 697a 655d 206f 720a 2020 2020 2020 2020  ize] or.        
+0001f800: 2020 2020 2020 5b62 6174 6368 202a 2073        [batch * s
+0001f810: 6571 5f6c 656e 6774 682c 2068 6964 6465  eq_length, hidde
+0001f820: 6e5f 7369 7a65 5d2e 0a20 2020 2020 2020  n_size]..       
+0001f830: 2020 2020 202d 202a 2a6c 6179 6572 5f70       - **layer_p
+0001f840: 7265 7365 6e74 2a2a 2028 5475 706c 6529  resent** (Tuple)
+0001f850: 202d 2041 2074 7570 6c65 2c20 7768 6572   - A tuple, wher
+0001f860: 6520 6561 6368 2074 7570 6c65 2069 7320  e each tuple is 
+0001f870: 7468 6520 7465 6e73 6f72 206f 6620 7468  the tensor of th
+0001f880: 6520 7072 6f6a 6563 7465 6420 6b65 7920  e projected key 
+0001f890: 616e 6420 7661 6c75 650a 2020 2020 2020  and value.      
+0001f8a0: 2020 2020 2020 2020 7665 6374 6f72 2069          vector i
+0001f8b0: 6e20 7365 6c66 2061 7474 656e 7469 6f6e  n self attention
+0001f8c0: 2077 6974 6820 7368 6170 6520 2828 6261   with shape ((ba
+0001f8d0: 7463 685f 7369 7a65 2c20 6e75 6d5f 6865  tch_size, num_he
+0001f8e0: 6164 732c 2073 697a 655f 7065 725f 6865  ads, size_per_he
+0001f8f0: 6164 2c20 7467 745f 7365 715f 6c65 6e67  ad, tgt_seq_leng
+0001f900: 7468 292c 0a20 2020 2020 2020 2020 2020  th),.           
+0001f910: 2020 2028 6261 7463 685f 7369 7a65 2c20     (batch_size, 
+0001f920: 6e75 6d5f 6865 6164 732c 2074 6774 5f73  num_heads, tgt_s
+0001f930: 6571 5f6c 656e 6774 682c 2073 697a 655f  eq_length, size_
+0001f940: 7065 725f 6865 6164 292c 2061 6e64 206f  per_head), and o
+0001f950: 6620 7468 6520 7072 6f6a 6563 7465 6420  f the projected 
+0001f960: 6b65 7920 616e 6420 7661 6c75 6520 7665  key and value ve
+0001f970: 6374 6f72 0a20 2020 2020 2020 2020 2020  ctor.           
+0001f980: 2020 2069 6e20 6372 6f73 7320 6174 7465     in cross atte
+0001f990: 6e74 696f 6e20 7769 7468 2073 6861 7065  ntion with shape
+0001f9a0: 2020 2862 6174 6368 5f73 697a 652c 206e    (batch_size, n
+0001f9b0: 756d 5f68 6561 6473 2c20 7369 7a65 5f70  um_heads, size_p
+0001f9c0: 6572 5f68 6561 642c 2073 7263 5f73 6571  er_head, src_seq
+0001f9d0: 5f6c 656e 6774 6829 2c0a 2020 2020 2020  _length),.      
+0001f9e0: 2020 2020 2020 2020 2862 6174 6368 5f73          (batch_s
+0001f9f0: 697a 652c 206e 756d 5f68 6561 6473 2c20  ize, num_heads, 
+0001fa00: 7372 635f 7365 715f 6c65 6e67 7468 2c20  src_seq_length, 
+0001fa10: 7369 7a65 5f70 6572 5f68 6561 6429 292e  size_per_head)).
+0001fa20: 0a0a 2020 2020 2020 2020 5375 7070 6f72  ..        Suppor
+0001fa30: 7465 6420 506c 6174 666f 726d 733a 0a20  ted Platforms:. 
+0001fa40: 2020 2020 2020 2020 2020 2060 6041 7363             ``Asc
+0001fa50: 656e 6460 6020 6060 4750 5560 600a 0a20  end`` ``GPU``.. 
+0001fa60: 2020 2020 2020 2045 7861 6d70 6c65 733a         Examples:
+0001fa70: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
+0001fa80: 2069 6d70 6f72 7420 6e75 6d70 7920 6173   import numpy as
+0001fa90: 206e 700a 2020 2020 2020 2020 2020 2020   np.            
+0001faa0: 3e3e 3e20 6672 6f6d 206d 696e 6473 706f  >>> from mindspo
+0001fab0: 7265 2069 6d70 6f72 7420 6474 7970 6520  re import dtype 
+0001fac0: 6173 206d 7374 7970 650a 2020 2020 2020  as mstype.      
+0001fad0: 2020 2020 2020 3e3e 3e20 6672 6f6d 206d        >>> from m
+0001fae0: 696e 6466 6f72 6d65 7273 2e6d 6f64 756c  indformers.modul
+0001faf0: 6573 2e74 7261 6e73 666f 726d 6572 2069  es.transformer i
+0001fb00: 6d70 6f72 7420 5472 616e 7366 6f72 6d65  mport Transforme
+0001fb10: 7244 6563 6f64 6572 4c61 7965 720a 2020  rDecoderLayer.  
+0001fb20: 2020 2020 2020 2020 2020 3e3e 3e20 6672            >>> fr
+0001fb30: 6f6d 206d 696e 6473 706f 7265 2069 6d70  om mindspore imp
+0001fb40: 6f72 7420 5465 6e73 6f72 0a20 2020 2020  ort Tensor.     
+0001fb50: 2020 2020 2020 203e 3e3e 206d 6f64 656c         >>> model
+0001fb60: 203d 2054 7261 6e73 666f 726d 6572 4465   = TransformerDe
+0001fb70: 636f 6465 724c 6179 6572 2862 6174 6368  coderLayer(batch
+0001fb80: 5f73 697a 653d 322c 2068 6964 6465 6e5f  _size=2, hidden_
+0001fb90: 7369 7a65 3d36 342c 2066 666e 5f68 6964  size=64, ffn_hid
+0001fba0: 6465 6e5f 7369 7a65 3d36 342c 206e 756d  den_size=64, num
+0001fbb0: 5f68 6561 6473 3d32 2c0a 2020 2020 2020  _heads=2,.      
+0001fbc0: 2020 2020 2020 2e2e 2e20 2020 2020 2020        ...       
+0001fbd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001fbe0: 2020 2020 2020 2020 2020 7372 635f 7365            src_se
+0001fbf0: 715f 6c65 6e67 7468 3d32 302c 2074 6774  q_length=20, tgt
+0001fc00: 5f73 6571 5f6c 656e 6774 683d 3130 290a  _seq_length=10).
+0001fc10: 2020 2020 2020 2020 2020 2020 3e3e 3e20              >>> 
+0001fc20: 656e 636f 6465 725f 696e 7075 745f 7661  encoder_input_va
+0001fc30: 6c75 6520 3d20 5465 6e73 6f72 286e 702e  lue = Tensor(np.
+0001fc40: 6f6e 6573 2828 322c 2032 302c 2036 3429  ones((2, 20, 64)
+0001fc50: 292c 206d 7374 7970 652e 666c 6f61 7433  ), mstype.float3
+0001fc60: 3229 0a20 2020 2020 2020 2020 2020 203e  2).            >
+0001fc70: 3e3e 2064 6563 6f64 6572 5f69 6e70 7574  >> decoder_input
+0001fc80: 5f76 616c 7565 203d 2054 656e 736f 7228  _value = Tensor(
+0001fc90: 6e70 2e6f 6e65 7328 2832 2c20 3130 2c20  np.ones((2, 10, 
+0001fca0: 3634 2929 2c20 6d73 7479 7065 2e66 6c6f  64)), mstype.flo
+0001fcb0: 6174 3332 290a 2020 2020 2020 2020 2020  at32).          
+0001fcc0: 2020 3e3e 3e20 6465 636f 6465 725f 696e    >>> decoder_in
+0001fcd0: 7075 745f 6d61 736b 203d 2054 656e 736f  put_mask = Tenso
+0001fce0: 7228 6e70 2e6f 6e65 7328 2832 2c20 3130  r(np.ones((2, 10
+0001fcf0: 2c20 3130 2929 2c20 6d73 7479 7065 2e66  , 10)), mstype.f
+0001fd00: 6c6f 6174 3136 290a 2020 2020 2020 2020  loat16).        
+0001fd10: 2020 2020 3e3e 3e20 6d65 6d6f 7279 5f6d      >>> memory_m
+0001fd20: 6173 6b20 3d20 5465 6e73 6f72 286e 702e  ask = Tensor(np.
+0001fd30: 6f6e 6573 2828 322c 2031 302c 2032 3029  ones((2, 10, 20)
+0001fd40: 292c 206d 7374 7970 652e 666c 6f61 7431  ), mstype.float1
+0001fd50: 3629 0a20 2020 2020 2020 2020 2020 203e  6).            >
+0001fd60: 3e3e 206f 7574 7075 742c 2070 6173 7420  >> output, past 
+0001fd70: 3d20 6d6f 6465 6c28 6465 636f 6465 725f  = model(decoder_
+0001fd80: 696e 7075 745f 7661 6c75 652c 2064 6563  input_value, dec
+0001fd90: 6f64 6572 5f69 6e70 7574 5f6d 6173 6b2c  oder_input_mask,
+0001fda0: 2065 6e63 6f64 6572 5f69 6e70 7574 5f76   encoder_input_v
+0001fdb0: 616c 7565 2c20 6d65 6d6f 7279 5f6d 6173  alue, memory_mas
+0001fdc0: 6b29 0a20 2020 2020 2020 2020 2020 203e  k).            >
+0001fdd0: 3e3e 2070 7269 6e74 286f 7574 7075 742e  >> print(output.
+0001fde0: 7368 6170 6529 0a20 2020 2020 2020 2020  shape).         
+0001fdf0: 2020 2028 322c 2031 302c 2036 3429 0a20     (2, 10, 64). 
+0001fe00: 2020 2020 2020 2020 2020 203e 3e3e 2070             >>> p
+0001fe10: 7269 6e74 2870 6173 745b 305d 2e73 6861  rint(past[0].sha
+0001fe20: 7065 290a 2020 2020 2020 2020 2020 2020  pe).            
+0001fe30: 2832 2c20 322c 2033 322c 2031 3029 0a20  (2, 2, 32, 10). 
+0001fe40: 2020 2020 2020 2020 2020 203e 3e3e 2070             >>> p
+0001fe50: 7269 6e74 2870 6173 745b 315d 2e73 6861  rint(past[1].sha
+0001fe60: 7065 290a 2020 2020 2020 2020 2020 2020  pe).            
+0001fe70: 2832 2c20 322c 2031 302c 2033 3229 0a20  (2, 2, 10, 32). 
+0001fe80: 2020 2020 2020 2020 2020 203e 3e3e 2070             >>> p
+0001fe90: 7269 6e74 2870 6173 745b 325d 2e73 6861  rint(past[2].sha
+0001fea0: 7065 290a 2020 2020 2020 2020 2020 2020  pe).            
+0001feb0: 2832 2c20 322c 2033 322c 2032 3029 0a20  (2, 2, 32, 20). 
+0001fec0: 2020 2020 2020 2020 2020 203e 3e3e 2070             >>> p
+0001fed0: 7269 6e74 2870 6173 745b 335d 2e73 6861  rint(past[3].sha
+0001fee0: 7065 290a 2020 2020 2020 2020 2020 2020  pe).            
+0001fef0: 2832 2c20 322c 2032 302c 2033 3229 0a20  (2, 2, 20, 32). 
+0001ff00: 2020 2022 2222 0a0a 2020 2020 405f 4c6f     """..    @_Lo
+0001ff10: 6741 6374 696f 6e4f 6e63 6528 6d5f 6c6f  gActionOnce(m_lo
+0001ff20: 6767 6572 3d6c 6f67 6765 722c 206b 6579  gger=logger, key
+0001ff30: 3d27 5472 616e 7366 6f72 6d65 7244 6563  ='TransformerDec
+0001ff40: 6f64 6572 4c61 7965 7227 2c0a 2020 2020  oderLayer',.    
+0001ff50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001ff60: 6e6f 5f77 6172 6e69 6e67 3d5f 6765 745f  no_warning=_get_
+0001ff70: 7061 7261 6c6c 656c 5f6d 6f64 6528 2920  parallel_mode() 
+0001ff80: 696e 2028 5061 7261 6c6c 656c 4d6f 6465  in (ParallelMode
+0001ff90: 2e53 5441 4e44 5f41 4c4f 4e45 2c29 290a  .STAND_ALONE,)).
+0001ffa0: 2020 2020 405f 6172 6773 5f74 7970 655f      @_args_type_
+0001ffb0: 7661 6c69 6461 746f 725f 6368 6563 6b28  validator_check(
+0001ffc0: 6869 6464 656e 5f73 697a 653d 5661 6c69  hidden_size=Vali
+0001ffd0: 6461 746f 722e 6368 6563 6b5f 706f 7369  dator.check_posi
+0001ffe0: 7469 7665 5f69 6e74 2c0a 2020 2020 2020  tive_int,.      
+0001fff0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020000: 2020 2020 2020 2020 2020 6e75 6d5f 6865            num_he
+00020010: 6164 733d 5661 6c69 6461 746f 722e 6368  ads=Validator.ch
+00020020: 6563 6b5f 706f 7369 7469 7665 5f69 6e74  eck_positive_int
+00020030: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00020040: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020050: 2020 6666 6e5f 6869 6464 656e 5f73 697a    ffn_hidden_siz
+00020060: 653d 5661 6c69 6461 746f 722e 6368 6563  e=Validator.chec
+00020070: 6b5f 706f 7369 7469 7665 5f69 6e74 2c0a  k_positive_int,.
+00020080: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020090: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000200a0: 7372 635f 7365 715f 6c65 6e67 7468 3d56  src_seq_length=V
+000200b0: 616c 6964 6174 6f72 2e63 6865 636b 5f70  alidator.check_p
+000200c0: 6f73 6974 6976 655f 696e 742c 0a20 2020  ositive_int,.   
+000200d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000200e0: 2020 2020 2020 2020 2020 2020 2074 6774               tgt
+000200f0: 5f73 6571 5f6c 656e 6774 683d 5661 6c69  _seq_length=Vali
+00020100: 6461 746f 722e 6368 6563 6b5f 706f 7369  dator.check_posi
+00020110: 7469 7665 5f69 6e74 2c0a 2020 2020 2020  tive_int,.      
 00020120: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020130: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020140: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020150: 2020 2020 2254 7261 6e73 666f 726d 6572      "Transformer
-00020160: 4465 636f 6465 724c 6179 6572 2229 2c0a  DecoderLayer"),.
-00020170: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020130: 2020 2020 2020 2020 2020 6174 7465 6e74            attent
+00020140: 696f 6e5f 6472 6f70 6f75 745f 7261 7465  ion_dropout_rate
+00020150: 3d56 616c 6964 6174 6f72 2e63 6865 636b  =Validator.check
+00020160: 5f6e 6f6e 5f6e 6567 6174 6976 655f 666c  _non_negative_fl
+00020170: 6f61 742c 0a20 2020 2020 2020 2020 2020  oat,.           
 00020180: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020190: 7573 655f 7061 7374 3d56 616c 6964 6174  use_past=Validat
-000201a0: 6f72 2e63 6865 636b 5f62 6f6f 6c29 0a20  or.check_bool). 
-000201b0: 2020 2064 6566 205f 5f69 6e69 745f 5f28     def __init__(
-000201c0: 7365 6c66 2c20 6869 6464 656e 5f73 697a  self, hidden_siz
-000201d0: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
-000201e0: 2020 2020 6666 6e5f 6869 6464 656e 5f73      ffn_hidden_s
-000201f0: 697a 652c 0a20 2020 2020 2020 2020 2020  ize,.           
-00020200: 2020 2020 2020 6e75 6d5f 6865 6164 732c        num_heads,
-00020210: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00020220: 2020 6261 7463 685f 7369 7a65 2c0a 2020    batch_size,.  
-00020230: 2020 2020 2020 2020 2020 2020 2020 2073                 s
-00020240: 7263 5f73 6571 5f6c 656e 6774 682c 0a20  rc_seq_length,. 
-00020250: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020260: 7467 745f 7365 715f 6c65 6e67 7468 2c0a  tgt_seq_length,.
-00020270: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020280: 2061 7474 656e 7469 6f6e 5f64 726f 706f   attention_dropo
-00020290: 7574 5f72 6174 653d 302e 312c 0a20 2020  ut_rate=0.1,.   
-000202a0: 2020 2020 2020 2020 2020 2020 2020 6869                hi
-000202b0: 6464 656e 5f64 726f 706f 7574 5f72 6174  dden_dropout_rat
-000202c0: 653d 302e 312c 0a20 2020 2020 2020 2020  e=0.1,.         
-000202d0: 2020 2020 2020 2020 706f 7374 5f6c 6179          post_lay
-000202e0: 6572 6e6f 726d 5f72 6573 6964 7561 6c3d  ernorm_residual=
-000202f0: 4661 6c73 652c 0a20 2020 2020 2020 2020  False,.         
-00020300: 2020 2020 2020 2020 7573 655f 7061 7374          use_past
-00020310: 3d46 616c 7365 2c0a 2020 2020 2020 2020  =False,.        
-00020320: 2020 2020 2020 2020 206c 6179 6572 6e6f           layerno
-00020330: 726d 5f63 6f6d 7075 7465 5f74 7970 653d  rm_compute_type=
-00020340: 6d73 7479 7065 2e66 6c6f 6174 3332 2c0a  mstype.float32,.
+00020190: 2020 2020 2068 6964 6465 6e5f 6472 6f70       hidden_drop
+000201a0: 6f75 745f 7261 7465 3d56 616c 6964 6174  out_rate=Validat
+000201b0: 6f72 2e63 6865 636b 5f6e 6f6e 5f6e 6567  or.check_non_neg
+000201c0: 6174 6976 655f 666c 6f61 742c 0a20 2020  ative_float,.   
+000201d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000201e0: 2020 2020 2020 2020 2020 2020 2070 6f73               pos
+000201f0: 745f 6c61 7965 726e 6f72 6d5f 7265 7369  t_layernorm_resi
+00020200: 6475 616c 3d56 616c 6964 6174 6f72 2e63  dual=Validator.c
+00020210: 6865 636b 5f62 6f6f 6c2c 0a20 2020 2020  heck_bool,.     
+00020220: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020230: 2020 2020 2020 2020 2020 206c 6179 6572             layer
+00020240: 6e6f 726d 5f63 6f6d 7075 7465 5f74 7970  norm_compute_typ
+00020250: 653d 5f76 616c 6964 5f76 616c 7565 5f63  e=_valid_value_c
+00020260: 6865 636b 7328 5b6d 7374 7970 652e 666c  hecks([mstype.fl
+00020270: 6f61 7433 322c 0a20 2020 2020 2020 2020  oat32,.         
+00020280: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020290: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000202a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000202b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000202c0: 2020 206d 7374 7970 652e 666c 6f61 7431     mstype.float1
+000202d0: 362c 206d 7374 7970 652e 6266 6c6f 6174  6, mstype.bfloat
+000202e0: 3136 5d2c 0a20 2020 2020 2020 2020 2020  16],.           
+000202f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020300: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020310: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020320: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020330: 2254 7261 6e73 666f 726d 6572 4465 636f  "TransformerDeco
+00020340: 6465 724c 6179 6572 2229 2c0a 2020 2020  derLayer"),.    
 00020350: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020360: 2073 6f66 746d 6178 5f63 6f6d 7075 7465   softmax_compute
-00020370: 5f74 7970 653d 6d73 7479 7065 2e66 6c6f  _type=mstype.flo
-00020380: 6174 3332 2c0a 2020 2020 2020 2020 2020  at32,.          
-00020390: 2020 2020 2020 2070 6172 616d 5f69 6e69         param_ini
-000203a0: 745f 7479 7065 3d6d 7374 7970 652e 666c  t_type=mstype.fl
-000203b0: 6f61 7433 322c 0a20 2020 2020 2020 2020  oat32,.         
-000203c0: 2020 2020 2020 2020 6869 6464 656e 5f61          hidden_a
-000203d0: 6374 3d27 6765 6c75 272c 0a20 2020 2020  ct='gelu',.     
-000203e0: 2020 2020 2020 2020 2020 2020 6d6f 655f              moe_
-000203f0: 636f 6e66 6967 3d64 6566 6175 6c74 5f6d  config=default_m
-00020400: 6f65 5f63 6f6e 6669 672c 0a20 2020 2020  oe_config,.     
-00020410: 2020 2020 2020 2020 2020 2020 7061 7261              para
-00020420: 6c6c 656c 5f63 6f6e 6669 673d 6465 6661  llel_config=defa
-00020430: 756c 745f 6470 6d70 5f63 6f6e 6669 6729  ult_dpmp_config)
-00020440: 3a0a 2020 2020 2020 2020 7375 7065 7228  :.        super(
-00020450: 5472 616e 7366 6f72 6d65 7244 6563 6f64  TransformerDecod
-00020460: 6572 4c61 7965 722c 2073 656c 6629 2e5f  erLayer, self)._
-00020470: 5f69 6e69 745f 5f28 290a 2020 2020 2020  _init__().      
-00020480: 2020 5f63 6865 636b 5f6d 6f65 5f63 6f6e    _check_moe_con
-00020490: 6669 6728 6d6f 655f 636f 6e66 6967 2c20  fig(moe_config, 
-000204a0: 7061 7261 6c6c 656c 5f63 6f6e 6669 6729  parallel_config)
-000204b0: 0a20 2020 2020 2020 2073 656c 662e 7573  .        self.us
-000204c0: 655f 6d6f 6520 3d20 286d 6f65 5f63 6f6e  e_moe = (moe_con
-000204d0: 6669 672e 6578 7065 7274 5f6e 756d 203e  fig.expert_num >
-000204e0: 2031 290a 2020 2020 2020 2020 636f 6e66   1).        conf
-000204f0: 6967 5f74 6f5f 6174 7465 6e74 696f 6e20  ig_to_attention 
-00020500: 3d20 7061 7261 6c6c 656c 5f63 6f6e 6669  = parallel_confi
-00020510: 672e 6470 6d70 2069 6620 7365 6c66 2e75  g.dpmp if self.u
-00020520: 7365 5f6d 6f65 2065 6c73 6520 7061 7261  se_moe else para
-00020530: 6c6c 656c 5f63 6f6e 6669 670a 2020 2020  llel_config.    
-00020540: 2020 2020 6966 2062 6174 6368 5f73 697a      if batch_siz
-00020550: 6520 6f72 2075 7365 5f70 6173 743a 0a20  e or use_past:. 
-00020560: 2020 2020 2020 2020 2020 2056 616c 6964             Valid
-00020570: 6174 6f72 2e63 6865 636b 5f70 6f73 6974  ator.check_posit
-00020580: 6976 655f 696e 7428 6261 7463 685f 7369  ive_int(batch_si
-00020590: 7a65 290a 2020 2020 2020 2020 6966 205f  ze).        if _
-000205a0: 6765 745f 7061 7261 6c6c 656c 5f6d 6f64  get_parallel_mod
-000205b0: 6528 2920 696e 2028 5061 7261 6c6c 656c  e() in (Parallel
-000205c0: 4d6f 6465 2e41 5554 4f5f 5041 5241 4c4c  Mode.AUTO_PARALL
-000205d0: 454c 2c29 3a0a 2020 2020 2020 2020 2020  EL,):.          
-000205e0: 2020 5f63 6865 636b 5f63 6f6e 6669 6728    _check_config(
-000205f0: 7061 7261 6c6c 656c 5f63 6f6e 6669 6729  parallel_config)
-00020600: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if 
-00020610: 6e75 6d5f 6865 6164 7320 2520 7061 7261  num_heads % para
-00020620: 6c6c 656c 5f63 6f6e 6669 672e 6d6f 6465  llel_config.mode
-00020630: 6c5f 7061 7261 6c6c 656c 2021 3d20 303a  l_parallel != 0:
-00020640: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00020650: 2072 6169 7365 2056 616c 7565 4572 726f   raise ValueErro
-00020660: 7228 2246 6f72 2027 5472 616e 7366 6f72  r("For 'Transfor
-00020670: 6d65 7244 6563 6f64 6572 4c61 7965 7227  merDecoderLayer'
-00020680: 2c20 7468 6520 636c 6173 7320 7661 7269  , the class vari
-00020690: 6162 6c65 2027 6e75 6d5f 6865 6164 7327  able 'num_heads'
-000206a0: 206d 7573 7420 6265 2064 6976 6973 6962   must be divisib
-000206b0: 6c65 6420 6279 2022 0a20 2020 2020 2020  led by ".       
-000206c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000206d0: 2020 2020 2020 2020 2020 2227 7061 7261            "'para
-000206e0: 6c6c 656c 5f63 6f6e 6669 672e 6d6f 6465  llel_config.mode
-000206f0: 6c5f 7061 7261 6c6c 656c 272c 2062 7574  l_parallel', but
-00020700: 2067 6f74 2074 6865 206e 756d 5f68 6561   got the num_hea
-00020710: 6473 2069 7320 7b7d 2061 6e64 2022 0a20  ds is {} and ". 
-00020720: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020730: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020740: 2270 6172 616c 6c65 6c5f 636f 6e66 6967  "parallel_config
-00020750: 2e6d 6f64 656c 5f70 6172 616c 6c65 6c20  .model_parallel 
-00020760: 6973 207b 7d2e 222e 666f 726d 6174 286e  is {}.".format(n
-00020770: 756d 5f68 6561 6473 2c0a 2020 2020 2020  um_heads,.      
-00020780: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020790: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000207a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000207b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000207c0: 2020 2020 2020 2020 2020 7061 7261 6c6c            parall
-000207d0: 656c 5f63 6f6e 6669 672e 6d6f 6465 6c5f  el_config.model_
-000207e0: 7061 7261 6c6c 656c 2929 0a20 2020 2020  parallel)).     
-000207f0: 2020 2020 2020 2069 6620 6869 6464 656e         if hidden
-00020800: 5f73 697a 6520 2520 7061 7261 6c6c 656c  _size % parallel
-00020810: 5f63 6f6e 6669 672e 6d6f 6465 6c5f 7061  _config.model_pa
-00020820: 7261 6c6c 656c 2021 3d20 303a 0a20 2020  rallel != 0:.   
-00020830: 2020 2020 2020 2020 2020 2020 2072 6169               rai
-00020840: 7365 2056 616c 7565 4572 726f 7228 0a20  se ValueError(. 
-00020850: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020860: 2020 2022 466f 7220 2754 7261 6e73 666f     "For 'Transfo
-00020870: 726d 6572 4465 636f 6465 724c 6179 6572  rmerDecoderLayer
-00020880: 272c 2074 6865 2063 6c61 7373 2076 6172  ', the class var
-00020890: 6961 626c 6520 2768 6964 6465 6e5f 7369  iable 'hidden_si
-000208a0: 7a65 2720 6d75 7374 2062 6520 6469 7669  ze' must be divi
-000208b0: 7369 626c 6564 2062 7920 220a 2020 2020  sibled by ".    
-000208c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000208d0: 2227 7061 7261 6c6c 656c 5f63 6f6e 6669  "'parallel_confi
-000208e0: 672e 6d6f 6465 6c5f 7061 7261 6c6c 656c  g.model_parallel
-000208f0: 272c 2062 7574 2067 6f74 2074 6865 2068  ', but got the h
-00020900: 6964 6465 6e5f 7369 7a65 2069 7320 7b7d  idden_size is {}
-00020910: 2061 6e64 2022 0a20 2020 2020 2020 2020   and ".         
-00020920: 2020 2020 2020 2020 2020 2022 7061 7261             "para
-00020930: 6c6c 656c 5f63 6f6e 6669 672e 6d6f 6465  llel_config.mode
-00020940: 6c5f 7061 7261 6c6c 656c 2069 7320 7b7d  l_parallel is {}
-00020950: 2e22 0a20 2020 2020 2020 2020 2020 2020  .".             
-00020960: 2020 2020 2020 202e 666f 726d 6174 2868         .format(h
-00020970: 6964 6465 6e5f 7369 7a65 2c20 7061 7261  idden_size, para
-00020980: 6c6c 656c 5f63 6f6e 6669 672e 6d6f 6465  llel_config.mode
-00020990: 6c5f 7061 7261 6c6c 656c 2929 0a20 2020  l_parallel)).   
-000209a0: 2020 2020 2020 2020 2069 6620 6666 6e5f           if ffn_
-000209b0: 6869 6464 656e 5f73 697a 6520 2520 7061  hidden_size % pa
-000209c0: 7261 6c6c 656c 5f63 6f6e 6669 672e 6d6f  rallel_config.mo
-000209d0: 6465 6c5f 7061 7261 6c6c 656c 2021 3d20  del_parallel != 
-000209e0: 303a 0a20 2020 2020 2020 2020 2020 2020  0:.             
-000209f0: 2020 2072 6169 7365 2056 616c 7565 4572     raise ValueEr
-00020a00: 726f 7228 2246 6f72 2027 5472 616e 7366  ror("For 'Transf
-00020a10: 6f72 6d65 7244 6563 6f64 6572 4c61 7965  ormerDecoderLaye
-00020a20: 7227 2c20 7468 6520 636c 6173 7320 7661  r', the class va
-00020a30: 7269 6162 6c65 2027 6666 6e5f 6869 6464  riable 'ffn_hidd
-00020a40: 656e 5f73 697a 6527 206d 7573 7420 6265  en_size' must be
-00020a50: 2022 0a20 2020 2020 2020 2020 2020 2020   ".             
-00020a60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020a70: 2020 2020 2264 6976 6973 6962 6c65 6420      "divisibled 
-00020a80: 6279 2027 7061 7261 6c6c 656c 5f63 6f6e  by 'parallel_con
-00020a90: 6669 672e 6d6f 6465 6c5f 7061 7261 6c6c  fig.model_parall
-00020aa0: 656c 272c 2062 7574 2067 6f74 2074 6865  el', but got the
-00020ab0: 2066 666e 5f68 6964 6465 6e5f 7369 7a65   ffn_hidden_size
-00020ac0: 2069 7320 7b7d 2022 0a20 2020 2020 2020   is {} ".       
-00020ad0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020ae0: 2020 2020 2020 2020 2020 2261 6e64 2070            "and p
-00020af0: 6172 616c 6c65 6c5f 636f 6e66 6967 2e6d  arallel_config.m
-00020b00: 6f64 656c 5f70 6172 616c 6c65 6c20 6973  odel_parallel is
-00020b10: 207b 7d2e 220a 2020 2020 2020 2020 2020   {}.".          
-00020b20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020b30: 2020 2020 2020 202e 666f 726d 6174 2866         .format(f
-00020b40: 666e 5f68 6964 6465 6e5f 7369 7a65 2c20  fn_hidden_size, 
-00020b50: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
-00020b60: 6d6f 6465 6c5f 7061 7261 6c6c 656c 2929  model_parallel))
-00020b70: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if 
-00020b80: 7573 655f 7061 7374 3a0a 2020 2020 2020  use_past:.      
-00020b90: 2020 2020 2020 2020 2020 7261 6973 6520            raise 
-00020ba0: 5661 6c75 6545 7272 6f72 2866 2254 6865  ValueError(f"The
-00020bb0: 207b 7365 6c66 2e63 6c73 5f6e 616d 657d   {self.cls_name}
-00020bc0: 2064 6f65 7320 6e6f 7420 7375 7070 6f72   does not suppor
-00020bd0: 7420 7573 655f 7061 7374 3d54 7275 652e  t use_past=True.
-00020be0: 2229 0a20 2020 2020 2020 2020 2020 2073  ").            s
-00020bf0: 656c 662e 6261 7463 685f 7369 7a65 203d  elf.batch_size =
-00020c00: 2062 6174 6368 5f73 697a 650a 2020 2020   batch_size.    
-00020c10: 2020 2020 2020 2020 7365 6c66 2e75 7365          self.use
-00020c20: 5f70 6173 7420 3d20 7573 655f 7061 7374  _past = use_past
-00020c30: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-00020c40: 662e 736f 6674 6d61 785f 636f 6d70 7574  f.softmax_comput
-00020c50: 655f 7479 7065 203d 2073 6f66 746d 6178  e_type = softmax
-00020c60: 5f63 6f6d 7075 7465 5f74 7970 650a 0a20  _compute_type.. 
-00020c70: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-00020c80: 7372 635f 7365 715f 6c65 6e67 7468 203d  src_seq_length =
-00020c90: 2073 7263 5f73 6571 5f6c 656e 6774 680a   src_seq_length.
-00020ca0: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-00020cb0: 2e74 6774 5f73 6571 5f6c 656e 6774 6820  .tgt_seq_length 
-00020cc0: 3d20 7467 745f 7365 715f 6c65 6e67 7468  = tgt_seq_length
-00020cd0: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-00020ce0: 662e 7573 655f 7061 7374 203d 2075 7365  f.use_past = use
-00020cf0: 5f70 6173 740a 2020 2020 2020 2020 2020  _past.          
-00020d00: 2020 7365 6c66 2e68 6964 6465 6e5f 7369    self.hidden_si
-00020d10: 7a65 203d 2068 6964 6465 6e5f 7369 7a65  ze = hidden_size
-00020d20: 0a0a 2020 2020 2020 2020 2020 2020 7365  ..            se
-00020d30: 6c66 2e6c 6179 6572 6e6f 726d 3120 3d20  lf.layernorm1 = 
-00020d40: 4c61 7965 724e 6f72 6d28 2868 6964 6465  LayerNorm((hidde
-00020d50: 6e5f 7369 7a65 2c29 292e 746f 5f66 6c6f  n_size,)).to_flo
-00020d60: 6174 286c 6179 6572 6e6f 726d 5f63 6f6d  at(layernorm_com
-00020d70: 7075 7465 5f74 7970 6529 0a20 2020 2020  pute_type).     
-00020d80: 2020 2020 2020 2073 656c 662e 6c61 7965         self.laye
-00020d90: 726e 6f72 6d32 203d 204c 6179 6572 4e6f  rnorm2 = LayerNo
-00020da0: 726d 2828 6869 6464 656e 5f73 697a 652c  rm((hidden_size,
-00020db0: 2929 2e74 6f5f 666c 6f61 7428 6c61 7965  )).to_float(laye
-00020dc0: 726e 6f72 6d5f 636f 6d70 7574 655f 7479  rnorm_compute_ty
-00020dd0: 7065 290a 2020 2020 2020 2020 2020 2020  pe).            
-00020de0: 7365 6c66 2e61 7474 656e 7469 6f6e 203d  self.attention =
-00020df0: 204d 756c 7469 4865 6164 4174 7465 6e74   MultiHeadAttent
-00020e00: 696f 6e28 6869 6464 656e 5f73 697a 653d  ion(hidden_size=
-00020e10: 6869 6464 656e 5f73 697a 652c 0a20 2020  hidden_size,.   
-00020e20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020e30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020e40: 2020 2020 2020 2020 2020 2020 206e 756d               num
-00020e50: 5f68 6561 6473 3d6e 756d 5f68 6561 6473  _heads=num_heads
-00020e60: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00020e70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020e80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020e90: 2020 6261 7463 685f 7369 7a65 3d62 6174    batch_size=bat
-00020ea0: 6368 5f73 697a 652c 0a20 2020 2020 2020  ch_size,.       
-00020eb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020ec0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020ed0: 2020 2020 2020 2020 2073 7263 5f73 6571           src_seq
-00020ee0: 5f6c 656e 6774 683d 7467 745f 7365 715f  _length=tgt_seq_
-00020ef0: 6c65 6e67 7468 2c0a 2020 2020 2020 2020  length,.        
+00020360: 2020 2020 2020 2020 2020 2020 736f 6674              soft
+00020370: 6d61 785f 636f 6d70 7574 655f 7479 7065  max_compute_type
+00020380: 3d5f 7661 6c69 645f 7661 6c75 655f 6368  =_valid_value_ch
+00020390: 6563 6b73 285b 6d73 7479 7065 2e66 6c6f  ecks([mstype.flo
+000203a0: 6174 3332 2c0a 2020 2020 2020 2020 2020  at32,.          
+000203b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000203c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000203d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000203e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000203f0: 6d73 7479 7065 2e66 6c6f 6174 3136 2c20  mstype.float16, 
+00020400: 6d73 7479 7065 2e62 666c 6f61 7431 365d  mstype.bfloat16]
+00020410: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00020420: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020430: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020440: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020450: 2020 2020 2020 2020 2020 2022 5472 616e             "Tran
+00020460: 7366 6f72 6d65 7244 6563 6f64 6572 4c61  sformerDecoderLa
+00020470: 7965 7222 292c 0a20 2020 2020 2020 2020  yer"),.         
+00020480: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020490: 2020 2020 2020 2070 6172 616d 5f69 6e69         param_ini
+000204a0: 745f 7479 7065 3d5f 7661 6c69 645f 7661  t_type=_valid_va
+000204b0: 6c75 655f 6368 6563 6b73 285b 6d73 7479  lue_checks([msty
+000204c0: 7065 2e66 6c6f 6174 3332 2c20 6d73 7479  pe.float32, msty
+000204d0: 7065 2e66 6c6f 6174 3136 2c20 6d73 7479  pe.float16, msty
+000204e0: 7065 2e62 666c 6f61 7431 365d 2c0a 2020  pe.bfloat16],.  
+000204f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020500: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020510: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020520: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020530: 2020 2254 7261 6e73 666f 726d 6572 4465    "TransformerDe
+00020540: 636f 6465 724c 6179 6572 2229 2c0a 2020  coderLayer"),.  
+00020550: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020560: 2020 2020 2020 2020 2020 2020 2020 7061                pa
+00020570: 7261 6c6c 656c 5f63 6f6e 6669 673d 5f76  rallel_config=_v
+00020580: 616c 6964 5f74 7970 655f 6368 6563 6b73  alid_type_checks
+00020590: 285b 4f70 5061 7261 6c6c 656c 436f 6e66  ([OpParallelConf
+000205a0: 6967 2c20 4d6f 4550 6172 616c 6c65 6c43  ig, MoEParallelC
+000205b0: 6f6e 6669 675d 2c0a 2020 2020 2020 2020  onfig],.        
+000205c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000205d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000205e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000205f0: 2020 2020 2020 2020 2020 2022 5472 616e             "Tran
+00020600: 7366 6f72 6d65 7244 6563 6f64 6572 4c61  sformerDecoderLa
+00020610: 7965 7222 292c 0a20 2020 2020 2020 2020  yer"),.         
+00020620: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020630: 2020 2020 2020 2075 7365 5f70 6173 743d         use_past=
+00020640: 5661 6c69 6461 746f 722e 6368 6563 6b5f  Validator.check_
+00020650: 626f 6f6c 290a 2020 2020 6465 6620 5f5f  bool).    def __
+00020660: 696e 6974 5f5f 2873 656c 662c 2068 6964  init__(self, hid
+00020670: 6465 6e5f 7369 7a65 2c0a 2020 2020 2020  den_size,.      
+00020680: 2020 2020 2020 2020 2020 2066 666e 5f68             ffn_h
+00020690: 6964 6465 6e5f 7369 7a65 2c0a 2020 2020  idden_size,.    
+000206a0: 2020 2020 2020 2020 2020 2020 206e 756d               num
+000206b0: 5f68 6561 6473 2c0a 2020 2020 2020 2020  _heads,.        
+000206c0: 2020 2020 2020 2020 2062 6174 6368 5f73           batch_s
+000206d0: 697a 652c 0a20 2020 2020 2020 2020 2020  ize,.           
+000206e0: 2020 2020 2020 7372 635f 7365 715f 6c65        src_seq_le
+000206f0: 6e67 7468 2c0a 2020 2020 2020 2020 2020  ngth,.          
+00020700: 2020 2020 2020 2074 6774 5f73 6571 5f6c         tgt_seq_l
+00020710: 656e 6774 682c 0a20 2020 2020 2020 2020  ength,.         
+00020720: 2020 2020 2020 2020 6174 7465 6e74 696f          attentio
+00020730: 6e5f 6472 6f70 6f75 745f 7261 7465 3d30  n_dropout_rate=0
+00020740: 2e31 2c0a 2020 2020 2020 2020 2020 2020  .1,.            
+00020750: 2020 2020 2068 6964 6465 6e5f 6472 6f70       hidden_drop
+00020760: 6f75 745f 7261 7465 3d30 2e31 2c0a 2020  out_rate=0.1,.  
+00020770: 2020 2020 2020 2020 2020 2020 2020 2070                 p
+00020780: 6f73 745f 6c61 7965 726e 6f72 6d5f 7265  ost_layernorm_re
+00020790: 7369 6475 616c 3d46 616c 7365 2c0a 2020  sidual=False,.  
+000207a0: 2020 2020 2020 2020 2020 2020 2020 2075                 u
+000207b0: 7365 5f70 6173 743d 4661 6c73 652c 0a20  se_past=False,. 
+000207c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000207d0: 6c61 7965 726e 6f72 6d5f 636f 6d70 7574  layernorm_comput
+000207e0: 655f 7479 7065 3d6d 7374 7970 652e 666c  e_type=mstype.fl
+000207f0: 6f61 7433 322c 0a20 2020 2020 2020 2020  oat32,.         
+00020800: 2020 2020 2020 2020 736f 6674 6d61 785f          softmax_
+00020810: 636f 6d70 7574 655f 7479 7065 3d6d 7374  compute_type=mst
+00020820: 7970 652e 666c 6f61 7433 322c 0a20 2020  ype.float32,.   
+00020830: 2020 2020 2020 2020 2020 2020 2020 7061                pa
+00020840: 7261 6d5f 696e 6974 5f74 7970 653d 6d73  ram_init_type=ms
+00020850: 7479 7065 2e66 6c6f 6174 3332 2c0a 2020  type.float32,.  
+00020860: 2020 2020 2020 2020 2020 2020 2020 2068                 h
+00020870: 6964 6465 6e5f 6163 743d 2767 656c 7527  idden_act='gelu'
+00020880: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00020890: 2020 206d 6f65 5f63 6f6e 6669 673d 6465     moe_config=de
+000208a0: 6661 756c 745f 6d6f 655f 636f 6e66 6967  fault_moe_config
+000208b0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+000208c0: 2020 2070 6172 616c 6c65 6c5f 636f 6e66     parallel_conf
+000208d0: 6967 3d64 6566 6175 6c74 5f64 706d 705f  ig=default_dpmp_
+000208e0: 636f 6e66 6967 293a 0a20 2020 2020 2020  config):.       
+000208f0: 2073 7570 6572 2854 7261 6e73 666f 726d   super(Transform
+00020900: 6572 4465 636f 6465 724c 6179 6572 2c20  erDecoderLayer, 
+00020910: 7365 6c66 292e 5f5f 696e 6974 5f5f 2829  self).__init__()
+00020920: 0a20 2020 2020 2020 205f 6368 6563 6b5f  .        _check_
+00020930: 6d6f 655f 636f 6e66 6967 286d 6f65 5f63  moe_config(moe_c
+00020940: 6f6e 6669 672c 2070 6172 616c 6c65 6c5f  onfig, parallel_
+00020950: 636f 6e66 6967 290a 2020 2020 2020 2020  config).        
+00020960: 7365 6c66 2e75 7365 5f6d 6f65 203d 2028  self.use_moe = (
+00020970: 6d6f 655f 636f 6e66 6967 2e65 7870 6572  moe_config.exper
+00020980: 745f 6e75 6d20 3e20 3129 0a20 2020 2020  t_num > 1).     
+00020990: 2020 2063 6f6e 6669 675f 746f 5f61 7474     config_to_att
+000209a0: 656e 7469 6f6e 203d 2070 6172 616c 6c65  ention = paralle
+000209b0: 6c5f 636f 6e66 6967 2e64 706d 7020 6966  l_config.dpmp if
+000209c0: 2073 656c 662e 7573 655f 6d6f 6520 656c   self.use_moe el
+000209d0: 7365 2070 6172 616c 6c65 6c5f 636f 6e66  se parallel_conf
+000209e0: 6967 0a20 2020 2020 2020 2069 6620 6261  ig.        if ba
+000209f0: 7463 685f 7369 7a65 206f 7220 7573 655f  tch_size or use_
+00020a00: 7061 7374 3a0a 2020 2020 2020 2020 2020  past:.          
+00020a10: 2020 5661 6c69 6461 746f 722e 6368 6563    Validator.chec
+00020a20: 6b5f 706f 7369 7469 7665 5f69 6e74 2862  k_positive_int(b
+00020a30: 6174 6368 5f73 697a 6529 0a20 2020 2020  atch_size).     
+00020a40: 2020 2069 6620 5f67 6574 5f70 6172 616c     if _get_paral
+00020a50: 6c65 6c5f 6d6f 6465 2829 2069 6e20 2850  lel_mode() in (P
+00020a60: 6172 616c 6c65 6c4d 6f64 652e 4155 544f  arallelMode.AUTO
+00020a70: 5f50 4152 414c 4c45 4c2c 293a 0a20 2020  _PARALLEL,):.   
+00020a80: 2020 2020 2020 2020 205f 6368 6563 6b5f           _check_
+00020a90: 636f 6e66 6967 2870 6172 616c 6c65 6c5f  config(parallel_
+00020aa0: 636f 6e66 6967 290a 2020 2020 2020 2020  config).        
+00020ab0: 2020 2020 6966 206e 756d 5f68 6561 6473      if num_heads
+00020ac0: 2025 2070 6172 616c 6c65 6c5f 636f 6e66   % parallel_conf
+00020ad0: 6967 2e6d 6f64 656c 5f70 6172 616c 6c65  ig.model_paralle
+00020ae0: 6c20 213d 2030 3a0a 2020 2020 2020 2020  l != 0:.        
+00020af0: 2020 2020 2020 2020 7261 6973 6520 5661          raise Va
+00020b00: 6c75 6545 7272 6f72 2822 466f 7220 2754  lueError("For 'T
+00020b10: 7261 6e73 666f 726d 6572 4465 636f 6465  ransformerDecode
+00020b20: 724c 6179 6572 272c 2074 6865 2063 6c61  rLayer', the cla
+00020b30: 7373 2076 6172 6961 626c 6520 276e 756d  ss variable 'num
+00020b40: 5f68 6561 6473 2720 6d75 7374 2062 6520  _heads' must be 
+00020b50: 6469 7669 7369 626c 6564 2062 7920 220a  divisibled by ".
+00020b60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020b70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020b80: 2022 2770 6172 616c 6c65 6c5f 636f 6e66   "'parallel_conf
+00020b90: 6967 2e6d 6f64 656c 5f70 6172 616c 6c65  ig.model_paralle
+00020ba0: 6c27 2c20 6275 7420 676f 7420 7468 6520  l', but got the 
+00020bb0: 6e75 6d5f 6865 6164 7320 6973 207b 7d20  num_heads is {} 
+00020bc0: 616e 6420 220a 2020 2020 2020 2020 2020  and ".          
+00020bd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020be0: 2020 2020 2020 2022 7061 7261 6c6c 656c         "parallel
+00020bf0: 5f63 6f6e 6669 672e 6d6f 6465 6c5f 7061  _config.model_pa
+00020c00: 7261 6c6c 656c 2069 7320 7b7d 2e22 2e66  rallel is {}.".f
+00020c10: 6f72 6d61 7428 6e75 6d5f 6865 6164 732c  ormat(num_heads,
+00020c20: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00020c30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020c40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020c50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020c60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020c70: 2070 6172 616c 6c65 6c5f 636f 6e66 6967   parallel_config
+00020c80: 2e6d 6f64 656c 5f70 6172 616c 6c65 6c29  .model_parallel)
+00020c90: 290a 2020 2020 2020 2020 2020 2020 6966  ).            if
+00020ca0: 2068 6964 6465 6e5f 7369 7a65 2025 2070   hidden_size % p
+00020cb0: 6172 616c 6c65 6c5f 636f 6e66 6967 2e6d  arallel_config.m
+00020cc0: 6f64 656c 5f70 6172 616c 6c65 6c20 213d  odel_parallel !=
+00020cd0: 2030 3a0a 2020 2020 2020 2020 2020 2020   0:.            
+00020ce0: 2020 2020 7261 6973 6520 5661 6c75 6545      raise ValueE
+00020cf0: 7272 6f72 280a 2020 2020 2020 2020 2020  rror(.          
+00020d00: 2020 2020 2020 2020 2020 2246 6f72 2027            "For '
+00020d10: 5472 616e 7366 6f72 6d65 7244 6563 6f64  TransformerDecod
+00020d20: 6572 4c61 7965 7227 2c20 7468 6520 636c  erLayer', the cl
+00020d30: 6173 7320 7661 7269 6162 6c65 2027 6869  ass variable 'hi
+00020d40: 6464 656e 5f73 697a 6527 206d 7573 7420  dden_size' must 
+00020d50: 6265 2064 6976 6973 6962 6c65 6420 6279  be divisibled by
+00020d60: 2022 0a20 2020 2020 2020 2020 2020 2020   ".             
+00020d70: 2020 2020 2020 2022 2770 6172 616c 6c65         "'paralle
+00020d80: 6c5f 636f 6e66 6967 2e6d 6f64 656c 5f70  l_config.model_p
+00020d90: 6172 616c 6c65 6c27 2c20 6275 7420 676f  arallel', but go
+00020da0: 7420 7468 6520 6869 6464 656e 5f73 697a  t the hidden_siz
+00020db0: 6520 6973 207b 7d20 616e 6420 220a 2020  e is {} and ".  
+00020dc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020dd0: 2020 2270 6172 616c 6c65 6c5f 636f 6e66    "parallel_conf
+00020de0: 6967 2e6d 6f64 656c 5f70 6172 616c 6c65  ig.model_paralle
+00020df0: 6c20 6973 207b 7d2e 220a 2020 2020 2020  l is {}.".      
+00020e00: 2020 2020 2020 2020 2020 2020 2020 2e66                .f
+00020e10: 6f72 6d61 7428 6869 6464 656e 5f73 697a  ormat(hidden_siz
+00020e20: 652c 2070 6172 616c 6c65 6c5f 636f 6e66  e, parallel_conf
+00020e30: 6967 2e6d 6f64 656c 5f70 6172 616c 6c65  ig.model_paralle
+00020e40: 6c29 290a 2020 2020 2020 2020 2020 2020  l)).            
+00020e50: 6966 2066 666e 5f68 6964 6465 6e5f 7369  if ffn_hidden_si
+00020e60: 7a65 2025 2070 6172 616c 6c65 6c5f 636f  ze % parallel_co
+00020e70: 6e66 6967 2e6d 6f64 656c 5f70 6172 616c  nfig.model_paral
+00020e80: 6c65 6c20 213d 2030 3a0a 2020 2020 2020  lel != 0:.      
+00020e90: 2020 2020 2020 2020 2020 7261 6973 6520            raise 
+00020ea0: 5661 6c75 6545 7272 6f72 2822 466f 7220  ValueError("For 
+00020eb0: 2754 7261 6e73 666f 726d 6572 4465 636f  'TransformerDeco
+00020ec0: 6465 724c 6179 6572 272c 2074 6865 2063  derLayer', the c
+00020ed0: 6c61 7373 2076 6172 6961 626c 6520 2766  lass variable 'f
+00020ee0: 666e 5f68 6964 6465 6e5f 7369 7a65 2720  fn_hidden_size' 
+00020ef0: 6d75 7374 2062 6520 220a 2020 2020 2020  must be ".      
 00020f00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020f10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020f20: 2020 2020 2020 2020 7467 745f 7365 715f          tgt_seq_
-00020f30: 6c65 6e67 7468 3d74 6774 5f73 6571 5f6c  length=tgt_seq_l
-00020f40: 656e 6774 682c 0a20 2020 2020 2020 2020  ength,.         
-00020f50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020f60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020f70: 2020 2020 2020 2068 6964 6465 6e5f 6472         hidden_dr
-00020f80: 6f70 6f75 745f 7261 7465 3d68 6964 6465  opout_rate=hidde
-00020f90: 6e5f 6472 6f70 6f75 745f 7261 7465 2c0a  n_dropout_rate,.
-00020fa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020fb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020f10: 2020 2020 2020 2020 2020 2022 6469 7669             "divi
+00020f20: 7369 626c 6564 2062 7920 2770 6172 616c  sibled by 'paral
+00020f30: 6c65 6c5f 636f 6e66 6967 2e6d 6f64 656c  lel_config.model
+00020f40: 5f70 6172 616c 6c65 6c27 2c20 6275 7420  _parallel', but 
+00020f50: 676f 7420 7468 6520 6666 6e5f 6869 6464  got the ffn_hidd
+00020f60: 656e 5f73 697a 6520 6973 207b 7d20 220a  en_size is {} ".
+00020f70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020f80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00020f90: 2022 616e 6420 7061 7261 6c6c 656c 5f63   "and parallel_c
+00020fa0: 6f6e 6669 672e 6d6f 6465 6c5f 7061 7261  onfig.model_para
+00020fb0: 6c6c 656c 2069 7320 7b7d 2e22 0a20 2020  llel is {}.".   
 00020fc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00020fd0: 6174 7465 6e74 696f 6e5f 6472 6f70 6f75  attention_dropou
-00020fe0: 745f 7261 7465 3d61 7474 656e 7469 6f6e  t_rate=attention
-00020ff0: 5f64 726f 706f 7574 5f72 6174 652c 0a20  _dropout_rate,. 
-00021000: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021010: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021020: 2020 2020 2020 2020 2020 2020 2020 2075                 u
-00021030: 7365 5f70 6173 743d 7573 655f 7061 7374  se_past=use_past
-00021040: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00021050: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021060: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021070: 2020 736f 6674 6d61 785f 636f 6d70 7574    softmax_comput
-00021080: 655f 7479 7065 3d73 6f66 746d 6178 5f63  e_type=softmax_c
-00021090: 6f6d 7075 7465 5f74 7970 652c 0a20 2020  ompute_type,.   
-000210a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000210b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000210c0: 2020 2020 2020 2020 2020 2020 2070 6172               par
-000210d0: 616d 5f69 6e69 745f 7479 7065 3d70 6172  am_init_type=par
-000210e0: 616d 5f69 6e69 745f 7479 7065 2c0a 2020  am_init_type,.  
-000210f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021100: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021110: 2020 2020 2020 2020 2020 2020 2020 7061                pa
-00021120: 7261 6c6c 656c 5f63 6f6e 6669 673d 636f  rallel_config=co
-00021130: 6e66 6967 5f74 6f5f 6174 7465 6e74 696f  nfig_to_attentio
-00021140: 6e29 0a0a 2020 2020 2020 2020 2020 2020  n)..            
-00021150: 2320 4372 6f73 7320 6174 7465 6e74 696f  # Cross attentio
-00021160: 6e20 7769 7468 2074 6865 206f 7574 7075  n with the outpu
-00021170: 7420 6f66 2065 6e63 6f64 6572 2061 7320  t of encoder as 
-00021180: 6d65 6d6f 7279 2074 656e 736f 720a 2020  memory tensor.  
-00021190: 2020 2020 2020 2020 2020 7365 6c66 2e63            self.c
-000211a0: 726f 7373 5f61 7474 656e 7469 6f6e 203d  ross_attention =
-000211b0: 204d 756c 7469 4865 6164 4174 7465 6e74   MultiHeadAttent
-000211c0: 696f 6e28 6869 6464 656e 5f73 697a 653d  ion(hidden_size=
-000211d0: 6869 6464 656e 5f73 697a 652c 0a20 2020  hidden_size,.   
-000211e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000211f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021200: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021210: 2020 206e 756d 5f68 6561 6473 3d6e 756d     num_heads=num
-00021220: 5f68 6561 6473 2c0a 2020 2020 2020 2020  _heads,.        
-00021230: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021240: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021250: 2020 2020 2020 2020 2020 2020 2020 6261                ba
-00021260: 7463 685f 7369 7a65 3d62 6174 6368 5f73  tch_size=batch_s
-00021270: 697a 652c 0a20 2020 2020 2020 2020 2020  ize,.           
-00021280: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021290: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000212a0: 2020 2020 2020 2020 2020 2073 7263 5f73             src_s
-000212b0: 6571 5f6c 656e 6774 683d 7467 745f 7365  eq_length=tgt_se
-000212c0: 715f 6c65 6e67 7468 2c0a 2020 2020 2020  q_length,.      
+00020fd0: 2020 2020 2020 2020 2020 2020 2020 2e66                .f
+00020fe0: 6f72 6d61 7428 6666 6e5f 6869 6464 656e  ormat(ffn_hidden
+00020ff0: 5f73 697a 652c 2070 6172 616c 6c65 6c5f  _size, parallel_
+00021000: 636f 6e66 6967 2e6d 6f64 656c 5f70 6172  config.model_par
+00021010: 616c 6c65 6c29 290a 2020 2020 2020 2020  allel)).        
+00021020: 2020 2020 6966 2075 7365 5f70 6173 743a      if use_past:
+00021030: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00021040: 2072 6169 7365 2056 616c 7565 4572 726f   raise ValueErro
+00021050: 7228 6622 5468 6520 7b73 656c 662e 636c  r(f"The {self.cl
+00021060: 735f 6e61 6d65 7d20 646f 6573 206e 6f74  s_name} does not
+00021070: 2073 7570 706f 7274 2075 7365 5f70 6173   support use_pas
+00021080: 743d 5472 7565 2e22 290a 2020 2020 2020  t=True.").      
+00021090: 2020 2020 2020 7365 6c66 2e62 6174 6368        self.batch
+000210a0: 5f73 697a 6520 3d20 6261 7463 685f 7369  _size = batch_si
+000210b0: 7a65 0a20 2020 2020 2020 2020 2020 2073  ze.            s
+000210c0: 656c 662e 7573 655f 7061 7374 203d 2075  elf.use_past = u
+000210d0: 7365 5f70 6173 740a 2020 2020 2020 2020  se_past.        
+000210e0: 2020 2020 7365 6c66 2e73 6f66 746d 6178      self.softmax
+000210f0: 5f63 6f6d 7075 7465 5f74 7970 6520 3d20  _compute_type = 
+00021100: 736f 6674 6d61 785f 636f 6d70 7574 655f  softmax_compute_
+00021110: 7479 7065 0a0a 2020 2020 2020 2020 2020  type..          
+00021120: 2020 7365 6c66 2e73 7263 5f73 6571 5f6c    self.src_seq_l
+00021130: 656e 6774 6820 3d20 7372 635f 7365 715f  ength = src_seq_
+00021140: 6c65 6e67 7468 0a20 2020 2020 2020 2020  length.         
+00021150: 2020 2073 656c 662e 7467 745f 7365 715f     self.tgt_seq_
+00021160: 6c65 6e67 7468 203d 2074 6774 5f73 6571  length = tgt_seq
+00021170: 5f6c 656e 6774 680a 2020 2020 2020 2020  _length.        
+00021180: 2020 2020 7365 6c66 2e75 7365 5f70 6173      self.use_pas
+00021190: 7420 3d20 7573 655f 7061 7374 0a20 2020  t = use_past.   
+000211a0: 2020 2020 2020 2020 2073 656c 662e 6869           self.hi
+000211b0: 6464 656e 5f73 697a 6520 3d20 6869 6464  dden_size = hidd
+000211c0: 656e 5f73 697a 650a 0a20 2020 2020 2020  en_size..       
+000211d0: 2020 2020 2073 656c 662e 6c61 7965 726e       self.layern
+000211e0: 6f72 6d31 203d 204c 6179 6572 4e6f 726d  orm1 = LayerNorm
+000211f0: 2828 6869 6464 656e 5f73 697a 652c 2929  ((hidden_size,))
+00021200: 2e74 6f5f 666c 6f61 7428 6c61 7965 726e  .to_float(layern
+00021210: 6f72 6d5f 636f 6d70 7574 655f 7479 7065  orm_compute_type
+00021220: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
+00021230: 6c66 2e6c 6179 6572 6e6f 726d 3220 3d20  lf.layernorm2 = 
+00021240: 4c61 7965 724e 6f72 6d28 2868 6964 6465  LayerNorm((hidde
+00021250: 6e5f 7369 7a65 2c29 292e 746f 5f66 6c6f  n_size,)).to_flo
+00021260: 6174 286c 6179 6572 6e6f 726d 5f63 6f6d  at(layernorm_com
+00021270: 7075 7465 5f74 7970 6529 0a20 2020 2020  pute_type).     
+00021280: 2020 2020 2020 2073 656c 662e 6174 7465         self.atte
+00021290: 6e74 696f 6e20 3d20 4d75 6c74 6948 6561  ntion = MultiHea
+000212a0: 6441 7474 656e 7469 6f6e 2868 6964 6465  dAttention(hidde
+000212b0: 6e5f 7369 7a65 3d68 6964 6465 6e5f 7369  n_size=hidden_si
+000212c0: 7a65 2c0a 2020 2020 2020 2020 2020 2020  ze,.            
 000212d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 000212e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000212f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021300: 7467 745f 7365 715f 6c65 6e67 7468 3d73  tgt_seq_length=s
-00021310: 7263 5f73 6571 5f6c 656e 6774 682c 0a20  rc_seq_length,. 
+000212f0: 2020 2020 6e75 6d5f 6865 6164 733d 6e75      num_heads=nu
+00021300: 6d5f 6865 6164 732c 0a20 2020 2020 2020  m_heads,.       
+00021310: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00021320: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021330: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021340: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021350: 2020 2020 2068 6964 6465 6e5f 6472 6f70       hidden_drop
-00021360: 6f75 745f 7261 7465 3d68 6964 6465 6e5f  out_rate=hidden_
-00021370: 6472 6f70 6f75 745f 7261 7465 2c0a 2020  dropout_rate,.  
-00021380: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021390: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021330: 2020 2020 2020 2020 2062 6174 6368 5f73           batch_s
+00021340: 697a 653d 6261 7463 685f 7369 7a65 2c0a  ize=batch_size,.
+00021350: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021360: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021370: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021380: 7372 635f 7365 715f 6c65 6e67 7468 3d74  src_seq_length=t
+00021390: 6774 5f73 6571 5f6c 656e 6774 682c 0a20  gt_seq_length,. 
 000213a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000213b0: 2020 2020 6174 7465 6e74 696f 6e5f 6472      attention_dr
-000213c0: 6f70 6f75 745f 7261 7465 3d61 7474 656e  opout_rate=atten
-000213d0: 7469 6f6e 5f64 726f 706f 7574 5f72 6174  tion_dropout_rat
-000213e0: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
+000213b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000213c0: 2020 2020 2020 2020 2020 2020 2020 2074                 t
+000213d0: 6774 5f73 6571 5f6c 656e 6774 683d 7467  gt_seq_length=tg
+000213e0: 745f 7365 715f 6c65 6e67 7468 2c0a 2020  t_seq_length,.  
 000213f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00021400: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021410: 2020 2020 2020 2020 2073 6f66 746d 6178           softmax
-00021420: 5f63 6f6d 7075 7465 5f74 7970 653d 736f  _compute_type=so
-00021430: 6674 6d61 785f 636f 6d70 7574 655f 7479  ftmax_compute_ty
-00021440: 7065 2c0a 2020 2020 2020 2020 2020 2020  pe,.            
+00021410: 2020 2020 2020 2020 2020 2020 2020 6869                hi
+00021420: 6464 656e 5f64 726f 706f 7574 5f72 6174  dden_dropout_rat
+00021430: 653d 6869 6464 656e 5f64 726f 706f 7574  e=hidden_dropout
+00021440: 5f72 6174 652c 0a20 2020 2020 2020 2020  _rate,.         
 00021450: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00021460: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021470: 2020 2020 2020 2020 2020 7573 655f 7061            use_pa
-00021480: 7374 3d75 7365 5f70 6173 742c 0a20 2020  st=use_past,.   
-00021490: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000214a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021470: 2020 2020 2020 2061 7474 656e 7469 6f6e         attention
+00021480: 5f64 726f 706f 7574 5f72 6174 653d 6174  _dropout_rate=at
+00021490: 7465 6e74 696f 6e5f 6472 6f70 6f75 745f  tention_dropout_
+000214a0: 7261 7465 2c0a 2020 2020 2020 2020 2020  rate,.          
 000214b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000214c0: 2020 2070 6172 616d 5f69 6e69 745f 7479     param_init_ty
-000214d0: 7065 3d70 6172 616d 5f69 6e69 745f 7479  pe=param_init_ty
-000214e0: 7065 2c0a 2020 2020 2020 2020 2020 2020  pe,.            
+000214c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000214d0: 2020 2020 2020 7573 655f 7061 7374 3d75        use_past=u
+000214e0: 7365 5f70 6173 742c 0a20 2020 2020 2020  se_past,.       
 000214f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00021500: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021510: 2020 2020 2020 2020 2020 7061 7261 6c6c            parall
-00021520: 656c 5f63 6f6e 6669 673d 636f 6e66 6967  el_config=config
-00021530: 5f74 6f5f 6174 7465 6e74 696f 6e29 0a20  _to_attention). 
-00021540: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-00021550: 6372 6f73 735f 6174 7465 6e74 696f 6e5f  cross_attention_
-00021560: 6c61 7965 726e 6f72 6d20 3d20 4c61 7965  layernorm = Laye
-00021570: 724e 6f72 6d28 2868 6964 6465 6e5f 7369  rNorm((hidden_si
-00021580: 7a65 2c29 292e 746f 5f66 6c6f 6174 280a  ze,)).to_float(.
-00021590: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000215a0: 6c61 7965 726e 6f72 6d5f 636f 6d70 7574  layernorm_comput
-000215b0: 655f 7479 7065 290a 0a20 2020 2020 2020  e_type)..       
-000215c0: 2020 2020 2069 6620 7365 6c66 2e75 7365       if self.use
-000215d0: 5f6d 6f65 3a0a 2020 2020 2020 2020 2020  _moe:.          
-000215e0: 2020 2020 2020 7365 6c66 2e6f 7574 7075        self.outpu
-000215f0: 7420 3d20 4d6f 4528 6869 6464 656e 5f73  t = MoE(hidden_s
-00021600: 697a 653d 6869 6464 656e 5f73 697a 652c  ize=hidden_size,
-00021610: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00021620: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021630: 2020 2064 726f 706f 7574 5f72 6174 653d     dropout_rate=
-00021640: 6869 6464 656e 5f64 726f 706f 7574 5f72  hidden_dropout_r
-00021650: 6174 652c 0a20 2020 2020 2020 2020 2020  ate,.           
-00021660: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021670: 2020 2020 2020 2066 666e 5f68 6964 6465         ffn_hidde
-00021680: 6e5f 7369 7a65 3d66 666e 5f68 6964 6465  n_size=ffn_hidde
-00021690: 6e5f 7369 7a65 2c0a 2020 2020 2020 2020  n_size,.        
+00021510: 2020 2020 2020 2020 2073 6f66 746d 6178           softmax
+00021520: 5f63 6f6d 7075 7465 5f74 7970 653d 736f  _compute_type=so
+00021530: 6674 6d61 785f 636f 6d70 7574 655f 7479  ftmax_compute_ty
+00021540: 7065 2c0a 2020 2020 2020 2020 2020 2020  pe,.            
+00021550: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021560: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021570: 2020 2020 7061 7261 6d5f 696e 6974 5f74      param_init_t
+00021580: 7970 653d 7061 7261 6d5f 696e 6974 5f74  ype=param_init_t
+00021590: 7970 652c 0a20 2020 2020 2020 2020 2020  ype,.           
+000215a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000215b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000215c0: 2020 2020 2070 6172 616c 6c65 6c5f 636f       parallel_co
+000215d0: 6e66 6967 3d63 6f6e 6669 675f 746f 5f61  nfig=config_to_a
+000215e0: 7474 656e 7469 6f6e 290a 0a20 2020 2020  ttention)..     
+000215f0: 2020 2020 2020 2023 2043 726f 7373 2061         # Cross a
+00021600: 7474 656e 7469 6f6e 2077 6974 6820 7468  ttention with th
+00021610: 6520 6f75 7470 7574 206f 6620 656e 636f  e output of enco
+00021620: 6465 7220 6173 206d 656d 6f72 7920 7465  der as memory te
+00021630: 6e73 6f72 0a20 2020 2020 2020 2020 2020  nsor.           
+00021640: 2073 656c 662e 6372 6f73 735f 6174 7465   self.cross_atte
+00021650: 6e74 696f 6e20 3d20 4d75 6c74 6948 6561  ntion = MultiHea
+00021660: 6441 7474 656e 7469 6f6e 2868 6964 6465  dAttention(hidde
+00021670: 6e5f 7369 7a65 3d68 6964 6465 6e5f 7369  n_size=hidden_si
+00021680: 7a65 2c0a 2020 2020 2020 2020 2020 2020  ze,.            
+00021690: 2020 2020 2020 2020 2020 2020 2020 2020                  
 000216a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000216b0: 2020 2020 2020 2020 2020 7061 7261 6d5f            param_
-000216c0: 696e 6974 5f74 7970 653d 7061 7261 6d5f  init_type=param_
-000216d0: 696e 6974 5f74 7970 652c 0a20 2020 2020  init_type,.     
+000216b0: 2020 2020 2020 2020 2020 6e75 6d5f 6865            num_he
+000216c0: 6164 733d 6e75 6d5f 6865 6164 732c 0a20  ads=num_heads,. 
+000216d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 000216e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000216f0: 2020 2020 2020 2020 2020 2020 2068 6964               hid
-00021700: 6465 6e5f 6163 743d 6869 6464 656e 5f61  den_act=hidden_a
-00021710: 6374 2c0a 2020 2020 2020 2020 2020 2020  ct,.            
+000216f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021700: 2020 2020 2062 6174 6368 5f73 697a 653d       batch_size=
+00021710: 6261 7463 685f 7369 7a65 2c0a 2020 2020  batch_size,.    
 00021720: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021730: 2020 2020 2020 6d6f 655f 636f 6e66 6967        moe_config
-00021740: 3d6d 6f65 5f63 6f6e 6669 672c 0a20 2020  =moe_config,.   
-00021750: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021760: 2020 2020 2020 2020 2020 2020 2020 2070                 p
-00021770: 6172 616c 6c65 6c5f 636f 6e66 6967 3d70  arallel_config=p
-00021780: 6172 616c 6c65 6c5f 636f 6e66 6967 290a  arallel_config).
-00021790: 2020 2020 2020 2020 2020 2020 656c 7365              else
-000217a0: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
-000217b0: 2020 2320 4665 6564 2046 6f72 7761 7264    # Feed Forward
-000217c0: 204e 6574 776f 726b 2c20 4646 4e0a 2020   Network, FFN.  
-000217d0: 2020 2020 2020 2020 2020 2020 2020 7365                se
-000217e0: 6c66 2e6f 7574 7075 7420 3d20 4665 6564  lf.output = Feed
-000217f0: 466f 7277 6172 6428 6869 6464 656e 5f73  Forward(hidden_s
-00021800: 697a 653d 6869 6464 656e 5f73 697a 652c  ize=hidden_size,
-00021810: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00021820: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021830: 2020 2020 2020 2020 2020 2064 726f 706f             dropo
-00021840: 7574 5f72 6174 653d 6869 6464 656e 5f64  ut_rate=hidden_d
-00021850: 726f 706f 7574 5f72 6174 652c 0a20 2020  ropout_rate,.   
-00021860: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021870: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021880: 2020 2020 2020 2066 666e 5f68 6964 6465         ffn_hidde
-00021890: 6e5f 7369 7a65 3d66 666e 5f68 6964 6465  n_size=ffn_hidde
-000218a0: 6e5f 7369 7a65 2c0a 2020 2020 2020 2020  n_size,.        
+00021730: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021740: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021750: 2020 7372 635f 7365 715f 6c65 6e67 7468    src_seq_length
+00021760: 3d74 6774 5f73 6571 5f6c 656e 6774 682c  =tgt_seq_length,
+00021770: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00021780: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021790: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000217a0: 2020 2020 2020 2074 6774 5f73 6571 5f6c         tgt_seq_l
+000217b0: 656e 6774 683d 7372 635f 7365 715f 6c65  ength=src_seq_le
+000217c0: 6e67 7468 2c0a 2020 2020 2020 2020 2020  ngth,.          
+000217d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000217e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000217f0: 2020 2020 2020 2020 2020 2020 6869 6464              hidd
+00021800: 656e 5f64 726f 706f 7574 5f72 6174 653d  en_dropout_rate=
+00021810: 6869 6464 656e 5f64 726f 706f 7574 5f72  hidden_dropout_r
+00021820: 6174 652c 0a20 2020 2020 2020 2020 2020  ate,.           
+00021830: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021840: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021850: 2020 2020 2020 2020 2020 2061 7474 656e             atten
+00021860: 7469 6f6e 5f64 726f 706f 7574 5f72 6174  tion_dropout_rat
+00021870: 653d 6174 7465 6e74 696f 6e5f 6472 6f70  e=attention_drop
+00021880: 6f75 745f 7261 7465 2c0a 2020 2020 2020  out_rate,.      
+00021890: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000218a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 000218b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000218c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000218d0: 2020 6869 6464 656e 5f61 6374 3d68 6964    hidden_act=hid
-000218e0: 6465 6e5f 6163 742c 0a20 2020 2020 2020  den_act,.       
+000218c0: 736f 6674 6d61 785f 636f 6d70 7574 655f  softmax_compute_
+000218d0: 7479 7065 3d73 6f66 746d 6178 5f63 6f6d  type=softmax_com
+000218e0: 7075 7465 5f74 7970 652c 0a20 2020 2020  pute_type,.     
 000218f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00021900: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021910: 2020 2070 6172 616d 5f69 6e69 745f 7479     param_init_ty
-00021920: 7065 3d70 6172 616d 5f69 6e69 745f 7479  pe=param_init_ty
-00021930: 7065 2c0a 2020 2020 2020 2020 2020 2020  pe,.            
+00021910: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021920: 2075 7365 5f70 6173 743d 7573 655f 7061   use_past=use_pa
+00021930: 7374 2c0a 2020 2020 2020 2020 2020 2020  st,.            
 00021940: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021950: 2020 2020 2020 2020 2020 2020 2020 7061                pa
-00021960: 7261 6c6c 656c 5f63 6f6e 6669 673d 7061  rallel_config=pa
-00021970: 7261 6c6c 656c 5f63 6f6e 6669 6729 0a20  rallel_config). 
-00021980: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-00021990: 706f 7374 5f6c 6179 6572 6e6f 726d 5f72  post_layernorm_r
-000219a0: 6573 6964 7561 6c20 3d20 706f 7374 5f6c  esidual = post_l
-000219b0: 6179 6572 6e6f 726d 5f72 6573 6964 7561  ayernorm_residua
-000219c0: 6c0a 2020 2020 2020 2020 2020 2020 7365  l.            se
-000219d0: 6c66 2e61 6464 203d 2050 2e41 6464 2829  lf.add = P.Add()
-000219e0: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-000219f0: 662e 6164 645f 3364 203d 2050 2e41 6464  f.add_3d = P.Add
-00021a00: 2829 0a20 2020 2020 2020 2020 2020 2073  ().            s
-00021a10: 656c 662e 6474 7970 6520 3d20 6d73 7479  elf.dtype = msty
-00021a20: 7065 2e66 6c6f 6174 3136 0a20 2020 2020  pe.float16.     
-00021a30: 2020 2020 2020 2073 656c 662e 6b65 795f         self.key_
-00021a40: 7061 7374 203d 204e 6f6e 650a 2020 2020  past = None.    
-00021a50: 2020 2020 2020 2020 7365 6c66 2e76 616c          self.val
-00021a60: 7565 5f70 6173 7420 3d20 4e6f 6e65 0a20  ue_past = None. 
-00021a70: 2020 2020 2020 2020 2020 2069 6620 7365             if se
-00021a80: 6c66 2e75 7365 5f70 6173 743a 0a20 2020  lf.use_past:.   
-00021a90: 2020 2020 2020 2020 2020 2020 2023 206f               # o
-00021aa0: 7065 7261 746f 7220 7573 6564 2066 6f72  perator used for
-00021ab0: 2073 7461 7465 2072 6575 7365 0a20 2020   state reuse.   
-00021ac0: 2020 2020 2020 2020 2020 2020 2073 656c               sel
-00021ad0: 662e 7265 6475 6365 7375 6d20 3d20 502e  f.reducesum = P.
-00021ae0: 5265 6475 6365 5375 6d28 292e 7368 6172  ReduceSum().shar
-00021af0: 6428 2828 312c 2031 2c20 312c 2031 292c  d(((1, 1, 1, 1),
-00021b00: 2929 0a20 2020 2020 2020 2020 2020 2020  )).             
-00021b10: 2020 2073 656c 662e 6e6f 745f 6571 7561     self.not_equa
-00021b20: 6c20 3d20 502e 4e6f 7445 7175 616c 2829  l = P.NotEqual()
-00021b30: 2e73 6861 7264 2828 2831 2c20 312c 2031  .shard(((1, 1, 1
-00021b40: 2c20 3129 2c20 2829 2929 0a20 2020 2020  , 1), ())).     
-00021b50: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-00021b60: 736c 6963 6520 3d20 502e 5374 7269 6465  slice = P.Stride
-00021b70: 6453 6c69 6365 2829 2e73 6861 7264 2828  dSlice().shard((
-00021b80: 2831 2c20 312c 2031 2c20 3129 2c29 290a  (1, 1, 1, 1),)).
+00021950: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021960: 2020 2020 2020 2020 2020 7061 7261 6d5f            param_
+00021970: 696e 6974 5f74 7970 653d 7061 7261 6d5f  init_type=param_
+00021980: 696e 6974 5f74 7970 652c 0a20 2020 2020  init_type,.     
+00021990: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000219a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000219b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000219c0: 2070 6172 616c 6c65 6c5f 636f 6e66 6967   parallel_config
+000219d0: 3d63 6f6e 6669 675f 746f 5f61 7474 656e  =config_to_atten
+000219e0: 7469 6f6e 290a 2020 2020 2020 2020 2020  tion).          
+000219f0: 2020 7365 6c66 2e63 726f 7373 5f61 7474    self.cross_att
+00021a00: 656e 7469 6f6e 5f6c 6179 6572 6e6f 726d  ention_layernorm
+00021a10: 203d 204c 6179 6572 4e6f 726d 2828 6869   = LayerNorm((hi
+00021a20: 6464 656e 5f73 697a 652c 2929 2e74 6f5f  dden_size,)).to_
+00021a30: 666c 6f61 7428 0a20 2020 2020 2020 2020  float(.         
+00021a40: 2020 2020 2020 206c 6179 6572 6e6f 726d         layernorm
+00021a50: 5f63 6f6d 7075 7465 5f74 7970 6529 0a0a  _compute_type)..
+00021a60: 2020 2020 2020 2020 2020 2020 6966 2073              if s
+00021a70: 656c 662e 7573 655f 6d6f 653a 0a20 2020  elf.use_moe:.   
+00021a80: 2020 2020 2020 2020 2020 2020 2073 656c               sel
+00021a90: 662e 6f75 7470 7574 203d 204d 6f45 2868  f.output = MoE(h
+00021aa0: 6964 6465 6e5f 7369 7a65 3d68 6964 6465  idden_size=hidde
+00021ab0: 6e5f 7369 7a65 2c0a 2020 2020 2020 2020  n_size,.        
+00021ac0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021ad0: 2020 2020 2020 2020 2020 6472 6f70 6f75            dropou
+00021ae0: 745f 7261 7465 3d68 6964 6465 6e5f 6472  t_rate=hidden_dr
+00021af0: 6f70 6f75 745f 7261 7465 2c0a 2020 2020  opout_rate,.    
+00021b00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021b10: 2020 2020 2020 2020 2020 2020 2020 6666                ff
+00021b20: 6e5f 6869 6464 656e 5f73 697a 653d 6666  n_hidden_size=ff
+00021b30: 6e5f 6869 6464 656e 5f73 697a 652c 0a20  n_hidden_size,. 
+00021b40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021b50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021b60: 2070 6172 616d 5f69 6e69 745f 7479 7065   param_init_type
+00021b70: 3d70 6172 616d 5f69 6e69 745f 7479 7065  =param_init_type
+00021b80: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
 00021b90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021ba0: 7369 7a65 5f70 6572 5f68 6561 6420 3d20  size_per_head = 
-00021bb0: 6869 6464 656e 5f73 697a 6520 2f2f 206e  hidden_size // n
-00021bc0: 756d 5f68 6561 6473 0a20 2020 2020 2020  um_heads.       
-00021bd0: 2020 2020 2020 2020 2073 656c 662e 6b65           self.ke
-00021be0: 795f 7368 6170 6520 3d20 2862 6174 6368  y_shape = (batch
-00021bf0: 5f73 697a 652c 206e 756d 5f68 6561 6473  _size, num_heads
-00021c00: 2c20 7369 7a65 5f70 6572 5f68 6561 642c  , size_per_head,
-00021c10: 2074 6774 5f73 6571 5f6c 656e 6774 6829   tgt_seq_length)
-00021c20: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00021c30: 2073 656c 662e 7661 6c75 655f 7368 6170   self.value_shap
-00021c40: 6520 3d20 2862 6174 6368 5f73 697a 652c  e = (batch_size,
-00021c50: 206e 756d 5f68 6561 6473 2c20 7467 745f   num_heads, tgt_
-00021c60: 7365 715f 6c65 6e67 7468 2c20 7369 7a65  seq_length, size
-00021c70: 5f70 6572 5f68 6561 6429 0a20 2020 2020  _per_head).     
-00021c80: 2020 2020 2020 2020 2020 2023 2070 6172             # par
-00021c90: 616d 6574 6572 7320 7361 7669 6e67 206b  ameters saving k
-00021ca0: 6579 2061 6e64 2076 616c 7565 2073 7461  ey and value sta
-00021cb0: 7465 730a 2020 2020 2020 2020 2020 2020  tes.            
-00021cc0: 2020 2020 7365 6c66 2e6b 6579 5f70 6173      self.key_pas
-00021cd0: 7420 3d20 5061 7261 6d65 7465 7228 5465  t = Parameter(Te
-00021ce0: 6e73 6f72 286e 702e 7a65 726f 7328 7368  nsor(np.zeros(sh
-00021cf0: 6170 653d 7365 6c66 2e6b 6579 5f73 6861  ape=self.key_sha
-00021d00: 7065 292c 2073 656c 662e 6474 7970 6529  pe), self.dtype)
-00021d10: 2c20 6e61 6d65 3d22 6b65 795f 7061 7374  , name="key_past
-00021d20: 2229 0a20 2020 2020 2020 2020 2020 2020  ").             
-00021d30: 2020 2073 656c 662e 7661 6c75 655f 7061     self.value_pa
-00021d40: 7374 203d 2050 6172 616d 6574 6572 2854  st = Parameter(T
-00021d50: 656e 736f 7228 6e70 2e7a 6572 6f73 2873  ensor(np.zeros(s
-00021d60: 6861 7065 3d73 656c 662e 7661 6c75 655f  hape=self.value_
-00021d70: 7368 6170 6529 2c20 7365 6c66 2e64 7479  shape), self.dty
-00021d80: 7065 292c 206e 616d 653d 2276 616c 7565  pe), name="value
-00021d90: 5f70 6173 7422 290a 2020 2020 2020 2020  _past").        
-00021da0: 2020 2020 2020 2020 7365 6c66 2e74 696c          self.til
-00021db0: 6520 3d20 502e 5469 6c65 2829 2e73 6861  e = P.Tile().sha
-00021dc0: 7264 2828 2831 2c20 3129 2c29 290a 2020  rd(((1, 1),)).  
-00021dd0: 2020 2020 2020 2020 2020 2020 2020 7365                se
-00021de0: 6c66 2e6d 756c 203d 2050 2e4d 756c 2829  lf.mul = P.Mul()
-00021df0: 2e73 6861 7264 2828 2831 2c20 312c 2031  .shard(((1, 1, 1
-00021e00: 2c20 3129 2c20 2831 2c29 2929 0a20 2020  , 1), (1,))).   
-00021e10: 2020 2020 2020 2020 2020 2020 2073 656c               sel
-00021e20: 662e 6173 7369 676e 203d 2050 2e41 7373  f.assign = P.Ass
-00021e30: 6967 6e28 292e 7368 6172 6428 2828 312c  ign().shard(((1,
-00021e40: 2031 2c20 312c 2031 292c 2028 312c 2031   1, 1, 1), (1, 1
-00021e50: 2c20 312c 2031 2929 290a 2020 2020 2020  , 1, 1))).      
-00021e60: 2020 656c 6966 205f 6765 745f 7061 7261    elif _get_para
-00021e70: 6c6c 656c 5f6d 6f64 6528 2920 6e6f 7420  llel_mode() not 
-00021e80: 696e 2028 5061 7261 6c6c 656c 4d6f 6465  in (ParallelMode
-00021e90: 2e41 5554 4f5f 5041 5241 4c4c 454c 2c29  .AUTO_PARALLEL,)
-00021ea0: 3a0a 2020 2020 2020 2020 2020 2020 5f63  :.            _c
-00021eb0: 6865 636b 5f63 6f6e 6669 6728 7061 7261  heck_config(para
-00021ec0: 6c6c 656c 5f63 6f6e 6669 6729 0a20 2020  llel_config).   
-00021ed0: 2020 2020 2020 2020 2069 6620 6e75 6d5f           if num_
-00021ee0: 6865 6164 7320 2520 7061 7261 6c6c 656c  heads % parallel
-00021ef0: 5f63 6f6e 6669 672e 6d6f 6465 6c5f 7061  _config.model_pa
-00021f00: 7261 6c6c 656c 2021 3d20 303a 0a20 2020  rallel != 0:.   
-00021f10: 2020 2020 2020 2020 2020 2020 2072 6169               rai
-00021f20: 7365 2056 616c 7565 4572 726f 7228 2246  se ValueError("F
-00021f30: 6f72 2027 5472 616e 7366 6f72 6d65 7244  or 'TransformerD
-00021f40: 6563 6f64 6572 4c61 7965 7227 2c20 7468  ecoderLayer', th
-00021f50: 6520 636c 6173 7320 7661 7269 6162 6c65  e class variable
-00021f60: 2027 6e75 6d5f 6865 6164 7327 206d 7573   'num_heads' mus
-00021f70: 7420 6265 2064 6976 6973 6962 6c65 6420  t be divisibled 
-00021f80: 6279 2022 0a20 2020 2020 2020 2020 2020  by ".           
-00021f90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00021fa0: 2020 2020 2020 2227 7061 7261 6c6c 656c        "'parallel
-00021fb0: 5f63 6f6e 6669 672e 6d6f 6465 6c5f 7061  _config.model_pa
-00021fc0: 7261 6c6c 656c 272c 2062 7574 2067 6f74  rallel', but got
-00021fd0: 2074 6865 206e 756d 5f68 6561 6473 2069   the num_heads i
-00021fe0: 7320 7b7d 2061 6e64 2022 0a20 2020 2020  s {} and ".     
-00021ff0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00022000: 2020 2020 2020 2020 2020 2020 2270 6172              "par
-00022010: 616c 6c65 6c5f 636f 6e66 6967 2e6d 6f64  allel_config.mod
-00022020: 656c 5f70 6172 616c 6c65 6c20 6973 207b  el_parallel is {
-00022030: 7d2e 222e 666f 726d 6174 286e 756d 5f68  }.".format(num_h
-00022040: 6561 6473 2c0a 2020 2020 2020 2020 2020  eads,.          
-00022050: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00022060: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021ba0: 2020 2020 6869 6464 656e 5f61 6374 3d68      hidden_act=h
+00021bb0: 6964 6465 6e5f 6163 742c 0a20 2020 2020  idden_act,.     
+00021bc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021bd0: 2020 2020 2020 2020 2020 2020 206d 6f65               moe
+00021be0: 5f63 6f6e 6669 673d 6d6f 655f 636f 6e66  _config=moe_conf
+00021bf0: 6967 2c0a 2020 2020 2020 2020 2020 2020  ig,.            
+00021c00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021c10: 2020 2020 2020 7061 7261 6c6c 656c 5f63        parallel_c
+00021c20: 6f6e 6669 673d 7061 7261 6c6c 656c 5f63  onfig=parallel_c
+00021c30: 6f6e 6669 6729 0a20 2020 2020 2020 2020  onfig).         
+00021c40: 2020 2065 6c73 653a 0a20 2020 2020 2020     else:.       
+00021c50: 2020 2020 2020 2020 2023 2046 6565 6420           # Feed 
+00021c60: 466f 7277 6172 6420 4e65 7477 6f72 6b2c  Forward Network,
+00021c70: 2046 464e 0a20 2020 2020 2020 2020 2020   FFN.           
+00021c80: 2020 2020 2073 656c 662e 6f75 7470 7574       self.output
+00021c90: 203d 2046 6565 6446 6f72 7761 7264 2868   = FeedForward(h
+00021ca0: 6964 6465 6e5f 7369 7a65 3d68 6964 6465  idden_size=hidde
+00021cb0: 6e5f 7369 7a65 2c0a 2020 2020 2020 2020  n_size,.        
+00021cc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021cd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021ce0: 2020 6472 6f70 6f75 745f 7261 7465 3d68    dropout_rate=h
+00021cf0: 6964 6465 6e5f 6472 6f70 6f75 745f 7261  idden_dropout_ra
+00021d00: 7465 2c0a 2020 2020 2020 2020 2020 2020  te,.            
+00021d10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021d20: 2020 2020 2020 2020 2020 2020 2020 6666                ff
+00021d30: 6e5f 6869 6464 656e 5f73 697a 653d 6666  n_hidden_size=ff
+00021d40: 6e5f 6869 6464 656e 5f73 697a 652c 0a20  n_hidden_size,. 
+00021d50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021d60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021d70: 2020 2020 2020 2020 2068 6964 6465 6e5f           hidden_
+00021d80: 6163 743d 6869 6464 656e 5f61 6374 2c0a  act=hidden_act,.
+00021d90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021da0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021db0: 2020 2020 2020 2020 2020 7061 7261 6d5f            param_
+00021dc0: 696e 6974 5f74 7970 653d 7061 7261 6d5f  init_type=param_
+00021dd0: 696e 6974 5f74 7970 652c 0a20 2020 2020  init_type,.     
+00021de0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021df0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00021e00: 2020 2020 2070 6172 616c 6c65 6c5f 636f       parallel_co
+00021e10: 6e66 6967 3d70 6172 616c 6c65 6c5f 636f  nfig=parallel_co
+00021e20: 6e66 6967 290a 2020 2020 2020 2020 2020  nfig).          
+00021e30: 2020 7365 6c66 2e70 6f73 745f 6c61 7965    self.post_laye
+00021e40: 726e 6f72 6d5f 7265 7369 6475 616c 203d  rnorm_residual =
+00021e50: 2070 6f73 745f 6c61 7965 726e 6f72 6d5f   post_layernorm_
+00021e60: 7265 7369 6475 616c 0a20 2020 2020 2020  residual.       
+00021e70: 2020 2020 2073 656c 662e 6164 6420 3d20       self.add = 
+00021e80: 502e 4164 6428 290a 2020 2020 2020 2020  P.Add().        
+00021e90: 2020 2020 7365 6c66 2e61 6464 5f33 6420      self.add_3d 
+00021ea0: 3d20 502e 4164 6428 290a 2020 2020 2020  = P.Add().      
+00021eb0: 2020 2020 2020 7365 6c66 2e64 7479 7065        self.dtype
+00021ec0: 203d 206d 7374 7970 652e 666c 6f61 7431   = mstype.float1
+00021ed0: 360a 2020 2020 2020 2020 2020 2020 7365  6.            se
+00021ee0: 6c66 2e6b 6579 5f70 6173 7420 3d20 4e6f  lf.key_past = No
+00021ef0: 6e65 0a20 2020 2020 2020 2020 2020 2073  ne.            s
+00021f00: 656c 662e 7661 6c75 655f 7061 7374 203d  elf.value_past =
+00021f10: 204e 6f6e 650a 2020 2020 2020 2020 2020   None.          
+00021f20: 2020 6966 2073 656c 662e 7573 655f 7061    if self.use_pa
+00021f30: 7374 3a0a 2020 2020 2020 2020 2020 2020  st:.            
+00021f40: 2020 2020 2320 6f70 6572 6174 6f72 2075      # operator u
+00021f50: 7365 6420 666f 7220 7374 6174 6520 7265  sed for state re
+00021f60: 7573 650a 2020 2020 2020 2020 2020 2020  use.            
+00021f70: 2020 2020 7365 6c66 2e72 6564 7563 6573      self.reduces
+00021f80: 756d 203d 2050 2e52 6564 7563 6553 756d  um = P.ReduceSum
+00021f90: 2829 2e73 6861 7264 2828 2831 2c20 312c  ().shard(((1, 1,
+00021fa0: 2031 2c20 3129 2c29 290a 2020 2020 2020   1, 1),)).      
+00021fb0: 2020 2020 2020 2020 2020 7365 6c66 2e6e            self.n
+00021fc0: 6f74 5f65 7175 616c 203d 2050 2e4e 6f74  ot_equal = P.Not
+00021fd0: 4571 7561 6c28 292e 7368 6172 6428 2828  Equal().shard(((
+00021fe0: 312c 2031 2c20 312c 2031 292c 2028 2929  1, 1, 1, 1), ())
+00021ff0: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
+00022000: 2020 7365 6c66 2e73 6c69 6365 203d 2050    self.slice = P
+00022010: 2e53 7472 6964 6564 536c 6963 6528 292e  .StridedSlice().
+00022020: 7368 6172 6428 2828 312c 2031 2c20 312c  shard(((1, 1, 1,
+00022030: 2031 292c 2929 0a20 2020 2020 2020 2020   1),)).         
+00022040: 2020 2020 2020 2073 697a 655f 7065 725f         size_per_
+00022050: 6865 6164 203d 2068 6964 6465 6e5f 7369  head = hidden_si
+00022060: 7a65 202f 2f20 6e75 6d5f 6865 6164 730a  ze // num_heads.
 00022070: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00022080: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00022090: 2020 2020 2020 7061 7261 6c6c 656c 5f63        parallel_c
-000220a0: 6f6e 6669 672e 6d6f 6465 6c5f 7061 7261  onfig.model_para
-000220b0: 6c6c 656c 2929 0a20 2020 2020 2020 2020  llel)).         
-000220c0: 2020 2069 6620 6869 6464 656e 5f73 697a     if hidden_siz
-000220d0: 6520 2520 7061 7261 6c6c 656c 5f63 6f6e  e % parallel_con
-000220e0: 6669 672e 6d6f 6465 6c5f 7061 7261 6c6c  fig.model_parall
-000220f0: 656c 2021 3d20 303a 0a20 2020 2020 2020  el != 0:.       
-00022100: 2020 2020 2020 2020 2072 6169 7365 2056           raise V
-00022110: 616c 7565 4572 726f 7228 0a20 2020 2020  alueError(.     
-00022120: 2020 2020 2020 2020 2020 2020 2020 2022                 "
-00022130: 466f 7220 2754 7261 6e73 666f 726d 6572  For 'Transformer
-00022140: 4465 636f 6465 724c 6179 6572 272c 2074  DecoderLayer', t
-00022150: 6865 2063 6c61 7373 2076 6172 6961 626c  he class variabl
-00022160: 6520 2768 6964 6465 6e5f 7369 7a65 2720  e 'hidden_size' 
-00022170: 6d75 7374 2062 6520 6469 7669 7369 626c  must be divisibl
-00022180: 6564 2062 7920 220a 2020 2020 2020 2020  ed by ".        
-00022190: 2020 2020 2020 2020 2020 2020 2227 7061              "'pa
-000221a0: 7261 6c6c 656c 5f63 6f6e 6669 672e 6d6f  rallel_config.mo
-000221b0: 6465 6c5f 7061 7261 6c6c 656c 272c 2062  del_parallel', b
-000221c0: 7574 2067 6f74 2074 6865 2068 6964 6465  ut got the hidde
-000221d0: 6e5f 7369 7a65 2069 7320 7b7d 2061 6e64  n_size is {} and
-000221e0: 2022 0a20 2020 2020 2020 2020 2020 2020   ".             
-000221f0: 2020 2020 2020 2022 7061 7261 6c6c 656c         "parallel
-00022200: 5f63 6f6e 6669 672e 6d6f 6465 6c5f 7061  _config.model_pa
-00022210: 7261 6c6c 656c 2069 7320 7b7d 2e22 0a20  rallel is {}.". 
-00022220: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00022230: 2020 202e 666f 726d 6174 2868 6964 6465     .format(hidde
-00022240: 6e5f 7369 7a65 2c20 7061 7261 6c6c 656c  n_size, parallel
-00022250: 5f63 6f6e 6669 672e 6d6f 6465 6c5f 7061  _config.model_pa
-00022260: 7261 6c6c 656c 2929 0a20 2020 2020 2020  rallel)).       
-00022270: 2020 2020 2069 6620 6666 6e5f 6869 6464       if ffn_hidd
-00022280: 656e 5f73 697a 6520 2520 7061 7261 6c6c  en_size % parall
-00022290: 656c 5f63 6f6e 6669 672e 6d6f 6465 6c5f  el_config.model_
-000222a0: 7061 7261 6c6c 656c 2021 3d20 303a 0a20  parallel != 0:. 
-000222b0: 2020 2020 2020 2020 2020 2020 2020 2072                 r
-000222c0: 6169 7365 2056 616c 7565 4572 726f 7228  aise ValueError(
-000222d0: 2246 6f72 2027 5472 616e 7366 6f72 6d65  "For 'Transforme
-000222e0: 7244 6563 6f64 6572 4c61 7965 7227 2c20  rDecoderLayer', 
-000222f0: 7468 6520 636c 6173 7320 7661 7269 6162  the class variab
-00022300: 6c65 2027 6666 6e5f 6869 6464 656e 5f73  le 'ffn_hidden_s
-00022310: 697a 6527 206d 7573 7420 6265 2022 0a20  ize' must be ". 
-00022320: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00022330: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00022340: 2264 6976 6973 6962 6c65 6420 6279 2027  "divisibled by '
-00022350: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
-00022360: 6d6f 6465 6c5f 7061 7261 6c6c 656c 272c  model_parallel',
-00022370: 2062 7574 2067 6f74 2074 6865 2066 666e   but got the ffn
-00022380: 5f68 6964 6465 6e5f 7369 7a65 2069 7320  _hidden_size is 
-00022390: 7b7d 2022 0a20 2020 2020 2020 2020 2020  {} ".           
-000223a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000223b0: 2020 2020 2020 2261 6e64 2070 6172 616c        "and paral
-000223c0: 6c65 6c5f 636f 6e66 6967 2e6d 6f64 656c  lel_config.model
-000223d0: 5f70 6172 616c 6c65 6c20 6973 207b 7d2e  _parallel is {}.
-000223e0: 220a 2020 2020 2020 2020 2020 2020 2020  ".              
-000223f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00022400: 2020 202e 666f 726d 6174 2866 666e 5f68     .format(ffn_h
-00022410: 6964 6465 6e5f 7369 7a65 2c20 7061 7261  idden_size, para
-00022420: 6c6c 656c 5f63 6f6e 6669 672e 6d6f 6465  llel_config.mode
-00022430: 6c5f 7061 7261 6c6c 656c 2929 0a20 2020  l_parallel)).   
-00022440: 2020 2020 2020 2020 2069 6620 7573 655f           if use_
-00022450: 7061 7374 3a0a 2020 2020 2020 2020 2020  past:.          
-00022460: 2020 2020 2020 7261 6973 6520 5661 6c75        raise Valu
-00022470: 6545 7272 6f72 2866 2254 6865 207b 7365  eError(f"The {se
-00022480: 6c66 2e63 6c73 5f6e 616d 657d 2064 6f65  lf.cls_name} doe
-00022490: 7320 6e6f 7420 7375 7070 6f72 7420 7573  s not support us
-000224a0: 655f 7061 7374 3d54 7275 652e 2229 0a20  e_past=True."). 
-000224b0: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-000224c0: 6261 7463 685f 7369 7a65 203d 2062 6174  batch_size = bat
-000224d0: 6368 5f73 697a 650a 2020 2020 2020 2020  ch_size.        
-000224e0: 2020 2020 7365 6c66 2e75 7365 5f70 6173      self.use_pas
-000224f0: 7420 3d20 7573 655f 7061 7374 0a20 2020  t = use_past.   
-00022500: 2020 2020 2020 2020 2073 656c 662e 736f           self.so
-00022510: 6674 6d61 785f 636f 6d70 7574 655f 7479  ftmax_compute_ty
-00022520: 7065 203d 2073 6f66 746d 6178 5f63 6f6d  pe = softmax_com
-00022530: 7075 7465 5f74 7970 650a 0a20 2020 2020  pute_type..     
-00022540: 2020 2020 2020 2073 656c 662e 7372 635f         self.src_
-00022550: 7365 715f 6c65 6e67 7468 203d 2073 7263  seq_length = src
-00022560: 5f73 6571 5f6c 656e 6774 680a 2020 2020  _seq_length.    
-00022570: 2020 2020 2020 2020 7365 6c66 2e74 6774          self.tgt
-00022580: 5f73 6571 5f6c 656e 6774 6820 3d20 7467  _seq_length = tg
-00022590: 745f 7365 715f 6c65 6e67 7468 0a20 2020  t_seq_length.   
-000225a0: 2020 2020 2020 2020 2073 656c 662e 7573           self.us
-000225b0: 655f 7061 7374 203d 2075 7365 5f70 6173  e_past = use_pas
-000225c0: 740a 2020 2020 2020 2020 2020 2020 7365  t.            se
-000225d0: 6c66 2e68 6964 6465 6e5f 7369 7a65 203d  lf.hidden_size =
-000225e0: 2068 6964 6465 6e5f 7369 7a65 0a0a 2020   hidden_size..  
-000225f0: 2020 2020 2020 2020 2020 7365 6c66 2e6c            self.l
-00022600: 6179 6572 6e6f 726d 3120 3d20 4c61 7965  ayernorm1 = Laye
-00022610: 724e 6f72 6d28 2868 6964 6465 6e5f 7369  rNorm((hidden_si
-00022620: 7a65 2c29 292e 746f 5f66 6c6f 6174 286c  ze,)).to_float(l
-00022630: 6179 6572 6e6f 726d 5f63 6f6d 7075 7465  ayernorm_compute
-00022640: 5f74 7970 6529 0a20 2020 2020 2020 2020  _type).         
-00022650: 2020 2073 656c 662e 6c61 7965 726e 6f72     self.layernor
-00022660: 6d31 2e73 6861 7264 2828 2870 6172 616c  m1.shard(((paral
-00022670: 6c65 6c5f 636f 6e66 6967 2e64 6174 615f  lel_config.data_
-00022680: 7061 7261 6c6c 656c 2c20 3129 2c29 290a  parallel, 1),)).
-00022690: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-000226a0: 2e6c 6179 6572 6e6f 726d 3220 3d20 4c61  .layernorm2 = La
-000226b0: 7965 724e 6f72 6d28 2868 6964 6465 6e5f  yerNorm((hidden_
-000226c0: 7369 7a65 2c29 292e 746f 5f66 6c6f 6174  size,)).to_float
-000226d0: 286c 6179 6572 6e6f 726d 5f63 6f6d 7075  (layernorm_compu
-000226e0: 7465 5f74 7970 6529 0a20 2020 2020 2020  te_type).       
-000226f0: 2020 2020 2073 656c 662e 6c61 7965 726e       self.layern
-00022700: 6f72 6d32 2e73 6861 7264 2828 2870 6172  orm2.shard(((par
-00022710: 616c 6c65 6c5f 636f 6e66 6967 2e64 6174  allel_config.dat
-00022720: 615f 7061 7261 6c6c 656c 2c20 3129 2c29  a_parallel, 1),)
-00022730: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
-00022740: 6c66 2e61 7474 656e 7469 6f6e 203d 204d  lf.attention = M
-00022750: 756c 7469 4865 6164 4174 7465 6e74 696f  ultiHeadAttentio
-00022760: 6e28 6869 6464 656e 5f73 697a 653d 6869  n(hidden_size=hi
-00022770: 6464 656e 5f73 697a 652c 0a20 2020 2020  dden_size,.     
-00022780: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00022790: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000227a0: 2020 2020 2020 2020 2020 206e 756d 5f68             num_h
-000227b0: 6561 6473 3d6e 756d 5f68 6561 6473 2c0a  eads=num_heads,.
-000227c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00022080: 7365 6c66 2e6b 6579 5f73 6861 7065 203d  self.key_shape =
+00022090: 2028 6261 7463 685f 7369 7a65 2c20 6e75   (batch_size, nu
+000220a0: 6d5f 6865 6164 732c 2073 697a 655f 7065  m_heads, size_pe
+000220b0: 725f 6865 6164 2c20 7467 745f 7365 715f  r_head, tgt_seq_
+000220c0: 6c65 6e67 7468 290a 2020 2020 2020 2020  length).        
+000220d0: 2020 2020 2020 2020 7365 6c66 2e76 616c          self.val
+000220e0: 7565 5f73 6861 7065 203d 2028 6261 7463  ue_shape = (batc
+000220f0: 685f 7369 7a65 2c20 6e75 6d5f 6865 6164  h_size, num_head
+00022100: 732c 2074 6774 5f73 6571 5f6c 656e 6774  s, tgt_seq_lengt
+00022110: 682c 2073 697a 655f 7065 725f 6865 6164  h, size_per_head
+00022120: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
+00022130: 2020 2320 7061 7261 6d65 7465 7273 2073    # parameters s
+00022140: 6176 696e 6720 6b65 7920 616e 6420 7661  aving key and va
+00022150: 6c75 6520 7374 6174 6573 0a20 2020 2020  lue states.     
+00022160: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+00022170: 6b65 795f 7061 7374 203d 2050 6172 616d  key_past = Param
+00022180: 6574 6572 2854 656e 736f 7228 6e70 2e7a  eter(Tensor(np.z
+00022190: 6572 6f73 2873 6861 7065 3d73 656c 662e  eros(shape=self.
+000221a0: 6b65 795f 7368 6170 6529 2c20 7365 6c66  key_shape), self
+000221b0: 2e64 7479 7065 292c 206e 616d 653d 226b  .dtype), name="k
+000221c0: 6579 5f70 6173 7422 290a 2020 2020 2020  ey_past").      
+000221d0: 2020 2020 2020 2020 2020 7365 6c66 2e76            self.v
+000221e0: 616c 7565 5f70 6173 7420 3d20 5061 7261  alue_past = Para
+000221f0: 6d65 7465 7228 5465 6e73 6f72 286e 702e  meter(Tensor(np.
+00022200: 7a65 726f 7328 7368 6170 653d 7365 6c66  zeros(shape=self
+00022210: 2e76 616c 7565 5f73 6861 7065 292c 2073  .value_shape), s
+00022220: 656c 662e 6474 7970 6529 2c20 6e61 6d65  elf.dtype), name
+00022230: 3d22 7661 6c75 655f 7061 7374 2229 0a20  ="value_past"). 
+00022240: 2020 2020 2020 2020 2020 2020 2020 2073                 s
+00022250: 656c 662e 7469 6c65 203d 2050 2e54 696c  elf.tile = P.Til
+00022260: 6528 292e 7368 6172 6428 2828 312c 2031  e().shard(((1, 1
+00022270: 292c 2929 0a20 2020 2020 2020 2020 2020  ),)).           
+00022280: 2020 2020 2073 656c 662e 6d75 6c20 3d20       self.mul = 
+00022290: 502e 4d75 6c28 292e 7368 6172 6428 2828  P.Mul().shard(((
+000222a0: 312c 2031 2c20 312c 2031 292c 2028 312c  1, 1, 1, 1), (1,
+000222b0: 2929 290a 2020 2020 2020 2020 2020 2020  ))).            
+000222c0: 2020 2020 7365 6c66 2e61 7373 6967 6e20      self.assign 
+000222d0: 3d20 502e 4173 7369 676e 2829 2e73 6861  = P.Assign().sha
+000222e0: 7264 2828 2831 2c20 312c 2031 2c20 3129  rd(((1, 1, 1, 1)
+000222f0: 2c20 2831 2c20 312c 2031 2c20 3129 2929  , (1, 1, 1, 1)))
+00022300: 0a20 2020 2020 2020 2065 6c69 6620 5f67  .        elif _g
+00022310: 6574 5f70 6172 616c 6c65 6c5f 6d6f 6465  et_parallel_mode
+00022320: 2829 206e 6f74 2069 6e20 2850 6172 616c  () not in (Paral
+00022330: 6c65 6c4d 6f64 652e 4155 544f 5f50 4152  lelMode.AUTO_PAR
+00022340: 414c 4c45 4c2c 293a 0a20 2020 2020 2020  ALLEL,):.       
+00022350: 2020 2020 205f 6368 6563 6b5f 636f 6e66       _check_conf
+00022360: 6967 2870 6172 616c 6c65 6c5f 636f 6e66  ig(parallel_conf
+00022370: 6967 290a 2020 2020 2020 2020 2020 2020  ig).            
+00022380: 6966 206e 756d 5f68 6561 6473 2025 2070  if num_heads % p
+00022390: 6172 616c 6c65 6c5f 636f 6e66 6967 2e6d  arallel_config.m
+000223a0: 6f64 656c 5f70 6172 616c 6c65 6c20 213d  odel_parallel !=
+000223b0: 2030 3a0a 2020 2020 2020 2020 2020 2020   0:.            
+000223c0: 2020 2020 7261 6973 6520 5661 6c75 6545      raise ValueE
+000223d0: 7272 6f72 2822 466f 7220 2754 7261 6e73  rror("For 'Trans
+000223e0: 666f 726d 6572 4465 636f 6465 724c 6179  formerDecoderLay
+000223f0: 6572 272c 2074 6865 2063 6c61 7373 2076  er', the class v
+00022400: 6172 6961 626c 6520 276e 756d 5f68 6561  ariable 'num_hea
+00022410: 6473 2720 6d75 7374 2062 6520 6469 7669  ds' must be divi
+00022420: 7369 626c 6564 2062 7920 220a 2020 2020  sibled by ".    
+00022430: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00022440: 2020 2020 2020 2020 2020 2020 2022 2770               "'p
+00022450: 6172 616c 6c65 6c5f 636f 6e66 6967 2e6d  arallel_config.m
+00022460: 6f64 656c 5f70 6172 616c 6c65 6c27 2c20  odel_parallel', 
+00022470: 6275 7420 676f 7420 7468 6520 6e75 6d5f  but got the num_
+00022480: 6865 6164 7320 6973 207b 7d20 616e 6420  heads is {} and 
+00022490: 220a 2020 2020 2020 2020 2020 2020 2020  ".              
+000224a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000224b0: 2020 2022 7061 7261 6c6c 656c 5f63 6f6e     "parallel_con
+000224c0: 6669 672e 6d6f 6465 6c5f 7061 7261 6c6c  fig.model_parall
+000224d0: 656c 2069 7320 7b7d 2e22 2e66 6f72 6d61  el is {}.".forma
+000224e0: 7428 6e75 6d5f 6865 6164 732c 0a20 2020  t(num_heads,.   
+000224f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00022500: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00022510: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00022520: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00022530: 2020 2020 2020 2020 2020 2020 2070 6172               par
+00022540: 616c 6c65 6c5f 636f 6e66 6967 2e6d 6f64  allel_config.mod
+00022550: 656c 5f70 6172 616c 6c65 6c29 290a 2020  el_parallel)).  
+00022560: 2020 2020 2020 2020 2020 6966 2068 6964            if hid
+00022570: 6465 6e5f 7369 7a65 2025 2070 6172 616c  den_size % paral
+00022580: 6c65 6c5f 636f 6e66 6967 2e6d 6f64 656c  lel_config.model
+00022590: 5f70 6172 616c 6c65 6c20 213d 2030 3a0a  _parallel != 0:.
+000225a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000225b0: 7261 6973 6520 5661 6c75 6545 7272 6f72  raise ValueError
+000225c0: 280a 2020 2020 2020 2020 2020 2020 2020  (.              
+000225d0: 2020 2020 2020 2246 6f72 2027 5472 616e        "For 'Tran
+000225e0: 7366 6f72 6d65 7244 6563 6f64 6572 4c61  sformerDecoderLa
+000225f0: 7965 7227 2c20 7468 6520 636c 6173 7320  yer', the class 
+00022600: 7661 7269 6162 6c65 2027 6869 6464 656e  variable 'hidden
+00022610: 5f73 697a 6527 206d 7573 7420 6265 2064  _size' must be d
+00022620: 6976 6973 6962 6c65 6420 6279 2022 0a20  ivisibled by ". 
+00022630: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00022640: 2020 2022 2770 6172 616c 6c65 6c5f 636f     "'parallel_co
+00022650: 6e66 6967 2e6d 6f64 656c 5f70 6172 616c  nfig.model_paral
+00022660: 6c65 6c27 2c20 6275 7420 676f 7420 7468  lel', but got th
+00022670: 6520 6869 6464 656e 5f73 697a 6520 6973  e hidden_size is
+00022680: 207b 7d20 616e 6420 220a 2020 2020 2020   {} and ".      
+00022690: 2020 2020 2020 2020 2020 2020 2020 2270                "p
+000226a0: 6172 616c 6c65 6c5f 636f 6e66 6967 2e6d  arallel_config.m
+000226b0: 6f64 656c 5f70 6172 616c 6c65 6c20 6973  odel_parallel is
+000226c0: 207b 7d2e 220a 2020 2020 2020 2020 2020   {}.".          
+000226d0: 2020 2020 2020 2020 2020 2e66 6f72 6d61            .forma
+000226e0: 7428 6869 6464 656e 5f73 697a 652c 2070  t(hidden_size, p
+000226f0: 6172 616c 6c65 6c5f 636f 6e66 6967 2e6d  arallel_config.m
+00022700: 6f64 656c 5f70 6172 616c 6c65 6c29 290a  odel_parallel)).
+00022710: 2020 2020 2020 2020 2020 2020 6966 2066              if f
+00022720: 666e 5f68 6964 6465 6e5f 7369 7a65 2025  fn_hidden_size %
+00022730: 2070 6172 616c 6c65 6c5f 636f 6e66 6967   parallel_config
+00022740: 2e6d 6f64 656c 5f70 6172 616c 6c65 6c20  .model_parallel 
+00022750: 213d 2030 3a0a 2020 2020 2020 2020 2020  != 0:.          
+00022760: 2020 2020 2020 7261 6973 6520 5661 6c75        raise Valu
+00022770: 6545 7272 6f72 2822 466f 7220 2754 7261  eError("For 'Tra
+00022780: 6e73 666f 726d 6572 4465 636f 6465 724c  nsformerDecoderL
+00022790: 6179 6572 272c 2074 6865 2063 6c61 7373  ayer', the class
+000227a0: 2076 6172 6961 626c 6520 2766 666e 5f68   variable 'ffn_h
+000227b0: 6964 6465 6e5f 7369 7a65 2720 6d75 7374  idden_size' must
+000227c0: 2062 6520 220a 2020 2020 2020 2020 2020   be ".          
 000227d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000227e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000227f0: 6261 7463 685f 7369 7a65 3d62 6174 6368  batch_size=batch
-00022800: 5f73 697a 652c 0a20 2020 2020 2020 2020  _size,.         
-00022810: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00022820: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00022830: 2020 2020 2020 2073 7263 5f73 6571 5f6c         src_seq_l
-00022840: 656e 6774 683d 7467 745f 7365 715f 6c65  ength=tgt_seq_le
-00022850: 6e67 7468 2c0a 2020 2020 2020 2020 2020  ngth,.          
-00022860: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00022870: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00022880: 2020 2020 2020 7467 745f 7365 715f 6c65        tgt_seq_le
-00022890: 6e67 7468 3d74 6774 5f73 6571 5f6c 656e  ngth=tgt_seq_len
-000228a0: 6774 682c 0a20 2020 2020 2020 2020 2020  gth,.           
-000228b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000228c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000228d0: 2020 2020 2068 6964 6465 6e5f 6472 6f70       hidden_drop
-000228e0: 6f75 745f 7261 7465 3d68 6964 6465 6e5f  out_rate=hidden_
-000228f0: 6472 6f70 6f75 745f 7261 7465 2c0a 2020  dropout_rate,.  
-00022900: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00022910: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00022920: 2020 2020 2020 2020 2020 2020 2020 6174                at
-00022930: 7465 6e74 696f 6e5f 6472 6f70 6f75 745f  tention_dropout_
-00022940: 7261 7465 3d61 7474 656e 7469 6f6e 5f64  rate=attention_d
-00022950: 726f 706f 7574 5f72 6174 652c 0a20 2020  ropout_rate,.   
-00022960: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00022970: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00022980: 2020 2020 2020 2020 2020 2020 2075 7365               use
-00022990: 5f70 6173 743d 7573 655f 7061 7374 2c0a  _past=use_past,.
-000229a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000229b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000229c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000229d0: 736f 6674 6d61 785f 636f 6d70 7574 655f  softmax_compute_
-000229e0: 7479 7065 3d73 6f66 746d 6178 5f63 6f6d  type=softmax_com
-000229f0: 7075 7465 5f74 7970 652c 0a20 2020 2020  pute_type,.     
-00022a00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00022a10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00022a20: 2020 2020 2020 2020 2020 2070 6172 616d             param
-00022a30: 5f69 6e69 745f 7479 7065 3d70 6172 616d  _init_type=param
-00022a40: 5f69 6e69 745f 7479 7065 2c0a 2020 2020  _init_type,.    
-00022a50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00022a60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00022a70: 2020 2020 2020 2020 2020 2020 7061 7261              para
-00022a80: 6c6c 656c 5f63 6f6e 6669 673d 636f 6e66  llel_config=conf
-00022a90: 6967 5f74 6f5f 6174 7465 6e74 696f 6e29  ig_to_attention)
-00022aa0: 0a0a 2020 2020 2020 2020 2020 2020 2320  ..            # 
-00022ab0: 4372 6f73 7320 6174 7465 6e74 696f 6e20  Cross attention 
-00022ac0: 7769 7468 2074 6865 206f 7574 7075 7420  with the output 
-00022ad0: 6f66 2065 6e63 6f64 6572 2061 7320 6d65  of encoder as me
-00022ae0: 6d6f 7279 2074 656e 736f 720a 2020 2020  mory tensor.    
-00022af0: 2020 2020 2020 2020 7365 6c66 2e63 726f          self.cro
-00022b00: 7373 5f61 7474 656e 7469 6f6e 203d 204d  ss_attention = M
-00022b10: 756c 7469 4865 6164 4174 7465 6e74 696f  ultiHeadAttentio
-00022b20: 6e28 6869 6464 656e 5f73 697a 653d 6869  n(hidden_size=hi
-00022b30: 6464 656e 5f73 697a 652c 0a20 2020 2020  dden_size,.     
-00022b40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00022b50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00022b60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00022b70: 206e 756d 5f68 6561 6473 3d6e 756d 5f68   num_heads=num_h
-00022b80: 6561 6473 2c0a 2020 2020 2020 2020 2020  eads,.          
-00022b90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00022ba0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00022bb0: 2020 2020 2020 2020 2020 2020 6261 7463              batc
-00022bc0: 685f 7369 7a65 3d62 6174 6368 5f73 697a  h_size=batch_siz
-00022bd0: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
-00022be0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00022bf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00022c00: 2020 2020 2020 2020 2073 7263 5f73 6571           src_seq
-00022c10: 5f6c 656e 6774 683d 7467 745f 7365 715f  _length=tgt_seq_
-00022c20: 6c65 6e67 7468 2c0a 2020 2020 2020 2020  length,.        
+000227e0: 2020 2020 2020 2022 6469 7669 7369 626c         "divisibl
+000227f0: 6564 2062 7920 2770 6172 616c 6c65 6c5f  ed by 'parallel_
+00022800: 636f 6e66 6967 2e6d 6f64 656c 5f70 6172  config.model_par
+00022810: 616c 6c65 6c27 2c20 6275 7420 676f 7420  allel', but got 
+00022820: 7468 6520 6666 6e5f 6869 6464 656e 5f73  the ffn_hidden_s
+00022830: 697a 6520 6973 207b 7d20 220a 2020 2020  ize is {} ".    
+00022840: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00022850: 2020 2020 2020 2020 2020 2020 2022 616e               "an
+00022860: 6420 7061 7261 6c6c 656c 5f63 6f6e 6669  d parallel_confi
+00022870: 672e 6d6f 6465 6c5f 7061 7261 6c6c 656c  g.model_parallel
+00022880: 2069 7320 7b7d 2e22 0a20 2020 2020 2020   is {}.".       
+00022890: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000228a0: 2020 2020 2020 2020 2020 2e66 6f72 6d61            .forma
+000228b0: 7428 6666 6e5f 6869 6464 656e 5f73 697a  t(ffn_hidden_siz
+000228c0: 652c 2070 6172 616c 6c65 6c5f 636f 6e66  e, parallel_conf
+000228d0: 6967 2e6d 6f64 656c 5f70 6172 616c 6c65  ig.model_paralle
+000228e0: 6c29 290a 2020 2020 2020 2020 2020 2020  l)).            
+000228f0: 6966 2075 7365 5f70 6173 743a 0a20 2020  if use_past:.   
+00022900: 2020 2020 2020 2020 2020 2020 2072 6169               rai
+00022910: 7365 2056 616c 7565 4572 726f 7228 6622  se ValueError(f"
+00022920: 5468 6520 7b73 656c 662e 636c 735f 6e61  The {self.cls_na
+00022930: 6d65 7d20 646f 6573 206e 6f74 2073 7570  me} does not sup
+00022940: 706f 7274 2075 7365 5f70 6173 743d 5472  port use_past=Tr
+00022950: 7565 2e22 290a 2020 2020 2020 2020 2020  ue.").          
+00022960: 2020 7365 6c66 2e62 6174 6368 5f73 697a    self.batch_siz
+00022970: 6520 3d20 6261 7463 685f 7369 7a65 0a20  e = batch_size. 
+00022980: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+00022990: 7573 655f 7061 7374 203d 2075 7365 5f70  use_past = use_p
+000229a0: 6173 740a 2020 2020 2020 2020 2020 2020  ast.            
+000229b0: 7365 6c66 2e73 6f66 746d 6178 5f63 6f6d  self.softmax_com
+000229c0: 7075 7465 5f74 7970 6520 3d20 736f 6674  pute_type = soft
+000229d0: 6d61 785f 636f 6d70 7574 655f 7479 7065  max_compute_type
+000229e0: 0a0a 2020 2020 2020 2020 2020 2020 7365  ..            se
+000229f0: 6c66 2e73 7263 5f73 6571 5f6c 656e 6774  lf.src_seq_lengt
+00022a00: 6820 3d20 7372 635f 7365 715f 6c65 6e67  h = src_seq_leng
+00022a10: 7468 0a20 2020 2020 2020 2020 2020 2073  th.            s
+00022a20: 656c 662e 7467 745f 7365 715f 6c65 6e67  elf.tgt_seq_leng
+00022a30: 7468 203d 2074 6774 5f73 6571 5f6c 656e  th = tgt_seq_len
+00022a40: 6774 680a 2020 2020 2020 2020 2020 2020  gth.            
+00022a50: 7365 6c66 2e75 7365 5f70 6173 7420 3d20  self.use_past = 
+00022a60: 7573 655f 7061 7374 0a20 2020 2020 2020  use_past.       
+00022a70: 2020 2020 2073 656c 662e 6869 6464 656e       self.hidden
+00022a80: 5f73 697a 6520 3d20 6869 6464 656e 5f73  _size = hidden_s
+00022a90: 697a 650a 0a20 2020 2020 2020 2020 2020  ize..           
+00022aa0: 2073 656c 662e 6c61 7965 726e 6f72 6d31   self.layernorm1
+00022ab0: 203d 204c 6179 6572 4e6f 726d 2828 6869   = LayerNorm((hi
+00022ac0: 6464 656e 5f73 697a 652c 2929 2e74 6f5f  dden_size,)).to_
+00022ad0: 666c 6f61 7428 6c61 7965 726e 6f72 6d5f  float(layernorm_
+00022ae0: 636f 6d70 7574 655f 7479 7065 290a 2020  compute_type).  
+00022af0: 2020 2020 2020 2020 2020 7365 6c66 2e6c            self.l
+00022b00: 6179 6572 6e6f 726d 312e 7368 6172 6428  ayernorm1.shard(
+00022b10: 2828 7061 7261 6c6c 656c 5f63 6f6e 6669  ((parallel_confi
+00022b20: 672e 6461 7461 5f70 6172 616c 6c65 6c2c  g.data_parallel,
+00022b30: 2031 292c 2929 0a20 2020 2020 2020 2020   1),)).         
+00022b40: 2020 2073 656c 662e 6c61 7965 726e 6f72     self.layernor
+00022b50: 6d32 203d 204c 6179 6572 4e6f 726d 2828  m2 = LayerNorm((
+00022b60: 6869 6464 656e 5f73 697a 652c 2929 2e74  hidden_size,)).t
+00022b70: 6f5f 666c 6f61 7428 6c61 7965 726e 6f72  o_float(layernor
+00022b80: 6d5f 636f 6d70 7574 655f 7479 7065 290a  m_compute_type).
+00022b90: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+00022ba0: 2e6c 6179 6572 6e6f 726d 322e 7368 6172  .layernorm2.shar
+00022bb0: 6428 2828 7061 7261 6c6c 656c 5f63 6f6e  d(((parallel_con
+00022bc0: 6669 672e 6461 7461 5f70 6172 616c 6c65  fig.data_paralle
+00022bd0: 6c2c 2031 292c 2929 0a20 2020 2020 2020  l, 1),)).       
+00022be0: 2020 2020 2073 656c 662e 6174 7465 6e74       self.attent
+00022bf0: 696f 6e20 3d20 4d75 6c74 6948 6561 6441  ion = MultiHeadA
+00022c00: 7474 656e 7469 6f6e 2868 6964 6465 6e5f  ttention(hidden_
+00022c10: 7369 7a65 3d68 6964 6465 6e5f 7369 7a65  size=hidden_size
+00022c20: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
 00022c30: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00022c40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00022c50: 2020 2020 2020 2020 2020 2020 2020 7467                tg
-00022c60: 745f 7365 715f 6c65 6e67 7468 3d73 7263  t_seq_length=src
-00022c70: 5f73 6571 5f6c 656e 6774 682c 0a20 2020  _seq_length,.   
+00022c50: 2020 6e75 6d5f 6865 6164 733d 6e75 6d5f    num_heads=num_
+00022c60: 6865 6164 732c 0a20 2020 2020 2020 2020  heads,.         
+00022c70: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00022c80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00022c90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00022ca0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00022cb0: 2020 2068 6964 6465 6e5f 6472 6f70 6f75     hidden_dropou
-00022cc0: 745f 7261 7465 3d68 6964 6465 6e5f 6472  t_rate=hidden_dr
-00022cd0: 6f70 6f75 745f 7261 7465 2c0a 2020 2020  opout_rate,.    
-00022ce0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00022cf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00022c90: 2020 2020 2020 2062 6174 6368 5f73 697a         batch_siz
+00022ca0: 653d 6261 7463 685f 7369 7a65 2c0a 2020  e=batch_size,.  
+00022cb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00022cc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00022cd0: 2020 2020 2020 2020 2020 2020 2020 7372                sr
+00022ce0: 635f 7365 715f 6c65 6e67 7468 3d74 6774  c_seq_length=tgt
+00022cf0: 5f73 6571 5f6c 656e 6774 682c 0a20 2020  _seq_length,.   
 00022d00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00022d10: 2020 6174 7465 6e74 696f 6e5f 6472 6f70    attention_drop
-00022d20: 6f75 745f 7261 7465 3d61 7474 656e 7469  out_rate=attenti
-00022d30: 6f6e 5f64 726f 706f 7574 5f72 6174 652c  on_dropout_rate,
-00022d40: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00022d10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00022d20: 2020 2020 2020 2020 2020 2020 2074 6774               tgt
+00022d30: 5f73 6571 5f6c 656e 6774 683d 7467 745f  _seq_length=tgt_
+00022d40: 7365 715f 6c65 6e67 7468 2c0a 2020 2020  seq_length,.    
 00022d50: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00022d60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00022d70: 2020 2020 2020 2073 6f66 746d 6178 5f63         softmax_c
-00022d80: 6f6d 7075 7465 5f74 7970 653d 736f 6674  ompute_type=soft
-00022d90: 6d61 785f 636f 6d70 7574 655f 7479 7065  max_compute_type
-00022da0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00022d70: 2020 2020 2020 2020 2020 2020 6869 6464              hidd
+00022d80: 656e 5f64 726f 706f 7574 5f72 6174 653d  en_dropout_rate=
+00022d90: 6869 6464 656e 5f64 726f 706f 7574 5f72  hidden_dropout_r
+00022da0: 6174 652c 0a20 2020 2020 2020 2020 2020  ate,.           
 00022db0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00022dc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00022dd0: 2020 2020 2020 2020 7573 655f 7061 7374          use_past
-00022de0: 3d75 7365 5f70 6173 742c 0a20 2020 2020  =use_past,.     
-00022df0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00022e00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00022dd0: 2020 2020 2061 7474 656e 7469 6f6e 5f64       attention_d
+00022de0: 726f 706f 7574 5f72 6174 653d 6174 7465  ropout_rate=atte
+00022df0: 6e74 696f 6e5f 6472 6f70 6f75 745f 7261  ntion_dropout_ra
+00022e00: 7465 2c0a 2020 2020 2020 2020 2020 2020  te,.            
 00022e10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00022e20: 2070 6172 616d 5f69 6e69 745f 7479 7065   param_init_type
-00022e30: 3d70 6172 616d 5f69 6e69 745f 7479 7065  =param_init_type
-00022e40: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00022e20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00022e30: 2020 2020 7573 655f 7061 7374 3d75 7365      use_past=use
+00022e40: 5f70 6173 742c 0a20 2020 2020 2020 2020  _past,.         
 00022e50: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00022e60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00022e70: 2020 2020 2020 2020 7061 7261 6c6c 656c          parallel
-00022e80: 5f63 6f6e 6669 673d 636f 6e66 6967 5f74  _config=config_t
-00022e90: 6f5f 6174 7465 6e74 696f 6e29 0a20 2020  o_attention).   
-00022ea0: 2020 2020 2020 2020 2073 656c 662e 6372           self.cr
-00022eb0: 6f73 735f 6174 7465 6e74 696f 6e5f 6c61  oss_attention_la
-00022ec0: 7965 726e 6f72 6d20 3d20 4c61 7965 724e  yernorm = LayerN
-00022ed0: 6f72 6d28 2868 6964 6465 6e5f 7369 7a65  orm((hidden_size
-00022ee0: 2c29 292e 746f 5f66 6c6f 6174 280a 2020  ,)).to_float(.  
-00022ef0: 2020 2020 2020 2020 2020 2020 2020 6c61                la
-00022f00: 7965 726e 6f72 6d5f 636f 6d70 7574 655f  yernorm_compute_
-00022f10: 7479 7065 290a 2020 2020 2020 2020 2020  type).          
-00022f20: 2020 7365 6c66 2e63 726f 7373 5f61 7474    self.cross_att
-00022f30: 656e 7469 6f6e 5f6c 6179 6572 6e6f 726d  ention_layernorm
-00022f40: 2e73 6861 7264 2828 2870 6172 616c 6c65  .shard(((paralle
-00022f50: 6c5f 636f 6e66 6967 2e64 6174 615f 7061  l_config.data_pa
-00022f60: 7261 6c6c 656c 2c20 3129 2c29 290a 0a20  rallel, 1),)).. 
-00022f70: 2020 2020 2020 2020 2020 2069 6620 7365             if se
-00022f80: 6c66 2e75 7365 5f6d 6f65 3a0a 2020 2020  lf.use_moe:.    
-00022f90: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-00022fa0: 2e6f 7574 7075 7420 3d20 4d6f 4528 6869  .output = MoE(hi
-00022fb0: 6464 656e 5f73 697a 653d 6869 6464 656e  dden_size=hidden
-00022fc0: 5f73 697a 652c 0a20 2020 2020 2020 2020  _size,.         
-00022fd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00022fe0: 2020 2020 2020 2020 2064 726f 706f 7574           dropout
-00022ff0: 5f72 6174 653d 6869 6464 656e 5f64 726f  _rate=hidden_dro
-00023000: 706f 7574 5f72 6174 652c 0a20 2020 2020  pout_rate,.     
-00023010: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00023020: 2020 2020 2020 2020 2020 2020 2066 666e               ffn
-00023030: 5f68 6964 6465 6e5f 7369 7a65 3d66 666e  _hidden_size=ffn
-00023040: 5f68 6964 6465 6e5f 7369 7a65 2c0a 2020  _hidden_size,.  
+00022e70: 2020 2020 2020 2073 6f66 746d 6178 5f63         softmax_c
+00022e80: 6f6d 7075 7465 5f74 7970 653d 736f 6674  ompute_type=soft
+00022e90: 6d61 785f 636f 6d70 7574 655f 7479 7065  max_compute_type
+00022ea0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00022eb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00022ec0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00022ed0: 2020 7061 7261 6d5f 696e 6974 5f74 7970    param_init_typ
+00022ee0: 653d 7061 7261 6d5f 696e 6974 5f74 7970  e=param_init_typ
+00022ef0: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
+00022f00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00022f10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00022f20: 2020 2070 6172 616c 6c65 6c5f 636f 6e66     parallel_conf
+00022f30: 6967 3d63 6f6e 6669 675f 746f 5f61 7474  ig=config_to_att
+00022f40: 656e 7469 6f6e 290a 0a20 2020 2020 2020  ention)..       
+00022f50: 2020 2020 2023 2043 726f 7373 2061 7474       # Cross att
+00022f60: 656e 7469 6f6e 2077 6974 6820 7468 6520  ention with the 
+00022f70: 6f75 7470 7574 206f 6620 656e 636f 6465  output of encode
+00022f80: 7220 6173 206d 656d 6f72 7920 7465 6e73  r as memory tens
+00022f90: 6f72 0a20 2020 2020 2020 2020 2020 2073  or.            s
+00022fa0: 656c 662e 6372 6f73 735f 6174 7465 6e74  elf.cross_attent
+00022fb0: 696f 6e20 3d20 4d75 6c74 6948 6561 6441  ion = MultiHeadA
+00022fc0: 7474 656e 7469 6f6e 2868 6964 6465 6e5f  ttention(hidden_
+00022fd0: 7369 7a65 3d68 6964 6465 6e5f 7369 7a65  size=hidden_size
+00022fe0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00022ff0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00023000: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00023010: 2020 2020 2020 2020 6e75 6d5f 6865 6164          num_head
+00023020: 733d 6e75 6d5f 6865 6164 732c 0a20 2020  s=num_heads,.   
+00023030: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00023040: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00023050: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00023060: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00023070: 7061 7261 6d5f 696e 6974 5f74 7970 653d  param_init_type=
-00023080: 7061 7261 6d5f 696e 6974 5f74 7970 652c  param_init_type,
-00023090: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00023060: 2020 2062 6174 6368 5f73 697a 653d 6261     batch_size=ba
+00023070: 7463 685f 7369 7a65 2c0a 2020 2020 2020  tch_size,.      
+00023080: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00023090: 2020 2020 2020 2020 2020 2020 2020 2020                  
 000230a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000230b0: 2020 2068 6964 6465 6e5f 6163 743d 6869     hidden_act=hi
-000230c0: 6464 656e 5f61 6374 2c0a 2020 2020 2020  dden_act,.      
+000230b0: 7372 635f 7365 715f 6c65 6e67 7468 3d74  src_seq_length=t
+000230c0: 6774 5f73 6571 5f6c 656e 6774 682c 0a20  gt_seq_length,. 
 000230d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000230e0: 2020 2020 2020 2020 2020 2020 6d6f 655f              moe_
-000230f0: 636f 6e66 6967 3d6d 6f65 5f63 6f6e 6669  config=moe_confi
-00023100: 672c 0a20 2020 2020 2020 2020 2020 2020  g,.             
-00023110: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00023120: 2020 2020 2070 6172 616c 6c65 6c5f 636f       parallel_co
-00023130: 6e66 6967 3d70 6172 616c 6c65 6c5f 636f  nfig=parallel_co
-00023140: 6e66 6967 290a 2020 2020 2020 2020 2020  nfig).          
-00023150: 2020 656c 7365 3a0a 2020 2020 2020 2020    else:.        
-00023160: 2020 2020 2020 2020 2320 4665 6564 2046          # Feed F
-00023170: 6f72 7761 7264 204e 6574 776f 726b 2c20  orward Network, 
-00023180: 4646 4e0a 2020 2020 2020 2020 2020 2020  FFN.            
-00023190: 2020 2020 7365 6c66 2e6f 7574 7075 7420      self.output 
-000231a0: 3d20 4665 6564 466f 7277 6172 6428 6869  = FeedForward(hi
-000231b0: 6464 656e 5f73 697a 653d 6869 6464 656e  dden_size=hidden
-000231c0: 5f73 697a 652c 0a20 2020 2020 2020 2020  _size,.         
-000231d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000231e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000231f0: 2064 726f 706f 7574 5f72 6174 653d 6869   dropout_rate=hi
-00023200: 6464 656e 5f64 726f 706f 7574 5f72 6174  dden_dropout_rat
-00023210: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
-00023220: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00023230: 2020 2020 2020 2020 2020 2020 2066 666e               ffn
-00023240: 5f68 6964 6465 6e5f 7369 7a65 3d66 666e  _hidden_size=ffn
-00023250: 5f68 6964 6465 6e5f 7369 7a65 2c0a 2020  _hidden_size,.  
+000230e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000230f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00023100: 2020 2020 2074 6774 5f73 6571 5f6c 656e       tgt_seq_len
+00023110: 6774 683d 7372 635f 7365 715f 6c65 6e67  gth=src_seq_leng
+00023120: 7468 2c0a 2020 2020 2020 2020 2020 2020  th,.            
+00023130: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00023140: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00023150: 2020 2020 2020 2020 2020 6869 6464 656e            hidden
+00023160: 5f64 726f 706f 7574 5f72 6174 653d 6869  _dropout_rate=hi
+00023170: 6464 656e 5f64 726f 706f 7574 5f72 6174  dden_dropout_rat
+00023180: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
+00023190: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000231a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000231b0: 2020 2020 2020 2020 2061 7474 656e 7469           attenti
+000231c0: 6f6e 5f64 726f 706f 7574 5f72 6174 653d  on_dropout_rate=
+000231d0: 6174 7465 6e74 696f 6e5f 6472 6f70 6f75  attention_dropou
+000231e0: 745f 7261 7465 2c0a 2020 2020 2020 2020  t_rate,.        
+000231f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00023200: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00023210: 2020 2020 2020 2020 2020 2020 2020 736f                so
+00023220: 6674 6d61 785f 636f 6d70 7574 655f 7479  ftmax_compute_ty
+00023230: 7065 3d73 6f66 746d 6178 5f63 6f6d 7075  pe=softmax_compu
+00023240: 7465 5f74 7970 652c 0a20 2020 2020 2020  te_type,.       
+00023250: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00023260: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00023270: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00023280: 2020 2020 2020 2020 6869 6464 656e 5f61          hidden_a
-00023290: 6374 3d68 6964 6465 6e5f 6163 742c 0a20  ct=hidden_act,. 
+00023270: 2020 2020 2020 2020 2020 2020 2020 2075                 u
+00023280: 7365 5f70 6173 743d 7573 655f 7061 7374  se_past=use_past
+00023290: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
 000232a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 000232b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000232c0: 2020 2020 2020 2020 2070 6172 616d 5f69           param_i
-000232d0: 6e69 745f 7479 7065 3d70 6172 616d 5f69  nit_type=param_i
-000232e0: 6e69 745f 7479 7065 2c0a 2020 2020 2020  nit_type,.      
+000232c0: 2020 2020 2020 2020 7061 7261 6d5f 696e          param_in
+000232d0: 6974 5f74 7970 653d 7061 7261 6d5f 696e  it_type=param_in
+000232e0: 6974 5f74 7970 652c 0a20 2020 2020 2020  it_type,.       
 000232f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00023300: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00023310: 2020 2020 7061 7261 6c6c 656c 5f63 6f6e      parallel_con
-00023320: 6669 673d 7061 7261 6c6c 656c 5f63 6f6e  fig=parallel_con
-00023330: 6669 6729 0a20 2020 2020 2020 2020 2020  fig).           
-00023340: 2073 656c 662e 706f 7374 5f6c 6179 6572   self.post_layer
-00023350: 6e6f 726d 5f72 6573 6964 7561 6c20 3d20  norm_residual = 
-00023360: 706f 7374 5f6c 6179 6572 6e6f 726d 5f72  post_layernorm_r
-00023370: 6573 6964 7561 6c0a 2020 2020 2020 2020  esidual.        
-00023380: 2020 2020 7365 6c66 2e61 6464 203d 2050      self.add = P
-00023390: 2e41 6464 2829 2e73 6861 7264 2828 2870  .Add().shard(((p
-000233a0: 6172 616c 6c65 6c5f 636f 6e66 6967 2e64  arallel_config.d
-000233b0: 6174 615f 7061 7261 6c6c 656c 2c20 3129  ata_parallel, 1)
-000233c0: 2c20 2870 6172 616c 6c65 6c5f 636f 6e66  , (parallel_conf
-000233d0: 6967 2e64 6174 615f 7061 7261 6c6c 656c  ig.data_parallel
-000233e0: 2c20 3129 2929 0a20 2020 2020 2020 2020  , 1))).         
-000233f0: 2020 2073 656c 662e 6164 645f 3364 203d     self.add_3d =
-00023400: 2050 2e41 6464 2829 2e73 6861 7264 2828   P.Add().shard((
-00023410: 2870 6172 616c 6c65 6c5f 636f 6e66 6967  (parallel_config
-00023420: 2e64 6174 615f 7061 7261 6c6c 656c 2c20  .data_parallel, 
-00023430: 312c 2031 292c 2028 7061 7261 6c6c 656c  1, 1), (parallel
-00023440: 5f63 6f6e 6669 672e 6461 7461 5f70 6172  _config.data_par
-00023450: 616c 6c65 6c2c 2031 2c20 3129 2929 0a20  allel, 1, 1))). 
-00023460: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-00023470: 6474 7970 6520 3d20 6d73 7479 7065 2e66  dtype = mstype.f
-00023480: 6c6f 6174 3136 0a20 2020 2020 2020 2020  loat16.         
-00023490: 2020 2073 656c 662e 6b65 795f 7061 7374     self.key_past
-000234a0: 203d 204e 6f6e 650a 2020 2020 2020 2020   = None.        
-000234b0: 2020 2020 7365 6c66 2e76 616c 7565 5f70      self.value_p
-000234c0: 6173 7420 3d20 4e6f 6e65 0a20 2020 2020  ast = None.     
-000234d0: 2020 2020 2020 2069 6620 7365 6c66 2e75         if self.u
-000234e0: 7365 5f70 6173 743a 0a20 2020 2020 2020  se_past:.       
-000234f0: 2020 2020 2020 2020 2023 206f 7065 7261           # opera
-00023500: 746f 7220 7573 6564 2066 6f72 2073 7461  tor used for sta
-00023510: 7465 2072 6575 7365 0a20 2020 2020 2020  te reuse.       
-00023520: 2020 2020 2020 2020 2073 656c 662e 7265           self.re
-00023530: 6475 6365 7375 6d20 3d20 502e 5265 6475  ducesum = P.Redu
-00023540: 6365 5375 6d28 292e 7368 6172 6428 2828  ceSum().shard(((
-00023550: 312c 2031 2c20 312c 2031 292c 2929 0a20  1, 1, 1, 1),)). 
-00023560: 2020 2020 2020 2020 2020 2020 2020 2073                 s
-00023570: 656c 662e 6e6f 745f 6571 7561 6c20 3d20  elf.not_equal = 
-00023580: 502e 4e6f 7445 7175 616c 2829 2e73 6861  P.NotEqual().sha
-00023590: 7264 2828 2831 2c20 312c 2031 2c20 3129  rd(((1, 1, 1, 1)
-000235a0: 2c20 2829 2929 0a20 2020 2020 2020 2020  , ())).         
-000235b0: 2020 2020 2020 2073 656c 662e 736c 6963         self.slic
-000235c0: 6520 3d20 502e 5374 7269 6465 6453 6c69  e = P.StridedSli
-000235d0: 6365 2829 2e73 6861 7264 2828 2831 2c20  ce().shard(((1, 
-000235e0: 312c 2031 2c20 3129 2c29 290a 2020 2020  1, 1, 1),)).    
-000235f0: 2020 2020 2020 2020 2020 2020 7369 7a65              size
-00023600: 5f70 6572 5f68 6561 6420 3d20 6869 6464  _per_head = hidd
-00023610: 656e 5f73 697a 6520 2f2f 206e 756d 5f68  en_size // num_h
-00023620: 6561 6473 0a20 2020 2020 2020 2020 2020  eads.           
-00023630: 2020 2020 2073 656c 662e 6b65 795f 7368       self.key_sh
-00023640: 6170 6520 3d20 2862 6174 6368 5f73 697a  ape = (batch_siz
-00023650: 652c 206e 756d 5f68 6561 6473 2c20 7369  e, num_heads, si
-00023660: 7a65 5f70 6572 5f68 6561 642c 2074 6774  ze_per_head, tgt
-00023670: 5f73 6571 5f6c 656e 6774 6829 0a20 2020  _seq_length).   
-00023680: 2020 2020 2020 2020 2020 2020 2073 656c               sel
-00023690: 662e 7661 6c75 655f 7368 6170 6520 3d20  f.value_shape = 
-000236a0: 2862 6174 6368 5f73 697a 652c 206e 756d  (batch_size, num
-000236b0: 5f68 6561 6473 2c20 7467 745f 7365 715f  _heads, tgt_seq_
-000236c0: 6c65 6e67 7468 2c20 7369 7a65 5f70 6572  length, size_per
-000236d0: 5f68 6561 6429 0a20 2020 2020 2020 2020  _head).         
-000236e0: 2020 2020 2020 2023 2070 6172 616d 6574         # paramet
-000236f0: 6572 7320 7361 7669 6e67 206b 6579 2061  ers saving key a
-00023700: 6e64 2076 616c 7565 2073 7461 7465 730a  nd value states.
+00023310: 2020 2020 2020 2020 2020 2020 2020 2070                 p
+00023320: 6172 616c 6c65 6c5f 636f 6e66 6967 3d63  arallel_config=c
+00023330: 6f6e 6669 675f 746f 5f61 7474 656e 7469  onfig_to_attenti
+00023340: 6f6e 290a 2020 2020 2020 2020 2020 2020  on).            
+00023350: 7365 6c66 2e63 726f 7373 5f61 7474 656e  self.cross_atten
+00023360: 7469 6f6e 5f6c 6179 6572 6e6f 726d 203d  tion_layernorm =
+00023370: 204c 6179 6572 4e6f 726d 2828 6869 6464   LayerNorm((hidd
+00023380: 656e 5f73 697a 652c 2929 2e74 6f5f 666c  en_size,)).to_fl
+00023390: 6f61 7428 0a20 2020 2020 2020 2020 2020  oat(.           
+000233a0: 2020 2020 206c 6179 6572 6e6f 726d 5f63       layernorm_c
+000233b0: 6f6d 7075 7465 5f74 7970 6529 0a20 2020  ompute_type).   
+000233c0: 2020 2020 2020 2020 2073 656c 662e 6372           self.cr
+000233d0: 6f73 735f 6174 7465 6e74 696f 6e5f 6c61  oss_attention_la
+000233e0: 7965 726e 6f72 6d2e 7368 6172 6428 2828  yernorm.shard(((
+000233f0: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
+00023400: 6461 7461 5f70 6172 616c 6c65 6c2c 2031  data_parallel, 1
+00023410: 292c 2929 0a0a 2020 2020 2020 2020 2020  ),))..          
+00023420: 2020 6966 2073 656c 662e 7573 655f 6d6f    if self.use_mo
+00023430: 653a 0a20 2020 2020 2020 2020 2020 2020  e:.             
+00023440: 2020 2073 656c 662e 6f75 7470 7574 203d     self.output =
+00023450: 204d 6f45 2868 6964 6465 6e5f 7369 7a65   MoE(hidden_size
+00023460: 3d68 6964 6465 6e5f 7369 7a65 2c0a 2020  =hidden_size,.  
+00023470: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00023480: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00023490: 6472 6f70 6f75 745f 7261 7465 3d68 6964  dropout_rate=hid
+000234a0: 6465 6e5f 6472 6f70 6f75 745f 7261 7465  den_dropout_rate
+000234b0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+000234c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000234d0: 2020 2020 6666 6e5f 6869 6464 656e 5f73      ffn_hidden_s
+000234e0: 697a 653d 6666 6e5f 6869 6464 656e 5f73  ize=ffn_hidden_s
+000234f0: 697a 652c 0a20 2020 2020 2020 2020 2020  ize,.           
+00023500: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00023510: 2020 2020 2020 2070 6172 616d 5f69 6e69         param_ini
+00023520: 745f 7479 7065 3d70 6172 616d 5f69 6e69  t_type=param_ini
+00023530: 745f 7479 7065 2c0a 2020 2020 2020 2020  t_type,.        
+00023540: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00023550: 2020 2020 2020 2020 2020 6869 6464 656e            hidden
+00023560: 5f61 6374 3d68 6964 6465 6e5f 6163 742c  _act=hidden_act,
+00023570: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00023580: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00023590: 2020 206d 6f65 5f63 6f6e 6669 673d 6d6f     moe_config=mo
+000235a0: 655f 636f 6e66 6967 2c0a 2020 2020 2020  e_config,.      
+000235b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000235c0: 2020 2020 2020 2020 2020 2020 7061 7261              para
+000235d0: 6c6c 656c 5f63 6f6e 6669 673d 7061 7261  llel_config=para
+000235e0: 6c6c 656c 5f63 6f6e 6669 6729 0a20 2020  llel_config).   
+000235f0: 2020 2020 2020 2020 2065 6c73 653a 0a20           else:. 
+00023600: 2020 2020 2020 2020 2020 2020 2020 2023                 #
+00023610: 2046 6565 6420 466f 7277 6172 6420 4e65   Feed Forward Ne
+00023620: 7477 6f72 6b2c 2046 464e 0a20 2020 2020  twork, FFN.     
+00023630: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+00023640: 6f75 7470 7574 203d 2046 6565 6446 6f72  output = FeedFor
+00023650: 7761 7264 2868 6964 6465 6e5f 7369 7a65  ward(hidden_size
+00023660: 3d68 6964 6465 6e5f 7369 7a65 2c0a 2020  =hidden_size,.  
+00023670: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00023680: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00023690: 2020 2020 2020 2020 6472 6f70 6f75 745f          dropout_
+000236a0: 7261 7465 3d68 6964 6465 6e5f 6472 6f70  rate=hidden_drop
+000236b0: 6f75 745f 7261 7465 2c0a 2020 2020 2020  out_rate,.      
+000236c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000236d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000236e0: 2020 2020 6666 6e5f 6869 6464 656e 5f73      ffn_hidden_s
+000236f0: 697a 653d 6666 6e5f 6869 6464 656e 5f73  ize=ffn_hidden_s
+00023700: 697a 652c 0a20 2020 2020 2020 2020 2020  ize,.           
 00023710: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00023720: 7365 6c66 2e6b 6579 5f70 6173 7420 3d20  self.key_past = 
-00023730: 5061 7261 6d65 7465 7228 5465 6e73 6f72  Parameter(Tensor
-00023740: 286e 702e 7a65 726f 7328 7368 6170 653d  (np.zeros(shape=
-00023750: 7365 6c66 2e6b 6579 5f73 6861 7065 292c  self.key_shape),
-00023760: 2073 656c 662e 6474 7970 6529 2c20 6e61   self.dtype), na
-00023770: 6d65 3d22 6b65 795f 7061 7374 2229 0a20  me="key_past"). 
-00023780: 2020 2020 2020 2020 2020 2020 2020 2073                 s
-00023790: 656c 662e 7661 6c75 655f 7061 7374 203d  elf.value_past =
-000237a0: 2050 6172 616d 6574 6572 2854 656e 736f   Parameter(Tenso
-000237b0: 7228 6e70 2e7a 6572 6f73 2873 6861 7065  r(np.zeros(shape
-000237c0: 3d73 656c 662e 7661 6c75 655f 7368 6170  =self.value_shap
-000237d0: 6529 2c20 7365 6c66 2e64 7479 7065 292c  e), self.dtype),
-000237e0: 206e 616d 653d 2276 616c 7565 5f70 6173   name="value_pas
-000237f0: 7422 290a 2020 2020 2020 2020 2020 2020  t").            
-00023800: 2020 2020 7365 6c66 2e74 696c 6520 3d20      self.tile = 
-00023810: 502e 5469 6c65 2829 2e73 6861 7264 2828  P.Tile().shard((
-00023820: 2831 2c20 3129 2c29 290a 2020 2020 2020  (1, 1),)).      
-00023830: 2020 2020 2020 2020 2020 7365 6c66 2e6d            self.m
-00023840: 756c 203d 2050 2e4d 756c 2829 2e73 6861  ul = P.Mul().sha
-00023850: 7264 2828 2831 2c20 312c 2031 2c20 3129  rd(((1, 1, 1, 1)
-00023860: 2c20 2831 2c29 2929 0a20 2020 2020 2020  , (1,))).       
-00023870: 2020 2020 2020 2020 2073 656c 662e 6173           self.as
-00023880: 7369 676e 203d 2050 2e41 7373 6967 6e28  sign = P.Assign(
-00023890: 292e 7368 6172 6428 2828 312c 2031 2c20  ).shard(((1, 1, 
-000238a0: 312c 2031 292c 2028 312c 2031 2c20 312c  1, 1), (1, 1, 1,
-000238b0: 2031 2929 290a 0a20 2020 2020 2020 2020   1)))..         
-000238c0: 2020 2069 6620 7061 7261 6c6c 656c 5f63     if parallel_c
-000238d0: 6f6e 6669 672e 7573 655f 7365 715f 7061  onfig.use_seq_pa
-000238e0: 7261 6c6c 656c 3a0a 2020 2020 2020 2020  rallel:.        
-000238f0: 2020 2020 2020 2020 7365 6c66 2e61 6464          self.add
-00023900: 2e73 6861 7264 2828 2870 6172 616c 6c65  .shard(((paralle
-00023910: 6c5f 636f 6e66 6967 2e64 6174 615f 7061  l_config.data_pa
-00023920: 7261 6c6c 656c 202a 2070 6172 616c 6c65  rallel * paralle
-00023930: 6c5f 636f 6e66 6967 2e6d 6f64 656c 5f70  l_config.model_p
-00023940: 6172 616c 6c65 6c2c 2031 292c 0a20 2020  arallel, 1),.   
-00023950: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00023960: 2020 2020 2020 2020 2020 2020 2028 7061               (pa
-00023970: 7261 6c6c 656c 5f63 6f6e 6669 672e 6461  rallel_config.da
-00023980: 7461 5f70 6172 616c 6c65 6c20 2a20 7061  ta_parallel * pa
-00023990: 7261 6c6c 656c 5f63 6f6e 6669 672e 6d6f  rallel_config.mo
-000239a0: 6465 6c5f 7061 7261 6c6c 656c 2c20 3129  del_parallel, 1)
-000239b0: 2929 0a20 2020 2020 2020 2020 2020 2020  )).             
-000239c0: 2020 2073 656c 662e 6c61 7965 726e 6f72     self.layernor
-000239d0: 6d31 2e73 6861 7264 2828 2870 6172 616c  m1.shard(((paral
-000239e0: 6c65 6c5f 636f 6e66 6967 2e64 6174 615f  lel_config.data_
-000239f0: 7061 7261 6c6c 656c 202a 2070 6172 616c  parallel * paral
-00023a00: 6c65 6c5f 636f 6e66 6967 2e6d 6f64 656c  lel_config.model
-00023a10: 5f70 6172 616c 6c65 6c2c 2031 292c 2929  _parallel, 1),))
-00023a20: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00023a30: 2073 656c 662e 6c61 7965 726e 6f72 6d32   self.layernorm2
-00023a40: 2e73 6861 7264 2828 2870 6172 616c 6c65  .shard(((paralle
-00023a50: 6c5f 636f 6e66 6967 2e64 6174 615f 7061  l_config.data_pa
-00023a60: 7261 6c6c 656c 202a 2070 6172 616c 6c65  rallel * paralle
-00023a70: 6c5f 636f 6e66 6967 2e6d 6f64 656c 5f70  l_config.model_p
-00023a80: 6172 616c 6c65 6c2c 2031 292c 2929 0a20  arallel, 1),)). 
-00023a90: 2020 2020 2020 2020 2020 2020 2020 2069                 i
-00023aa0: 6620 7061 7261 6c6c 656c 5f63 6f6e 6669  f parallel_confi
-00023ab0: 672e 7265 636f 6d70 7574 652e 7365 6c65  g.recompute.sele
-00023ac0: 6374 5f72 6563 6f6d 7075 7465 3a0a 2020  ct_recompute:.  
-00023ad0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00023ae0: 2020 2320 e6ad a4e5 a484 e4bc 9ae6 b688    # ............
-00023af0: e880 97e8 be83 e5a4 a7e5 8685 e5ad 98ef  ................
-00023b00: bc8c e5bc 80e5 90af e590 8ee4 bc9a e68d  ................
-00023b10: 9fe5 a4b1 e4b8 80e9 83a8 e588 86e8 aea1  ................
-00023b20: e7ae 97e6 80a7 e883 bd0a 2020 2020 2020  ..........      
-00023b30: 2020 2020 2020 2020 2020 2020 2020 7365                se
-00023b40: 6c66 2e6c 6179 6572 6e6f 726d 322e 6c61  lf.layernorm2.la
-00023b50: 7965 725f 6e6f 726d 2e72 6563 6f6d 7075  yer_norm.recompu
-00023b60: 7465 2829 0a20 2020 2020 2020 2020 2020  te().           
-00023b70: 2020 2020 2069 6620 6e6f 7420 7365 6c66       if not self
-00023b80: 2e75 7365 5f6d 6f65 3a0a 2020 2020 2020  .use_moe:.      
-00023b90: 2020 2020 2020 2020 2020 2020 2020 7365                se
-00023ba0: 6c66 2e6f 7574 7075 742e 7072 6f6a 6563  lf.output.projec
-00023bb0: 7469 6f6e 2e73 6861 7264 280a 2020 2020  tion.shard(.    
-00023bc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00023bd0: 2020 2020 7374 7261 7465 6779 5f62 6961      strategy_bia
-00023be0: 733d 2828 7061 7261 6c6c 656c 5f63 6f6e  s=((parallel_con
-00023bf0: 6669 672e 6461 7461 5f70 6172 616c 6c65  fig.data_paralle
-00023c00: 6c20 2a20 7061 7261 6c6c 656c 5f63 6f6e  l * parallel_con
-00023c10: 6669 672e 6d6f 6465 6c5f 7061 7261 6c6c  fig.model_parall
-00023c20: 656c 2c20 3129 2c20 2831 2c29 292c 0a20  el, 1), (1,)),. 
-00023c30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00023c40: 2020 2020 2020 2073 7472 6174 6567 795f         strategy_
-00023c50: 6d61 746d 756c 3d28 2870 6172 616c 6c65  matmul=((paralle
-00023c60: 6c5f 636f 6e66 6967 2e64 6174 615f 7061  l_config.data_pa
-00023c70: 7261 6c6c 656c 2c20 7061 7261 6c6c 656c  rallel, parallel
-00023c80: 5f63 6f6e 6669 672e 6d6f 6465 6c5f 7061  _config.model_pa
-00023c90: 7261 6c6c 656c 292c 0a20 2020 2020 2020  rallel),.       
-00023ca0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00023cb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00023cc0: 2020 2870 6172 616c 6c65 6c5f 636f 6e66    (parallel_conf
-00023cd0: 6967 2e6d 6f64 656c 5f70 6172 616c 6c65  ig.model_paralle
-00023ce0: 6c2c 2031 2929 2c0a 2020 2020 2020 2020  l, 1)),.        
-00023cf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00023d00: 6f75 745f 7374 7261 7465 6779 5f6d 6174  out_strategy_mat
-00023d10: 6d75 6c3d 2828 7061 7261 6c6c 656c 5f63  mul=((parallel_c
-00023d20: 6f6e 6669 672e 6461 7461 5f70 6172 616c  onfig.data_paral
-00023d30: 6c65 6c20 2a20 7061 7261 6c6c 656c 5f63  lel * parallel_c
-00023d40: 6f6e 6669 672e 6d6f 6465 6c5f 7061 7261  onfig.model_para
-00023d50: 6c6c 656c 2c20 3129 2c29 290a 2020 2020  llel, 1),)).    
-00023d60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00023d70: 7365 6c66 2e6f 7574 7075 742e 6472 6f70  self.output.drop
-00023d80: 6f75 742e 6472 6f70 6f75 742e 7368 6172  out.dropout.shar
-00023d90: 6428 0a20 2020 2020 2020 2020 2020 2020  d(.             
-00023da0: 2020 2020 2020 2020 2020 2028 2870 6172             ((par
-00023db0: 616c 6c65 6c5f 636f 6e66 6967 2e64 6174  allel_config.dat
-00023dc0: 615f 7061 7261 6c6c 656c 202a 2070 6172  a_parallel * par
-00023dd0: 616c 6c65 6c5f 636f 6e66 6967 2e6d 6f64  allel_config.mod
-00023de0: 656c 5f70 6172 616c 6c65 6c2c 2031 292c  el_parallel, 1),
-00023df0: 2929 0a20 2020 2020 2020 2065 6c73 653a  )).        else:
-00023e00: 0a20 2020 2020 2020 2020 2020 2072 6169  .            rai
-00023e10: 7365 2052 756e 7469 6d65 4572 726f 7228  se RuntimeError(
-00023e20: 6622 5468 6520 7b73 656c 662e 636c 735f  f"The {self.cls_
-00023e30: 6e61 6d65 7d20 6f6e 6c79 2073 7570 706f  name} only suppo
-00023e40: 7274 2073 6861 7264 696e 6720 7072 6f70  rt sharding prop
-00023e50: 6167 6174 696f 6e20 6f72 2022 0a20 2020  agation or ".   
-00023e60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00023e70: 2020 2020 2020 2020 2020 2020 6622 7365              f"se
-00023e80: 6d69 2d61 7574 6f20 7061 7261 6c6c 656c  mi-auto parallel
-00023e90: 206d 6f64 6520 6e6f 772e 2229 0a0a 2020   mode now.")..  
-00023ea0: 2020 6465 6620 636f 6e73 7472 7563 7428    def construct(
-00023eb0: 7365 6c66 2c20 6869 6464 656e 5f73 7461  self, hidden_sta
-00023ec0: 7473 2c0a 2020 2020 2020 2020 2020 2020  ts,.            
-00023ed0: 2020 2020 2020 6465 636f 6465 725f 6d61        decoder_ma
-00023ee0: 736b 2c0a 2020 2020 2020 2020 2020 2020  sk,.            
-00023ef0: 2020 2020 2020 656e 636f 6465 725f 6f75        encoder_ou
-00023f00: 7470 7574 3d4e 6f6e 652c 0a20 2020 2020  tput=None,.     
-00023f10: 2020 2020 2020 2020 2020 2020 206d 656d               mem
-00023f20: 6f72 795f 6d61 736b 3d4e 6f6e 652c 0a20  ory_mask=None,. 
-00023f30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00023f40: 2069 6e69 745f 7265 7365 743d 5472 7565   init_reset=True
-00023f50: 2c20 6261 7463 685f 7661 6c69 645f 6c65  , batch_valid_le
-00023f60: 6e67 7468 3d4e 6f6e 6529 3a0a 2020 2020  ngth=None):.    
-00023f70: 2020 2020 2222 2266 6f72 7761 7264 2070      """forward p
-00023f80: 726f 6365 7373 2222 220a 2020 2020 2020  rocess""".      
-00023f90: 2020 7365 6c66 2e5f 6368 6563 6b5f 696e    self._check_in
-00023fa0: 7075 7428 6869 6464 656e 5f73 7461 7473  put(hidden_stats
-00023fb0: 2c20 6465 636f 6465 725f 6d61 736b 2c20  , decoder_mask, 
-00023fc0: 656e 636f 6465 725f 6f75 7470 7574 2c20  encoder_output, 
-00023fd0: 6d65 6d6f 7279 5f6d 6173 6b2c 2069 6e69  memory_mask, ini
-00023fe0: 745f 7265 7365 742c 2062 6174 6368 5f76  t_reset, batch_v
-00023ff0: 616c 6964 5f6c 656e 6774 6829 0a20 2020  alid_length).   
-00024000: 2020 2020 2023 2074 6865 2072 6574 7572       # the retur
-00024010: 6e65 6420 7368 6170 6520 6973 205b 6273  ned shape is [bs
-00024020: 2c20 7365 715f 6c65 6e67 7468 2c20 656d  , seq_length, em
-00024030: 6265 6464 696e 675f 7369 7a65 5d20 6f72  bedding_size] or
-00024040: 205b 6273 202a 2073 6571 5f6c 656e 6774   [bs * seq_lengt
-00024050: 682c 2065 6d62 6564 6469 6e67 5f73 697a  h, embedding_siz
-00024060: 655d 0a20 2020 2020 2020 2068 6964 6465  e].        hidde
-00024070: 6e5f 7368 6170 6520 3d20 462e 7368 6170  n_shape = F.shap
-00024080: 6528 6869 6464 656e 5f73 7461 7473 290a  e(hidden_stats).
-00024090: 2020 2020 2020 2020 6869 6464 656e 5f73          hidden_s
-000240a0: 7461 7473 203d 2046 2e72 6573 6861 7065  tats = F.reshape
-000240b0: 2868 6964 6465 6e5f 7374 6174 732c 2028  (hidden_stats, (
-000240c0: 2d31 2c20 6869 6464 656e 5f73 6861 7065  -1, hidden_shape
-000240d0: 5b2d 315d 2929 0a20 2020 2020 2020 2069  [-1])).        i
-000240e0: 6e70 7574 5f78 203d 2073 656c 662e 6c61  nput_x = self.la
-000240f0: 7965 726e 6f72 6d31 2868 6964 6465 6e5f  yernorm1(hidden_
-00024100: 7374 6174 7329 0a20 2020 2020 2020 2069  stats).        i
-00024110: 6e70 7574 5f78 203d 2046 2e63 6173 7428  nput_x = F.cast(
-00024120: 696e 7075 745f 782c 2073 656c 662e 6474  input_x, self.dt
-00024130: 7970 6529 0a0a 2020 2020 2020 2020 2320  ype)..        # 
-00024140: 696e 6469 6361 7465 2077 6865 7468 6572  indicate whether
-00024150: 2072 6573 6574 2073 6176 6564 2073 7461   reset saved sta
-00024160: 7465 730a 2020 2020 2020 2020 6b65 795f  tes.        key_
-00024170: 7265 7365 7420 3d20 4e6f 6e65 0a20 2020  reset = None.   
-00024180: 2020 2020 2076 616c 7565 5f72 6573 6574       value_reset
-00024190: 203d 204e 6f6e 650a 2020 2020 2020 2020   = None.        
-000241a0: 6966 2073 656c 662e 7573 655f 7061 7374  if self.use_past
-000241b0: 3a0a 2020 2020 2020 2020 2020 2020 2320  :.            # 
-000241c0: 7265 7365 7420 7374 6174 6573 2c20 696e  reset states, in
-000241d0: 6974 5f72 6573 6574 2054 7275 6520 666f  it_reset True fo
-000241e0: 7220 7265 7573 6520 616e 6420 4661 6c73  r reuse and Fals
-000241f0: 6520 666f 7220 7265 7365 740a 2020 2020  e for reset.    
-00024200: 2020 2020 2020 2020 7365 6c66 2e61 7373          self.ass
-00024210: 6967 6e28 7365 6c66 2e6b 6579 5f70 6173  ign(self.key_pas
-00024220: 742c 2073 656c 662e 6d75 6c28 7365 6c66  t, self.mul(self
-00024230: 2e6b 6579 5f70 6173 742c 2046 2e63 6173  .key_past, F.cas
-00024240: 7428 696e 6974 5f72 6573 6574 2c20 7365  t(init_reset, se
-00024250: 6c66 2e64 7479 7065 2929 290a 2020 2020  lf.dtype))).    
-00024260: 2020 2020 2020 2020 6b65 795f 7265 7365          key_rese
-00024270: 7420 3d20 7365 6c66 2e6b 6579 5f70 6173  t = self.key_pas
-00024280: 740a 2020 2020 2020 2020 2020 2020 7365  t.            se
-00024290: 6c66 2e61 7373 6967 6e28 7365 6c66 2e76  lf.assign(self.v
-000242a0: 616c 7565 5f70 6173 742c 2073 656c 662e  alue_past, self.
-000242b0: 6d75 6c28 7365 6c66 2e76 616c 7565 5f70  mul(self.value_p
-000242c0: 6173 742c 2046 2e63 6173 7428 696e 6974  ast, F.cast(init
-000242d0: 5f72 6573 6574 2c20 7365 6c66 2e64 7479  _reset, self.dty
-000242e0: 7065 2929 290a 2020 2020 2020 2020 2020  pe))).          
-000242f0: 2020 7661 6c75 655f 7265 7365 7420 3d20    value_reset = 
-00024300: 7365 6c66 2e76 616c 7565 5f70 6173 740a  self.value_past.
-00024310: 2020 2020 2020 2020 2020 2020 2320 6164              # ad
-00024320: 6420 6465 7065 6e64 656e 6379 2066 6f72  d dependency for
-00024330: 2064 6573 6972 6564 2065 7865 6375 7469   desired executi
-00024340: 6f6e 206f 7264 6572 0a20 2020 2020 2020  on order.       
-00024350: 2020 2020 2069 6e70 7574 5f78 203d 2046       input_x = F
-00024360: 2e64 6570 656e 6428 696e 7075 745f 782c  .depend(input_x,
-00024370: 206b 6579 5f72 6573 6574 290a 2020 2020   key_reset).    
-00024380: 2020 2020 2020 2020 696e 7075 745f 7820          input_x 
-00024390: 3d20 462e 6465 7065 6e64 2869 6e70 7574  = F.depend(input
-000243a0: 5f78 2c20 7661 6c75 655f 7265 7365 7429  _x, value_reset)
-000243b0: 0a0a 2020 2020 2020 2020 6174 7465 6e74  ..        attent
-000243c0: 696f 6e2c 206c 6179 6572 5f70 7265 7365  ion, layer_prese
-000243d0: 6e74 203d 2073 656c 662e 6174 7465 6e74  nt = self.attent
-000243e0: 696f 6e28 696e 7075 745f 782c 2069 6e70  ion(input_x, inp
-000243f0: 7574 5f78 2c20 696e 7075 745f 782c 2064  ut_x, input_x, d
-00024400: 6563 6f64 6572 5f6d 6173 6b2c 2073 656c  ecoder_mask, sel
-00024410: 662e 6b65 795f 7061 7374 2c0a 2020 2020  f.key_past,.    
-00024420: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00024430: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00024440: 2020 2020 2020 2020 2020 2020 2020 7365                se
-00024450: 6c66 2e76 616c 7565 5f70 6173 742c 2062  lf.value_past, b
-00024460: 6174 6368 5f76 616c 6964 5f6c 656e 6774  atch_valid_lengt
-00024470: 6829 0a20 2020 2020 2020 2023 2046 6f72  h).        # For
-00024480: 2070 6f73 742d 6c61 7965 726e 6f72 6d20   post-layernorm 
-00024490: 7468 6520 696e 7075 7473 2066 6f72 2072  the inputs for r
-000244a0: 6573 6964 7561 6c20 7061 7468 2061 7265  esidual path are
-000244b0: 206f 7574 7075 7420 6f66 2073 656c 662d   output of self-
-000244c0: 6174 7465 6e74 696f 6e20 616e 6420 6f75  attention and ou
-000244d0: 7470 7574 206f 6620 6c61 7965 726e 6f72  tput of layernor
-000244e0: 6d0a 2020 2020 2020 2020 6966 2073 656c  m.        if sel
-000244f0: 662e 706f 7374 5f6c 6179 6572 6e6f 726d  f.post_layernorm
-00024500: 5f72 6573 6964 7561 6c3a 0a20 2020 2020  _residual:.     
-00024510: 2020 2020 2020 2078 203d 2073 656c 662e         x = self.
-00024520: 6164 6428 696e 7075 745f 782c 2061 7474  add(input_x, att
-00024530: 656e 7469 6f6e 290a 2020 2020 2020 2020  ention).        
-00024540: 2320 466f 7220 7072 652d 6c61 7965 726e  # For pre-layern
-00024550: 6f72 6d20 7468 6520 696e 7075 7473 2066  orm the inputs f
-00024560: 6f72 2072 6573 6964 7561 6c20 7061 7468  or residual path
-00024570: 2061 7265 206f 7574 7075 7420 6f66 2073   are output of s
-00024580: 656c 662d 6174 7465 6e74 696f 6e20 616e  elf-attention an
-00024590: 6420 696e 7075 7420 6f66 2074 6869 7320  d input of this 
-000245a0: 6c61 7965 720a 2020 2020 2020 2020 656c  layer.        el
-000245b0: 7365 3a0a 2020 2020 2020 2020 2020 2020  se:.            
-000245c0: 7820 3d20 7365 6c66 2e61 6464 2868 6964  x = self.add(hid
-000245d0: 6465 6e5f 7374 6174 732c 2061 7474 656e  den_stats, atten
-000245e0: 7469 6f6e 290a 0a20 2020 2020 2020 206d  tion)..        m
-000245f0: 6964 646c 655f 6f75 7470 7574 203d 204e  iddle_output = N
-00024600: 6f6e 650a 2020 2020 2020 2020 6966 2065  one.        if e
-00024610: 6e63 6f64 6572 5f6f 7574 7075 7420 6973  ncoder_output is
-00024620: 206e 6f74 204e 6f6e 653a 0a20 2020 2020   not None:.     
-00024630: 2020 2020 2020 206d 6964 646c 655f 6f75         middle_ou
-00024640: 7470 7574 203d 2073 656c 662e 6372 6f73  tput = self.cros
-00024650: 735f 6174 7465 6e74 696f 6e5f 6c61 7965  s_attention_laye
-00024660: 726e 6f72 6d28 7829 0a20 2020 2020 2020  rnorm(x).       
-00024670: 2020 2020 206d 6964 646c 655f 6f75 7470       middle_outp
-00024680: 7574 203d 2046 2e63 6173 7428 6d69 6464  ut = F.cast(midd
-00024690: 6c65 5f6f 7574 7075 742c 2073 656c 662e  le_output, self.
-000246a0: 6474 7970 6529 0a20 2020 2020 2020 2020  dtype).         
-000246b0: 2020 2065 6e63 6f64 6572 5f6f 7574 7075     encoder_outpu
-000246c0: 7420 3d20 462e 6361 7374 2865 6e63 6f64  t = F.cast(encod
-000246d0: 6572 5f6f 7574 7075 742c 2073 656c 662e  er_output, self.
-000246e0: 6474 7970 6529 0a20 2020 2020 2020 2020  dtype).         
-000246f0: 2020 2063 726f 7373 5f61 7474 6e5f 6f75     cross_attn_ou
-00024700: 7470 7574 2c20 6372 6f73 735f 6c61 7965  tput, cross_laye
-00024710: 725f 7072 6573 656e 7420 3d20 7365 6c66  r_present = self
-00024720: 2e63 726f 7373 5f61 7474 656e 7469 6f6e  .cross_attention
-00024730: 286d 6964 646c 655f 6f75 7470 7574 2c20  (middle_output, 
-00024740: 656e 636f 6465 725f 6f75 7470 7574 2c0a  encoder_output,.
-00024750: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00024760: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00024770: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00024780: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00024790: 2020 2020 2020 2020 2020 656e 636f 6465            encode
-000247a0: 725f 6f75 7470 7574 2c0a 2020 2020 2020  r_output,.      
-000247b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000247c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000247d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000247e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000247f0: 2020 2020 6d65 6d6f 7279 5f6d 6173 6b2c      memory_mask,
-00024800: 2073 656c 662e 6b65 795f 7061 7374 2c0a   self.key_past,.
-00024810: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00024820: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00024830: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00024840: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00024850: 2020 2020 2020 2020 2020 7365 6c66 2e76            self.v
-00024860: 616c 7565 5f70 6173 742c 2062 6174 6368  alue_past, batch
-00024870: 5f76 616c 6964 5f6c 656e 6774 6829 0a20  _valid_length). 
-00024880: 2020 2020 2020 2020 2020 206c 6179 6572             layer
-00024890: 5f70 7265 7365 6e74 202b 3d20 6372 6f73  _present += cros
-000248a0: 735f 6c61 7965 725f 7072 6573 656e 740a  s_layer_present.
-000248b0: 2020 2020 2020 2020 2020 2020 6966 2073              if s
-000248c0: 656c 662e 706f 7374 5f6c 6179 6572 6e6f  elf.post_layerno
-000248d0: 726d 5f72 6573 6964 7561 6c3a 0a20 2020  rm_residual:.   
-000248e0: 2020 2020 2020 2020 2020 2020 2078 203d               x =
-000248f0: 2073 656c 662e 6164 6428 6d69 6464 6c65   self.add(middle
-00024900: 5f6f 7574 7075 742c 2063 726f 7373 5f61  _output, cross_a
-00024910: 7474 6e5f 6f75 7470 7574 290a 2020 2020  ttn_output).    
-00024920: 2020 2020 2020 2020 656c 7365 3a0a 2020          else:.  
-00024930: 2020 2020 2020 2020 2020 2020 2020 7820                x 
-00024940: 3d20 7365 6c66 2e61 6464 2878 2c20 6372  = self.add(x, cr
-00024950: 6f73 735f 6174 746e 5f6f 7574 7075 7429  oss_attn_output)
-00024960: 0a0a 2020 2020 2020 2020 6f75 7470 7574  ..        output
-00024970: 5f78 203d 2073 656c 662e 6c61 7965 726e  _x = self.layern
-00024980: 6f72 6d32 2878 290a 2020 2020 2020 2020  orm2(x).        
-00024990: 6f75 7470 7574 5f78 203d 2046 2e63 6173  output_x = F.cas
-000249a0: 7428 6f75 7470 7574 5f78 2c20 7365 6c66  t(output_x, self
-000249b0: 2e64 7479 7065 290a 2020 2020 2020 2020  .dtype).        
-000249c0: 6175 785f 6c6f 7373 203d 204e 6f6e 650a  aux_loss = None.
-000249d0: 2020 2020 2020 2020 6966 2073 656c 662e          if self.
-000249e0: 7573 655f 6d6f 653a 0a20 2020 2020 2020  use_moe:.       
-000249f0: 2020 2020 206d 6c70 5f6c 6f67 6974 2c20       mlp_logit, 
-00024a00: 6175 785f 6c6f 7373 203d 2073 656c 662e  aux_loss = self.
-00024a10: 6f75 7470 7574 286f 7574 7075 745f 7829  output(output_x)
-00024a20: 0a20 2020 2020 2020 2065 6c73 653a 0a20  .        else:. 
-00024a30: 2020 2020 2020 2020 2020 206d 6c70 5f6c             mlp_l
-00024a40: 6f67 6974 203d 2073 656c 662e 6f75 7470  ogit = self.outp
-00024a50: 7574 286f 7574 7075 745f 7829 0a0a 2020  ut(output_x)..  
-00024a60: 2020 2020 2020 7661 6c75 655f 7570 6461        value_upda
-00024a70: 7465 203d 204e 6f6e 650a 2020 2020 2020  te = None.      
-00024a80: 2020 6b65 795f 7570 6461 7465 203d 204e    key_update = N
-00024a90: 6f6e 650a 2020 2020 2020 2020 6966 2073  one.        if s
-00024aa0: 656c 662e 7573 655f 7061 7374 3a0a 2020  elf.use_past:.  
-00024ab0: 2020 2020 2020 2020 2020 2320 6375 7272            # curr
-00024ac0: 656e 7420 6b65 7920 616e 6420 7661 6c75  ent key and valu
-00024ad0: 650a 2020 2020 2020 2020 2020 2020 6b65  e.            ke
-00024ae0: 795f 7072 6573 656e 742c 2076 616c 7565  y_present, value
-00024af0: 5f70 7265 7365 6e74 203d 206c 6179 6572  _present = layer
-00024b00: 5f70 7265 7365 6e74 0a20 2020 2020 2020  _present.       
-00024b10: 2020 2020 2023 2075 7064 6174 6520 6b65       # update ke
-00024b20: 7920 616e 6420 7661 6c75 6520 6361 6c63  y and value calc
-00024b30: 756c 6174 6564 2074 6869 7320 7374 6570  ulated this step
-00024b40: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-00024b50: 662e 6173 7369 676e 2873 656c 662e 6b65  f.assign(self.ke
-00024b60: 795f 7061 7374 2c20 6b65 795f 7072 6573  y_past, key_pres
-00024b70: 656e 7429 0a20 2020 2020 2020 2020 2020  ent).           
-00024b80: 206b 6579 5f75 7064 6174 6520 3d20 7365   key_update = se
-00024b90: 6c66 2e6b 6579 5f70 6173 740a 2020 2020  lf.key_past.    
-00024ba0: 2020 2020 2020 2020 7365 6c66 2e61 7373          self.ass
-00024bb0: 6967 6e28 7365 6c66 2e76 616c 7565 5f70  ign(self.value_p
-00024bc0: 6173 742c 2076 616c 7565 5f70 7265 7365  ast, value_prese
-00024bd0: 6e74 290a 2020 2020 2020 2020 2020 2020  nt).            
-00024be0: 7661 6c75 655f 7570 6461 7465 203d 2073  value_update = s
-00024bf0: 656c 662e 7661 6c75 655f 7061 7374 0a20  elf.value_past. 
-00024c00: 2020 2020 2020 2020 2020 2023 2061 6464             # add
-00024c10: 2064 6570 656e 6465 6e63 7920 666f 7220   dependency for 
-00024c20: 6465 7369 7265 6420 6578 6563 7574 696f  desired executio
-00024c30: 6e20 6f72 6465 720a 2020 2020 2020 2020  n order.        
-00024c40: 2020 2020 6b65 795f 7570 6461 7465 203d      key_update =
-00024c50: 2046 2e64 6570 656e 6428 6b65 795f 7570   F.depend(key_up
-00024c60: 6461 7465 2c20 6b65 795f 7265 7365 7429  date, key_reset)
-00024c70: 0a20 2020 2020 2020 2020 2020 2076 616c  .            val
-00024c80: 7565 5f75 7064 6174 6520 3d20 462e 6465  ue_update = F.de
-00024c90: 7065 6e64 2876 616c 7565 5f75 7064 6174  pend(value_updat
-00024ca0: 652c 2076 616c 7565 5f72 6573 6574 290a  e, value_reset).
-00024cb0: 0a20 2020 2020 2020 2023 2061 6464 2064  .        # add d
-00024cc0: 6570 656e 6465 6e63 7920 666f 7220 6465  ependency for de
-00024cd0: 7369 7265 6420 6578 6563 7574 696f 6e20  sired execution 
-00024ce0: 6f72 6465 720a 2020 2020 2020 2020 6d6c  order.        ml
-00024cf0: 705f 6c6f 6769 7420 3d20 462e 6465 7065  p_logit = F.depe
-00024d00: 6e64 286d 6c70 5f6c 6f67 6974 2c20 7661  nd(mlp_logit, va
-00024d10: 6c75 655f 7570 6461 7465 290a 2020 2020  lue_update).    
-00024d20: 2020 2020 6d6c 705f 6c6f 6769 7420 3d20      mlp_logit = 
-00024d30: 462e 6465 7065 6e64 286d 6c70 5f6c 6f67  F.depend(mlp_log
-00024d40: 6974 2c20 6b65 795f 7570 6461 7465 290a  it, key_update).
-00024d50: 0a20 2020 2020 2020 2023 2069 6620 7368  .        # if sh
-00024d60: 6170 6520 6973 2033 642c 2077 6520 7265  ape is 3d, we re
-00024d70: 7368 6170 6520 7468 6520 696e 7075 7473  shape the inputs
-00024d80: 206f 6620 7468 6520 6164 640a 2020 2020   of the add.    
-00024d90: 2020 2020 6966 206c 656e 2868 6964 6465      if len(hidde
-00024da0: 6e5f 7368 6170 6529 203d 3d20 333a 0a20  n_shape) == 3:. 
-00024db0: 2020 2020 2020 2020 2020 206f 7574 7075             outpu
-00024dc0: 745f 7820 3d20 502e 5265 7368 6170 6528  t_x = P.Reshape(
-00024dd0: 2928 6f75 7470 7574 5f78 2c20 6869 6464  )(output_x, hidd
-00024de0: 656e 5f73 6861 7065 290a 2020 2020 2020  en_shape).      
-00024df0: 2020 2020 2020 6d6c 705f 6c6f 6769 7420        mlp_logit 
-00024e00: 3d20 502e 5265 7368 6170 6528 2928 6d6c  = P.Reshape()(ml
-00024e10: 705f 6c6f 6769 742c 2068 6964 6465 6e5f  p_logit, hidden_
-00024e20: 7368 6170 6529 0a20 2020 2020 2020 2020  shape).         
-00024e30: 2020 2078 203d 2050 2e52 6573 6861 7065     x = P.Reshape
-00024e40: 2829 2878 2c20 6869 6464 656e 5f73 6861  ()(x, hidden_sha
-00024e50: 7065 290a 0a20 2020 2020 2020 2020 2020  pe)..           
-00024e60: 2069 6620 7365 6c66 2e70 6f73 745f 6c61   if self.post_la
-00024e70: 7965 726e 6f72 6d5f 7265 7369 6475 616c  yernorm_residual
-00024e80: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
-00024e90: 2020 6f75 7470 7574 203d 2073 656c 662e    output = self.
-00024ea0: 6164 645f 3364 286f 7574 7075 745f 782c  add_3d(output_x,
-00024eb0: 206d 6c70 5f6c 6f67 6974 290a 2020 2020   mlp_logit).    
-00024ec0: 2020 2020 2020 2020 656c 7365 3a0a 2020          else:.  
-00024ed0: 2020 2020 2020 2020 2020 2020 2020 6f75                ou
-00024ee0: 7470 7574 203d 2073 656c 662e 6164 645f  tput = self.add_
-00024ef0: 3364 2878 2c20 6d6c 705f 6c6f 6769 7429  3d(x, mlp_logit)
-00024f00: 0a20 2020 2020 2020 2065 6c73 653a 0a20  .        else:. 
-00024f10: 2020 2020 2020 2020 2020 2069 6620 7365             if se
-00024f20: 6c66 2e70 6f73 745f 6c61 7965 726e 6f72  lf.post_layernor
-00024f30: 6d5f 7265 7369 6475 616c 3a0a 2020 2020  m_residual:.    
-00024f40: 2020 2020 2020 2020 2020 2020 6f75 7470              outp
-00024f50: 7574 203d 2073 656c 662e 6164 6428 6f75  ut = self.add(ou
-00024f60: 7470 7574 5f78 2c20 6d6c 705f 6c6f 6769  tput_x, mlp_logi
-00024f70: 7429 0a20 2020 2020 2020 2020 2020 2065  t).            e
-00024f80: 6c73 653a 0a20 2020 2020 2020 2020 2020  lse:.           
-00024f90: 2020 2020 206f 7574 7075 7420 3d20 7365       output = se
-00024fa0: 6c66 2e61 6464 2878 2c20 6d6c 705f 6c6f  lf.add(x, mlp_lo
-00024fb0: 6769 7429 0a20 2020 2020 2020 2020 2020  git).           
-00024fc0: 206f 7574 7075 7420 3d20 462e 7265 7368   output = F.resh
-00024fd0: 6170 6528 6f75 7470 7574 2c20 6869 6464  ape(output, hidd
-00024fe0: 656e 5f73 6861 7065 290a 0a20 2020 2020  en_shape)..     
-00024ff0: 2020 2069 6620 7365 6c66 2e75 7365 5f6d     if self.use_m
-00025000: 6f65 3a0a 2020 2020 2020 2020 2020 2020  oe:.            
-00025010: 7265 7475 726e 206f 7574 7075 742c 206c  return output, l
-00025020: 6179 6572 5f70 7265 7365 6e74 2c20 6175  ayer_present, au
-00025030: 785f 6c6f 7373 0a20 2020 2020 2020 2072  x_loss.        r
-00025040: 6574 7572 6e20 6f75 7470 7574 2c20 6c61  eturn output, la
-00025050: 7965 725f 7072 6573 656e 740a 0a20 2020  yer_present..   
-00025060: 2064 6566 205f 6368 6563 6b5f 696e 7075   def _check_inpu
-00025070: 7428 7365 6c66 2c20 6869 6464 656e 5f73  t(self, hidden_s
-00025080: 7461 7465 732c 2061 7474 656e 7469 6f6e  tates, attention
-00025090: 5f6d 6173 6b2c 2065 6e63 6f64 6572 5f6f  _mask, encoder_o
-000250a0: 7574 7075 742c 206d 656d 6f72 795f 6d61  utput, memory_ma
-000250b0: 736b 2c20 696e 6974 5f72 6573 6574 2c20  sk, init_reset, 
-000250c0: 6261 7463 685f 7661 6c69 645f 6c65 6e67  batch_valid_leng
-000250d0: 7468 293a 0a20 2020 2020 2020 2072 2222  th):.        r""
-000250e0: 2243 6865 636b 2069 6e70 7574 7322 2222  "Check inputs"""
-000250f0: 0a20 2020 2020 2020 205f 6368 6563 6b5f  .        _check_
-00025100: 696e 7075 745f 6474 7970 6528 462e 6474  input_dtype(F.dt
-00025110: 7970 6528 6869 6464 656e 5f73 7461 7465  ype(hidden_state
-00025120: 7329 2c20 2268 6964 6465 6e5f 7374 6174  s), "hidden_stat
-00025130: 6573 222c 0a20 2020 2020 2020 2020 2020  es",.           
-00025140: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00025150: 5b6d 7374 7970 652e 666c 6f61 7433 322c  [mstype.float32,
-00025160: 206d 7374 7970 652e 666c 6f61 7431 362c   mstype.float16,
-00025170: 206d 7374 7970 652e 6266 6c6f 6174 3136   mstype.bfloat16
-00025180: 5d2c 2073 656c 662e 636c 735f 6e61 6d65  ], self.cls_name
-00025190: 290a 2020 2020 2020 2020 6966 2061 7474  ).        if att
-000251a0: 656e 7469 6f6e 5f6d 6173 6b20 6973 206e  ention_mask is n
-000251b0: 6f74 204e 6f6e 653a 0a20 2020 2020 2020  ot None:.       
-000251c0: 2020 2020 205f 6368 6563 6b5f 696e 7075       _check_inpu
-000251d0: 745f 6474 7970 6528 462e 6474 7970 6528  t_dtype(F.dtype(
-000251e0: 6174 7465 6e74 696f 6e5f 6d61 736b 292c  attention_mask),
-000251f0: 2022 6174 7465 6e74 696f 6e5f 6d61 736b   "attention_mask
-00025200: 222c 0a20 2020 2020 2020 2020 2020 2020  ",.             
-00025210: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00025220: 2020 5b6d 7374 7970 652e 666c 6f61 7433    [mstype.float3
-00025230: 322c 206d 7374 7970 652e 666c 6f61 7431  2, mstype.float1
-00025240: 362c 206d 7374 7970 652e 6266 6c6f 6174  6, mstype.bfloat
-00025250: 3136 5d2c 0a20 2020 2020 2020 2020 2020  16],.           
-00025260: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00025270: 2020 2020 7365 6c66 2e63 6c73 5f6e 616d      self.cls_nam
-00025280: 6529 0a20 2020 2020 2020 2069 6620 656e  e).        if en
-00025290: 636f 6465 725f 6f75 7470 7574 2069 7320  coder_output is 
-000252a0: 6e6f 7420 4e6f 6e65 3a0a 2020 2020 2020  not None:.      
-000252b0: 2020 2020 2020 5f63 6865 636b 5f69 6e70        _check_inp
-000252c0: 7574 5f64 7479 7065 2846 2e64 7479 7065  ut_dtype(F.dtype
-000252d0: 2865 6e63 6f64 6572 5f6f 7574 7075 7429  (encoder_output)
-000252e0: 2c20 2265 6e63 6f64 6572 5f6f 7574 7075  , "encoder_outpu
-000252f0: 7422 2c0a 2020 2020 2020 2020 2020 2020  t",.            
-00025300: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00025310: 2020 205b 6d73 7479 7065 2e66 6c6f 6174     [mstype.float
-00025320: 3332 2c20 6d73 7479 7065 2e66 6c6f 6174  32, mstype.float
-00025330: 3136 2c20 6d73 7479 7065 2e62 666c 6f61  16, mstype.bfloa
-00025340: 7431 365d 2c20 7365 6c66 2e63 6c73 5f6e  t16], self.cls_n
-00025350: 616d 6529 0a20 2020 2020 2020 2069 6620  ame).        if 
-00025360: 6d65 6d6f 7279 5f6d 6173 6b20 6973 206e  memory_mask is n
-00025370: 6f74 204e 6f6e 653a 0a20 2020 2020 2020  ot None:.       
-00025380: 2020 2020 205f 6368 6563 6b5f 696e 7075       _check_inpu
-00025390: 745f 6474 7970 6528 462e 6474 7970 6528  t_dtype(F.dtype(
-000253a0: 6d65 6d6f 7279 5f6d 6173 6b29 2c20 226d  memory_mask), "m
-000253b0: 656d 6f72 795f 6d61 736b 222c 0a20 2020  emory_mask",.   
-000253c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000253d0: 2020 2020 2020 2020 2020 2020 5b6d 7374              [mst
-000253e0: 7970 652e 666c 6f61 7433 322c 206d 7374  ype.float32, mst
-000253f0: 7970 652e 666c 6f61 7431 362c 206d 7374  ype.float16, mst
-00025400: 7970 652e 6266 6c6f 6174 3136 5d2c 2073  ype.bfloat16], s
-00025410: 656c 662e 636c 735f 6e61 6d65 290a 0a20  elf.cls_name).. 
-00025420: 2020 2020 2020 2069 6e69 745f 7265 7365         init_rese
-00025430: 745f 6973 5f74 656e 736f 7220 3d20 6973  t_is_tensor = is
-00025440: 696e 7374 616e 6365 2869 6e69 745f 7265  instance(init_re
-00025450: 7365 742c 2054 656e 736f 7229 0a20 2020  set, Tensor).   
-00025460: 2020 2020 2069 6e69 745f 7265 7365 745f       init_reset_
-00025470: 6973 5f64 6566 6175 6c74 203d 2069 6e69  is_default = ini
-00025480: 745f 7265 7365 7420 6973 2054 7275 650a  t_reset is True.
-00025490: 2020 2020 2020 2020 6261 7463 685f 7661          batch_va
-000254a0: 6c69 645f 6c65 6e67 7468 5f69 735f 7465  lid_length_is_te
-000254b0: 6e73 6f72 203d 2069 7369 6e73 7461 6e63  nsor = isinstanc
-000254c0: 6528 6261 7463 685f 7661 6c69 645f 6c65  e(batch_valid_le
-000254d0: 6e67 7468 2c20 5465 6e73 6f72 290a 2020  ngth, Tensor).  
-000254e0: 2020 2020 2020 6261 7463 685f 6973 5f64        batch_is_d
-000254f0: 6566 6175 6c74 203d 2062 6174 6368 5f76  efault = batch_v
-00025500: 616c 6964 5f6c 656e 6774 6820 6973 204e  alid_length is N
-00025510: 6f6e 650a 2020 2020 2020 2020 5f63 6865  one.        _che
-00025520: 636b 5f70 6173 745f 6e6f 6e65 5f69 6e70  ck_past_none_inp
-00025530: 7574 5f6e 6f6e 6528 7365 6c66 2e75 7365  ut_none(self.use
-00025540: 5f70 6173 742c 2022 696e 6974 5f72 6573  _past, "init_res
-00025550: 6574 222c 2073 656c 662e 636c 735f 6e61  et", self.cls_na
-00025560: 6d65 2c20 5472 7565 2c20 696e 6974 5f72  me, True, init_r
-00025570: 6573 6574 5f69 735f 7465 6e73 6f72 2c0a  eset_is_tensor,.
-00025580: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00025590: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000255a0: 2020 2020 696e 6974 5f72 6573 6574 5f69      init_reset_i
-000255b0: 735f 6465 6661 756c 7429 0a20 2020 2020  s_default).     
-000255c0: 2020 205f 6368 6563 6b5f 7061 7374 5f6e     _check_past_n
-000255d0: 6f6e 655f 696e 7075 745f 6e6f 6e65 2873  one_input_none(s
-000255e0: 656c 662e 7573 655f 7061 7374 2c20 2262  elf.use_past, "b
-000255f0: 6174 6368 5f76 616c 6964 5f6c 656e 6774  atch_valid_lengt
-00025600: 6822 2c20 7365 6c66 2e63 6c73 5f6e 616d  h", self.cls_nam
-00025610: 652c 204e 6f6e 652c 0a20 2020 2020 2020  e, None,.       
-00025620: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00025630: 2020 2020 2020 2020 2020 2020 2062 6174               bat
-00025640: 6368 5f76 616c 6964 5f6c 656e 6774 685f  ch_valid_length_
-00025650: 6973 5f74 656e 736f 722c 2062 6174 6368  is_tensor, batch
-00025660: 5f69 735f 6465 6661 756c 7429 0a0a 2020  _is_default)..  
-00025670: 2020 2020 2020 6966 2073 656c 662e 7573        if self.us
-00025680: 655f 7061 7374 3a0a 2020 2020 2020 2020  e_past:.        
-00025690: 2020 2020 5f63 6865 636b 5f69 6e70 7574      _check_input
-000256a0: 5f64 7479 7065 2846 2e64 7479 7065 2869  _dtype(F.dtype(i
-000256b0: 6e69 745f 7265 7365 7429 2c20 2269 6e69  nit_reset), "ini
-000256c0: 745f 7265 7365 7422 2c20 5b6d 7374 7970  t_reset", [mstyp
-000256d0: 652e 626f 6f6c 5f5d 2c20 7365 6c66 2e63  e.bool_], self.c
-000256e0: 6c73 5f6e 616d 6529 0a20 2020 2020 2020  ls_name).       
-000256f0: 2020 2020 205f 6368 6563 6b5f 696e 7075       _check_inpu
-00025700: 745f 6474 7970 6528 462e 6474 7970 6528  t_dtype(F.dtype(
-00025710: 6261 7463 685f 7661 6c69 645f 6c65 6e67  batch_valid_leng
-00025720: 7468 292c 2022 6261 7463 685f 7661 6c69  th), "batch_vali
-00025730: 645f 6c65 6e67 7468 222c 205b 6d73 7479  d_length", [msty
-00025740: 7065 2e69 6e74 3332 5d2c 2073 656c 662e  pe.int32], self.
-00025750: 636c 735f 6e61 6d65 290a 2020 2020 2020  cls_name).      
-00025760: 2020 7265 7475 726e 2054 7275 650a 0a0a    return True...
-00025770: 6465 6620 5f67 6574 5f6c 616d 6264 615f  def _get_lambda_
-00025780: 6675 6e63 2874 6f74 616c 5f6c 6179 6572  func(total_layer
-00025790: 3d4e 6f6e 6529 3a0a 2020 2020 7222 2222  =None):.    r"""
-000257a0: 0a20 2020 2041 2077 7261 7070 6572 2066  .    A wrapper f
-000257b0: 756e 6374 696f 6e20 6f66 2073 7065 6369  unction of speci
-000257c0: 6679 696e 6720 7069 7065 6c69 6e65 2073  fying pipeline s
-000257d0: 7461 6765 2061 6e64 2067 7261 6469 656e  tage and gradien
-000257e0: 7420 6167 6772 6567 6174 696f 6e20 6675  t aggregation fu
-000257f0: 7369 6f6e 2e20 4966 2074 6865 2074 6f74  sion. If the tot
-00025800: 616c 206c 6179 6572 0a20 2020 2069 7320  al layer.    is 
-00025810: 6e6f 7420 4e6f 6e65 2c20 666f 7220 6578  not None, for ex
-00025820: 616d 706c 652c 2073 6574 2069 6e20 7468  ample, set in th
-00025830: 6520 7472 616e 7366 6f72 6d65 7220 6d6f  e transformer mo
-00025840: 6465 6c2c 2074 6865 2070 6970 656c 696e  del, the pipelin
-00025850: 6520 7374 6167 6520 7365 7474 696e 6720  e stage setting 
-00025860: 6675 6e63 7469 6f6e 2077 696c 6c20 6265  function will be
-00025870: 0a20 2020 2060 286c 6179 6572 5f69 6420  .    `(layer_id 
-00025880: 2b20 3029 202f 2f20 2874 6f74 616c 5f6c  + 0) // (total_l
-00025890: 6179 6572 7320 2f20 7061 7261 6c6c 656c  ayers / parallel
-000258a0: 5f63 6f6e 6669 672e 7069 7065 6c69 6e65  _config.pipeline
-000258b0: 5f73 7461 6765 2960 2066 6f72 2074 6865  _stage)` for the
-000258c0: 2065 6e63 6f64 6572 2061 6e64 2c0a 2020   encoder and,.  
-000258d0: 2020 6028 6c61 7965 725f 6964 202b 206f    `(layer_id + o
-000258e0: 6666 7365 7429 202f 2f0a 2020 2020 2874  ffset) //.    (t
-000258f0: 6f74 616c 5f6c 6179 6572 7320 2f20 7061  otal_layers / pa
-00025900: 7261 6c6c 656c 5f63 6f6e 6669 672e 7069  rallel_config.pi
-00025910: 7065 6c69 6e65 5f73 7461 6765 2960 2066  peline_stage)` f
-00025920: 6f72 2074 6865 2064 6563 6f64 6572 2c20  or the decoder, 
-00025930: 7768 6572 6520 606f 6666 7365 7460 2069  where `offset` i
-00025940: 7320 7468 6520 6c61 7965 7273 2069 6e20  s the layers in 
-00025950: 7468 6520 656e 636f 6465 722e 0a20 2020  the encoder..   
-00025960: 2022 2222 0a0a 2020 2020 6465 6620 5f73   """..    def _s
-00025970: 6574 5f70 6172 616c 6c65 6c5f 636f 6e66  et_parallel_conf
-00025980: 6967 7572 655f 666f 725f 6c61 7965 7228  igure_for_layer(
-00025990: 6e65 7477 6f72 6b2c 206c 6179 6572 5f69  network, layer_i
-000259a0: 642c 206f 6666 7365 742c 2070 6172 616c  d, offset, paral
-000259b0: 6c65 6c5f 636f 6e66 6967 2c20 6c61 7965  lel_config, laye
-000259c0: 7273 293a 0a20 2020 2020 2020 2072 2222  rs):.        r""
-000259d0: 220a 2020 2020 2020 2020 4465 6661 756c  ".        Defaul
-000259e0: 7420 7365 7474 696e 6720 666f 7220 7468  t setting for th
-000259f0: 6520 7069 7065 6c69 6e65 2069 733a 2060  e pipeline is: `
-00025a00: 286c 6179 6572 5f69 6420 2b20 6f66 6673  (layer_id + offs
-00025a10: 6574 2920 2f2f 2028 6c61 7965 7273 202f  et) // (layers /
-00025a20: 2070 6970 656c 696e 655f 7374 6167 6529   pipeline_stage)
-00025a30: 602e 0a0a 2020 2020 2020 2020 4172 6773  `...        Args
-00025a40: 3a0a 2020 2020 2020 2020 2020 2020 6e65  :.            ne
-00025a50: 7477 6f72 6b28 4365 6c6c 2920 2d20 5265  twork(Cell) - Re
-00025a60: 7072 6573 656e 7473 2074 6865 2074 7261  presents the tra
-00025a70: 6e73 666f 726d 6572 2062 6c6f 636b 0a20  nsformer block. 
-00025a80: 2020 2020 2020 2020 2020 206c 6179 6572             layer
-00025a90: 5f69 6428 696e 7429 202d 204d 6561 6e73  _id(int) - Means
-00025aa0: 2074 6865 206c 6179 6572 2069 6e64 6578   the layer index
-00025ab0: 2066 6f72 2074 6865 2063 7572 7265 6e74   for the current
-00025ac0: 206d 6f64 756c 652c 2063 6f75 6e74 7320   module, counts 
-00025ad0: 6672 6f6d 207a 6572 6f2e 0a20 2020 2020  from zero..     
-00025ae0: 2020 2020 2020 206f 6666 7365 7428 696e         offset(in
-00025af0: 7429 202d 204d 6561 6e73 2074 6865 206c  t) - Means the l
-00025b00: 6179 6572 5f69 6e64 6578 206e 6565 6473  ayer_index needs
-00025b10: 2061 6e20 6f66 6673 6574 2c20 6966 2074   an offset, if t
-00025b20: 6865 7265 2061 7265 206f 7468 6572 206d  here are other m
-00025b30: 6f64 756c 6573 2069 6e20 7468 6520 6e65  odules in the ne
-00025b40: 742e 0a20 2020 2020 2020 2020 2020 206c  t..            l
-00025b50: 6179 6572 7328 696e 7429 202d 2054 6865  ayers(int) - The
-00025b60: 2074 6f74 616c 206c 6179 6572 7320 7573   total layers us
-00025b70: 6564 2066 6f72 2074 6865 206d 6f64 656c  ed for the model
-00025b80: 2e0a 2020 2020 2020 2020 2222 220a 2020  ..        """.  
-00025b90: 2020 2020 2020 2320 6f76 6572 7269 6465        # override
-00025ba0: 2074 6865 206c 6179 6572 730a 2020 2020   the layers.    
-00025bb0: 2020 2020 6966 2074 6f74 616c 5f6c 6179      if total_lay
-00025bc0: 6572 3a0a 2020 2020 2020 2020 2020 2020  er:.            
-00025bd0: 6c61 7965 7273 203d 2074 6f74 616c 5f6c  layers = total_l
-00025be0: 6179 6572 0a20 2020 2020 2020 2023 2055  ayer.        # U
-00025bf0: 7365 6420 666f 7220 7468 6520 7069 7065  sed for the pipe
-00025c00: 6c69 6e65 2773 2073 7461 6765 7320 7365  line's stages se
-00025c10: 7474 696e 670a 2020 2020 2020 2020 6966  tting.        if
-00025c20: 206c 6179 6572 7320 3c20 7061 7261 6c6c   layers < parall
-00025c30: 656c 5f63 6f6e 6669 672e 7069 7065 6c69  el_config.pipeli
-00025c40: 6e65 5f73 7461 6765 3a0a 2020 2020 2020  ne_stage:.      
-00025c50: 2020 2020 2020 7261 6973 6520 5661 6c75        raise Valu
-00025c60: 6545 7272 6f72 2866 226c 6179 6572 7320  eError(f"layers 
-00025c70: 7b6c 6179 6572 737d 206d 7573 7420 6265  {layers} must be
-00025c80: 206c 6172 6765 7220 7468 616e 2070 6970   larger than pip
-00025c90: 656c 696e 6520 7374 6167 6520 7b70 6172  eline stage {par
-00025ca0: 616c 6c65 6c5f 636f 6e66 6967 2e70 6970  allel_config.pip
-00025cb0: 656c 696e 655f 7374 6167 657d 2229 0a0a  eline_stage}")..
-00025cc0: 2020 2020 2020 2020 7070 5f64 6973 203d          pp_dis =
-00025cd0: 206d 6178 286c 6179 6572 7320 2f2f 2070   max(layers // p
-00025ce0: 6172 616c 6c65 6c5f 636f 6e66 6967 2e70  arallel_config.p
-00025cf0: 6970 656c 696e 655f 7374 6167 652c 2031  ipeline_stage, 1
-00025d00: 290a 2020 2020 2020 2020 2320 7468 6520  ).        # the 
-00025d10: 7069 7065 6c69 6e65 2073 7461 6765 206d  pipeline stage m
-00025d20: 7573 7420 6265 2069 6e20 5b30 2c20 7061  ust be in [0, pa
-00025d30: 7261 6c6c 656c 5f63 6f6e 6669 672e 7069  rallel_config.pi
-00025d40: 7065 6c69 6e65 5f73 7461 6765 202d 2031  peline_stage - 1
-00025d50: 5d0a 2020 2020 2020 2020 7070 5f69 6420  ].        pp_id 
-00025d60: 3d20 6d69 6e28 286c 6179 6572 5f69 6420  = min((layer_id 
-00025d70: 2b20 6f66 6673 6574 2920 2f2f 2070 705f  + offset) // pp_
-00025d80: 6469 732c 2070 6172 616c 6c65 6c5f 636f  dis, parallel_co
-00025d90: 6e66 6967 2e70 6970 656c 696e 655f 7374  nfig.pipeline_st
-00025da0: 6167 6520 2d20 3129 0a20 2020 2020 2020  age - 1).       
-00025db0: 206e 6574 776f 726b 2e70 6970 656c 696e   network.pipelin
-00025dc0: 655f 7374 6167 6520 3d20 7070 5f69 640a  e_stage = pp_id.
-00025dd0: 0a20 2020 2020 2020 2023 2055 7365 6420  .        # Used 
-00025de0: 666f 7220 6f70 7469 6d69 7a65 7227 7320  for optimizer's 
-00025df0: 6675 7369 6f6e 2074 6167 0a20 2020 2020  fusion tag.     
-00025e00: 2020 2064 6973 203d 206d 6178 286c 6179     dis = max(lay
-00025e10: 6572 7320 2f2f 2070 6172 616c 6c65 6c5f  ers // parallel_
-00025e20: 636f 6e66 6967 2e67 7261 6469 656e 745f  config.gradient_
-00025e30: 6167 6772 6567 6174 696f 6e5f 6772 6f75  aggregation_grou
-00025e40: 702c 2031 290a 2020 2020 2020 2020 6e65  p, 1).        ne
-00025e50: 7477 6f72 6b2e 7365 745f 636f 6d6d 5f66  twork.set_comm_f
-00025e60: 7573 696f 6e28 286c 6179 6572 5f69 6420  usion((layer_id 
-00025e70: 2b20 6f66 6673 6574 2920 2f2f 2064 6973  + offset) // dis
-00025e80: 202b 2031 290a 2020 2020 2020 2020 2320   + 1).        # 
-00025e90: 5573 6564 2066 6f72 2065 6e61 626c 696e  Used for enablin
-00025ea0: 6720 7265 636f 6d70 7574 6174 696f 6e20  g recomputation 
-00025eb0: 6f66 2074 6865 2062 6c6f 636b 0a20 2020  of the block.   
-00025ec0: 2020 2020 2069 6620 6973 696e 7374 616e       if isinstan
-00025ed0: 6365 2870 6172 616c 6c65 6c5f 636f 6e66  ce(parallel_conf
-00025ee0: 6967 2e72 6563 6f6d 7075 7465 2c20 626f  ig.recompute, bo
-00025ef0: 6f6c 293a 0a20 2020 2020 2020 2020 2020  ol):.           
-00025f00: 2069 6620 7061 7261 6c6c 656c 5f63 6f6e   if parallel_con
-00025f10: 6669 672e 7265 636f 6d70 7574 6520 616e  fig.recompute an
-00025f20: 6420 6e6f 7420 7061 7261 6c6c 656c 5f63  d not parallel_c
-00025f30: 6f6e 6669 672e 7365 6c65 6374 5f72 6563  onfig.select_rec
-00025f40: 6f6d 7075 7465 3a0a 2020 2020 2020 2020  ompute:.        
-00025f50: 2020 2020 2020 2020 6e65 7477 6f72 6b2e          network.
-00025f60: 7265 636f 6d70 7574 6528 290a 2020 2020  recompute().    
-00025f70: 2020 2020 656c 7365 3a0a 2020 2020 2020      else:.      
-00025f80: 2020 2020 2020 6966 2070 6172 616c 6c65        if paralle
-00025f90: 6c5f 636f 6e66 6967 2e72 6563 6f6d 7075  l_config.recompu
-00025fa0: 7465 2e72 6563 6f6d 7075 7465 2061 6e64  te.recompute and
-00025fb0: 206e 6f74 2070 6172 616c 6c65 6c5f 636f   not parallel_co
-00025fc0: 6e66 6967 2e72 6563 6f6d 7075 7465 2e73  nfig.recompute.s
-00025fd0: 656c 6563 745f 7265 636f 6d70 7574 653a  elect_recompute:
-00025fe0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00025ff0: 2070 6172 616c 656c 5f6f 705f 636f 6d6d   paralel_op_comm
-00026000: 5f63 6f6d 7075 7465 203d 2070 6172 616c  _compute = paral
-00026010: 6c65 6c5f 636f 6e66 6967 2e72 6563 6f6d  lel_config.recom
-00026020: 7075 7465 2e70 6172 616c 6c65 6c5f 6f70  pute.parallel_op
-00026030: 7469 6d69 7a65 725f 636f 6d6d 5f72 6563  timizer_comm_rec
-00026040: 6f6d 7075 7465 0a20 2020 2020 2020 2020  ompute.         
-00026050: 2020 2020 2020 206e 6574 776f 726b 2e72         network.r
-00026060: 6563 6f6d 7075 7465 2870 6172 616c 6c65  ecompute(paralle
-00026070: 6c5f 6f70 7469 6d69 7a65 725f 636f 6d6d  l_optimizer_comm
-00026080: 5f72 6563 6f6d 7075 7465 3d70 6172 616c  _recompute=paral
-00026090: 656c 5f6f 705f 636f 6d6d 5f63 6f6d 7075  el_op_comm_compu
-000260a0: 7465 2c0a 2020 2020 2020 2020 2020 2020  te,.            
-000260b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000260c0: 2020 2020 2020 6d70 5f63 6f6d 6d5f 7265        mp_comm_re
-000260d0: 636f 6d70 7574 653d 7061 7261 6c6c 656c  compute=parallel
-000260e0: 5f63 6f6e 6669 672e 7265 636f 6d70 7574  _config.recomput
-000260f0: 652e 6d70 5f63 6f6d 6d5f 7265 636f 6d70  e.mp_comm_recomp
-00026100: 7574 652c 0a20 2020 2020 2020 2020 2020  ute,.           
-00026110: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00026120: 2020 2020 2020 2072 6563 6f6d 7075 7465         recompute
-00026130: 5f73 6c69 6365 5f61 6374 6976 6174 696f  _slice_activatio
-00026140: 6e3d 7061 7261 6c6c 656c 5f63 6f6e 6669  n=parallel_confi
-00026150: 672e 7265 636f 6d70 7574 652e 7265 636f  g.recompute.reco
-00026160: 6d70 7574 655f 736c 6963 655f 6163 7469  mpute_slice_acti
-00026170: 7661 7469 6f6e 290a 0a20 2020 2072 6574  vation)..    ret
-00026180: 7572 6e20 5f73 6574 5f70 6172 616c 6c65  urn _set_paralle
-00026190: 6c5f 636f 6e66 6967 7572 655f 666f 725f  l_configure_for_
-000261a0: 6c61 7965 720a 0a0a 636c 6173 7320 5472  layer...class Tr
-000261b0: 616e 7366 6f72 6d65 7245 6e63 6f64 6572  ansformerEncoder
-000261c0: 2843 656c 6c29 3a0a 2020 2020 7222 2222  (Cell):.    r"""
-000261d0: 0a20 2020 2020 2020 2054 7261 6e73 666f  .        Transfo
-000261e0: 726d 6572 2045 6e63 6f64 6572 206d 6f64  rmer Encoder mod
-000261f0: 756c 6520 7769 7468 206d 756c 7469 2d6c  ule with multi-l
-00026200: 6179 6572 2073 7461 636b 6564 206f 6620  ayer stacked of 
-00026210: 6054 7261 6e73 666f 726d 6572 456e 636f  `TransformerEnco
-00026220: 6465 724c 6179 6572 602c 2069 6e63 6c75  derLayer`, inclu
-00026230: 6469 6e67 206d 756c 7469 6865 6164 2073  ding multihead s
-00026240: 656c 660a 2020 2020 2020 2020 6174 7465  elf.        atte
-00026250: 6e74 696f 6e20 616e 6420 6665 6564 666f  ntion and feedfo
-00026260: 7277 6172 6420 6c61 7965 722e 0a0a 2020  rward layer...  
-00026270: 2020 2020 2020 4172 6773 3a0a 2020 2020        Args:.    
-00026280: 2020 2020 2020 2020 6261 7463 685f 7369          batch_si
-00026290: 7a65 2869 6e74 293a 2054 6865 2062 6174  ze(int): The bat
-000262a0: 6368 2073 697a 6520 6f66 2074 6865 2069  ch size of the i
-000262b0: 6e70 7574 2074 656e 736f 7220 7768 656e  nput tensor when
-000262c0: 2064 6f20 696e 6372 656e 6d65 6e74 616c   do increnmental
-000262d0: 2070 7265 6469 6374 696f 6e2e 2053 686f   prediction. Sho
-000262e0: 756c 6420 6265 2061 2070 6f73 6974 6976  uld be a positiv
-000262f0: 650a 2020 2020 2020 2020 2020 2020 2020  e.              
-00026300: 2020 7661 6c75 652e 2057 6865 6e20 646f    value. When do
-00026310: 2074 7261 696e 696e 6720 6f72 2070 7265   training or pre
-00026320: 6469 6374 696f 6e2c 2074 6865 2061 7267  diction, the arg
-00026330: 756d 656e 7420 7769 6c6c 206e 6f74 2077  ument will not w
-00026340: 6f72 6b20 616e 6420 7468 6520 7573 6572  ork and the user
-00026350: 2063 616e 206a 7573 7420 7061 7373 204e   can just pass N
-00026360: 6f6e 6520 746f 0a20 2020 2020 2020 2020  one to.         
-00026370: 2020 2020 2020 2074 6865 2061 7267 756d         the argum
-00026380: 656e 742e 0a20 2020 2020 2020 2020 2020  ent..           
-00026390: 206e 756d 5f6c 6179 6572 7328 696e 7429   num_layers(int)
-000263a0: 3a20 5468 6520 6c61 7965 7273 206f 6620  : The layers of 
-000263b0: 7468 6520 6054 7261 6e73 666f 726d 6572  the `Transformer
-000263c0: 456e 636f 6465 724c 6179 6572 600a 2020  EncoderLayer`.  
-000263d0: 2020 2020 2020 2020 2020 6869 6464 656e            hidden
-000263e0: 5f73 697a 6528 696e 7429 3a20 5468 6520  _size(int): The 
-000263f0: 6869 6464 656e 2073 697a 6520 6f66 2074  hidden size of t
-00026400: 6865 2069 6e70 7574 2e0a 2020 2020 2020  he input..      
-00026410: 2020 2020 2020 6666 6e5f 6869 6464 656e        ffn_hidden
-00026420: 5f73 697a 6528 696e 7429 3a20 5468 6520  _size(int): The 
-00026430: 6869 6464 656e 2073 697a 6520 6f66 2062  hidden size of b
-00026440: 6f74 746c 656e 6563 6b20 696e 2074 6865  ottleneck in the
-00026450: 2066 6565 6466 6f72 7761 7264 206c 6179   feedforward lay
-00026460: 6572 2e0a 2020 2020 2020 2020 2020 2020  er..            
-00026470: 7365 715f 6c65 6e67 7468 2869 6e74 293a  seq_length(int):
-00026480: 2054 6865 2073 6571 5f6c 656e 6774 6820   The seq_length 
-00026490: 6f66 2074 6865 2069 6e70 7574 2074 656e  of the input ten
-000264a0: 736f 722e 0a20 2020 2020 2020 2020 2020  sor..           
-000264b0: 206e 756d 5f68 6561 6473 2869 6e74 293a   num_heads(int):
-000264c0: 2054 6865 206e 756d 6265 7220 6f66 2074   The number of t
-000264d0: 6865 2068 6561 6473 2e0a 2020 2020 2020  he heads..      
-000264e0: 2020 2020 2020 6174 7465 6e74 696f 6e5f        attention_
-000264f0: 6472 6f70 6f75 745f 7261 7465 2866 6c6f  dropout_rate(flo
-00026500: 6174 293a 2054 6865 2064 726f 706f 7574  at): The dropout
-00026510: 2072 6174 6520 6f66 2074 6865 2061 7474   rate of the att
-00026520: 656e 7469 6f6e 2073 636f 7265 732e 2044  ention scores. D
-00026530: 6566 6175 6c74 3a30 2e31 2e0a 2020 2020  efault:0.1..    
-00026540: 2020 2020 2020 2020 6869 6464 656e 5f64          hidden_d
-00026550: 726f 706f 7574 5f72 6174 6528 666c 6f61  ropout_rate(floa
-00026560: 7429 3a20 5468 6520 6472 6f70 6f75 7420  t): The dropout 
-00026570: 7261 7465 206f 6620 7468 6520 6669 6e61  rate of the fina
-00026580: 6c20 6f75 7470 7574 206f 6620 7468 6520  l output of the 
-00026590: 6c61 7965 722e 2044 6566 6175 6c74 3a20  layer. Default: 
-000265a0: 302e 312e 0a20 2020 2020 2020 2020 2020  0.1..           
-000265b0: 2068 6964 6465 6e5f 6163 7420 2873 7472   hidden_act (str
-000265c0: 2c20 6e6e 2e43 656c 6c29 3a20 5468 6520  , nn.Cell): The 
-000265d0: 6163 7469 7661 7469 6f6e 206f 6620 7468  activation of th
-000265e0: 6520 696e 7465 726e 616c 2066 6565 6466  e internal feedf
-000265f0: 6f72 7761 7264 206c 6179 6572 2e20 5375  orward layer. Su
-00026600: 7070 6f72 7473 2027 7265 6c75 272c 0a20  pports 'relu',. 
-00026610: 2020 2020 2020 2020 2020 2020 2020 2027                 '
-00026620: 7265 6c75 3627 2c20 2774 616e 6827 2c20  relu6', 'tanh', 
-00026630: 2767 656c 7527 2c20 2766 6173 745f 6765  'gelu', 'fast_ge
-00026640: 6c75 272c 2027 656c 7527 2c20 2773 6967  lu', 'elu', 'sig
-00026650: 6d6f 6964 272c 2027 7072 656c 7527 2c20  moid', 'prelu', 
-00026660: 276c 6561 6b79 7265 6c75 272c 2027 6873  'leakyrelu', 'hs
-00026670: 7769 7368 272c 0a20 2020 2020 2020 2020  wish',.         
-00026680: 2020 2020 2020 2027 6873 6967 6d6f 6964         'hsigmoid
-00026690: 272c 2027 6c6f 6773 6967 6d6f 6964 2720  ', 'logsigmoid' 
-000266a0: 616e 6420 736f 206f 6e2e 2055 7365 7220  and so on. User 
-000266b0: 6361 6e20 7072 6f76 6964 6520 6375 7374  can provide cust
-000266c0: 6f6d 2061 6374 6976 6974 696f 6e20 746f  om activition to
-000266d0: 2074 6865 2061 7267 756d 656e 742e 0a20   the argument.. 
-000266e0: 2020 2020 2020 2020 2020 2020 2020 2049                 I
-000266f0: 6620 7573 6572 2077 616e 7473 2074 6f20  f user wants to 
-00026700: 7275 6e20 7468 6520 6e65 7420 696e 2074  run the net in t
-00026710: 6865 2070 6172 616c 6c65 6c20 6d6f 6465  he parallel mode
-00026720: 2c20 7468 6520 6375 7374 6f6d 2061 6374  , the custom act
-00026730: 6976 6174 696f 6e20 6d75 7374 2061 6c73  ivation must als
-00026740: 6f20 7072 6f76 6964 650a 2020 2020 2020  o provide.      
-00026750: 2020 2020 2020 2020 2020 7468 6520 6061            the `a
-00026760: 6374 6976 6174 696f 6e5f 7368 6172 6460  ctivation_shard`
-00026770: 2066 756e 6374 696f 6e2e 2050 6c65 6173   function. Pleas
-00026780: 6520 7365 6520 7468 6520 6578 616d 706c  e see the exampl
-00026790: 6573 206f 6620 7468 650a 2020 2020 2020  es of the.      
-000267a0: 2020 2020 2020 2020 2020 636c 6173 733a            class:
-000267b0: 606d 696e 6466 6f72 6d65 7273 2e6d 6f64  `mindformers.mod
-000267c0: 756c 6573 2e74 7261 6e73 666f 726d 6572  ules.transformer
-000267d0: 2e46 6565 6446 6f72 7761 7264 602e 2044  .FeedForward`. D
-000267e0: 6566 6175 6c74 3a20 6765 6c75 2e0a 2020  efault: gelu..  
-000267f0: 2020 2020 2020 2020 2020 706f 7374 5f6c            post_l
-00026800: 6179 6572 6e6f 726d 5f72 6573 6964 7561  ayernorm_residua
-00026810: 6c28 626f 6f6c 293a 2044 6f20 7265 7369  l(bool): Do resi
-00026820: 6475 616c 7320 6164 6473 2062 6566 6f72  duals adds befor
-00026830: 6520 7468 6520 6c61 7965 726e 6f72 6d2e  e the layernorm.
-00026840: 2044 6566 6175 6c74 2046 616c 7365 2e0a   Default False..
-00026850: 2020 2020 2020 2020 2020 2020 6c61 7965              laye
-00026860: 726e 6f72 6d5f 636f 6d70 7574 655f 7479  rnorm_compute_ty
-00026870: 7065 2864 7479 7065 2e4e 756d 6265 7229  pe(dtype.Number)
-00026880: 3a20 5468 6520 636f 6d70 7574 6174 696f  : The computatio
-00026890: 6e20 7479 7065 206f 6620 7468 6520 6c61  n type of the la
-000268a0: 7965 726e 6f72 6d2e 0a20 2020 2020 2020  yernorm..       
-000268b0: 2020 2020 2020 2020 2053 686f 756c 6420           Should 
-000268c0: 6265 206d 7374 7970 652e 666c 6f61 7433  be mstype.float3
-000268d0: 3220 6f72 206d 7374 7970 652e 666c 6f61  2 or mstype.floa
-000268e0: 7431 362e 2044 6566 6175 6c74 206d 7374  t16. Default mst
-000268f0: 7970 652e 666c 6f61 7433 322e 0a20 2020  ype.float32..   
-00026900: 2020 2020 2020 2020 2073 6f66 746d 6178           softmax
-00026910: 5f63 6f6d 7075 7465 5f74 7970 6528 6474  _compute_type(dt
-00026920: 7970 652e 4e75 6d62 6572 293a 2054 6865  ype.Number): The
-00026930: 2063 6f6d 7075 7461 7469 6f6e 2074 7970   computation typ
-00026940: 6520 6f66 2074 6865 2073 6f66 746d 6178  e of the softmax
-00026950: 2069 6e20 7468 6520 6174 7465 6e74 696f   in the attentio
-00026960: 6e2e 0a20 2020 2020 2020 2020 2020 2020  n..             
-00026970: 2020 2053 686f 756c 6420 6265 206d 7374     Should be mst
-00026980: 7970 652e 666c 6f61 7433 3220 6f72 206d  ype.float32 or m
-00026990: 7374 7970 652e 666c 6f61 7431 362e 2044  stype.float16. D
-000269a0: 6566 6175 6c74 3a20 6d73 7479 7065 2e66  efault: mstype.f
-000269b0: 6c6f 6174 3332 2e0a 2020 2020 2020 2020  loat32..        
-000269c0: 2020 2020 7061 7261 6d5f 696e 6974 5f74      param_init_t
-000269d0: 7970 6528 6474 7970 652e 4e75 6d62 6572  ype(dtype.Number
-000269e0: 293a 2054 6865 2070 6172 616d 6574 6572  ): The parameter
-000269f0: 2069 6e69 7469 616c 697a 6174 696f 6e20   initialization 
-00026a00: 7479 7065 206f 6620 7468 6520 6d6f 6475  type of the modu
-00026a10: 6c65 2e0a 2020 2020 2020 2020 2020 2020  le..            
-00026a20: 2020 2020 5368 6f75 6c64 2062 6520 6d73      Should be ms
-00026a30: 7479 7065 2e66 6c6f 6174 3332 206f 7220  type.float32 or 
-00026a40: 6d73 7479 7065 2e66 6c6f 6174 3136 2e20  mstype.float16. 
-00026a50: 4465 6661 756c 743a 206d 7374 7970 652e  Default: mstype.
-00026a60: 666c 6f61 7433 322e 0a20 2020 2020 2020  float32..       
-00026a70: 2020 2020 206c 616d 6264 615f 6675 6e63       lambda_func
-00026a80: 2866 756e 6374 696f 6e29 3a20 4120 6675  (function): A fu
-00026a90: 6e63 7469 6f6e 2063 616e 2064 6574 6572  nction can deter
-00026aa0: 6d69 6e65 2074 6865 2066 7573 696f 6e20  mine the fusion 
-00026ab0: 696e 6465 782c 0a20 2020 2020 2020 2020  index,.         
-00026ac0: 2020 2020 2020 2070 6970 656c 696e 6520         pipeline 
-00026ad0: 7374 6167 6573 2061 6e64 2072 6563 6f6d  stages and recom
-00026ae0: 7075 7465 2061 7474 7269 6275 7465 2e20  pute attribute. 
-00026af0: 4966 2074 6865 0a20 2020 2020 2020 2020  If the.         
-00026b00: 2020 2020 2020 2075 7365 7220 7761 6e74         user want
-00026b10: 7320 746f 2064 6574 6572 6d69 6e65 2074  s to determine t
-00026b20: 6865 2070 6970 656c 696e 6520 7374 6167  he pipeline stag
-00026b30: 6520 616e 6420 6772 6164 6965 6e74 2061  e and gradient a
-00026b40: 6767 7265 6761 7469 6f6e 2066 7573 696f  ggregation fusio
-00026b50: 6e2c 2074 6865 2075 7365 7220 6361 6e20  n, the user can 
-00026b60: 7061 7373 2061 0a20 2020 2020 2020 2020  pass a.         
-00026b70: 2020 2020 2020 2066 756e 6374 696f 6e20         function 
-00026b80: 7468 6174 2061 6363 6570 7473 2060 6e65  that accepts `ne
-00026b90: 7477 6f72 6b60 2c20 606c 6179 6572 5f69  twork`, `layer_i
-00026ba0: 6460 2c20 606f 6666 7365 7460 2c20 6070  d`, `offset`, `p
-00026bb0: 6172 616c 6c65 6c5f 636f 6e66 6967 602c  arallel_config`,
-00026bc0: 2060 6c61 7965 7273 602e 2054 6865 2060   `layers`. The `
-00026bd0: 6e65 7477 6f72 6b28 4365 6c6c 2960 0a20  network(Cell)`. 
-00026be0: 2020 2020 2020 2020 2020 2020 2020 2072                 r
-00026bf0: 6570 7265 7365 6e74 7320 7468 6520 7472  epresents the tr
-00026c00: 616e 7366 6f72 6d65 7220 626c 6f63 6b2c  ansformer block,
-00026c10: 2060 6c61 7965 725f 6964 2869 6e74 2960   `layer_id(int)`
-00026c20: 206d 6561 6e73 2074 6865 206c 6179 6572   means the layer
-00026c30: 2069 6e64 6578 2066 6f72 2074 6865 2063   index for the c
-00026c40: 7572 7265 6e74 206d 6f64 756c 652c 2063  urrent module, c
-00026c50: 6f75 6e74 730a 2020 2020 2020 2020 2020  ounts.          
-00026c60: 2020 2020 2020 6672 6f6d 207a 6572 6f2c        from zero,
-00026c70: 2060 6f66 6673 6574 2869 6e74 2960 206d   `offset(int)` m
-00026c80: 6561 6e73 2074 6865 206c 6179 6572 5f69  eans the layer_i
-00026c90: 6e64 6578 206e 6565 6473 2061 6e20 6f66  ndex needs an of
-00026ca0: 6673 6574 2c20 6966 2074 6865 7265 2061  fset, if there a
-00026cb0: 7265 206f 7468 6572 206d 6f64 756c 6573  re other modules
-00026cc0: 2069 6e20 7468 6520 6e65 742e 0a20 2020   in the net..   
-00026cd0: 2020 2020 2020 2020 2020 2020 2054 6865               The
-00026ce0: 2064 6566 6175 6c74 2073 6574 7469 6e67   default setting
-00026cf0: 2066 6f72 2074 6865 2070 6970 656c 696e   for the pipelin
-00026d00: 6520 6973 3a20 6028 6c61 7965 725f 6964  e is: `(layer_id
-00026d10: 202b 206f 6666 7365 7429 202f 2f20 286c   + offset) // (l
-00026d20: 6179 6572 7320 2f20 7069 7065 6c69 6e65  ayers / pipeline
-00026d30: 5f73 7461 6765 2960 2e0a 2020 2020 2020  _stage)`..      
-00026d40: 2020 2020 2020 2020 2020 4465 6661 756c            Defaul
-00026d50: 743a 204e 6f6e 652e 0a20 2020 2020 2020  t: None..       
-00026d60: 2020 2020 206f 6666 7365 7428 696e 7429       offset(int)
-00026d70: 3a20 5468 6520 696e 6974 6961 6c20 6c61  : The initial la
-00026d80: 7965 7220 696e 6465 7820 666f 7220 7468  yer index for th
-00026d90: 6520 6065 6e63 6f64 6572 602e 2055 7365  e `encoder`. Use
-00026da0: 6420 666f 7220 7365 7474 696e 6720 7468  d for setting th
-00026db0: 6520 6675 7369 6f6e 2069 6420 616e 6420  e fusion id and 
-00026dc0: 7374 6167 6520 6964 2c20 746f 206e 6f74  stage id, to not
-00026dd0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00026de0: 206f 7665 726c 6170 2077 6974 6820 7468   overlap with th
-00026df0: 6520 656e 636f 6465 7220 6c61 7965 722e  e encoder layer.
-00026e00: 2044 6566 6175 6c74 2030 2e0a 2020 2020   Default 0..    
-00026e10: 2020 2020 2020 2020 7573 655f 7061 7374          use_past
-00026e20: 2862 6f6f 6c29 3a20 5573 6520 7468 6520  (bool): Use the 
-00026e30: 7061 7374 2073 7461 7465 2074 6f20 636f  past state to co
-00026e40: 6d70 7574 652c 2075 7365 6420 666f 7220  mpute, used for 
-00026e50: 696e 6372 656d 656e 7461 6c20 7072 6564  incremental pred
-00026e60: 6963 7469 6f6e 2e20 466f 7220 6578 616d  iction. For exam
-00026e70: 706c 652c 2069 6620 7765 2068 6176 6520  ple, if we have 
-00026e80: 7477 6f0a 2020 2020 2020 2020 2020 2020  two.            
-00026e90: 2020 2020 776f 7264 7320 616e 6420 7761      words and wa
-00026ea0: 6e74 2074 6f20 6765 6e65 7261 7465 2074  nt to generate t
-00026eb0: 6865 2074 656e 206d 6f72 6520 776f 7264  he ten more word
-00026ec0: 732e 2057 6520 6a75 7374 206e 6565 6420  s. We just need 
-00026ed0: 746f 2063 6f6d 7075 7465 2074 6865 2074  to compute the t
-00026ee0: 776f 2077 6f72 6473 2720 7374 6174 6520  wo words' state 
-00026ef0: 6f6e 6c79 206f 6e63 652c 0a20 2020 2020  only once,.     
-00026f00: 2020 2020 2020 2020 2020 2061 6e64 2067             and g
-00026f10: 656e 6572 6174 6520 7468 6520 6e65 7874  enerate the next
-00026f20: 2077 6f72 6420 6f6e 6520 6279 206f 6e65   word one by one
-00026f30: 2e20 5768 656e 2075 7365 5f70 6173 7420  . When use_past 
-00026f40: 6973 2054 7275 652c 2074 6865 7265 2061  is True, there a
-00026f50: 7265 2074 776f 2073 7465 7073 2074 6f20  re two steps to 
-00026f60: 7275 6e20 7468 6520 7072 6564 6963 7469  run the predicti
-00026f70: 6f6e 2e0a 2020 2020 2020 2020 2020 2020  on..            
-00026f80: 2020 2020 496e 2074 6865 2066 6972 7374      In the first
-00026f90: 2073 7465 702c 2073 6574 2074 6865 2069   step, set the i
-00026fa0: 735f 6669 7273 745f 6974 6572 6174 696f  s_first_iteratio
-00026fb0: 6e20 746f 2062 6520 5472 7565 2062 790a  n to be True by.
-00026fc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00026fd0: 606d 6f64 656c 2e61 6464 5f66 6c61 6773  `model.add_flags
-00026fe0: 5f72 6563 7572 7369 7665 2869 735f 6669  _recursive(is_fi
-00026ff0: 7273 745f 6974 6572 6174 696f 6e3d 5472  rst_iteration=Tr
-00027000: 7565 2960 2c20 616e 6420 7061 7373 2074  ue)`, and pass t
-00027010: 6865 2066 756c 6c20 696e 7075 7473 2e20  he full inputs. 
-00027020: 5468 656e 2c20 7365 7420 7468 650a 2020  Then, set the.  
-00027030: 2020 2020 2020 2020 2020 2020 2020 6973                is
-00027040: 5f66 6972 7374 5f69 7465 7261 7469 6f6e  _first_iteration
-00027050: 2074 6f20 6265 2046 616c 7365 2062 7920   to be False by 
-00027060: 606d 6f64 656c 2e61 6464 5f66 6c61 6773  `model.add_flags
-00027070: 5f72 6563 7572 7369 7665 2869 735f 6669  _recursive(is_fi
-00027080: 7273 745f 6974 6572 6174 696f 6e3d 4661  rst_iteration=Fa
-00027090: 6c73 6529 602e 2041 7420 7468 6973 206d  lse)`. At this m
-000270a0: 6f6d 656e 742c 0a20 2020 2020 2020 2020  oment,.         
-000270b0: 2020 2020 2020 2070 6173 7320 7468 6520         pass the 
-000270c0: 7369 6e67 6c65 2073 7465 7027 7320 696e  single step's in
-000270d0: 7075 7420 7465 6e73 6f72 2c20 616e 6420  put tensor, and 
-000270e0: 6c6f 6f70 2069 742e 2044 6566 6175 6c74  loop it. Default
-000270f0: 3a20 4661 6c73 652e 0a20 2020 2020 2020  : False..       
-00027100: 2020 2020 206d 6f65 5f63 6f6e 6669 6728       moe_config(
-00027110: 4d6f 4543 6f6e 6669 6729 3a20 5468 6520  MoEConfig): The 
-00027120: 636f 6e66 6967 7572 6174 696f 6e20 6f66  configuration of
-00027130: 204d 6f45 2028 4d69 7874 7572 6520 6f66   MoE (Mixture of
-00027140: 2045 7870 6572 7429 2e20 4465 6661 756c   Expert). Defaul
-00027150: 7420 6973 2061 6e20 696e 7374 616e 6365  t is an instance
-00027160: 206f 6620 4d6f 4543 6f6e 6669 670a 2020   of MoEConfig.  
-00027170: 2020 2020 2020 2020 2020 2020 2020 7769                wi
-00027180: 7468 2064 6566 6175 6c74 2076 616c 7565  th default value
-00027190: 732e 2050 6c65 6173 6520 7365 6520 604d  s. Please see `M
-000271a0: 6f45 436f 6e66 6967 602e 0a20 2020 2020  oEConfig`..     
-000271b0: 2020 2020 2020 2070 6172 616c 6c65 6c5f         parallel_
-000271c0: 636f 6e66 6967 2854 7261 6e73 666f 726d  config(Transform
-000271d0: 6572 4f70 5061 7261 6c6c 656c 436f 6e66  erOpParallelConf
-000271e0: 6967 293a 2054 6865 2070 6172 616c 6c65  ig): The paralle
-000271f0: 6c20 636f 6e66 6967 7572 652e 2044 6566  l configure. Def
-00027200: 6175 6c74 2060 6465 6661 756c 745f 7472  ault `default_tr
-00027210: 616e 7366 6f72 6d65 725f 636f 6e66 6967  ansformer_config
-00027220: 602c 0a20 2020 2020 2020 2020 2020 2020  `,.             
-00027230: 2020 2061 6e20 696e 7374 616e 6365 206f     an instance o
-00027240: 6620 6054 7261 6e73 666f 726d 6572 4f70  f `TransformerOp
-00027250: 5061 7261 6c6c 656c 436f 6e66 6967 6020  ParallelConfig` 
-00027260: 7769 7468 2064 6566 6175 6c74 2061 7267  with default arg
-00027270: 732e 0a0a 2020 2020 2020 2020 496e 7075  s...        Inpu
-00027280: 7473 3a0a 2020 2020 2020 2020 2020 2020  ts:.            
-00027290: 2d20 2a2a 6869 6464 656e 5f73 7461 7465  - **hidden_state
-000272a0: 732a 2a20 2854 656e 736f 7229 202d 2054  s** (Tensor) - T
-000272b0: 656e 736f 722c 2073 6861 7065 2073 686f  ensor, shape sho
-000272c0: 756c 6420 6265 205b 6261 7463 685f 7369  uld be [batch_si
-000272d0: 7a65 2c20 7365 715f 6c65 6e67 7468 2c20  ze, seq_length, 
-000272e0: 6869 6464 656e 5f73 697a 655d 206f 720a  hidden_size] or.
-000272f0: 2020 2020 2020 2020 2020 2020 2020 5b62                [b
-00027300: 6174 6368 5f73 697a 6520 2a20 7365 715f  atch_size * seq_
-00027310: 6c65 6e67 7468 2c20 6869 6464 656e 5f73  length, hidden_s
-00027320: 697a 655d 2c20 6966 2074 6865 2075 7365  ize], if the use
-00027330: 5f70 6173 7420 6973 2046 616c 7365 206f  _past is False o
-00027340: 7220 6973 5f66 6972 7374 5f69 7465 7261  r is_first_itera
-00027350: 7469 6f6e 3d54 7275 652e 204f 7468 6572  tion=True. Other
-00027360: 7769 7365 2c0a 2020 2020 2020 2020 2020  wise,.          
-00027370: 2020 2020 7368 6f75 6c64 2062 6520 5b62      should be [b
-00027380: 6174 6368 5f73 697a 652c 2031 2c20 6869  atch_size, 1, hi
-00027390: 6464 656e 5f73 697a 655d 2e0a 2020 2020  dden_size]..    
-000273a0: 2020 2020 2020 2020 2d20 2a2a 6174 7465          - **atte
-000273b0: 6e74 696f 6e5f 6d61 736b 2a2a 2028 5465  ntion_mask** (Te
-000273c0: 6e73 6f72 2920 2d20 466c 6f61 7420 5465  nsor) - Float Te
-000273d0: 6e73 6f72 2c20 4966 2074 6865 2075 7365  nsor, If the use
-000273e0: 5f70 6173 7420 6973 2046 616c 7365 206f  _past is False o
-000273f0: 7220 6973 5f66 6972 7374 5f69 7465 7261  r is_first_itera
-00027400: 7469 6f6e 3d54 7275 652c 0a20 2020 2020  tion=True,.     
-00027410: 2020 2020 2020 2020 2074 6865 2061 7474           the att
-00027420: 656e 7469 6f6e 206d 6173 6b20 6d61 7472  ention mask matr
-00027430: 6978 2073 686f 756c 6420 6261 205b 6261  ix should ba [ba
-00027440: 7463 685f 7369 7a65 2c20 7365 715f 6c65  tch_size, seq_le
-00027450: 6e67 7468 2c20 7365 715f 6c65 6e67 7468  ngth, seq_length
-00027460: 5d2c 206f 7220 4e6f 6e65 2e20 4e6f 6e65  ], or None. None
-00027470: 206d 6561 6e73 2074 6865 7265 2077 696c   means there wil
-00027480: 6c0a 2020 2020 2020 2020 2020 2020 2020  l.              
-00027490: 6265 206e 6f20 6d61 736b 2069 6e20 736f  be no mask in so
-000274a0: 6674 6d61 7820 636f 6d70 7574 6174 696f  ftmax computatio
-000274b0: 6e2e 204f 7468 6572 7769 7365 2c20 7368  n. Otherwise, sh
-000274c0: 6f75 6c64 2062 6520 5b62 6174 6368 5f73  ould be [batch_s
-000274d0: 697a 652c 2031 2c20 6869 6464 656e 5f73  ize, 1, hidden_s
-000274e0: 697a 655d 0a20 2020 2020 2020 2020 2020  ize].           
-000274f0: 202d 202a 2a69 6e69 745f 7265 7365 742a   - **init_reset*
-00027500: 2a20 2854 656e 736f 7229 202d 2041 2062  * (Tensor) - A b
-00027510: 6f6f 6c20 7465 6e73 6f72 2077 6974 6820  ool tensor with 
-00027520: 7368 6170 6520 5b31 5d2c 2075 7365 6420  shape [1], used 
-00027530: 746f 2063 6c65 6172 2074 6865 2070 6173  to clear the pas
-00027540: 7420 6b65 7920 7061 7261 6d65 7465 7220  t key parameter 
-00027550: 616e 640a 2020 2020 2020 2020 2020 2020  and.            
-00027560: 2020 7061 7374 2076 616c 7565 2070 6172    past value par
-00027570: 616d 6574 6572 2075 7365 6420 696e 2074  ameter used in t
-00027580: 6865 2069 6e63 7265 6d65 6e74 616c 2070  he incremental p
-00027590: 7265 6469 6374 696f 6e2e 204f 6e6c 7920  rediction. Only 
-000275a0: 7661 6c69 6420 7768 656e 2075 7365 5f70  valid when use_p
-000275b0: 6173 7420 6973 2054 7275 652e 2044 6566  ast is True. Def
-000275c0: 6175 6c74 2054 7275 652e 0a20 2020 2020  ault True..     
-000275d0: 2020 2020 2020 202d 202a 2a62 6174 6368         - **batch
-000275e0: 5f76 616c 6964 5f6c 656e 6774 682a 2a20  _valid_length** 
-000275f0: 2854 656e 736f 7229 202d 2049 6e74 3332  (Tensor) - Int32
-00027600: 2074 656e 736f 7220 7769 7468 2073 6861   tensor with sha
-00027610: 7065 205b 6261 7463 685f 7369 7a65 5d20  pe [batch_size] 
-00027620: 7468 6520 7061 7374 2063 616c 6375 6c61  the past calcula
-00027630: 7465 6420 7468 6520 696e 6465 782e 0a20  ted the index.. 
-00027640: 2020 2020 2020 2020 2020 2020 2055 7365               Use
-00027650: 6420 666f 7220 696e 6372 656d 656e 7461  d for incrementa
-00027660: 6c20 7072 6564 6963 7469 6f6e 2077 6865  l prediction whe
-00027670: 6e20 7468 6520 7573 655f 7061 7374 2069  n the use_past i
-00027680: 7320 5472 7565 2e20 4465 6661 756c 7420  s True. Default 
-00027690: 4e6f 6e65 2e0a 0a20 2020 2020 2020 204f  None...        O
-000276a0: 7574 7075 7473 3a0a 2020 2020 2020 2020  utputs:.        
-000276b0: 2020 2020 5475 706c 652c 2061 2074 7570      Tuple, a tup
-000276c0: 6c65 2063 6f6e 7461 696e 7328 606f 7574  le contains(`out
-000276d0: 7075 7460 2c20 606c 6179 6572 5f70 7265  put`, `layer_pre
-000276e0: 7365 6e74 6029 0a0a 2020 2020 2020 2020  sent`)..        
-000276f0: 2020 2020 2d20 2a2a 6f75 7470 7574 2a2a      - **output**
-00027700: 2028 5465 6e73 6f72 2920 2d20 5468 6520   (Tensor) - The 
-00027710: 666c 6f61 7420 7465 6e73 6f72 206f 6620  float tensor of 
-00027720: 7468 6520 6f75 7470 7574 206f 6620 7468  the output of th
-00027730: 6520 6c61 7965 7220 7769 7468 0a20 2020  e layer with.   
-00027740: 2020 2020 2020 2020 2020 2073 6861 7065             shape
-00027750: 2028 6261 7463 685f 7369 7a65 2c20 7365   (batch_size, se
-00027760: 715f 6c65 6e67 7468 2c20 6869 6464 656e  q_length, hidden
-00027770: 5f73 697a 6529 206f 7220 2862 6174 6368  _size) or (batch
-00027780: 5f73 697a 6520 2a20 7365 715f 6c65 6e67  _size * seq_leng
-00027790: 7468 2c20 6869 6464 656e 5f73 697a 6529  th, hidden_size)
-000277a0: 2c20 6966 2074 6865 2075 7365 5f70 6173  , if the use_pas
-000277b0: 7420 6973 0a20 2020 2020 2020 2020 2020  t is.           
-000277c0: 2020 2046 616c 7365 206f 7220 6973 5f66     False or is_f
-000277d0: 6972 7374 5f69 7465 7261 7469 6f6e 3d54  irst_iteration=T
-000277e0: 7275 652e 204f 7468 6572 7769 7365 2c20  rue. Otherwise, 
-000277f0: 6974 2077 696c 6c20 6265 2028 6261 7463  it will be (batc
-00027800: 685f 7369 7a65 2c20 312c 2068 6964 6465  h_size, 1, hidde
-00027810: 6e5f 7369 7a65 292e 0a20 2020 2020 2020  n_size)..       
-00027820: 2020 2020 202d 202a 2a6c 6179 6572 5f70       - **layer_p
-00027830: 7265 7365 6e74 2a2a 2028 5475 706c 6529  resent** (Tuple)
-00027840: 202d 2041 2074 7570 6c65 2077 6974 6820   - A tuple with 
-00027850: 7369 7a65 206f 6620 6e75 6d5f 6c61 7965  size of num_laye
-00027860: 7273 2c20 7768 6572 6520 6561 6368 2074  rs, where each t
-00027870: 7570 6c65 2063 6f6e 7461 696e 7320 7468  uple contains th
-00027880: 6520 5465 6e73 6f72 2074 6865 0a20 2020  e Tensor the.   
-00027890: 2020 2020 2020 2020 2020 2070 726f 6a65             proje
-000278a0: 6374 6564 206b 6579 2061 6e64 2076 616c  cted key and val
-000278b0: 7565 2076 6563 746f 7220 7769 7468 2073  ue vector with s
-000278c0: 6861 7065 2028 2862 6174 6368 5f73 697a  hape ((batch_siz
-000278d0: 652c 206e 756d 5f68 6561 6473 2c20 7369  e, num_heads, si
-000278e0: 7a65 5f70 6572 5f68 6561 642c 2073 6571  ze_per_head, seq
-000278f0: 5f6c 656e 6774 6829 2c0a 2020 2020 2020  _length),.      
-00027900: 2020 2020 2020 2020 616e 6420 2862 6174          and (bat
-00027910: 6368 5f73 697a 652c 206e 756d 5f68 6561  ch_size, num_hea
-00027920: 6473 2c20 7365 715f 6c65 6e67 7468 2c20  ds, seq_length, 
-00027930: 7369 7a65 5f70 6572 5f68 6561 6429 292e  size_per_head)).
-00027940: 0a0a 2020 2020 2020 2020 5375 7070 6f72  ..        Suppor
-00027950: 7465 6420 506c 6174 666f 726d 733a 0a20  ted Platforms:. 
-00027960: 2020 2020 2020 2020 2020 2060 6041 7363             ``Asc
-00027970: 656e 6460 6020 6060 4750 5560 600a 0a20  end`` ``GPU``.. 
-00027980: 2020 2020 2020 2045 7861 6d70 6c65 733a         Examples:
-00027990: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
-000279a0: 2069 6d70 6f72 7420 6e75 6d70 7920 6173   import numpy as
-000279b0: 206e 700a 2020 2020 2020 2020 2020 2020   np.            
-000279c0: 3e3e 3e20 6672 6f6d 206d 696e 6473 706f  >>> from mindspo
-000279d0: 7265 2069 6d70 6f72 7420 6474 7970 6520  re import dtype 
-000279e0: 6173 206d 7374 7970 650a 2020 2020 2020  as mstype.      
-000279f0: 2020 2020 2020 3e3e 3e20 6672 6f6d 206d        >>> from m
-00027a00: 696e 6466 6f72 6d65 7273 2e6d 6f64 756c  indformers.modul
-00027a10: 6573 2e74 7261 6e73 666f 726d 6572 2069  es.transformer i
-00027a20: 6d70 6f72 7420 5472 616e 7366 6f72 6d65  mport Transforme
-00027a30: 7245 6e63 6f64 6572 0a20 2020 2020 2020  rEncoder.       
-00027a40: 2020 2020 203e 3e3e 2066 726f 6d20 6d69       >>> from mi
-00027a50: 6e64 7370 6f72 6520 696d 706f 7274 2054  ndspore import T
-00027a60: 656e 736f 720a 2020 2020 2020 2020 2020  ensor.          
-00027a70: 2020 3e3e 3e20 6d6f 6465 6c20 3d20 5472    >>> model = Tr
-00027a80: 616e 7366 6f72 6d65 7245 6e63 6f64 6572  ansformerEncoder
-00027a90: 2862 6174 6368 5f73 697a 653d 322c 206e  (batch_size=2, n
-00027aa0: 756d 5f6c 6179 6572 733d 322c 2068 6964  um_layers=2, hid
-00027ab0: 6465 6e5f 7369 7a65 3d38 2c20 6666 6e5f  den_size=8, ffn_
-00027ac0: 6869 6464 656e 5f73 697a 653d 3634 2c0a  hidden_size=64,.
-00027ad0: 2020 2020 2020 2020 2020 2020 2e2e 2e20              ... 
-00027ae0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00027af0: 2020 2020 2020 2020 2020 2073 6571 5f6c             seq_l
-00027b00: 656e 6774 683d 3136 2c20 6e75 6d5f 6865  ength=16, num_he
-00027b10: 6164 733d 3229 0a20 2020 2020 2020 2020  ads=2).         
-00027b20: 2020 203e 3e3e 2065 6e63 6f64 6572 5f69     >>> encoder_i
-00027b30: 6e70 7574 5f76 616c 7565 203d 2054 656e  nput_value = Ten
-00027b40: 736f 7228 6e70 2e6f 6e65 7328 2832 2c20  sor(np.ones((2, 
-00027b50: 3136 2c20 3829 292c 206d 7374 7970 652e  16, 8)), mstype.
-00027b60: 666c 6f61 7433 3229 0a20 2020 2020 2020  float32).       
-00027b70: 2020 2020 203e 3e3e 2065 6e63 6f64 6572       >>> encoder
-00027b80: 5f69 6e70 7574 5f6d 6173 6b20 3d20 5465  _input_mask = Te
-00027b90: 6e73 6f72 286e 702e 6f6e 6573 2828 322c  nsor(np.ones((2,
-00027ba0: 2031 362c 2031 3629 292c 206d 7374 7970   16, 16)), mstyp
-00027bb0: 652e 666c 6f61 7431 3629 0a20 2020 2020  e.float16).     
-00027bc0: 2020 2020 2020 203e 3e3e 206f 7574 7075         >>> outpu
-00027bd0: 742c 2070 6173 7420 3d20 6d6f 6465 6c28  t, past = model(
-00027be0: 656e 636f 6465 725f 696e 7075 745f 7661  encoder_input_va
-00027bf0: 6c75 652c 2065 6e63 6f64 6572 5f69 6e70  lue, encoder_inp
-00027c00: 7574 5f6d 6173 6b29 0a20 2020 2020 2020  ut_mask).       
-00027c10: 2020 2020 203e 3e3e 2070 7269 6e74 286f       >>> print(o
-00027c20: 7574 7075 742e 7368 6170 6529 0a20 2020  utput.shape).   
-00027c30: 2020 2020 2020 2020 2028 322c 2031 362c           (2, 16,
-00027c40: 2038 290a 2020 2020 2020 2020 2020 2020   8).            
-00027c50: 3e3e 3e20 7072 696e 7428 6c65 6e28 7061  >>> print(len(pa
-00027c60: 7374 2929 0a20 2020 2020 2020 2020 2020  st)).           
-00027c70: 2032 0a20 2020 2020 2020 2020 2020 203e   2.            >
-00027c80: 3e3e 2070 7269 6e74 2870 6173 745b 305d  >> print(past[0]
-00027c90: 5b30 5d2e 7368 6170 6529 0a20 2020 2020  [0].shape).     
-00027ca0: 2020 2020 2020 2028 322c 2032 2c20 342c         (2, 2, 4,
-00027cb0: 2031 3629 0a20 2020 2020 2020 2020 2020   16).           
-00027cc0: 203e 3e3e 2070 7269 6e74 2870 6173 745b   >>> print(past[
-00027cd0: 305d 5b31 5d2e 7368 6170 6529 0a20 2020  0][1].shape).   
-00027ce0: 2020 2020 2020 2020 2028 322c 2032 2c20           (2, 2, 
-00027cf0: 3136 2c20 3429 0a20 2020 2020 2020 2020  16, 4).         
-00027d00: 2020 203e 3e3e 2023 2057 6865 6e20 7573     >>> # When us
-00027d10: 6520 7573 655f 7061 7374 3d54 7275 652c  e use_past=True,
-00027d20: 2069 7420 696e 636c 7564 6573 2074 776f   it includes two
-00027d30: 2073 7465 7073 2074 6f20 696d 706c 656d   steps to implem
-00027d40: 656e 7420 7468 6520 696e 6372 656d 656e  ent the incremen
-00027d50: 7461 6c20 7072 6564 6963 7469 6f6e 2e0a  tal prediction..
-00027d60: 2020 2020 2020 2020 2020 2020 3e3e 3e20              >>> 
-00027d70: 2320 5374 6570 2031 3a20 7365 7420 6973  # Step 1: set is
-00027d80: 5f66 6972 7374 5f69 7465 7261 7469 6f6e  _first_iteration
-00027d90: 3d54 7275 652c 2061 6e64 2069 6e70 7574  =True, and input
-00027da0: 2074 6865 2066 756c 6c20 7365 7175 656e   the full sequen
-00027db0: 6365 206c 656e 6774 6827 7320 7374 6174  ce length's stat
-00027dc0: 652e 0a20 2020 2020 2020 2020 2020 203e  e..            >
-00027dd0: 3e3e 2062 6174 6368 5f76 616c 6964 5f6c  >> batch_valid_l
-00027de0: 656e 6774 6820 3d20 5465 6e73 6f72 286e  ength = Tensor(n
-00027df0: 702e 6f6e 6573 2828 322c 2929 2c20 6d73  p.ones((2,)), ms
-00027e00: 7479 7065 2e69 6e74 3332 290a 2020 2020  type.int32).    
-00027e10: 2020 2020 2020 2020 3e3e 3e20 696e 6974          >>> init
-00027e20: 5f72 6573 6574 203d 2054 656e 736f 7228  _reset = Tensor(
-00027e30: 5b54 7275 655d 2c20 6d73 7479 7065 2e62  [True], mstype.b
-00027e40: 6f6f 6c5f 290a 2020 2020 2020 2020 2020  ool_).          
-00027e50: 2020 3e3e 3e20 2320 5365 7420 6973 5f66    >>> # Set is_f
-00027e60: 6972 7374 5f69 7465 7261 7469 6f6e 3d54  irst_iteration=T
-00027e70: 7275 6520 746f 2067 656e 6572 6174 6520  rue to generate 
-00027e80: 7468 6520 6675 6c6c 206d 656d 6f72 7920  the full memory 
-00027e90: 7374 6174 6573 0a20 2020 2020 2020 2020  states.         
-00027ea0: 2020 203e 3e3e 206d 6f64 656c 203d 2054     >>> model = T
-00027eb0: 7261 6e73 666f 726d 6572 456e 636f 6465  ransformerEncode
-00027ec0: 7228 6261 7463 685f 7369 7a65 3d32 2c20  r(batch_size=2, 
-00027ed0: 6869 6464 656e 5f73 697a 653d 382c 2066  hidden_size=8, f
-00027ee0: 666e 5f68 6964 6465 6e5f 7369 7a65 3d36  fn_hidden_size=6
-00027ef0: 342c 2073 6571 5f6c 656e 6774 683d 3136  4, seq_length=16
-00027f00: 2c0a 2020 2020 2020 2020 2020 2020 2e2e  ,.            ..
-00027f10: 2e20 2020 2020 2020 2020 2020 2020 2020  .               
-00027f20: 2020 2020 2020 2020 2020 2020 206e 756d               num
-00027f30: 5f68 6561 6473 3d32 2c20 6e75 6d5f 6c61  _heads=2, num_la
-00027f40: 7965 7273 3d32 2c20 7573 655f 7061 7374  yers=2, use_past
-00027f50: 3d54 7275 6529 0a20 2020 2020 2020 2020  =True).         
-00027f60: 2020 203e 3e3e 206d 6f64 656c 2e61 6464     >>> model.add
-00027f70: 5f66 6c61 6773 5f72 6563 7572 7369 7665  _flags_recursive
-00027f80: 2869 735f 6669 7273 745f 6974 6572 6174  (is_first_iterat
-00027f90: 696f 6e3d 5472 7565 290a 2020 2020 2020  ion=True).      
-00027fa0: 2020 2020 2020 3e3e 3e20 6869 6464 656e        >>> hidden
-00027fb0: 2c20 7061 7374 203d 206d 6f64 656c 2865  , past = model(e
-00027fc0: 6e63 6f64 6572 5f69 6e70 7574 5f76 616c  ncoder_input_val
-00027fd0: 7565 2c20 656e 636f 6465 725f 696e 7075  ue, encoder_inpu
-00027fe0: 745f 6d61 736b 2c20 696e 6974 5f72 6573  t_mask, init_res
-00027ff0: 6574 2c20 6261 7463 685f 7661 6c69 645f  et, batch_valid_
-00028000: 6c65 6e67 7468 290a 2020 2020 2020 2020  length).        
-00028010: 2020 2020 3e3e 3e20 7072 696e 7428 6869      >>> print(hi
-00028020: 6464 656e 2e73 6861 7065 290a 2020 2020  dden.shape).    
-00028030: 2020 2020 2020 2020 2832 2c20 3136 2c20          (2, 16, 
-00028040: 3829 0a20 2020 2020 2020 2020 2020 203e  8).            >
-00028050: 3e3e 2070 7269 6e74 2870 6173 745b 305d  >> print(past[0]
-00028060: 5b30 5d2e 7368 6170 6529 0a20 2020 2020  [0].shape).     
-00028070: 2020 2020 2020 2028 322c 2032 2c20 342c         (2, 2, 4,
-00028080: 2031 3629 0a20 2020 2020 2020 2020 2020   16).           
-00028090: 203e 3e3e 2070 7269 6e74 2870 6173 745b   >>> print(past[
-000280a0: 305d 5b31 5d2e 7368 6170 6529 0a20 2020  0][1].shape).   
-000280b0: 2020 2020 2020 2020 2028 322c 2032 2c20           (2, 2, 
-000280c0: 3136 2c20 3429 0a20 2020 2020 2020 2020  16, 4).         
-000280d0: 2020 203e 3e3e 2065 6e63 6f64 6572 5f69     >>> encoder_i
-000280e0: 6e70 7574 5f76 616c 7565 203d 2054 656e  nput_value = Ten
-000280f0: 736f 7228 6e70 2e6f 6e65 7328 2832 2c20  sor(np.ones((2, 
-00028100: 312c 2038 2929 2c20 6d73 7479 7065 2e66  1, 8)), mstype.f
-00028110: 6c6f 6174 3332 290a 2020 2020 2020 2020  loat32).        
-00028120: 2020 2020 3e3e 3e20 656e 636f 6465 725f      >>> encoder_
-00028130: 696e 7075 745f 6d61 736b 203d 2054 656e  input_mask = Ten
-00028140: 736f 7228 6e70 2e6f 6e65 7328 2832 2c20  sor(np.ones((2, 
-00028150: 312c 2031 3629 292c 206d 7374 7970 652e  1, 16)), mstype.
-00028160: 666c 6f61 7431 3629 0a20 2020 2020 2020  float16).       
-00028170: 2020 2020 203e 3e3e 2069 6e69 745f 7265       >>> init_re
-00028180: 7365 7420 3d20 5465 6e73 6f72 285b 4661  set = Tensor([Fa
-00028190: 6c73 655d 2c20 6d73 7479 7065 2e62 6f6f  lse], mstype.boo
-000281a0: 6c5f 290a 2020 2020 2020 2020 2020 2020  l_).            
-000281b0: 3e3e 3e20 2320 5374 6570 2032 3a20 7365  >>> # Step 2: se
-000281c0: 7420 6973 5f66 6972 7374 5f69 7465 7261  t is_first_itera
-000281d0: 7469 6f6e 3d46 616c 7365 2c20 616e 6420  tion=False, and 
-000281e0: 7061 7373 2074 6865 2073 696e 676c 6520  pass the single 
-000281f0: 776f 7264 2074 6f20 7275 6e20 7468 6520  word to run the 
-00028200: 7072 6564 6963 7469 6f6e 2072 6174 6865  prediction rathe
-00028210: 7220 7468 616e 0a20 2020 2020 2020 2020  r than.         
-00028220: 2020 203e 3e3e 2023 2074 6865 2066 756c     >>> # the ful
-00028230: 6c20 7365 7175 656e 6365 2e0a 2020 2020  l sequence..    
-00028240: 2020 2020 2020 2020 3e3e 3e20 6d6f 6465          >>> mode
-00028250: 6c2e 6164 645f 666c 6167 735f 7265 6375  l.add_flags_recu
-00028260: 7273 6976 6528 6973 5f66 6972 7374 5f69  rsive(is_first_i
-00028270: 7465 7261 7469 6f6e 3d46 616c 7365 290a  teration=False).
-00028280: 2020 2020 2020 2020 2020 2020 3e3e 3e20              >>> 
-00028290: 6869 6464 656e 2c20 7061 7374 203d 206d  hidden, past = m
-000282a0: 6f64 656c 2865 6e63 6f64 6572 5f69 6e70  odel(encoder_inp
-000282b0: 7574 5f76 616c 7565 2c20 656e 636f 6465  ut_value, encode
-000282c0: 725f 696e 7075 745f 6d61 736b 2c20 696e  r_input_mask, in
-000282d0: 6974 5f72 6573 6574 2c20 6261 7463 685f  it_reset, batch_
-000282e0: 7661 6c69 645f 6c65 6e67 7468 290a 2020  valid_length).  
-000282f0: 2020 2020 2020 2020 2020 3e3e 3e20 7072            >>> pr
-00028300: 696e 7428 6869 6464 656e 2e73 6861 7065  int(hidden.shape
-00028310: 290a 2020 2020 2020 2020 2020 2020 2832  ).            (2
-00028320: 2c20 312c 2038 290a 2020 2020 2020 2020  , 1, 8).        
-00028330: 2020 2020 3e3e 3e20 7072 696e 7428 7061      >>> print(pa
-00028340: 7374 5b30 5d5b 305d 2e73 6861 7065 290a  st[0][0].shape).
-00028350: 2020 2020 2020 2020 2020 2020 2832 2c20              (2, 
-00028360: 322c 2034 2c20 3136 290a 2020 2020 2020  2, 4, 16).      
-00028370: 2020 2020 2020 3e3e 3e20 7072 696e 7428        >>> print(
-00028380: 7061 7374 5b30 5d5b 315d 2e73 6861 7065  past[0][1].shape
-00028390: 290a 2020 2020 2020 2020 2020 2020 2832  ).            (2
-000283a0: 2c20 322c 2031 362c 2034 290a 2020 2020  , 2, 16, 4).    
-000283b0: 2222 220a 0a20 2020 2040 5f4c 6f67 4163  """..    @_LogAc
-000283c0: 7469 6f6e 4f6e 6365 286d 5f6c 6f67 6765  tionOnce(m_logge
-000283d0: 723d 6c6f 6767 6572 2c20 6b65 793d 2754  r=logger, key='T
-000283e0: 7261 6e73 666f 726d 6572 456e 636f 6465  ransformerEncode
-000283f0: 7227 2c0a 2020 2020 2020 2020 2020 2020  r',.            
-00028400: 2020 2020 2020 2020 6e6f 5f77 6172 6e69          no_warni
-00028410: 6e67 3d5f 6765 745f 7061 7261 6c6c 656c  ng=_get_parallel
-00028420: 5f6d 6f64 6528 2920 696e 2028 5061 7261  _mode() in (Para
-00028430: 6c6c 656c 4d6f 6465 2e53 5441 4e44 5f41  llelMode.STAND_A
-00028440: 4c4f 4e45 2c29 290a 2020 2020 405f 6172  LONE,)).    @_ar
-00028450: 6773 5f74 7970 655f 7661 6c69 6461 746f  gs_type_validato
-00028460: 725f 6368 6563 6b28 6869 6464 656e 5f73  r_check(hidden_s
-00028470: 697a 653d 5661 6c69 6461 746f 722e 6368  ize=Validator.ch
-00028480: 6563 6b5f 706f 7369 7469 7665 5f69 6e74  eck_positive_int
-00028490: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-000284a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000284b0: 2020 6e75 6d5f 6865 6164 733d 5661 6c69    num_heads=Vali
-000284c0: 6461 746f 722e 6368 6563 6b5f 706f 7369  dator.check_posi
-000284d0: 7469 7665 5f69 6e74 2c0a 2020 2020 2020  tive_int,.      
-000284e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000284f0: 2020 2020 2020 2020 2020 6666 6e5f 6869            ffn_hi
-00028500: 6464 656e 5f73 697a 653d 5661 6c69 6461  dden_size=Valida
-00028510: 746f 722e 6368 6563 6b5f 706f 7369 7469  tor.check_positi
-00028520: 7665 5f69 6e74 2c0a 2020 2020 2020 2020  ve_int,.        
-00028530: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00028540: 2020 2020 2020 2020 7365 715f 6c65 6e67          seq_leng
-00028550: 7468 3d56 616c 6964 6174 6f72 2e63 6865  th=Validator.che
-00028560: 636b 5f70 6f73 6974 6976 655f 696e 742c  ck_positive_int,
-00028570: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00028580: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00028590: 206e 756d 5f6c 6179 6572 733d 5661 6c69   num_layers=Vali
-000285a0: 6461 746f 722e 6368 6563 6b5f 706f 7369  dator.check_posi
-000285b0: 7469 7665 5f69 6e74 2c0a 2020 2020 2020  tive_int,.      
-000285c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000285d0: 2020 2020 2020 2020 2020 6f66 6673 6574            offset
-000285e0: 3d56 616c 6964 6174 6f72 2e63 6865 636b  =Validator.check
-000285f0: 5f6e 6f6e 5f6e 6567 6174 6976 655f 696e  _non_negative_in
-00028600: 742c 0a20 2020 2020 2020 2020 2020 2020  t,.             
-00028610: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00028620: 2020 2061 7474 656e 7469 6f6e 5f64 726f     attention_dro
-00028630: 706f 7574 5f72 6174 653d 5661 6c69 6461  pout_rate=Valida
-00028640: 746f 722e 6368 6563 6b5f 6e6f 6e5f 6e65  tor.check_non_ne
-00028650: 6761 7469 7665 5f66 6c6f 6174 2c0a 2020  gative_float,.  
-00028660: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00028670: 2020 2020 2020 2020 2020 2020 2020 6869                hi
-00028680: 6464 656e 5f64 726f 706f 7574 5f72 6174  dden_dropout_rat
-00028690: 653d 5661 6c69 6461 746f 722e 6368 6563  e=Validator.chec
-000286a0: 6b5f 6e6f 6e5f 6e65 6761 7469 7665 5f66  k_non_negative_f
-000286b0: 6c6f 6174 2c0a 2020 2020 2020 2020 2020  loat,.          
-000286c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000286d0: 2020 2020 2020 706f 7374 5f6c 6179 6572        post_layer
-000286e0: 6e6f 726d 5f72 6573 6964 7561 6c3d 5661  norm_residual=Va
-000286f0: 6c69 6461 746f 722e 6368 6563 6b5f 626f  lidator.check_bo
-00028700: 6f6c 2c0a 2020 2020 2020 2020 2020 2020  ol,.            
-00028710: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00028720: 2020 2020 6c61 7965 726e 6f72 6d5f 636f      layernorm_co
-00028730: 6d70 7574 655f 7479 7065 3d5f 7661 6c69  mpute_type=_vali
-00028740: 645f 7661 6c75 655f 6368 6563 6b73 285b  d_value_checks([
-00028750: 6d73 7479 7065 2e66 6c6f 6174 3332 2c0a  mstype.float32,.
-00028760: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00028770: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00028780: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00028790: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000287a0: 2020 2020 2020 2020 2020 2020 6d73 7479              msty
-000287b0: 7065 2e66 6c6f 6174 3136 2c20 6d73 7479  pe.float16, msty
-000287c0: 7065 2e62 666c 6f61 7431 365d 2c0a 2020  pe.bfloat16],.  
-000287d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000287e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000287f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00028800: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00028810: 2020 2020 2020 2020 2022 5472 616e 7366           "Transf
-00028820: 6f72 6d65 7245 6e63 6f64 6572 2229 2c0a  ormerEncoder"),.
-00028830: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00028840: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00028850: 736f 6674 6d61 785f 636f 6d70 7574 655f  softmax_compute_
-00028860: 7479 7065 3d5f 7661 6c69 645f 7661 6c75  type=_valid_valu
-00028870: 655f 6368 6563 6b73 285b 6d73 7479 7065  e_checks([mstype
-00028880: 2e66 6c6f 6174 3332 2c0a 2020 2020 2020  .float32,.      
-00028890: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000288a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000288b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000288c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000288d0: 2020 2020 6d73 7479 7065 2e66 6c6f 6174      mstype.float
-000288e0: 3136 2c20 6d73 7479 7065 2e62 666c 6f61  16, mstype.bfloa
-000288f0: 7431 365d 2c0a 2020 2020 2020 2020 2020  t16],.          
-00028900: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00028910: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00028920: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00028930: 2020 2020 2020 2020 2020 2020 2020 2022                 "
-00028940: 5472 616e 7366 6f72 6d65 7245 6e63 6f64  TransformerEncod
-00028950: 6572 2229 2c0a 2020 2020 2020 2020 2020  er"),.          
-00028960: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00028970: 2020 2020 2020 7061 7261 6d5f 696e 6974        param_init
-00028980: 5f74 7970 653d 5f76 616c 6964 5f76 616c  _type=_valid_val
-00028990: 7565 5f63 6865 636b 7328 5b6d 7374 7970  ue_checks([mstyp
-000289a0: 652e 666c 6f61 7433 322c 206d 7374 7970  e.float32, mstyp
-000289b0: 652e 666c 6f61 7431 362c 206d 7374 7970  e.float16, mstyp
-000289c0: 652e 6266 6c6f 6174 3136 5d2c 0a20 2020  e.bfloat16],.   
+00023720: 2020 2020 2020 2020 2020 2020 2020 2068                 h
+00023730: 6964 6465 6e5f 6163 743d 6869 6464 656e  idden_act=hidden
+00023740: 5f61 6374 2c0a 2020 2020 2020 2020 2020  _act,.          
+00023750: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00023760: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00023770: 7061 7261 6d5f 696e 6974 5f74 7970 653d  param_init_type=
+00023780: 7061 7261 6d5f 696e 6974 5f74 7970 652c  param_init_type,
+00023790: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+000237a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000237b0: 2020 2020 2020 2020 2020 2070 6172 616c             paral
+000237c0: 6c65 6c5f 636f 6e66 6967 3d70 6172 616c  lel_config=paral
+000237d0: 6c65 6c5f 636f 6e66 6967 290a 2020 2020  lel_config).    
+000237e0: 2020 2020 2020 2020 7365 6c66 2e70 6f73          self.pos
+000237f0: 745f 6c61 7965 726e 6f72 6d5f 7265 7369  t_layernorm_resi
+00023800: 6475 616c 203d 2070 6f73 745f 6c61 7965  dual = post_laye
+00023810: 726e 6f72 6d5f 7265 7369 6475 616c 0a20  rnorm_residual. 
+00023820: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+00023830: 6164 6420 3d20 502e 4164 6428 292e 7368  add = P.Add().sh
+00023840: 6172 6428 2828 7061 7261 6c6c 656c 5f63  ard(((parallel_c
+00023850: 6f6e 6669 672e 6461 7461 5f70 6172 616c  onfig.data_paral
+00023860: 6c65 6c2c 2031 292c 2028 7061 7261 6c6c  lel, 1), (parall
+00023870: 656c 5f63 6f6e 6669 672e 6461 7461 5f70  el_config.data_p
+00023880: 6172 616c 6c65 6c2c 2031 2929 290a 2020  arallel, 1))).  
+00023890: 2020 2020 2020 2020 2020 7365 6c66 2e61            self.a
+000238a0: 6464 5f33 6420 3d20 502e 4164 6428 292e  dd_3d = P.Add().
+000238b0: 7368 6172 6428 2828 7061 7261 6c6c 656c  shard(((parallel
+000238c0: 5f63 6f6e 6669 672e 6461 7461 5f70 6172  _config.data_par
+000238d0: 616c 6c65 6c2c 2031 2c20 3129 2c20 2870  allel, 1, 1), (p
+000238e0: 6172 616c 6c65 6c5f 636f 6e66 6967 2e64  arallel_config.d
+000238f0: 6174 615f 7061 7261 6c6c 656c 2c20 312c  ata_parallel, 1,
+00023900: 2031 2929 290a 2020 2020 2020 2020 2020   1))).          
+00023910: 2020 7365 6c66 2e64 7479 7065 203d 206d    self.dtype = m
+00023920: 7374 7970 652e 666c 6f61 7431 360a 2020  stype.float16.  
+00023930: 2020 2020 2020 2020 2020 7365 6c66 2e6b            self.k
+00023940: 6579 5f70 6173 7420 3d20 4e6f 6e65 0a20  ey_past = None. 
+00023950: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+00023960: 7661 6c75 655f 7061 7374 203d 204e 6f6e  value_past = Non
+00023970: 650a 2020 2020 2020 2020 2020 2020 6966  e.            if
+00023980: 2073 656c 662e 7573 655f 7061 7374 3a0a   self.use_past:.
+00023990: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000239a0: 2320 6f70 6572 6174 6f72 2075 7365 6420  # operator used 
+000239b0: 666f 7220 7374 6174 6520 7265 7573 650a  for state reuse.
+000239c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000239d0: 7365 6c66 2e72 6564 7563 6573 756d 203d  self.reducesum =
+000239e0: 2050 2e52 6564 7563 6553 756d 2829 2e73   P.ReduceSum().s
+000239f0: 6861 7264 2828 2831 2c20 312c 2031 2c20  hard(((1, 1, 1, 
+00023a00: 3129 2c29 290a 2020 2020 2020 2020 2020  1),)).          
+00023a10: 2020 2020 2020 7365 6c66 2e6e 6f74 5f65        self.not_e
+00023a20: 7175 616c 203d 2050 2e4e 6f74 4571 7561  qual = P.NotEqua
+00023a30: 6c28 292e 7368 6172 6428 2828 312c 2031  l().shard(((1, 1
+00023a40: 2c20 312c 2031 292c 2028 2929 290a 2020  , 1, 1), ())).  
+00023a50: 2020 2020 2020 2020 2020 2020 2020 7365                se
+00023a60: 6c66 2e73 6c69 6365 203d 2050 2e53 7472  lf.slice = P.Str
+00023a70: 6964 6564 536c 6963 6528 292e 7368 6172  idedSlice().shar
+00023a80: 6428 2828 312c 2031 2c20 312c 2031 292c  d(((1, 1, 1, 1),
+00023a90: 2929 0a20 2020 2020 2020 2020 2020 2020  )).             
+00023aa0: 2020 2073 697a 655f 7065 725f 6865 6164     size_per_head
+00023ab0: 203d 2068 6964 6465 6e5f 7369 7a65 202f   = hidden_size /
+00023ac0: 2f20 6e75 6d5f 6865 6164 730a 2020 2020  / num_heads.    
+00023ad0: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+00023ae0: 2e6b 6579 5f73 6861 7065 203d 2028 6261  .key_shape = (ba
+00023af0: 7463 685f 7369 7a65 2c20 6e75 6d5f 6865  tch_size, num_he
+00023b00: 6164 732c 2073 697a 655f 7065 725f 6865  ads, size_per_he
+00023b10: 6164 2c20 7467 745f 7365 715f 6c65 6e67  ad, tgt_seq_leng
+00023b20: 7468 290a 2020 2020 2020 2020 2020 2020  th).            
+00023b30: 2020 2020 7365 6c66 2e76 616c 7565 5f73      self.value_s
+00023b40: 6861 7065 203d 2028 6261 7463 685f 7369  hape = (batch_si
+00023b50: 7a65 2c20 6e75 6d5f 6865 6164 732c 2074  ze, num_heads, t
+00023b60: 6774 5f73 6571 5f6c 656e 6774 682c 2073  gt_seq_length, s
+00023b70: 697a 655f 7065 725f 6865 6164 290a 2020  ize_per_head).  
+00023b80: 2020 2020 2020 2020 2020 2020 2020 2320                # 
+00023b90: 7061 7261 6d65 7465 7273 2073 6176 696e  parameters savin
+00023ba0: 6720 6b65 7920 616e 6420 7661 6c75 6520  g key and value 
+00023bb0: 7374 6174 6573 0a20 2020 2020 2020 2020  states.         
+00023bc0: 2020 2020 2020 2073 656c 662e 6b65 795f         self.key_
+00023bd0: 7061 7374 203d 2050 6172 616d 6574 6572  past = Parameter
+00023be0: 2854 656e 736f 7228 6e70 2e7a 6572 6f73  (Tensor(np.zeros
+00023bf0: 2873 6861 7065 3d73 656c 662e 6b65 795f  (shape=self.key_
+00023c00: 7368 6170 6529 2c20 7365 6c66 2e64 7479  shape), self.dty
+00023c10: 7065 292c 206e 616d 653d 226b 6579 5f70  pe), name="key_p
+00023c20: 6173 7422 290a 2020 2020 2020 2020 2020  ast").          
+00023c30: 2020 2020 2020 7365 6c66 2e76 616c 7565        self.value
+00023c40: 5f70 6173 7420 3d20 5061 7261 6d65 7465  _past = Paramete
+00023c50: 7228 5465 6e73 6f72 286e 702e 7a65 726f  r(Tensor(np.zero
+00023c60: 7328 7368 6170 653d 7365 6c66 2e76 616c  s(shape=self.val
+00023c70: 7565 5f73 6861 7065 292c 2073 656c 662e  ue_shape), self.
+00023c80: 6474 7970 6529 2c20 6e61 6d65 3d22 7661  dtype), name="va
+00023c90: 6c75 655f 7061 7374 2229 0a20 2020 2020  lue_past").     
+00023ca0: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+00023cb0: 7469 6c65 203d 2050 2e54 696c 6528 292e  tile = P.Tile().
+00023cc0: 7368 6172 6428 2828 312c 2031 292c 2929  shard(((1, 1),))
+00023cd0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00023ce0: 2073 656c 662e 6d75 6c20 3d20 502e 4d75   self.mul = P.Mu
+00023cf0: 6c28 292e 7368 6172 6428 2828 312c 2031  l().shard(((1, 1
+00023d00: 2c20 312c 2031 292c 2028 312c 2929 290a  , 1, 1), (1,))).
+00023d10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00023d20: 7365 6c66 2e61 7373 6967 6e20 3d20 502e  self.assign = P.
+00023d30: 4173 7369 676e 2829 2e73 6861 7264 2828  Assign().shard((
+00023d40: 2831 2c20 312c 2031 2c20 3129 2c20 2831  (1, 1, 1, 1), (1
+00023d50: 2c20 312c 2031 2c20 3129 2929 0a0a 2020  , 1, 1, 1)))..  
+00023d60: 2020 2020 2020 2020 2020 6966 2070 6172            if par
+00023d70: 616c 6c65 6c5f 636f 6e66 6967 2e75 7365  allel_config.use
+00023d80: 5f73 6571 5f70 6172 616c 6c65 6c3a 0a20  _seq_parallel:. 
+00023d90: 2020 2020 2020 2020 2020 2020 2020 2073                 s
+00023da0: 656c 662e 6164 642e 7368 6172 6428 2828  elf.add.shard(((
+00023db0: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
+00023dc0: 6461 7461 5f70 6172 616c 6c65 6c20 2a20  data_parallel * 
+00023dd0: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
+00023de0: 6d6f 6465 6c5f 7061 7261 6c6c 656c 2c20  model_parallel, 
+00023df0: 3129 2c0a 2020 2020 2020 2020 2020 2020  1),.            
+00023e00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00023e10: 2020 2020 2870 6172 616c 6c65 6c5f 636f      (parallel_co
+00023e20: 6e66 6967 2e64 6174 615f 7061 7261 6c6c  nfig.data_parall
+00023e30: 656c 202a 2070 6172 616c 6c65 6c5f 636f  el * parallel_co
+00023e40: 6e66 6967 2e6d 6f64 656c 5f70 6172 616c  nfig.model_paral
+00023e50: 6c65 6c2c 2031 2929 290a 2020 2020 2020  lel, 1))).      
+00023e60: 2020 2020 2020 2020 2020 7365 6c66 2e6c            self.l
+00023e70: 6179 6572 6e6f 726d 312e 7368 6172 6428  ayernorm1.shard(
+00023e80: 2828 7061 7261 6c6c 656c 5f63 6f6e 6669  ((parallel_confi
+00023e90: 672e 6461 7461 5f70 6172 616c 6c65 6c20  g.data_parallel 
+00023ea0: 2a20 7061 7261 6c6c 656c 5f63 6f6e 6669  * parallel_confi
+00023eb0: 672e 6d6f 6465 6c5f 7061 7261 6c6c 656c  g.model_parallel
+00023ec0: 2c20 3129 2c29 290a 2020 2020 2020 2020  , 1),)).        
+00023ed0: 2020 2020 2020 2020 7365 6c66 2e6c 6179          self.lay
+00023ee0: 6572 6e6f 726d 322e 7368 6172 6428 2828  ernorm2.shard(((
+00023ef0: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
+00023f00: 6461 7461 5f70 6172 616c 6c65 6c20 2a20  data_parallel * 
+00023f10: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
+00023f20: 6d6f 6465 6c5f 7061 7261 6c6c 656c 2c20  model_parallel, 
+00023f30: 3129 2c29 290a 2020 2020 2020 2020 2020  1),)).          
+00023f40: 2020 2020 2020 6966 2070 6172 616c 6c65        if paralle
+00023f50: 6c5f 636f 6e66 6967 2e72 6563 6f6d 7075  l_config.recompu
+00023f60: 7465 2e73 656c 6563 745f 7265 636f 6d70  te.select_recomp
+00023f70: 7574 653a 0a20 2020 2020 2020 2020 2020  ute:.           
+00023f80: 2020 2020 2020 2020 2023 20e6 ada4 e5a4           # .....
+00023f90: 84e4 bc9a e6b6 88e8 8097 e8be 83e5 a4a7  ................
+00023fa0: e586 85e5 ad98 efbc 8ce5 bc80 e590 afe5  ................
+00023fb0: 908e e4bc 9ae6 8d9f e5a4 b1e4 b880 e983  ................
+00023fc0: a8e5 8886 e8ae a1e7 ae97 e680 a7e8 83bd  ................
+00023fd0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00023fe0: 2020 2020 2073 656c 662e 6c61 7965 726e       self.layern
+00023ff0: 6f72 6d32 2e6c 6179 6572 5f6e 6f72 6d2e  orm2.layer_norm.
+00024000: 7265 636f 6d70 7574 6528 290a 2020 2020  recompute().    
+00024010: 2020 2020 2020 2020 2020 2020 6966 206e              if n
+00024020: 6f74 2073 656c 662e 7573 655f 6d6f 653a  ot self.use_moe:
+00024030: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00024040: 2020 2020 2073 656c 662e 6f75 7470 7574       self.output
+00024050: 2e70 726f 6a65 6374 696f 6e2e 7368 6172  .projection.shar
+00024060: 6428 0a20 2020 2020 2020 2020 2020 2020  d(.             
+00024070: 2020 2020 2020 2020 2020 2073 7472 6174             strat
+00024080: 6567 795f 6269 6173 3d28 2870 6172 616c  egy_bias=((paral
+00024090: 6c65 6c5f 636f 6e66 6967 2e64 6174 615f  lel_config.data_
+000240a0: 7061 7261 6c6c 656c 202a 2070 6172 616c  parallel * paral
+000240b0: 6c65 6c5f 636f 6e66 6967 2e6d 6f64 656c  lel_config.model
+000240c0: 5f70 6172 616c 6c65 6c2c 2031 292c 2028  _parallel, 1), (
+000240d0: 312c 2929 2c0a 2020 2020 2020 2020 2020  1,)),.          
+000240e0: 2020 2020 2020 2020 2020 2020 2020 7374                st
+000240f0: 7261 7465 6779 5f6d 6174 6d75 6c3d 2828  rategy_matmul=((
+00024100: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
+00024110: 6461 7461 5f70 6172 616c 6c65 6c2c 2070  data_parallel, p
+00024120: 6172 616c 6c65 6c5f 636f 6e66 6967 2e6d  arallel_config.m
+00024130: 6f64 656c 5f70 6172 616c 6c65 6c29 2c0a  odel_parallel),.
+00024140: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00024150: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00024160: 2020 2020 2020 2020 2028 7061 7261 6c6c           (parall
+00024170: 656c 5f63 6f6e 6669 672e 6d6f 6465 6c5f  el_config.model_
+00024180: 7061 7261 6c6c 656c 2c20 3129 292c 0a20  parallel, 1)),. 
+00024190: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000241a0: 2020 2020 2020 206f 7574 5f73 7472 6174         out_strat
+000241b0: 6567 795f 6d61 746d 756c 3d28 2870 6172  egy_matmul=((par
+000241c0: 616c 6c65 6c5f 636f 6e66 6967 2e64 6174  allel_config.dat
+000241d0: 615f 7061 7261 6c6c 656c 202a 2070 6172  a_parallel * par
+000241e0: 616c 6c65 6c5f 636f 6e66 6967 2e6d 6f64  allel_config.mod
+000241f0: 656c 5f70 6172 616c 6c65 6c2c 2031 292c  el_parallel, 1),
+00024200: 2929 0a20 2020 2020 2020 2020 2020 2020  )).             
+00024210: 2020 2020 2020 2073 656c 662e 6f75 7470         self.outp
+00024220: 7574 2e64 726f 706f 7574 2e64 726f 706f  ut.dropout.dropo
+00024230: 7574 2e73 6861 7264 280a 2020 2020 2020  ut.shard(.      
+00024240: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00024250: 2020 2828 7061 7261 6c6c 656c 5f63 6f6e    ((parallel_con
+00024260: 6669 672e 6461 7461 5f70 6172 616c 6c65  fig.data_paralle
+00024270: 6c20 2a20 7061 7261 6c6c 656c 5f63 6f6e  l * parallel_con
+00024280: 6669 672e 6d6f 6465 6c5f 7061 7261 6c6c  fig.model_parall
+00024290: 656c 2c20 3129 2c29 290a 2020 2020 2020  el, 1),)).      
+000242a0: 2020 656c 7365 3a0a 2020 2020 2020 2020    else:.        
+000242b0: 2020 2020 7261 6973 6520 5275 6e74 696d      raise Runtim
+000242c0: 6545 7272 6f72 2866 2254 6865 207b 7365  eError(f"The {se
+000242d0: 6c66 2e63 6c73 5f6e 616d 657d 206f 6e6c  lf.cls_name} onl
+000242e0: 7920 7375 7070 6f72 7420 7368 6172 6469  y support shardi
+000242f0: 6e67 2070 726f 7061 6761 7469 6f6e 206f  ng propagation o
+00024300: 7220 220a 2020 2020 2020 2020 2020 2020  r ".            
+00024310: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00024320: 2020 2066 2273 656d 692d 6175 746f 2070     f"semi-auto p
+00024330: 6172 616c 6c65 6c20 6d6f 6465 206e 6f77  arallel mode now
+00024340: 2e22 290a 0a20 2020 2064 6566 2063 6f6e  .")..    def con
+00024350: 7374 7275 6374 2873 656c 662c 2068 6964  struct(self, hid
+00024360: 6465 6e5f 7374 6174 732c 0a20 2020 2020  den_stats,.     
+00024370: 2020 2020 2020 2020 2020 2020 2064 6563               dec
+00024380: 6f64 6572 5f6d 6173 6b2c 0a20 2020 2020  oder_mask,.     
+00024390: 2020 2020 2020 2020 2020 2020 2065 6e63               enc
+000243a0: 6f64 6572 5f6f 7574 7075 743d 4e6f 6e65  oder_output=None
+000243b0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+000243c0: 2020 2020 6d65 6d6f 7279 5f6d 6173 6b3d      memory_mask=
+000243d0: 4e6f 6e65 2c0a 2020 2020 2020 2020 2020  None,.          
+000243e0: 2020 2020 2020 2020 696e 6974 5f72 6573          init_res
+000243f0: 6574 3d54 7275 652c 2062 6174 6368 5f76  et=True, batch_v
+00024400: 616c 6964 5f6c 656e 6774 683d 4e6f 6e65  alid_length=None
+00024410: 293a 0a20 2020 2020 2020 2022 2222 666f  ):.        """fo
+00024420: 7277 6172 6420 7072 6f63 6573 7322 2222  rward process"""
+00024430: 0a20 2020 2020 2020 2073 656c 662e 5f63  .        self._c
+00024440: 6865 636b 5f69 6e70 7574 2868 6964 6465  heck_input(hidde
+00024450: 6e5f 7374 6174 732c 2064 6563 6f64 6572  n_stats, decoder
+00024460: 5f6d 6173 6b2c 2065 6e63 6f64 6572 5f6f  _mask, encoder_o
+00024470: 7574 7075 742c 206d 656d 6f72 795f 6d61  utput, memory_ma
+00024480: 736b 2c20 696e 6974 5f72 6573 6574 2c20  sk, init_reset, 
+00024490: 6261 7463 685f 7661 6c69 645f 6c65 6e67  batch_valid_leng
+000244a0: 7468 290a 2020 2020 2020 2020 2320 7468  th).        # th
+000244b0: 6520 7265 7475 726e 6564 2073 6861 7065  e returned shape
+000244c0: 2069 7320 5b62 732c 2073 6571 5f6c 656e   is [bs, seq_len
+000244d0: 6774 682c 2065 6d62 6564 6469 6e67 5f73  gth, embedding_s
+000244e0: 697a 655d 206f 7220 5b62 7320 2a20 7365  ize] or [bs * se
+000244f0: 715f 6c65 6e67 7468 2c20 656d 6265 6464  q_length, embedd
+00024500: 696e 675f 7369 7a65 5d0a 2020 2020 2020  ing_size].      
+00024510: 2020 6869 6464 656e 5f73 6861 7065 203d    hidden_shape =
+00024520: 2046 2e73 6861 7065 2868 6964 6465 6e5f   F.shape(hidden_
+00024530: 7374 6174 7329 0a20 2020 2020 2020 2068  stats).        h
+00024540: 6964 6465 6e5f 7374 6174 7320 3d20 462e  idden_stats = F.
+00024550: 7265 7368 6170 6528 6869 6464 656e 5f73  reshape(hidden_s
+00024560: 7461 7473 2c20 282d 312c 2068 6964 6465  tats, (-1, hidde
+00024570: 6e5f 7368 6170 655b 2d31 5d29 290a 2020  n_shape[-1])).  
+00024580: 2020 2020 2020 696e 7075 745f 7820 3d20        input_x = 
+00024590: 7365 6c66 2e6c 6179 6572 6e6f 726d 3128  self.layernorm1(
+000245a0: 6869 6464 656e 5f73 7461 7473 290a 2020  hidden_stats).  
+000245b0: 2020 2020 2020 696e 7075 745f 7820 3d20        input_x = 
+000245c0: 462e 6361 7374 2869 6e70 7574 5f78 2c20  F.cast(input_x, 
+000245d0: 7365 6c66 2e64 7479 7065 290a 0a20 2020  self.dtype)..   
+000245e0: 2020 2020 2023 2069 6e64 6963 6174 6520       # indicate 
+000245f0: 7768 6574 6865 7220 7265 7365 7420 7361  whether reset sa
+00024600: 7665 6420 7374 6174 6573 0a20 2020 2020  ved states.     
+00024610: 2020 206b 6579 5f72 6573 6574 203d 204e     key_reset = N
+00024620: 6f6e 650a 2020 2020 2020 2020 7661 6c75  one.        valu
+00024630: 655f 7265 7365 7420 3d20 4e6f 6e65 0a20  e_reset = None. 
+00024640: 2020 2020 2020 2069 6620 7365 6c66 2e75         if self.u
+00024650: 7365 5f70 6173 743a 0a20 2020 2020 2020  se_past:.       
+00024660: 2020 2020 2023 2072 6573 6574 2073 7461       # reset sta
+00024670: 7465 732c 2069 6e69 745f 7265 7365 7420  tes, init_reset 
+00024680: 5472 7565 2066 6f72 2072 6575 7365 2061  True for reuse a
+00024690: 6e64 2046 616c 7365 2066 6f72 2072 6573  nd False for res
+000246a0: 6574 0a20 2020 2020 2020 2020 2020 2073  et.            s
+000246b0: 656c 662e 6173 7369 676e 2873 656c 662e  elf.assign(self.
+000246c0: 6b65 795f 7061 7374 2c20 7365 6c66 2e6d  key_past, self.m
+000246d0: 756c 2873 656c 662e 6b65 795f 7061 7374  ul(self.key_past
+000246e0: 2c20 462e 6361 7374 2869 6e69 745f 7265  , F.cast(init_re
+000246f0: 7365 742c 2073 656c 662e 6474 7970 6529  set, self.dtype)
+00024700: 2929 0a20 2020 2020 2020 2020 2020 206b  )).            k
+00024710: 6579 5f72 6573 6574 203d 2073 656c 662e  ey_reset = self.
+00024720: 6b65 795f 7061 7374 0a20 2020 2020 2020  key_past.       
+00024730: 2020 2020 2073 656c 662e 6173 7369 676e       self.assign
+00024740: 2873 656c 662e 7661 6c75 655f 7061 7374  (self.value_past
+00024750: 2c20 7365 6c66 2e6d 756c 2873 656c 662e  , self.mul(self.
+00024760: 7661 6c75 655f 7061 7374 2c20 462e 6361  value_past, F.ca
+00024770: 7374 2869 6e69 745f 7265 7365 742c 2073  st(init_reset, s
+00024780: 656c 662e 6474 7970 6529 2929 0a20 2020  elf.dtype))).   
+00024790: 2020 2020 2020 2020 2076 616c 7565 5f72           value_r
+000247a0: 6573 6574 203d 2073 656c 662e 7661 6c75  eset = self.valu
+000247b0: 655f 7061 7374 0a20 2020 2020 2020 2020  e_past.         
+000247c0: 2020 2023 2061 6464 2064 6570 656e 6465     # add depende
+000247d0: 6e63 7920 666f 7220 6465 7369 7265 6420  ncy for desired 
+000247e0: 6578 6563 7574 696f 6e20 6f72 6465 720a  execution order.
+000247f0: 2020 2020 2020 2020 2020 2020 696e 7075              inpu
+00024800: 745f 7820 3d20 462e 6465 7065 6e64 2869  t_x = F.depend(i
+00024810: 6e70 7574 5f78 2c20 6b65 795f 7265 7365  nput_x, key_rese
+00024820: 7429 0a20 2020 2020 2020 2020 2020 2069  t).            i
+00024830: 6e70 7574 5f78 203d 2046 2e64 6570 656e  nput_x = F.depen
+00024840: 6428 696e 7075 745f 782c 2076 616c 7565  d(input_x, value
+00024850: 5f72 6573 6574 290a 0a20 2020 2020 2020  _reset)..       
+00024860: 2061 7474 656e 7469 6f6e 2c20 6c61 7965   attention, laye
+00024870: 725f 7072 6573 656e 7420 3d20 7365 6c66  r_present = self
+00024880: 2e61 7474 656e 7469 6f6e 2869 6e70 7574  .attention(input
+00024890: 5f78 2c20 696e 7075 745f 782c 2069 6e70  _x, input_x, inp
+000248a0: 7574 5f78 2c20 6465 636f 6465 725f 6d61  ut_x, decoder_ma
+000248b0: 736b 2c20 7365 6c66 2e6b 6579 5f70 6173  sk, self.key_pas
+000248c0: 742c 0a20 2020 2020 2020 2020 2020 2020  t,.             
+000248d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000248e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000248f0: 2020 2020 2073 656c 662e 7661 6c75 655f       self.value_
+00024900: 7061 7374 2c20 6261 7463 685f 7661 6c69  past, batch_vali
+00024910: 645f 6c65 6e67 7468 290a 2020 2020 2020  d_length).      
+00024920: 2020 2320 466f 7220 706f 7374 2d6c 6179    # For post-lay
+00024930: 6572 6e6f 726d 2074 6865 2069 6e70 7574  ernorm the input
+00024940: 7320 666f 7220 7265 7369 6475 616c 2070  s for residual p
+00024950: 6174 6820 6172 6520 6f75 7470 7574 206f  ath are output o
+00024960: 6620 7365 6c66 2d61 7474 656e 7469 6f6e  f self-attention
+00024970: 2061 6e64 206f 7574 7075 7420 6f66 206c   and output of l
+00024980: 6179 6572 6e6f 726d 0a20 2020 2020 2020  ayernorm.       
+00024990: 2069 6620 7365 6c66 2e70 6f73 745f 6c61   if self.post_la
+000249a0: 7965 726e 6f72 6d5f 7265 7369 6475 616c  yernorm_residual
+000249b0: 3a0a 2020 2020 2020 2020 2020 2020 7820  :.            x 
+000249c0: 3d20 7365 6c66 2e61 6464 2869 6e70 7574  = self.add(input
+000249d0: 5f78 2c20 6174 7465 6e74 696f 6e29 0a20  _x, attention). 
+000249e0: 2020 2020 2020 2023 2046 6f72 2070 7265         # For pre
+000249f0: 2d6c 6179 6572 6e6f 726d 2074 6865 2069  -layernorm the i
+00024a00: 6e70 7574 7320 666f 7220 7265 7369 6475  nputs for residu
+00024a10: 616c 2070 6174 6820 6172 6520 6f75 7470  al path are outp
+00024a20: 7574 206f 6620 7365 6c66 2d61 7474 656e  ut of self-atten
+00024a30: 7469 6f6e 2061 6e64 2069 6e70 7574 206f  tion and input o
+00024a40: 6620 7468 6973 206c 6179 6572 0a20 2020  f this layer.   
+00024a50: 2020 2020 2065 6c73 653a 0a20 2020 2020       else:.     
+00024a60: 2020 2020 2020 2078 203d 2073 656c 662e         x = self.
+00024a70: 6164 6428 6869 6464 656e 5f73 7461 7473  add(hidden_stats
+00024a80: 2c20 6174 7465 6e74 696f 6e29 0a0a 2020  , attention)..  
+00024a90: 2020 2020 2020 6d69 6464 6c65 5f6f 7574        middle_out
+00024aa0: 7075 7420 3d20 4e6f 6e65 0a20 2020 2020  put = None.     
+00024ab0: 2020 2069 6620 656e 636f 6465 725f 6f75     if encoder_ou
+00024ac0: 7470 7574 2069 7320 6e6f 7420 4e6f 6e65  tput is not None
+00024ad0: 3a0a 2020 2020 2020 2020 2020 2020 6d69  :.            mi
+00024ae0: 6464 6c65 5f6f 7574 7075 7420 3d20 7365  ddle_output = se
+00024af0: 6c66 2e63 726f 7373 5f61 7474 656e 7469  lf.cross_attenti
+00024b00: 6f6e 5f6c 6179 6572 6e6f 726d 2878 290a  on_layernorm(x).
+00024b10: 2020 2020 2020 2020 2020 2020 6d69 6464              midd
+00024b20: 6c65 5f6f 7574 7075 7420 3d20 462e 6361  le_output = F.ca
+00024b30: 7374 286d 6964 646c 655f 6f75 7470 7574  st(middle_output
+00024b40: 2c20 7365 6c66 2e64 7479 7065 290a 2020  , self.dtype).  
+00024b50: 2020 2020 2020 2020 2020 656e 636f 6465            encode
+00024b60: 725f 6f75 7470 7574 203d 2046 2e63 6173  r_output = F.cas
+00024b70: 7428 656e 636f 6465 725f 6f75 7470 7574  t(encoder_output
+00024b80: 2c20 7365 6c66 2e64 7479 7065 290a 2020  , self.dtype).  
+00024b90: 2020 2020 2020 2020 2020 6372 6f73 735f            cross_
+00024ba0: 6174 746e 5f6f 7574 7075 742c 2063 726f  attn_output, cro
+00024bb0: 7373 5f6c 6179 6572 5f70 7265 7365 6e74  ss_layer_present
+00024bc0: 203d 2073 656c 662e 6372 6f73 735f 6174   = self.cross_at
+00024bd0: 7465 6e74 696f 6e28 6d69 6464 6c65 5f6f  tention(middle_o
+00024be0: 7574 7075 742c 2065 6e63 6f64 6572 5f6f  utput, encoder_o
+00024bf0: 7574 7075 742c 0a20 2020 2020 2020 2020  utput,.         
+00024c00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00024c10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00024c20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00024c30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00024c40: 2065 6e63 6f64 6572 5f6f 7574 7075 742c   encoder_output,
+00024c50: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00024c60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00024c70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00024c80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00024c90: 2020 2020 2020 2020 2020 206d 656d 6f72             memor
+00024ca0: 795f 6d61 736b 2c20 7365 6c66 2e6b 6579  y_mask, self.key
+00024cb0: 5f70 6173 742c 0a20 2020 2020 2020 2020  _past,.         
+00024cc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00024cd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00024ce0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00024cf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00024d00: 2073 656c 662e 7661 6c75 655f 7061 7374   self.value_past
+00024d10: 2c20 6261 7463 685f 7661 6c69 645f 6c65  , batch_valid_le
+00024d20: 6e67 7468 290a 2020 2020 2020 2020 2020  ngth).          
+00024d30: 2020 6c61 7965 725f 7072 6573 656e 7420    layer_present 
+00024d40: 2b3d 2063 726f 7373 5f6c 6179 6572 5f70  += cross_layer_p
+00024d50: 7265 7365 6e74 0a20 2020 2020 2020 2020  resent.         
+00024d60: 2020 2069 6620 7365 6c66 2e70 6f73 745f     if self.post_
+00024d70: 6c61 7965 726e 6f72 6d5f 7265 7369 6475  layernorm_residu
+00024d80: 616c 3a0a 2020 2020 2020 2020 2020 2020  al:.            
+00024d90: 2020 2020 7820 3d20 7365 6c66 2e61 6464      x = self.add
+00024da0: 286d 6964 646c 655f 6f75 7470 7574 2c20  (middle_output, 
+00024db0: 6372 6f73 735f 6174 746e 5f6f 7574 7075  cross_attn_outpu
+00024dc0: 7429 0a20 2020 2020 2020 2020 2020 2065  t).            e
+00024dd0: 6c73 653a 0a20 2020 2020 2020 2020 2020  lse:.           
+00024de0: 2020 2020 2078 203d 2073 656c 662e 6164       x = self.ad
+00024df0: 6428 782c 2063 726f 7373 5f61 7474 6e5f  d(x, cross_attn_
+00024e00: 6f75 7470 7574 290a 0a20 2020 2020 2020  output)..       
+00024e10: 206f 7574 7075 745f 7820 3d20 7365 6c66   output_x = self
+00024e20: 2e6c 6179 6572 6e6f 726d 3228 7829 0a20  .layernorm2(x). 
+00024e30: 2020 2020 2020 206f 7574 7075 745f 7820         output_x 
+00024e40: 3d20 462e 6361 7374 286f 7574 7075 745f  = F.cast(output_
+00024e50: 782c 2073 656c 662e 6474 7970 6529 0a20  x, self.dtype). 
+00024e60: 2020 2020 2020 2061 7578 5f6c 6f73 7320         aux_loss 
+00024e70: 3d20 4e6f 6e65 0a20 2020 2020 2020 2069  = None.        i
+00024e80: 6620 7365 6c66 2e75 7365 5f6d 6f65 3a0a  f self.use_moe:.
+00024e90: 2020 2020 2020 2020 2020 2020 6d6c 705f              mlp_
+00024ea0: 6c6f 6769 742c 2061 7578 5f6c 6f73 7320  logit, aux_loss 
+00024eb0: 3d20 7365 6c66 2e6f 7574 7075 7428 6f75  = self.output(ou
+00024ec0: 7470 7574 5f78 290a 2020 2020 2020 2020  tput_x).        
+00024ed0: 656c 7365 3a0a 2020 2020 2020 2020 2020  else:.          
+00024ee0: 2020 6d6c 705f 6c6f 6769 7420 3d20 7365    mlp_logit = se
+00024ef0: 6c66 2e6f 7574 7075 7428 6f75 7470 7574  lf.output(output
+00024f00: 5f78 290a 0a20 2020 2020 2020 2076 616c  _x)..        val
+00024f10: 7565 5f75 7064 6174 6520 3d20 4e6f 6e65  ue_update = None
+00024f20: 0a20 2020 2020 2020 206b 6579 5f75 7064  .        key_upd
+00024f30: 6174 6520 3d20 4e6f 6e65 0a20 2020 2020  ate = None.     
+00024f40: 2020 2069 6620 7365 6c66 2e75 7365 5f70     if self.use_p
+00024f50: 6173 743a 0a20 2020 2020 2020 2020 2020  ast:.           
+00024f60: 2023 2063 7572 7265 6e74 206b 6579 2061   # current key a
+00024f70: 6e64 2076 616c 7565 0a20 2020 2020 2020  nd value.       
+00024f80: 2020 2020 206b 6579 5f70 7265 7365 6e74       key_present
+00024f90: 2c20 7661 6c75 655f 7072 6573 656e 7420  , value_present 
+00024fa0: 3d20 6c61 7965 725f 7072 6573 656e 740a  = layer_present.
+00024fb0: 2020 2020 2020 2020 2020 2020 2320 7570              # up
+00024fc0: 6461 7465 206b 6579 2061 6e64 2076 616c  date key and val
+00024fd0: 7565 2063 616c 6375 6c61 7465 6420 7468  ue calculated th
+00024fe0: 6973 2073 7465 700a 2020 2020 2020 2020  is step.        
+00024ff0: 2020 2020 7365 6c66 2e61 7373 6967 6e28      self.assign(
+00025000: 7365 6c66 2e6b 6579 5f70 6173 742c 206b  self.key_past, k
+00025010: 6579 5f70 7265 7365 6e74 290a 2020 2020  ey_present).    
+00025020: 2020 2020 2020 2020 6b65 795f 7570 6461          key_upda
+00025030: 7465 203d 2073 656c 662e 6b65 795f 7061  te = self.key_pa
+00025040: 7374 0a20 2020 2020 2020 2020 2020 2073  st.            s
+00025050: 656c 662e 6173 7369 676e 2873 656c 662e  elf.assign(self.
+00025060: 7661 6c75 655f 7061 7374 2c20 7661 6c75  value_past, valu
+00025070: 655f 7072 6573 656e 7429 0a20 2020 2020  e_present).     
+00025080: 2020 2020 2020 2076 616c 7565 5f75 7064         value_upd
+00025090: 6174 6520 3d20 7365 6c66 2e76 616c 7565  ate = self.value
+000250a0: 5f70 6173 740a 2020 2020 2020 2020 2020  _past.          
+000250b0: 2020 2320 6164 6420 6465 7065 6e64 656e    # add dependen
+000250c0: 6379 2066 6f72 2064 6573 6972 6564 2065  cy for desired e
+000250d0: 7865 6375 7469 6f6e 206f 7264 6572 0a20  xecution order. 
+000250e0: 2020 2020 2020 2020 2020 206b 6579 5f75             key_u
+000250f0: 7064 6174 6520 3d20 462e 6465 7065 6e64  pdate = F.depend
+00025100: 286b 6579 5f75 7064 6174 652c 206b 6579  (key_update, key
+00025110: 5f72 6573 6574 290a 2020 2020 2020 2020  _reset).        
+00025120: 2020 2020 7661 6c75 655f 7570 6461 7465      value_update
+00025130: 203d 2046 2e64 6570 656e 6428 7661 6c75   = F.depend(valu
+00025140: 655f 7570 6461 7465 2c20 7661 6c75 655f  e_update, value_
+00025150: 7265 7365 7429 0a0a 2020 2020 2020 2020  reset)..        
+00025160: 2320 6164 6420 6465 7065 6e64 656e 6379  # add dependency
+00025170: 2066 6f72 2064 6573 6972 6564 2065 7865   for desired exe
+00025180: 6375 7469 6f6e 206f 7264 6572 0a20 2020  cution order.   
+00025190: 2020 2020 206d 6c70 5f6c 6f67 6974 203d       mlp_logit =
+000251a0: 2046 2e64 6570 656e 6428 6d6c 705f 6c6f   F.depend(mlp_lo
+000251b0: 6769 742c 2076 616c 7565 5f75 7064 6174  git, value_updat
+000251c0: 6529 0a20 2020 2020 2020 206d 6c70 5f6c  e).        mlp_l
+000251d0: 6f67 6974 203d 2046 2e64 6570 656e 6428  ogit = F.depend(
+000251e0: 6d6c 705f 6c6f 6769 742c 206b 6579 5f75  mlp_logit, key_u
+000251f0: 7064 6174 6529 0a0a 2020 2020 2020 2020  pdate)..        
+00025200: 2320 6966 2073 6861 7065 2069 7320 3364  # if shape is 3d
+00025210: 2c20 7765 2072 6573 6861 7065 2074 6865  , we reshape the
+00025220: 2069 6e70 7574 7320 6f66 2074 6865 2061   inputs of the a
+00025230: 6464 0a20 2020 2020 2020 2069 6620 6c65  dd.        if le
+00025240: 6e28 6869 6464 656e 5f73 6861 7065 2920  n(hidden_shape) 
+00025250: 3d3d 2033 3a0a 2020 2020 2020 2020 2020  == 3:.          
+00025260: 2020 6f75 7470 7574 5f78 203d 2050 2e52    output_x = P.R
+00025270: 6573 6861 7065 2829 286f 7574 7075 745f  eshape()(output_
+00025280: 782c 2068 6964 6465 6e5f 7368 6170 6529  x, hidden_shape)
+00025290: 0a20 2020 2020 2020 2020 2020 206d 6c70  .            mlp
+000252a0: 5f6c 6f67 6974 203d 2050 2e52 6573 6861  _logit = P.Resha
+000252b0: 7065 2829 286d 6c70 5f6c 6f67 6974 2c20  pe()(mlp_logit, 
+000252c0: 6869 6464 656e 5f73 6861 7065 290a 2020  hidden_shape).  
+000252d0: 2020 2020 2020 2020 2020 7820 3d20 502e            x = P.
+000252e0: 5265 7368 6170 6528 2928 782c 2068 6964  Reshape()(x, hid
+000252f0: 6465 6e5f 7368 6170 6529 0a0a 2020 2020  den_shape)..    
+00025300: 2020 2020 2020 2020 6966 2073 656c 662e          if self.
+00025310: 706f 7374 5f6c 6179 6572 6e6f 726d 5f72  post_layernorm_r
+00025320: 6573 6964 7561 6c3a 0a20 2020 2020 2020  esidual:.       
+00025330: 2020 2020 2020 2020 206f 7574 7075 7420           output 
+00025340: 3d20 7365 6c66 2e61 6464 5f33 6428 6f75  = self.add_3d(ou
+00025350: 7470 7574 5f78 2c20 6d6c 705f 6c6f 6769  tput_x, mlp_logi
+00025360: 7429 0a20 2020 2020 2020 2020 2020 2065  t).            e
+00025370: 6c73 653a 0a20 2020 2020 2020 2020 2020  lse:.           
+00025380: 2020 2020 206f 7574 7075 7420 3d20 7365       output = se
+00025390: 6c66 2e61 6464 5f33 6428 782c 206d 6c70  lf.add_3d(x, mlp
+000253a0: 5f6c 6f67 6974 290a 2020 2020 2020 2020  _logit).        
+000253b0: 656c 7365 3a0a 2020 2020 2020 2020 2020  else:.          
+000253c0: 2020 6966 2073 656c 662e 706f 7374 5f6c    if self.post_l
+000253d0: 6179 6572 6e6f 726d 5f72 6573 6964 7561  ayernorm_residua
+000253e0: 6c3a 0a20 2020 2020 2020 2020 2020 2020  l:.             
+000253f0: 2020 206f 7574 7075 7420 3d20 7365 6c66     output = self
+00025400: 2e61 6464 286f 7574 7075 745f 782c 206d  .add(output_x, m
+00025410: 6c70 5f6c 6f67 6974 290a 2020 2020 2020  lp_logit).      
+00025420: 2020 2020 2020 656c 7365 3a0a 2020 2020        else:.    
+00025430: 2020 2020 2020 2020 2020 2020 6f75 7470              outp
+00025440: 7574 203d 2073 656c 662e 6164 6428 782c  ut = self.add(x,
+00025450: 206d 6c70 5f6c 6f67 6974 290a 2020 2020   mlp_logit).    
+00025460: 2020 2020 2020 2020 6f75 7470 7574 203d          output =
+00025470: 2046 2e72 6573 6861 7065 286f 7574 7075   F.reshape(outpu
+00025480: 742c 2068 6964 6465 6e5f 7368 6170 6529  t, hidden_shape)
+00025490: 0a0a 2020 2020 2020 2020 6966 2073 656c  ..        if sel
+000254a0: 662e 7573 655f 6d6f 653a 0a20 2020 2020  f.use_moe:.     
+000254b0: 2020 2020 2020 2072 6574 7572 6e20 6f75         return ou
+000254c0: 7470 7574 2c20 6c61 7965 725f 7072 6573  tput, layer_pres
+000254d0: 656e 742c 2061 7578 5f6c 6f73 730a 2020  ent, aux_loss.  
+000254e0: 2020 2020 2020 7265 7475 726e 206f 7574        return out
+000254f0: 7075 742c 206c 6179 6572 5f70 7265 7365  put, layer_prese
+00025500: 6e74 0a0a 2020 2020 6465 6620 5f63 6865  nt..    def _che
+00025510: 636b 5f69 6e70 7574 2873 656c 662c 2068  ck_input(self, h
+00025520: 6964 6465 6e5f 7374 6174 6573 2c20 6174  idden_states, at
+00025530: 7465 6e74 696f 6e5f 6d61 736b 2c20 656e  tention_mask, en
+00025540: 636f 6465 725f 6f75 7470 7574 2c20 6d65  coder_output, me
+00025550: 6d6f 7279 5f6d 6173 6b2c 2069 6e69 745f  mory_mask, init_
+00025560: 7265 7365 742c 2062 6174 6368 5f76 616c  reset, batch_val
+00025570: 6964 5f6c 656e 6774 6829 3a0a 2020 2020  id_length):.    
+00025580: 2020 2020 7222 2222 4368 6563 6b20 696e      r"""Check in
+00025590: 7075 7473 2222 220a 2020 2020 2020 2020  puts""".        
+000255a0: 5f63 6865 636b 5f69 6e70 7574 5f64 7479  _check_input_dty
+000255b0: 7065 2846 2e64 7479 7065 2868 6964 6465  pe(F.dtype(hidde
+000255c0: 6e5f 7374 6174 6573 292c 2022 6869 6464  n_states), "hidd
+000255d0: 656e 5f73 7461 7465 7322 2c0a 2020 2020  en_states",.    
+000255e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000255f0: 2020 2020 2020 205b 6d73 7479 7065 2e66         [mstype.f
+00025600: 6c6f 6174 3332 2c20 6d73 7479 7065 2e66  loat32, mstype.f
+00025610: 6c6f 6174 3136 2c20 6d73 7479 7065 2e62  loat16, mstype.b
+00025620: 666c 6f61 7431 365d 2c20 7365 6c66 2e63  float16], self.c
+00025630: 6c73 5f6e 616d 6529 0a20 2020 2020 2020  ls_name).       
+00025640: 2069 6620 6174 7465 6e74 696f 6e5f 6d61   if attention_ma
+00025650: 736b 2069 7320 6e6f 7420 4e6f 6e65 3a0a  sk is not None:.
+00025660: 2020 2020 2020 2020 2020 2020 5f63 6865              _che
+00025670: 636b 5f69 6e70 7574 5f64 7479 7065 2846  ck_input_dtype(F
+00025680: 2e64 7479 7065 2861 7474 656e 7469 6f6e  .dtype(attention
+00025690: 5f6d 6173 6b29 2c20 2261 7474 656e 7469  _mask), "attenti
+000256a0: 6f6e 5f6d 6173 6b22 2c0a 2020 2020 2020  on_mask",.      
+000256b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000256c0: 2020 2020 2020 2020 205b 6d73 7479 7065           [mstype
+000256d0: 2e66 6c6f 6174 3332 2c20 6d73 7479 7065  .float32, mstype
+000256e0: 2e66 6c6f 6174 3136 2c20 6d73 7479 7065  .float16, mstype
+000256f0: 2e62 666c 6f61 7431 365d 2c0a 2020 2020  .bfloat16],.    
+00025700: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00025710: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+00025720: 636c 735f 6e61 6d65 290a 2020 2020 2020  cls_name).      
+00025730: 2020 6966 2065 6e63 6f64 6572 5f6f 7574    if encoder_out
+00025740: 7075 7420 6973 206e 6f74 204e 6f6e 653a  put is not None:
+00025750: 0a20 2020 2020 2020 2020 2020 205f 6368  .            _ch
+00025760: 6563 6b5f 696e 7075 745f 6474 7970 6528  eck_input_dtype(
+00025770: 462e 6474 7970 6528 656e 636f 6465 725f  F.dtype(encoder_
+00025780: 6f75 7470 7574 292c 2022 656e 636f 6465  output), "encode
+00025790: 725f 6f75 7470 7574 222c 0a20 2020 2020  r_output",.     
+000257a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000257b0: 2020 2020 2020 2020 2020 5b6d 7374 7970            [mstyp
+000257c0: 652e 666c 6f61 7433 322c 206d 7374 7970  e.float32, mstyp
+000257d0: 652e 666c 6f61 7431 362c 206d 7374 7970  e.float16, mstyp
+000257e0: 652e 6266 6c6f 6174 3136 5d2c 2073 656c  e.bfloat16], sel
+000257f0: 662e 636c 735f 6e61 6d65 290a 2020 2020  f.cls_name).    
+00025800: 2020 2020 6966 206d 656d 6f72 795f 6d61      if memory_ma
+00025810: 736b 2069 7320 6e6f 7420 4e6f 6e65 3a0a  sk is not None:.
+00025820: 2020 2020 2020 2020 2020 2020 5f63 6865              _che
+00025830: 636b 5f69 6e70 7574 5f64 7479 7065 2846  ck_input_dtype(F
+00025840: 2e64 7479 7065 286d 656d 6f72 795f 6d61  .dtype(memory_ma
+00025850: 736b 292c 2022 6d65 6d6f 7279 5f6d 6173  sk), "memory_mas
+00025860: 6b22 2c0a 2020 2020 2020 2020 2020 2020  k",.            
+00025870: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00025880: 2020 205b 6d73 7479 7065 2e66 6c6f 6174     [mstype.float
+00025890: 3332 2c20 6d73 7479 7065 2e66 6c6f 6174  32, mstype.float
+000258a0: 3136 2c20 6d73 7479 7065 2e62 666c 6f61  16, mstype.bfloa
+000258b0: 7431 365d 2c20 7365 6c66 2e63 6c73 5f6e  t16], self.cls_n
+000258c0: 616d 6529 0a0a 2020 2020 2020 2020 696e  ame)..        in
+000258d0: 6974 5f72 6573 6574 5f69 735f 7465 6e73  it_reset_is_tens
+000258e0: 6f72 203d 2069 7369 6e73 7461 6e63 6528  or = isinstance(
+000258f0: 696e 6974 5f72 6573 6574 2c20 5465 6e73  init_reset, Tens
+00025900: 6f72 290a 2020 2020 2020 2020 696e 6974  or).        init
+00025910: 5f72 6573 6574 5f69 735f 6465 6661 756c  _reset_is_defaul
+00025920: 7420 3d20 696e 6974 5f72 6573 6574 2069  t = init_reset i
+00025930: 7320 5472 7565 0a20 2020 2020 2020 2062  s True.        b
+00025940: 6174 6368 5f76 616c 6964 5f6c 656e 6774  atch_valid_lengt
+00025950: 685f 6973 5f74 656e 736f 7220 3d20 6973  h_is_tensor = is
+00025960: 696e 7374 616e 6365 2862 6174 6368 5f76  instance(batch_v
+00025970: 616c 6964 5f6c 656e 6774 682c 2054 656e  alid_length, Ten
+00025980: 736f 7229 0a20 2020 2020 2020 2062 6174  sor).        bat
+00025990: 6368 5f69 735f 6465 6661 756c 7420 3d20  ch_is_default = 
+000259a0: 6261 7463 685f 7661 6c69 645f 6c65 6e67  batch_valid_leng
+000259b0: 7468 2069 7320 4e6f 6e65 0a20 2020 2020  th is None.     
+000259c0: 2020 205f 6368 6563 6b5f 7061 7374 5f6e     _check_past_n
+000259d0: 6f6e 655f 696e 7075 745f 6e6f 6e65 2873  one_input_none(s
+000259e0: 656c 662e 7573 655f 7061 7374 2c20 2269  elf.use_past, "i
+000259f0: 6e69 745f 7265 7365 7422 2c20 7365 6c66  nit_reset", self
+00025a00: 2e63 6c73 5f6e 616d 652c 2054 7275 652c  .cls_name, True,
+00025a10: 2069 6e69 745f 7265 7365 745f 6973 5f74   init_reset_is_t
+00025a20: 656e 736f 722c 0a20 2020 2020 2020 2020  ensor,.         
+00025a30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00025a40: 2020 2020 2020 2020 2020 2069 6e69 745f             init_
+00025a50: 7265 7365 745f 6973 5f64 6566 6175 6c74  reset_is_default
+00025a60: 290a 2020 2020 2020 2020 5f63 6865 636b  ).        _check
+00025a70: 5f70 6173 745f 6e6f 6e65 5f69 6e70 7574  _past_none_input
+00025a80: 5f6e 6f6e 6528 7365 6c66 2e75 7365 5f70  _none(self.use_p
+00025a90: 6173 742c 2022 6261 7463 685f 7661 6c69  ast, "batch_vali
+00025aa0: 645f 6c65 6e67 7468 222c 2073 656c 662e  d_length", self.
+00025ab0: 636c 735f 6e61 6d65 2c20 4e6f 6e65 2c0a  cls_name, None,.
+00025ac0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00025ad0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00025ae0: 2020 2020 6261 7463 685f 7661 6c69 645f      batch_valid_
+00025af0: 6c65 6e67 7468 5f69 735f 7465 6e73 6f72  length_is_tensor
+00025b00: 2c20 6261 7463 685f 6973 5f64 6566 6175  , batch_is_defau
+00025b10: 6c74 290a 0a20 2020 2020 2020 2069 6620  lt)..        if 
+00025b20: 7365 6c66 2e75 7365 5f70 6173 743a 0a20  self.use_past:. 
+00025b30: 2020 2020 2020 2020 2020 205f 6368 6563             _chec
+00025b40: 6b5f 696e 7075 745f 6474 7970 6528 462e  k_input_dtype(F.
+00025b50: 6474 7970 6528 696e 6974 5f72 6573 6574  dtype(init_reset
+00025b60: 292c 2022 696e 6974 5f72 6573 6574 222c  ), "init_reset",
+00025b70: 205b 6d73 7479 7065 2e62 6f6f 6c5f 5d2c   [mstype.bool_],
+00025b80: 2073 656c 662e 636c 735f 6e61 6d65 290a   self.cls_name).
+00025b90: 2020 2020 2020 2020 2020 2020 5f63 6865              _che
+00025ba0: 636b 5f69 6e70 7574 5f64 7479 7065 2846  ck_input_dtype(F
+00025bb0: 2e64 7479 7065 2862 6174 6368 5f76 616c  .dtype(batch_val
+00025bc0: 6964 5f6c 656e 6774 6829 2c20 2262 6174  id_length), "bat
+00025bd0: 6368 5f76 616c 6964 5f6c 656e 6774 6822  ch_valid_length"
+00025be0: 2c20 5b6d 7374 7970 652e 696e 7433 325d  , [mstype.int32]
+00025bf0: 2c20 7365 6c66 2e63 6c73 5f6e 616d 6529  , self.cls_name)
+00025c00: 0a20 2020 2020 2020 2072 6574 7572 6e20  .        return 
+00025c10: 5472 7565 0a0a 0a64 6566 205f 6765 745f  True...def _get_
+00025c20: 6c61 6d62 6461 5f66 756e 6328 746f 7461  lambda_func(tota
+00025c30: 6c5f 6c61 7965 723d 4e6f 6e65 293a 0a20  l_layer=None):. 
+00025c40: 2020 2072 2222 220a 2020 2020 4120 7772     r""".    A wr
+00025c50: 6170 7065 7220 6675 6e63 7469 6f6e 206f  apper function o
+00025c60: 6620 7370 6563 6966 7969 6e67 2070 6970  f specifying pip
+00025c70: 656c 696e 6520 7374 6167 6520 616e 6420  eline stage and 
+00025c80: 6772 6164 6965 6e74 2061 6767 7265 6761  gradient aggrega
+00025c90: 7469 6f6e 2066 7573 696f 6e2e 2049 6620  tion fusion. If 
+00025ca0: 7468 6520 746f 7461 6c20 6c61 7965 720a  the total layer.
+00025cb0: 2020 2020 6973 206e 6f74 204e 6f6e 652c      is not None,
+00025cc0: 2066 6f72 2065 7861 6d70 6c65 2c20 7365   for example, se
+00025cd0: 7420 696e 2074 6865 2074 7261 6e73 666f  t in the transfo
+00025ce0: 726d 6572 206d 6f64 656c 2c20 7468 6520  rmer model, the 
+00025cf0: 7069 7065 6c69 6e65 2073 7461 6765 2073  pipeline stage s
+00025d00: 6574 7469 6e67 2066 756e 6374 696f 6e20  etting function 
+00025d10: 7769 6c6c 2062 650a 2020 2020 6028 6c61  will be.    `(la
+00025d20: 7965 725f 6964 202b 2030 2920 2f2f 2028  yer_id + 0) // (
+00025d30: 746f 7461 6c5f 6c61 7965 7273 202f 2070  total_layers / p
+00025d40: 6172 616c 6c65 6c5f 636f 6e66 6967 2e70  arallel_config.p
+00025d50: 6970 656c 696e 655f 7374 6167 6529 6020  ipeline_stage)` 
+00025d60: 666f 7220 7468 6520 656e 636f 6465 7220  for the encoder 
+00025d70: 616e 642c 0a20 2020 2060 286c 6179 6572  and,.    `(layer
+00025d80: 5f69 6420 2b20 6f66 6673 6574 2920 2f2f  _id + offset) //
+00025d90: 0a20 2020 2028 746f 7461 6c5f 6c61 7965  .    (total_laye
+00025da0: 7273 202f 2070 6172 616c 6c65 6c5f 636f  rs / parallel_co
+00025db0: 6e66 6967 2e70 6970 656c 696e 655f 7374  nfig.pipeline_st
+00025dc0: 6167 6529 6020 666f 7220 7468 6520 6465  age)` for the de
+00025dd0: 636f 6465 722c 2077 6865 7265 2060 6f66  coder, where `of
+00025de0: 6673 6574 6020 6973 2074 6865 206c 6179  fset` is the lay
+00025df0: 6572 7320 696e 2074 6865 2065 6e63 6f64  ers in the encod
+00025e00: 6572 2e0a 2020 2020 2222 220a 0a20 2020  er..    """..   
+00025e10: 2064 6566 205f 7365 745f 7061 7261 6c6c   def _set_parall
+00025e20: 656c 5f63 6f6e 6669 6775 7265 5f66 6f72  el_configure_for
+00025e30: 5f6c 6179 6572 286e 6574 776f 726b 2c20  _layer(network, 
+00025e40: 6c61 7965 725f 6964 2c20 6f66 6673 6574  layer_id, offset
+00025e50: 2c20 7061 7261 6c6c 656c 5f63 6f6e 6669  , parallel_confi
+00025e60: 672c 206c 6179 6572 7329 3a0a 2020 2020  g, layers):.    
+00025e70: 2020 2020 7222 2222 0a20 2020 2020 2020      r""".       
+00025e80: 2044 6566 6175 6c74 2073 6574 7469 6e67   Default setting
+00025e90: 2066 6f72 2074 6865 2070 6970 656c 696e   for the pipelin
+00025ea0: 6520 6973 3a20 6028 6c61 7965 725f 6964  e is: `(layer_id
+00025eb0: 202b 206f 6666 7365 7429 202f 2f20 286c   + offset) // (l
+00025ec0: 6179 6572 7320 2f20 7069 7065 6c69 6e65  ayers / pipeline
+00025ed0: 5f73 7461 6765 2960 2e0a 0a20 2020 2020  _stage)`...     
+00025ee0: 2020 2041 7267 733a 0a20 2020 2020 2020     Args:.       
+00025ef0: 2020 2020 206e 6574 776f 726b 2843 656c       network(Cel
+00025f00: 6c29 202d 2052 6570 7265 7365 6e74 7320  l) - Represents 
+00025f10: 7468 6520 7472 616e 7366 6f72 6d65 7220  the transformer 
+00025f20: 626c 6f63 6b0a 2020 2020 2020 2020 2020  block.          
+00025f30: 2020 6c61 7965 725f 6964 2869 6e74 2920    layer_id(int) 
+00025f40: 2d20 4d65 616e 7320 7468 6520 6c61 7965  - Means the laye
+00025f50: 7220 696e 6465 7820 666f 7220 7468 6520  r index for the 
+00025f60: 6375 7272 656e 7420 6d6f 6475 6c65 2c20  current module, 
+00025f70: 636f 756e 7473 2066 726f 6d20 7a65 726f  counts from zero
+00025f80: 2e0a 2020 2020 2020 2020 2020 2020 6f66  ..            of
+00025f90: 6673 6574 2869 6e74 2920 2d20 4d65 616e  fset(int) - Mean
+00025fa0: 7320 7468 6520 6c61 7965 725f 696e 6465  s the layer_inde
+00025fb0: 7820 6e65 6564 7320 616e 206f 6666 7365  x needs an offse
+00025fc0: 742c 2069 6620 7468 6572 6520 6172 6520  t, if there are 
+00025fd0: 6f74 6865 7220 6d6f 6475 6c65 7320 696e  other modules in
+00025fe0: 2074 6865 206e 6574 2e0a 2020 2020 2020   the net..      
+00025ff0: 2020 2020 2020 6c61 7965 7273 2869 6e74        layers(int
+00026000: 2920 2d20 5468 6520 746f 7461 6c20 6c61  ) - The total la
+00026010: 7965 7273 2075 7365 6420 666f 7220 7468  yers used for th
+00026020: 6520 6d6f 6465 6c2e 0a20 2020 2020 2020  e model..       
+00026030: 2022 2222 0a20 2020 2020 2020 2023 206f   """.        # o
+00026040: 7665 7272 6964 6520 7468 6520 6c61 7965  verride the laye
+00026050: 7273 0a20 2020 2020 2020 2069 6620 746f  rs.        if to
+00026060: 7461 6c5f 6c61 7965 723a 0a20 2020 2020  tal_layer:.     
+00026070: 2020 2020 2020 206c 6179 6572 7320 3d20         layers = 
+00026080: 746f 7461 6c5f 6c61 7965 720a 2020 2020  total_layer.    
+00026090: 2020 2020 2320 5573 6564 2066 6f72 2074      # Used for t
+000260a0: 6865 2070 6970 656c 696e 6527 7320 7374  he pipeline's st
+000260b0: 6167 6573 2073 6574 7469 6e67 0a20 2020  ages setting.   
+000260c0: 2020 2020 2069 6620 6c61 7965 7273 203c       if layers <
+000260d0: 2070 6172 616c 6c65 6c5f 636f 6e66 6967   parallel_config
+000260e0: 2e70 6970 656c 696e 655f 7374 6167 653a  .pipeline_stage:
+000260f0: 0a20 2020 2020 2020 2020 2020 2072 6169  .            rai
+00026100: 7365 2056 616c 7565 4572 726f 7228 6622  se ValueError(f"
+00026110: 6c61 7965 7273 207b 6c61 7965 7273 7d20  layers {layers} 
+00026120: 6d75 7374 2062 6520 6c61 7267 6572 2074  must be larger t
+00026130: 6861 6e20 7069 7065 6c69 6e65 2073 7461  han pipeline sta
+00026140: 6765 207b 7061 7261 6c6c 656c 5f63 6f6e  ge {parallel_con
+00026150: 6669 672e 7069 7065 6c69 6e65 5f73 7461  fig.pipeline_sta
+00026160: 6765 7d22 290a 0a20 2020 2020 2020 2070  ge}")..        p
+00026170: 705f 6469 7320 3d20 6d61 7828 6c61 7965  p_dis = max(laye
+00026180: 7273 202f 2f20 7061 7261 6c6c 656c 5f63  rs // parallel_c
+00026190: 6f6e 6669 672e 7069 7065 6c69 6e65 5f73  onfig.pipeline_s
+000261a0: 7461 6765 2c20 3129 0a20 2020 2020 2020  tage, 1).       
+000261b0: 2023 2074 6865 2070 6970 656c 696e 6520   # the pipeline 
+000261c0: 7374 6167 6520 6d75 7374 2062 6520 696e  stage must be in
+000261d0: 205b 302c 2070 6172 616c 6c65 6c5f 636f   [0, parallel_co
+000261e0: 6e66 6967 2e70 6970 656c 696e 655f 7374  nfig.pipeline_st
+000261f0: 6167 6520 2d20 315d 0a20 2020 2020 2020  age - 1].       
+00026200: 2070 705f 6964 203d 206d 696e 2828 6c61   pp_id = min((la
+00026210: 7965 725f 6964 202b 206f 6666 7365 7429  yer_id + offset)
+00026220: 202f 2f20 7070 5f64 6973 2c20 7061 7261   // pp_dis, para
+00026230: 6c6c 656c 5f63 6f6e 6669 672e 7069 7065  llel_config.pipe
+00026240: 6c69 6e65 5f73 7461 6765 202d 2031 290a  line_stage - 1).
+00026250: 2020 2020 2020 2020 6e65 7477 6f72 6b2e          network.
+00026260: 7069 7065 6c69 6e65 5f73 7461 6765 203d  pipeline_stage =
+00026270: 2070 705f 6964 0a0a 2020 2020 2020 2020   pp_id..        
+00026280: 2320 5573 6564 2066 6f72 206f 7074 696d  # Used for optim
+00026290: 697a 6572 2773 2066 7573 696f 6e20 7461  izer's fusion ta
+000262a0: 670a 2020 2020 2020 2020 6469 7320 3d20  g.        dis = 
+000262b0: 6d61 7828 6c61 7965 7273 202f 2f20 7061  max(layers // pa
+000262c0: 7261 6c6c 656c 5f63 6f6e 6669 672e 6772  rallel_config.gr
+000262d0: 6164 6965 6e74 5f61 6767 7265 6761 7469  adient_aggregati
+000262e0: 6f6e 5f67 726f 7570 2c20 3129 0a20 2020  on_group, 1).   
+000262f0: 2020 2020 206e 6574 776f 726b 2e73 6574       network.set
+00026300: 5f63 6f6d 6d5f 6675 7369 6f6e 2828 6c61  _comm_fusion((la
+00026310: 7965 725f 6964 202b 206f 6666 7365 7429  yer_id + offset)
+00026320: 202f 2f20 6469 7320 2b20 3129 0a20 2020   // dis + 1).   
+00026330: 2020 2020 2023 2055 7365 6420 666f 7220       # Used for 
+00026340: 656e 6162 6c69 6e67 2072 6563 6f6d 7075  enabling recompu
+00026350: 7461 7469 6f6e 206f 6620 7468 6520 626c  tation of the bl
+00026360: 6f63 6b0a 2020 2020 2020 2020 6966 2069  ock.        if i
+00026370: 7369 6e73 7461 6e63 6528 7061 7261 6c6c  sinstance(parall
+00026380: 656c 5f63 6f6e 6669 672e 7265 636f 6d70  el_config.recomp
+00026390: 7574 652c 2062 6f6f 6c29 3a0a 2020 2020  ute, bool):.    
+000263a0: 2020 2020 2020 2020 6966 2070 6172 616c          if paral
+000263b0: 6c65 6c5f 636f 6e66 6967 2e72 6563 6f6d  lel_config.recom
+000263c0: 7075 7465 2061 6e64 206e 6f74 2070 6172  pute and not par
+000263d0: 616c 6c65 6c5f 636f 6e66 6967 2e73 656c  allel_config.sel
+000263e0: 6563 745f 7265 636f 6d70 7574 653a 0a20  ect_recompute:. 
+000263f0: 2020 2020 2020 2020 2020 2020 2020 206e                 n
+00026400: 6574 776f 726b 2e72 6563 6f6d 7075 7465  etwork.recompute
+00026410: 2829 0a20 2020 2020 2020 2065 6c73 653a  ().        else:
+00026420: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if 
+00026430: 7061 7261 6c6c 656c 5f63 6f6e 6669 672e  parallel_config.
+00026440: 7265 636f 6d70 7574 652e 7265 636f 6d70  recompute.recomp
+00026450: 7574 6520 616e 6420 6e6f 7420 7061 7261  ute and not para
+00026460: 6c6c 656c 5f63 6f6e 6669 672e 7265 636f  llel_config.reco
+00026470: 6d70 7574 652e 7365 6c65 6374 5f72 6563  mpute.select_rec
+00026480: 6f6d 7075 7465 3a0a 2020 2020 2020 2020  ompute:.        
+00026490: 2020 2020 2020 2020 7061 7261 6c65 6c5f          paralel_
+000264a0: 6f70 5f63 6f6d 6d5f 636f 6d70 7574 6520  op_comm_compute 
+000264b0: 3d20 7061 7261 6c6c 656c 5f63 6f6e 6669  = parallel_confi
+000264c0: 672e 7265 636f 6d70 7574 652e 7061 7261  g.recompute.para
+000264d0: 6c6c 656c 5f6f 7074 696d 697a 6572 5f63  llel_optimizer_c
+000264e0: 6f6d 6d5f 7265 636f 6d70 7574 650a 2020  omm_recompute.  
+000264f0: 2020 2020 2020 2020 2020 2020 2020 6e65                ne
+00026500: 7477 6f72 6b2e 7265 636f 6d70 7574 6528  twork.recompute(
+00026510: 7061 7261 6c6c 656c 5f6f 7074 696d 697a  parallel_optimiz
+00026520: 6572 5f63 6f6d 6d5f 7265 636f 6d70 7574  er_comm_recomput
+00026530: 653d 7061 7261 6c65 6c5f 6f70 5f63 6f6d  e=paralel_op_com
+00026540: 6d5f 636f 6d70 7574 652c 0a20 2020 2020  m_compute,.     
+00026550: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00026560: 2020 2020 2020 2020 2020 2020 206d 705f               mp_
+00026570: 636f 6d6d 5f72 6563 6f6d 7075 7465 3d70  comm_recompute=p
+00026580: 6172 616c 6c65 6c5f 636f 6e66 6967 2e72  arallel_config.r
+00026590: 6563 6f6d 7075 7465 2e6d 705f 636f 6d6d  ecompute.mp_comm
+000265a0: 5f72 6563 6f6d 7075 7465 2c0a 2020 2020  _recompute,.    
+000265b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000265c0: 2020 2020 2020 2020 2020 2020 2020 7265                re
+000265d0: 636f 6d70 7574 655f 736c 6963 655f 6163  compute_slice_ac
+000265e0: 7469 7661 7469 6f6e 3d70 6172 616c 6c65  tivation=paralle
+000265f0: 6c5f 636f 6e66 6967 2e72 6563 6f6d 7075  l_config.recompu
+00026600: 7465 2e72 6563 6f6d 7075 7465 5f73 6c69  te.recompute_sli
+00026610: 6365 5f61 6374 6976 6174 696f 6e29 0a0a  ce_activation)..
+00026620: 2020 2020 7265 7475 726e 205f 7365 745f      return _set_
+00026630: 7061 7261 6c6c 656c 5f63 6f6e 6669 6775  parallel_configu
+00026640: 7265 5f66 6f72 5f6c 6179 6572 0a0a 0a63  re_for_layer...c
+00026650: 6c61 7373 2054 7261 6e73 666f 726d 6572  lass Transformer
+00026660: 456e 636f 6465 7228 4365 6c6c 293a 0a20  Encoder(Cell):. 
+00026670: 2020 2072 2222 220a 2020 2020 2020 2020     r""".        
+00026680: 5472 616e 7366 6f72 6d65 7220 456e 636f  Transformer Enco
+00026690: 6465 7220 6d6f 6475 6c65 2077 6974 6820  der module with 
+000266a0: 6d75 6c74 692d 6c61 7965 7220 7374 6163  multi-layer stac
+000266b0: 6b65 6420 6f66 2060 5472 616e 7366 6f72  ked of `Transfor
+000266c0: 6d65 7245 6e63 6f64 6572 4c61 7965 7260  merEncoderLayer`
+000266d0: 2c20 696e 636c 7564 696e 6720 6d75 6c74  , including mult
+000266e0: 6968 6561 6420 7365 6c66 0a20 2020 2020  ihead self.     
+000266f0: 2020 2061 7474 656e 7469 6f6e 2061 6e64     attention and
+00026700: 2066 6565 6466 6f72 7761 7264 206c 6179   feedforward lay
+00026710: 6572 2e0a 0a20 2020 2020 2020 2041 7267  er...        Arg
+00026720: 733a 0a20 2020 2020 2020 2020 2020 2062  s:.            b
+00026730: 6174 6368 5f73 697a 6528 696e 7429 3a20  atch_size(int): 
+00026740: 5468 6520 6261 7463 6820 7369 7a65 206f  The batch size o
+00026750: 6620 7468 6520 696e 7075 7420 7465 6e73  f the input tens
+00026760: 6f72 2077 6865 6e20 646f 2069 6e63 7265  or when do incre
+00026770: 6e6d 656e 7461 6c20 7072 6564 6963 7469  nmental predicti
+00026780: 6f6e 2e20 5368 6f75 6c64 2062 6520 6120  on. Should be a 
+00026790: 706f 7369 7469 7665 0a20 2020 2020 2020  positive.       
+000267a0: 2020 2020 2020 2020 2076 616c 7565 2e20           value. 
+000267b0: 5768 656e 2064 6f20 7472 6169 6e69 6e67  When do training
+000267c0: 206f 7220 7072 6564 6963 7469 6f6e 2c20   or prediction, 
+000267d0: 7468 6520 6172 6775 6d65 6e74 2077 696c  the argument wil
+000267e0: 6c20 6e6f 7420 776f 726b 2061 6e64 2074  l not work and t
+000267f0: 6865 2075 7365 7220 6361 6e20 6a75 7374  he user can just
+00026800: 2070 6173 7320 4e6f 6e65 2074 6f0a 2020   pass None to.  
+00026810: 2020 2020 2020 2020 2020 2020 2020 7468                th
+00026820: 6520 6172 6775 6d65 6e74 2e0a 2020 2020  e argument..    
+00026830: 2020 2020 2020 2020 6e75 6d5f 6c61 7965          num_laye
+00026840: 7273 2869 6e74 293a 2054 6865 206c 6179  rs(int): The lay
+00026850: 6572 7320 6f66 2074 6865 2060 5472 616e  ers of the `Tran
+00026860: 7366 6f72 6d65 7245 6e63 6f64 6572 4c61  sformerEncoderLa
+00026870: 7965 7260 0a20 2020 2020 2020 2020 2020  yer`.           
+00026880: 2068 6964 6465 6e5f 7369 7a65 2869 6e74   hidden_size(int
+00026890: 293a 2054 6865 2068 6964 6465 6e20 7369  ): The hidden si
+000268a0: 7a65 206f 6620 7468 6520 696e 7075 742e  ze of the input.
+000268b0: 0a20 2020 2020 2020 2020 2020 2066 666e  .            ffn
+000268c0: 5f68 6964 6465 6e5f 7369 7a65 2869 6e74  _hidden_size(int
+000268d0: 293a 2054 6865 2068 6964 6465 6e20 7369  ): The hidden si
+000268e0: 7a65 206f 6620 626f 7474 6c65 6e65 636b  ze of bottleneck
+000268f0: 2069 6e20 7468 6520 6665 6564 666f 7277   in the feedforw
+00026900: 6172 6420 6c61 7965 722e 0a20 2020 2020  ard layer..     
+00026910: 2020 2020 2020 2073 6571 5f6c 656e 6774         seq_lengt
+00026920: 6828 696e 7429 3a20 5468 6520 7365 715f  h(int): The seq_
+00026930: 6c65 6e67 7468 206f 6620 7468 6520 696e  length of the in
+00026940: 7075 7420 7465 6e73 6f72 2e0a 2020 2020  put tensor..    
+00026950: 2020 2020 2020 2020 6e75 6d5f 6865 6164          num_head
+00026960: 7328 696e 7429 3a20 5468 6520 6e75 6d62  s(int): The numb
+00026970: 6572 206f 6620 7468 6520 6865 6164 732e  er of the heads.
+00026980: 0a20 2020 2020 2020 2020 2020 2061 7474  .            att
+00026990: 656e 7469 6f6e 5f64 726f 706f 7574 5f72  ention_dropout_r
+000269a0: 6174 6528 666c 6f61 7429 3a20 5468 6520  ate(float): The 
+000269b0: 6472 6f70 6f75 7420 7261 7465 206f 6620  dropout rate of 
+000269c0: 7468 6520 6174 7465 6e74 696f 6e20 7363  the attention sc
+000269d0: 6f72 6573 2e20 4465 6661 756c 743a 302e  ores. Default:0.
+000269e0: 312e 0a20 2020 2020 2020 2020 2020 2068  1..            h
+000269f0: 6964 6465 6e5f 6472 6f70 6f75 745f 7261  idden_dropout_ra
+00026a00: 7465 2866 6c6f 6174 293a 2054 6865 2064  te(float): The d
+00026a10: 726f 706f 7574 2072 6174 6520 6f66 2074  ropout rate of t
+00026a20: 6865 2066 696e 616c 206f 7574 7075 7420  he final output 
+00026a30: 6f66 2074 6865 206c 6179 6572 2e20 4465  of the layer. De
+00026a40: 6661 756c 743a 2030 2e31 2e0a 2020 2020  fault: 0.1..    
+00026a50: 2020 2020 2020 2020 6869 6464 656e 5f61          hidden_a
+00026a60: 6374 2028 7374 722c 206e 6e2e 4365 6c6c  ct (str, nn.Cell
+00026a70: 293a 2054 6865 2061 6374 6976 6174 696f  ): The activatio
+00026a80: 6e20 6f66 2074 6865 2069 6e74 6572 6e61  n of the interna
+00026a90: 6c20 6665 6564 666f 7277 6172 6420 6c61  l feedforward la
+00026aa0: 7965 722e 2053 7570 706f 7274 7320 2772  yer. Supports 'r
+00026ab0: 656c 7527 2c0a 2020 2020 2020 2020 2020  elu',.          
+00026ac0: 2020 2020 2020 2772 656c 7536 272c 2027        'relu6', '
+00026ad0: 7461 6e68 272c 2027 6765 6c75 272c 2027  tanh', 'gelu', '
+00026ae0: 6661 7374 5f67 656c 7527 2c20 2765 6c75  fast_gelu', 'elu
+00026af0: 272c 2027 7369 676d 6f69 6427 2c20 2770  ', 'sigmoid', 'p
+00026b00: 7265 6c75 272c 2027 6c65 616b 7972 656c  relu', 'leakyrel
+00026b10: 7527 2c20 2768 7377 6973 6827 2c0a 2020  u', 'hswish',.  
+00026b20: 2020 2020 2020 2020 2020 2020 2020 2768                'h
+00026b30: 7369 676d 6f69 6427 2c20 276c 6f67 7369  sigmoid', 'logsi
+00026b40: 676d 6f69 6427 2061 6e64 2073 6f20 6f6e  gmoid' and so on
+00026b50: 2e20 5573 6572 2063 616e 2070 726f 7669  . User can provi
+00026b60: 6465 2063 7573 746f 6d20 6163 7469 7669  de custom activi
+00026b70: 7469 6f6e 2074 6f20 7468 6520 6172 6775  tion to the argu
+00026b80: 6d65 6e74 2e0a 2020 2020 2020 2020 2020  ment..          
+00026b90: 2020 2020 2020 4966 2075 7365 7220 7761        If user wa
+00026ba0: 6e74 7320 746f 2072 756e 2074 6865 206e  nts to run the n
+00026bb0: 6574 2069 6e20 7468 6520 7061 7261 6c6c  et in the parall
+00026bc0: 656c 206d 6f64 652c 2074 6865 2063 7573  el mode, the cus
+00026bd0: 746f 6d20 6163 7469 7661 7469 6f6e 206d  tom activation m
+00026be0: 7573 7420 616c 736f 2070 726f 7669 6465  ust also provide
+00026bf0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00026c00: 2074 6865 2060 6163 7469 7661 7469 6f6e   the `activation
+00026c10: 5f73 6861 7264 6020 6675 6e63 7469 6f6e  _shard` function
+00026c20: 2e20 506c 6561 7365 2073 6565 2074 6865  . Please see the
+00026c30: 2065 7861 6d70 6c65 7320 6f66 2074 6865   examples of the
+00026c40: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00026c50: 2063 6c61 7373 3a60 6d69 6e64 666f 726d   class:`mindform
+00026c60: 6572 732e 6d6f 6475 6c65 732e 7472 616e  ers.modules.tran
+00026c70: 7366 6f72 6d65 722e 4665 6564 466f 7277  sformer.FeedForw
+00026c80: 6172 6460 2e20 4465 6661 756c 743a 2067  ard`. Default: g
+00026c90: 656c 752e 0a20 2020 2020 2020 2020 2020  elu..           
+00026ca0: 2070 6f73 745f 6c61 7965 726e 6f72 6d5f   post_layernorm_
+00026cb0: 7265 7369 6475 616c 2862 6f6f 6c29 3a20  residual(bool): 
+00026cc0: 446f 2072 6573 6964 7561 6c73 2061 6464  Do residuals add
+00026cd0: 7320 6265 666f 7265 2074 6865 206c 6179  s before the lay
+00026ce0: 6572 6e6f 726d 2e20 4465 6661 756c 7420  ernorm. Default 
+00026cf0: 4661 6c73 652e 0a20 2020 2020 2020 2020  False..         
+00026d00: 2020 206c 6179 6572 6e6f 726d 5f63 6f6d     layernorm_com
+00026d10: 7075 7465 5f74 7970 6528 6474 7970 652e  pute_type(dtype.
+00026d20: 4e75 6d62 6572 293a 2054 6865 2063 6f6d  Number): The com
+00026d30: 7075 7461 7469 6f6e 2074 7970 6520 6f66  putation type of
+00026d40: 2074 6865 206c 6179 6572 6e6f 726d 2e0a   the layernorm..
+00026d50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00026d60: 5368 6f75 6c64 2062 6520 6d73 7479 7065  Should be mstype
+00026d70: 2e66 6c6f 6174 3332 206f 7220 6d73 7479  .float32 or msty
+00026d80: 7065 2e66 6c6f 6174 3136 2e20 4465 6661  pe.float16. Defa
+00026d90: 756c 7420 6d73 7479 7065 2e66 6c6f 6174  ult mstype.float
+00026da0: 3332 2e0a 2020 2020 2020 2020 2020 2020  32..            
+00026db0: 736f 6674 6d61 785f 636f 6d70 7574 655f  softmax_compute_
+00026dc0: 7479 7065 2864 7479 7065 2e4e 756d 6265  type(dtype.Numbe
+00026dd0: 7229 3a20 5468 6520 636f 6d70 7574 6174  r): The computat
+00026de0: 696f 6e20 7479 7065 206f 6620 7468 6520  ion type of the 
+00026df0: 736f 6674 6d61 7820 696e 2074 6865 2061  softmax in the a
+00026e00: 7474 656e 7469 6f6e 2e0a 2020 2020 2020  ttention..      
+00026e10: 2020 2020 2020 2020 2020 5368 6f75 6c64            Should
+00026e20: 2062 6520 6d73 7479 7065 2e66 6c6f 6174   be mstype.float
+00026e30: 3332 206f 7220 6d73 7479 7065 2e66 6c6f  32 or mstype.flo
+00026e40: 6174 3136 2e20 4465 6661 756c 743a 206d  at16. Default: m
+00026e50: 7374 7970 652e 666c 6f61 7433 322e 0a20  stype.float32.. 
+00026e60: 2020 2020 2020 2020 2020 2070 6172 616d             param
+00026e70: 5f69 6e69 745f 7479 7065 2864 7479 7065  _init_type(dtype
+00026e80: 2e4e 756d 6265 7229 3a20 5468 6520 7061  .Number): The pa
+00026e90: 7261 6d65 7465 7220 696e 6974 6961 6c69  rameter initiali
+00026ea0: 7a61 7469 6f6e 2074 7970 6520 6f66 2074  zation type of t
+00026eb0: 6865 206d 6f64 756c 652e 0a20 2020 2020  he module..     
+00026ec0: 2020 2020 2020 2020 2020 2053 686f 756c             Shoul
+00026ed0: 6420 6265 206d 7374 7970 652e 666c 6f61  d be mstype.floa
+00026ee0: 7433 3220 6f72 206d 7374 7970 652e 666c  t32 or mstype.fl
+00026ef0: 6f61 7431 362e 2044 6566 6175 6c74 3a20  oat16. Default: 
+00026f00: 6d73 7479 7065 2e66 6c6f 6174 3332 2e0a  mstype.float32..
+00026f10: 2020 2020 2020 2020 2020 2020 6c61 6d62              lamb
+00026f20: 6461 5f66 756e 6328 6675 6e63 7469 6f6e  da_func(function
+00026f30: 293a 2041 2066 756e 6374 696f 6e20 6361  ): A function ca
+00026f40: 6e20 6465 7465 726d 696e 6520 7468 6520  n determine the 
+00026f50: 6675 7369 6f6e 2069 6e64 6578 2c0a 2020  fusion index,.  
+00026f60: 2020 2020 2020 2020 2020 2020 2020 7069                pi
+00026f70: 7065 6c69 6e65 2073 7461 6765 7320 616e  peline stages an
+00026f80: 6420 7265 636f 6d70 7574 6520 6174 7472  d recompute attr
+00026f90: 6962 7574 652e 2049 6620 7468 650a 2020  ibute. If the.  
+00026fa0: 2020 2020 2020 2020 2020 2020 2020 7573                us
+00026fb0: 6572 2077 616e 7473 2074 6f20 6465 7465  er wants to dete
+00026fc0: 726d 696e 6520 7468 6520 7069 7065 6c69  rmine the pipeli
+00026fd0: 6e65 2073 7461 6765 2061 6e64 2067 7261  ne stage and gra
+00026fe0: 6469 656e 7420 6167 6772 6567 6174 696f  dient aggregatio
+00026ff0: 6e20 6675 7369 6f6e 2c20 7468 6520 7573  n fusion, the us
+00027000: 6572 2063 616e 2070 6173 7320 610a 2020  er can pass a.  
+00027010: 2020 2020 2020 2020 2020 2020 2020 6675                fu
+00027020: 6e63 7469 6f6e 2074 6861 7420 6163 6365  nction that acce
+00027030: 7074 7320 606e 6574 776f 726b 602c 2060  pts `network`, `
+00027040: 6c61 7965 725f 6964 602c 2060 6f66 6673  layer_id`, `offs
+00027050: 6574 602c 2060 7061 7261 6c6c 656c 5f63  et`, `parallel_c
+00027060: 6f6e 6669 6760 2c20 606c 6179 6572 7360  onfig`, `layers`
+00027070: 2e20 5468 6520 606e 6574 776f 726b 2843  . The `network(C
+00027080: 656c 6c29 600a 2020 2020 2020 2020 2020  ell)`.          
+00027090: 2020 2020 2020 7265 7072 6573 656e 7473        represents
+000270a0: 2074 6865 2074 7261 6e73 666f 726d 6572   the transformer
+000270b0: 2062 6c6f 636b 2c20 606c 6179 6572 5f69   block, `layer_i
+000270c0: 6428 696e 7429 6020 6d65 616e 7320 7468  d(int)` means th
+000270d0: 6520 6c61 7965 7220 696e 6465 7820 666f  e layer index fo
+000270e0: 7220 7468 6520 6375 7272 656e 7420 6d6f  r the current mo
+000270f0: 6475 6c65 2c20 636f 756e 7473 0a20 2020  dule, counts.   
+00027100: 2020 2020 2020 2020 2020 2020 2066 726f               fro
+00027110: 6d20 7a65 726f 2c20 606f 6666 7365 7428  m zero, `offset(
+00027120: 696e 7429 6020 6d65 616e 7320 7468 6520  int)` means the 
+00027130: 6c61 7965 725f 696e 6465 7820 6e65 6564  layer_index need
+00027140: 7320 616e 206f 6666 7365 742c 2069 6620  s an offset, if 
+00027150: 7468 6572 6520 6172 6520 6f74 6865 7220  there are other 
+00027160: 6d6f 6475 6c65 7320 696e 2074 6865 206e  modules in the n
+00027170: 6574 2e0a 2020 2020 2020 2020 2020 2020  et..            
+00027180: 2020 2020 5468 6520 6465 6661 756c 7420      The default 
+00027190: 7365 7474 696e 6720 666f 7220 7468 6520  setting for the 
+000271a0: 7069 7065 6c69 6e65 2069 733a 2060 286c  pipeline is: `(l
+000271b0: 6179 6572 5f69 6420 2b20 6f66 6673 6574  ayer_id + offset
+000271c0: 2920 2f2f 2028 6c61 7965 7273 202f 2070  ) // (layers / p
+000271d0: 6970 656c 696e 655f 7374 6167 6529 602e  ipeline_stage)`.
+000271e0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+000271f0: 2044 6566 6175 6c74 3a20 4e6f 6e65 2e0a   Default: None..
+00027200: 2020 2020 2020 2020 2020 2020 6f66 6673              offs
+00027210: 6574 2869 6e74 293a 2054 6865 2069 6e69  et(int): The ini
+00027220: 7469 616c 206c 6179 6572 2069 6e64 6578  tial layer index
+00027230: 2066 6f72 2074 6865 2060 656e 636f 6465   for the `encode
+00027240: 7260 2e20 5573 6564 2066 6f72 2073 6574  r`. Used for set
+00027250: 7469 6e67 2074 6865 2066 7573 696f 6e20  ting the fusion 
+00027260: 6964 2061 6e64 2073 7461 6765 2069 642c  id and stage id,
+00027270: 2074 6f20 6e6f 740a 2020 2020 2020 2020   to not.        
+00027280: 2020 2020 2020 2020 6f76 6572 6c61 7020          overlap 
+00027290: 7769 7468 2074 6865 2065 6e63 6f64 6572  with the encoder
+000272a0: 206c 6179 6572 2e20 4465 6661 756c 7420   layer. Default 
+000272b0: 302e 0a20 2020 2020 2020 2020 2020 2075  0..            u
+000272c0: 7365 5f70 6173 7428 626f 6f6c 293a 2055  se_past(bool): U
+000272d0: 7365 2074 6865 2070 6173 7420 7374 6174  se the past stat
+000272e0: 6520 746f 2063 6f6d 7075 7465 2c20 7573  e to compute, us
+000272f0: 6564 2066 6f72 2069 6e63 7265 6d65 6e74  ed for increment
+00027300: 616c 2070 7265 6469 6374 696f 6e2e 2046  al prediction. F
+00027310: 6f72 2065 7861 6d70 6c65 2c20 6966 2077  or example, if w
+00027320: 6520 6861 7665 2074 776f 0a20 2020 2020  e have two.     
+00027330: 2020 2020 2020 2020 2020 2077 6f72 6473             words
+00027340: 2061 6e64 2077 616e 7420 746f 2067 656e   and want to gen
+00027350: 6572 6174 6520 7468 6520 7465 6e20 6d6f  erate the ten mo
+00027360: 7265 2077 6f72 6473 2e20 5765 206a 7573  re words. We jus
+00027370: 7420 6e65 6564 2074 6f20 636f 6d70 7574  t need to comput
+00027380: 6520 7468 6520 7477 6f20 776f 7264 7327  e the two words'
+00027390: 2073 7461 7465 206f 6e6c 7920 6f6e 6365   state only once
+000273a0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+000273b0: 2020 616e 6420 6765 6e65 7261 7465 2074    and generate t
+000273c0: 6865 206e 6578 7420 776f 7264 206f 6e65  he next word one
+000273d0: 2062 7920 6f6e 652e 2057 6865 6e20 7573   by one. When us
+000273e0: 655f 7061 7374 2069 7320 5472 7565 2c20  e_past is True, 
+000273f0: 7468 6572 6520 6172 6520 7477 6f20 7374  there are two st
+00027400: 6570 7320 746f 2072 756e 2074 6865 2070  eps to run the p
+00027410: 7265 6469 6374 696f 6e2e 0a20 2020 2020  rediction..     
+00027420: 2020 2020 2020 2020 2020 2049 6e20 7468             In th
+00027430: 6520 6669 7273 7420 7374 6570 2c20 7365  e first step, se
+00027440: 7420 7468 6520 6973 5f66 6972 7374 5f69  t the is_first_i
+00027450: 7465 7261 7469 6f6e 2074 6f20 6265 2054  teration to be T
+00027460: 7275 6520 6279 0a20 2020 2020 2020 2020  rue by.         
+00027470: 2020 2020 2020 2060 6d6f 6465 6c2e 6164         `model.ad
+00027480: 645f 666c 6167 735f 7265 6375 7273 6976  d_flags_recursiv
+00027490: 6528 6973 5f66 6972 7374 5f69 7465 7261  e(is_first_itera
+000274a0: 7469 6f6e 3d54 7275 6529 602c 2061 6e64  tion=True)`, and
+000274b0: 2070 6173 7320 7468 6520 6675 6c6c 2069   pass the full i
+000274c0: 6e70 7574 732e 2054 6865 6e2c 2073 6574  nputs. Then, set
+000274d0: 2074 6865 0a20 2020 2020 2020 2020 2020   the.           
+000274e0: 2020 2020 2069 735f 6669 7273 745f 6974       is_first_it
+000274f0: 6572 6174 696f 6e20 746f 2062 6520 4661  eration to be Fa
+00027500: 6c73 6520 6279 2060 6d6f 6465 6c2e 6164  lse by `model.ad
+00027510: 645f 666c 6167 735f 7265 6375 7273 6976  d_flags_recursiv
+00027520: 6528 6973 5f66 6972 7374 5f69 7465 7261  e(is_first_itera
+00027530: 7469 6f6e 3d46 616c 7365 2960 2e20 4174  tion=False)`. At
+00027540: 2074 6869 7320 6d6f 6d65 6e74 2c0a 2020   this moment,.  
+00027550: 2020 2020 2020 2020 2020 2020 2020 7061                pa
+00027560: 7373 2074 6865 2073 696e 676c 6520 7374  ss the single st
+00027570: 6570 2773 2069 6e70 7574 2074 656e 736f  ep's input tenso
+00027580: 722c 2061 6e64 206c 6f6f 7020 6974 2e20  r, and loop it. 
+00027590: 4465 6661 756c 743a 2046 616c 7365 2e0a  Default: False..
+000275a0: 2020 2020 2020 2020 2020 2020 6d6f 655f              moe_
+000275b0: 636f 6e66 6967 284d 6f45 436f 6e66 6967  config(MoEConfig
+000275c0: 293a 2054 6865 2063 6f6e 6669 6775 7261  ): The configura
+000275d0: 7469 6f6e 206f 6620 4d6f 4520 284d 6978  tion of MoE (Mix
+000275e0: 7475 7265 206f 6620 4578 7065 7274 292e  ture of Expert).
+000275f0: 2044 6566 6175 6c74 2069 7320 616e 2069   Default is an i
+00027600: 6e73 7461 6e63 6520 6f66 204d 6f45 436f  nstance of MoECo
+00027610: 6e66 6967 0a20 2020 2020 2020 2020 2020  nfig.           
+00027620: 2020 2020 2077 6974 6820 6465 6661 756c       with defaul
+00027630: 7420 7661 6c75 6573 2e20 506c 6561 7365  t values. Please
+00027640: 2073 6565 2060 4d6f 4543 6f6e 6669 6760   see `MoEConfig`
+00027650: 2e0a 2020 2020 2020 2020 2020 2020 7061  ..            pa
+00027660: 7261 6c6c 656c 5f63 6f6e 6669 6728 5472  rallel_config(Tr
+00027670: 616e 7366 6f72 6d65 724f 7050 6172 616c  ansformerOpParal
+00027680: 6c65 6c43 6f6e 6669 6729 3a20 5468 6520  lelConfig): The 
+00027690: 7061 7261 6c6c 656c 2063 6f6e 6669 6775  parallel configu
+000276a0: 7265 2e20 4465 6661 756c 7420 6064 6566  re. Default `def
+000276b0: 6175 6c74 5f74 7261 6e73 666f 726d 6572  ault_transformer
+000276c0: 5f63 6f6e 6669 6760 2c0a 2020 2020 2020  _config`,.      
+000276d0: 2020 2020 2020 2020 2020 616e 2069 6e73            an ins
+000276e0: 7461 6e63 6520 6f66 2060 5472 616e 7366  tance of `Transf
+000276f0: 6f72 6d65 724f 7050 6172 616c 6c65 6c43  ormerOpParallelC
+00027700: 6f6e 6669 6760 2077 6974 6820 6465 6661  onfig` with defa
+00027710: 756c 7420 6172 6773 2e0a 0a20 2020 2020  ult args...     
+00027720: 2020 2049 6e70 7574 733a 0a20 2020 2020     Inputs:.     
+00027730: 2020 2020 2020 202d 202a 2a68 6964 6465         - **hidde
+00027740: 6e5f 7374 6174 6573 2a2a 2028 5465 6e73  n_states** (Tens
+00027750: 6f72 2920 2d20 5465 6e73 6f72 2c20 7368  or) - Tensor, sh
+00027760: 6170 6520 7368 6f75 6c64 2062 6520 5b62  ape should be [b
+00027770: 6174 6368 5f73 697a 652c 2073 6571 5f6c  atch_size, seq_l
+00027780: 656e 6774 682c 2068 6964 6465 6e5f 7369  ength, hidden_si
+00027790: 7a65 5d20 6f72 0a20 2020 2020 2020 2020  ze] or.         
+000277a0: 2020 2020 205b 6261 7463 685f 7369 7a65       [batch_size
+000277b0: 202a 2073 6571 5f6c 656e 6774 682c 2068   * seq_length, h
+000277c0: 6964 6465 6e5f 7369 7a65 5d2c 2069 6620  idden_size], if 
+000277d0: 7468 6520 7573 655f 7061 7374 2069 7320  the use_past is 
+000277e0: 4661 6c73 6520 6f72 2069 735f 6669 7273  False or is_firs
+000277f0: 745f 6974 6572 6174 696f 6e3d 5472 7565  t_iteration=True
+00027800: 2e20 4f74 6865 7277 6973 652c 0a20 2020  . Otherwise,.   
+00027810: 2020 2020 2020 2020 2020 2073 686f 756c             shoul
+00027820: 6420 6265 205b 6261 7463 685f 7369 7a65  d be [batch_size
+00027830: 2c20 312c 2068 6964 6465 6e5f 7369 7a65  , 1, hidden_size
+00027840: 5d2e 0a20 2020 2020 2020 2020 2020 202d  ]..            -
+00027850: 202a 2a61 7474 656e 7469 6f6e 5f6d 6173   **attention_mas
+00027860: 6b2a 2a20 2854 656e 736f 7229 202d 2046  k** (Tensor) - F
+00027870: 6c6f 6174 2054 656e 736f 722c 2049 6620  loat Tensor, If 
+00027880: 7468 6520 7573 655f 7061 7374 2069 7320  the use_past is 
+00027890: 4661 6c73 6520 6f72 2069 735f 6669 7273  False or is_firs
+000278a0: 745f 6974 6572 6174 696f 6e3d 5472 7565  t_iteration=True
+000278b0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+000278c0: 7468 6520 6174 7465 6e74 696f 6e20 6d61  the attention ma
+000278d0: 736b 206d 6174 7269 7820 7368 6f75 6c64  sk matrix should
+000278e0: 2062 6120 5b62 6174 6368 5f73 697a 652c   ba [batch_size,
+000278f0: 2073 6571 5f6c 656e 6774 682c 2073 6571   seq_length, seq
+00027900: 5f6c 656e 6774 685d 2c20 6f72 204e 6f6e  _length], or Non
+00027910: 652e 204e 6f6e 6520 6d65 616e 7320 7468  e. None means th
+00027920: 6572 6520 7769 6c6c 0a20 2020 2020 2020  ere will.       
+00027930: 2020 2020 2020 2062 6520 6e6f 206d 6173         be no mas
+00027940: 6b20 696e 2073 6f66 746d 6178 2063 6f6d  k in softmax com
+00027950: 7075 7461 7469 6f6e 2e20 4f74 6865 7277  putation. Otherw
+00027960: 6973 652c 2073 686f 756c 6420 6265 205b  ise, should be [
+00027970: 6261 7463 685f 7369 7a65 2c20 312c 2068  batch_size, 1, h
+00027980: 6964 6465 6e5f 7369 7a65 5d0a 2020 2020  idden_size].    
+00027990: 2020 2020 2020 2020 2d20 2a2a 696e 6974          - **init
+000279a0: 5f72 6573 6574 2a2a 2028 5465 6e73 6f72  _reset** (Tensor
+000279b0: 2920 2d20 4120 626f 6f6c 2074 656e 736f  ) - A bool tenso
+000279c0: 7220 7769 7468 2073 6861 7065 205b 315d  r with shape [1]
+000279d0: 2c20 7573 6564 2074 6f20 636c 6561 7220  , used to clear 
+000279e0: 7468 6520 7061 7374 206b 6579 2070 6172  the past key par
+000279f0: 616d 6574 6572 2061 6e64 0a20 2020 2020  ameter and.     
+00027a00: 2020 2020 2020 2020 2070 6173 7420 7661           past va
+00027a10: 6c75 6520 7061 7261 6d65 7465 7220 7573  lue parameter us
+00027a20: 6564 2069 6e20 7468 6520 696e 6372 656d  ed in the increm
+00027a30: 656e 7461 6c20 7072 6564 6963 7469 6f6e  ental prediction
+00027a40: 2e20 4f6e 6c79 2076 616c 6964 2077 6865  . Only valid whe
+00027a50: 6e20 7573 655f 7061 7374 2069 7320 5472  n use_past is Tr
+00027a60: 7565 2e20 4465 6661 756c 7420 5472 7565  ue. Default True
+00027a70: 2e0a 2020 2020 2020 2020 2020 2020 2d20  ..            - 
+00027a80: 2a2a 6261 7463 685f 7661 6c69 645f 6c65  **batch_valid_le
+00027a90: 6e67 7468 2a2a 2028 5465 6e73 6f72 2920  ngth** (Tensor) 
+00027aa0: 2d20 496e 7433 3220 7465 6e73 6f72 2077  - Int32 tensor w
+00027ab0: 6974 6820 7368 6170 6520 5b62 6174 6368  ith shape [batch
+00027ac0: 5f73 697a 655d 2074 6865 2070 6173 7420  _size] the past 
+00027ad0: 6361 6c63 756c 6174 6564 2074 6865 2069  calculated the i
+00027ae0: 6e64 6578 2e0a 2020 2020 2020 2020 2020  ndex..          
+00027af0: 2020 2020 5573 6564 2066 6f72 2069 6e63      Used for inc
+00027b00: 7265 6d65 6e74 616c 2070 7265 6469 6374  remental predict
+00027b10: 696f 6e20 7768 656e 2074 6865 2075 7365  ion when the use
+00027b20: 5f70 6173 7420 6973 2054 7275 652e 2044  _past is True. D
+00027b30: 6566 6175 6c74 204e 6f6e 652e 0a0a 2020  efault None...  
+00027b40: 2020 2020 2020 4f75 7470 7574 733a 0a20        Outputs:. 
+00027b50: 2020 2020 2020 2020 2020 2054 7570 6c65             Tuple
+00027b60: 2c20 6120 7475 706c 6520 636f 6e74 6169  , a tuple contai
+00027b70: 6e73 2860 6f75 7470 7574 602c 2060 6c61  ns(`output`, `la
+00027b80: 7965 725f 7072 6573 656e 7460 290a 0a20  yer_present`).. 
+00027b90: 2020 2020 2020 2020 2020 202d 202a 2a6f             - **o
+00027ba0: 7574 7075 742a 2a20 2854 656e 736f 7229  utput** (Tensor)
+00027bb0: 202d 2054 6865 2066 6c6f 6174 2074 656e   - The float ten
+00027bc0: 736f 7220 6f66 2074 6865 206f 7574 7075  sor of the outpu
+00027bd0: 7420 6f66 2074 6865 206c 6179 6572 2077  t of the layer w
+00027be0: 6974 680a 2020 2020 2020 2020 2020 2020  ith.            
+00027bf0: 2020 7368 6170 6520 2862 6174 6368 5f73    shape (batch_s
+00027c00: 697a 652c 2073 6571 5f6c 656e 6774 682c  ize, seq_length,
+00027c10: 2068 6964 6465 6e5f 7369 7a65 2920 6f72   hidden_size) or
+00027c20: 2028 6261 7463 685f 7369 7a65 202a 2073   (batch_size * s
+00027c30: 6571 5f6c 656e 6774 682c 2068 6964 6465  eq_length, hidde
+00027c40: 6e5f 7369 7a65 292c 2069 6620 7468 6520  n_size), if the 
+00027c50: 7573 655f 7061 7374 2069 730a 2020 2020  use_past is.    
+00027c60: 2020 2020 2020 2020 2020 4661 6c73 6520            False 
+00027c70: 6f72 2069 735f 6669 7273 745f 6974 6572  or is_first_iter
+00027c80: 6174 696f 6e3d 5472 7565 2e20 4f74 6865  ation=True. Othe
+00027c90: 7277 6973 652c 2069 7420 7769 6c6c 2062  rwise, it will b
+00027ca0: 6520 2862 6174 6368 5f73 697a 652c 2031  e (batch_size, 1
+00027cb0: 2c20 6869 6464 656e 5f73 697a 6529 2e0a  , hidden_size)..
+00027cc0: 2020 2020 2020 2020 2020 2020 2d20 2a2a              - **
+00027cd0: 6c61 7965 725f 7072 6573 656e 742a 2a20  layer_present** 
+00027ce0: 2854 7570 6c65 2920 2d20 4120 7475 706c  (Tuple) - A tupl
+00027cf0: 6520 7769 7468 2073 697a 6520 6f66 206e  e with size of n
+00027d00: 756d 5f6c 6179 6572 732c 2077 6865 7265  um_layers, where
+00027d10: 2065 6163 6820 7475 706c 6520 636f 6e74   each tuple cont
+00027d20: 6169 6e73 2074 6865 2054 656e 736f 7220  ains the Tensor 
+00027d30: 7468 650a 2020 2020 2020 2020 2020 2020  the.            
+00027d40: 2020 7072 6f6a 6563 7465 6420 6b65 7920    projected key 
+00027d50: 616e 6420 7661 6c75 6520 7665 6374 6f72  and value vector
+00027d60: 2077 6974 6820 7368 6170 6520 2828 6261   with shape ((ba
+00027d70: 7463 685f 7369 7a65 2c20 6e75 6d5f 6865  tch_size, num_he
+00027d80: 6164 732c 2073 697a 655f 7065 725f 6865  ads, size_per_he
+00027d90: 6164 2c20 7365 715f 6c65 6e67 7468 292c  ad, seq_length),
+00027da0: 0a20 2020 2020 2020 2020 2020 2020 2061  .              a
+00027db0: 6e64 2028 6261 7463 685f 7369 7a65 2c20  nd (batch_size, 
+00027dc0: 6e75 6d5f 6865 6164 732c 2073 6571 5f6c  num_heads, seq_l
+00027dd0: 656e 6774 682c 2073 697a 655f 7065 725f  ength, size_per_
+00027de0: 6865 6164 2929 2e0a 0a20 2020 2020 2020  head))...       
+00027df0: 2053 7570 706f 7274 6564 2050 6c61 7466   Supported Platf
+00027e00: 6f72 6d73 3a0a 2020 2020 2020 2020 2020  orms:.          
+00027e10: 2020 6060 4173 6365 6e64 6060 2060 6047    ``Ascend`` ``G
+00027e20: 5055 6060 0a0a 2020 2020 2020 2020 4578  PU``..        Ex
+00027e30: 616d 706c 6573 3a0a 2020 2020 2020 2020  amples:.        
+00027e40: 2020 2020 3e3e 3e20 696d 706f 7274 206e      >>> import n
+00027e50: 756d 7079 2061 7320 6e70 0a20 2020 2020  umpy as np.     
+00027e60: 2020 2020 2020 203e 3e3e 2066 726f 6d20         >>> from 
+00027e70: 6d69 6e64 7370 6f72 6520 696d 706f 7274  mindspore import
+00027e80: 2064 7479 7065 2061 7320 6d73 7479 7065   dtype as mstype
+00027e90: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
+00027ea0: 2066 726f 6d20 6d69 6e64 666f 726d 6572   from mindformer
+00027eb0: 732e 6d6f 6475 6c65 732e 7472 616e 7366  s.modules.transf
+00027ec0: 6f72 6d65 7220 696d 706f 7274 2054 7261  ormer import Tra
+00027ed0: 6e73 666f 726d 6572 456e 636f 6465 720a  nsformerEncoder.
+00027ee0: 2020 2020 2020 2020 2020 2020 3e3e 3e20              >>> 
+00027ef0: 6672 6f6d 206d 696e 6473 706f 7265 2069  from mindspore i
+00027f00: 6d70 6f72 7420 5465 6e73 6f72 0a20 2020  mport Tensor.   
+00027f10: 2020 2020 2020 2020 203e 3e3e 206d 6f64           >>> mod
+00027f20: 656c 203d 2054 7261 6e73 666f 726d 6572  el = Transformer
+00027f30: 456e 636f 6465 7228 6261 7463 685f 7369  Encoder(batch_si
+00027f40: 7a65 3d32 2c20 6e75 6d5f 6c61 7965 7273  ze=2, num_layers
+00027f50: 3d32 2c20 6869 6464 656e 5f73 697a 653d  =2, hidden_size=
+00027f60: 382c 2066 666e 5f68 6964 6465 6e5f 7369  8, ffn_hidden_si
+00027f70: 7a65 3d36 342c 0a20 2020 2020 2020 2020  ze=64,.         
+00027f80: 2020 202e 2e2e 2020 2020 2020 2020 2020     ...          
+00027f90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00027fa0: 2020 7365 715f 6c65 6e67 7468 3d31 362c    seq_length=16,
+00027fb0: 206e 756d 5f68 6561 6473 3d32 290a 2020   num_heads=2).  
+00027fc0: 2020 2020 2020 2020 2020 3e3e 3e20 656e            >>> en
+00027fd0: 636f 6465 725f 696e 7075 745f 7661 6c75  coder_input_valu
+00027fe0: 6520 3d20 5465 6e73 6f72 286e 702e 6f6e  e = Tensor(np.on
+00027ff0: 6573 2828 322c 2031 362c 2038 2929 2c20  es((2, 16, 8)), 
+00028000: 6d73 7479 7065 2e66 6c6f 6174 3332 290a  mstype.float32).
+00028010: 2020 2020 2020 2020 2020 2020 3e3e 3e20              >>> 
+00028020: 656e 636f 6465 725f 696e 7075 745f 6d61  encoder_input_ma
+00028030: 736b 203d 2054 656e 736f 7228 6e70 2e6f  sk = Tensor(np.o
+00028040: 6e65 7328 2832 2c20 3136 2c20 3136 2929  nes((2, 16, 16))
+00028050: 2c20 6d73 7479 7065 2e66 6c6f 6174 3136  , mstype.float16
+00028060: 290a 2020 2020 2020 2020 2020 2020 3e3e  ).            >>
+00028070: 3e20 6f75 7470 7574 2c20 7061 7374 203d  > output, past =
+00028080: 206d 6f64 656c 2865 6e63 6f64 6572 5f69   model(encoder_i
+00028090: 6e70 7574 5f76 616c 7565 2c20 656e 636f  nput_value, enco
+000280a0: 6465 725f 696e 7075 745f 6d61 736b 290a  der_input_mask).
+000280b0: 2020 2020 2020 2020 2020 2020 3e3e 3e20              >>> 
+000280c0: 7072 696e 7428 6f75 7470 7574 2e73 6861  print(output.sha
+000280d0: 7065 290a 2020 2020 2020 2020 2020 2020  pe).            
+000280e0: 2832 2c20 3136 2c20 3829 0a20 2020 2020  (2, 16, 8).     
+000280f0: 2020 2020 2020 203e 3e3e 2070 7269 6e74         >>> print
+00028100: 286c 656e 2870 6173 7429 290a 2020 2020  (len(past)).    
+00028110: 2020 2020 2020 2020 320a 2020 2020 2020          2.      
+00028120: 2020 2020 2020 3e3e 3e20 7072 696e 7428        >>> print(
+00028130: 7061 7374 5b30 5d5b 305d 2e73 6861 7065  past[0][0].shape
+00028140: 290a 2020 2020 2020 2020 2020 2020 2832  ).            (2
+00028150: 2c20 322c 2034 2c20 3136 290a 2020 2020  , 2, 4, 16).    
+00028160: 2020 2020 2020 2020 3e3e 3e20 7072 696e          >>> prin
+00028170: 7428 7061 7374 5b30 5d5b 315d 2e73 6861  t(past[0][1].sha
+00028180: 7065 290a 2020 2020 2020 2020 2020 2020  pe).            
+00028190: 2832 2c20 322c 2031 362c 2034 290a 2020  (2, 2, 16, 4).  
+000281a0: 2020 2020 2020 2020 2020 3e3e 3e20 2320            >>> # 
+000281b0: 5768 656e 2075 7365 2075 7365 5f70 6173  When use use_pas
+000281c0: 743d 5472 7565 2c20 6974 2069 6e63 6c75  t=True, it inclu
+000281d0: 6465 7320 7477 6f20 7374 6570 7320 746f  des two steps to
+000281e0: 2069 6d70 6c65 6d65 6e74 2074 6865 2069   implement the i
+000281f0: 6e63 7265 6d65 6e74 616c 2070 7265 6469  ncremental predi
+00028200: 6374 696f 6e2e 0a20 2020 2020 2020 2020  ction..         
+00028210: 2020 203e 3e3e 2023 2053 7465 7020 313a     >>> # Step 1:
+00028220: 2073 6574 2069 735f 6669 7273 745f 6974   set is_first_it
+00028230: 6572 6174 696f 6e3d 5472 7565 2c20 616e  eration=True, an
+00028240: 6420 696e 7075 7420 7468 6520 6675 6c6c  d input the full
+00028250: 2073 6571 7565 6e63 6520 6c65 6e67 7468   sequence length
+00028260: 2773 2073 7461 7465 2e0a 2020 2020 2020  's state..      
+00028270: 2020 2020 2020 3e3e 3e20 6261 7463 685f        >>> batch_
+00028280: 7661 6c69 645f 6c65 6e67 7468 203d 2054  valid_length = T
+00028290: 656e 736f 7228 6e70 2e6f 6e65 7328 2832  ensor(np.ones((2
+000282a0: 2c29 292c 206d 7374 7970 652e 696e 7433  ,)), mstype.int3
+000282b0: 3229 0a20 2020 2020 2020 2020 2020 203e  2).            >
+000282c0: 3e3e 2069 6e69 745f 7265 7365 7420 3d20  >> init_reset = 
+000282d0: 5465 6e73 6f72 285b 5472 7565 5d2c 206d  Tensor([True], m
+000282e0: 7374 7970 652e 626f 6f6c 5f29 0a20 2020  stype.bool_).   
+000282f0: 2020 2020 2020 2020 203e 3e3e 2023 2053           >>> # S
+00028300: 6574 2069 735f 6669 7273 745f 6974 6572  et is_first_iter
+00028310: 6174 696f 6e3d 5472 7565 2074 6f20 6765  ation=True to ge
+00028320: 6e65 7261 7465 2074 6865 2066 756c 6c20  nerate the full 
+00028330: 6d65 6d6f 7279 2073 7461 7465 730a 2020  memory states.  
+00028340: 2020 2020 2020 2020 2020 3e3e 3e20 6d6f            >>> mo
+00028350: 6465 6c20 3d20 5472 616e 7366 6f72 6d65  del = Transforme
+00028360: 7245 6e63 6f64 6572 2862 6174 6368 5f73  rEncoder(batch_s
+00028370: 697a 653d 322c 2068 6964 6465 6e5f 7369  ize=2, hidden_si
+00028380: 7a65 3d38 2c20 6666 6e5f 6869 6464 656e  ze=8, ffn_hidden
+00028390: 5f73 697a 653d 3634 2c20 7365 715f 6c65  _size=64, seq_le
+000283a0: 6e67 7468 3d31 362c 0a20 2020 2020 2020  ngth=16,.       
+000283b0: 2020 2020 202e 2e2e 2020 2020 2020 2020       ...        
+000283c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000283d0: 2020 2020 6e75 6d5f 6865 6164 733d 322c      num_heads=2,
+000283e0: 206e 756d 5f6c 6179 6572 733d 322c 2075   num_layers=2, u
+000283f0: 7365 5f70 6173 743d 5472 7565 290a 2020  se_past=True).  
+00028400: 2020 2020 2020 2020 2020 3e3e 3e20 6d6f            >>> mo
+00028410: 6465 6c2e 6164 645f 666c 6167 735f 7265  del.add_flags_re
+00028420: 6375 7273 6976 6528 6973 5f66 6972 7374  cursive(is_first
+00028430: 5f69 7465 7261 7469 6f6e 3d54 7275 6529  _iteration=True)
+00028440: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
+00028450: 2068 6964 6465 6e2c 2070 6173 7420 3d20   hidden, past = 
+00028460: 6d6f 6465 6c28 656e 636f 6465 725f 696e  model(encoder_in
+00028470: 7075 745f 7661 6c75 652c 2065 6e63 6f64  put_value, encod
+00028480: 6572 5f69 6e70 7574 5f6d 6173 6b2c 2069  er_input_mask, i
+00028490: 6e69 745f 7265 7365 742c 2062 6174 6368  nit_reset, batch
+000284a0: 5f76 616c 6964 5f6c 656e 6774 6829 0a20  _valid_length). 
+000284b0: 2020 2020 2020 2020 2020 203e 3e3e 2070             >>> p
+000284c0: 7269 6e74 2868 6964 6465 6e2e 7368 6170  rint(hidden.shap
+000284d0: 6529 0a20 2020 2020 2020 2020 2020 2028  e).            (
+000284e0: 322c 2031 362c 2038 290a 2020 2020 2020  2, 16, 8).      
+000284f0: 2020 2020 2020 3e3e 3e20 7072 696e 7428        >>> print(
+00028500: 7061 7374 5b30 5d5b 305d 2e73 6861 7065  past[0][0].shape
+00028510: 290a 2020 2020 2020 2020 2020 2020 2832  ).            (2
+00028520: 2c20 322c 2034 2c20 3136 290a 2020 2020  , 2, 4, 16).    
+00028530: 2020 2020 2020 2020 3e3e 3e20 7072 696e          >>> prin
+00028540: 7428 7061 7374 5b30 5d5b 315d 2e73 6861  t(past[0][1].sha
+00028550: 7065 290a 2020 2020 2020 2020 2020 2020  pe).            
+00028560: 2832 2c20 322c 2031 362c 2034 290a 2020  (2, 2, 16, 4).  
+00028570: 2020 2020 2020 2020 2020 3e3e 3e20 656e            >>> en
+00028580: 636f 6465 725f 696e 7075 745f 7661 6c75  coder_input_valu
+00028590: 6520 3d20 5465 6e73 6f72 286e 702e 6f6e  e = Tensor(np.on
+000285a0: 6573 2828 322c 2031 2c20 3829 292c 206d  es((2, 1, 8)), m
+000285b0: 7374 7970 652e 666c 6f61 7433 3229 0a20  stype.float32). 
+000285c0: 2020 2020 2020 2020 2020 203e 3e3e 2065             >>> e
+000285d0: 6e63 6f64 6572 5f69 6e70 7574 5f6d 6173  ncoder_input_mas
+000285e0: 6b20 3d20 5465 6e73 6f72 286e 702e 6f6e  k = Tensor(np.on
+000285f0: 6573 2828 322c 2031 2c20 3136 2929 2c20  es((2, 1, 16)), 
+00028600: 6d73 7479 7065 2e66 6c6f 6174 3136 290a  mstype.float16).
+00028610: 2020 2020 2020 2020 2020 2020 3e3e 3e20              >>> 
+00028620: 696e 6974 5f72 6573 6574 203d 2054 656e  init_reset = Ten
+00028630: 736f 7228 5b46 616c 7365 5d2c 206d 7374  sor([False], mst
+00028640: 7970 652e 626f 6f6c 5f29 0a20 2020 2020  ype.bool_).     
+00028650: 2020 2020 2020 203e 3e3e 2023 2053 7465         >>> # Ste
+00028660: 7020 323a 2073 6574 2069 735f 6669 7273  p 2: set is_firs
+00028670: 745f 6974 6572 6174 696f 6e3d 4661 6c73  t_iteration=Fals
+00028680: 652c 2061 6e64 2070 6173 7320 7468 6520  e, and pass the 
+00028690: 7369 6e67 6c65 2077 6f72 6420 746f 2072  single word to r
+000286a0: 756e 2074 6865 2070 7265 6469 6374 696f  un the predictio
+000286b0: 6e20 7261 7468 6572 2074 6861 6e0a 2020  n rather than.  
+000286c0: 2020 2020 2020 2020 2020 3e3e 3e20 2320            >>> # 
+000286d0: 7468 6520 6675 6c6c 2073 6571 7565 6e63  the full sequenc
+000286e0: 652e 0a20 2020 2020 2020 2020 2020 203e  e..            >
+000286f0: 3e3e 206d 6f64 656c 2e61 6464 5f66 6c61  >> model.add_fla
+00028700: 6773 5f72 6563 7572 7369 7665 2869 735f  gs_recursive(is_
+00028710: 6669 7273 745f 6974 6572 6174 696f 6e3d  first_iteration=
+00028720: 4661 6c73 6529 0a20 2020 2020 2020 2020  False).         
+00028730: 2020 203e 3e3e 2068 6964 6465 6e2c 2070     >>> hidden, p
+00028740: 6173 7420 3d20 6d6f 6465 6c28 656e 636f  ast = model(enco
+00028750: 6465 725f 696e 7075 745f 7661 6c75 652c  der_input_value,
+00028760: 2065 6e63 6f64 6572 5f69 6e70 7574 5f6d   encoder_input_m
+00028770: 6173 6b2c 2069 6e69 745f 7265 7365 742c  ask, init_reset,
+00028780: 2062 6174 6368 5f76 616c 6964 5f6c 656e   batch_valid_len
+00028790: 6774 6829 0a20 2020 2020 2020 2020 2020  gth).           
+000287a0: 203e 3e3e 2070 7269 6e74 2868 6964 6465   >>> print(hidde
+000287b0: 6e2e 7368 6170 6529 0a20 2020 2020 2020  n.shape).       
+000287c0: 2020 2020 2028 322c 2031 2c20 3829 0a20       (2, 1, 8). 
+000287d0: 2020 2020 2020 2020 2020 203e 3e3e 2070             >>> p
+000287e0: 7269 6e74 2870 6173 745b 305d 5b30 5d2e  rint(past[0][0].
+000287f0: 7368 6170 6529 0a20 2020 2020 2020 2020  shape).         
+00028800: 2020 2028 322c 2032 2c20 342c 2031 3629     (2, 2, 4, 16)
+00028810: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
+00028820: 2070 7269 6e74 2870 6173 745b 305d 5b31   print(past[0][1
+00028830: 5d2e 7368 6170 6529 0a20 2020 2020 2020  ].shape).       
+00028840: 2020 2020 2028 322c 2032 2c20 3136 2c20       (2, 2, 16, 
+00028850: 3429 0a20 2020 2022 2222 0a0a 2020 2020  4).    """..    
+00028860: 405f 4c6f 6741 6374 696f 6e4f 6e63 6528  @_LogActionOnce(
+00028870: 6d5f 6c6f 6767 6572 3d6c 6f67 6765 722c  m_logger=logger,
+00028880: 206b 6579 3d27 5472 616e 7366 6f72 6d65   key='Transforme
+00028890: 7245 6e63 6f64 6572 272c 0a20 2020 2020  rEncoder',.     
+000288a0: 2020 2020 2020 2020 2020 2020 2020 206e                 n
+000288b0: 6f5f 7761 726e 696e 673d 5f67 6574 5f70  o_warning=_get_p
+000288c0: 6172 616c 6c65 6c5f 6d6f 6465 2829 2069  arallel_mode() i
+000288d0: 6e20 2850 6172 616c 6c65 6c4d 6f64 652e  n (ParallelMode.
+000288e0: 5354 414e 445f 414c 4f4e 452c 2929 0a20  STAND_ALONE,)). 
+000288f0: 2020 2040 5f61 7267 735f 7479 7065 5f76     @_args_type_v
+00028900: 616c 6964 6174 6f72 5f63 6865 636b 2868  alidator_check(h
+00028910: 6964 6465 6e5f 7369 7a65 3d56 616c 6964  idden_size=Valid
+00028920: 6174 6f72 2e63 6865 636b 5f70 6f73 6974  ator.check_posit
+00028930: 6976 655f 696e 742c 0a20 2020 2020 2020  ive_int,.       
+00028940: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00028950: 2020 2020 2020 2020 206e 756d 5f68 6561           num_hea
+00028960: 6473 3d56 616c 6964 6174 6f72 2e63 6865  ds=Validator.che
+00028970: 636b 5f70 6f73 6974 6976 655f 696e 742c  ck_positive_int,
+00028980: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00028990: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000289a0: 2066 666e 5f68 6964 6465 6e5f 7369 7a65   ffn_hidden_size
+000289b0: 3d56 616c 6964 6174 6f72 2e63 6865 636b  =Validator.check
+000289c0: 5f70 6f73 6974 6976 655f 696e 742c 0a20  _positive_int,. 
 000289d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000289e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000289f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00028a00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00028a10: 2022 5472 616e 7366 6f72 6d65 7245 6e63   "TransformerEnc
-00028a20: 6f64 6572 2229 2c0a 2020 2020 2020 2020  oder"),.        
-00028a30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00028a40: 2020 2020 2020 2020 7061 7261 6c6c 656c          parallel
-00028a50: 5f63 6f6e 6669 673d 5f76 616c 6964 5f74  _config=_valid_t
-00028a60: 7970 655f 6368 6563 6b73 285b 5472 616e  ype_checks([Tran
-00028a70: 7366 6f72 6d65 724f 7050 6172 616c 6c65  sformerOpParalle
-00028a80: 6c43 6f6e 6669 675d 2c0a 2020 2020 2020  lConfig],.      
-00028a90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00028aa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000289e0: 2020 2020 2020 2020 2020 2020 2020 2073                 s
+000289f0: 6571 5f6c 656e 6774 683d 5661 6c69 6461  eq_length=Valida
+00028a00: 746f 722e 6368 6563 6b5f 706f 7369 7469  tor.check_positi
+00028a10: 7665 5f69 6e74 2c0a 2020 2020 2020 2020  ve_int,.        
+00028a20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00028a30: 2020 2020 2020 2020 6e75 6d5f 6c61 7965          num_laye
+00028a40: 7273 3d56 616c 6964 6174 6f72 2e63 6865  rs=Validator.che
+00028a50: 636b 5f70 6f73 6974 6976 655f 696e 742c  ck_positive_int,
+00028a60: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00028a70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00028a80: 206f 6666 7365 743d 5661 6c69 6461 746f   offset=Validato
+00028a90: 722e 6368 6563 6b5f 6e6f 6e5f 6e65 6761  r.check_non_nega
+00028aa0: 7469 7665 5f69 6e74 2c0a 2020 2020 2020  tive_int,.      
 00028ab0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00028ac0: 2020 2020 2020 2020 2020 2020 2022 5472               "Tr
-00028ad0: 616e 7366 6f72 6d65 7245 6e63 6f64 6572  ansformerEncoder
-00028ae0: 2229 2c0a 2020 2020 2020 2020 2020 2020  "),.            
-00028af0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00028b00: 2020 2020 7573 655f 7061 7374 3d56 616c      use_past=Val
-00028b10: 6964 6174 6f72 2e63 6865 636b 5f62 6f6f  idator.check_boo
-00028b20: 6c29 0a20 2020 2064 6566 205f 5f69 6e69  l).    def __ini
-00028b30: 745f 5f28 7365 6c66 2c0a 2020 2020 2020  t__(self,.      
-00028b40: 2020 2020 2020 2020 2020 2062 6174 6368             batch
-00028b50: 5f73 697a 652c 0a20 2020 2020 2020 2020  _size,.         
-00028b60: 2020 2020 2020 2020 6e75 6d5f 6c61 7965          num_laye
-00028b70: 7273 2c0a 2020 2020 2020 2020 2020 2020  rs,.            
-00028b80: 2020 2020 2068 6964 6465 6e5f 7369 7a65       hidden_size
-00028b90: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00028ba0: 2020 2066 666e 5f68 6964 6465 6e5f 7369     ffn_hidden_si
-00028bb0: 7a65 2c0a 2020 2020 2020 2020 2020 2020  ze,.            
-00028bc0: 2020 2020 2073 6571 5f6c 656e 6774 682c       seq_length,
-00028bd0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00028be0: 2020 6e75 6d5f 6865 6164 732c 0a20 2020    num_heads,.   
-00028bf0: 2020 2020 2020 2020 2020 2020 2020 6174                at
-00028c00: 7465 6e74 696f 6e5f 6472 6f70 6f75 745f  tention_dropout_
-00028c10: 7261 7465 3d30 2e31 2c0a 2020 2020 2020  rate=0.1,.      
-00028c20: 2020 2020 2020 2020 2020 2068 6964 6465             hidde
-00028c30: 6e5f 6472 6f70 6f75 745f 7261 7465 3d30  n_dropout_rate=0
-00028c40: 2e31 2c0a 2020 2020 2020 2020 2020 2020  .1,.            
-00028c50: 2020 2020 2068 6964 6465 6e5f 6163 743d       hidden_act=
-00028c60: 2767 656c 7527 2c0a 2020 2020 2020 2020  'gelu',.        
-00028c70: 2020 2020 2020 2020 2070 6f73 745f 6c61           post_la
-00028c80: 7965 726e 6f72 6d5f 7265 7369 6475 616c  yernorm_residual
-00028c90: 3d46 616c 7365 2c0a 2020 2020 2020 2020  =False,.        
-00028ca0: 2020 2020 2020 2020 206c 6179 6572 6e6f           layerno
-00028cb0: 726d 5f63 6f6d 7075 7465 5f74 7970 653d  rm_compute_type=
-00028cc0: 6d73 7479 7065 2e66 6c6f 6174 3332 2c0a  mstype.float32,.
-00028cd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00028ce0: 2073 6f66 746d 6178 5f63 6f6d 7075 7465   softmax_compute
-00028cf0: 5f74 7970 653d 6d73 7479 7065 2e66 6c6f  _type=mstype.flo
-00028d00: 6174 3332 2c0a 2020 2020 2020 2020 2020  at32,.          
-00028d10: 2020 2020 2020 2070 6172 616d 5f69 6e69         param_ini
-00028d20: 745f 7479 7065 3d6d 7374 7970 652e 666c  t_type=mstype.fl
-00028d30: 6f61 7433 322c 0a20 2020 2020 2020 2020  oat32,.         
-00028d40: 2020 2020 2020 2020 6c61 6d62 6461 5f66          lambda_f
-00028d50: 756e 633d 4e6f 6e65 2c0a 2020 2020 2020  unc=None,.      
-00028d60: 2020 2020 2020 2020 2020 206f 6666 7365             offse
-00028d70: 743d 302c 0a20 2020 2020 2020 2020 2020  t=0,.           
-00028d80: 2020 2020 2020 7573 655f 7061 7374 3d46        use_past=F
-00028d90: 616c 7365 2c0a 2020 2020 2020 2020 2020  alse,.          
-00028da0: 2020 2020 2020 206d 6f65 5f63 6f6e 6669         moe_confi
-00028db0: 673d 6465 6661 756c 745f 6d6f 655f 636f  g=default_moe_co
-00028dc0: 6e66 6967 2c0a 2020 2020 2020 2020 2020  nfig,.          
-00028dd0: 2020 2020 2020 2070 6172 616c 6c65 6c5f         parallel_
-00028de0: 636f 6e66 6967 3d64 6566 6175 6c74 5f74  config=default_t
-00028df0: 7261 6e73 666f 726d 6572 5f63 6f6e 6669  ransformer_confi
-00028e00: 6729 3a0a 2020 2020 2020 2020 7375 7065  g):.        supe
-00028e10: 7228 5472 616e 7366 6f72 6d65 7245 6e63  r(TransformerEnc
-00028e20: 6f64 6572 2c20 7365 6c66 292e 5f5f 696e  oder, self).__in
-00028e30: 6974 5f5f 2829 0a20 2020 2020 2020 205f  it__().        _
-00028e40: 6368 6563 6b5f 636f 6e66 6967 2870 6172  check_config(par
-00028e50: 616c 6c65 6c5f 636f 6e66 6967 290a 2020  allel_config).  
-00028e60: 2020 2020 2020 5f63 6865 636b 5f6d 6f65        _check_moe
-00028e70: 5f63 6f6e 6669 6728 6d6f 655f 636f 6e66  _config(moe_conf
-00028e80: 6967 2c20 7061 7261 6c6c 656c 5f63 6f6e  ig, parallel_con
-00028e90: 6669 6729 0a20 2020 2020 2020 2073 656c  fig).        sel
-00028ea0: 662e 7573 655f 6d6f 6520 3d20 286d 6f65  f.use_moe = (moe
-00028eb0: 5f63 6f6e 6669 672e 6578 7065 7274 5f6e  _config.expert_n
-00028ec0: 756d 203e 2031 290a 2020 2020 2020 2020  um > 1).        
-00028ed0: 6966 2062 6174 6368 5f73 697a 6520 6f72  if batch_size or
-00028ee0: 2075 7365 5f70 6173 743a 0a20 2020 2020   use_past:.     
-00028ef0: 2020 2020 2020 2056 616c 6964 6174 6f72         Validator
-00028f00: 2e63 6865 636b 5f70 6f73 6974 6976 655f  .check_positive_
-00028f10: 696e 7428 6261 7463 685f 7369 7a65 290a  int(batch_size).
-00028f20: 2020 2020 2020 2020 636f 6e66 6967 5f74          config_t
-00028f30: 6f5f 6c61 7965 7220 3d20 7061 7261 6c6c  o_layer = parall
-00028f40: 656c 5f63 6f6e 6669 672e 6d6f 655f 7061  el_config.moe_pa
-00028f50: 7261 6c6c 656c 5f63 6f6e 6669 6720 6966  rallel_config if
-00028f60: 2073 656c 662e 7573 655f 6d6f 6520 656c   self.use_moe el
-00028f70: 7365 2070 6172 616c 6c65 6c5f 636f 6e66  se parallel_conf
-00028f80: 6967 2e64 705f 6d70 5f63 6f6e 6669 670a  ig.dp_mp_config.
-00028f90: 2020 2020 2020 2020 6966 205f 6765 745f          if _get_
-00028fa0: 7061 7261 6c6c 656c 5f6d 6f64 6528 2920  parallel_mode() 
-00028fb0: 696e 2028 5061 7261 6c6c 656c 4d6f 6465  in (ParallelMode
-00028fc0: 2e41 5554 4f5f 5041 5241 4c4c 454c 2c29  .AUTO_PARALLEL,)
-00028fd0: 3a0a 2020 2020 2020 2020 2020 2020 7365  :.            se
-00028fe0: 6c66 2e61 6464 203d 2050 2e41 6464 2829  lf.add = P.Add()
-00028ff0: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-00029000: 662e 6175 785f 6c6f 7373 203d 2054 656e  f.aux_loss = Ten
-00029010: 736f 7228 302e 302c 206d 7374 7970 652e  sor(0.0, mstype.
-00029020: 666c 6f61 7433 3229 0a20 2020 2020 2020  float32).       
-00029030: 2020 2020 2073 656c 662e 6e75 6d5f 6c61       self.num_la
-00029040: 7965 7273 203d 206e 756d 5f6c 6179 6572  yers = num_layer
-00029050: 730a 2020 2020 2020 2020 2020 2020 7365  s.            se
-00029060: 6c66 2e62 6c6f 636b 7320 3d20 6e6e 2e43  lf.blocks = nn.C
-00029070: 656c 6c4c 6973 7428 290a 2020 2020 2020  ellList().      
-00029080: 2020 2020 2020 666f 7220 6920 696e 2072        for i in r
-00029090: 616e 6765 286e 756d 5f6c 6179 6572 7329  ange(num_layers)
-000290a0: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
-000290b0: 2020 626c 6f63 6b20 3d20 5472 616e 7366    block = Transf
-000290c0: 6f72 6d65 7245 6e63 6f64 6572 4c61 7965  ormerEncoderLaye
-000290d0: 7228 6869 6464 656e 5f73 697a 653d 6869  r(hidden_size=hi
-000290e0: 6464 656e 5f73 697a 652c 0a20 2020 2020  dden_size,.     
-000290f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029100: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029110: 2020 2020 2020 2020 2020 2062 6174 6368             batch
-00029120: 5f73 697a 653d 6261 7463 685f 7369 7a65  _size=batch_size
-00029130: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00028ac0: 2020 2020 2020 2020 2020 6174 7465 6e74            attent
+00028ad0: 696f 6e5f 6472 6f70 6f75 745f 7261 7465  ion_dropout_rate
+00028ae0: 3d56 616c 6964 6174 6f72 2e63 6865 636b  =Validator.check
+00028af0: 5f6e 6f6e 5f6e 6567 6174 6976 655f 666c  _non_negative_fl
+00028b00: 6f61 742c 0a20 2020 2020 2020 2020 2020  oat,.           
+00028b10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00028b20: 2020 2020 2068 6964 6465 6e5f 6472 6f70       hidden_drop
+00028b30: 6f75 745f 7261 7465 3d56 616c 6964 6174  out_rate=Validat
+00028b40: 6f72 2e63 6865 636b 5f6e 6f6e 5f6e 6567  or.check_non_neg
+00028b50: 6174 6976 655f 666c 6f61 742c 0a20 2020  ative_float,.   
+00028b60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00028b70: 2020 2020 2020 2020 2020 2020 2070 6f73               pos
+00028b80: 745f 6c61 7965 726e 6f72 6d5f 7265 7369  t_layernorm_resi
+00028b90: 6475 616c 3d56 616c 6964 6174 6f72 2e63  dual=Validator.c
+00028ba0: 6865 636b 5f62 6f6f 6c2c 0a20 2020 2020  heck_bool,.     
+00028bb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00028bc0: 2020 2020 2020 2020 2020 206c 6179 6572             layer
+00028bd0: 6e6f 726d 5f63 6f6d 7075 7465 5f74 7970  norm_compute_typ
+00028be0: 653d 5f76 616c 6964 5f76 616c 7565 5f63  e=_valid_value_c
+00028bf0: 6865 636b 7328 5b6d 7374 7970 652e 666c  hecks([mstype.fl
+00028c00: 6f61 7433 322c 0a20 2020 2020 2020 2020  oat32,.         
+00028c10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00028c20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00028c30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00028c40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00028c50: 2020 206d 7374 7970 652e 666c 6f61 7431     mstype.float1
+00028c60: 362c 206d 7374 7970 652e 6266 6c6f 6174  6, mstype.bfloat
+00028c70: 3136 5d2c 0a20 2020 2020 2020 2020 2020  16],.           
+00028c80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00028c90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00028ca0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00028cb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00028cc0: 2254 7261 6e73 666f 726d 6572 456e 636f  "TransformerEnco
+00028cd0: 6465 7222 292c 0a20 2020 2020 2020 2020  der"),.         
+00028ce0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00028cf0: 2020 2020 2020 2073 6f66 746d 6178 5f63         softmax_c
+00028d00: 6f6d 7075 7465 5f74 7970 653d 5f76 616c  ompute_type=_val
+00028d10: 6964 5f76 616c 7565 5f63 6865 636b 7328  id_value_checks(
+00028d20: 5b6d 7374 7970 652e 666c 6f61 7433 322c  [mstype.float32,
+00028d30: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00028d40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00028d50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00028d60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00028d70: 2020 2020 2020 2020 2020 206d 7374 7970             mstyp
+00028d80: 652e 666c 6f61 7431 362c 206d 7374 7970  e.float16, mstyp
+00028d90: 652e 6266 6c6f 6174 3136 5d2c 0a20 2020  e.bfloat16],.   
+00028da0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00028db0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00028dc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00028dd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00028de0: 2020 2020 2020 2254 7261 6e73 666f 726d        "Transform
+00028df0: 6572 456e 636f 6465 7222 292c 0a20 2020  erEncoder"),.   
+00028e00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00028e10: 2020 2020 2020 2020 2020 2020 2070 6172               par
+00028e20: 616d 5f69 6e69 745f 7479 7065 3d5f 7661  am_init_type=_va
+00028e30: 6c69 645f 7661 6c75 655f 6368 6563 6b73  lid_value_checks
+00028e40: 285b 6d73 7479 7065 2e66 6c6f 6174 3332  ([mstype.float32
+00028e50: 2c20 6d73 7479 7065 2e66 6c6f 6174 3136  , mstype.float16
+00028e60: 2c20 6d73 7479 7065 2e62 666c 6f61 7431  , mstype.bfloat1
+00028e70: 365d 2c0a 2020 2020 2020 2020 2020 2020  6],.            
+00028e80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00028e90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00028ea0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00028eb0: 2020 2020 2020 2020 2254 7261 6e73 666f          "Transfo
+00028ec0: 726d 6572 456e 636f 6465 7222 292c 0a20  rmerEncoder"),. 
+00028ed0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00028ee0: 2020 2020 2020 2020 2020 2020 2020 2070                 p
+00028ef0: 6172 616c 6c65 6c5f 636f 6e66 6967 3d5f  arallel_config=_
+00028f00: 7661 6c69 645f 7479 7065 5f63 6865 636b  valid_type_check
+00028f10: 7328 5b54 7261 6e73 666f 726d 6572 4f70  s([TransformerOp
+00028f20: 5061 7261 6c6c 656c 436f 6e66 6967 5d2c  ParallelConfig],
+00028f30: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00028f40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00028f50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00028f60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00028f70: 2020 2020 2254 7261 6e73 666f 726d 6572      "Transformer
+00028f80: 456e 636f 6465 7222 292c 0a20 2020 2020  Encoder"),.     
+00028f90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00028fa0: 2020 2020 2020 2020 2020 2075 7365 5f70             use_p
+00028fb0: 6173 743d 5661 6c69 6461 746f 722e 6368  ast=Validator.ch
+00028fc0: 6563 6b5f 626f 6f6c 290a 2020 2020 6465  eck_bool).    de
+00028fd0: 6620 5f5f 696e 6974 5f5f 2873 656c 662c  f __init__(self,
+00028fe0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00028ff0: 2020 6261 7463 685f 7369 7a65 2c0a 2020    batch_size,.  
+00029000: 2020 2020 2020 2020 2020 2020 2020 206e                 n
+00029010: 756d 5f6c 6179 6572 732c 0a20 2020 2020  um_layers,.     
+00029020: 2020 2020 2020 2020 2020 2020 6869 6464              hidd
+00029030: 656e 5f73 697a 652c 0a20 2020 2020 2020  en_size,.       
+00029040: 2020 2020 2020 2020 2020 6666 6e5f 6869            ffn_hi
+00029050: 6464 656e 5f73 697a 652c 0a20 2020 2020  dden_size,.     
+00029060: 2020 2020 2020 2020 2020 2020 7365 715f              seq_
+00029070: 6c65 6e67 7468 2c0a 2020 2020 2020 2020  length,.        
+00029080: 2020 2020 2020 2020 206e 756d 5f68 6561           num_hea
+00029090: 6473 2c0a 2020 2020 2020 2020 2020 2020  ds,.            
+000290a0: 2020 2020 2061 7474 656e 7469 6f6e 5f64       attention_d
+000290b0: 726f 706f 7574 5f72 6174 653d 302e 312c  ropout_rate=0.1,
+000290c0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+000290d0: 2020 6869 6464 656e 5f64 726f 706f 7574    hidden_dropout
+000290e0: 5f72 6174 653d 302e 312c 0a20 2020 2020  _rate=0.1,.     
+000290f0: 2020 2020 2020 2020 2020 2020 6869 6464              hidd
+00029100: 656e 5f61 6374 3d27 6765 6c75 272c 0a20  en_act='gelu',. 
+00029110: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029120: 706f 7374 5f6c 6179 6572 6e6f 726d 5f72  post_layernorm_r
+00029130: 6573 6964 7561 6c3d 4661 6c73 652c 0a20  esidual=False,. 
 00029140: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029150: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029160: 2020 6666 6e5f 6869 6464 656e 5f73 697a    ffn_hidden_siz
-00029170: 653d 6666 6e5f 6869 6464 656e 5f73 697a  e=ffn_hidden_siz
-00029180: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
-00029190: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000291a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000291b0: 2020 2073 6571 5f6c 656e 6774 683d 7365     seq_length=se
-000291c0: 715f 6c65 6e67 7468 2c0a 2020 2020 2020  q_length,.      
-000291d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000291e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000291f0: 2020 2020 2020 2020 2020 6174 7465 6e74            attent
-00029200: 696f 6e5f 6472 6f70 6f75 745f 7261 7465  ion_dropout_rate
-00029210: 3d61 7474 656e 7469 6f6e 5f64 726f 706f  =attention_dropo
-00029220: 7574 5f72 6174 652c 0a20 2020 2020 2020  ut_rate,.       
-00029230: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029240: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029250: 2020 2020 2020 2020 2068 6964 6465 6e5f           hidden_
-00029260: 6472 6f70 6f75 745f 7261 7465 3d68 6964  dropout_rate=hid
-00029270: 6465 6e5f 6472 6f70 6f75 745f 7261 7465  den_dropout_rate
-00029280: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00029290: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000292a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000292b0: 2020 6c61 7965 726e 6f72 6d5f 636f 6d70    layernorm_comp
-000292c0: 7574 655f 7479 7065 3d6c 6179 6572 6e6f  ute_type=layerno
-000292d0: 726d 5f63 6f6d 7075 7465 5f74 7970 652c  rm_compute_type,
-000292e0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-000292f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029300: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029310: 2073 6f66 746d 6178 5f63 6f6d 7075 7465   softmax_compute
-00029320: 5f74 7970 653d 736f 6674 6d61 785f 636f  _type=softmax_co
-00029330: 6d70 7574 655f 7479 7065 2c0a 2020 2020  mpute_type,.    
-00029340: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029350: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029360: 2020 2020 2020 2020 2020 2020 6e75 6d5f              num_
-00029370: 6865 6164 733d 6e75 6d5f 6865 6164 732c  heads=num_heads,
-00029380: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00029390: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000293a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000293b0: 2068 6964 6465 6e5f 6163 743d 6869 6464   hidden_act=hidd
-000293c0: 656e 5f61 6374 2c0a 2020 2020 2020 2020  en_act,.        
-000293d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000293e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000293f0: 2020 2020 2020 2020 706f 7374 5f6c 6179          post_lay
-00029400: 6572 6e6f 726d 5f72 6573 6964 7561 6c3d  ernorm_residual=
-00029410: 706f 7374 5f6c 6179 6572 6e6f 726d 5f72  post_layernorm_r
-00029420: 6573 6964 7561 6c2c 0a20 2020 2020 2020  esidual,.       
-00029430: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029440: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029450: 2020 2020 2020 2020 2070 6172 616d 5f69           param_i
-00029460: 6e69 745f 7479 7065 3d70 6172 616d 5f69  nit_type=param_i
-00029470: 6e69 745f 7479 7065 2c0a 2020 2020 2020  nit_type,.      
-00029480: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029490: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000294a0: 2020 2020 2020 2020 2020 7573 655f 7061            use_pa
-000294b0: 7374 3d75 7365 5f70 6173 742c 0a20 2020  st=use_past,.   
-000294c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000294d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000294e0: 2020 2020 2020 2020 2020 2020 206d 6f65               moe
-000294f0: 5f63 6f6e 6669 673d 6d6f 655f 636f 6e66  _config=moe_conf
-00029500: 6967 2c0a 2020 2020 2020 2020 2020 2020  ig,.            
-00029510: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029520: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029530: 2020 2020 7061 7261 6c6c 656c 5f63 6f6e      parallel_con
-00029540: 6669 673d 636f 6e66 6967 5f74 6f5f 6c61  fig=config_to_la
-00029550: 7965 7229 0a20 2020 2020 2020 2020 2020  yer).           
-00029560: 2020 2020 2023 2049 6620 7468 6520 7573       # If the us
-00029570: 6572 2064 6f65 736e 2774 2070 6173 7320  er doesn't pass 
-00029580: 7468 6520 6675 7369 6f6e 2066 756e 6374  the fusion funct
-00029590: 696f 6e2c 2075 7365 2074 6865 2064 6566  ion, use the def
-000295a0: 6175 6c74 206f 6e65 0a20 2020 2020 2020  ault one.       
-000295b0: 2020 2020 2020 2020 2069 6620 6e6f 7420           if not 
-000295c0: 6c61 6d62 6461 5f66 756e 633a 0a20 2020  lambda_func:.   
-000295d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000295e0: 206c 616d 6264 615f 6675 6e63 203d 205f   lambda_func = _
-000295f0: 6765 745f 6c61 6d62 6461 5f66 756e 6328  get_lambda_func(
-00029600: 290a 0a20 2020 2020 2020 2020 2020 2020  )..             
-00029610: 2020 206c 616d 6264 615f 6675 6e63 2862     lambda_func(b
-00029620: 6c6f 636b 2c20 6c61 7965 725f 6964 3d69  lock, layer_id=i
-00029630: 2c20 6c61 7965 7273 3d6e 756d 5f6c 6179  , layers=num_lay
-00029640: 6572 732c 0a20 2020 2020 2020 2020 2020  ers,.           
-00029650: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029660: 206f 6666 7365 743d 6f66 6673 6574 2c20   offset=offset, 
-00029670: 7061 7261 6c6c 656c 5f63 6f6e 6669 673d  parallel_config=
-00029680: 7061 7261 6c6c 656c 5f63 6f6e 6669 6729  parallel_config)
-00029690: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-000296a0: 2073 656c 662e 626c 6f63 6b73 2e61 7070   self.blocks.app
-000296b0: 656e 6428 626c 6f63 6b29 0a20 2020 2020  end(block).     
-000296c0: 2020 2065 6c69 6620 5f67 6574 5f70 6172     elif _get_par
-000296d0: 616c 6c65 6c5f 6d6f 6465 2829 206e 6f74  allel_mode() not
-000296e0: 2069 6e20 2850 6172 616c 6c65 6c4d 6f64   in (ParallelMod
-000296f0: 652e 4155 544f 5f50 4152 414c 4c45 4c2c  e.AUTO_PARALLEL,
-00029700: 293a 0a20 2020 2020 2020 2020 2020 2073  ):.            s
-00029710: 656c 662e 6164 6420 3d20 502e 4164 6428  elf.add = P.Add(
-00029720: 292e 7368 6172 6428 2828 292c 2028 2929  ).shard(((), ())
-00029730: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
-00029740: 6c66 2e61 7578 5f6c 6f73 7320 3d20 5465  lf.aux_loss = Te
-00029750: 6e73 6f72 2830 2e30 2c20 6d73 7479 7065  nsor(0.0, mstype
-00029760: 2e66 6c6f 6174 3332 290a 2020 2020 2020  .float32).      
-00029770: 2020 2020 2020 6c6f 6767 6572 2e77 6172        logger.war
-00029780: 6e69 6e67 2822 466f 7220 7061 7261 6c6c  ning("For parall
-00029790: 656c 206d 6f64 652c 2073 6861 7264 696e  el mode, shardin
-000297a0: 6720 7072 6f70 6167 6174 696f 6e20 6973  g propagation is
-000297b0: 2072 6563 6f6d 6d65 6e64 6564 2c20 796f   recommended, yo
-000297c0: 7520 6361 6e20 7573 6520 6974 2062 7920  u can use it by 
-000297d0: 7365 7474 696e 6720 220a 2020 2020 2020  setting ".      
-000297e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000297f0: 2020 2020 2022 2773 6574 5f61 7574 6f5f       "'set_auto_
-00029800: 7061 7261 6c6c 656c 5f63 6f6e 7465 7874  parallel_context
-00029810: 2870 6172 616c 6c65 6c5f 6d6f 6465 3d50  (parallel_mode=P
-00029820: 6172 616c 6c65 6c4d 6f64 652e 4155 544f  arallelMode.AUTO
-00029830: 5f50 4152 414c 4c45 4c2c 2022 0a20 2020  _PARALLEL, ".   
+00029150: 6c61 7965 726e 6f72 6d5f 636f 6d70 7574  layernorm_comput
+00029160: 655f 7479 7065 3d6d 7374 7970 652e 666c  e_type=mstype.fl
+00029170: 6f61 7433 322c 0a20 2020 2020 2020 2020  oat32,.         
+00029180: 2020 2020 2020 2020 736f 6674 6d61 785f          softmax_
+00029190: 636f 6d70 7574 655f 7479 7065 3d6d 7374  compute_type=mst
+000291a0: 7970 652e 666c 6f61 7433 322c 0a20 2020  ype.float32,.   
+000291b0: 2020 2020 2020 2020 2020 2020 2020 7061                pa
+000291c0: 7261 6d5f 696e 6974 5f74 7970 653d 6d73  ram_init_type=ms
+000291d0: 7479 7065 2e66 6c6f 6174 3332 2c0a 2020  type.float32,.  
+000291e0: 2020 2020 2020 2020 2020 2020 2020 206c                 l
+000291f0: 616d 6264 615f 6675 6e63 3d4e 6f6e 652c  ambda_func=None,
+00029200: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00029210: 2020 6f66 6673 6574 3d30 2c0a 2020 2020    offset=0,.    
+00029220: 2020 2020 2020 2020 2020 2020 2075 7365               use
+00029230: 5f70 6173 743d 4661 6c73 652c 0a20 2020  _past=False,.   
+00029240: 2020 2020 2020 2020 2020 2020 2020 6d6f                mo
+00029250: 655f 636f 6e66 6967 3d64 6566 6175 6c74  e_config=default
+00029260: 5f6d 6f65 5f63 6f6e 6669 672c 0a20 2020  _moe_config,.   
+00029270: 2020 2020 2020 2020 2020 2020 2020 7061                pa
+00029280: 7261 6c6c 656c 5f63 6f6e 6669 673d 6465  rallel_config=de
+00029290: 6661 756c 745f 7472 616e 7366 6f72 6d65  fault_transforme
+000292a0: 725f 636f 6e66 6967 293a 0a20 2020 2020  r_config):.     
+000292b0: 2020 2073 7570 6572 2854 7261 6e73 666f     super(Transfo
+000292c0: 726d 6572 456e 636f 6465 722c 2073 656c  rmerEncoder, sel
+000292d0: 6629 2e5f 5f69 6e69 745f 5f28 290a 2020  f).__init__().  
+000292e0: 2020 2020 2020 5f63 6865 636b 5f63 6f6e        _check_con
+000292f0: 6669 6728 7061 7261 6c6c 656c 5f63 6f6e  fig(parallel_con
+00029300: 6669 6729 0a20 2020 2020 2020 205f 6368  fig).        _ch
+00029310: 6563 6b5f 6d6f 655f 636f 6e66 6967 286d  eck_moe_config(m
+00029320: 6f65 5f63 6f6e 6669 672c 2070 6172 616c  oe_config, paral
+00029330: 6c65 6c5f 636f 6e66 6967 290a 2020 2020  lel_config).    
+00029340: 2020 2020 7365 6c66 2e75 7365 5f6d 6f65      self.use_moe
+00029350: 203d 2028 6d6f 655f 636f 6e66 6967 2e65   = (moe_config.e
+00029360: 7870 6572 745f 6e75 6d20 3e20 3129 0a20  xpert_num > 1). 
+00029370: 2020 2020 2020 2069 6620 6261 7463 685f         if batch_
+00029380: 7369 7a65 206f 7220 7573 655f 7061 7374  size or use_past
+00029390: 3a0a 2020 2020 2020 2020 2020 2020 5661  :.            Va
+000293a0: 6c69 6461 746f 722e 6368 6563 6b5f 706f  lidator.check_po
+000293b0: 7369 7469 7665 5f69 6e74 2862 6174 6368  sitive_int(batch
+000293c0: 5f73 697a 6529 0a20 2020 2020 2020 2063  _size).        c
+000293d0: 6f6e 6669 675f 746f 5f6c 6179 6572 203d  onfig_to_layer =
+000293e0: 2070 6172 616c 6c65 6c5f 636f 6e66 6967   parallel_config
+000293f0: 2e6d 6f65 5f70 6172 616c 6c65 6c5f 636f  .moe_parallel_co
+00029400: 6e66 6967 2069 6620 7365 6c66 2e75 7365  nfig if self.use
+00029410: 5f6d 6f65 2065 6c73 6520 7061 7261 6c6c  _moe else parall
+00029420: 656c 5f63 6f6e 6669 672e 6470 5f6d 705f  el_config.dp_mp_
+00029430: 636f 6e66 6967 0a20 2020 2020 2020 2069  config.        i
+00029440: 6620 5f67 6574 5f70 6172 616c 6c65 6c5f  f _get_parallel_
+00029450: 6d6f 6465 2829 2069 6e20 2850 6172 616c  mode() in (Paral
+00029460: 6c65 6c4d 6f64 652e 4155 544f 5f50 4152  lelMode.AUTO_PAR
+00029470: 414c 4c45 4c2c 293a 0a20 2020 2020 2020  ALLEL,):.       
+00029480: 2020 2020 2073 656c 662e 6164 6420 3d20       self.add = 
+00029490: 502e 4164 6428 290a 2020 2020 2020 2020  P.Add().        
+000294a0: 2020 2020 7365 6c66 2e61 7578 5f6c 6f73      self.aux_los
+000294b0: 7320 3d20 5465 6e73 6f72 2830 2e30 2c20  s = Tensor(0.0, 
+000294c0: 6d73 7479 7065 2e66 6c6f 6174 3332 290a  mstype.float32).
+000294d0: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+000294e0: 2e6e 756d 5f6c 6179 6572 7320 3d20 6e75  .num_layers = nu
+000294f0: 6d5f 6c61 7965 7273 0a20 2020 2020 2020  m_layers.       
+00029500: 2020 2020 2073 656c 662e 626c 6f63 6b73       self.blocks
+00029510: 203d 206e 6e2e 4365 6c6c 4c69 7374 2829   = nn.CellList()
+00029520: 0a20 2020 2020 2020 2020 2020 2066 6f72  .            for
+00029530: 2069 2069 6e20 7261 6e67 6528 6e75 6d5f   i in range(num_
+00029540: 6c61 7965 7273 293a 0a20 2020 2020 2020  layers):.       
+00029550: 2020 2020 2020 2020 2062 6c6f 636b 203d           block =
+00029560: 2054 7261 6e73 666f 726d 6572 456e 636f   TransformerEnco
+00029570: 6465 724c 6179 6572 2868 6964 6465 6e5f  derLayer(hidden_
+00029580: 7369 7a65 3d68 6964 6465 6e5f 7369 7a65  size=hidden_size
+00029590: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+000295a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000295b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000295c0: 2020 6261 7463 685f 7369 7a65 3d62 6174    batch_size=bat
+000295d0: 6368 5f73 697a 652c 0a20 2020 2020 2020  ch_size,.       
+000295e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000295f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029600: 2020 2020 2020 2020 2066 666e 5f68 6964           ffn_hid
+00029610: 6465 6e5f 7369 7a65 3d66 666e 5f68 6964  den_size=ffn_hid
+00029620: 6465 6e5f 7369 7a65 2c0a 2020 2020 2020  den_size,.      
+00029630: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029640: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029650: 2020 2020 2020 2020 2020 7365 715f 6c65            seq_le
+00029660: 6e67 7468 3d73 6571 5f6c 656e 6774 682c  ngth=seq_length,
+00029670: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00029680: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029690: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000296a0: 2061 7474 656e 7469 6f6e 5f64 726f 706f   attention_dropo
+000296b0: 7574 5f72 6174 653d 6174 7465 6e74 696f  ut_rate=attentio
+000296c0: 6e5f 6472 6f70 6f75 745f 7261 7465 2c0a  n_dropout_rate,.
+000296d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000296e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000296f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029700: 6869 6464 656e 5f64 726f 706f 7574 5f72  hidden_dropout_r
+00029710: 6174 653d 6869 6464 656e 5f64 726f 706f  ate=hidden_dropo
+00029720: 7574 5f72 6174 652c 0a20 2020 2020 2020  ut_rate,.       
+00029730: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029740: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029750: 2020 2020 2020 2020 206c 6179 6572 6e6f           layerno
+00029760: 726d 5f63 6f6d 7075 7465 5f74 7970 653d  rm_compute_type=
+00029770: 6c61 7965 726e 6f72 6d5f 636f 6d70 7574  layernorm_comput
+00029780: 655f 7479 7065 2c0a 2020 2020 2020 2020  e_type,.        
+00029790: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000297a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000297b0: 2020 2020 2020 2020 736f 6674 6d61 785f          softmax_
+000297c0: 636f 6d70 7574 655f 7479 7065 3d73 6f66  compute_type=sof
+000297d0: 746d 6178 5f63 6f6d 7075 7465 5f74 7970  tmax_compute_typ
+000297e0: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
+000297f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029800: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029810: 2020 206e 756d 5f68 6561 6473 3d6e 756d     num_heads=num
+00029820: 5f68 6561 6473 2c0a 2020 2020 2020 2020  _heads,.        
+00029830: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00029840: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029850: 2020 2020 2020 2020 2273 6561 7263 685f          "search_
-00029860: 6d6f 6465 3d5c 2273 6861 7264 696e 675f  mode=\"sharding_
-00029870: 7072 6f70 6167 6174 696f 6e5c 2229 2720  propagation\")' 
-00029880: 616e 6420 220a 2020 2020 2020 2020 2020  and ".          
-00029890: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000298a0: 2022 2773 6574 5f61 6c67 6f5f 7061 7261   "'set_algo_para
-000298b0: 6d65 7465 7273 2865 6c65 6d65 6e74 7769  meters(elementwi
-000298c0: 7365 5f6f 705f 7374 7261 7465 6779 5f66  se_op_strategy_f
-000298d0: 6f6c 6c6f 773d 4661 6c73 652c 2066 756c  ollow=False, ful
-000298e0: 6c79 5f75 7365 5f64 6576 6963 6573 3d46  ly_use_devices=F
-000298f0: 616c 7365 2927 2229 0a20 2020 2020 2020  alse)'").       
-00029900: 2020 2020 2073 656c 662e 6e75 6d5f 6c61       self.num_la
-00029910: 7965 7273 203d 206e 756d 5f6c 6179 6572  yers = num_layer
-00029920: 730a 2020 2020 2020 2020 2020 2020 7365  s.            se
-00029930: 6c66 2e62 6c6f 636b 7320 3d20 6e6e 2e43  lf.blocks = nn.C
-00029940: 656c 6c4c 6973 7428 290a 2020 2020 2020  ellList().      
-00029950: 2020 2020 2020 666f 7220 6920 696e 2072        for i in r
-00029960: 616e 6765 286e 756d 5f6c 6179 6572 7329  ange(num_layers)
-00029970: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
-00029980: 2020 626c 6f63 6b20 3d20 5472 616e 7366    block = Transf
-00029990: 6f72 6d65 7245 6e63 6f64 6572 4c61 7965  ormerEncoderLaye
-000299a0: 7228 6869 6464 656e 5f73 697a 653d 6869  r(hidden_size=hi
-000299b0: 6464 656e 5f73 697a 652c 0a20 2020 2020  dden_size,.     
+00029850: 2020 2020 2020 2020 6869 6464 656e 5f61          hidden_a
+00029860: 6374 3d68 6964 6465 6e5f 6163 742c 0a20  ct=hidden_act,. 
+00029870: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029880: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029890: 2020 2020 2020 2020 2020 2020 2020 2070                 p
+000298a0: 6f73 745f 6c61 7965 726e 6f72 6d5f 7265  ost_layernorm_re
+000298b0: 7369 6475 616c 3d70 6f73 745f 6c61 7965  sidual=post_laye
+000298c0: 726e 6f72 6d5f 7265 7369 6475 616c 2c0a  rnorm_residual,.
+000298d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000298e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000298f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029900: 7061 7261 6d5f 696e 6974 5f74 7970 653d  param_init_type=
+00029910: 7061 7261 6d5f 696e 6974 5f74 7970 652c  param_init_type,
+00029920: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00029930: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029940: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029950: 2075 7365 5f70 6173 743d 7573 655f 7061   use_past=use_pa
+00029960: 7374 2c0a 2020 2020 2020 2020 2020 2020  st,.            
+00029970: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029980: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029990: 2020 2020 6d6f 655f 636f 6e66 6967 3d6d      moe_config=m
+000299a0: 6f65 5f63 6f6e 6669 672c 0a20 2020 2020  oe_config,.     
+000299b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 000299c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000299d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000299e0: 2020 2020 2020 2020 2020 2062 6174 6368             batch
-000299f0: 5f73 697a 653d 6261 7463 685f 7369 7a65  _size=batch_size
-00029a00: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00029a10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029a20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029a30: 2020 6666 6e5f 6869 6464 656e 5f73 697a    ffn_hidden_siz
-00029a40: 653d 6666 6e5f 6869 6464 656e 5f73 697a  e=ffn_hidden_siz
-00029a50: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
-00029a60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029a70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029a80: 2020 2073 6571 5f6c 656e 6774 683d 7365     seq_length=se
-00029a90: 715f 6c65 6e67 7468 2c0a 2020 2020 2020  q_length,.      
-00029aa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029ab0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029ac0: 2020 2020 2020 2020 2020 6174 7465 6e74            attent
-00029ad0: 696f 6e5f 6472 6f70 6f75 745f 7261 7465  ion_dropout_rate
-00029ae0: 3d61 7474 656e 7469 6f6e 5f64 726f 706f  =attention_dropo
-00029af0: 7574 5f72 6174 652c 0a20 2020 2020 2020  ut_rate,.       
-00029b00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029b10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029b20: 2020 2020 2020 2020 2068 6964 6465 6e5f           hidden_
-00029b30: 6472 6f70 6f75 745f 7261 7465 3d68 6964  dropout_rate=hid
-00029b40: 6465 6e5f 6472 6f70 6f75 745f 7261 7465  den_dropout_rate
-00029b50: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00029b60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029b70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029b80: 2020 6c61 7965 726e 6f72 6d5f 636f 6d70    layernorm_comp
-00029b90: 7574 655f 7479 7065 3d6c 6179 6572 6e6f  ute_type=layerno
-00029ba0: 726d 5f63 6f6d 7075 7465 5f74 7970 652c  rm_compute_type,
-00029bb0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00029bc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029bd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029be0: 2073 6f66 746d 6178 5f63 6f6d 7075 7465   softmax_compute
-00029bf0: 5f74 7970 653d 736f 6674 6d61 785f 636f  _type=softmax_co
-00029c00: 6d70 7574 655f 7479 7065 2c0a 2020 2020  mpute_type,.    
-00029c10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029c20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029c30: 2020 2020 2020 2020 2020 2020 6e75 6d5f              num_
-00029c40: 6865 6164 733d 6e75 6d5f 6865 6164 732c  heads=num_heads,
-00029c50: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00029c60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029c70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029c80: 2068 6964 6465 6e5f 6163 743d 6869 6464   hidden_act=hidd
-00029c90: 656e 5f61 6374 2c0a 2020 2020 2020 2020  en_act,.        
-00029ca0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029cb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029cc0: 2020 2020 2020 2020 706f 7374 5f6c 6179          post_lay
-00029cd0: 6572 6e6f 726d 5f72 6573 6964 7561 6c3d  ernorm_residual=
-00029ce0: 706f 7374 5f6c 6179 6572 6e6f 726d 5f72  post_layernorm_r
-00029cf0: 6573 6964 7561 6c2c 0a20 2020 2020 2020  esidual,.       
-00029d00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029d10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029d20: 2020 2020 2020 2020 2070 6172 616d 5f69           param_i
-00029d30: 6e69 745f 7479 7065 3d70 6172 616d 5f69  nit_type=param_i
-00029d40: 6e69 745f 7479 7065 2c0a 2020 2020 2020  nit_type,.      
-00029d50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029d60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029d70: 2020 2020 2020 2020 2020 7573 655f 7061            use_pa
-00029d80: 7374 3d75 7365 5f70 6173 742c 0a20 2020  st=use_past,.   
-00029d90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029da0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029db0: 2020 2020 2020 2020 2020 2020 206d 6f65               moe
-00029dc0: 5f63 6f6e 6669 673d 6d6f 655f 636f 6e66  _config=moe_conf
-00029dd0: 6967 2c0a 2020 2020 2020 2020 2020 2020  ig,.            
-00029de0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029df0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029e00: 2020 2020 7061 7261 6c6c 656c 5f63 6f6e      parallel_con
-00029e10: 6669 673d 636f 6e66 6967 5f74 6f5f 6c61  fig=config_to_la
-00029e20: 7965 7229 0a20 2020 2020 2020 2020 2020  yer).           
-00029e30: 2020 2020 2023 2049 6620 7468 6520 7573       # If the us
-00029e40: 6572 2064 6f65 736e 2774 2070 6173 7320  er doesn't pass 
-00029e50: 7468 6520 6675 7369 6f6e 2066 756e 6374  the fusion funct
-00029e60: 696f 6e2c 2075 7365 2074 6865 2064 6566  ion, use the def
-00029e70: 6175 6c74 206f 6e65 0a20 2020 2020 2020  ault one.       
-00029e80: 2020 2020 2020 2020 2069 6620 6e6f 7420           if not 
-00029e90: 6c61 6d62 6461 5f66 756e 633a 0a20 2020  lambda_func:.   
-00029ea0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029eb0: 206c 616d 6264 615f 6675 6e63 203d 205f   lambda_func = _
-00029ec0: 6765 745f 6c61 6d62 6461 5f66 756e 6328  get_lambda_func(
-00029ed0: 290a 0a20 2020 2020 2020 2020 2020 2020  )..             
-00029ee0: 2020 206c 616d 6264 615f 6675 6e63 2862     lambda_func(b
-00029ef0: 6c6f 636b 2c20 6c61 7965 725f 6964 3d69  lock, layer_id=i
-00029f00: 2c20 6c61 7965 7273 3d6e 756d 5f6c 6179  , layers=num_lay
-00029f10: 6572 732c 0a20 2020 2020 2020 2020 2020  ers,.           
-00029f20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00029f30: 206f 6666 7365 743d 6f66 6673 6574 2c20   offset=offset, 
-00029f40: 7061 7261 6c6c 656c 5f63 6f6e 6669 673d  parallel_config=
-00029f50: 7061 7261 6c6c 656c 5f63 6f6e 6669 6729  parallel_config)
-00029f60: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00029f70: 2073 656c 662e 626c 6f63 6b73 2e61 7070   self.blocks.app
-00029f80: 656e 6428 626c 6f63 6b29 0a20 2020 2020  end(block).     
-00029f90: 2020 2065 6c73 653a 0a20 2020 2020 2020     else:.       
-00029fa0: 2020 2020 2072 6169 7365 2052 756e 7469       raise Runti
-00029fb0: 6d65 4572 726f 7228 6622 5468 6520 7b73  meError(f"The {s
-00029fc0: 656c 662e 636c 735f 6e61 6d65 7d20 6f6e  elf.cls_name} on
-00029fd0: 6c79 2073 7570 706f 7274 2073 6861 7264  ly support shard
-00029fe0: 696e 6720 7072 6f70 6167 6174 696f 6e20  ing propagation 
-00029ff0: 6f72 2022 0a20 2020 2020 2020 2020 2020  or ".           
+000299d0: 2020 2020 2020 2020 2020 2070 6172 616c             paral
+000299e0: 6c65 6c5f 636f 6e66 6967 3d63 6f6e 6669  lel_config=confi
+000299f0: 675f 746f 5f6c 6179 6572 290a 2020 2020  g_to_layer).    
+00029a00: 2020 2020 2020 2020 2020 2020 2320 4966              # If
+00029a10: 2074 6865 2075 7365 7220 646f 6573 6e27   the user doesn'
+00029a20: 7420 7061 7373 2074 6865 2066 7573 696f  t pass the fusio
+00029a30: 6e20 6675 6e63 7469 6f6e 2c20 7573 6520  n function, use 
+00029a40: 7468 6520 6465 6661 756c 7420 6f6e 650a  the default one.
+00029a50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029a60: 6966 206e 6f74 206c 616d 6264 615f 6675  if not lambda_fu
+00029a70: 6e63 3a0a 2020 2020 2020 2020 2020 2020  nc:.            
+00029a80: 2020 2020 2020 2020 6c61 6d62 6461 5f66          lambda_f
+00029a90: 756e 6320 3d20 5f67 6574 5f6c 616d 6264  unc = _get_lambd
+00029aa0: 615f 6675 6e63 2829 0a0a 2020 2020 2020  a_func()..      
+00029ab0: 2020 2020 2020 2020 2020 6c61 6d62 6461            lambda
+00029ac0: 5f66 756e 6328 626c 6f63 6b2c 206c 6179  _func(block, lay
+00029ad0: 6572 5f69 643d 692c 206c 6179 6572 733d  er_id=i, layers=
+00029ae0: 6e75 6d5f 6c61 7965 7273 2c0a 2020 2020  num_layers,.    
+00029af0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029b00: 2020 2020 2020 2020 6f66 6673 6574 3d6f          offset=o
+00029b10: 6666 7365 742c 2070 6172 616c 6c65 6c5f  ffset, parallel_
+00029b20: 636f 6e66 6967 3d70 6172 616c 6c65 6c5f  config=parallel_
+00029b30: 636f 6e66 6967 290a 2020 2020 2020 2020  config).        
+00029b40: 2020 2020 2020 2020 7365 6c66 2e62 6c6f          self.blo
+00029b50: 636b 732e 6170 7065 6e64 2862 6c6f 636b  cks.append(block
+00029b60: 290a 2020 2020 2020 2020 656c 6966 205f  ).        elif _
+00029b70: 6765 745f 7061 7261 6c6c 656c 5f6d 6f64  get_parallel_mod
+00029b80: 6528 2920 6e6f 7420 696e 2028 5061 7261  e() not in (Para
+00029b90: 6c6c 656c 4d6f 6465 2e41 5554 4f5f 5041  llelMode.AUTO_PA
+00029ba0: 5241 4c4c 454c 2c29 3a0a 2020 2020 2020  RALLEL,):.      
+00029bb0: 2020 2020 2020 7365 6c66 2e61 6464 203d        self.add =
+00029bc0: 2050 2e41 6464 2829 2e73 6861 7264 2828   P.Add().shard((
+00029bd0: 2829 2c20 2829 2929 0a20 2020 2020 2020  (), ())).       
+00029be0: 2020 2020 2073 656c 662e 6175 785f 6c6f       self.aux_lo
+00029bf0: 7373 203d 2054 656e 736f 7228 302e 302c  ss = Tensor(0.0,
+00029c00: 206d 7374 7970 652e 666c 6f61 7433 3229   mstype.float32)
+00029c10: 0a20 2020 2020 2020 2020 2020 206c 6f67  .            log
+00029c20: 6765 722e 7761 726e 696e 6728 2246 6f72  ger.warning("For
+00029c30: 2070 6172 616c 6c65 6c20 6d6f 6465 2c20   parallel mode, 
+00029c40: 7368 6172 6469 6e67 2070 726f 7061 6761  sharding propaga
+00029c50: 7469 6f6e 2069 7320 7265 636f 6d6d 656e  tion is recommen
+00029c60: 6465 642c 2079 6f75 2063 616e 2075 7365  ded, you can use
+00029c70: 2069 7420 6279 2073 6574 7469 6e67 2022   it by setting "
+00029c80: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00029c90: 2020 2020 2020 2020 2020 2020 2227 7365              "'se
+00029ca0: 745f 6175 746f 5f70 6172 616c 6c65 6c5f  t_auto_parallel_
+00029cb0: 636f 6e74 6578 7428 7061 7261 6c6c 656c  context(parallel
+00029cc0: 5f6d 6f64 653d 5061 7261 6c6c 656c 4d6f  _mode=ParallelMo
+00029cd0: 6465 2e41 5554 4f5f 5041 5241 4c4c 454c  de.AUTO_PARALLEL
+00029ce0: 2c20 220a 2020 2020 2020 2020 2020 2020  , ".            
+00029cf0: 2020 2020 2020 2020 2020 2020 2020 2022                 "
+00029d00: 7365 6172 6368 5f6d 6f64 653d 5c22 7368  search_mode=\"sh
+00029d10: 6172 6469 6e67 5f70 726f 7061 6761 7469  arding_propagati
+00029d20: 6f6e 5c22 2927 2061 6e64 2022 0a20 2020  on\")' and ".   
+00029d30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029d40: 2020 2020 2020 2020 2227 7365 745f 616c          "'set_al
+00029d50: 676f 5f70 6172 616d 6574 6572 7328 656c  go_parameters(el
+00029d60: 656d 656e 7477 6973 655f 6f70 5f73 7472  ementwise_op_str
+00029d70: 6174 6567 795f 666f 6c6c 6f77 3d46 616c  ategy_follow=Fal
+00029d80: 7365 2c20 6675 6c6c 795f 7573 655f 6465  se, fully_use_de
+00029d90: 7669 6365 733d 4661 6c73 6529 2722 290a  vices=False)'").
+00029da0: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+00029db0: 2e6e 756d 5f6c 6179 6572 7320 3d20 6e75  .num_layers = nu
+00029dc0: 6d5f 6c61 7965 7273 0a20 2020 2020 2020  m_layers.       
+00029dd0: 2020 2020 2073 656c 662e 626c 6f63 6b73       self.blocks
+00029de0: 203d 206e 6e2e 4365 6c6c 4c69 7374 2829   = nn.CellList()
+00029df0: 0a20 2020 2020 2020 2020 2020 2066 6f72  .            for
+00029e00: 2069 2069 6e20 7261 6e67 6528 6e75 6d5f   i in range(num_
+00029e10: 6c61 7965 7273 293a 0a20 2020 2020 2020  layers):.       
+00029e20: 2020 2020 2020 2020 2062 6c6f 636b 203d           block =
+00029e30: 2054 7261 6e73 666f 726d 6572 456e 636f   TransformerEnco
+00029e40: 6465 724c 6179 6572 2868 6964 6465 6e5f  derLayer(hidden_
+00029e50: 7369 7a65 3d68 6964 6465 6e5f 7369 7a65  size=hidden_size
+00029e60: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00029e70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029e80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029e90: 2020 6261 7463 685f 7369 7a65 3d62 6174    batch_size=bat
+00029ea0: 6368 5f73 697a 652c 0a20 2020 2020 2020  ch_size,.       
+00029eb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029ec0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029ed0: 2020 2020 2020 2020 2066 666e 5f68 6964           ffn_hid
+00029ee0: 6465 6e5f 7369 7a65 3d66 666e 5f68 6964  den_size=ffn_hid
+00029ef0: 6465 6e5f 7369 7a65 2c0a 2020 2020 2020  den_size,.      
+00029f00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029f10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029f20: 2020 2020 2020 2020 2020 7365 715f 6c65            seq_le
+00029f30: 6e67 7468 3d73 6571 5f6c 656e 6774 682c  ngth=seq_length,
+00029f40: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00029f50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029f60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029f70: 2061 7474 656e 7469 6f6e 5f64 726f 706f   attention_dropo
+00029f80: 7574 5f72 6174 653d 6174 7465 6e74 696f  ut_rate=attentio
+00029f90: 6e5f 6472 6f70 6f75 745f 7261 7465 2c0a  n_dropout_rate,.
+00029fa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029fb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029fc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00029fd0: 6869 6464 656e 5f64 726f 706f 7574 5f72  hidden_dropout_r
+00029fe0: 6174 653d 6869 6464 656e 5f64 726f 706f  ate=hidden_dropo
+00029ff0: 7574 5f72 6174 652c 0a20 2020 2020 2020  ut_rate,.       
 0002a000: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002a010: 2020 2020 6622 7365 6d69 2d61 7574 6f20      f"semi-auto 
-0002a020: 7061 7261 6c6c 656c 206d 6f64 6520 6e6f  parallel mode no
-0002a030: 772e 2229 0a0a 2020 2020 6465 6620 636f  w.")..    def co
-0002a040: 6e73 7472 7563 7428 7365 6c66 2c20 6869  nstruct(self, hi
-0002a050: 6464 656e 5f73 7461 7465 732c 2061 7474  dden_states, att
-0002a060: 656e 7469 6f6e 5f6d 6173 6b2c 2069 6e69  ention_mask, ini
-0002a070: 745f 7265 7365 743d 5472 7565 2c20 6261  t_reset=True, ba
-0002a080: 7463 685f 7661 6c69 645f 6c65 6e67 7468  tch_valid_length
-0002a090: 3d4e 6f6e 6529 3a0a 2020 2020 2020 2020  =None):.        
-0002a0a0: 2222 2266 6f72 7761 7264 2070 726f 6365  """forward proce
-0002a0b0: 7373 2222 220a 2020 2020 2020 2020 7072  ss""".        pr
-0002a0c0: 6573 656e 745f 6c61 7965 7220 3d20 2829  esent_layer = ()
-0002a0d0: 0a20 2020 2020 2020 2069 6620 7365 6c66  .        if self
-0002a0e0: 2e75 7365 5f6d 6f65 3a0a 2020 2020 2020  .use_moe:.      
-0002a0f0: 2020 2020 2020 6163 6375 6d5f 6c6f 7373        accum_loss
-0002a100: 203d 2073 656c 662e 6175 785f 6c6f 7373   = self.aux_loss
-0002a110: 0a20 2020 2020 2020 2020 2020 2066 6f72  .            for
-0002a120: 2069 2069 6e20 7261 6e67 6528 7365 6c66   i in range(self
-0002a130: 2e6e 756d 5f6c 6179 6572 7329 3a0a 2020  .num_layers):.  
-0002a140: 2020 2020 2020 2020 2020 2020 2020 6869                hi
-0002a150: 6464 656e 5f73 7461 7465 732c 2070 7265  dden_states, pre
-0002a160: 7365 6e74 2c20 6175 785f 6c6f 7373 203d  sent, aux_loss =
-0002a170: 2073 656c 662e 626c 6f63 6b73 5b69 5d28   self.blocks[i](
-0002a180: 6869 6464 656e 5f73 7461 7465 732c 0a20  hidden_states,. 
-0002a190: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a010: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a020: 2020 2020 2020 2020 206c 6179 6572 6e6f           layerno
+0002a030: 726d 5f63 6f6d 7075 7465 5f74 7970 653d  rm_compute_type=
+0002a040: 6c61 7965 726e 6f72 6d5f 636f 6d70 7574  layernorm_comput
+0002a050: 655f 7479 7065 2c0a 2020 2020 2020 2020  e_type,.        
+0002a060: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a070: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a080: 2020 2020 2020 2020 736f 6674 6d61 785f          softmax_
+0002a090: 636f 6d70 7574 655f 7479 7065 3d73 6f66  compute_type=sof
+0002a0a0: 746d 6178 5f63 6f6d 7075 7465 5f74 7970  tmax_compute_typ
+0002a0b0: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
+0002a0c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a0d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a0e0: 2020 206e 756d 5f68 6561 6473 3d6e 756d     num_heads=num
+0002a0f0: 5f68 6561 6473 2c0a 2020 2020 2020 2020  _heads,.        
+0002a100: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a110: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a120: 2020 2020 2020 2020 6869 6464 656e 5f61          hidden_a
+0002a130: 6374 3d68 6964 6465 6e5f 6163 742c 0a20  ct=hidden_act,. 
+0002a140: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a150: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a160: 2020 2020 2020 2020 2020 2020 2020 2070                 p
+0002a170: 6f73 745f 6c61 7965 726e 6f72 6d5f 7265  ost_layernorm_re
+0002a180: 7369 6475 616c 3d70 6f73 745f 6c61 7965  sidual=post_laye
+0002a190: 726e 6f72 6d5f 7265 7369 6475 616c 2c0a  rnorm_residual,.
 0002a1a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 0002a1b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 0002a1c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002a1d0: 2061 7474 656e 7469 6f6e 5f6d 6173 6b2c   attention_mask,
-0002a1e0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0002a1f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a1d0: 7061 7261 6d5f 696e 6974 5f74 7970 653d  param_init_type=
+0002a1e0: 7061 7261 6d5f 696e 6974 5f74 7970 652c  param_init_type,
+0002a1f0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
 0002a200: 2020 2020 2020 2020 2020 2020 2020 2020                  
 0002a210: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002a220: 2020 2069 6e69 745f 7265 7365 742c 0a20     init_reset,. 
-0002a230: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a220: 2075 7365 5f70 6173 743d 7573 655f 7061   use_past=use_pa
+0002a230: 7374 2c0a 2020 2020 2020 2020 2020 2020  st,.            
 0002a240: 2020 2020 2020 2020 2020 2020 2020 2020                  
 0002a250: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002a260: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002a270: 2062 6174 6368 5f76 616c 6964 5f6c 656e   batch_valid_len
-0002a280: 6774 6829 0a20 2020 2020 2020 2020 2020  gth).           
-0002a290: 2020 2020 2070 7265 7365 6e74 5f6c 6179       present_lay
-0002a2a0: 6572 203d 2070 7265 7365 6e74 5f6c 6179  er = present_lay
-0002a2b0: 6572 202b 2028 7072 6573 656e 742c 290a  er + (present,).
-0002a2c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002a2d0: 6163 6375 6d5f 6c6f 7373 203d 2073 656c  accum_loss = sel
-0002a2e0: 662e 6164 6428 6163 6375 6d5f 6c6f 7373  f.add(accum_loss
-0002a2f0: 2c20 6175 785f 6c6f 7373 290a 2020 2020  , aux_loss).    
-0002a300: 2020 2020 2020 2020 7265 7475 726e 2068          return h
-0002a310: 6964 6465 6e5f 7374 6174 6573 2c20 7072  idden_states, pr
-0002a320: 6573 656e 745f 6c61 7965 722c 2061 6363  esent_layer, acc
-0002a330: 756d 5f6c 6f73 730a 0a20 2020 2020 2020  um_loss..       
-0002a340: 2066 6f72 2069 2069 6e20 7261 6e67 6528   for i in range(
-0002a350: 7365 6c66 2e6e 756d 5f6c 6179 6572 7329  self.num_layers)
-0002a360: 3a0a 2020 2020 2020 2020 2020 2020 6869  :.            hi
-0002a370: 6464 656e 5f73 7461 7465 732c 2070 7265  dden_states, pre
-0002a380: 7365 6e74 203d 2073 656c 662e 626c 6f63  sent = self.bloc
-0002a390: 6b73 5b69 5d28 6869 6464 656e 5f73 7461  ks[i](hidden_sta
-0002a3a0: 7465 732c 0a20 2020 2020 2020 2020 2020  tes,.           
-0002a3b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a260: 2020 2020 6d6f 655f 636f 6e66 6967 3d6d      moe_config=m
+0002a270: 6f65 5f63 6f6e 6669 672c 0a20 2020 2020  oe_config,.     
+0002a280: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a290: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a2a0: 2020 2020 2020 2020 2020 2070 6172 616c             paral
+0002a2b0: 6c65 6c5f 636f 6e66 6967 3d63 6f6e 6669  lel_config=confi
+0002a2c0: 675f 746f 5f6c 6179 6572 290a 2020 2020  g_to_layer).    
+0002a2d0: 2020 2020 2020 2020 2020 2020 2320 4966              # If
+0002a2e0: 2074 6865 2075 7365 7220 646f 6573 6e27   the user doesn'
+0002a2f0: 7420 7061 7373 2074 6865 2066 7573 696f  t pass the fusio
+0002a300: 6e20 6675 6e63 7469 6f6e 2c20 7573 6520  n function, use 
+0002a310: 7468 6520 6465 6661 756c 7420 6f6e 650a  the default one.
+0002a320: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a330: 6966 206e 6f74 206c 616d 6264 615f 6675  if not lambda_fu
+0002a340: 6e63 3a0a 2020 2020 2020 2020 2020 2020  nc:.            
+0002a350: 2020 2020 2020 2020 6c61 6d62 6461 5f66          lambda_f
+0002a360: 756e 6320 3d20 5f67 6574 5f6c 616d 6264  unc = _get_lambd
+0002a370: 615f 6675 6e63 2829 0a0a 2020 2020 2020  a_func()..      
+0002a380: 2020 2020 2020 2020 2020 6c61 6d62 6461            lambda
+0002a390: 5f66 756e 6328 626c 6f63 6b2c 206c 6179  _func(block, lay
+0002a3a0: 6572 5f69 643d 692c 206c 6179 6572 733d  er_id=i, layers=
+0002a3b0: 6e75 6d5f 6c61 7965 7273 2c0a 2020 2020  num_layers,.    
 0002a3c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002a3d0: 2020 2020 2020 2020 2061 7474 656e 7469           attenti
-0002a3e0: 6f6e 5f6d 6173 6b2c 0a20 2020 2020 2020  on_mask,.       
-0002a3f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002a400: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002a410: 2020 2020 2020 2020 2020 2020 2069 6e69               ini
-0002a420: 745f 7265 7365 742c 0a20 2020 2020 2020  t_reset,.       
-0002a430: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002a440: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002a450: 2020 2020 2020 2020 2020 2020 2062 6174               bat
-0002a460: 6368 5f76 616c 6964 5f6c 656e 6774 6829  ch_valid_length)
-0002a470: 0a20 2020 2020 2020 2020 2020 2070 7265  .            pre
-0002a480: 7365 6e74 5f6c 6179 6572 203d 2070 7265  sent_layer = pre
-0002a490: 7365 6e74 5f6c 6179 6572 202b 2028 7072  sent_layer + (pr
-0002a4a0: 6573 656e 742c 290a 0a20 2020 2020 2020  esent,)..       
-0002a4b0: 2072 6574 7572 6e20 6869 6464 656e 5f73   return hidden_s
-0002a4c0: 7461 7465 732c 2070 7265 7365 6e74 5f6c  tates, present_l
-0002a4d0: 6179 6572 0a0a 0a63 6c61 7373 2054 7261  ayer...class Tra
-0002a4e0: 6e73 666f 726d 6572 4465 636f 6465 7228  nsformerDecoder(
-0002a4f0: 4365 6c6c 293a 0a20 2020 2072 2222 220a  Cell):.    r""".
-0002a500: 2020 2020 2020 2020 5472 616e 7366 6f72          Transfor
-0002a510: 6d65 7220 4465 636f 6465 7220 6d6f 6475  mer Decoder modu
-0002a520: 6c65 2077 6974 6820 6d75 6c74 692d 6c61  le with multi-la
-0002a530: 7965 7220 7374 6163 6b65 6420 6f66 2060  yer stacked of `
-0002a540: 5472 616e 7366 6f72 6d65 7244 6563 6f64  TransformerDecod
-0002a550: 6572 4c61 7965 7260 2c20 696e 636c 7564  erLayer`, includ
-0002a560: 696e 6720 6d75 6c74 6968 6561 6420 7365  ing multihead se
-0002a570: 6c66 0a20 2020 2020 2020 2061 7474 656e  lf.        atten
-0002a580: 7469 6f6e 2c20 6372 6f73 7320 6174 7465  tion, cross atte
-0002a590: 6e74 696f 6e20 616e 6420 6665 6564 666f  ntion and feedfo
-0002a5a0: 7277 6172 6420 6c61 7965 722e 0a0a 2020  rward layer...  
-0002a5b0: 2020 2020 2020 4172 6773 3a0a 2020 2020        Args:.    
-0002a5c0: 2020 2020 2020 2020 6e75 6d5f 6c61 7965          num_laye
-0002a5d0: 7273 2869 6e74 293a 2054 6865 206c 6179  rs(int): The lay
-0002a5e0: 6572 7320 6f66 2074 6865 2060 5472 616e  ers of the `Tran
-0002a5f0: 7366 6f72 6d65 7244 6563 6f64 6572 4c61  sformerDecoderLa
-0002a600: 7965 7260 2e0a 2020 2020 2020 2020 2020  yer`..          
-0002a610: 2020 6261 7463 685f 7369 7a65 2869 6e74    batch_size(int
-0002a620: 293a 2054 6865 2062 6174 6368 2073 697a  ): The batch siz
-0002a630: 6520 6f66 2074 6865 2069 6e70 7574 2074  e of the input t
-0002a640: 656e 736f 7220 7768 656e 2064 6f20 696e  ensor when do in
-0002a650: 6372 656e 6d65 6e74 616c 2070 7265 6469  crenmental predi
-0002a660: 6374 696f 6e2e 2053 686f 756c 6420 6265  ction. Should be
-0002a670: 2061 2070 6f73 6974 6976 650a 2020 2020   a positive.    
-0002a680: 2020 2020 2020 2020 2020 2020 7661 6c75              valu
-0002a690: 652e 2057 6865 6e20 646f 2074 7261 696e  e. When do train
-0002a6a0: 696e 6720 6f72 2070 7265 6469 6374 696f  ing or predictio
-0002a6b0: 6e2c 2074 6865 2061 7267 756d 656e 7420  n, the argument 
-0002a6c0: 7769 6c6c 206e 6f74 2077 6f72 6b20 616e  will not work an
-0002a6d0: 6420 7468 6520 7573 6572 2063 616e 206a  d the user can j
-0002a6e0: 7573 7420 7061 7373 204e 6f6e 6520 746f  ust pass None to
-0002a6f0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0002a700: 2074 6865 2061 7267 756d 656e 742e 0a20   the argument.. 
-0002a710: 2020 2020 2020 2020 2020 2068 6964 6465             hidde
-0002a720: 6e5f 7369 7a65 2869 6e74 293a 2054 6865  n_size(int): The
-0002a730: 2068 6964 6465 6e20 7369 7a65 206f 6620   hidden size of 
-0002a740: 7468 6520 696e 7075 742e 0a20 2020 2020  the input..     
-0002a750: 2020 2020 2020 2066 666e 5f68 6964 6465         ffn_hidde
-0002a760: 6e5f 7369 7a65 2869 6e74 293a 2054 6865  n_size(int): The
-0002a770: 2068 6964 6465 6e20 7369 7a65 206f 6620   hidden size of 
-0002a780: 626f 7474 6c65 6e65 636b 2069 6e20 7468  bottleneck in th
-0002a790: 6520 6665 6564 666f 7277 6172 6420 6c61  e feedforward la
-0002a7a0: 7965 722e 0a20 2020 2020 2020 2020 2020  yer..           
-0002a7b0: 2073 7263 5f73 6571 5f6c 656e 6774 6828   src_seq_length(
-0002a7c0: 696e 7429 3a20 5468 6520 696e 7075 7420  int): The input 
-0002a7d0: 736f 7572 6365 2073 6571 7565 6e63 6520  source sequence 
-0002a7e0: 6c65 6e67 7468 2e0a 2020 2020 2020 2020  length..        
-0002a7f0: 2020 2020 7467 745f 7365 715f 6c65 6e67      tgt_seq_leng
-0002a800: 7468 2869 6e74 293a 2054 6865 2069 6e70  th(int): The inp
-0002a810: 7574 2074 6172 6765 7420 7365 7175 656e  ut target sequen
-0002a820: 6365 206c 656e 6774 682e 0a20 2020 2020  ce length..     
-0002a830: 2020 2020 2020 206e 756d 5f68 6561 6473         num_heads
-0002a840: 2869 6e74 293a 2054 6865 206e 756d 6265  (int): The numbe
-0002a850: 7220 6f66 2074 6865 2068 6561 6473 2e0a  r of the heads..
-0002a860: 2020 2020 2020 2020 2020 2020 6174 7465              atte
-0002a870: 6e74 696f 6e5f 6472 6f70 6f75 745f 7261  ntion_dropout_ra
-0002a880: 7465 2866 6c6f 6174 293a 2054 6865 2064  te(float): The d
-0002a890: 726f 706f 7574 2072 6174 6520 6f66 2074  ropout rate of t
-0002a8a0: 6865 2061 7474 656e 7469 6f6e 2073 636f  he attention sco
-0002a8b0: 7265 732e 2044 6566 6175 6c74 3a30 2e31  res. Default:0.1
-0002a8c0: 2e0a 2020 2020 2020 2020 2020 2020 6869  ..            hi
-0002a8d0: 6464 656e 5f64 726f 706f 7574 5f72 6174  dden_dropout_rat
-0002a8e0: 6528 666c 6f61 7429 3a20 5468 6520 6472  e(float): The dr
-0002a8f0: 6f70 6f75 7420 7261 7465 206f 6620 7468  opout rate of th
-0002a900: 6520 6669 6e61 6c20 6f75 7470 7574 206f  e final output o
-0002a910: 6620 7468 6520 6c61 7965 722e 2044 6566  f the layer. Def
-0002a920: 6175 6c74 3a30 2e31 2e0a 2020 2020 2020  ault:0.1..      
-0002a930: 2020 2020 2020 706f 7374 5f6c 6179 6572        post_layer
-0002a940: 6e6f 726d 5f72 6573 6964 7561 6c28 626f  norm_residual(bo
-0002a950: 6f6c 293a 2044 6f20 7265 7369 6475 616c  ol): Do residual
-0002a960: 7320 6164 6473 2062 6566 6f72 6520 7468  s adds before th
-0002a970: 6520 6c61 7965 726e 6f72 6d2e 2044 6566  e layernorm. Def
-0002a980: 6175 6c74 2046 616c 7365 2e0a 2020 2020  ault False..    
-0002a990: 2020 2020 2020 2020 6c61 7965 726e 6f72          layernor
-0002a9a0: 6d5f 636f 6d70 7574 655f 7479 7065 2864  m_compute_type(d
-0002a9b0: 7479 7065 2e4e 756d 6265 7229 3a20 5468  type.Number): Th
-0002a9c0: 6520 636f 6d70 7574 6174 696f 6e20 7479  e computation ty
-0002a9d0: 7065 206f 6620 7468 6520 6c61 7965 726e  pe of the layern
-0002a9e0: 6f72 6d2e 0a20 2020 2020 2020 2020 2020  orm..           
-0002a9f0: 2020 2020 2053 686f 756c 6420 6265 206d       Should be m
-0002aa00: 7374 7970 652e 666c 6f61 7433 3220 6f72  stype.float32 or
-0002aa10: 206d 7374 7970 652e 666c 6f61 7431 362e   mstype.float16.
-0002aa20: 2044 6566 6175 6c74 206d 7374 7970 652e   Default mstype.
-0002aa30: 666c 6f61 7433 322e 0a20 2020 2020 2020  float32..       
-0002aa40: 2020 2020 2073 6f66 746d 6178 5f63 6f6d       softmax_com
-0002aa50: 7075 7465 5f74 7970 6528 6474 7970 652e  pute_type(dtype.
-0002aa60: 4e75 6d62 6572 293a 2054 6865 2063 6f6d  Number): The com
-0002aa70: 7075 7461 7469 6f6e 2074 7970 6520 6f66  putation type of
-0002aa80: 2074 6865 2073 6f66 746d 6178 2069 6e20   the softmax in 
-0002aa90: 7468 6520 6174 7465 6e74 696f 6e2e 0a20  the attention.. 
-0002aaa0: 2020 2020 2020 2020 2020 2020 2020 2053                 S
-0002aab0: 686f 756c 6420 6265 206d 7374 7970 652e  hould be mstype.
-0002aac0: 666c 6f61 7433 3220 6f72 206d 7374 7970  float32 or mstyp
-0002aad0: 652e 666c 6f61 7431 362e 2044 6566 6175  e.float16. Defau
-0002aae0: 6c74 206d 7374 7970 652e 666c 6f61 7433  lt mstype.float3
-0002aaf0: 322e 0a20 2020 2020 2020 2020 2020 2070  2..            p
-0002ab00: 6172 616d 5f69 6e69 745f 7479 7065 2864  aram_init_type(d
-0002ab10: 7479 7065 2e4e 756d 6265 7229 3a20 5468  type.Number): Th
-0002ab20: 6520 7061 7261 6d65 7465 7220 696e 6974  e parameter init
-0002ab30: 6961 6c69 7a61 7469 6f6e 2074 7970 6520  ialization type 
-0002ab40: 6f66 2074 6865 206d 6f64 756c 652e 0a20  of the module.. 
-0002ab50: 2020 2020 2020 2020 2020 2020 2020 2053                 S
-0002ab60: 686f 756c 6420 6265 206d 7374 7970 652e  hould be mstype.
-0002ab70: 666c 6f61 7433 3220 6f72 206d 7374 7970  float32 or mstyp
-0002ab80: 652e 666c 6f61 7431 362e 2044 6566 6175  e.float16. Defau
-0002ab90: 6c74 206d 7374 7970 652e 666c 6f61 7433  lt mstype.float3
-0002aba0: 322e 0a20 2020 2020 2020 2020 2020 2068  2..            h
-0002abb0: 6964 6465 6e5f 6163 7420 2873 7472 2c20  idden_act (str, 
-0002abc0: 6e6e 2e43 656c 6c29 3a20 5468 6520 6163  nn.Cell): The ac
-0002abd0: 7469 7661 7469 6f6e 206f 6620 7468 6520  tivation of the 
-0002abe0: 696e 7465 726e 616c 2066 6565 6466 6f72  internal feedfor
-0002abf0: 7761 7264 206c 6179 6572 2e20 5375 7070  ward layer. Supp
-0002ac00: 6f72 7473 2027 7265 6c75 272c 0a20 2020  orts 'relu',.   
-0002ac10: 2020 2020 2020 2020 2020 2020 2027 7265               're
-0002ac20: 6c75 3627 2c20 2774 616e 6827 2c20 2767  lu6', 'tanh', 'g
-0002ac30: 656c 7527 2c20 2766 6173 745f 6765 6c75  elu', 'fast_gelu
-0002ac40: 272c 2027 656c 7527 2c20 2773 6967 6d6f  ', 'elu', 'sigmo
-0002ac50: 6964 272c 2027 7072 656c 7527 2c20 276c  id', 'prelu', 'l
-0002ac60: 6561 6b79 7265 6c75 272c 2027 6873 7769  eakyrelu', 'hswi
-0002ac70: 7368 272c 0a20 2020 2020 2020 2020 2020  sh',.           
-0002ac80: 2020 2020 2027 6873 6967 6d6f 6964 272c       'hsigmoid',
-0002ac90: 2027 6c6f 6773 6967 6d6f 6964 2720 616e   'logsigmoid' an
-0002aca0: 6420 736f 206f 6e2e 2055 7365 7220 6361  d so on. User ca
-0002acb0: 6e20 7072 6f76 6964 6520 6375 7374 6f6d  n provide custom
-0002acc0: 2061 6374 6976 6974 696f 6e20 746f 2074   activition to t
-0002acd0: 6865 2061 7267 756d 656e 742e 0a20 2020  he argument..   
-0002ace0: 2020 2020 2020 2020 2020 2020 2049 6620               If 
-0002acf0: 7573 6572 2077 616e 7473 2074 6f20 7275  user wants to ru
-0002ad00: 6e20 7468 6520 6e65 7420 696e 2074 6865  n the net in the
-0002ad10: 2070 6172 616c 6c65 6c20 6d6f 6465 2c20   parallel mode, 
-0002ad20: 7468 6520 6375 7374 6f6d 2061 6374 6976  the custom activ
-0002ad30: 6174 696f 6e20 6d75 7374 2061 6c73 6f20  ation must also 
-0002ad40: 7072 6f76 6964 650a 2020 2020 2020 2020  provide.        
-0002ad50: 2020 2020 2020 2020 7468 6520 6061 6374          the `act
-0002ad60: 6976 6174 696f 6e5f 7368 6172 6460 2066  ivation_shard` f
-0002ad70: 756e 6374 696f 6e2e 2050 6c65 6173 6520  unction. Please 
-0002ad80: 7365 6520 7468 6520 6578 616d 706c 6573  see the examples
-0002ad90: 206f 6620 7468 650a 2020 2020 2020 2020   of the.        
-0002ada0: 2020 2020 2020 2020 636c 6173 733a 606d          class:`m
-0002adb0: 696e 6466 6f72 6d65 7273 2e6d 6f64 756c  indformers.modul
-0002adc0: 6573 2e74 7261 6e73 666f 726d 6572 2e46  es.transformer.F
-0002add0: 6565 6446 6f72 7761 7264 602e 2044 6566  eedForward`. Def
-0002ade0: 6175 6c74 3a20 6765 6c75 2e0a 2020 2020  ault: gelu..    
-0002adf0: 2020 2020 2020 2020 6c61 6d62 6461 5f66          lambda_f
-0002ae00: 756e 6328 6675 6e63 7469 6f6e 293a 2041  unc(function): A
-0002ae10: 2066 756e 6374 696f 6e20 6361 6e20 6465   function can de
-0002ae20: 7465 726d 696e 6520 7468 6520 6675 7369  termine the fusi
-0002ae30: 6f6e 2069 6e64 6578 2c0a 2020 2020 2020  on index,.      
-0002ae40: 2020 2020 2020 2020 2020 7069 7065 6c69            pipeli
-0002ae50: 6e65 2073 7461 6765 7320 616e 6420 7265  ne stages and re
-0002ae60: 636f 6d70 7574 6520 6174 7472 6962 7574  compute attribut
-0002ae70: 652e 2049 6620 7468 650a 2020 2020 2020  e. If the.      
-0002ae80: 2020 2020 2020 2020 2020 7573 6572 2077            user w
-0002ae90: 616e 7473 2074 6f20 6465 7465 726d 696e  ants to determin
-0002aea0: 6520 7468 6520 7069 7065 6c69 6e65 2073  e the pipeline s
-0002aeb0: 7461 6765 2061 6e64 2067 7261 6469 656e  tage and gradien
-0002aec0: 7420 6167 6772 6567 6174 696f 6e20 6675  t aggregation fu
-0002aed0: 7369 6f6e 2c20 7468 6520 7573 6572 2063  sion, the user c
-0002aee0: 616e 2070 6173 7320 610a 2020 2020 2020  an pass a.      
-0002aef0: 2020 2020 2020 2020 2020 6675 6e63 7469            functi
-0002af00: 6f6e 2074 6861 7420 6163 6365 7074 7320  on that accepts 
-0002af10: 606e 6574 776f 726b 602c 2060 6c61 7965  `network`, `laye
-0002af20: 725f 6964 602c 2060 6f66 6673 6574 602c  r_id`, `offset`,
-0002af30: 2060 7061 7261 6c6c 656c 5f63 6f6e 6669   `parallel_confi
-0002af40: 6760 2c20 606c 6179 6572 7360 2e20 5468  g`, `layers`. Th
-0002af50: 6520 606e 6574 776f 726b 2843 656c 6c29  e `network(Cell)
-0002af60: 600a 2020 2020 2020 2020 2020 2020 2020  `.              
-0002af70: 2020 7265 7072 6573 656e 7473 2074 6865    represents the
-0002af80: 2074 7261 6e73 666f 726d 6572 2062 6c6f   transformer blo
-0002af90: 636b 2c20 606c 6179 6572 5f69 6428 696e  ck, `layer_id(in
-0002afa0: 7429 6020 6d65 616e 7320 7468 6520 6c61  t)` means the la
-0002afb0: 7965 7220 696e 6465 7820 666f 7220 7468  yer index for th
-0002afc0: 6520 6375 7272 656e 7420 6d6f 6475 6c65  e current module
-0002afd0: 2c20 636f 756e 7473 0a20 2020 2020 2020  , counts.       
-0002afe0: 2020 2020 2020 2020 2066 726f 6d20 7a65           from ze
-0002aff0: 726f 2c20 606f 6666 7365 7428 696e 7429  ro, `offset(int)
-0002b000: 6020 6d65 616e 7320 7468 6520 6c61 7965  ` means the laye
-0002b010: 725f 696e 6465 7820 6e65 6564 7320 616e  r_index needs an
-0002b020: 206f 6666 7365 742c 2069 6620 7468 6572   offset, if ther
-0002b030: 6520 6172 6520 6f74 6865 7220 6d6f 6475  e are other modu
-0002b040: 6c65 7320 696e 2074 6865 206e 6574 2e0a  les in the net..
-0002b050: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002b060: 5468 6520 6465 6661 756c 7420 7365 7474  The default sett
-0002b070: 696e 6720 666f 7220 7468 6520 7069 7065  ing for the pipe
-0002b080: 6c69 6e65 2069 733a 2060 286c 6179 6572  line is: `(layer
-0002b090: 5f69 6420 2b20 6f66 6673 6574 2920 2f2f  _id + offset) //
-0002b0a0: 2028 6c61 7965 7273 202f 2070 6970 656c   (layers / pipel
-0002b0b0: 696e 655f 7374 6167 6529 602e 0a20 2020  ine_stage)`..   
-0002b0c0: 2020 2020 2020 2020 2020 2020 2044 6566               Def
-0002b0d0: 6175 6c74 3a20 4e6f 6e65 2e0a 2020 2020  ault: None..    
-0002b0e0: 2020 2020 2020 2020 7573 655f 7061 7374          use_past
-0002b0f0: 2862 6f6f 6c29 3a20 5573 6520 7468 6520  (bool): Use the 
-0002b100: 7061 7374 2073 7461 7465 2074 6f20 636f  past state to co
-0002b110: 6d70 7574 652c 2075 7365 6420 666f 7220  mpute, used for 
-0002b120: 696e 6372 656d 656e 7461 6c20 7072 6564  incremental pred
-0002b130: 6963 7469 6f6e 2e20 4465 6661 756c 7420  iction. Default 
-0002b140: 4661 6c73 652e 0a20 2020 2020 2020 2020  False..         
-0002b150: 2020 206f 6666 7365 7428 696e 7429 3a20     offset(int): 
-0002b160: 5468 6520 696e 6974 6961 6c20 6c61 7965  The initial laye
-0002b170: 7220 696e 6465 7820 666f 7220 7468 6520  r index for the 
-0002b180: 6064 6563 6f64 6572 602e 2055 7365 6420  `decoder`. Used 
-0002b190: 666f 7220 7365 7474 696e 6720 7468 6520  for setting the 
-0002b1a0: 6675 7369 6f6e 2069 6420 616e 6420 7374  fusion id and st
-0002b1b0: 6167 6520 6964 2c20 746f 206e 6f74 0a20  age id, to not. 
-0002b1c0: 2020 2020 2020 2020 2020 2020 2020 206f                 o
-0002b1d0: 7665 726c 6170 2077 6974 6820 7468 6520  verlap with the 
-0002b1e0: 656e 636f 6465 7220 6c61 7965 722e 2044  encoder layer. D
-0002b1f0: 6566 6175 6c74 2030 2e0a 2020 2020 2020  efault 0..      
-0002b200: 2020 2020 2020 6d6f 655f 636f 6e66 6967        moe_config
-0002b210: 284d 6f45 436f 6e66 6967 293a 2054 6865  (MoEConfig): The
-0002b220: 2063 6f6e 6669 6775 7261 7469 6f6e 206f   configuration o
-0002b230: 6620 4d6f 4520 284d 6978 7475 7265 206f  f MoE (Mixture o
-0002b240: 6620 4578 7065 7274 292e 2044 6566 6175  f Expert). Defau
-0002b250: 6c74 2069 7320 616e 2069 6e73 7461 6e63  lt is an instanc
-0002b260: 6520 6f66 204d 6f45 436f 6e66 6967 0a20  e of MoEConfig. 
-0002b270: 2020 2020 2020 2020 2020 2020 2020 2077                 w
-0002b280: 6974 6820 6465 6661 756c 7420 7661 6c75  ith default valu
-0002b290: 6573 2e20 506c 6561 7365 2073 6565 2060  es. Please see `
-0002b2a0: 4d6f 4543 6f6e 6669 6760 2e0a 2020 2020  MoEConfig`..    
-0002b2b0: 2020 2020 2020 2020 7061 7261 6c6c 656c          parallel
-0002b2c0: 5f63 6f6e 6669 6728 5472 616e 7366 6f72  _config(Transfor
-0002b2d0: 6d65 724f 7050 6172 616c 6c65 6c43 6f6e  merOpParallelCon
-0002b2e0: 6669 6729 3a20 5468 6520 7061 7261 6c6c  fig): The parall
-0002b2f0: 656c 2063 6f6e 6669 6775 7265 2e20 4465  el configure. De
-0002b300: 6661 756c 7420 6064 6566 6175 6c74 5f74  fault `default_t
-0002b310: 7261 6e73 666f 726d 6572 5f63 6f6e 6669  ransformer_confi
-0002b320: 6760 2c0a 2020 2020 2020 2020 2020 2020  g`,.            
-0002b330: 2020 2020 616e 2069 6e73 7461 6e63 6520      an instance 
-0002b340: 6f66 2060 5472 616e 7366 6f72 6d65 724f  of `TransformerO
-0002b350: 7050 6172 616c 6c65 6c43 6f6e 6669 6760  pParallelConfig`
-0002b360: 2077 6974 6820 6465 6661 756c 7420 6172   with default ar
-0002b370: 6773 2e0a 0a20 2020 2020 2020 2049 6e70  gs...        Inp
-0002b380: 7574 733a 0a20 2020 2020 2020 2020 2020  uts:.           
-0002b390: 202d 202a 2a68 6964 6465 6e5f 7374 6174   - **hidden_stat
-0002b3a0: 732a 2a20 2854 656e 736f 7229 202d 2054  s** (Tensor) - T
-0002b3b0: 6865 2069 6e70 7574 2074 656e 736f 7220  he input tensor 
-0002b3c0: 7769 7468 2073 6861 7065 205b 6261 7463  with shape [batc
-0002b3d0: 685f 7369 7a65 2c20 7365 715f 6c65 6e67  h_size, seq_leng
-0002b3e0: 7468 2c20 6869 6464 656e 5f73 697a 655d  th, hidden_size]
-0002b3f0: 206f 720a 2020 2020 2020 2020 2020 2020   or.            
-0002b400: 2020 5b62 6174 6368 5f73 697a 6520 2a20    [batch_size * 
-0002b410: 7365 715f 6c65 6e67 7468 2c20 6869 6464  seq_length, hidd
-0002b420: 656e 5f73 697a 655d 0a20 2020 2020 2020  en_size].       
-0002b430: 2020 2020 202d 202a 2a61 7474 656e 7469       - **attenti
-0002b440: 6f6e 5f6d 6173 6b2a 2a20 2854 656e 736f  on_mask** (Tenso
-0002b450: 7229 202d 2054 6865 2061 7474 656e 7469  r) - The attenti
-0002b460: 6f6e 206d 6173 6b20 666f 7220 6465 636f  on mask for deco
-0002b470: 6465 7220 7769 7468 2073 6861 7065 0a20  der with shape. 
-0002b480: 2020 2020 2020 2020 2020 2020 205b 6261               [ba
-0002b490: 7463 685f 7369 7a65 2c20 7365 715f 6c65  tch_size, seq_le
-0002b4a0: 6e67 7468 2c20 7365 715f 6c65 6e67 7468  ngth, seq_length
-0002b4b0: 5d20 6f72 204e 6f6e 652e 204e 6f6e 6520  ] or None. None 
-0002b4c0: 6d65 616e 7320 7468 6572 6520 7769 6c6c  means there will
-0002b4d0: 2062 6520 6e6f 206d 6173 6b20 696e 2073   be no mask in s
-0002b4e0: 6f66 746d 6178 0a20 2020 2020 2020 2020  oftmax.         
-0002b4f0: 2020 2020 2063 6f6d 7075 7461 7469 6f6e       computation
-0002b500: 2069 6e20 7365 6c66 2061 7474 656e 7469   in self attenti
-0002b510: 6f6e 2e0a 2020 2020 2020 2020 2020 2020  on..            
-0002b520: 2d20 2a2a 656e 636f 6465 725f 6f75 7470  - **encoder_outp
-0002b530: 7574 2a2a 2028 5465 6e73 6f72 2920 2d20  ut** (Tensor) - 
-0002b540: 5468 6520 6f75 7470 7574 206f 6620 7468  The output of th
-0002b550: 6520 656e 636f 6465 7220 7769 7468 2073  e encoder with s
-0002b560: 6861 7065 205b 6261 7463 685f 7369 7a65  hape [batch_size
-0002b570: 2c20 7365 715f 6c65 6e67 7468 2c20 6869  , seq_length, hi
-0002b580: 6464 656e 5f73 697a 655d 0a20 2020 2020  dden_size].     
-0002b590: 2020 2020 2020 2020 206f 7220 5b62 6174           or [bat
-0002b5a0: 6368 5f73 697a 6520 2a20 7365 715f 6c65  ch_size * seq_le
-0002b5b0: 6e67 7468 2c20 6869 6464 656e 5f73 697a  ngth, hidden_siz
-0002b5c0: 655d 2e20 4e6f 7465 2074 6869 7320 6172  e]. Note this ar
-0002b5d0: 6773 2063 616e 206e 6f74 2062 6520 7061  gs can not be pa
-0002b5e0: 7373 6564 2062 7920 4e6f 6e65 2077 6865  ssed by None whe
-0002b5f0: 6e20 7468 6520 6e65 7420 6973 2069 6e0a  n the net is in.
-0002b600: 2020 2020 2020 2020 2020 2020 2020 6f75                ou
-0002b610: 7465 726d 6f73 7420 6c61 7965 722e 2044  termost layer. D
-0002b620: 6566 6175 6c74 204e 6f6e 652e 0a20 2020  efault None..   
-0002b630: 2020 2020 2020 2020 202d 202a 2a6d 656d           - **mem
-0002b640: 6f72 795f 6d61 736b 2a2a 2028 5465 6e73  ory_mask** (Tens
-0002b650: 6f72 2920 2d20 5468 6520 6d65 6d6f 7279  or) - The memory
-0002b660: 206d 6173 6b20 6f66 2074 6865 2063 726f   mask of the cro
-0002b670: 7373 2061 7474 656e 7469 6f6e 2077 6974  ss attention wit
-0002b680: 6820 7368 6170 6520 5b62 6174 6368 2c20  h shape [batch, 
-0002b690: 7467 745f 7365 715f 6c65 6e67 7468 2c0a  tgt_seq_length,.
-0002b6a0: 2020 2020 2020 2020 2020 2020 2020 7372                sr
-0002b6b0: 635f 7365 715f 6c65 6e67 7468 5d20 7768  c_seq_length] wh
-0002b6c0: 6572 6520 7467 745f 7365 715f 6c65 6e67  ere tgt_seq_leng
-0002b6d0: 7468 2069 7320 7468 6520 6c65 6e67 7468  th is the length
-0002b6e0: 206f 6620 7468 6520 6465 636f 6465 722e   of the decoder.
-0002b6f0: 2054 6865 2075 7365 7220 6361 6e20 616c   The user can al
-0002b700: 736f 2070 6173 7320 4e6f 6e65 2e20 4e6f  so pass None. No
-0002b710: 6e65 0a20 2020 2020 2020 2020 2020 2020  ne.             
-0002b720: 206d 6561 6e73 2074 6865 7265 2077 696c   means there wil
-0002b730: 6c20 6265 206e 6f20 6d61 736b 2069 6e20  l be no mask in 
-0002b740: 736f 6674 6d61 7820 636f 6d70 7574 6174  softmax computat
-0002b750: 696f 6e20 696e 2063 726f 7373 2061 7474  ion in cross att
-0002b760: 656e 7469 6f6e 2e20 4465 6661 756c 7420  ention. Default 
-0002b770: 4e6f 6e65 2e0a 2020 2020 2020 2020 2020  None..          
-0002b780: 2020 2d20 2a2a 696e 6974 5f72 6573 6574    - **init_reset
-0002b790: 2a2a 2028 5465 6e73 6f72 2920 2d20 4120  ** (Tensor) - A 
-0002b7a0: 626f 6f6c 2074 656e 736f 7220 7769 7468  bool tensor with
-0002b7b0: 2073 6861 7065 205b 315d 2c20 7573 6564   shape [1], used
-0002b7c0: 2074 6f20 636c 6561 7220 7468 6520 7061   to clear the pa
-0002b7d0: 7374 206b 6579 2070 6172 616d 6574 6572  st key parameter
-0002b7e0: 2061 6e64 0a20 2020 2020 2020 2020 2020   and.           
-0002b7f0: 2020 2070 6173 7420 7661 6c75 6520 7061     past value pa
-0002b800: 7261 6d65 7465 7220 7573 6564 2069 6e20  rameter used in 
-0002b810: 7468 6520 696e 6372 656d 656e 7461 6c20  the incremental 
-0002b820: 7072 6564 6963 7469 6f6e 2e20 4f6e 6c79  prediction. Only
-0002b830: 2076 616c 6964 2077 6865 6e20 7573 655f   valid when use_
-0002b840: 7061 7374 2069 7320 5472 7565 2e20 4465  past is True. De
-0002b850: 6661 756c 7420 5472 7565 2e0a 2020 2020  fault True..    
-0002b860: 2020 2020 2020 2020 2d20 2a2a 6261 7463          - **batc
-0002b870: 685f 7661 6c69 645f 6c65 6e67 7468 2a2a  h_valid_length**
-0002b880: 2028 5465 6e73 6f72 2920 2d20 496e 7433   (Tensor) - Int3
-0002b890: 3220 7465 6e73 6f72 2077 6974 6820 7368  2 tensor with sh
-0002b8a0: 6170 6520 5b62 6174 6368 5f73 697a 655d  ape [batch_size]
-0002b8b0: 2074 6865 2070 6173 7420 6361 6c63 756c   the past calcul
-0002b8c0: 6174 6564 2074 6865 2069 6e64 6578 2e0a  ated the index..
-0002b8d0: 2020 2020 2020 2020 2020 2020 2020 5573                Us
-0002b8e0: 6564 2066 6f72 2069 6e63 7265 6d65 6e74  ed for increment
-0002b8f0: 616c 2070 7265 6469 6374 696f 6e20 7768  al prediction wh
-0002b900: 656e 2074 6865 2075 7365 5f70 6173 7420  en the use_past 
-0002b910: 6973 2054 7275 652e 2044 6566 6175 6c74  is True. Default
-0002b920: 204e 6f6e 652e 0a0a 2020 2020 2020 2020   None...        
-0002b930: 4f75 7470 7574 733a 0a20 2020 2020 2020  Outputs:.       
-0002b940: 2020 2020 2054 7570 6c65 2c20 6120 7475       Tuple, a tu
-0002b950: 706c 6520 636f 6e74 6169 6e73 2860 6f75  ple contains(`ou
-0002b960: 7470 7574 602c 2060 6c61 7965 725f 7072  tput`, `layer_pr
-0002b970: 6573 656e 7460 290a 0a20 2020 2020 2020  esent`)..       
-0002b980: 2020 2020 202d 202a 2a6f 7574 7075 742a       - **output*
-0002b990: 2a20 2854 656e 736f 7229 202d 2054 6865  * (Tensor) - The
-0002b9a0: 206f 7574 7075 7420 6c6f 6769 7420 6f66   output logit of
-0002b9b0: 2074 6869 7320 6c61 7965 722e 2054 6865   this layer. The
-0002b9c0: 2073 6861 7065 2069 7320 5b62 6174 6368   shape is [batch
-0002b9d0: 2c20 7467 745f 7365 715f 6c65 6e67 7468  , tgt_seq_length
-0002b9e0: 2c20 6869 6464 656e 5f73 697a 655d 206f  , hidden_size] o
-0002b9f0: 720a 2020 2020 2020 2020 2020 2020 2020  r.              
-0002ba00: 5b62 6174 6368 202a 2074 6774 5f73 6571  [batch * tgt_seq
-0002ba10: 5f6c 656e 6774 682c 2068 6964 6465 6e5f  _length, hidden_
-0002ba20: 7369 7a65 5d0a 2020 2020 2020 2020 2020  size].          
-0002ba30: 2020 2d20 2a2a 6c61 7965 725f 7072 6573    - **layer_pres
-0002ba40: 656e 742a 2a20 2854 7570 6c65 2920 2d20  ent** (Tuple) - 
-0002ba50: 4120 7475 706c 6520 7769 7468 2073 697a  A tuple with siz
-0002ba60: 6520 6f66 206e 756d 5f6c 6179 6572 732c  e of num_layers,
-0002ba70: 2077 6865 7265 2065 6163 6820 7475 706c   where each tupl
-0002ba80: 6520 6973 2074 6865 2074 656e 736f 7220  e is the tensor 
-0002ba90: 6f66 2074 6865 0a20 2020 2020 2020 2020  of the.         
-0002baa0: 2020 2020 2070 726f 6a65 6374 6564 206b       projected k
-0002bab0: 6579 2061 6e64 2076 616c 7565 2076 6563  ey and value vec
-0002bac0: 746f 7220 696e 2073 656c 6620 6174 7465  tor in self atte
-0002bad0: 6e74 696f 6e20 7769 7468 2073 6861 7065  ntion with shape
-0002bae0: 2028 2862 6174 6368 5f73 697a 652c 206e   ((batch_size, n
-0002baf0: 756d 5f68 6561 6473 2c20 7369 7a65 5f70  um_heads, size_p
-0002bb00: 6572 5f68 6561 642c 0a20 2020 2020 2020  er_head,.       
-0002bb10: 2020 2020 2020 2074 6774 5f73 6571 5f6c         tgt_seq_l
-0002bb20: 656e 6774 6829 2c20 2862 6174 6368 5f73  ength), (batch_s
-0002bb30: 697a 652c 206e 756d 5f68 6561 6473 2c20  ize, num_heads, 
-0002bb40: 7467 745f 7365 715f 6c65 6e67 7468 2c20  tgt_seq_length, 
-0002bb50: 7369 7a65 5f70 6572 5f68 6561 6429 2c20  size_per_head), 
-0002bb60: 616e 6420 6f66 2074 6865 2070 726f 6a65  and of the proje
-0002bb70: 6374 6564 206b 6579 0a20 2020 2020 2020  cted key.       
-0002bb80: 2020 2020 2020 2061 6e64 2076 616c 7565         and value
-0002bb90: 2076 6563 746f 7220 696e 2063 726f 7373   vector in cross
-0002bba0: 2061 7474 656e 7469 6f6e 2077 6974 6820   attention with 
-0002bbb0: 7368 6170 6520 2028 6261 7463 685f 7369  shape  (batch_si
-0002bbc0: 7a65 2c20 6e75 6d5f 6865 6164 732c 2073  ze, num_heads, s
-0002bbd0: 697a 655f 7065 725f 6865 6164 2c20 7372  ize_per_head, sr
-0002bbe0: 635f 7365 715f 6c65 6e67 7468 292c 0a20  c_seq_length),. 
-0002bbf0: 2020 2020 2020 2020 2020 2020 2028 6261               (ba
-0002bc00: 7463 685f 7369 7a65 2c20 6e75 6d5f 6865  tch_size, num_he
-0002bc10: 6164 732c 2073 7263 5f73 6571 5f6c 656e  ads, src_seq_len
-0002bc20: 6774 682c 2073 697a 655f 7065 725f 6865  gth, size_per_he
-0002bc30: 6164 2929 2e0a 0a20 2020 2020 2020 2053  ad))...        S
-0002bc40: 7570 706f 7274 6564 2050 6c61 7466 6f72  upported Platfor
-0002bc50: 6d73 3a0a 2020 2020 2020 2020 2020 2020  ms:.            
-0002bc60: 6060 4173 6365 6e64 6060 2060 6047 5055  ``Ascend`` ``GPU
-0002bc70: 6060 0a0a 2020 2020 2020 2020 4578 616d  ``..        Exam
-0002bc80: 706c 6573 3a0a 2020 2020 2020 2020 2020  ples:.          
-0002bc90: 2020 3e3e 3e20 696d 706f 7274 206e 756d    >>> import num
-0002bca0: 7079 2061 7320 6e70 0a20 2020 2020 2020  py as np.       
-0002bcb0: 2020 2020 203e 3e3e 2066 726f 6d20 6d69       >>> from mi
-0002bcc0: 6e64 7370 6f72 6520 696d 706f 7274 2064  ndspore import d
-0002bcd0: 7479 7065 2061 7320 6d73 7479 7065 0a20  type as mstype. 
-0002bce0: 2020 2020 2020 2020 2020 203e 3e3e 2066             >>> f
-0002bcf0: 726f 6d20 6d69 6e64 666f 726d 6572 732e  rom mindformers.
-0002bd00: 6d6f 6475 6c65 732e 7472 616e 7366 6f72  modules.transfor
-0002bd10: 6d65 7220 696d 706f 7274 2054 7261 6e73  mer import Trans
-0002bd20: 666f 726d 6572 4465 636f 6465 720a 2020  formerDecoder.  
-0002bd30: 2020 2020 2020 2020 2020 3e3e 3e20 6672            >>> fr
-0002bd40: 6f6d 206d 696e 6473 706f 7265 2069 6d70  om mindspore imp
-0002bd50: 6f72 7420 5465 6e73 6f72 0a20 2020 2020  ort Tensor.     
-0002bd60: 2020 2020 2020 203e 3e3e 206d 6f64 656c         >>> model
-0002bd70: 203d 2054 7261 6e73 666f 726d 6572 4465   = TransformerDe
-0002bd80: 636f 6465 7228 6261 7463 685f 7369 7a65  coder(batch_size
-0002bd90: 3d32 2c20 6e75 6d5f 6c61 7965 7273 3d31  =2, num_layers=1
-0002bda0: 2c20 6869 6464 656e 5f73 697a 653d 3634  , hidden_size=64
-0002bdb0: 2c20 6666 6e5f 6869 6464 656e 5f73 697a  , ffn_hidden_siz
-0002bdc0: 653d 3634 2c0a 2020 2020 2020 2020 2020  e=64,.          
-0002bdd0: 2020 2e2e 2e20 2020 2020 2020 2020 2020    ...           
-0002bde0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002bdf0: 206e 756d 5f68 6561 6473 3d32 2c20 7372   num_heads=2, sr
-0002be00: 635f 7365 715f 6c65 6e67 7468 3d32 302c  c_seq_length=20,
-0002be10: 2074 6774 5f73 6571 5f6c 656e 6774 683d   tgt_seq_length=
-0002be20: 3130 290a 2020 2020 2020 2020 2020 2020  10).            
-0002be30: 3e3e 3e20 656e 636f 6465 725f 696e 7075  >>> encoder_inpu
-0002be40: 745f 7661 6c75 6520 3d20 5465 6e73 6f72  t_value = Tensor
-0002be50: 286e 702e 6f6e 6573 2828 322c 2032 302c  (np.ones((2, 20,
-0002be60: 2036 3429 292c 206d 7374 7970 652e 666c   64)), mstype.fl
-0002be70: 6f61 7433 3229 0a20 2020 2020 2020 2020  oat32).         
-0002be80: 2020 203e 3e3e 2064 6563 6f64 6572 5f69     >>> decoder_i
-0002be90: 6e70 7574 5f76 616c 7565 203d 2054 656e  nput_value = Ten
-0002bea0: 736f 7228 6e70 2e6f 6e65 7328 2832 2c20  sor(np.ones((2, 
-0002beb0: 3130 2c20 3634 2929 2c20 6d73 7479 7065  10, 64)), mstype
-0002bec0: 2e66 6c6f 6174 3332 290a 2020 2020 2020  .float32).      
-0002bed0: 2020 2020 2020 3e3e 3e20 6465 636f 6465        >>> decode
-0002bee0: 725f 696e 7075 745f 6d61 736b 203d 2054  r_input_mask = T
-0002bef0: 656e 736f 7228 6e70 2e6f 6e65 7328 2832  ensor(np.ones((2
-0002bf00: 2c20 3130 2c20 3130 2929 2c20 6d73 7479  , 10, 10)), msty
-0002bf10: 7065 2e66 6c6f 6174 3136 290a 2020 2020  pe.float16).    
-0002bf20: 2020 2020 2020 2020 3e3e 3e20 6d65 6d6f          >>> memo
-0002bf30: 7279 5f6d 6173 6b20 3d20 5465 6e73 6f72  ry_mask = Tensor
-0002bf40: 286e 702e 6f6e 6573 2828 322c 2031 302c  (np.ones((2, 10,
-0002bf50: 2032 3029 292c 206d 7374 7970 652e 666c   20)), mstype.fl
-0002bf60: 6f61 7431 3629 0a20 2020 2020 2020 2020  oat16).         
-0002bf70: 2020 203e 3e3e 206f 7574 7075 742c 2070     >>> output, p
-0002bf80: 6173 7420 3d20 6d6f 6465 6c28 6465 636f  ast = model(deco
-0002bf90: 6465 725f 696e 7075 745f 7661 6c75 652c  der_input_value,
-0002bfa0: 2064 6563 6f64 6572 5f69 6e70 7574 5f6d   decoder_input_m
-0002bfb0: 6173 6b2c 2065 6e63 6f64 6572 5f69 6e70  ask, encoder_inp
-0002bfc0: 7574 5f76 616c 7565 2c20 6d65 6d6f 7279  ut_value, memory
-0002bfd0: 5f6d 6173 6b29 0a20 2020 2020 2020 2020  _mask).         
-0002bfe0: 2020 203e 3e3e 2070 7269 6e74 286f 7574     >>> print(out
-0002bff0: 7075 742e 7368 6170 6529 0a20 2020 2020  put.shape).     
-0002c000: 2020 2020 2020 2028 322c 2031 302c 2036         (2, 10, 6
-0002c010: 3429 0a20 2020 2020 2020 2020 2020 203e  4).            >
-0002c020: 3e3e 2070 7269 6e74 286c 656e 2870 6173  >> print(len(pas
-0002c030: 7429 290a 2020 2020 2020 2020 2020 2020  t)).            
-0002c040: 310a 2020 2020 2020 2020 2020 2020 3e3e  1.            >>
-0002c050: 3e20 7072 696e 7428 7061 7374 5b30 5d5b  > print(past[0][
-0002c060: 305d 2e73 6861 7065 290a 2020 2020 2020  0].shape).      
-0002c070: 2020 2020 2020 2832 2c20 322c 2033 322c        (2, 2, 32,
-0002c080: 2031 3029 0a20 2020 2020 2020 2020 2020   10).           
-0002c090: 203e 3e3e 2070 7269 6e74 2870 6173 745b   >>> print(past[
-0002c0a0: 305d 5b31 5d2e 7368 6170 6529 0a20 2020  0][1].shape).   
-0002c0b0: 2020 2020 2020 2020 2028 322c 2032 2c20           (2, 2, 
-0002c0c0: 3130 2c20 3332 290a 2020 2020 2020 2020  10, 32).        
-0002c0d0: 2020 2020 3e3e 3e20 7072 696e 7428 7061      >>> print(pa
-0002c0e0: 7374 5b30 5d5b 325d 2e73 6861 7065 290a  st[0][2].shape).
-0002c0f0: 2020 2020 2020 2020 2020 2020 2832 2c20              (2, 
-0002c100: 322c 2033 322c 2032 3029 0a20 2020 2020  2, 32, 20).     
-0002c110: 2020 2020 2020 203e 3e3e 2070 7269 6e74         >>> print
-0002c120: 2870 6173 745b 305d 5b33 5d2e 7368 6170  (past[0][3].shap
-0002c130: 6529 0a20 2020 2020 2020 2020 2020 2028  e).            (
-0002c140: 322c 2032 2c20 3230 2c20 3332 290a 2020  2, 2, 20, 32).  
-0002c150: 2020 2222 220a 0a20 2020 2040 5f4c 6f67    """..    @_Log
-0002c160: 4163 7469 6f6e 4f6e 6365 286d 5f6c 6f67  ActionOnce(m_log
-0002c170: 6765 723d 6c6f 6767 6572 2c20 6b65 793d  ger=logger, key=
-0002c180: 2754 7261 6e73 666f 726d 6572 4465 636f  'TransformerDeco
-0002c190: 6465 7227 2c0a 2020 2020 2020 2020 2020  der',.          
-0002c1a0: 2020 2020 2020 2020 2020 6e6f 5f77 6172            no_war
-0002c1b0: 6e69 6e67 3d5f 6765 745f 7061 7261 6c6c  ning=_get_parall
-0002c1c0: 656c 5f6d 6f64 6528 2920 696e 2028 5061  el_mode() in (Pa
-0002c1d0: 7261 6c6c 656c 4d6f 6465 2e53 5441 4e44  rallelMode.STAND
-0002c1e0: 5f41 4c4f 4e45 2c29 290a 2020 2020 405f  _ALONE,)).    @_
-0002c1f0: 6172 6773 5f74 7970 655f 7661 6c69 6461  args_type_valida
-0002c200: 746f 725f 6368 6563 6b28 6869 6464 656e  tor_check(hidden
-0002c210: 5f73 697a 653d 5661 6c69 6461 746f 722e  _size=Validator.
-0002c220: 6368 6563 6b5f 706f 7369 7469 7665 5f69  check_positive_i
-0002c230: 6e74 2c0a 2020 2020 2020 2020 2020 2020  nt,.            
-0002c240: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002c250: 2020 2020 6e75 6d5f 6865 6164 733d 5661      num_heads=Va
-0002c260: 6c69 6461 746f 722e 6368 6563 6b5f 706f  lidator.check_po
-0002c270: 7369 7469 7665 5f69 6e74 2c0a 2020 2020  sitive_int,.    
+0002a3d0: 2020 2020 2020 2020 6f66 6673 6574 3d6f          offset=o
+0002a3e0: 6666 7365 742c 2070 6172 616c 6c65 6c5f  ffset, parallel_
+0002a3f0: 636f 6e66 6967 3d70 6172 616c 6c65 6c5f  config=parallel_
+0002a400: 636f 6e66 6967 290a 2020 2020 2020 2020  config).        
+0002a410: 2020 2020 2020 2020 7365 6c66 2e62 6c6f          self.blo
+0002a420: 636b 732e 6170 7065 6e64 2862 6c6f 636b  cks.append(block
+0002a430: 290a 2020 2020 2020 2020 656c 7365 3a0a  ).        else:.
+0002a440: 2020 2020 2020 2020 2020 2020 7261 6973              rais
+0002a450: 6520 5275 6e74 696d 6545 7272 6f72 2866  e RuntimeError(f
+0002a460: 2254 6865 207b 7365 6c66 2e63 6c73 5f6e  "The {self.cls_n
+0002a470: 616d 657d 206f 6e6c 7920 7375 7070 6f72  ame} only suppor
+0002a480: 7420 7368 6172 6469 6e67 2070 726f 7061  t sharding propa
+0002a490: 6761 7469 6f6e 206f 7220 220a 2020 2020  gation or ".    
+0002a4a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a4b0: 2020 2020 2020 2020 2020 2066 2273 656d             f"sem
+0002a4c0: 692d 6175 746f 2070 6172 616c 6c65 6c20  i-auto parallel 
+0002a4d0: 6d6f 6465 206e 6f77 2e22 290a 0a20 2020  mode now.")..   
+0002a4e0: 2064 6566 2063 6f6e 7374 7275 6374 2873   def construct(s
+0002a4f0: 656c 662c 2068 6964 6465 6e5f 7374 6174  elf, hidden_stat
+0002a500: 6573 2c20 6174 7465 6e74 696f 6e5f 6d61  es, attention_ma
+0002a510: 736b 2c20 696e 6974 5f72 6573 6574 3d54  sk, init_reset=T
+0002a520: 7275 652c 2062 6174 6368 5f76 616c 6964  rue, batch_valid
+0002a530: 5f6c 656e 6774 683d 4e6f 6e65 293a 0a20  _length=None):. 
+0002a540: 2020 2020 2020 2022 2222 666f 7277 6172         """forwar
+0002a550: 6420 7072 6f63 6573 7322 2222 0a20 2020  d process""".   
+0002a560: 2020 2020 2070 7265 7365 6e74 5f6c 6179       present_lay
+0002a570: 6572 203d 2028 290a 2020 2020 2020 2020  er = ().        
+0002a580: 6966 2073 656c 662e 7573 655f 6d6f 653a  if self.use_moe:
+0002a590: 0a20 2020 2020 2020 2020 2020 2061 6363  .            acc
+0002a5a0: 756d 5f6c 6f73 7320 3d20 7365 6c66 2e61  um_loss = self.a
+0002a5b0: 7578 5f6c 6f73 730a 2020 2020 2020 2020  ux_loss.        
+0002a5c0: 2020 2020 666f 7220 6920 696e 2072 616e      for i in ran
+0002a5d0: 6765 2873 656c 662e 6e75 6d5f 6c61 7965  ge(self.num_laye
+0002a5e0: 7273 293a 0a20 2020 2020 2020 2020 2020  rs):.           
+0002a5f0: 2020 2020 2068 6964 6465 6e5f 7374 6174       hidden_stat
+0002a600: 6573 2c20 7072 6573 656e 742c 2061 7578  es, present, aux
+0002a610: 5f6c 6f73 7320 3d20 7365 6c66 2e62 6c6f  _loss = self.blo
+0002a620: 636b 735b 695d 2868 6964 6465 6e5f 7374  cks[i](hidden_st
+0002a630: 6174 6573 2c0a 2020 2020 2020 2020 2020  ates,.          
+0002a640: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a650: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a660: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a670: 2020 2020 2020 2020 6174 7465 6e74 696f          attentio
+0002a680: 6e5f 6d61 736b 2c0a 2020 2020 2020 2020  n_mask,.        
+0002a690: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a6a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a6b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a6c0: 2020 2020 2020 2020 2020 696e 6974 5f72            init_r
+0002a6d0: 6573 6574 2c0a 2020 2020 2020 2020 2020  eset,.          
+0002a6e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a6f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a700: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a710: 2020 2020 2020 2020 6261 7463 685f 7661          batch_va
+0002a720: 6c69 645f 6c65 6e67 7468 290a 2020 2020  lid_length).    
+0002a730: 2020 2020 2020 2020 2020 2020 7072 6573              pres
+0002a740: 656e 745f 6c61 7965 7220 3d20 7072 6573  ent_layer = pres
+0002a750: 656e 745f 6c61 7965 7220 2b20 2870 7265  ent_layer + (pre
+0002a760: 7365 6e74 2c29 0a20 2020 2020 2020 2020  sent,).         
+0002a770: 2020 2020 2020 2061 6363 756d 5f6c 6f73         accum_los
+0002a780: 7320 3d20 7365 6c66 2e61 6464 2861 6363  s = self.add(acc
+0002a790: 756d 5f6c 6f73 732c 2061 7578 5f6c 6f73  um_loss, aux_los
+0002a7a0: 7329 0a20 2020 2020 2020 2020 2020 2072  s).            r
+0002a7b0: 6574 7572 6e20 6869 6464 656e 5f73 7461  eturn hidden_sta
+0002a7c0: 7465 732c 2070 7265 7365 6e74 5f6c 6179  tes, present_lay
+0002a7d0: 6572 2c20 6163 6375 6d5f 6c6f 7373 0a0a  er, accum_loss..
+0002a7e0: 2020 2020 2020 2020 666f 7220 6920 696e          for i in
+0002a7f0: 2072 616e 6765 2873 656c 662e 6e75 6d5f   range(self.num_
+0002a800: 6c61 7965 7273 293a 0a20 2020 2020 2020  layers):.       
+0002a810: 2020 2020 2068 6964 6465 6e5f 7374 6174       hidden_stat
+0002a820: 6573 2c20 7072 6573 656e 7420 3d20 7365  es, present = se
+0002a830: 6c66 2e62 6c6f 636b 735b 695d 2868 6964  lf.blocks[i](hid
+0002a840: 6465 6e5f 7374 6174 6573 2c0a 2020 2020  den_states,.    
+0002a850: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a860: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a870: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a880: 6174 7465 6e74 696f 6e5f 6d61 736b 2c0a  attention_mask,.
+0002a890: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a8a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a8b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a8c0: 2020 2020 696e 6974 5f72 6573 6574 2c0a      init_reset,.
+0002a8d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a8e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a8f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002a900: 2020 2020 6261 7463 685f 7661 6c69 645f      batch_valid_
+0002a910: 6c65 6e67 7468 290a 2020 2020 2020 2020  length).        
+0002a920: 2020 2020 7072 6573 656e 745f 6c61 7965      present_laye
+0002a930: 7220 3d20 7072 6573 656e 745f 6c61 7965  r = present_laye
+0002a940: 7220 2b20 2870 7265 7365 6e74 2c29 0a0a  r + (present,)..
+0002a950: 2020 2020 2020 2020 7265 7475 726e 2068          return h
+0002a960: 6964 6465 6e5f 7374 6174 6573 2c20 7072  idden_states, pr
+0002a970: 6573 656e 745f 6c61 7965 720a 0a0a 636c  esent_layer...cl
+0002a980: 6173 7320 5472 616e 7366 6f72 6d65 7244  ass TransformerD
+0002a990: 6563 6f64 6572 2843 656c 6c29 3a0a 2020  ecoder(Cell):.  
+0002a9a0: 2020 7222 2222 0a20 2020 2020 2020 2054    r""".        T
+0002a9b0: 7261 6e73 666f 726d 6572 2044 6563 6f64  ransformer Decod
+0002a9c0: 6572 206d 6f64 756c 6520 7769 7468 206d  er module with m
+0002a9d0: 756c 7469 2d6c 6179 6572 2073 7461 636b  ulti-layer stack
+0002a9e0: 6564 206f 6620 6054 7261 6e73 666f 726d  ed of `Transform
+0002a9f0: 6572 4465 636f 6465 724c 6179 6572 602c  erDecoderLayer`,
+0002aa00: 2069 6e63 6c75 6469 6e67 206d 756c 7469   including multi
+0002aa10: 6865 6164 2073 656c 660a 2020 2020 2020  head self.      
+0002aa20: 2020 6174 7465 6e74 696f 6e2c 2063 726f    attention, cro
+0002aa30: 7373 2061 7474 656e 7469 6f6e 2061 6e64  ss attention and
+0002aa40: 2066 6565 6466 6f72 7761 7264 206c 6179   feedforward lay
+0002aa50: 6572 2e0a 0a20 2020 2020 2020 2041 7267  er...        Arg
+0002aa60: 733a 0a20 2020 2020 2020 2020 2020 206e  s:.            n
+0002aa70: 756d 5f6c 6179 6572 7328 696e 7429 3a20  um_layers(int): 
+0002aa80: 5468 6520 6c61 7965 7273 206f 6620 7468  The layers of th
+0002aa90: 6520 6054 7261 6e73 666f 726d 6572 4465  e `TransformerDe
+0002aaa0: 636f 6465 724c 6179 6572 602e 0a20 2020  coderLayer`..   
+0002aab0: 2020 2020 2020 2020 2062 6174 6368 5f73           batch_s
+0002aac0: 697a 6528 696e 7429 3a20 5468 6520 6261  ize(int): The ba
+0002aad0: 7463 6820 7369 7a65 206f 6620 7468 6520  tch size of the 
+0002aae0: 696e 7075 7420 7465 6e73 6f72 2077 6865  input tensor whe
+0002aaf0: 6e20 646f 2069 6e63 7265 6e6d 656e 7461  n do increnmenta
+0002ab00: 6c20 7072 6564 6963 7469 6f6e 2e20 5368  l prediction. Sh
+0002ab10: 6f75 6c64 2062 6520 6120 706f 7369 7469  ould be a positi
+0002ab20: 7665 0a20 2020 2020 2020 2020 2020 2020  ve.             
+0002ab30: 2020 2076 616c 7565 2e20 5768 656e 2064     value. When d
+0002ab40: 6f20 7472 6169 6e69 6e67 206f 7220 7072  o training or pr
+0002ab50: 6564 6963 7469 6f6e 2c20 7468 6520 6172  ediction, the ar
+0002ab60: 6775 6d65 6e74 2077 696c 6c20 6e6f 7420  gument will not 
+0002ab70: 776f 726b 2061 6e64 2074 6865 2075 7365  work and the use
+0002ab80: 7220 6361 6e20 6a75 7374 2070 6173 7320  r can just pass 
+0002ab90: 4e6f 6e65 2074 6f0a 2020 2020 2020 2020  None to.        
+0002aba0: 2020 2020 2020 2020 7468 6520 6172 6775          the argu
+0002abb0: 6d65 6e74 2e0a 2020 2020 2020 2020 2020  ment..          
+0002abc0: 2020 6869 6464 656e 5f73 697a 6528 696e    hidden_size(in
+0002abd0: 7429 3a20 5468 6520 6869 6464 656e 2073  t): The hidden s
+0002abe0: 697a 6520 6f66 2074 6865 2069 6e70 7574  ize of the input
+0002abf0: 2e0a 2020 2020 2020 2020 2020 2020 6666  ..            ff
+0002ac00: 6e5f 6869 6464 656e 5f73 697a 6528 696e  n_hidden_size(in
+0002ac10: 7429 3a20 5468 6520 6869 6464 656e 2073  t): The hidden s
+0002ac20: 697a 6520 6f66 2062 6f74 746c 656e 6563  ize of bottlenec
+0002ac30: 6b20 696e 2074 6865 2066 6565 6466 6f72  k in the feedfor
+0002ac40: 7761 7264 206c 6179 6572 2e0a 2020 2020  ward layer..    
+0002ac50: 2020 2020 2020 2020 7372 635f 7365 715f          src_seq_
+0002ac60: 6c65 6e67 7468 2869 6e74 293a 2054 6865  length(int): The
+0002ac70: 2069 6e70 7574 2073 6f75 7263 6520 7365   input source se
+0002ac80: 7175 656e 6365 206c 656e 6774 682e 0a20  quence length.. 
+0002ac90: 2020 2020 2020 2020 2020 2074 6774 5f73             tgt_s
+0002aca0: 6571 5f6c 656e 6774 6828 696e 7429 3a20  eq_length(int): 
+0002acb0: 5468 6520 696e 7075 7420 7461 7267 6574  The input target
+0002acc0: 2073 6571 7565 6e63 6520 6c65 6e67 7468   sequence length
+0002acd0: 2e0a 2020 2020 2020 2020 2020 2020 6e75  ..            nu
+0002ace0: 6d5f 6865 6164 7328 696e 7429 3a20 5468  m_heads(int): Th
+0002acf0: 6520 6e75 6d62 6572 206f 6620 7468 6520  e number of the 
+0002ad00: 6865 6164 732e 0a20 2020 2020 2020 2020  heads..         
+0002ad10: 2020 2061 7474 656e 7469 6f6e 5f64 726f     attention_dro
+0002ad20: 706f 7574 5f72 6174 6528 666c 6f61 7429  pout_rate(float)
+0002ad30: 3a20 5468 6520 6472 6f70 6f75 7420 7261  : The dropout ra
+0002ad40: 7465 206f 6620 7468 6520 6174 7465 6e74  te of the attent
+0002ad50: 696f 6e20 7363 6f72 6573 2e20 4465 6661  ion scores. Defa
+0002ad60: 756c 743a 302e 312e 0a20 2020 2020 2020  ult:0.1..       
+0002ad70: 2020 2020 2068 6964 6465 6e5f 6472 6f70       hidden_drop
+0002ad80: 6f75 745f 7261 7465 2866 6c6f 6174 293a  out_rate(float):
+0002ad90: 2054 6865 2064 726f 706f 7574 2072 6174   The dropout rat
+0002ada0: 6520 6f66 2074 6865 2066 696e 616c 206f  e of the final o
+0002adb0: 7574 7075 7420 6f66 2074 6865 206c 6179  utput of the lay
+0002adc0: 6572 2e20 4465 6661 756c 743a 302e 312e  er. Default:0.1.
+0002add0: 0a20 2020 2020 2020 2020 2020 2070 6f73  .            pos
+0002ade0: 745f 6c61 7965 726e 6f72 6d5f 7265 7369  t_layernorm_resi
+0002adf0: 6475 616c 2862 6f6f 6c29 3a20 446f 2072  dual(bool): Do r
+0002ae00: 6573 6964 7561 6c73 2061 6464 7320 6265  esiduals adds be
+0002ae10: 666f 7265 2074 6865 206c 6179 6572 6e6f  fore the layerno
+0002ae20: 726d 2e20 4465 6661 756c 7420 4661 6c73  rm. Default Fals
+0002ae30: 652e 0a20 2020 2020 2020 2020 2020 206c  e..            l
+0002ae40: 6179 6572 6e6f 726d 5f63 6f6d 7075 7465  ayernorm_compute
+0002ae50: 5f74 7970 6528 6474 7970 652e 4e75 6d62  _type(dtype.Numb
+0002ae60: 6572 293a 2054 6865 2063 6f6d 7075 7461  er): The computa
+0002ae70: 7469 6f6e 2074 7970 6520 6f66 2074 6865  tion type of the
+0002ae80: 206c 6179 6572 6e6f 726d 2e0a 2020 2020   layernorm..    
+0002ae90: 2020 2020 2020 2020 2020 2020 5368 6f75              Shou
+0002aea0: 6c64 2062 6520 6d73 7479 7065 2e66 6c6f  ld be mstype.flo
+0002aeb0: 6174 3332 206f 7220 6d73 7479 7065 2e66  at32 or mstype.f
+0002aec0: 6c6f 6174 3136 2e20 4465 6661 756c 7420  loat16. Default 
+0002aed0: 6d73 7479 7065 2e66 6c6f 6174 3332 2e0a  mstype.float32..
+0002aee0: 2020 2020 2020 2020 2020 2020 736f 6674              soft
+0002aef0: 6d61 785f 636f 6d70 7574 655f 7479 7065  max_compute_type
+0002af00: 2864 7479 7065 2e4e 756d 6265 7229 3a20  (dtype.Number): 
+0002af10: 5468 6520 636f 6d70 7574 6174 696f 6e20  The computation 
+0002af20: 7479 7065 206f 6620 7468 6520 736f 6674  type of the soft
+0002af30: 6d61 7820 696e 2074 6865 2061 7474 656e  max in the atten
+0002af40: 7469 6f6e 2e0a 2020 2020 2020 2020 2020  tion..          
+0002af50: 2020 2020 2020 5368 6f75 6c64 2062 6520        Should be 
+0002af60: 6d73 7479 7065 2e66 6c6f 6174 3332 206f  mstype.float32 o
+0002af70: 7220 6d73 7479 7065 2e66 6c6f 6174 3136  r mstype.float16
+0002af80: 2e20 4465 6661 756c 7420 6d73 7479 7065  . Default mstype
+0002af90: 2e66 6c6f 6174 3332 2e0a 2020 2020 2020  .float32..      
+0002afa0: 2020 2020 2020 7061 7261 6d5f 696e 6974        param_init
+0002afb0: 5f74 7970 6528 6474 7970 652e 4e75 6d62  _type(dtype.Numb
+0002afc0: 6572 293a 2054 6865 2070 6172 616d 6574  er): The paramet
+0002afd0: 6572 2069 6e69 7469 616c 697a 6174 696f  er initializatio
+0002afe0: 6e20 7479 7065 206f 6620 7468 6520 6d6f  n type of the mo
+0002aff0: 6475 6c65 2e0a 2020 2020 2020 2020 2020  dule..          
+0002b000: 2020 2020 2020 5368 6f75 6c64 2062 6520        Should be 
+0002b010: 6d73 7479 7065 2e66 6c6f 6174 3332 206f  mstype.float32 o
+0002b020: 7220 6d73 7479 7065 2e66 6c6f 6174 3136  r mstype.float16
+0002b030: 2e20 4465 6661 756c 7420 6d73 7479 7065  . Default mstype
+0002b040: 2e66 6c6f 6174 3332 2e0a 2020 2020 2020  .float32..      
+0002b050: 2020 2020 2020 6869 6464 656e 5f61 6374        hidden_act
+0002b060: 2028 7374 722c 206e 6e2e 4365 6c6c 293a   (str, nn.Cell):
+0002b070: 2054 6865 2061 6374 6976 6174 696f 6e20   The activation 
+0002b080: 6f66 2074 6865 2069 6e74 6572 6e61 6c20  of the internal 
+0002b090: 6665 6564 666f 7277 6172 6420 6c61 7965  feedforward laye
+0002b0a0: 722e 2053 7570 706f 7274 7320 2772 656c  r. Supports 'rel
+0002b0b0: 7527 2c0a 2020 2020 2020 2020 2020 2020  u',.            
+0002b0c0: 2020 2020 2772 656c 7536 272c 2027 7461      'relu6', 'ta
+0002b0d0: 6e68 272c 2027 6765 6c75 272c 2027 6661  nh', 'gelu', 'fa
+0002b0e0: 7374 5f67 656c 7527 2c20 2765 6c75 272c  st_gelu', 'elu',
+0002b0f0: 2027 7369 676d 6f69 6427 2c20 2770 7265   'sigmoid', 'pre
+0002b100: 6c75 272c 2027 6c65 616b 7972 656c 7527  lu', 'leakyrelu'
+0002b110: 2c20 2768 7377 6973 6827 2c0a 2020 2020  , 'hswish',.    
+0002b120: 2020 2020 2020 2020 2020 2020 2768 7369              'hsi
+0002b130: 676d 6f69 6427 2c20 276c 6f67 7369 676d  gmoid', 'logsigm
+0002b140: 6f69 6427 2061 6e64 2073 6f20 6f6e 2e20  oid' and so on. 
+0002b150: 5573 6572 2063 616e 2070 726f 7669 6465  User can provide
+0002b160: 2063 7573 746f 6d20 6163 7469 7669 7469   custom activiti
+0002b170: 6f6e 2074 6f20 7468 6520 6172 6775 6d65  on to the argume
+0002b180: 6e74 2e0a 2020 2020 2020 2020 2020 2020  nt..            
+0002b190: 2020 2020 4966 2075 7365 7220 7761 6e74      If user want
+0002b1a0: 7320 746f 2072 756e 2074 6865 206e 6574  s to run the net
+0002b1b0: 2069 6e20 7468 6520 7061 7261 6c6c 656c   in the parallel
+0002b1c0: 206d 6f64 652c 2074 6865 2063 7573 746f   mode, the custo
+0002b1d0: 6d20 6163 7469 7661 7469 6f6e 206d 7573  m activation mus
+0002b1e0: 7420 616c 736f 2070 726f 7669 6465 0a20  t also provide. 
+0002b1f0: 2020 2020 2020 2020 2020 2020 2020 2074                 t
+0002b200: 6865 2060 6163 7469 7661 7469 6f6e 5f73  he `activation_s
+0002b210: 6861 7264 6020 6675 6e63 7469 6f6e 2e20  hard` function. 
+0002b220: 506c 6561 7365 2073 6565 2074 6865 2065  Please see the e
+0002b230: 7861 6d70 6c65 7320 6f66 2074 6865 0a20  xamples of the. 
+0002b240: 2020 2020 2020 2020 2020 2020 2020 2063                 c
+0002b250: 6c61 7373 3a60 6d69 6e64 666f 726d 6572  lass:`mindformer
+0002b260: 732e 6d6f 6475 6c65 732e 7472 616e 7366  s.modules.transf
+0002b270: 6f72 6d65 722e 4665 6564 466f 7277 6172  ormer.FeedForwar
+0002b280: 6460 2e20 4465 6661 756c 743a 2067 656c  d`. Default: gel
+0002b290: 752e 0a20 2020 2020 2020 2020 2020 206c  u..            l
+0002b2a0: 616d 6264 615f 6675 6e63 2866 756e 6374  ambda_func(funct
+0002b2b0: 696f 6e29 3a20 4120 6675 6e63 7469 6f6e  ion): A function
+0002b2c0: 2063 616e 2064 6574 6572 6d69 6e65 2074   can determine t
+0002b2d0: 6865 2066 7573 696f 6e20 696e 6465 782c  he fusion index,
+0002b2e0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0002b2f0: 2070 6970 656c 696e 6520 7374 6167 6573   pipeline stages
+0002b300: 2061 6e64 2072 6563 6f6d 7075 7465 2061   and recompute a
+0002b310: 7474 7269 6275 7465 2e20 4966 2074 6865  ttribute. If the
+0002b320: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0002b330: 2075 7365 7220 7761 6e74 7320 746f 2064   user wants to d
+0002b340: 6574 6572 6d69 6e65 2074 6865 2070 6970  etermine the pip
+0002b350: 656c 696e 6520 7374 6167 6520 616e 6420  eline stage and 
+0002b360: 6772 6164 6965 6e74 2061 6767 7265 6761  gradient aggrega
+0002b370: 7469 6f6e 2066 7573 696f 6e2c 2074 6865  tion fusion, the
+0002b380: 2075 7365 7220 6361 6e20 7061 7373 2061   user can pass a
+0002b390: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0002b3a0: 2066 756e 6374 696f 6e20 7468 6174 2061   function that a
+0002b3b0: 6363 6570 7473 2060 6e65 7477 6f72 6b60  ccepts `network`
+0002b3c0: 2c20 606c 6179 6572 5f69 6460 2c20 606f  , `layer_id`, `o
+0002b3d0: 6666 7365 7460 2c20 6070 6172 616c 6c65  ffset`, `paralle
+0002b3e0: 6c5f 636f 6e66 6967 602c 2060 6c61 7965  l_config`, `laye
+0002b3f0: 7273 602e 2054 6865 2060 6e65 7477 6f72  rs`. The `networ
+0002b400: 6b28 4365 6c6c 2960 0a20 2020 2020 2020  k(Cell)`.       
+0002b410: 2020 2020 2020 2020 2072 6570 7265 7365           represe
+0002b420: 6e74 7320 7468 6520 7472 616e 7366 6f72  nts the transfor
+0002b430: 6d65 7220 626c 6f63 6b2c 2060 6c61 7965  mer block, `laye
+0002b440: 725f 6964 2869 6e74 2960 206d 6561 6e73  r_id(int)` means
+0002b450: 2074 6865 206c 6179 6572 2069 6e64 6578   the layer index
+0002b460: 2066 6f72 2074 6865 2063 7572 7265 6e74   for the current
+0002b470: 206d 6f64 756c 652c 2063 6f75 6e74 730a   module, counts.
+0002b480: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002b490: 6672 6f6d 207a 6572 6f2c 2060 6f66 6673  from zero, `offs
+0002b4a0: 6574 2869 6e74 2960 206d 6561 6e73 2074  et(int)` means t
+0002b4b0: 6865 206c 6179 6572 5f69 6e64 6578 206e  he layer_index n
+0002b4c0: 6565 6473 2061 6e20 6f66 6673 6574 2c20  eeds an offset, 
+0002b4d0: 6966 2074 6865 7265 2061 7265 206f 7468  if there are oth
+0002b4e0: 6572 206d 6f64 756c 6573 2069 6e20 7468  er modules in th
+0002b4f0: 6520 6e65 742e 0a20 2020 2020 2020 2020  e net..         
+0002b500: 2020 2020 2020 2054 6865 2064 6566 6175         The defau
+0002b510: 6c74 2073 6574 7469 6e67 2066 6f72 2074  lt setting for t
+0002b520: 6865 2070 6970 656c 696e 6520 6973 3a20  he pipeline is: 
+0002b530: 6028 6c61 7965 725f 6964 202b 206f 6666  `(layer_id + off
+0002b540: 7365 7429 202f 2f20 286c 6179 6572 7320  set) // (layers 
+0002b550: 2f20 7069 7065 6c69 6e65 5f73 7461 6765  / pipeline_stage
+0002b560: 2960 2e0a 2020 2020 2020 2020 2020 2020  )`..            
+0002b570: 2020 2020 4465 6661 756c 743a 204e 6f6e      Default: Non
+0002b580: 652e 0a20 2020 2020 2020 2020 2020 2075  e..            u
+0002b590: 7365 5f70 6173 7428 626f 6f6c 293a 2055  se_past(bool): U
+0002b5a0: 7365 2074 6865 2070 6173 7420 7374 6174  se the past stat
+0002b5b0: 6520 746f 2063 6f6d 7075 7465 2c20 7573  e to compute, us
+0002b5c0: 6564 2066 6f72 2069 6e63 7265 6d65 6e74  ed for increment
+0002b5d0: 616c 2070 7265 6469 6374 696f 6e2e 2044  al prediction. D
+0002b5e0: 6566 6175 6c74 2046 616c 7365 2e0a 2020  efault False..  
+0002b5f0: 2020 2020 2020 2020 2020 6f66 6673 6574            offset
+0002b600: 2869 6e74 293a 2054 6865 2069 6e69 7469  (int): The initi
+0002b610: 616c 206c 6179 6572 2069 6e64 6578 2066  al layer index f
+0002b620: 6f72 2074 6865 2060 6465 636f 6465 7260  or the `decoder`
+0002b630: 2e20 5573 6564 2066 6f72 2073 6574 7469  . Used for setti
+0002b640: 6e67 2074 6865 2066 7573 696f 6e20 6964  ng the fusion id
+0002b650: 2061 6e64 2073 7461 6765 2069 642c 2074   and stage id, t
+0002b660: 6f20 6e6f 740a 2020 2020 2020 2020 2020  o not.          
+0002b670: 2020 2020 2020 6f76 6572 6c61 7020 7769        overlap wi
+0002b680: 7468 2074 6865 2065 6e63 6f64 6572 206c  th the encoder l
+0002b690: 6179 6572 2e20 4465 6661 756c 7420 302e  ayer. Default 0.
+0002b6a0: 0a20 2020 2020 2020 2020 2020 206d 6f65  .            moe
+0002b6b0: 5f63 6f6e 6669 6728 4d6f 4543 6f6e 6669  _config(MoEConfi
+0002b6c0: 6729 3a20 5468 6520 636f 6e66 6967 7572  g): The configur
+0002b6d0: 6174 696f 6e20 6f66 204d 6f45 2028 4d69  ation of MoE (Mi
+0002b6e0: 7874 7572 6520 6f66 2045 7870 6572 7429  xture of Expert)
+0002b6f0: 2e20 4465 6661 756c 7420 6973 2061 6e20  . Default is an 
+0002b700: 696e 7374 616e 6365 206f 6620 4d6f 4543  instance of MoEC
+0002b710: 6f6e 6669 670a 2020 2020 2020 2020 2020  onfig.          
+0002b720: 2020 2020 2020 7769 7468 2064 6566 6175        with defau
+0002b730: 6c74 2076 616c 7565 732e 2050 6c65 6173  lt values. Pleas
+0002b740: 6520 7365 6520 604d 6f45 436f 6e66 6967  e see `MoEConfig
+0002b750: 602e 0a20 2020 2020 2020 2020 2020 2070  `..            p
+0002b760: 6172 616c 6c65 6c5f 636f 6e66 6967 2854  arallel_config(T
+0002b770: 7261 6e73 666f 726d 6572 4f70 5061 7261  ransformerOpPara
+0002b780: 6c6c 656c 436f 6e66 6967 293a 2054 6865  llelConfig): The
+0002b790: 2070 6172 616c 6c65 6c20 636f 6e66 6967   parallel config
+0002b7a0: 7572 652e 2044 6566 6175 6c74 2060 6465  ure. Default `de
+0002b7b0: 6661 756c 745f 7472 616e 7366 6f72 6d65  fault_transforme
+0002b7c0: 725f 636f 6e66 6967 602c 0a20 2020 2020  r_config`,.     
+0002b7d0: 2020 2020 2020 2020 2020 2061 6e20 696e             an in
+0002b7e0: 7374 616e 6365 206f 6620 6054 7261 6e73  stance of `Trans
+0002b7f0: 666f 726d 6572 4f70 5061 7261 6c6c 656c  formerOpParallel
+0002b800: 436f 6e66 6967 6020 7769 7468 2064 6566  Config` with def
+0002b810: 6175 6c74 2061 7267 732e 0a0a 2020 2020  ault args...    
+0002b820: 2020 2020 496e 7075 7473 3a0a 2020 2020      Inputs:.    
+0002b830: 2020 2020 2020 2020 2d20 2a2a 6869 6464          - **hidd
+0002b840: 656e 5f73 7461 7473 2a2a 2028 5465 6e73  en_stats** (Tens
+0002b850: 6f72 2920 2d20 5468 6520 696e 7075 7420  or) - The input 
+0002b860: 7465 6e73 6f72 2077 6974 6820 7368 6170  tensor with shap
+0002b870: 6520 5b62 6174 6368 5f73 697a 652c 2073  e [batch_size, s
+0002b880: 6571 5f6c 656e 6774 682c 2068 6964 6465  eq_length, hidde
+0002b890: 6e5f 7369 7a65 5d20 6f72 0a20 2020 2020  n_size] or.     
+0002b8a0: 2020 2020 2020 2020 205b 6261 7463 685f           [batch_
+0002b8b0: 7369 7a65 202a 2073 6571 5f6c 656e 6774  size * seq_lengt
+0002b8c0: 682c 2068 6964 6465 6e5f 7369 7a65 5d0a  h, hidden_size].
+0002b8d0: 2020 2020 2020 2020 2020 2020 2d20 2a2a              - **
+0002b8e0: 6174 7465 6e74 696f 6e5f 6d61 736b 2a2a  attention_mask**
+0002b8f0: 2028 5465 6e73 6f72 2920 2d20 5468 6520   (Tensor) - The 
+0002b900: 6174 7465 6e74 696f 6e20 6d61 736b 2066  attention mask f
+0002b910: 6f72 2064 6563 6f64 6572 2077 6974 6820  or decoder with 
+0002b920: 7368 6170 650a 2020 2020 2020 2020 2020  shape.          
+0002b930: 2020 2020 5b62 6174 6368 5f73 697a 652c      [batch_size,
+0002b940: 2073 6571 5f6c 656e 6774 682c 2073 6571   seq_length, seq
+0002b950: 5f6c 656e 6774 685d 206f 7220 4e6f 6e65  _length] or None
+0002b960: 2e20 4e6f 6e65 206d 6561 6e73 2074 6865  . None means the
+0002b970: 7265 2077 696c 6c20 6265 206e 6f20 6d61  re will be no ma
+0002b980: 736b 2069 6e20 736f 6674 6d61 780a 2020  sk in softmax.  
+0002b990: 2020 2020 2020 2020 2020 2020 636f 6d70              comp
+0002b9a0: 7574 6174 696f 6e20 696e 2073 656c 6620  utation in self 
+0002b9b0: 6174 7465 6e74 696f 6e2e 0a20 2020 2020  attention..     
+0002b9c0: 2020 2020 2020 202d 202a 2a65 6e63 6f64         - **encod
+0002b9d0: 6572 5f6f 7574 7075 742a 2a20 2854 656e  er_output** (Ten
+0002b9e0: 736f 7229 202d 2054 6865 206f 7574 7075  sor) - The outpu
+0002b9f0: 7420 6f66 2074 6865 2065 6e63 6f64 6572  t of the encoder
+0002ba00: 2077 6974 6820 7368 6170 6520 5b62 6174   with shape [bat
+0002ba10: 6368 5f73 697a 652c 2073 6571 5f6c 656e  ch_size, seq_len
+0002ba20: 6774 682c 2068 6964 6465 6e5f 7369 7a65  gth, hidden_size
+0002ba30: 5d0a 2020 2020 2020 2020 2020 2020 2020  ].              
+0002ba40: 6f72 205b 6261 7463 685f 7369 7a65 202a  or [batch_size *
+0002ba50: 2073 6571 5f6c 656e 6774 682c 2068 6964   seq_length, hid
+0002ba60: 6465 6e5f 7369 7a65 5d2e 204e 6f74 6520  den_size]. Note 
+0002ba70: 7468 6973 2061 7267 7320 6361 6e20 6e6f  this args can no
+0002ba80: 7420 6265 2070 6173 7365 6420 6279 204e  t be passed by N
+0002ba90: 6f6e 6520 7768 656e 2074 6865 206e 6574  one when the net
+0002baa0: 2069 7320 696e 0a20 2020 2020 2020 2020   is in.         
+0002bab0: 2020 2020 206f 7574 6572 6d6f 7374 206c       outermost l
+0002bac0: 6179 6572 2e20 4465 6661 756c 7420 4e6f  ayer. Default No
+0002bad0: 6e65 2e0a 2020 2020 2020 2020 2020 2020  ne..            
+0002bae0: 2d20 2a2a 6d65 6d6f 7279 5f6d 6173 6b2a  - **memory_mask*
+0002baf0: 2a20 2854 656e 736f 7229 202d 2054 6865  * (Tensor) - The
+0002bb00: 206d 656d 6f72 7920 6d61 736b 206f 6620   memory mask of 
+0002bb10: 7468 6520 6372 6f73 7320 6174 7465 6e74  the cross attent
+0002bb20: 696f 6e20 7769 7468 2073 6861 7065 205b  ion with shape [
+0002bb30: 6261 7463 682c 2074 6774 5f73 6571 5f6c  batch, tgt_seq_l
+0002bb40: 656e 6774 682c 0a20 2020 2020 2020 2020  ength,.         
+0002bb50: 2020 2020 2073 7263 5f73 6571 5f6c 656e       src_seq_len
+0002bb60: 6774 685d 2077 6865 7265 2074 6774 5f73  gth] where tgt_s
+0002bb70: 6571 5f6c 656e 6774 6820 6973 2074 6865  eq_length is the
+0002bb80: 206c 656e 6774 6820 6f66 2074 6865 2064   length of the d
+0002bb90: 6563 6f64 6572 2e20 5468 6520 7573 6572  ecoder. The user
+0002bba0: 2063 616e 2061 6c73 6f20 7061 7373 204e   can also pass N
+0002bbb0: 6f6e 652e 204e 6f6e 650a 2020 2020 2020  one. None.      
+0002bbc0: 2020 2020 2020 2020 6d65 616e 7320 7468          means th
+0002bbd0: 6572 6520 7769 6c6c 2062 6520 6e6f 206d  ere will be no m
+0002bbe0: 6173 6b20 696e 2073 6f66 746d 6178 2063  ask in softmax c
+0002bbf0: 6f6d 7075 7461 7469 6f6e 2069 6e20 6372  omputation in cr
+0002bc00: 6f73 7320 6174 7465 6e74 696f 6e2e 2044  oss attention. D
+0002bc10: 6566 6175 6c74 204e 6f6e 652e 0a20 2020  efault None..   
+0002bc20: 2020 2020 2020 2020 202d 202a 2a69 6e69           - **ini
+0002bc30: 745f 7265 7365 742a 2a20 2854 656e 736f  t_reset** (Tenso
+0002bc40: 7229 202d 2041 2062 6f6f 6c20 7465 6e73  r) - A bool tens
+0002bc50: 6f72 2077 6974 6820 7368 6170 6520 5b31  or with shape [1
+0002bc60: 5d2c 2075 7365 6420 746f 2063 6c65 6172  ], used to clear
+0002bc70: 2074 6865 2070 6173 7420 6b65 7920 7061   the past key pa
+0002bc80: 7261 6d65 7465 7220 616e 640a 2020 2020  rameter and.    
+0002bc90: 2020 2020 2020 2020 2020 7061 7374 2076            past v
+0002bca0: 616c 7565 2070 6172 616d 6574 6572 2075  alue parameter u
+0002bcb0: 7365 6420 696e 2074 6865 2069 6e63 7265  sed in the incre
+0002bcc0: 6d65 6e74 616c 2070 7265 6469 6374 696f  mental predictio
+0002bcd0: 6e2e 204f 6e6c 7920 7661 6c69 6420 7768  n. Only valid wh
+0002bce0: 656e 2075 7365 5f70 6173 7420 6973 2054  en use_past is T
+0002bcf0: 7275 652e 2044 6566 6175 6c74 2054 7275  rue. Default Tru
+0002bd00: 652e 0a20 2020 2020 2020 2020 2020 202d  e..            -
+0002bd10: 202a 2a62 6174 6368 5f76 616c 6964 5f6c   **batch_valid_l
+0002bd20: 656e 6774 682a 2a20 2854 656e 736f 7229  ength** (Tensor)
+0002bd30: 202d 2049 6e74 3332 2074 656e 736f 7220   - Int32 tensor 
+0002bd40: 7769 7468 2073 6861 7065 205b 6261 7463  with shape [batc
+0002bd50: 685f 7369 7a65 5d20 7468 6520 7061 7374  h_size] the past
+0002bd60: 2063 616c 6375 6c61 7465 6420 7468 6520   calculated the 
+0002bd70: 696e 6465 782e 0a20 2020 2020 2020 2020  index..         
+0002bd80: 2020 2020 2055 7365 6420 666f 7220 696e       Used for in
+0002bd90: 6372 656d 656e 7461 6c20 7072 6564 6963  cremental predic
+0002bda0: 7469 6f6e 2077 6865 6e20 7468 6520 7573  tion when the us
+0002bdb0: 655f 7061 7374 2069 7320 5472 7565 2e20  e_past is True. 
+0002bdc0: 4465 6661 756c 7420 4e6f 6e65 2e0a 0a20  Default None... 
+0002bdd0: 2020 2020 2020 204f 7574 7075 7473 3a0a         Outputs:.
+0002bde0: 2020 2020 2020 2020 2020 2020 5475 706c              Tupl
+0002bdf0: 652c 2061 2074 7570 6c65 2063 6f6e 7461  e, a tuple conta
+0002be00: 696e 7328 606f 7574 7075 7460 2c20 606c  ins(`output`, `l
+0002be10: 6179 6572 5f70 7265 7365 6e74 6029 0a0a  ayer_present`)..
+0002be20: 2020 2020 2020 2020 2020 2020 2d20 2a2a              - **
+0002be30: 6f75 7470 7574 2a2a 2028 5465 6e73 6f72  output** (Tensor
+0002be40: 2920 2d20 5468 6520 6f75 7470 7574 206c  ) - The output l
+0002be50: 6f67 6974 206f 6620 7468 6973 206c 6179  ogit of this lay
+0002be60: 6572 2e20 5468 6520 7368 6170 6520 6973  er. The shape is
+0002be70: 205b 6261 7463 682c 2074 6774 5f73 6571   [batch, tgt_seq
+0002be80: 5f6c 656e 6774 682c 2068 6964 6465 6e5f  _length, hidden_
+0002be90: 7369 7a65 5d20 6f72 0a20 2020 2020 2020  size] or.       
+0002bea0: 2020 2020 2020 205b 6261 7463 6820 2a20         [batch * 
+0002beb0: 7467 745f 7365 715f 6c65 6e67 7468 2c20  tgt_seq_length, 
+0002bec0: 6869 6464 656e 5f73 697a 655d 0a20 2020  hidden_size].   
+0002bed0: 2020 2020 2020 2020 202d 202a 2a6c 6179           - **lay
+0002bee0: 6572 5f70 7265 7365 6e74 2a2a 2028 5475  er_present** (Tu
+0002bef0: 706c 6529 202d 2041 2074 7570 6c65 2077  ple) - A tuple w
+0002bf00: 6974 6820 7369 7a65 206f 6620 6e75 6d5f  ith size of num_
+0002bf10: 6c61 7965 7273 2c20 7768 6572 6520 6561  layers, where ea
+0002bf20: 6368 2074 7570 6c65 2069 7320 7468 6520  ch tuple is the 
+0002bf30: 7465 6e73 6f72 206f 6620 7468 650a 2020  tensor of the.  
+0002bf40: 2020 2020 2020 2020 2020 2020 7072 6f6a              proj
+0002bf50: 6563 7465 6420 6b65 7920 616e 6420 7661  ected key and va
+0002bf60: 6c75 6520 7665 6374 6f72 2069 6e20 7365  lue vector in se
+0002bf70: 6c66 2061 7474 656e 7469 6f6e 2077 6974  lf attention wit
+0002bf80: 6820 7368 6170 6520 2828 6261 7463 685f  h shape ((batch_
+0002bf90: 7369 7a65 2c20 6e75 6d5f 6865 6164 732c  size, num_heads,
+0002bfa0: 2073 697a 655f 7065 725f 6865 6164 2c0a   size_per_head,.
+0002bfb0: 2020 2020 2020 2020 2020 2020 2020 7467                tg
+0002bfc0: 745f 7365 715f 6c65 6e67 7468 292c 2028  t_seq_length), (
+0002bfd0: 6261 7463 685f 7369 7a65 2c20 6e75 6d5f  batch_size, num_
+0002bfe0: 6865 6164 732c 2074 6774 5f73 6571 5f6c  heads, tgt_seq_l
+0002bff0: 656e 6774 682c 2073 697a 655f 7065 725f  ength, size_per_
+0002c000: 6865 6164 292c 2061 6e64 206f 6620 7468  head), and of th
+0002c010: 6520 7072 6f6a 6563 7465 6420 6b65 790a  e projected key.
+0002c020: 2020 2020 2020 2020 2020 2020 2020 616e                an
+0002c030: 6420 7661 6c75 6520 7665 6374 6f72 2069  d value vector i
+0002c040: 6e20 6372 6f73 7320 6174 7465 6e74 696f  n cross attentio
+0002c050: 6e20 7769 7468 2073 6861 7065 2020 2862  n with shape  (b
+0002c060: 6174 6368 5f73 697a 652c 206e 756d 5f68  atch_size, num_h
+0002c070: 6561 6473 2c20 7369 7a65 5f70 6572 5f68  eads, size_per_h
+0002c080: 6561 642c 2073 7263 5f73 6571 5f6c 656e  ead, src_seq_len
+0002c090: 6774 6829 2c0a 2020 2020 2020 2020 2020  gth),.          
+0002c0a0: 2020 2020 2862 6174 6368 5f73 697a 652c      (batch_size,
+0002c0b0: 206e 756d 5f68 6561 6473 2c20 7372 635f   num_heads, src_
+0002c0c0: 7365 715f 6c65 6e67 7468 2c20 7369 7a65  seq_length, size
+0002c0d0: 5f70 6572 5f68 6561 6429 292e 0a0a 2020  _per_head))...  
+0002c0e0: 2020 2020 2020 5375 7070 6f72 7465 6420        Supported 
+0002c0f0: 506c 6174 666f 726d 733a 0a20 2020 2020  Platforms:.     
+0002c100: 2020 2020 2020 2060 6041 7363 656e 6460         ``Ascend`
+0002c110: 6020 6060 4750 5560 600a 0a20 2020 2020  ` ``GPU``..     
+0002c120: 2020 2045 7861 6d70 6c65 733a 0a20 2020     Examples:.   
+0002c130: 2020 2020 2020 2020 203e 3e3e 2069 6d70           >>> imp
+0002c140: 6f72 7420 6e75 6d70 7920 6173 206e 700a  ort numpy as np.
+0002c150: 2020 2020 2020 2020 2020 2020 3e3e 3e20              >>> 
+0002c160: 6672 6f6d 206d 696e 6473 706f 7265 2069  from mindspore i
+0002c170: 6d70 6f72 7420 6474 7970 6520 6173 206d  mport dtype as m
+0002c180: 7374 7970 650a 2020 2020 2020 2020 2020  stype.          
+0002c190: 2020 3e3e 3e20 6672 6f6d 206d 696e 6466    >>> from mindf
+0002c1a0: 6f72 6d65 7273 2e6d 6f64 756c 6573 2e74  ormers.modules.t
+0002c1b0: 7261 6e73 666f 726d 6572 2069 6d70 6f72  ransformer impor
+0002c1c0: 7420 5472 616e 7366 6f72 6d65 7244 6563  t TransformerDec
+0002c1d0: 6f64 6572 0a20 2020 2020 2020 2020 2020  oder.           
+0002c1e0: 203e 3e3e 2066 726f 6d20 6d69 6e64 7370   >>> from mindsp
+0002c1f0: 6f72 6520 696d 706f 7274 2054 656e 736f  ore import Tenso
+0002c200: 720a 2020 2020 2020 2020 2020 2020 3e3e  r.            >>
+0002c210: 3e20 6d6f 6465 6c20 3d20 5472 616e 7366  > model = Transf
+0002c220: 6f72 6d65 7244 6563 6f64 6572 2862 6174  ormerDecoder(bat
+0002c230: 6368 5f73 697a 653d 322c 206e 756d 5f6c  ch_size=2, num_l
+0002c240: 6179 6572 733d 312c 2068 6964 6465 6e5f  ayers=1, hidden_
+0002c250: 7369 7a65 3d36 342c 2066 666e 5f68 6964  size=64, ffn_hid
+0002c260: 6465 6e5f 7369 7a65 3d36 342c 0a20 2020  den_size=64,.   
+0002c270: 2020 2020 2020 2020 202e 2e2e 2020 2020           ...    
 0002c280: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002c290: 2020 2020 2020 2020 2020 2020 6666 6e5f              ffn_
-0002c2a0: 6869 6464 656e 5f73 697a 653d 5661 6c69  hidden_size=Vali
-0002c2b0: 6461 746f 722e 6368 6563 6b5f 706f 7369  dator.check_posi
-0002c2c0: 7469 7665 5f69 6e74 2c0a 2020 2020 2020  tive_int,.      
-0002c2d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002c2e0: 2020 2020 2020 2020 2020 7372 635f 7365            src_se
-0002c2f0: 715f 6c65 6e67 7468 3d56 616c 6964 6174  q_length=Validat
-0002c300: 6f72 2e63 6865 636b 5f70 6f73 6974 6976  or.check_positiv
-0002c310: 655f 696e 742c 0a20 2020 2020 2020 2020  e_int,.         
-0002c320: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002c330: 2020 2020 2020 206e 756d 5f6c 6179 6572         num_layer
-0002c340: 733d 5661 6c69 6461 746f 722e 6368 6563  s=Validator.chec
-0002c350: 6b5f 706f 7369 7469 7665 5f69 6e74 2c0a  k_positive_int,.
-0002c360: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002c370: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002c380: 7467 745f 7365 715f 6c65 6e67 7468 3d56  tgt_seq_length=V
-0002c390: 616c 6964 6174 6f72 2e63 6865 636b 5f70  alidator.check_p
-0002c3a0: 6f73 6974 6976 655f 696e 742c 0a20 2020  ositive_int,.   
-0002c3b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002c3c0: 2020 2020 2020 2020 2020 2020 206f 6666               off
-0002c3d0: 7365 743d 5661 6c69 6461 746f 722e 6368  set=Validator.ch
-0002c3e0: 6563 6b5f 6e6f 6e5f 6e65 6761 7469 7665  eck_non_negative
-0002c3f0: 5f69 6e74 2c0a 2020 2020 2020 2020 2020  _int,.          
-0002c400: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002c410: 2020 2020 2020 6174 7465 6e74 696f 6e5f        attention_
-0002c420: 6472 6f70 6f75 745f 7261 7465 3d56 616c  dropout_rate=Val
-0002c430: 6964 6174 6f72 2e63 6865 636b 5f6e 6f6e  idator.check_non
-0002c440: 5f6e 6567 6174 6976 655f 666c 6f61 742c  _negative_float,
-0002c450: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0002c460: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002c470: 2068 6964 6465 6e5f 6472 6f70 6f75 745f   hidden_dropout_
-0002c480: 7261 7465 3d56 616c 6964 6174 6f72 2e63  rate=Validator.c
-0002c490: 6865 636b 5f6e 6f6e 5f6e 6567 6174 6976  heck_non_negativ
-0002c4a0: 655f 666c 6f61 742c 0a20 2020 2020 2020  e_float,.       
-0002c4b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002c4c0: 2020 2020 2020 2020 2070 6f73 745f 6c61           post_la
-0002c4d0: 7965 726e 6f72 6d5f 7265 7369 6475 616c  yernorm_residual
-0002c4e0: 3d56 616c 6964 6174 6f72 2e63 6865 636b  =Validator.check
-0002c4f0: 5f62 6f6f 6c2c 0a20 2020 2020 2020 2020  _bool,.         
-0002c500: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002c510: 2020 2020 2020 206c 6179 6572 6e6f 726d         layernorm
-0002c520: 5f63 6f6d 7075 7465 5f74 7970 653d 5f76  _compute_type=_v
-0002c530: 616c 6964 5f76 616c 7565 5f63 6865 636b  alid_value_check
-0002c540: 7328 5b6d 7374 7970 652e 666c 6f61 7433  s([mstype.float3
-0002c550: 322c 0a20 2020 2020 2020 2020 2020 2020  2,.             
-0002c560: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002c570: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002c580: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002c590: 2020 2020 2020 2020 2020 2020 2020 206d                 m
-0002c5a0: 7374 7970 652e 666c 6f61 7431 362c 206d  stype.float16, m
-0002c5b0: 7374 7970 652e 6266 6c6f 6174 3136 5d2c  stype.bfloat16],
-0002c5c0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0002c5d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002c5e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002c5f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002c600: 2020 2020 2020 2020 2020 2020 2254 7261              "Tra
-0002c610: 6e73 666f 726d 6572 4465 636f 6465 7222  nsformerDecoder"
-0002c620: 292c 0a20 2020 2020 2020 2020 2020 2020  ),.             
-0002c630: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002c640: 2020 2073 6f66 746d 6178 5f63 6f6d 7075     softmax_compu
-0002c650: 7465 5f74 7970 653d 5f76 616c 6964 5f76  te_type=_valid_v
-0002c660: 616c 7565 5f63 6865 636b 7328 5b6d 7374  alue_checks([mst
-0002c670: 7970 652e 666c 6f61 7433 322c 0a20 2020  ype.float32,.   
-0002c680: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002c690: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002c6a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002c6b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002c6c0: 2020 2020 2020 206d 7374 7970 652e 666c         mstype.fl
-0002c6d0: 6f61 7431 362c 206d 7374 7970 652e 6266  oat16, mstype.bf
-0002c6e0: 6c6f 6174 3136 5d2c 0a20 2020 2020 2020  loat16],.       
-0002c6f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002c700: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002c710: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002c720: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002c730: 2020 2254 7261 6e73 666f 726d 6572 4465    "TransformerDe
-0002c740: 636f 6465 7222 292c 0a20 2020 2020 2020  coder"),.       
-0002c750: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002c760: 2020 2020 2020 2020 2070 6172 616d 5f69           param_i
-0002c770: 6e69 745f 7479 7065 3d5f 7661 6c69 645f  nit_type=_valid_
-0002c780: 7661 6c75 655f 6368 6563 6b73 285b 6d73  value_checks([ms
-0002c790: 7479 7065 2e66 6c6f 6174 3332 2c20 6d73  type.float32, ms
-0002c7a0: 7479 7065 2e66 6c6f 6174 3136 2c20 6d73  type.float16, ms
-0002c7b0: 7479 7065 2e62 666c 6f61 7431 365d 2c0a  type.bfloat16],.
+0002c290: 2020 2020 2020 2020 6e75 6d5f 6865 6164          num_head
+0002c2a0: 733d 322c 2073 7263 5f73 6571 5f6c 656e  s=2, src_seq_len
+0002c2b0: 6774 683d 3230 2c20 7467 745f 7365 715f  gth=20, tgt_seq_
+0002c2c0: 6c65 6e67 7468 3d31 3029 0a20 2020 2020  length=10).     
+0002c2d0: 2020 2020 2020 203e 3e3e 2065 6e63 6f64         >>> encod
+0002c2e0: 6572 5f69 6e70 7574 5f76 616c 7565 203d  er_input_value =
+0002c2f0: 2054 656e 736f 7228 6e70 2e6f 6e65 7328   Tensor(np.ones(
+0002c300: 2832 2c20 3230 2c20 3634 2929 2c20 6d73  (2, 20, 64)), ms
+0002c310: 7479 7065 2e66 6c6f 6174 3332 290a 2020  type.float32).  
+0002c320: 2020 2020 2020 2020 2020 3e3e 3e20 6465            >>> de
+0002c330: 636f 6465 725f 696e 7075 745f 7661 6c75  coder_input_valu
+0002c340: 6520 3d20 5465 6e73 6f72 286e 702e 6f6e  e = Tensor(np.on
+0002c350: 6573 2828 322c 2031 302c 2036 3429 292c  es((2, 10, 64)),
+0002c360: 206d 7374 7970 652e 666c 6f61 7433 3229   mstype.float32)
+0002c370: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
+0002c380: 2064 6563 6f64 6572 5f69 6e70 7574 5f6d   decoder_input_m
+0002c390: 6173 6b20 3d20 5465 6e73 6f72 286e 702e  ask = Tensor(np.
+0002c3a0: 6f6e 6573 2828 322c 2031 302c 2031 3029  ones((2, 10, 10)
+0002c3b0: 292c 206d 7374 7970 652e 666c 6f61 7431  ), mstype.float1
+0002c3c0: 3629 0a20 2020 2020 2020 2020 2020 203e  6).            >
+0002c3d0: 3e3e 206d 656d 6f72 795f 6d61 736b 203d  >> memory_mask =
+0002c3e0: 2054 656e 736f 7228 6e70 2e6f 6e65 7328   Tensor(np.ones(
+0002c3f0: 2832 2c20 3130 2c20 3230 2929 2c20 6d73  (2, 10, 20)), ms
+0002c400: 7479 7065 2e66 6c6f 6174 3136 290a 2020  type.float16).  
+0002c410: 2020 2020 2020 2020 2020 3e3e 3e20 6f75            >>> ou
+0002c420: 7470 7574 2c20 7061 7374 203d 206d 6f64  tput, past = mod
+0002c430: 656c 2864 6563 6f64 6572 5f69 6e70 7574  el(decoder_input
+0002c440: 5f76 616c 7565 2c20 6465 636f 6465 725f  _value, decoder_
+0002c450: 696e 7075 745f 6d61 736b 2c20 656e 636f  input_mask, enco
+0002c460: 6465 725f 696e 7075 745f 7661 6c75 652c  der_input_value,
+0002c470: 206d 656d 6f72 795f 6d61 736b 290a 2020   memory_mask).  
+0002c480: 2020 2020 2020 2020 2020 3e3e 3e20 7072            >>> pr
+0002c490: 696e 7428 6f75 7470 7574 2e73 6861 7065  int(output.shape
+0002c4a0: 290a 2020 2020 2020 2020 2020 2020 2832  ).            (2
+0002c4b0: 2c20 3130 2c20 3634 290a 2020 2020 2020  , 10, 64).      
+0002c4c0: 2020 2020 2020 3e3e 3e20 7072 696e 7428        >>> print(
+0002c4d0: 6c65 6e28 7061 7374 2929 0a20 2020 2020  len(past)).     
+0002c4e0: 2020 2020 2020 2031 0a20 2020 2020 2020         1.       
+0002c4f0: 2020 2020 203e 3e3e 2070 7269 6e74 2870       >>> print(p
+0002c500: 6173 745b 305d 5b30 5d2e 7368 6170 6529  ast[0][0].shape)
+0002c510: 0a20 2020 2020 2020 2020 2020 2028 322c  .            (2,
+0002c520: 2032 2c20 3332 2c20 3130 290a 2020 2020   2, 32, 10).    
+0002c530: 2020 2020 2020 2020 3e3e 3e20 7072 696e          >>> prin
+0002c540: 7428 7061 7374 5b30 5d5b 315d 2e73 6861  t(past[0][1].sha
+0002c550: 7065 290a 2020 2020 2020 2020 2020 2020  pe).            
+0002c560: 2832 2c20 322c 2031 302c 2033 3229 0a20  (2, 2, 10, 32). 
+0002c570: 2020 2020 2020 2020 2020 203e 3e3e 2070             >>> p
+0002c580: 7269 6e74 2870 6173 745b 305d 5b32 5d2e  rint(past[0][2].
+0002c590: 7368 6170 6529 0a20 2020 2020 2020 2020  shape).         
+0002c5a0: 2020 2028 322c 2032 2c20 3332 2c20 3230     (2, 2, 32, 20
+0002c5b0: 290a 2020 2020 2020 2020 2020 2020 3e3e  ).            >>
+0002c5c0: 3e20 7072 696e 7428 7061 7374 5b30 5d5b  > print(past[0][
+0002c5d0: 335d 2e73 6861 7065 290a 2020 2020 2020  3].shape).      
+0002c5e0: 2020 2020 2020 2832 2c20 322c 2032 302c        (2, 2, 20,
+0002c5f0: 2033 3229 0a20 2020 2022 2222 0a0a 2020   32).    """..  
+0002c600: 2020 405f 4c6f 6741 6374 696f 6e4f 6e63    @_LogActionOnc
+0002c610: 6528 6d5f 6c6f 6767 6572 3d6c 6f67 6765  e(m_logger=logge
+0002c620: 722c 206b 6579 3d27 5472 616e 7366 6f72  r, key='Transfor
+0002c630: 6d65 7244 6563 6f64 6572 272c 0a20 2020  merDecoder',.   
+0002c640: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002c650: 206e 6f5f 7761 726e 696e 673d 5f67 6574   no_warning=_get
+0002c660: 5f70 6172 616c 6c65 6c5f 6d6f 6465 2829  _parallel_mode()
+0002c670: 2069 6e20 2850 6172 616c 6c65 6c4d 6f64   in (ParallelMod
+0002c680: 652e 5354 414e 445f 414c 4f4e 452c 2929  e.STAND_ALONE,))
+0002c690: 0a20 2020 2040 5f61 7267 735f 7479 7065  .    @_args_type
+0002c6a0: 5f76 616c 6964 6174 6f72 5f63 6865 636b  _validator_check
+0002c6b0: 2868 6964 6465 6e5f 7369 7a65 3d56 616c  (hidden_size=Val
+0002c6c0: 6964 6174 6f72 2e63 6865 636b 5f70 6f73  idator.check_pos
+0002c6d0: 6974 6976 655f 696e 742c 0a20 2020 2020  itive_int,.     
+0002c6e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002c6f0: 2020 2020 2020 2020 2020 206e 756d 5f68             num_h
+0002c700: 6561 6473 3d56 616c 6964 6174 6f72 2e63  eads=Validator.c
+0002c710: 6865 636b 5f70 6f73 6974 6976 655f 696e  heck_positive_in
+0002c720: 742c 0a20 2020 2020 2020 2020 2020 2020  t,.             
+0002c730: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002c740: 2020 2066 666e 5f68 6964 6465 6e5f 7369     ffn_hidden_si
+0002c750: 7a65 3d56 616c 6964 6174 6f72 2e63 6865  ze=Validator.che
+0002c760: 636b 5f70 6f73 6974 6976 655f 696e 742c  ck_positive_int,
+0002c770: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0002c780: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002c790: 2073 7263 5f73 6571 5f6c 656e 6774 683d   src_seq_length=
+0002c7a0: 5661 6c69 6461 746f 722e 6368 6563 6b5f  Validator.check_
+0002c7b0: 706f 7369 7469 7665 5f69 6e74 2c0a 2020  positive_int,.  
 0002c7c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002c7d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002c7e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002c7f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002c800: 2020 2020 2254 7261 6e73 666f 726d 6572      "Transformer
-0002c810: 4465 636f 6465 7222 292c 0a20 2020 2020  Decoder"),.     
-0002c820: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002c830: 2020 2020 2020 2020 2020 2070 6172 616c             paral
-0002c840: 6c65 6c5f 636f 6e66 6967 3d5f 7661 6c69  lel_config=_vali
-0002c850: 645f 7479 7065 5f63 6865 636b 7328 5b54  d_type_checks([T
-0002c860: 7261 6e73 666f 726d 6572 4f70 5061 7261  ransformerOpPara
-0002c870: 6c6c 656c 436f 6e66 6967 5d2c 0a20 2020  llelConfig],.   
-0002c880: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002c890: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002c7d0: 2020 2020 2020 2020 2020 2020 2020 6e75                nu
+0002c7e0: 6d5f 6c61 7965 7273 3d56 616c 6964 6174  m_layers=Validat
+0002c7f0: 6f72 2e63 6865 636b 5f70 6f73 6974 6976  or.check_positiv
+0002c800: 655f 696e 742c 0a20 2020 2020 2020 2020  e_int,.         
+0002c810: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002c820: 2020 2020 2020 2074 6774 5f73 6571 5f6c         tgt_seq_l
+0002c830: 656e 6774 683d 5661 6c69 6461 746f 722e  ength=Validator.
+0002c840: 6368 6563 6b5f 706f 7369 7469 7665 5f69  check_positive_i
+0002c850: 6e74 2c0a 2020 2020 2020 2020 2020 2020  nt,.            
+0002c860: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002c870: 2020 2020 6f66 6673 6574 3d56 616c 6964      offset=Valid
+0002c880: 6174 6f72 2e63 6865 636b 5f6e 6f6e 5f6e  ator.check_non_n
+0002c890: 6567 6174 6976 655f 696e 742c 0a20 2020  egative_int,.   
 0002c8a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002c8b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002c8c0: 2254 7261 6e73 666f 726d 6572 4465 636f  "TransformerDeco
-0002c8d0: 6465 7222 292c 0a20 2020 2020 2020 2020  der"),.         
-0002c8e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002c8f0: 2020 2020 2020 2075 7365 5f70 6173 743d         use_past=
-0002c900: 5661 6c69 6461 746f 722e 6368 6563 6b5f  Validator.check_
-0002c910: 626f 6f6c 290a 2020 2020 6465 6620 5f5f  bool).    def __
-0002c920: 696e 6974 5f5f 2873 656c 662c 0a20 2020  init__(self,.   
-0002c930: 2020 2020 2020 2020 2020 2020 2020 6e75                nu
-0002c940: 6d5f 6c61 7965 7273 2c0a 2020 2020 2020  m_layers,.      
-0002c950: 2020 2020 2020 2020 2020 2062 6174 6368             batch
-0002c960: 5f73 697a 652c 0a20 2020 2020 2020 2020  _size,.         
-0002c970: 2020 2020 2020 2020 6869 6464 656e 5f73          hidden_s
-0002c980: 697a 652c 0a20 2020 2020 2020 2020 2020  ize,.           
-0002c990: 2020 2020 2020 6666 6e5f 6869 6464 656e        ffn_hidden
-0002c9a0: 5f73 697a 652c 0a20 2020 2020 2020 2020  _size,.         
-0002c9b0: 2020 2020 2020 2020 7372 635f 7365 715f          src_seq_
-0002c9c0: 6c65 6e67 7468 2c0a 2020 2020 2020 2020  length,.        
-0002c9d0: 2020 2020 2020 2020 2074 6774 5f73 6571           tgt_seq
-0002c9e0: 5f6c 656e 6774 682c 0a20 2020 2020 2020  _length,.       
-0002c9f0: 2020 2020 2020 2020 2020 6e75 6d5f 6865            num_he
-0002ca00: 6164 732c 0a20 2020 2020 2020 2020 2020  ads,.           
-0002ca10: 2020 2020 2020 6174 7465 6e74 696f 6e5f        attention_
-0002ca20: 6472 6f70 6f75 745f 7261 7465 3d30 2e31  dropout_rate=0.1
-0002ca30: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-0002ca40: 2020 2068 6964 6465 6e5f 6472 6f70 6f75     hidden_dropou
-0002ca50: 745f 7261 7465 3d30 2e31 2c0a 2020 2020  t_rate=0.1,.    
-0002ca60: 2020 2020 2020 2020 2020 2020 2070 6f73               pos
-0002ca70: 745f 6c61 7965 726e 6f72 6d5f 7265 7369  t_layernorm_resi
-0002ca80: 6475 616c 3d46 616c 7365 2c0a 2020 2020  dual=False,.    
-0002ca90: 2020 2020 2020 2020 2020 2020 206c 6179               lay
-0002caa0: 6572 6e6f 726d 5f63 6f6d 7075 7465 5f74  ernorm_compute_t
-0002cab0: 7970 653d 6d73 7479 7065 2e66 6c6f 6174  ype=mstype.float
-0002cac0: 3332 2c0a 2020 2020 2020 2020 2020 2020  32,.            
-0002cad0: 2020 2020 2073 6f66 746d 6178 5f63 6f6d       softmax_com
-0002cae0: 7075 7465 5f74 7970 653d 6d73 7479 7065  pute_type=mstype
-0002caf0: 2e66 6c6f 6174 3332 2c0a 2020 2020 2020  .float32,.      
-0002cb00: 2020 2020 2020 2020 2020 2070 6172 616d             param
-0002cb10: 5f69 6e69 745f 7479 7065 3d6d 7374 7970  _init_type=mstyp
-0002cb20: 652e 666c 6f61 7433 322c 0a20 2020 2020  e.float32,.     
-0002cb30: 2020 2020 2020 2020 2020 2020 6869 6464              hidd
-0002cb40: 656e 5f61 6374 3d27 6765 6c75 272c 0a20  en_act='gelu',. 
+0002c8b0: 2020 2020 2020 2020 2020 2020 2061 7474               att
+0002c8c0: 656e 7469 6f6e 5f64 726f 706f 7574 5f72  ention_dropout_r
+0002c8d0: 6174 653d 5661 6c69 6461 746f 722e 6368  ate=Validator.ch
+0002c8e0: 6563 6b5f 6e6f 6e5f 6e65 6761 7469 7665  eck_non_negative
+0002c8f0: 5f66 6c6f 6174 2c0a 2020 2020 2020 2020  _float,.        
+0002c900: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002c910: 2020 2020 2020 2020 6869 6464 656e 5f64          hidden_d
+0002c920: 726f 706f 7574 5f72 6174 653d 5661 6c69  ropout_rate=Vali
+0002c930: 6461 746f 722e 6368 6563 6b5f 6e6f 6e5f  dator.check_non_
+0002c940: 6e65 6761 7469 7665 5f66 6c6f 6174 2c0a  negative_float,.
+0002c950: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002c960: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002c970: 706f 7374 5f6c 6179 6572 6e6f 726d 5f72  post_layernorm_r
+0002c980: 6573 6964 7561 6c3d 5661 6c69 6461 746f  esidual=Validato
+0002c990: 722e 6368 6563 6b5f 626f 6f6c 2c0a 2020  r.check_bool,.  
+0002c9a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002c9b0: 2020 2020 2020 2020 2020 2020 2020 6c61                la
+0002c9c0: 7965 726e 6f72 6d5f 636f 6d70 7574 655f  yernorm_compute_
+0002c9d0: 7479 7065 3d5f 7661 6c69 645f 7661 6c75  type=_valid_valu
+0002c9e0: 655f 6368 6563 6b73 285b 6d73 7479 7065  e_checks([mstype
+0002c9f0: 2e66 6c6f 6174 3332 2c0a 2020 2020 2020  .float32,.      
+0002ca00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002ca10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002ca20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002ca30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002ca40: 2020 2020 2020 6d73 7479 7065 2e66 6c6f        mstype.flo
+0002ca50: 6174 3136 2c20 6d73 7479 7065 2e62 666c  at16, mstype.bfl
+0002ca60: 6f61 7431 365d 2c0a 2020 2020 2020 2020  oat16],.        
+0002ca70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002ca80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002ca90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002caa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002cab0: 2020 2022 5472 616e 7366 6f72 6d65 7244     "TransformerD
+0002cac0: 6563 6f64 6572 2229 2c0a 2020 2020 2020  ecoder"),.      
+0002cad0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002cae0: 2020 2020 2020 2020 2020 736f 6674 6d61            softma
+0002caf0: 785f 636f 6d70 7574 655f 7479 7065 3d5f  x_compute_type=_
+0002cb00: 7661 6c69 645f 7661 6c75 655f 6368 6563  valid_value_chec
+0002cb10: 6b73 285b 6d73 7479 7065 2e66 6c6f 6174  ks([mstype.float
+0002cb20: 3332 2c0a 2020 2020 2020 2020 2020 2020  32,.            
+0002cb30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002cb40: 2020 2020 2020 2020 2020 2020 2020 2020                  
 0002cb50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002cb60: 6c61 6d62 6461 5f66 756e 633d 4e6f 6e65  lambda_func=None
-0002cb70: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-0002cb80: 2020 2075 7365 5f70 6173 743d 4661 6c73     use_past=Fals
-0002cb90: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
-0002cba0: 2020 2020 6f66 6673 6574 3d30 2c0a 2020      offset=0,.  
-0002cbb0: 2020 2020 2020 2020 2020 2020 2020 206d                 m
-0002cbc0: 6f65 5f63 6f6e 6669 673d 6465 6661 756c  oe_config=defaul
-0002cbd0: 745f 6d6f 655f 636f 6e66 6967 2c0a 2020  t_moe_config,.  
-0002cbe0: 2020 2020 2020 2020 2020 2020 2020 2070                 p
-0002cbf0: 6172 616c 6c65 6c5f 636f 6e66 6967 3d64  arallel_config=d
-0002cc00: 6566 6175 6c74 5f74 7261 6e73 666f 726d  efault_transform
-0002cc10: 6572 5f63 6f6e 6669 6729 3a0a 2020 2020  er_config):.    
-0002cc20: 2020 2020 7375 7065 7228 5472 616e 7366      super(Transf
-0002cc30: 6f72 6d65 7244 6563 6f64 6572 2c20 7365  ormerDecoder, se
-0002cc40: 6c66 292e 5f5f 696e 6974 5f5f 2829 0a20  lf).__init__(). 
-0002cc50: 2020 2020 2020 205f 6368 6563 6b5f 6d6f         _check_mo
-0002cc60: 655f 636f 6e66 6967 286d 6f65 5f63 6f6e  e_config(moe_con
-0002cc70: 6669 672c 2070 6172 616c 6c65 6c5f 636f  fig, parallel_co
-0002cc80: 6e66 6967 290a 2020 2020 2020 2020 5f63  nfig).        _c
-0002cc90: 6865 636b 5f63 6f6e 6669 6728 7061 7261  heck_config(para
-0002cca0: 6c6c 656c 5f63 6f6e 6669 6729 0a20 2020  llel_config).   
-0002ccb0: 2020 2020 2073 656c 662e 7573 655f 6d6f       self.use_mo
-0002ccc0: 6520 3d20 286d 6f65 5f63 6f6e 6669 672e  e = (moe_config.
-0002ccd0: 6578 7065 7274 5f6e 756d 203e 2031 290a  expert_num > 1).
-0002cce0: 2020 2020 2020 2020 6966 2062 6174 6368          if batch
-0002ccf0: 5f73 697a 6520 6f72 2075 7365 5f70 6173  _size or use_pas
-0002cd00: 743a 0a20 2020 2020 2020 2020 2020 2056  t:.            V
-0002cd10: 616c 6964 6174 6f72 2e63 6865 636b 5f70  alidator.check_p
-0002cd20: 6f73 6974 6976 655f 696e 7428 6261 7463  ositive_int(batc
-0002cd30: 685f 7369 7a65 290a 2020 2020 2020 2020  h_size).        
-0002cd40: 636f 6e66 6967 5f74 6f5f 6c61 7965 7220  config_to_layer 
-0002cd50: 3d20 7061 7261 6c6c 656c 5f63 6f6e 6669  = parallel_confi
-0002cd60: 672e 6d6f 655f 7061 7261 6c6c 656c 5f63  g.moe_parallel_c
-0002cd70: 6f6e 6669 6720 6966 2073 656c 662e 7573  onfig if self.us
-0002cd80: 655f 6d6f 6520 656c 7365 2070 6172 616c  e_moe else paral
-0002cd90: 6c65 6c5f 636f 6e66 6967 2e64 705f 6d70  lel_config.dp_mp
-0002cda0: 5f63 6f6e 6669 670a 2020 2020 2020 2020  _config.        
-0002cdb0: 6966 205f 6765 745f 7061 7261 6c6c 656c  if _get_parallel
-0002cdc0: 5f6d 6f64 6528 2920 696e 2028 5061 7261  _mode() in (Para
-0002cdd0: 6c6c 656c 4d6f 6465 2e41 5554 4f5f 5041  llelMode.AUTO_PA
-0002cde0: 5241 4c4c 454c 2c29 3a0a 2020 2020 2020  RALLEL,):.      
-0002cdf0: 2020 2020 2020 7365 6c66 2e61 6464 203d        self.add =
-0002ce00: 2050 2e41 6464 2829 0a20 2020 2020 2020   P.Add().       
-0002ce10: 2020 2020 2073 656c 662e 6175 785f 6c6f       self.aux_lo
-0002ce20: 7373 203d 2054 656e 736f 7228 302e 302c  ss = Tensor(0.0,
-0002ce30: 206d 7374 7970 652e 666c 6f61 7433 3229   mstype.float32)
-0002ce40: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-0002ce50: 662e 6e75 6d5f 6c61 7965 7273 203d 206e  f.num_layers = n
-0002ce60: 756d 5f6c 6179 6572 730a 2020 2020 2020  um_layers.      
-0002ce70: 2020 2020 2020 7365 6c66 2e62 6c6f 636b        self.block
-0002ce80: 7320 3d20 6e6e 2e43 656c 6c4c 6973 7428  s = nn.CellList(
-0002ce90: 290a 0a20 2020 2020 2020 2020 2020 2066  )..            f
-0002cea0: 6f72 2069 2069 6e20 7261 6e67 6528 6e75  or i in range(nu
-0002ceb0: 6d5f 6c61 7965 7273 293a 0a20 2020 2020  m_layers):.     
-0002cec0: 2020 2020 2020 2020 2020 2062 6c6f 636b             block
-0002ced0: 203d 2054 7261 6e73 666f 726d 6572 4465   = TransformerDe
-0002cee0: 636f 6465 724c 6179 6572 2868 6964 6465  coderLayer(hidde
-0002cef0: 6e5f 7369 7a65 3d68 6964 6465 6e5f 7369  n_size=hidden_si
-0002cf00: 7a65 2c0a 2020 2020 2020 2020 2020 2020  ze,.            
-0002cf10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002cf20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002cf30: 2020 2020 6261 7463 685f 7369 7a65 3d62      batch_size=b
-0002cf40: 6174 6368 5f73 697a 652c 0a20 2020 2020  atch_size,.     
-0002cf50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002cf60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002cf70: 2020 2020 2020 2020 2020 2066 666e 5f68             ffn_h
-0002cf80: 6964 6465 6e5f 7369 7a65 3d66 666e 5f68  idden_size=ffn_h
-0002cf90: 6964 6465 6e5f 7369 7a65 2c0a 2020 2020  idden_size,.    
-0002cfa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002cfb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002cfc0: 2020 2020 2020 2020 2020 2020 7372 635f              src_
-0002cfd0: 7365 715f 6c65 6e67 7468 3d73 7263 5f73  seq_length=src_s
-0002cfe0: 6571 5f6c 656e 6774 682c 0a20 2020 2020  eq_length,.     
-0002cff0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002d000: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002d010: 2020 2020 2020 2020 2020 2074 6774 5f73             tgt_s
-0002d020: 6571 5f6c 656e 6774 683d 7467 745f 7365  eq_length=tgt_se
-0002d030: 715f 6c65 6e67 7468 2c0a 2020 2020 2020  q_length,.      
-0002d040: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002d050: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002d060: 2020 2020 2020 2020 2020 6174 7465 6e74            attent
-0002d070: 696f 6e5f 6472 6f70 6f75 745f 7261 7465  ion_dropout_rate
-0002d080: 3d61 7474 656e 7469 6f6e 5f64 726f 706f  =attention_dropo
-0002d090: 7574 5f72 6174 652c 0a20 2020 2020 2020  ut_rate,.       
-0002d0a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002d0b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002d0c0: 2020 2020 2020 2020 2068 6964 6465 6e5f           hidden_
-0002d0d0: 6472 6f70 6f75 745f 7261 7465 3d68 6964  dropout_rate=hid
-0002d0e0: 6465 6e5f 6472 6f70 6f75 745f 7261 7465  den_dropout_rate
-0002d0f0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-0002d100: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002d110: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002d120: 2020 6e75 6d5f 6865 6164 733d 6e75 6d5f    num_heads=num_
-0002d130: 6865 6164 732c 0a20 2020 2020 2020 2020  heads,.         
-0002d140: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002d150: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002d160: 2020 2020 2020 206c 6179 6572 6e6f 726d         layernorm
-0002d170: 5f63 6f6d 7075 7465 5f74 7970 653d 6c61  _compute_type=la
-0002d180: 7965 726e 6f72 6d5f 636f 6d70 7574 655f  yernorm_compute_
-0002d190: 7479 7065 2c0a 2020 2020 2020 2020 2020  type,.          
-0002d1a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002d1b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002d1c0: 2020 2020 2020 736f 6674 6d61 785f 636f        softmax_co
-0002d1d0: 6d70 7574 655f 7479 7065 3d73 6f66 746d  mpute_type=softm
-0002d1e0: 6178 5f63 6f6d 7075 7465 5f74 7970 652c  ax_compute_type,
-0002d1f0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0002d200: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002d210: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002d220: 2068 6964 6465 6e5f 6163 743d 6869 6464   hidden_act=hidd
-0002d230: 656e 5f61 6374 2c0a 2020 2020 2020 2020  en_act,.        
-0002d240: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002d250: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002d260: 2020 2020 2020 2020 7573 655f 7061 7374          use_past
-0002d270: 3d75 7365 5f70 6173 742c 0a20 2020 2020  =use_past,.     
-0002d280: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002d290: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002d2a0: 2020 2020 2020 2020 2020 2070 6172 616d             param
-0002d2b0: 5f69 6e69 745f 7479 7065 3d70 6172 616d  _init_type=param
-0002d2c0: 5f69 6e69 745f 7479 7065 2c0a 2020 2020  _init_type,.    
-0002d2d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002d2e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002d2f0: 2020 2020 2020 2020 2020 2020 706f 7374              post
-0002d300: 5f6c 6179 6572 6e6f 726d 5f72 6573 6964  _layernorm_resid
-0002d310: 7561 6c3d 706f 7374 5f6c 6179 6572 6e6f  ual=post_layerno
-0002d320: 726d 5f72 6573 6964 7561 6c2c 0a20 2020  rm_residual,.   
-0002d330: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002d340: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002d350: 2020 2020 2020 2020 2020 2020 206d 6f65               moe
-0002d360: 5f63 6f6e 6669 673d 6d6f 655f 636f 6e66  _config=moe_conf
-0002d370: 6967 2c0a 2020 2020 2020 2020 2020 2020  ig,.            
-0002d380: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002d390: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002d3a0: 2020 2020 7061 7261 6c6c 656c 5f63 6f6e      parallel_con
-0002d3b0: 6669 673d 636f 6e66 6967 5f74 6f5f 6c61  fig=config_to_la
-0002d3c0: 7965 7229 0a20 2020 2020 2020 2020 2020  yer).           
-0002d3d0: 2020 2020 2023 2049 6620 7468 6520 7573       # If the us
-0002d3e0: 6572 2064 6f65 736e 2774 2070 6173 7320  er doesn't pass 
-0002d3f0: 7468 6520 6675 7369 6f6e 2066 756e 6374  the fusion funct
-0002d400: 696f 6e2c 2075 7365 2074 6865 2064 6566  ion, use the def
-0002d410: 6175 6c74 206f 6e65 0a20 2020 2020 2020  ault one.       
-0002d420: 2020 2020 2020 2020 2069 6620 6e6f 7420           if not 
-0002d430: 6c61 6d62 6461 5f66 756e 633a 0a20 2020  lambda_func:.   
-0002d440: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002d450: 206c 616d 6264 615f 6675 6e63 203d 205f   lambda_func = _
-0002d460: 6765 745f 6c61 6d62 6461 5f66 756e 6328  get_lambda_func(
-0002d470: 290a 0a20 2020 2020 2020 2020 2020 2020  )..             
-0002d480: 2020 206c 616d 6264 615f 6675 6e63 2862     lambda_func(b
-0002d490: 6c6f 636b 2c20 6c61 7965 725f 6964 3d69  lock, layer_id=i
-0002d4a0: 2c20 6c61 7965 7273 3d6e 756d 5f6c 6179  , layers=num_lay
-0002d4b0: 6572 732c 0a20 2020 2020 2020 2020 2020  ers,.           
-0002d4c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002d4d0: 206f 6666 7365 743d 6f66 6673 6574 2c20   offset=offset, 
-0002d4e0: 7061 7261 6c6c 656c 5f63 6f6e 6669 673d  parallel_config=
-0002d4f0: 7061 7261 6c6c 656c 5f63 6f6e 6669 6729  parallel_config)
-0002d500: 0a0a 2020 2020 2020 2020 2020 2020 2020  ..              
-0002d510: 2020 7365 6c66 2e62 6c6f 636b 732e 6170    self.blocks.ap
-0002d520: 7065 6e64 2862 6c6f 636b 290a 2020 2020  pend(block).    
-0002d530: 2020 2020 656c 6966 205f 6765 745f 7061      elif _get_pa
-0002d540: 7261 6c6c 656c 5f6d 6f64 6528 2920 6e6f  rallel_mode() no
-0002d550: 7420 696e 2028 5061 7261 6c6c 656c 4d6f  t in (ParallelMo
-0002d560: 6465 2e41 5554 4f5f 5041 5241 4c4c 454c  de.AUTO_PARALLEL
-0002d570: 2c29 3a0a 2020 2020 2020 2020 2020 2020  ,):.            
-0002d580: 7365 6c66 2e61 6464 203d 2050 2e41 6464  self.add = P.Add
-0002d590: 2829 2e73 6861 7264 2828 2829 2c20 2829  ().shard(((), ()
-0002d5a0: 2929 0a20 2020 2020 2020 2020 2020 2073  )).            s
-0002d5b0: 656c 662e 6175 785f 6c6f 7373 203d 2054  elf.aux_loss = T
-0002d5c0: 656e 736f 7228 302e 302c 206d 7374 7970  ensor(0.0, mstyp
-0002d5d0: 652e 666c 6f61 7433 3229 0a20 2020 2020  e.float32).     
-0002d5e0: 2020 2020 2020 206c 6f67 6765 722e 7761         logger.wa
-0002d5f0: 726e 696e 6728 2246 6f72 2070 6172 616c  rning("For paral
-0002d600: 6c65 6c20 6d6f 6465 2c20 7368 6172 6469  lel mode, shardi
-0002d610: 6e67 2070 726f 7061 6761 7469 6f6e 2069  ng propagation i
-0002d620: 7320 7265 636f 6d6d 656e 6465 642c 2079  s recommended, y
-0002d630: 6f75 2063 616e 2075 7365 2069 7420 6279  ou can use it by
-0002d640: 2073 6574 7469 6e67 2022 0a20 2020 2020   setting ".     
+0002cb60: 2020 2020 2020 2020 2020 2020 2020 6d73                ms
+0002cb70: 7479 7065 2e66 6c6f 6174 3136 2c20 6d73  type.float16, ms
+0002cb80: 7479 7065 2e62 666c 6f61 7431 365d 2c0a  type.bfloat16],.
+0002cb90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002cba0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002cbb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002cbc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002cbd0: 2020 2020 2020 2020 2022 5472 616e 7366           "Transf
+0002cbe0: 6f72 6d65 7244 6563 6f64 6572 2229 2c0a  ormerDecoder"),.
+0002cbf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002cc00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002cc10: 7061 7261 6d5f 696e 6974 5f74 7970 653d  param_init_type=
+0002cc20: 5f76 616c 6964 5f76 616c 7565 5f63 6865  _valid_value_che
+0002cc30: 636b 7328 5b6d 7374 7970 652e 666c 6f61  cks([mstype.floa
+0002cc40: 7433 322c 206d 7374 7970 652e 666c 6f61  t32, mstype.floa
+0002cc50: 7431 362c 206d 7374 7970 652e 6266 6c6f  t16, mstype.bflo
+0002cc60: 6174 3136 5d2c 0a20 2020 2020 2020 2020  at16],.         
+0002cc70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002cc80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002cc90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002cca0: 2020 2020 2020 2020 2020 2022 5472 616e             "Tran
+0002ccb0: 7366 6f72 6d65 7244 6563 6f64 6572 2229  sformerDecoder")
+0002ccc0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+0002ccd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002cce0: 2020 7061 7261 6c6c 656c 5f63 6f6e 6669    parallel_confi
+0002ccf0: 673d 5f76 616c 6964 5f74 7970 655f 6368  g=_valid_type_ch
+0002cd00: 6563 6b73 285b 5472 616e 7366 6f72 6d65  ecks([Transforme
+0002cd10: 724f 7050 6172 616c 6c65 6c43 6f6e 6669  rOpParallelConfi
+0002cd20: 675d 2c0a 2020 2020 2020 2020 2020 2020  g],.            
+0002cd30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002cd40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002cd50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002cd60: 2020 2020 2020 2022 5472 616e 7366 6f72         "Transfor
+0002cd70: 6d65 7244 6563 6f64 6572 2229 2c0a 2020  merDecoder"),.  
+0002cd80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002cd90: 2020 2020 2020 2020 2020 2020 2020 7573                us
+0002cda0: 655f 7061 7374 3d56 616c 6964 6174 6f72  e_past=Validator
+0002cdb0: 2e63 6865 636b 5f62 6f6f 6c29 0a20 2020  .check_bool).   
+0002cdc0: 2064 6566 205f 5f69 6e69 745f 5f28 7365   def __init__(se
+0002cdd0: 6c66 2c0a 2020 2020 2020 2020 2020 2020  lf,.            
+0002cde0: 2020 2020 206e 756d 5f6c 6179 6572 732c       num_layers,
+0002cdf0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0002ce00: 2020 6261 7463 685f 7369 7a65 2c0a 2020    batch_size,.  
+0002ce10: 2020 2020 2020 2020 2020 2020 2020 2068                 h
+0002ce20: 6964 6465 6e5f 7369 7a65 2c0a 2020 2020  idden_size,.    
+0002ce30: 2020 2020 2020 2020 2020 2020 2066 666e               ffn
+0002ce40: 5f68 6964 6465 6e5f 7369 7a65 2c0a 2020  _hidden_size,.  
+0002ce50: 2020 2020 2020 2020 2020 2020 2020 2073                 s
+0002ce60: 7263 5f73 6571 5f6c 656e 6774 682c 0a20  rc_seq_length,. 
+0002ce70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002ce80: 7467 745f 7365 715f 6c65 6e67 7468 2c0a  tgt_seq_length,.
+0002ce90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002cea0: 206e 756d 5f68 6561 6473 2c0a 2020 2020   num_heads,.    
+0002ceb0: 2020 2020 2020 2020 2020 2020 2061 7474               att
+0002cec0: 656e 7469 6f6e 5f64 726f 706f 7574 5f72  ention_dropout_r
+0002ced0: 6174 653d 302e 312c 0a20 2020 2020 2020  ate=0.1,.       
+0002cee0: 2020 2020 2020 2020 2020 6869 6464 656e            hidden
+0002cef0: 5f64 726f 706f 7574 5f72 6174 653d 302e  _dropout_rate=0.
+0002cf00: 312c 0a20 2020 2020 2020 2020 2020 2020  1,.             
+0002cf10: 2020 2020 706f 7374 5f6c 6179 6572 6e6f      post_layerno
+0002cf20: 726d 5f72 6573 6964 7561 6c3d 4661 6c73  rm_residual=Fals
+0002cf30: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
+0002cf40: 2020 2020 6c61 7965 726e 6f72 6d5f 636f      layernorm_co
+0002cf50: 6d70 7574 655f 7479 7065 3d6d 7374 7970  mpute_type=mstyp
+0002cf60: 652e 666c 6f61 7433 322c 0a20 2020 2020  e.float32,.     
+0002cf70: 2020 2020 2020 2020 2020 2020 736f 6674              soft
+0002cf80: 6d61 785f 636f 6d70 7574 655f 7479 7065  max_compute_type
+0002cf90: 3d6d 7374 7970 652e 666c 6f61 7433 322c  =mstype.float32,
+0002cfa0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0002cfb0: 2020 7061 7261 6d5f 696e 6974 5f74 7970    param_init_typ
+0002cfc0: 653d 6d73 7479 7065 2e66 6c6f 6174 3332  e=mstype.float32
+0002cfd0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+0002cfe0: 2020 2068 6964 6465 6e5f 6163 743d 2767     hidden_act='g
+0002cff0: 656c 7527 2c0a 2020 2020 2020 2020 2020  elu',.          
+0002d000: 2020 2020 2020 206c 616d 6264 615f 6675         lambda_fu
+0002d010: 6e63 3d4e 6f6e 652c 0a20 2020 2020 2020  nc=None,.       
+0002d020: 2020 2020 2020 2020 2020 7573 655f 7061            use_pa
+0002d030: 7374 3d46 616c 7365 2c0a 2020 2020 2020  st=False,.      
+0002d040: 2020 2020 2020 2020 2020 206f 6666 7365             offse
+0002d050: 743d 302c 0a20 2020 2020 2020 2020 2020  t=0,.           
+0002d060: 2020 2020 2020 6d6f 655f 636f 6e66 6967        moe_config
+0002d070: 3d64 6566 6175 6c74 5f6d 6f65 5f63 6f6e  =default_moe_con
+0002d080: 6669 672c 0a20 2020 2020 2020 2020 2020  fig,.           
+0002d090: 2020 2020 2020 7061 7261 6c6c 656c 5f63        parallel_c
+0002d0a0: 6f6e 6669 673d 6465 6661 756c 745f 7472  onfig=default_tr
+0002d0b0: 616e 7366 6f72 6d65 725f 636f 6e66 6967  ansformer_config
+0002d0c0: 293a 0a20 2020 2020 2020 2073 7570 6572  ):.        super
+0002d0d0: 2854 7261 6e73 666f 726d 6572 4465 636f  (TransformerDeco
+0002d0e0: 6465 722c 2073 656c 6629 2e5f 5f69 6e69  der, self).__ini
+0002d0f0: 745f 5f28 290a 2020 2020 2020 2020 5f63  t__().        _c
+0002d100: 6865 636b 5f6d 6f65 5f63 6f6e 6669 6728  heck_moe_config(
+0002d110: 6d6f 655f 636f 6e66 6967 2c20 7061 7261  moe_config, para
+0002d120: 6c6c 656c 5f63 6f6e 6669 6729 0a20 2020  llel_config).   
+0002d130: 2020 2020 205f 6368 6563 6b5f 636f 6e66       _check_conf
+0002d140: 6967 2870 6172 616c 6c65 6c5f 636f 6e66  ig(parallel_conf
+0002d150: 6967 290a 2020 2020 2020 2020 7365 6c66  ig).        self
+0002d160: 2e75 7365 5f6d 6f65 203d 2028 6d6f 655f  .use_moe = (moe_
+0002d170: 636f 6e66 6967 2e65 7870 6572 745f 6e75  config.expert_nu
+0002d180: 6d20 3e20 3129 0a20 2020 2020 2020 2069  m > 1).        i
+0002d190: 6620 6261 7463 685f 7369 7a65 206f 7220  f batch_size or 
+0002d1a0: 7573 655f 7061 7374 3a0a 2020 2020 2020  use_past:.      
+0002d1b0: 2020 2020 2020 5661 6c69 6461 746f 722e        Validator.
+0002d1c0: 6368 6563 6b5f 706f 7369 7469 7665 5f69  check_positive_i
+0002d1d0: 6e74 2862 6174 6368 5f73 697a 6529 0a20  nt(batch_size). 
+0002d1e0: 2020 2020 2020 2063 6f6e 6669 675f 746f         config_to
+0002d1f0: 5f6c 6179 6572 203d 2070 6172 616c 6c65  _layer = paralle
+0002d200: 6c5f 636f 6e66 6967 2e6d 6f65 5f70 6172  l_config.moe_par
+0002d210: 616c 6c65 6c5f 636f 6e66 6967 2069 6620  allel_config if 
+0002d220: 7365 6c66 2e75 7365 5f6d 6f65 2065 6c73  self.use_moe els
+0002d230: 6520 7061 7261 6c6c 656c 5f63 6f6e 6669  e parallel_confi
+0002d240: 672e 6470 5f6d 705f 636f 6e66 6967 0a20  g.dp_mp_config. 
+0002d250: 2020 2020 2020 2069 6620 5f67 6574 5f70         if _get_p
+0002d260: 6172 616c 6c65 6c5f 6d6f 6465 2829 2069  arallel_mode() i
+0002d270: 6e20 2850 6172 616c 6c65 6c4d 6f64 652e  n (ParallelMode.
+0002d280: 4155 544f 5f50 4152 414c 4c45 4c2c 293a  AUTO_PARALLEL,):
+0002d290: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
+0002d2a0: 662e 6164 6420 3d20 502e 4164 6428 290a  f.add = P.Add().
+0002d2b0: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+0002d2c0: 2e61 7578 5f6c 6f73 7320 3d20 5465 6e73  .aux_loss = Tens
+0002d2d0: 6f72 2830 2e30 2c20 6d73 7479 7065 2e66  or(0.0, mstype.f
+0002d2e0: 6c6f 6174 3332 290a 2020 2020 2020 2020  loat32).        
+0002d2f0: 2020 2020 7365 6c66 2e6e 756d 5f6c 6179      self.num_lay
+0002d300: 6572 7320 3d20 6e75 6d5f 6c61 7965 7273  ers = num_layers
+0002d310: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
+0002d320: 662e 626c 6f63 6b73 203d 206e 6e2e 4365  f.blocks = nn.Ce
+0002d330: 6c6c 4c69 7374 2829 0a0a 2020 2020 2020  llList()..      
+0002d340: 2020 2020 2020 666f 7220 6920 696e 2072        for i in r
+0002d350: 616e 6765 286e 756d 5f6c 6179 6572 7329  ange(num_layers)
+0002d360: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
+0002d370: 2020 626c 6f63 6b20 3d20 5472 616e 7366    block = Transf
+0002d380: 6f72 6d65 7244 6563 6f64 6572 4c61 7965  ormerDecoderLaye
+0002d390: 7228 6869 6464 656e 5f73 697a 653d 6869  r(hidden_size=hi
+0002d3a0: 6464 656e 5f73 697a 652c 0a20 2020 2020  dden_size,.     
+0002d3b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002d3c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002d3d0: 2020 2020 2020 2020 2020 2062 6174 6368             batch
+0002d3e0: 5f73 697a 653d 6261 7463 685f 7369 7a65  _size=batch_size
+0002d3f0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+0002d400: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002d410: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002d420: 2020 6666 6e5f 6869 6464 656e 5f73 697a    ffn_hidden_siz
+0002d430: 653d 6666 6e5f 6869 6464 656e 5f73 697a  e=ffn_hidden_siz
+0002d440: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
+0002d450: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002d460: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002d470: 2020 2073 7263 5f73 6571 5f6c 656e 6774     src_seq_lengt
+0002d480: 683d 7372 635f 7365 715f 6c65 6e67 7468  h=src_seq_length
+0002d490: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+0002d4a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002d4b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002d4c0: 2020 7467 745f 7365 715f 6c65 6e67 7468    tgt_seq_length
+0002d4d0: 3d74 6774 5f73 6571 5f6c 656e 6774 682c  =tgt_seq_length,
+0002d4e0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0002d4f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002d500: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002d510: 2061 7474 656e 7469 6f6e 5f64 726f 706f   attention_dropo
+0002d520: 7574 5f72 6174 653d 6174 7465 6e74 696f  ut_rate=attentio
+0002d530: 6e5f 6472 6f70 6f75 745f 7261 7465 2c0a  n_dropout_rate,.
+0002d540: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002d550: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002d560: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002d570: 6869 6464 656e 5f64 726f 706f 7574 5f72  hidden_dropout_r
+0002d580: 6174 653d 6869 6464 656e 5f64 726f 706f  ate=hidden_dropo
+0002d590: 7574 5f72 6174 652c 0a20 2020 2020 2020  ut_rate,.       
+0002d5a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002d5b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002d5c0: 2020 2020 2020 2020 206e 756d 5f68 6561           num_hea
+0002d5d0: 6473 3d6e 756d 5f68 6561 6473 2c0a 2020  ds=num_heads,.  
+0002d5e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002d5f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002d600: 2020 2020 2020 2020 2020 2020 2020 6c61                la
+0002d610: 7965 726e 6f72 6d5f 636f 6d70 7574 655f  yernorm_compute_
+0002d620: 7479 7065 3d6c 6179 6572 6e6f 726d 5f63  type=layernorm_c
+0002d630: 6f6d 7075 7465 5f74 7970 652c 0a20 2020  ompute_type,.   
+0002d640: 2020 2020 2020 2020 2020 2020 2020 2020                  
 0002d650: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002d660: 2020 2020 2020 2227 7365 745f 6175 746f        "'set_auto
-0002d670: 5f70 6172 616c 6c65 6c5f 636f 6e74 6578  _parallel_contex
-0002d680: 7428 7061 7261 6c6c 656c 5f6d 6f64 653d  t(parallel_mode=
-0002d690: 5061 7261 6c6c 656c 4d6f 6465 2e41 5554  ParallelMode.AUT
-0002d6a0: 4f5f 5041 5241 4c4c 454c 2c20 220a 2020  O_PARALLEL, ".  
+0002d660: 2020 2020 2020 2020 2020 2020 2073 6f66               sof
+0002d670: 746d 6178 5f63 6f6d 7075 7465 5f74 7970  tmax_compute_typ
+0002d680: 653d 736f 6674 6d61 785f 636f 6d70 7574  e=softmax_comput
+0002d690: 655f 7479 7065 2c0a 2020 2020 2020 2020  e_type,.        
+0002d6a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 0002d6b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002d6c0: 2020 2020 2020 2020 2022 7365 6172 6368           "search
-0002d6d0: 5f6d 6f64 653d 5c22 7368 6172 6469 6e67  _mode=\"sharding
-0002d6e0: 5f70 726f 7061 6761 7469 6f6e 5c22 2927  _propagation\")'
-0002d6f0: 2061 6e64 2022 0a20 2020 2020 2020 2020   and ".         
-0002d700: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002d710: 2020 2227 7365 745f 616c 676f 5f70 6172    "'set_algo_par
-0002d720: 616d 6574 6572 7328 656c 656d 656e 7477  ameters(elementw
-0002d730: 6973 655f 6f70 5f73 7472 6174 6567 795f  ise_op_strategy_
-0002d740: 666f 6c6c 6f77 3d46 616c 7365 2c20 6675  follow=False, fu
-0002d750: 6c6c 795f 7573 655f 6465 7669 6365 733d  lly_use_devices=
-0002d760: 4661 6c73 6529 2722 290a 2020 2020 2020  False)'").      
-0002d770: 2020 2020 2020 7365 6c66 2e6e 756d 5f6c        self.num_l
-0002d780: 6179 6572 7320 3d20 6e75 6d5f 6c61 7965  ayers = num_laye
-0002d790: 7273 0a20 2020 2020 2020 2020 2020 2073  rs.            s
-0002d7a0: 656c 662e 626c 6f63 6b73 203d 206e 6e2e  elf.blocks = nn.
-0002d7b0: 4365 6c6c 4c69 7374 2829 0a20 2020 2020  CellList().     
-0002d7c0: 2020 2020 2020 2066 6f72 2069 2069 6e20         for i in 
-0002d7d0: 7261 6e67 6528 6e75 6d5f 6c61 7965 7273  range(num_layers
-0002d7e0: 293a 0a20 2020 2020 2020 2020 2020 2020  ):.             
-0002d7f0: 2020 2062 6c6f 636b 203d 2054 7261 6e73     block = Trans
-0002d800: 666f 726d 6572 4465 636f 6465 724c 6179  formerDecoderLay
-0002d810: 6572 2868 6964 6465 6e5f 7369 7a65 3d68  er(hidden_size=h
-0002d820: 6964 6465 6e5f 7369 7a65 2c0a 2020 2020  idden_size,.    
+0002d6c0: 2020 2020 2020 2020 6869 6464 656e 5f61          hidden_a
+0002d6d0: 6374 3d68 6964 6465 6e5f 6163 742c 0a20  ct=hidden_act,. 
+0002d6e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002d6f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002d700: 2020 2020 2020 2020 2020 2020 2020 2075                 u
+0002d710: 7365 5f70 6173 743d 7573 655f 7061 7374  se_past=use_past
+0002d720: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+0002d730: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002d740: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002d750: 2020 7061 7261 6d5f 696e 6974 5f74 7970    param_init_typ
+0002d760: 653d 7061 7261 6d5f 696e 6974 5f74 7970  e=param_init_typ
+0002d770: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
+0002d780: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002d790: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002d7a0: 2020 2070 6f73 745f 6c61 7965 726e 6f72     post_layernor
+0002d7b0: 6d5f 7265 7369 6475 616c 3d70 6f73 745f  m_residual=post_
+0002d7c0: 6c61 7965 726e 6f72 6d5f 7265 7369 6475  layernorm_residu
+0002d7d0: 616c 2c0a 2020 2020 2020 2020 2020 2020  al,.            
+0002d7e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002d7f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002d800: 2020 2020 6d6f 655f 636f 6e66 6967 3d6d      moe_config=m
+0002d810: 6f65 5f63 6f6e 6669 672c 0a20 2020 2020  oe_config,.     
+0002d820: 2020 2020 2020 2020 2020 2020 2020 2020                  
 0002d830: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002d840: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002d850: 2020 2020 2020 2020 2020 2020 6261 7463              batc
-0002d860: 685f 7369 7a65 3d62 6174 6368 5f73 697a  h_size=batch_siz
-0002d870: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
-0002d880: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002d890: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002d8a0: 2020 2066 666e 5f68 6964 6465 6e5f 7369     ffn_hidden_si
-0002d8b0: 7a65 3d66 666e 5f68 6964 6465 6e5f 7369  ze=ffn_hidden_si
-0002d8c0: 7a65 2c0a 2020 2020 2020 2020 2020 2020  ze,.            
-0002d8d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002d8e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002d8f0: 2020 2020 7372 635f 7365 715f 6c65 6e67      src_seq_leng
-0002d900: 7468 3d73 7263 5f73 6571 5f6c 656e 6774  th=src_seq_lengt
-0002d910: 682c 0a20 2020 2020 2020 2020 2020 2020  h,.             
-0002d920: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002d930: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002d940: 2020 2074 6774 5f73 6571 5f6c 656e 6774     tgt_seq_lengt
-0002d950: 683d 7467 745f 7365 715f 6c65 6e67 7468  h=tgt_seq_length
-0002d960: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-0002d970: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002d980: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002d990: 2020 6174 7465 6e74 696f 6e5f 6472 6f70    attention_drop
-0002d9a0: 6f75 745f 7261 7465 3d61 7474 656e 7469  out_rate=attenti
-0002d9b0: 6f6e 5f64 726f 706f 7574 5f72 6174 652c  on_dropout_rate,
-0002d9c0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0002d9d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002d9e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002d9f0: 2068 6964 6465 6e5f 6472 6f70 6f75 745f   hidden_dropout_
-0002da00: 7261 7465 3d68 6964 6465 6e5f 6472 6f70  rate=hidden_drop
-0002da10: 6f75 745f 7261 7465 2c0a 2020 2020 2020  out_rate,.      
-0002da20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002da30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002da40: 2020 2020 2020 2020 2020 6e75 6d5f 6865            num_he
-0002da50: 6164 733d 6e75 6d5f 6865 6164 732c 0a20  ads=num_heads,. 
-0002da60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002da70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002da80: 2020 2020 2020 2020 2020 2020 2020 206c                 l
-0002da90: 6179 6572 6e6f 726d 5f63 6f6d 7075 7465  ayernorm_compute
-0002daa0: 5f74 7970 653d 6c61 7965 726e 6f72 6d5f  _type=layernorm_
-0002dab0: 636f 6d70 7574 655f 7479 7065 2c0a 2020  compute_type,.  
-0002dac0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002dad0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002dae0: 2020 2020 2020 2020 2020 2020 2020 736f                so
-0002daf0: 6674 6d61 785f 636f 6d70 7574 655f 7479  ftmax_compute_ty
-0002db00: 7065 3d73 6f66 746d 6178 5f63 6f6d 7075  pe=softmax_compu
-0002db10: 7465 5f74 7970 652c 0a20 2020 2020 2020  te_type,.       
-0002db20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002db30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002db40: 2020 2020 2020 2020 2068 6964 6465 6e5f           hidden_
-0002db50: 6163 743d 6869 6464 656e 5f61 6374 2c0a  act=hidden_act,.
+0002d840: 2020 2020 2020 2020 2020 2070 6172 616c             paral
+0002d850: 6c65 6c5f 636f 6e66 6967 3d63 6f6e 6669  lel_config=confi
+0002d860: 675f 746f 5f6c 6179 6572 290a 2020 2020  g_to_layer).    
+0002d870: 2020 2020 2020 2020 2020 2020 2320 4966              # If
+0002d880: 2074 6865 2075 7365 7220 646f 6573 6e27   the user doesn'
+0002d890: 7420 7061 7373 2074 6865 2066 7573 696f  t pass the fusio
+0002d8a0: 6e20 6675 6e63 7469 6f6e 2c20 7573 6520  n function, use 
+0002d8b0: 7468 6520 6465 6661 756c 7420 6f6e 650a  the default one.
+0002d8c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002d8d0: 6966 206e 6f74 206c 616d 6264 615f 6675  if not lambda_fu
+0002d8e0: 6e63 3a0a 2020 2020 2020 2020 2020 2020  nc:.            
+0002d8f0: 2020 2020 2020 2020 6c61 6d62 6461 5f66          lambda_f
+0002d900: 756e 6320 3d20 5f67 6574 5f6c 616d 6264  unc = _get_lambd
+0002d910: 615f 6675 6e63 2829 0a0a 2020 2020 2020  a_func()..      
+0002d920: 2020 2020 2020 2020 2020 6c61 6d62 6461            lambda
+0002d930: 5f66 756e 6328 626c 6f63 6b2c 206c 6179  _func(block, lay
+0002d940: 6572 5f69 643d 692c 206c 6179 6572 733d  er_id=i, layers=
+0002d950: 6e75 6d5f 6c61 7965 7273 2c0a 2020 2020  num_layers,.    
+0002d960: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002d970: 2020 2020 2020 2020 6f66 6673 6574 3d6f          offset=o
+0002d980: 6666 7365 742c 2070 6172 616c 6c65 6c5f  ffset, parallel_
+0002d990: 636f 6e66 6967 3d70 6172 616c 6c65 6c5f  config=parallel_
+0002d9a0: 636f 6e66 6967 290a 0a20 2020 2020 2020  config)..       
+0002d9b0: 2020 2020 2020 2020 2073 656c 662e 626c           self.bl
+0002d9c0: 6f63 6b73 2e61 7070 656e 6428 626c 6f63  ocks.append(bloc
+0002d9d0: 6b29 0a20 2020 2020 2020 2065 6c69 6620  k).        elif 
+0002d9e0: 5f67 6574 5f70 6172 616c 6c65 6c5f 6d6f  _get_parallel_mo
+0002d9f0: 6465 2829 206e 6f74 2069 6e20 2850 6172  de() not in (Par
+0002da00: 616c 6c65 6c4d 6f64 652e 4155 544f 5f50  allelMode.AUTO_P
+0002da10: 4152 414c 4c45 4c2c 293a 0a20 2020 2020  ARALLEL,):.     
+0002da20: 2020 2020 2020 2073 656c 662e 6164 6420         self.add 
+0002da30: 3d20 502e 4164 6428 292e 7368 6172 6428  = P.Add().shard(
+0002da40: 2828 292c 2028 2929 290a 2020 2020 2020  ((), ())).      
+0002da50: 2020 2020 2020 7365 6c66 2e61 7578 5f6c        self.aux_l
+0002da60: 6f73 7320 3d20 5465 6e73 6f72 2830 2e30  oss = Tensor(0.0
+0002da70: 2c20 6d73 7479 7065 2e66 6c6f 6174 3332  , mstype.float32
+0002da80: 290a 2020 2020 2020 2020 2020 2020 6c6f  ).            lo
+0002da90: 6767 6572 2e77 6172 6e69 6e67 2822 466f  gger.warning("Fo
+0002daa0: 7220 7061 7261 6c6c 656c 206d 6f64 652c  r parallel mode,
+0002dab0: 2073 6861 7264 696e 6720 7072 6f70 6167   sharding propag
+0002dac0: 6174 696f 6e20 6973 2072 6563 6f6d 6d65  ation is recomme
+0002dad0: 6e64 6564 2c20 796f 7520 6361 6e20 7573  nded, you can us
+0002dae0: 6520 6974 2062 7920 7365 7474 696e 6720  e it by setting 
+0002daf0: 220a 2020 2020 2020 2020 2020 2020 2020  ".              
+0002db00: 2020 2020 2020 2020 2020 2020 2022 2773               "'s
+0002db10: 6574 5f61 7574 6f5f 7061 7261 6c6c 656c  et_auto_parallel
+0002db20: 5f63 6f6e 7465 7874 2870 6172 616c 6c65  _context(paralle
+0002db30: 6c5f 6d6f 6465 3d50 6172 616c 6c65 6c4d  l_mode=ParallelM
+0002db40: 6f64 652e 4155 544f 5f50 4152 414c 4c45  ode.AUTO_PARALLE
+0002db50: 4c2c 2022 0a20 2020 2020 2020 2020 2020  L, ".           
 0002db60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002db70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002db80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002db90: 7573 655f 7061 7374 3d75 7365 5f70 6173  use_past=use_pas
-0002dba0: 742c 0a20 2020 2020 2020 2020 2020 2020  t,.             
-0002dbb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002dbc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002dbd0: 2020 2070 6172 616d 5f69 6e69 745f 7479     param_init_ty
-0002dbe0: 7065 3d70 6172 616d 5f69 6e69 745f 7479  pe=param_init_ty
-0002dbf0: 7065 2c0a 2020 2020 2020 2020 2020 2020  pe,.            
-0002dc00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002dc10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002dc20: 2020 2020 706f 7374 5f6c 6179 6572 6e6f      post_layerno
-0002dc30: 726d 5f72 6573 6964 7561 6c3d 706f 7374  rm_residual=post
-0002dc40: 5f6c 6179 6572 6e6f 726d 5f72 6573 6964  _layernorm_resid
-0002dc50: 7561 6c2c 0a20 2020 2020 2020 2020 2020  ual,.           
-0002dc60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002dc70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002dc80: 2020 2020 206d 6f65 5f63 6f6e 6669 673d       moe_config=
-0002dc90: 6d6f 655f 636f 6e66 6967 2c0a 2020 2020  moe_config,.    
-0002dca0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002dcb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002dcc0: 2020 2020 2020 2020 2020 2020 7061 7261              para
-0002dcd0: 6c6c 656c 5f63 6f6e 6669 673d 636f 6e66  llel_config=conf
-0002dce0: 6967 5f74 6f5f 6c61 7965 7229 0a20 2020  ig_to_layer).   
-0002dcf0: 2020 2020 2020 2020 2020 2020 2023 2049               # I
-0002dd00: 6620 7468 6520 7573 6572 2064 6f65 736e  f the user doesn
-0002dd10: 2774 2070 6173 7320 7468 6520 6675 7369  't pass the fusi
-0002dd20: 6f6e 2066 756e 6374 696f 6e2c 2075 7365  on function, use
-0002dd30: 2074 6865 2064 6566 6175 6c74 206f 6e65   the default one
-0002dd40: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0002dd50: 2069 6620 6e6f 7420 6c61 6d62 6461 5f66   if not lambda_f
-0002dd60: 756e 633a 0a20 2020 2020 2020 2020 2020  unc:.           
-0002dd70: 2020 2020 2020 2020 206c 616d 6264 615f           lambda_
-0002dd80: 6675 6e63 203d 205f 6765 745f 6c61 6d62  func = _get_lamb
-0002dd90: 6461 5f66 756e 6328 290a 0a20 2020 2020  da_func()..     
-0002dda0: 2020 2020 2020 2020 2020 206c 616d 6264             lambd
-0002ddb0: 615f 6675 6e63 2862 6c6f 636b 2c20 6c61  a_func(block, la
-0002ddc0: 7965 725f 6964 3d69 2c20 6c61 7965 7273  yer_id=i, layers
-0002ddd0: 3d6e 756d 5f6c 6179 6572 732c 0a20 2020  =num_layers,.   
-0002dde0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002ddf0: 2020 2020 2020 2020 206f 6666 7365 743d           offset=
-0002de00: 6f66 6673 6574 2c20 7061 7261 6c6c 656c  offset, parallel
-0002de10: 5f63 6f6e 6669 673d 7061 7261 6c6c 656c  _config=parallel
-0002de20: 5f63 6f6e 6669 6729 0a0a 2020 2020 2020  _config)..      
-0002de30: 2020 2020 2020 2020 2020 7365 6c66 2e62            self.b
-0002de40: 6c6f 636b 732e 6170 7065 6e64 2862 6c6f  locks.append(blo
-0002de50: 636b 290a 2020 2020 2020 2020 656c 7365  ck).        else
-0002de60: 3a0a 2020 2020 2020 2020 2020 2020 7261  :.            ra
-0002de70: 6973 6520 5275 6e74 696d 6545 7272 6f72  ise RuntimeError
-0002de80: 2866 2254 6865 207b 7365 6c66 2e63 6c73  (f"The {self.cls
-0002de90: 5f6e 616d 657d 206f 6e6c 7920 7375 7070  _name} only supp
-0002dea0: 6f72 7420 7368 6172 6469 6e67 2070 726f  ort sharding pro
-0002deb0: 7061 6761 7469 6f6e 206f 7220 220a 2020  pagation or ".  
-0002dec0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002ded0: 2020 2020 2020 2020 2020 2020 2066 2273               f"s
-0002dee0: 656d 692d 6175 746f 2070 6172 616c 6c65  emi-auto paralle
-0002def0: 6c20 6d6f 6465 206e 6f77 2e22 290a 0a20  l mode now.").. 
-0002df00: 2020 2064 6566 2063 6f6e 7374 7275 6374     def construct
-0002df10: 2873 656c 662c 2068 6964 6465 6e5f 7374  (self, hidden_st
-0002df20: 6174 6573 2c20 6174 7465 6e74 696f 6e5f  ates, attention_
-0002df30: 6d61 736b 2c20 656e 636f 6465 725f 6f75  mask, encoder_ou
-0002df40: 7470 7574 3d4e 6f6e 652c 206d 656d 6f72  tput=None, memor
-0002df50: 795f 6d61 736b 3d4e 6f6e 652c 0a20 2020  y_mask=None,.   
-0002df60: 2020 2020 2020 2020 2020 2020 2020 2069                 i
-0002df70: 6e69 745f 7265 7365 743d 5472 7565 2c20  nit_reset=True, 
-0002df80: 6261 7463 685f 7661 6c69 645f 6c65 6e67  batch_valid_leng
-0002df90: 7468 3d4e 6f6e 6529 3a0a 2020 2020 2020  th=None):.      
-0002dfa0: 2020 2222 2266 6f72 7761 7264 2070 726f    """forward pro
-0002dfb0: 6365 7373 2222 220a 2020 2020 2020 2020  cess""".        
-0002dfc0: 7072 6573 656e 745f 6c61 7965 7220 3d20  present_layer = 
-0002dfd0: 2829 0a20 2020 2020 2020 2069 6620 7365  ().        if se
-0002dfe0: 6c66 2e75 7365 5f6d 6f65 3a0a 2020 2020  lf.use_moe:.    
-0002dff0: 2020 2020 2020 2020 6163 6375 6d5f 6c6f          accum_lo
-0002e000: 7373 203d 2073 656c 662e 6175 785f 6c6f  ss = self.aux_lo
-0002e010: 7373 0a20 2020 2020 2020 2020 2020 2066  ss.            f
-0002e020: 6f72 2069 2069 6e20 7261 6e67 6528 7365  or i in range(se
-0002e030: 6c66 2e6e 756d 5f6c 6179 6572 7329 3a0a  lf.num_layers):.
-0002e040: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e050: 6869 6464 656e 5f73 7461 7465 732c 2070  hidden_states, p
-0002e060: 7265 7365 6e74 2c20 6175 785f 6c6f 7373  resent, aux_loss
-0002e070: 203d 2073 656c 662e 626c 6f63 6b73 5b69   = self.blocks[i
-0002e080: 5d28 6869 6464 656e 5f73 7461 7465 732c  ](hidden_states,
-0002e090: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0002db70: 2273 6561 7263 685f 6d6f 6465 3d5c 2273  "search_mode=\"s
+0002db80: 6861 7264 696e 675f 7072 6f70 6167 6174  harding_propagat
+0002db90: 696f 6e5c 2229 2720 616e 6420 220a 2020  ion\")' and ".  
+0002dba0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002dbb0: 2020 2020 2020 2020 2022 2773 6574 5f61           "'set_a
+0002dbc0: 6c67 6f5f 7061 7261 6d65 7465 7273 2865  lgo_parameters(e
+0002dbd0: 6c65 6d65 6e74 7769 7365 5f6f 705f 7374  lementwise_op_st
+0002dbe0: 7261 7465 6779 5f66 6f6c 6c6f 773d 4661  rategy_follow=Fa
+0002dbf0: 6c73 652c 2066 756c 6c79 5f75 7365 5f64  lse, fully_use_d
+0002dc00: 6576 6963 6573 3d46 616c 7365 2927 2229  evices=False)'")
+0002dc10: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
+0002dc20: 662e 6e75 6d5f 6c61 7965 7273 203d 206e  f.num_layers = n
+0002dc30: 756d 5f6c 6179 6572 730a 2020 2020 2020  um_layers.      
+0002dc40: 2020 2020 2020 7365 6c66 2e62 6c6f 636b        self.block
+0002dc50: 7320 3d20 6e6e 2e43 656c 6c4c 6973 7428  s = nn.CellList(
+0002dc60: 290a 2020 2020 2020 2020 2020 2020 666f  ).            fo
+0002dc70: 7220 6920 696e 2072 616e 6765 286e 756d  r i in range(num
+0002dc80: 5f6c 6179 6572 7329 3a0a 2020 2020 2020  _layers):.      
+0002dc90: 2020 2020 2020 2020 2020 626c 6f63 6b20            block 
+0002dca0: 3d20 5472 616e 7366 6f72 6d65 7244 6563  = TransformerDec
+0002dcb0: 6f64 6572 4c61 7965 7228 6869 6464 656e  oderLayer(hidden
+0002dcc0: 5f73 697a 653d 6869 6464 656e 5f73 697a  _size=hidden_siz
+0002dcd0: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
+0002dce0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002dcf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002dd00: 2020 2062 6174 6368 5f73 697a 653d 6261     batch_size=ba
+0002dd10: 7463 685f 7369 7a65 2c0a 2020 2020 2020  tch_size,.      
+0002dd20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002dd30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002dd40: 2020 2020 2020 2020 2020 6666 6e5f 6869            ffn_hi
+0002dd50: 6464 656e 5f73 697a 653d 6666 6e5f 6869  dden_size=ffn_hi
+0002dd60: 6464 656e 5f73 697a 652c 0a20 2020 2020  dden_size,.     
+0002dd70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002dd80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002dd90: 2020 2020 2020 2020 2020 2073 7263 5f73             src_s
+0002dda0: 6571 5f6c 656e 6774 683d 7372 635f 7365  eq_length=src_se
+0002ddb0: 715f 6c65 6e67 7468 2c0a 2020 2020 2020  q_length,.      
+0002ddc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002ddd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002dde0: 2020 2020 2020 2020 2020 7467 745f 7365            tgt_se
+0002ddf0: 715f 6c65 6e67 7468 3d74 6774 5f73 6571  q_length=tgt_seq
+0002de00: 5f6c 656e 6774 682c 0a20 2020 2020 2020  _length,.       
+0002de10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002de20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002de30: 2020 2020 2020 2020 2061 7474 656e 7469           attenti
+0002de40: 6f6e 5f64 726f 706f 7574 5f72 6174 653d  on_dropout_rate=
+0002de50: 6174 7465 6e74 696f 6e5f 6472 6f70 6f75  attention_dropou
+0002de60: 745f 7261 7465 2c0a 2020 2020 2020 2020  t_rate,.        
+0002de70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002de80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002de90: 2020 2020 2020 2020 6869 6464 656e 5f64          hidden_d
+0002dea0: 726f 706f 7574 5f72 6174 653d 6869 6464  ropout_rate=hidd
+0002deb0: 656e 5f64 726f 706f 7574 5f72 6174 652c  en_dropout_rate,
+0002dec0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0002ded0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002dee0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002def0: 206e 756d 5f68 6561 6473 3d6e 756d 5f68   num_heads=num_h
+0002df00: 6561 6473 2c0a 2020 2020 2020 2020 2020  eads,.          
+0002df10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002df20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002df30: 2020 2020 2020 6c61 7965 726e 6f72 6d5f        layernorm_
+0002df40: 636f 6d70 7574 655f 7479 7065 3d6c 6179  compute_type=lay
+0002df50: 6572 6e6f 726d 5f63 6f6d 7075 7465 5f74  ernorm_compute_t
+0002df60: 7970 652c 0a20 2020 2020 2020 2020 2020  ype,.           
+0002df70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002df80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002df90: 2020 2020 2073 6f66 746d 6178 5f63 6f6d       softmax_com
+0002dfa0: 7075 7465 5f74 7970 653d 736f 6674 6d61  pute_type=softma
+0002dfb0: 785f 636f 6d70 7574 655f 7479 7065 2c0a  x_compute_type,.
+0002dfc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002dfd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002dfe0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002dff0: 6869 6464 656e 5f61 6374 3d68 6964 6465  hidden_act=hidde
+0002e000: 6e5f 6163 742c 0a20 2020 2020 2020 2020  n_act,.         
+0002e010: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e020: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e030: 2020 2020 2020 2075 7365 5f70 6173 743d         use_past=
+0002e040: 7573 655f 7061 7374 2c0a 2020 2020 2020  use_past,.      
+0002e050: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e060: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e070: 2020 2020 2020 2020 2020 7061 7261 6d5f            param_
+0002e080: 696e 6974 5f74 7970 653d 7061 7261 6d5f  init_type=param_
+0002e090: 696e 6974 5f74 7970 652c 0a20 2020 2020  init_type,.     
 0002e0a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 0002e0b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e0c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e0d0: 2020 2061 7474 656e 7469 6f6e 5f6d 6173     attention_mas
-0002e0e0: 6b2c 0a20 2020 2020 2020 2020 2020 2020  k,.             
-0002e0f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e0c0: 2020 2020 2020 2020 2020 2070 6f73 745f             post_
+0002e0d0: 6c61 7965 726e 6f72 6d5f 7265 7369 6475  layernorm_residu
+0002e0e0: 616c 3d70 6f73 745f 6c61 7965 726e 6f72  al=post_layernor
+0002e0f0: 6d5f 7265 7369 6475 616c 2c0a 2020 2020  m_residual,.    
 0002e100: 2020 2020 2020 2020 2020 2020 2020 2020                  
 0002e110: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e120: 2020 2020 2065 6e63 6f64 6572 5f6f 7574       encoder_out
-0002e130: 7075 742c 0a20 2020 2020 2020 2020 2020  put,.           
-0002e140: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e120: 2020 2020 2020 2020 2020 2020 6d6f 655f              moe_
+0002e130: 636f 6e66 6967 3d6d 6f65 5f63 6f6e 6669  config=moe_confi
+0002e140: 672c 0a20 2020 2020 2020 2020 2020 2020  g,.             
 0002e150: 2020 2020 2020 2020 2020 2020 2020 2020                  
 0002e160: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e170: 2020 2020 2020 206d 656d 6f72 795f 6d61         memory_ma
-0002e180: 736b 2c0a 2020 2020 2020 2020 2020 2020  sk,.            
-0002e190: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e1a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e1b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e1c0: 2020 2020 2020 696e 6974 5f72 6573 6574        init_reset
-0002e1d0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-0002e1e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e1f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e200: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e210: 2020 2020 6261 7463 685f 7661 6c69 645f      batch_valid_
-0002e220: 6c65 6e67 7468 290a 2020 2020 2020 2020  length).        
-0002e230: 2020 2020 2020 2020 7072 6573 656e 745f          present_
-0002e240: 6c61 7965 7220 3d20 7072 6573 656e 745f  layer = present_
-0002e250: 6c61 7965 7220 2b20 2870 7265 7365 6e74  layer + (present
-0002e260: 2c29 0a20 2020 2020 2020 2020 2020 2020  ,).             
-0002e270: 2020 2061 6363 756d 5f6c 6f73 7320 3d20     accum_loss = 
-0002e280: 7365 6c66 2e61 6464 2861 6363 756d 5f6c  self.add(accum_l
-0002e290: 6f73 732c 2061 7578 5f6c 6f73 7329 0a20  oss, aux_loss). 
-0002e2a0: 2020 2020 2020 2020 2020 2072 6574 7572             retur
-0002e2b0: 6e20 6869 6464 656e 5f73 7461 7465 732c  n hidden_states,
-0002e2c0: 2070 7265 7365 6e74 5f6c 6179 6572 2c20   present_layer, 
-0002e2d0: 6163 6375 6d5f 6c6f 7373 0a0a 2020 2020  accum_loss..    
-0002e2e0: 2020 2020 2320 4c6f 6f70 2074 6872 6f75      # Loop throu
-0002e2f0: 6768 2065 6163 6820 7365 6c66 2d61 7474  gh each self-att
-0002e300: 656e 7469 6f6e 206c 6179 6572 0a20 2020  ention layer.   
-0002e310: 2020 2020 2066 6f72 2069 2069 6e20 7261       for i in ra
-0002e320: 6e67 6528 7365 6c66 2e6e 756d 5f6c 6179  nge(self.num_lay
-0002e330: 6572 7329 3a0a 2020 2020 2020 2020 2020  ers):.          
-0002e340: 2020 6869 6464 656e 5f73 7461 7465 732c    hidden_states,
-0002e350: 2070 7265 7365 6e74 203d 2073 656c 662e   present = self.
-0002e360: 626c 6f63 6b73 5b69 5d28 6869 6464 656e  blocks[i](hidden
-0002e370: 5f73 7461 7465 732c 0a20 2020 2020 2020  _states,.       
-0002e380: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e390: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e3a0: 2020 2020 2020 2020 2020 2020 2061 7474               att
-0002e3b0: 656e 7469 6f6e 5f6d 6173 6b2c 0a20 2020  ention_mask,.   
-0002e3c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e3d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e3e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e3f0: 2065 6e63 6f64 6572 5f6f 7574 7075 742c   encoder_output,
-0002e400: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0002e410: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e420: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e430: 2020 2020 206d 656d 6f72 795f 6d61 736b       memory_mask
-0002e440: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-0002e450: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e460: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e470: 2020 2020 2020 696e 6974 5f72 6573 6574        init_reset
-0002e480: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-0002e490: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e4a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002e4b0: 2020 2020 2020 6261 7463 685f 7661 6c69        batch_vali
-0002e4c0: 645f 6c65 6e67 7468 290a 2020 2020 2020  d_length).      
-0002e4d0: 2020 2020 2020 7072 6573 656e 745f 6c61        present_la
-0002e4e0: 7965 7220 3d20 7072 6573 656e 745f 6c61  yer = present_la
-0002e4f0: 7965 7220 2b20 2870 7265 7365 6e74 2c29  yer + (present,)
-0002e500: 0a0a 2020 2020 2020 2020 7265 7475 726e  ..        return
-0002e510: 2068 6964 6465 6e5f 7374 6174 6573 2c20   hidden_states, 
-0002e520: 7072 6573 656e 745f 6c61 7965 720a 0a0a  present_layer...
-0002e530: 636c 6173 7320 5472 616e 7366 6f72 6d65  class Transforme
-0002e540: 7228 4365 6c6c 293a 0a20 2020 2072 2222  r(Cell):.    r""
-0002e550: 220a 2020 2020 2020 2020 5472 616e 7366  ".        Transf
-0002e560: 6f72 6d65 7220 6d6f 6475 6c65 2069 6e63  ormer module inc
-0002e570: 6c75 6469 6e67 2065 6e63 6f64 6572 2061  luding encoder a
-0002e580: 6e64 2064 6563 6f64 6572 2e20 5468 6520  nd decoder. The 
-0002e590: 6469 6666 6572 656e 6365 2077 6974 6820  difference with 
-0002e5a0: 7468 6520 6f72 6967 696e 616c 2069 6d70  the original imp
-0002e5b0: 6c65 6d65 6e74 7320 6973 2074 6865 206d  lements is the m
-0002e5c0: 6f64 756c 6520 7573 650a 2020 2020 2020  odule use.      
-0002e5d0: 2020 7468 6520 7265 7369 6475 616c 2061    the residual a
-0002e5e0: 6464 6974 696f 6e20 6265 666f 7265 2074  ddition before t
-0002e5f0: 6865 206c 6179 6572 206e 6f72 6d61 6c69  he layer normali
-0002e600: 7a61 7469 6f6e 2e20 416e 6420 7468 6520  zation. And the 
-0002e610: 6465 6661 756c 7420 6869 6464 656e 2061  default hidden a
-0002e620: 6374 2069 7320 6067 656c 7560 2e0a 2020  ct is `gelu`..  
-0002e630: 2020 2020 2020 5468 6520 6465 7461 696c        The detail
-0002e640: 7320 6361 6e20 6265 2066 6f75 6e64 2069  s can be found i
-0002e650: 6e20 6041 7474 656e 7469 6f6e 2069 7320  n `Attention is 
-0002e660: 616c 6c20 796f 7520 6e65 6564 203c 6874  all you need <ht
-0002e670: 7470 733a 2f2f 6172 7869 762e 6f72 672f  tps://arxiv.org/
-0002e680: 7064 662f 3137 3036 2e30 3337 3632 7635  pdf/1706.03762v5
-0002e690: 2e70 6466 3e60 5f2e 0a0a 2020 2020 2020  .pdf>`_...      
-0002e6a0: 2020 4e6f 7465 3a0a 2020 2020 2020 2020    Note:.        
-0002e6b0: 2020 2020 5468 6973 2069 7320 616e 2065      This is an e
-0002e6c0: 7870 6572 696d 656e 7461 6c20 696e 7465  xperimental inte
-0002e6d0: 7266 6163 6520 7468 6174 2069 7320 7375  rface that is su
-0002e6e0: 626a 6563 7420 746f 2063 6861 6e67 6520  bject to change 
-0002e6f0: 6f72 2064 656c 6574 696f 6e2e 0a0a 2020  or deletion...  
-0002e700: 2020 2020 2020 4172 6773 3a0a 2020 2020        Args:.    
-0002e710: 2020 2020 2020 2020 6869 6464 656e 5f73          hidden_s
-0002e720: 697a 6528 696e 7429 3a20 5468 6520 6869  ize(int): The hi
-0002e730: 6464 656e 2073 697a 6520 6f66 2074 6865  dden size of the
-0002e740: 2069 6e70 7574 2e0a 2020 2020 2020 2020   input..        
-0002e750: 2020 2020 6261 7463 685f 7369 7a65 2869      batch_size(i
-0002e760: 6e74 293a 2054 6865 2062 6174 6368 2073  nt): The batch s
-0002e770: 697a 6520 6f66 2074 6865 2069 6e70 7574  ize of the input
-0002e780: 2074 656e 736f 7220 7768 656e 2064 6f20   tensor when do 
-0002e790: 696e 6372 656e 6d65 6e74 616c 2070 7265  increnmental pre
-0002e7a0: 6469 6374 696f 6e2e 2053 686f 756c 6420  diction. Should 
-0002e7b0: 6265 2061 2070 6f73 6974 6976 650a 2020  be a positive.  
-0002e7c0: 2020 2020 2020 2020 2020 2020 2020 7661                va
-0002e7d0: 6c75 652e 2057 6865 6e20 646f 2074 7261  lue. When do tra
-0002e7e0: 696e 696e 6720 6f72 2070 7265 6469 6374  ining or predict
-0002e7f0: 696f 6e2c 2074 6865 2061 7267 756d 656e  ion, the argumen
-0002e800: 7420 7769 6c6c 206e 6f74 2077 6f72 6b20  t will not work 
-0002e810: 616e 6420 7468 6520 7573 6572 2063 616e  and the user can
-0002e820: 206a 7573 7420 7061 7373 204e 6f6e 6520   just pass None 
-0002e830: 746f 0a20 2020 2020 2020 2020 2020 2020  to.             
-0002e840: 2020 2074 6865 2061 7267 756d 656e 742e     the argument.
-0002e850: 0a20 2020 2020 2020 2020 2020 2066 666e  .            ffn
-0002e860: 5f68 6964 6465 6e5f 7369 7a65 2869 6e74  _hidden_size(int
-0002e870: 293a 2054 6865 2068 6964 6465 6e20 7369  ): The hidden si
-0002e880: 7a65 206f 6620 626f 7474 6c65 6e65 636b  ze of bottleneck
-0002e890: 2069 6e20 7468 6520 6665 6564 666f 7277   in the feedforw
-0002e8a0: 6172 6420 6c61 7965 722e 0a20 2020 2020  ard layer..     
-0002e8b0: 2020 2020 2020 2073 7263 5f73 6571 5f6c         src_seq_l
-0002e8c0: 656e 6774 6828 696e 7429 3a20 5468 6520  ength(int): The 
-0002e8d0: 7365 715f 6c65 6e67 7468 206f 6620 7468  seq_length of th
-0002e8e0: 6520 656e 636f 6465 7227 7320 696e 7075  e encoder's inpu
-0002e8f0: 7420 7465 6e73 6f72 2e0a 2020 2020 2020  t tensor..      
-0002e900: 2020 2020 2020 7467 745f 7365 715f 6c65        tgt_seq_le
-0002e910: 6e67 7468 2869 6e74 293a 2054 6865 2073  ngth(int): The s
-0002e920: 6571 5f6c 656e 6774 6820 6f66 2074 6865  eq_length of the
-0002e930: 2064 6563 6f64 6572 2773 2069 6e70 7574   decoder's input
-0002e940: 2074 656e 736f 722e 0a20 2020 2020 2020   tensor..       
-0002e950: 2020 2020 2065 6e63 6f64 6572 5f6c 6179       encoder_lay
-0002e960: 6572 7328 696e 7429 3a20 5468 6520 6c61  ers(int): The la
-0002e970: 7965 7273 206f 6620 7468 6520 6054 7261  yers of the `Tra
-0002e980: 6e73 666f 726d 6572 456e 636f 6465 724c  nsformerEncoderL
-0002e990: 6179 6572 602e 2044 6566 6175 6c74 2033  ayer`. Default 3
-0002e9a0: 2e0a 2020 2020 2020 2020 2020 2020 6465  ..            de
-0002e9b0: 636f 6465 725f 6c61 7965 7273 2869 6e74  coder_layers(int
-0002e9c0: 293a 2054 6865 206c 6179 6572 7320 6f66  ): The layers of
-0002e9d0: 2074 6865 2060 5472 616e 7366 6f72 6d65   the `Transforme
-0002e9e0: 7244 6563 6f64 6572 4c61 7965 7260 2e20  rDecoderLayer`. 
-0002e9f0: 4465 6661 756c 7420 332e 0a20 2020 2020  Default 3..     
-0002ea00: 2020 2020 2020 206e 756d 5f68 6561 6473         num_heads
-0002ea10: 2869 6e74 293a 2054 6865 206e 756d 6265  (int): The numbe
-0002ea20: 7220 6f66 2074 6865 2068 6561 6473 2e20  r of the heads. 
-0002ea30: 4465 6661 756c 743a 2032 2e0a 2020 2020  Default: 2..    
-0002ea40: 2020 2020 2020 2020 6174 7465 6e74 696f          attentio
-0002ea50: 6e5f 6472 6f70 6f75 745f 7261 7465 2866  n_dropout_rate(f
-0002ea60: 6c6f 6174 293a 2054 6865 2064 726f 706f  loat): The dropo
-0002ea70: 7574 2072 6174 6520 6f66 2074 6865 2061  ut rate of the a
-0002ea80: 7474 656e 7469 6f6e 2073 636f 7265 732e  ttention scores.
-0002ea90: 2044 6566 6175 6c74 3a30 2e31 2e0a 2020   Default:0.1..  
-0002eaa0: 2020 2020 2020 2020 2020 6869 6464 656e            hidden
-0002eab0: 5f64 726f 706f 7574 5f72 6174 6528 666c  _dropout_rate(fl
-0002eac0: 6f61 7429 3a20 5468 6520 6472 6f70 6f75  oat): The dropou
-0002ead0: 7420 7261 7465 206f 6620 7468 6520 6669  t rate of the fi
-0002eae0: 6e61 6c20 6f75 7470 7574 206f 6620 7468  nal output of th
-0002eaf0: 6520 6c61 7965 722e 2044 6566 6175 6c74  e layer. Default
-0002eb00: 3a30 2e31 2e0a 2020 2020 2020 2020 2020  :0.1..          
-0002eb10: 2020 6869 6464 656e 5f61 6374 2028 7374    hidden_act (st
-0002eb20: 722c 206e 6e2e 4365 6c6c 293a 2054 6865  r, nn.Cell): The
-0002eb30: 2061 6374 6976 6174 696f 6e20 6f66 2074   activation of t
-0002eb40: 6865 2069 6e74 6572 6e61 6c20 6665 6564  he internal feed
-0002eb50: 666f 7277 6172 6420 6c61 7965 722e 2053  forward layer. S
-0002eb60: 7570 706f 7274 7320 2772 656c 7527 2c0a  upports 'relu',.
-0002eb70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002eb80: 2772 656c 7536 272c 2027 7461 6e68 272c  'relu6', 'tanh',
-0002eb90: 2027 6765 6c75 272c 2027 6661 7374 5f67   'gelu', 'fast_g
-0002eba0: 656c 7527 2c20 2765 6c75 272c 2027 7369  elu', 'elu', 'si
-0002ebb0: 676d 6f69 6427 2c20 2770 7265 6c75 272c  gmoid', 'prelu',
-0002ebc0: 2027 6c65 616b 7972 656c 7527 2c20 2768   'leakyrelu', 'h
-0002ebd0: 7377 6973 6827 2c0a 2020 2020 2020 2020  swish',.        
-0002ebe0: 2020 2020 2020 2020 2768 7369 676d 6f69          'hsigmoi
-0002ebf0: 6427 2c20 276c 6f67 7369 676d 6f69 6427  d', 'logsigmoid'
-0002ec00: 2061 6e64 2073 6f20 6f6e 2e20 5573 6572   and so on. User
-0002ec10: 2063 616e 2070 726f 7669 6465 2063 7573   can provide cus
-0002ec20: 746f 6d20 6163 7469 7669 7469 6f6e 2074  tom activition t
-0002ec30: 6f20 7468 6520 6172 6775 6d65 6e74 2e0a  o the argument..
-0002ec40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002ec50: 4966 2075 7365 7220 7761 6e74 7320 746f  If user wants to
-0002ec60: 2072 756e 2074 6865 206e 6574 2069 6e20   run the net in 
-0002ec70: 7468 6520 7061 7261 6c6c 656c 206d 6f64  the parallel mod
-0002ec80: 652c 2074 6865 2063 7573 746f 6d20 6163  e, the custom ac
-0002ec90: 7469 7661 7469 6f6e 206d 7573 7420 616c  tivation must al
-0002eca0: 736f 2070 726f 7669 6465 0a20 2020 2020  so provide.     
-0002ecb0: 2020 2020 2020 2020 2020 2074 6865 2060             the `
-0002ecc0: 6163 7469 7661 7469 6f6e 5f73 6861 7264  activation_shard
-0002ecd0: 6020 6675 6e63 7469 6f6e 2e20 506c 6561  ` function. Plea
-0002ece0: 7365 2073 6565 2074 6865 2065 7861 6d70  se see the examp
-0002ecf0: 6c65 7320 6f66 2074 6865 0a20 2020 2020  les of the.     
-0002ed00: 2020 2020 2020 2020 2020 2063 6c61 7373             class
-0002ed10: 3a60 6d69 6e64 666f 726d 6572 732e 6d6f  :`mindformers.mo
-0002ed20: 6475 6c65 732e 7472 616e 7366 6f72 6d65  dules.transforme
-0002ed30: 722e 4665 6564 466f 7277 6172 6460 2e20  r.FeedForward`. 
-0002ed40: 4465 6661 756c 743a 2067 656c 752e 0a20  Default: gelu.. 
-0002ed50: 2020 2020 2020 2020 2020 2070 6f73 745f             post_
-0002ed60: 6c61 7965 726e 6f72 6d5f 7265 7369 6475  layernorm_residu
-0002ed70: 616c 2862 6f6f 6c29 3a20 446f 2072 6573  al(bool): Do res
-0002ed80: 6964 7561 6c73 2061 6464 7320 6265 666f  iduals adds befo
-0002ed90: 7265 2074 6865 206c 6179 6572 6e6f 726d  re the layernorm
-0002eda0: 2e20 4465 6661 756c 7420 4661 6c73 652e  . Default False.
-0002edb0: 0a20 2020 2020 2020 2020 2020 206c 6179  .            lay
-0002edc0: 6572 6e6f 726d 5f63 6f6d 7075 7465 5f74  ernorm_compute_t
-0002edd0: 7970 6528 6474 7970 652e 4e75 6d62 6572  ype(dtype.Number
-0002ede0: 293a 2054 6865 2063 6f6d 7075 7461 7469  ): The computati
-0002edf0: 6f6e 2074 7970 6520 6f66 2074 6865 206c  on type of the l
-0002ee00: 6179 6572 6e6f 726d 2e0a 2020 2020 2020  ayernorm..      
-0002ee10: 2020 2020 2020 2020 2020 5368 6f75 6c64            Should
-0002ee20: 2062 6520 6474 7970 652e 666c 6f61 7433   be dtype.float3
-0002ee30: 3220 6f72 2064 7479 7065 2e66 6c6f 6174  2 or dtype.float
-0002ee40: 3136 2e20 4465 6661 756c 7420 6474 7970  16. Default dtyp
-0002ee50: 652e 666c 6f61 7433 322e 0a20 2020 2020  e.float32..     
-0002ee60: 2020 2020 2020 2073 6f66 746d 6178 5f63         softmax_c
-0002ee70: 6f6d 7075 7465 5f74 7970 6528 6474 7970  ompute_type(dtyp
-0002ee80: 652e 4e75 6d62 6572 293a 2054 6865 2063  e.Number): The c
-0002ee90: 6f6d 7075 7461 7469 6f6e 2074 7970 6520  omputation type 
-0002eea0: 6f66 2074 6865 2073 6f66 746d 6178 2069  of the softmax i
-0002eeb0: 6e20 7468 6520 6174 7465 6e74 696f 6e2e  n the attention.
-0002eec0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0002eed0: 2053 686f 756c 6420 6265 2064 7479 7065   Should be dtype
-0002eee0: 2e66 6c6f 6174 3332 206f 7220 6474 7970  .float32 or dtyp
-0002eef0: 652e 666c 6f61 7431 362e 2044 6566 6175  e.float16. Defau
-0002ef00: 6c74 206d 7374 7970 652e 666c 6f61 7433  lt mstype.float3
-0002ef10: 322e 0a20 2020 2020 2020 2020 2020 2070  2..            p
-0002ef20: 6172 616d 5f69 6e69 745f 7479 7065 2864  aram_init_type(d
-0002ef30: 7479 7065 2e4e 756d 6265 7229 3a20 5468  type.Number): Th
-0002ef40: 6520 7061 7261 6d65 7465 7220 696e 6974  e parameter init
-0002ef50: 6961 6c69 7a61 7469 6f6e 2074 7970 6520  ialization type 
-0002ef60: 6f66 2074 6865 206d 6f64 756c 652e 0a20  of the module.. 
-0002ef70: 2020 2020 2020 2020 2020 2020 2020 2053                 S
-0002ef80: 686f 756c 6420 6265 2064 7479 7065 2e66  hould be dtype.f
-0002ef90: 6c6f 6174 3332 206f 7220 6474 7970 652e  loat32 or dtype.
-0002efa0: 666c 6f61 7431 362e 2044 6566 6175 6c74  float16. Default
-0002efb0: 2064 7479 7065 2e66 6c6f 6174 3332 2e0a   dtype.float32..
-0002efc0: 2020 2020 2020 2020 2020 2020 6c61 6d62              lamb
-0002efd0: 6461 5f66 756e 633a 2041 2066 756e 6374  da_func: A funct
-0002efe0: 696f 6e20 6361 6e20 6465 7465 726d 696e  ion can determin
-0002eff0: 6520 7468 6520 6675 7369 6f6e 2069 6e64  e the fusion ind
-0002f000: 6578 2c20 7069 7065 6c69 6e65 2073 7461  ex, pipeline sta
-0002f010: 6765 7320 616e 6420 7265 636f 6d70 7574  ges and recomput
-0002f020: 6520 6174 7472 6962 7574 652e 2049 6620  e attribute. If 
-0002f030: 7468 6520 7573 6572 0a20 2020 2020 2020  the user.       
-0002f040: 2020 2020 2020 2020 2077 616e 7473 2074           wants t
-0002f050: 6f20 6465 7465 726d 696e 6520 7468 6520  o determine the 
-0002f060: 7069 7065 6c69 6e65 2073 7461 6765 2061  pipeline stage a
-0002f070: 6e64 2067 7261 6469 656e 7420 6167 6772  nd gradient aggr
-0002f080: 6567 6174 696f 6e20 6675 7369 6f6e 2c20  egation fusion, 
-0002f090: 7468 6520 7573 6572 2063 616e 2070 6173  the user can pas
-0002f0a0: 7320 6120 6675 6e63 7469 6f6e 0a20 2020  s a function.   
-0002f0b0: 2020 2020 2020 2020 2020 2020 2074 6861               tha
-0002f0c0: 7420 6163 6365 7074 7320 606e 6574 776f  t accepts `netwo
-0002f0d0: 726b 602c 2060 6c61 7965 725f 6964 602c  rk`, `layer_id`,
-0002f0e0: 2060 6f66 6673 6574 602c 2060 7061 7261   `offset`, `para
-0002f0f0: 6c6c 656c 5f63 6f6e 6669 6760 2c20 606c  llel_config`, `l
-0002f100: 6179 6572 7360 2e20 5468 6520 606e 6574  ayers`. The `net
-0002f110: 776f 726b 2843 656c 6c29 600a 2020 2020  work(Cell)`.    
-0002f120: 2020 2020 2020 2020 2020 2020 7265 7072              repr
-0002f130: 6573 656e 7473 2074 6865 2074 7261 6e73  esents the trans
-0002f140: 666f 726d 6572 2062 6c6f 636b 2c20 606c  former block, `l
-0002f150: 6179 6572 5f69 6428 696e 7429 6020 6d65  ayer_id(int)` me
-0002f160: 616e 7320 7468 6520 6c61 7965 7220 696e  ans the layer in
-0002f170: 6465 7820 666f 7220 7468 6520 6375 7272  dex for the curr
-0002f180: 656e 7420 6d6f 6475 6c65 2c20 636f 756e  ent module, coun
-0002f190: 7473 0a20 2020 2020 2020 2020 2020 2020  ts.             
-0002f1a0: 2020 2066 726f 6d20 7a65 726f 2c20 606f     from zero, `o
-0002f1b0: 6666 7365 7428 696e 7429 6020 6d65 616e  ffset(int)` mean
-0002f1c0: 7320 7468 6520 6c61 7965 725f 696e 6465  s the layer_inde
-0002f1d0: 7820 6e65 6564 7320 616e 206f 6666 7365  x needs an offse
-0002f1e0: 742c 2069 6620 7468 6572 6520 6172 6520  t, if there are 
-0002f1f0: 6f74 6865 7220 6d6f 6475 6c65 7320 696e  other modules in
-0002f200: 2074 6865 206e 6574 2e0a 2020 2020 2020   the net..      
-0002f210: 2020 2020 2020 2020 2020 5468 6520 6465            The de
-0002f220: 6661 756c 7420 7365 7474 696e 6720 666f  fault setting fo
-0002f230: 7220 7468 6520 7069 7065 6c69 6e65 2069  r the pipeline i
-0002f240: 733a 2060 286c 6179 6572 5f69 6420 2b20  s: `(layer_id + 
-0002f250: 6f66 6673 6574 2920 2f2f 2028 2865 6e63  offset) // ((enc
-0002f260: 6f64 6572 5f6c 6179 6572 7320 2b20 6465  oder_layers + de
-0002f270: 636f 6465 725f 6c61 7965 7273 290a 2020  coder_layers).  
-0002f280: 2020 2020 2020 2020 2020 2020 2020 2f20                / 
-0002f290: 7069 7065 6c69 6e65 5f73 7461 6765 2960  pipeline_stage)`
-0002f2a0: 2e20 4465 6661 756c 7420 4e6f 6e65 2e0a  . Default None..
-0002f2b0: 2020 2020 2020 2020 2020 2020 7573 655f              use_
-0002f2c0: 7061 7374 2862 6f6f 6c29 3a20 5573 6520  past(bool): Use 
-0002f2d0: 7468 6520 7061 7374 2073 7461 7465 2074  the past state t
-0002f2e0: 6f20 636f 6d70 7574 652c 2075 7365 6420  o compute, used 
-0002f2f0: 666f 7220 696e 6372 656d 656e 7461 6c20  for incremental 
-0002f300: 7072 6564 6963 7469 6f6e 2e20 4465 6661  prediction. Defa
-0002f310: 756c 7420 4661 6c73 652e 0a20 2020 2020  ult False..     
-0002f320: 2020 2020 2020 206d 6f65 5f63 6f6e 6669         moe_confi
-0002f330: 6728 4d6f 4543 6f6e 6669 6729 3a20 5468  g(MoEConfig): Th
-0002f340: 6520 636f 6e66 6967 7572 6174 696f 6e20  e configuration 
-0002f350: 6f66 204d 6f45 2028 4d69 7874 7572 6520  of MoE (Mixture 
-0002f360: 6f66 2045 7870 6572 7429 2e20 4465 6661  of Expert). Defa
-0002f370: 756c 7420 6973 2061 6e20 696e 7374 616e  ult is an instan
-0002f380: 6365 206f 6620 4d6f 4543 6f6e 6669 670a  ce of MoEConfig.
-0002f390: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0002f3a0: 7769 7468 2064 6566 6175 6c74 2076 616c  with default val
-0002f3b0: 7565 732e 2050 6c65 6173 6520 7365 6520  ues. Please see 
-0002f3c0: 604d 6f45 436f 6e66 6967 602e 0a20 2020  `MoEConfig`..   
-0002f3d0: 2020 2020 2020 2020 2070 6172 616c 6c65           paralle
-0002f3e0: 6c5f 636f 6e66 6967 2854 7261 6e73 666f  l_config(Transfo
-0002f3f0: 726d 6572 4f70 5061 7261 6c6c 656c 436f  rmerOpParallelCo
-0002f400: 6e66 6967 293a 2054 6865 2070 6172 616c  nfig): The paral
-0002f410: 6c65 6c20 636f 6e66 6967 7572 652e 2044  lel configure. D
-0002f420: 6566 6175 6c74 2060 6465 6661 756c 745f  efault `default_
-0002f430: 7472 616e 7366 6f72 6d65 725f 636f 6e66  transformer_conf
-0002f440: 6967 602c 0a20 2020 2020 2020 2020 2020  ig`,.           
-0002f450: 2020 2020 2061 6e20 696e 7374 616e 6365       an instance
-0002f460: 206f 6620 6054 7261 6e73 666f 726d 6572   of `Transformer
-0002f470: 4f70 5061 7261 6c6c 656c 436f 6e66 6967  OpParallelConfig
-0002f480: 6020 7769 7468 2064 6566 6175 6c74 2061  ` with default a
-0002f490: 7267 732e 0a0a 2020 2020 2020 2020 496e  rgs...        In
-0002f4a0: 7075 7473 3a0a 2020 2020 2020 2020 2020  puts:.          
-0002f4b0: 2020 2d20 2a2a 656e 636f 6465 725f 696e    - **encoder_in
-0002f4c0: 7075 7473 2a2a 2028 5465 6e73 6f72 2920  puts** (Tensor) 
-0002f4d0: 2d20 5468 6520 696e 7075 7420 7465 6e73  - The input tens
-0002f4e0: 6f72 2077 6974 6820 7368 6170 6520 5b62  or with shape [b
-0002f4f0: 6174 6368 5f73 697a 652c 2073 6571 5f6c  atch_size, seq_l
-0002f500: 656e 6774 682c 2068 6964 6465 6e5f 7369  ength, hidden_si
-0002f510: 7a65 5d20 6f72 0a20 2020 2020 2020 2020  ze] or.         
-0002f520: 2020 2020 205b 6261 7463 685f 7369 7a65       [batch_size
-0002f530: 202a 2073 6571 5f6c 656e 6774 682c 2068   * seq_length, h
-0002f540: 6964 6465 6e5f 7369 7a65 5d2e 0a20 2020  idden_size]..   
-0002f550: 2020 2020 2020 2020 202d 202a 2a65 6e63           - **enc
-0002f560: 6f64 6572 5f6d 6173 6b73 2a2a 2028 5465  oder_masks** (Te
-0002f570: 6e73 6f72 2920 2d20 5468 6520 6174 7465  nsor) - The atte
-0002f580: 6e74 696f 6e20 6d61 736b 2066 6f72 2064  ntion mask for d
-0002f590: 6563 6f64 6572 2077 6974 6820 7368 6170  ecoder with shap
-0002f5a0: 650a 2020 2020 2020 2020 2020 2020 2020  e.              
-0002f5b0: 5b62 6174 6368 5f73 697a 652c 2073 6571  [batch_size, seq
-0002f5c0: 5f6c 656e 6774 682c 2073 6571 5f6c 656e  _length, seq_len
-0002f5d0: 6774 685d 206f 7220 4e6f 6e65 2e20 4e6f  gth] or None. No
-0002f5e0: 6e65 206d 6561 6e73 2074 6865 7265 2077  ne means there w
-0002f5f0: 696c 6c20 6265 206e 6f20 6d61 736b 2069  ill be no mask i
-0002f600: 6e20 736f 6674 6d61 7820 636f 6d70 7574  n softmax comput
-0002f610: 6174 696f 6e0a 2020 2020 2020 2020 2020  ation.          
-0002f620: 2020 2020 696e 2073 656c 6620 6174 7465      in self atte
-0002f630: 6e74 696f 6e20 6f66 2074 6865 2065 6e63  ntion of the enc
-0002f640: 6f64 6572 206d 6f64 756c 652e 0a20 2020  oder module..   
-0002f650: 2020 2020 2020 2020 202d 202a 2a64 6563           - **dec
-0002f660: 6f64 6572 5f69 6e70 7574 732a 2a20 2854  oder_inputs** (T
-0002f670: 656e 736f 7229 202d 2054 6865 206f 7574  ensor) - The out
-0002f680: 7075 7420 6f66 2074 6865 2065 6e63 6f64  put of the encod
-0002f690: 6572 2077 6974 6820 7368 6170 6520 5b62  er with shape [b
-0002f6a0: 6174 6368 5f73 697a 652c 2073 6571 5f6c  atch_size, seq_l
-0002f6b0: 656e 6774 682c 2068 6964 6465 6e5f 7369  ength, hidden_si
-0002f6c0: 7a65 5d0a 2020 2020 2020 2020 2020 2020  ze].            
-0002f6d0: 2020 6f72 205b 6261 7463 685f 7369 7a65    or [batch_size
-0002f6e0: 202a 2073 6571 5f6c 656e 6774 682c 2068   * seq_length, h
-0002f6f0: 6964 6465 6e5f 7369 7a65 5d2c 2074 6869  idden_size], thi
-0002f700: 7320 7368 6f75 6c64 2062 6520 6e6f 6e65  s should be none
-0002f710: 2069 6620 7468 6520 6465 636f 6465 7220   if the decoder 
-0002f720: 6c61 7965 7220 6973 2030 2e0a 2020 2020  layer is 0..    
-0002f730: 2020 2020 2020 2020 2d20 2a2a 6465 636f          - **deco
-0002f740: 6465 725f 6d61 736b 732a 2a20 2854 656e  der_masks** (Ten
-0002f750: 736f 7229 202d 2054 6865 2061 7474 656e  sor) - The atten
-0002f760: 7469 6f6e 206d 6173 6b20 666f 7220 6465  tion mask for de
-0002f770: 636f 6465 7220 7769 7468 2073 6861 7065  coder with shape
-0002f780: 0a20 2020 2020 2020 2020 2020 2020 205b  .              [
-0002f790: 6261 7463 685f 7369 7a65 2c20 7365 715f  batch_size, seq_
-0002f7a0: 6c65 6e67 7468 2c20 7365 715f 6c65 6e67  length, seq_leng
-0002f7b0: 7468 5d20 6f72 204e 6f6e 652e 204e 6f6e  th] or None. Non
-0002f7c0: 6520 6d65 616e 7320 7468 6572 6520 7769  e means there wi
-0002f7d0: 6c6c 2062 6520 6e6f 206d 6173 6b20 696e  ll be no mask in
-0002f7e0: 2073 6f66 746d 6178 2063 6f6d 7075 7461   softmax computa
-0002f7f0: 7469 6f6e 0a20 2020 2020 2020 2020 2020  tion.           
-0002f800: 2020 2069 6e20 7365 6c66 2061 7474 656e     in self atten
-0002f810: 7469 6f6e 206f 6620 7468 6520 6465 636f  tion of the deco
-0002f820: 6465 7220 6d6f 6475 6c65 2e0a 2020 2020  der module..    
-0002f830: 2020 2020 2020 2020 2d20 2a2a 6d65 6d6f          - **memo
-0002f840: 7279 5f6d 6173 6b2a 2a20 2854 656e 736f  ry_mask** (Tenso
-0002f850: 7229 202d 2054 6865 206d 656d 6f72 7920  r) - The memory 
-0002f860: 6d61 736b 206f 6620 7468 6520 6372 6f73  mask of the cros
-0002f870: 7320 6174 7465 6e74 696f 6e20 7769 7468  s attention with
-0002f880: 2073 6861 7065 205b 6261 7463 682c 2074   shape [batch, t
-0002f890: 6774 5f73 6571 5f6c 656e 6774 682c 0a20  gt_seq_length,. 
-0002f8a0: 2020 2020 2020 2020 2020 2020 2073 7263               src
-0002f8b0: 5f73 6571 5f6c 656e 6774 685d 0a20 2020  _seq_length].   
-0002f8c0: 2020 2020 2020 2020 2020 2077 6865 7265             where
-0002f8d0: 2074 6774 5f73 6571 5f6c 656e 6774 6820   tgt_seq_length 
-0002f8e0: 6973 2074 6865 206c 656e 6774 6820 6f66  is the length of
-0002f8f0: 2074 6865 2064 6563 6f64 6572 2e20 5468   the decoder. Th
-0002f900: 6520 6f75 7470 7574 206f 6620 7468 6520  e output of the 
-0002f910: 656e 636f 6465 7220 7769 7468 2073 6861  encoder with sha
-0002f920: 7065 205b 6261 7463 685f 7369 7a65 2c0a  pe [batch_size,.
-0002f930: 2020 2020 2020 2020 2020 2020 2020 7365                se
-0002f940: 715f 6c65 6e67 7468 2c20 6869 6464 656e  q_length, hidden
-0002f950: 5f73 697a 655d 2c20 7468 6973 2073 686f  _size], this sho
-0002f960: 756c 6420 6265 206e 6f6e 6520 6966 2074  uld be none if t
-0002f970: 6865 2064 6563 6f64 6572 206c 6179 6572  he decoder layer
-0002f980: 2069 7320 3020 6f72 2074 6865 2075 7365   is 0 or the use
-0002f990: 7220 7761 6e74 7320 6e6f 206d 6173 6b2e  r wants no mask.
-0002f9a0: 0a20 2020 2020 2020 2020 2020 202d 202a  .            - *
-0002f9b0: 2a69 6e69 745f 7265 7365 742a 2a20 2854  *init_reset** (T
-0002f9c0: 656e 736f 7229 202d 2041 2062 6f6f 6c20  ensor) - A bool 
-0002f9d0: 7465 6e73 6f72 2077 6974 6820 7368 6170  tensor with shap
-0002f9e0: 6520 5b31 5d2c 2075 7365 6420 746f 2063  e [1], used to c
-0002f9f0: 6c65 6172 2074 6865 2070 6173 7420 6b65  lear the past ke
-0002fa00: 7920 7061 7261 6d65 7465 7220 616e 640a  y parameter and.
-0002fa10: 2020 2020 2020 2020 2020 2020 2020 7061                pa
-0002fa20: 7374 2076 616c 7565 2070 6172 616d 6574  st value paramet
-0002fa30: 6572 2075 7365 6420 696e 2074 6865 2069  er used in the i
-0002fa40: 6e63 7265 6d65 6e74 616c 2070 7265 6469  ncremental predi
-0002fa50: 6374 696f 6e2e 204f 6e6c 7920 7661 6c69  ction. Only vali
-0002fa60: 6420 7768 656e 2075 7365 5f70 6173 7420  d when use_past 
-0002fa70: 6973 2054 7275 652e 2044 6566 6175 6c74  is True. Default
-0002fa80: 2054 7275 652e 0a20 2020 2020 2020 2020   True..         
-0002fa90: 2020 202d 202a 2a62 6174 6368 5f76 616c     - **batch_val
-0002faa0: 6964 5f6c 656e 6774 682a 2a20 2854 656e  id_length** (Ten
-0002fab0: 736f 7229 202d 2049 6e74 3332 2074 656e  sor) - Int32 ten
-0002fac0: 736f 7220 7769 7468 2073 6861 7065 205b  sor with shape [
-0002fad0: 6261 7463 685f 7369 7a65 5d20 7468 6520  batch_size] the 
-0002fae0: 7061 7374 2063 616c 6375 6c61 7465 6420  past calculated 
-0002faf0: 7468 6520 696e 6465 782e 0a20 2020 2020  the index..     
-0002fb00: 2020 2020 2020 2020 2055 7365 6420 666f           Used fo
-0002fb10: 7220 696e 6372 656d 656e 7461 6c20 7072  r incremental pr
-0002fb20: 6564 6963 7469 6f6e 2077 6865 6e20 7468  ediction when th
-0002fb30: 6520 7573 655f 7061 7374 2069 7320 5472  e use_past is Tr
-0002fb40: 7565 2e20 4465 6661 756c 7420 4e6f 6e65  ue. Default None
-0002fb50: 2e0a 0a20 2020 2020 2020 204f 7574 7075  ...        Outpu
-0002fb60: 7473 3a0a 2020 2020 2020 2020 2020 2020  ts:.            
-0002fb70: 5475 706c 652c 2061 2074 7570 6c65 2063  Tuple, a tuple c
-0002fb80: 6f6e 7461 696e 7328 606f 7574 7075 7460  ontains(`output`
-0002fb90: 2c20 6065 6e63 6f64 6572 5f6c 6179 6572  , `encoder_layer
-0002fba0: 5f70 7265 7365 6e74 602c 2060 6465 636f  _present`, `deco
-0002fbb0: 6465 725f 6c61 7965 725f 7072 6573 656e  der_layer_presen
-0002fbc0: 7460 2c20 6061 6363 756d 5f6c 6f73 7360  t`, `accum_loss`
-0002fbd0: 290a 0a20 2020 2020 2020 2020 2020 202d  )..            -
-0002fbe0: 202a 2a6f 7574 7075 742a 2a20 2854 656e   **output** (Ten
-0002fbf0: 736f 7229 202d 2049 6620 7468 6572 6520  sor) - If there 
-0002fc00: 6973 206f 6e6c 7920 656e 636f 6465 722c  is only encoder,
-0002fc10: 2074 6865 206f 7574 7075 7420 6c6f 6769   the output logi
-0002fc20: 7420 6f66 2074 6865 2065 6e63 6f64 6572  t of the encoder
-0002fc30: 206c 6179 6572 2e20 5468 6520 7368 6170   layer. The shap
-0002fc40: 6520 6973 0a20 2020 2020 2020 2020 2020  e is.           
-0002fc50: 2020 205b 6261 7463 682c 2073 7263 5f73     [batch, src_s
-0002fc60: 6571 5f6c 656e 6774 682c 2068 6964 6465  eq_length, hidde
-0002fc70: 6e5f 7369 7a65 5d20 6f72 205b 6261 7463  n_size] or [batc
-0002fc80: 6820 2a20 7372 635f 7365 715f 6c65 6e67  h * src_seq_leng
-0002fc90: 7468 2c20 6869 6464 656e 5f73 697a 655d  th, hidden_size]
-0002fca0: 2c20 6966 2074 6865 7265 2061 7265 2065  , if there are e
-0002fcb0: 6e63 6f64 6572 2061 6e64 0a20 2020 2020  ncoder and.     
-0002fcc0: 2020 2020 2020 2020 2064 6563 6f64 6572           decoder
-0002fcd0: 732c 2074 6865 206f 7574 7075 7420 6973  s, the output is
-0002fce0: 2066 726f 6d20 7468 6520 6465 636f 6465   from the decode
-0002fcf0: 7220 6c61 7965 722e 2054 6865 2073 6861  r layer. The sha
-0002fd00: 7065 2069 7320 5b62 6174 6368 2c20 7467  pe is [batch, tg
-0002fd10: 745f 7365 715f 6c65 6e67 7468 2c20 6869  t_seq_length, hi
-0002fd20: 6464 656e 5f73 697a 655d 206f 720a 2020  dden_size] or.  
-0002fd30: 2020 2020 2020 2020 2020 2020 5b62 6174              [bat
-0002fd40: 6368 202a 2074 6774 5f73 6571 5f6c 656e  ch * tgt_seq_len
-0002fd50: 6774 682c 2068 6964 6465 6e5f 7369 7a65  gth, hidden_size
-0002fd60: 5d2e 0a20 2020 2020 2020 2020 2020 202d  ]..            -
-0002fd70: 202a 2a65 6e63 6f64 6572 5f6c 6179 6572   **encoder_layer
-0002fd80: 5f70 7265 7365 6e74 2a2a 2028 5475 706c  _present** (Tupl
-0002fd90: 6529 202d 2041 2074 7570 6c65 2077 6974  e) - A tuple wit
-0002fda0: 6820 7369 7a65 206f 6620 6e75 6d5f 6c61  h size of num_la
-0002fdb0: 7965 7273 2c20 7768 6572 6520 6561 6368  yers, where each
-0002fdc0: 2074 7570 6c65 2069 7320 7468 6520 7465   tuple is the te
-0002fdd0: 6e73 6f72 2074 6865 0a20 2020 2020 2020  nsor the.       
-0002fde0: 2020 2020 2020 2070 726f 6a65 6374 6564         projected
-0002fdf0: 206b 6579 2061 6e64 2076 616c 7565 2076   key and value v
-0002fe00: 6563 746f 7220 696e 2073 656c 6620 6174  ector in self at
-0002fe10: 7465 6e74 696f 6e20 7769 7468 2073 6861  tention with sha
-0002fe20: 7065 2028 2862 6174 6368 5f73 697a 652c  pe ((batch_size,
-0002fe30: 206e 756d 5f68 6561 6473 2c20 7369 7a65   num_heads, size
-0002fe40: 5f70 6572 5f68 6561 642c 0a20 2020 2020  _per_head,.     
-0002fe50: 2020 2020 2020 2020 2073 7263 5f73 6571           src_seq
-0002fe60: 5f6c 656e 6774 6829 2c20 2862 6174 6368  _length), (batch
-0002fe70: 5f73 697a 652c 206e 756d 5f68 6561 6473  _size, num_heads
-0002fe80: 2c20 7372 635f 7365 715f 6c65 6e67 7468  , src_seq_length
-0002fe90: 2c20 7369 7a65 5f70 6572 5f68 6561 6429  , size_per_head)
-0002fea0: 292e 0a20 2020 2020 2020 2020 2020 202d  )..            -
-0002feb0: 202a 2a64 6563 6f64 6572 5f6c 6179 6572   **decoder_layer
-0002fec0: 5f70 7265 7365 6e74 2a2a 2028 5475 706c  _present** (Tupl
-0002fed0: 6529 202d 2041 2074 7570 6c65 2077 6974  e) - A tuple wit
-0002fee0: 6820 7369 7a65 206f 6620 6e75 6d5f 6c61  h size of num_la
-0002fef0: 7965 7273 2c20 7768 6572 6520 6561 6368  yers, where each
-0002ff00: 2074 7570 6c65 2069 7320 7468 6520 7465   tuple is the te
-0002ff10: 6e73 6f72 0a20 2020 2020 2020 2020 2020  nsor.           
-0002ff20: 2020 206f 6620 7468 6520 7072 6f6a 6563     of the projec
-0002ff30: 7465 6420 6b65 7920 616e 6420 7661 6c75  ted key and valu
-0002ff40: 6520 7665 6374 6f72 2069 6e20 7365 6c66  e vector in self
-0002ff50: 2061 7474 656e 7469 6f6e 2077 6974 6820   attention with 
-0002ff60: 7368 6170 6520 2828 6261 7463 685f 7369  shape ((batch_si
-0002ff70: 7a65 2c20 6e75 6d5f 6865 6164 732c 2073  ze, num_heads, s
-0002ff80: 697a 655f 7065 725f 6865 6164 2c0a 2020  ize_per_head,.  
-0002ff90: 2020 2020 2020 2020 2020 2020 7467 745f              tgt_
-0002ffa0: 7365 715f 6c65 6e67 7468 292c 2028 6261  seq_length), (ba
-0002ffb0: 7463 685f 7369 7a65 2c20 6e75 6d5f 6865  tch_size, num_he
-0002ffc0: 6164 732c 2074 6774 5f73 6571 5f6c 656e  ads, tgt_seq_len
-0002ffd0: 6774 682c 2073 697a 655f 7065 725f 6865  gth, size_per_he
-0002ffe0: 6164 2929 2c20 616e 6420 7468 650a 2020  ad)), and the.  
-0002fff0: 2020 2020 2020 2020 2020 2020 7072 6f6a              proj
-00030000: 6563 7465 6420 6b65 7920 616e 6420 7661  ected key and va
-00030010: 6c75 6520 7665 6374 6f72 2069 6e20 6372  lue vector in cr
-00030020: 6f73 7320 6174 7465 6e74 696f 6e20 7769  oss attention wi
-00030030: 7468 2073 6861 7065 0a20 2020 2020 2020  th shape.       
-00030040: 2020 2020 2020 2028 2862 6174 6368 5f73         ((batch_s
-00030050: 697a 652c 206e 756d 5f68 6561 6473 2c20  ize, num_heads, 
-00030060: 7369 7a65 5f70 6572 5f68 6561 642c 2073  size_per_head, s
-00030070: 7263 5f73 6571 5f6c 656e 6774 6829 2c0a  rc_seq_length),.
-00030080: 2020 2020 2020 2020 2020 2020 2020 2862                (b
-00030090: 6174 6368 5f73 697a 652c 206e 756d 5f68  atch_size, num_h
-000300a0: 6561 6473 2c20 7372 635f 7365 715f 6c65  eads, src_seq_le
-000300b0: 6e67 7468 2c20 7369 7a65 5f70 6572 5f68  ngth, size_per_h
-000300c0: 6561 6429 292e 2049 6620 7468 6520 6465  ead)). If the de
-000300d0: 636f 6465 7220 6973 206e 6f74 2073 6574  coder is not set
-000300e0: 2c20 7468 650a 2020 2020 2020 2020 2020  , the.          
-000300f0: 2020 2020 7265 7475 726e 6564 2076 616c      returned val
-00030100: 7565 2077 696c 6c20 6265 204e 6f6e 652e  ue will be None.
-00030110: 0a20 2020 2020 2020 2020 2020 202d 202a  .            - *
-00030120: 2a61 6363 756d 5f6c 6f73 732a 2a20 2854  *accum_loss** (T
-00030130: 656e 736f 7229 202d 2041 2054 656e 736f  ensor) - A Tenso
-00030140: 7220 696e 6469 6361 7465 7320 616e 2061  r indicates an a
-00030150: 7578 696c 6961 7279 206c 6f73 7320 746f  uxiliary loss to
-00030160: 206d 696e 696d 697a 6520 7468 6520 6d65   minimize the me
-00030170: 616e 2073 7175 6172 6520 6f66 2074 6865  an square of the
-00030180: 2064 6174 610a 2020 2020 2020 2020 2020   data.          
-00030190: 2020 2020 7061 7274 2072 6f75 7465 6420      part routed 
-000301a0: 746f 2065 6163 6820 6578 7065 7274 2c20  to each expert, 
-000301b0: 616e 6420 6f6e 6c79 2072 6574 7572 6e65  and only returne
-000301c0: 6420 6966 2074 6865 206e 756d 6265 7220  d if the number 
-000301d0: 6f66 2065 7870 6572 7473 2069 7320 6772  of experts is gr
-000301e0: 6561 7465 7220 7468 616e 2031 2e0a 0a20  eater than 1... 
-000301f0: 2020 2020 2020 2053 7570 706f 7274 6564         Supported
-00030200: 2050 6c61 7466 6f72 6d73 3a0a 2020 2020   Platforms:.    
-00030210: 2020 2020 2020 2020 6060 4173 6365 6e64          ``Ascend
-00030220: 6060 2060 6047 5055 6060 0a0a 2020 2020  `` ``GPU``..    
-00030230: 2020 2020 4578 616d 706c 6573 3a0a 2020      Examples:.  
-00030240: 2020 2020 2020 2020 2020 3e3e 3e20 696d            >>> im
-00030250: 706f 7274 206e 756d 7079 2061 7320 6e70  port numpy as np
-00030260: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
-00030270: 2066 726f 6d20 6d69 6e64 7370 6f72 6520   from mindspore 
-00030280: 696d 706f 7274 2064 7479 7065 2061 7320  import dtype as 
-00030290: 6d73 7479 7065 0a20 2020 2020 2020 2020  mstype.         
-000302a0: 2020 203e 3e3e 2066 726f 6d20 6d69 6e64     >>> from mind
-000302b0: 666f 726d 6572 732e 6d6f 6475 6c65 732e  formers.modules.
-000302c0: 7472 616e 7366 6f72 6d65 7220 696d 706f  transformer impo
-000302d0: 7274 2054 7261 6e73 666f 726d 6572 0a20  rt Transformer. 
-000302e0: 2020 2020 2020 2020 2020 203e 3e3e 2066             >>> f
-000302f0: 726f 6d20 6d69 6e64 7370 6f72 6520 696d  rom mindspore im
-00030300: 706f 7274 2054 656e 736f 720a 2020 2020  port Tensor.    
-00030310: 2020 2020 2020 2020 3e3e 3e20 6d6f 6465          >>> mode
-00030320: 6c20 3d20 5472 616e 7366 6f72 6d65 7228  l = Transformer(
-00030330: 6261 7463 685f 7369 7a65 3d32 2c20 656e  batch_size=2, en
-00030340: 636f 6465 725f 6c61 7965 7273 3d31 2c20  coder_layers=1, 
-00030350: 6465 636f 6465 725f 6c61 7965 7273 3d32  decoder_layers=2
-00030360: 2c20 6869 6464 656e 5f73 697a 653d 3634  , hidden_size=64
-00030370: 2c0a 2020 2020 2020 2020 2020 2020 2e2e  ,.            ..
-00030380: 2e20 2020 2020 2020 2020 2020 2020 2020  .               
-00030390: 2020 2020 2020 6666 6e5f 6869 6464 656e        ffn_hidden
-000303a0: 5f73 697a 653d 3634 2c20 7372 635f 7365  _size=64, src_se
-000303b0: 715f 6c65 6e67 7468 3d32 302c 2074 6774  q_length=20, tgt
-000303c0: 5f73 6571 5f6c 656e 6774 683d 3130 290a  _seq_length=10).
-000303d0: 2020 2020 2020 2020 2020 2020 3e3e 3e20              >>> 
-000303e0: 656e 636f 6465 725f 696e 7075 745f 7661  encoder_input_va
-000303f0: 6c75 6520 3d20 5465 6e73 6f72 286e 702e  lue = Tensor(np.
-00030400: 6f6e 6573 2828 322c 2032 302c 2036 3429  ones((2, 20, 64)
-00030410: 292c 206d 7374 7970 652e 666c 6f61 7433  ), mstype.float3
-00030420: 3229 0a20 2020 2020 2020 2020 2020 203e  2).            >
-00030430: 3e3e 2065 6e63 6f64 6572 5f69 6e70 7574  >> encoder_input
-00030440: 5f6d 6173 6b20 3d20 5465 6e73 6f72 286e  _mask = Tensor(n
-00030450: 702e 6f6e 6573 2828 322c 2032 302c 2032  p.ones((2, 20, 2
-00030460: 3029 292c 206d 7374 7970 652e 666c 6f61  0)), mstype.floa
-00030470: 7431 3629 0a20 2020 2020 2020 2020 2020  t16).           
-00030480: 203e 3e3e 2064 6563 6f64 6572 5f69 6e70   >>> decoder_inp
-00030490: 7574 5f76 616c 7565 203d 2054 656e 736f  ut_value = Tenso
-000304a0: 7228 6e70 2e6f 6e65 7328 2832 2c20 3130  r(np.ones((2, 10
-000304b0: 2c20 3634 2929 2c20 6d73 7479 7065 2e66  , 64)), mstype.f
-000304c0: 6c6f 6174 3332 290a 2020 2020 2020 2020  loat32).        
-000304d0: 2020 2020 3e3e 3e20 6465 636f 6465 725f      >>> decoder_
-000304e0: 696e 7075 745f 6d61 736b 203d 2054 656e  input_mask = Ten
-000304f0: 736f 7228 6e70 2e6f 6e65 7328 2832 2c20  sor(np.ones((2, 
-00030500: 3130 2c20 3130 2929 2c20 6d73 7479 7065  10, 10)), mstype
-00030510: 2e66 6c6f 6174 3136 290a 2020 2020 2020  .float16).      
-00030520: 2020 2020 2020 3e3e 3e20 6d65 6d6f 7279        >>> memory
-00030530: 5f6d 6173 6b20 3d20 5465 6e73 6f72 286e  _mask = Tensor(n
-00030540: 702e 6f6e 6573 2828 322c 2031 302c 2032  p.ones((2, 10, 2
-00030550: 3029 292c 206d 7374 7970 652e 666c 6f61  0)), mstype.floa
-00030560: 7431 3629 0a20 2020 2020 2020 2020 2020  t16).           
-00030570: 203e 3e3e 206f 7574 7075 742c 2065 6e5f   >>> output, en_
-00030580: 7061 7374 2c20 6465 5f70 6173 7420 3d20  past, de_past = 
-00030590: 6d6f 6465 6c28 656e 636f 6465 725f 696e  model(encoder_in
-000305a0: 7075 745f 7661 6c75 652c 2065 6e63 6f64  put_value, encod
-000305b0: 6572 5f69 6e70 7574 5f6d 6173 6b2c 2064  er_input_mask, d
-000305c0: 6563 6f64 6572 5f69 6e70 7574 5f76 616c  ecoder_input_val
-000305d0: 7565 2c0a 2020 2020 2020 2020 2020 2020  ue,.            
-000305e0: 2e2e 2e20 2020 2020 2020 2020 2020 2020  ...             
-000305f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030600: 2020 2020 2064 6563 6f64 6572 5f69 6e70       decoder_inp
-00030610: 7574 5f6d 6173 6b2c 206d 656d 6f72 795f  ut_mask, memory_
-00030620: 6d61 736b 290a 2020 2020 2020 2020 2020  mask).          
-00030630: 2020 3e3e 3e20 7072 696e 7428 6f75 7470    >>> print(outp
-00030640: 7574 2e73 6861 7065 290a 2020 2020 2020  ut.shape).      
-00030650: 2020 2020 2020 2832 2c20 3130 2c20 3634        (2, 10, 64
-00030660: 290a 2020 2020 2020 2020 2020 2020 3e3e  ).            >>
-00030670: 3e20 7072 696e 7428 6c65 6e28 656e 5f70  > print(len(en_p
-00030680: 6173 7429 290a 2020 2020 2020 2020 2020  ast)).          
-00030690: 2020 310a 2020 2020 2020 2020 2020 2020    1.            
-000306a0: 3e3e 3e20 7072 696e 7428 6c65 6e28 6465  >>> print(len(de
-000306b0: 5f70 6173 7429 290a 2020 2020 2020 2020  _past)).        
-000306c0: 2020 2020 320a 2020 2020 2020 2020 2020      2.          
-000306d0: 2020 3e3e 3e20 7072 696e 7428 656e 5f70    >>> print(en_p
-000306e0: 6173 745b 305d 5b30 5d2e 7368 6170 6529  ast[0][0].shape)
-000306f0: 0a20 2020 2020 2020 2020 2020 2028 322c  .            (2,
-00030700: 2032 2c20 3332 2c20 3230 290a 2020 2020   2, 32, 20).    
-00030710: 2020 2020 2020 2020 3e3e 3e20 7072 696e          >>> prin
-00030720: 7428 656e 5f70 6173 745b 305d 5b31 5d2e  t(en_past[0][1].
-00030730: 7368 6170 6529 0a20 2020 2020 2020 2020  shape).         
-00030740: 2020 2028 322c 2032 2c20 3230 2c20 3332     (2, 2, 20, 32
-00030750: 290a 2020 2020 2020 2020 2020 2020 3e3e  ).            >>
-00030760: 3e20 7072 696e 7428 6465 5f70 6173 745b  > print(de_past[
-00030770: 305d 5b30 5d2e 7368 6170 6529 0a20 2020  0][0].shape).   
-00030780: 2020 2020 2020 2020 2028 322c 2032 2c20           (2, 2, 
-00030790: 3332 2c20 3130 290a 2020 2020 2020 2020  32, 10).        
-000307a0: 2020 2020 3e3e 3e20 7072 696e 7428 6465      >>> print(de
-000307b0: 5f70 6173 745b 305d 5b31 5d2e 7368 6170  _past[0][1].shap
-000307c0: 6529 0a20 2020 2020 2020 2020 2020 2028  e).            (
-000307d0: 322c 2032 2c20 3130 2c20 3332 290a 2020  2, 2, 10, 32).  
-000307e0: 2020 2020 2020 2020 2020 3e3e 3e20 7072            >>> pr
-000307f0: 696e 7428 6465 5f70 6173 745b 305d 5b32  int(de_past[0][2
-00030800: 5d2e 7368 6170 6529 0a20 2020 2020 2020  ].shape).       
-00030810: 2020 2020 2028 322c 2032 2c20 3332 2c20       (2, 2, 32, 
-00030820: 3230 290a 2020 2020 2020 2020 2020 2020  20).            
-00030830: 3e3e 3e20 7072 696e 7428 6465 5f70 6173  >>> print(de_pas
-00030840: 745b 305d 5b33 5d2e 7368 6170 6529 0a20  t[0][3].shape). 
-00030850: 2020 2020 2020 2020 2020 2028 322c 2032             (2, 2
-00030860: 2c20 3230 2c20 3332 290a 2020 2020 2222  , 20, 32).    ""
-00030870: 220a 0a20 2020 2040 5f4c 6f67 4163 7469  "..    @_LogActi
-00030880: 6f6e 4f6e 6365 286d 5f6c 6f67 6765 723d  onOnce(m_logger=
-00030890: 6c6f 6767 6572 2c20 6b65 793d 2754 7261  logger, key='Tra
-000308a0: 6e73 666f 726d 6572 272c 0a20 2020 2020  nsformer',.     
-000308b0: 2020 2020 2020 2020 2020 2020 2020 206e                 n
-000308c0: 6f5f 7761 726e 696e 673d 5f67 6574 5f70  o_warning=_get_p
-000308d0: 6172 616c 6c65 6c5f 6d6f 6465 2829 2069  arallel_mode() i
-000308e0: 6e20 2850 6172 616c 6c65 6c4d 6f64 652e  n (ParallelMode.
-000308f0: 5354 414e 445f 414c 4f4e 452c 2929 0a20  STAND_ALONE,)). 
-00030900: 2020 2040 5f61 7267 735f 7479 7065 5f76     @_args_type_v
-00030910: 616c 6964 6174 6f72 5f63 6865 636b 2868  alidator_check(h
-00030920: 6964 6465 6e5f 7369 7a65 3d56 616c 6964  idden_size=Valid
-00030930: 6174 6f72 2e63 6865 636b 5f70 6f73 6974  ator.check_posit
-00030940: 6976 655f 696e 742c 0a20 2020 2020 2020  ive_int,.       
-00030950: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030960: 2020 2020 2020 2020 206e 756d 5f68 6561           num_hea
-00030970: 6473 3d56 616c 6964 6174 6f72 2e63 6865  ds=Validator.che
-00030980: 636b 5f70 6f73 6974 6976 655f 696e 742c  ck_positive_int,
-00030990: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-000309a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000309b0: 2066 666e 5f68 6964 6465 6e5f 7369 7a65   ffn_hidden_size
-000309c0: 3d56 616c 6964 6174 6f72 2e63 6865 636b  =Validator.check
-000309d0: 5f70 6f73 6974 6976 655f 696e 742c 0a20  _positive_int,. 
-000309e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000309f0: 2020 2020 2020 2020 2020 2020 2020 2073                 s
-00030a00: 7263 5f73 6571 5f6c 656e 6774 683d 5661  rc_seq_length=Va
-00030a10: 6c69 6461 746f 722e 6368 6563 6b5f 706f  lidator.check_po
-00030a20: 7369 7469 7665 5f69 6e74 2c0a 2020 2020  sitive_int,.    
-00030a30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030a40: 2020 2020 2020 2020 2020 2020 656e 636f              enco
-00030a50: 6465 725f 6c61 7965 7273 3d56 616c 6964  der_layers=Valid
-00030a60: 6174 6f72 2e63 6865 636b 5f70 6f73 6974  ator.check_posit
-00030a70: 6976 655f 696e 742c 0a20 2020 2020 2020  ive_int,.       
-00030a80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030a90: 2020 2020 2020 2020 2064 6563 6f64 6572           decoder
-00030aa0: 5f6c 6179 6572 733d 5661 6c69 6461 746f  _layers=Validato
-00030ab0: 722e 6368 6563 6b5f 6e6f 6e5f 6e65 6761  r.check_non_nega
-00030ac0: 7469 7665 5f69 6e74 2c0a 2020 2020 2020  tive_int,.      
-00030ad0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030ae0: 2020 2020 2020 2020 2020 7467 745f 7365            tgt_se
-00030af0: 715f 6c65 6e67 7468 3d56 616c 6964 6174  q_length=Validat
-00030b00: 6f72 2e63 6865 636b 5f70 6f73 6974 6976  or.check_positiv
-00030b10: 655f 696e 742c 0a20 2020 2020 2020 2020  e_int,.         
-00030b20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030b30: 2020 2020 2020 2061 7474 656e 7469 6f6e         attention
-00030b40: 5f64 726f 706f 7574 5f72 6174 653d 5661  _dropout_rate=Va
-00030b50: 6c69 6461 746f 722e 6368 6563 6b5f 6e6f  lidator.check_no
-00030b60: 6e5f 6e65 6761 7469 7665 5f66 6c6f 6174  n_negative_float
-00030b70: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00030b80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030b90: 2020 6869 6464 656e 5f64 726f 706f 7574    hidden_dropout
-00030ba0: 5f72 6174 653d 5661 6c69 6461 746f 722e  _rate=Validator.
-00030bb0: 6368 6563 6b5f 6e6f 6e5f 6e65 6761 7469  check_non_negati
-00030bc0: 7665 5f66 6c6f 6174 2c0a 2020 2020 2020  ve_float,.      
-00030bd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030be0: 2020 2020 2020 2020 2020 706f 7374 5f6c            post_l
-00030bf0: 6179 6572 6e6f 726d 5f72 6573 6964 7561  ayernorm_residua
-00030c00: 6c3d 5661 6c69 6461 746f 722e 6368 6563  l=Validator.chec
-00030c10: 6b5f 626f 6f6c 2c0a 2020 2020 2020 2020  k_bool,.        
-00030c20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030c30: 2020 2020 2020 2020 6c61 7965 726e 6f72          layernor
-00030c40: 6d5f 636f 6d70 7574 655f 7479 7065 3d5f  m_compute_type=_
-00030c50: 7661 6c69 645f 7661 6c75 655f 6368 6563  valid_value_chec
-00030c60: 6b73 285b 6d73 7479 7065 2e66 6c6f 6174  ks([mstype.float
-00030c70: 3332 2c0a 2020 2020 2020 2020 2020 2020  32,.            
-00030c80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030c90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030ca0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030cb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030cc0: 6d73 7479 7065 2e66 6c6f 6174 3136 2c20  mstype.float16, 
-00030cd0: 6d73 7479 7065 2e62 666c 6f61 7431 365d  mstype.bfloat16]
-00030ce0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00030cf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030d00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030d10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030d20: 2020 2020 2020 2020 2020 2020 2022 5472               "Tr
-00030d30: 616e 7366 6f72 6d65 7222 292c 0a20 2020  ansformer"),.   
-00030d40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030d50: 2020 2020 2020 2020 2020 2020 2073 6f66               sof
-00030d60: 746d 6178 5f63 6f6d 7075 7465 5f74 7970  tmax_compute_typ
-00030d70: 653d 5f76 616c 6964 5f76 616c 7565 5f63  e=_valid_value_c
-00030d80: 6865 636b 7328 5b6d 7374 7970 652e 666c  hecks([mstype.fl
-00030d90: 6f61 7433 322c 0a20 2020 2020 2020 2020  oat32,.         
-00030da0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030db0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030dc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030dd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030de0: 206d 7374 7970 652e 666c 6f61 7431 362c   mstype.float16,
-00030df0: 206d 7374 7970 652e 6266 6c6f 6174 3136   mstype.bfloat16
-00030e00: 5d2c 0a20 2020 2020 2020 2020 2020 2020  ],.             
-00030e10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030e20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030e30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030e40: 2020 2020 2020 2020 2020 2020 2254 7261              "Tra
-00030e50: 6e73 666f 726d 6572 2229 2c0a 2020 2020  nsformer"),.    
-00030e60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030e70: 2020 2020 2020 2020 2020 2020 7061 7261              para
-00030e80: 6d5f 696e 6974 5f74 7970 653d 5f76 616c  m_init_type=_val
-00030e90: 6964 5f76 616c 7565 5f63 6865 636b 7328  id_value_checks(
-00030ea0: 5b6d 7374 7970 652e 666c 6f61 7433 322c  [mstype.float32,
-00030eb0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00030ec0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030ed0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e170: 2020 2070 6172 616c 6c65 6c5f 636f 6e66     parallel_conf
+0002e180: 6967 3d63 6f6e 6669 675f 746f 5f6c 6179  ig=config_to_lay
+0002e190: 6572 290a 2020 2020 2020 2020 2020 2020  er).            
+0002e1a0: 2020 2020 2320 4966 2074 6865 2075 7365      # If the use
+0002e1b0: 7220 646f 6573 6e27 7420 7061 7373 2074  r doesn't pass t
+0002e1c0: 6865 2066 7573 696f 6e20 6675 6e63 7469  he fusion functi
+0002e1d0: 6f6e 2c20 7573 6520 7468 6520 6465 6661  on, use the defa
+0002e1e0: 756c 7420 6f6e 650a 2020 2020 2020 2020  ult one.        
+0002e1f0: 2020 2020 2020 2020 6966 206e 6f74 206c          if not l
+0002e200: 616d 6264 615f 6675 6e63 3a0a 2020 2020  ambda_func:.    
+0002e210: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e220: 6c61 6d62 6461 5f66 756e 6320 3d20 5f67  lambda_func = _g
+0002e230: 6574 5f6c 616d 6264 615f 6675 6e63 2829  et_lambda_func()
+0002e240: 0a0a 2020 2020 2020 2020 2020 2020 2020  ..              
+0002e250: 2020 6c61 6d62 6461 5f66 756e 6328 626c    lambda_func(bl
+0002e260: 6f63 6b2c 206c 6179 6572 5f69 643d 692c  ock, layer_id=i,
+0002e270: 206c 6179 6572 733d 6e75 6d5f 6c61 7965   layers=num_laye
+0002e280: 7273 2c0a 2020 2020 2020 2020 2020 2020  rs,.            
+0002e290: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e2a0: 6f66 6673 6574 3d6f 6666 7365 742c 2070  offset=offset, p
+0002e2b0: 6172 616c 6c65 6c5f 636f 6e66 6967 3d70  arallel_config=p
+0002e2c0: 6172 616c 6c65 6c5f 636f 6e66 6967 290a  arallel_config).
+0002e2d0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0002e2e0: 2073 656c 662e 626c 6f63 6b73 2e61 7070   self.blocks.app
+0002e2f0: 656e 6428 626c 6f63 6b29 0a20 2020 2020  end(block).     
+0002e300: 2020 2065 6c73 653a 0a20 2020 2020 2020     else:.       
+0002e310: 2020 2020 2072 6169 7365 2052 756e 7469       raise Runti
+0002e320: 6d65 4572 726f 7228 6622 5468 6520 7b73  meError(f"The {s
+0002e330: 656c 662e 636c 735f 6e61 6d65 7d20 6f6e  elf.cls_name} on
+0002e340: 6c79 2073 7570 706f 7274 2073 6861 7264  ly support shard
+0002e350: 696e 6720 7072 6f70 6167 6174 696f 6e20  ing propagation 
+0002e360: 6f72 2022 0a20 2020 2020 2020 2020 2020  or ".           
+0002e370: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e380: 2020 2020 6622 7365 6d69 2d61 7574 6f20      f"semi-auto 
+0002e390: 7061 7261 6c6c 656c 206d 6f64 6520 6e6f  parallel mode no
+0002e3a0: 772e 2229 0a0a 2020 2020 6465 6620 636f  w.")..    def co
+0002e3b0: 6e73 7472 7563 7428 7365 6c66 2c20 6869  nstruct(self, hi
+0002e3c0: 6464 656e 5f73 7461 7465 732c 2061 7474  dden_states, att
+0002e3d0: 656e 7469 6f6e 5f6d 6173 6b2c 2065 6e63  ention_mask, enc
+0002e3e0: 6f64 6572 5f6f 7574 7075 743d 4e6f 6e65  oder_output=None
+0002e3f0: 2c20 6d65 6d6f 7279 5f6d 6173 6b3d 4e6f  , memory_mask=No
+0002e400: 6e65 2c0a 2020 2020 2020 2020 2020 2020  ne,.            
+0002e410: 2020 2020 2020 696e 6974 5f72 6573 6574        init_reset
+0002e420: 3d54 7275 652c 2062 6174 6368 5f76 616c  =True, batch_val
+0002e430: 6964 5f6c 656e 6774 683d 4e6f 6e65 293a  id_length=None):
+0002e440: 0a20 2020 2020 2020 2022 2222 666f 7277  .        """forw
+0002e450: 6172 6420 7072 6f63 6573 7322 2222 0a20  ard process""". 
+0002e460: 2020 2020 2020 2070 7265 7365 6e74 5f6c         present_l
+0002e470: 6179 6572 203d 2028 290a 2020 2020 2020  ayer = ().      
+0002e480: 2020 6966 2073 656c 662e 7573 655f 6d6f    if self.use_mo
+0002e490: 653a 0a20 2020 2020 2020 2020 2020 2061  e:.            a
+0002e4a0: 6363 756d 5f6c 6f73 7320 3d20 7365 6c66  ccum_loss = self
+0002e4b0: 2e61 7578 5f6c 6f73 730a 2020 2020 2020  .aux_loss.      
+0002e4c0: 2020 2020 2020 666f 7220 6920 696e 2072        for i in r
+0002e4d0: 616e 6765 2873 656c 662e 6e75 6d5f 6c61  ange(self.num_la
+0002e4e0: 7965 7273 293a 0a20 2020 2020 2020 2020  yers):.         
+0002e4f0: 2020 2020 2020 2068 6964 6465 6e5f 7374         hidden_st
+0002e500: 6174 6573 2c20 7072 6573 656e 742c 2061  ates, present, a
+0002e510: 7578 5f6c 6f73 7320 3d20 7365 6c66 2e62  ux_loss = self.b
+0002e520: 6c6f 636b 735b 695d 2868 6964 6465 6e5f  locks[i](hidden_
+0002e530: 7374 6174 6573 2c0a 2020 2020 2020 2020  states,.        
+0002e540: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e550: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e560: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e570: 2020 2020 2020 2020 2020 6174 7465 6e74            attent
+0002e580: 696f 6e5f 6d61 736b 2c0a 2020 2020 2020  ion_mask,.      
+0002e590: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e5a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e5b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e5c0: 2020 2020 2020 2020 2020 2020 656e 636f              enco
+0002e5d0: 6465 725f 6f75 7470 7574 2c0a 2020 2020  der_output,.    
+0002e5e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e5f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e600: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e610: 2020 2020 2020 2020 2020 2020 2020 6d65                me
+0002e620: 6d6f 7279 5f6d 6173 6b2c 0a20 2020 2020  mory_mask,.     
+0002e630: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e640: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e650: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e660: 2020 2020 2020 2020 2020 2020 2069 6e69               ini
+0002e670: 745f 7265 7365 742c 0a20 2020 2020 2020  t_reset,.       
+0002e680: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e690: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e6a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e6b0: 2020 2020 2020 2020 2020 2062 6174 6368             batch
+0002e6c0: 5f76 616c 6964 5f6c 656e 6774 6829 0a20  _valid_length). 
+0002e6d0: 2020 2020 2020 2020 2020 2020 2020 2070                 p
+0002e6e0: 7265 7365 6e74 5f6c 6179 6572 203d 2070  resent_layer = p
+0002e6f0: 7265 7365 6e74 5f6c 6179 6572 202b 2028  resent_layer + (
+0002e700: 7072 6573 656e 742c 290a 2020 2020 2020  present,).      
+0002e710: 2020 2020 2020 2020 2020 6163 6375 6d5f            accum_
+0002e720: 6c6f 7373 203d 2073 656c 662e 6164 6428  loss = self.add(
+0002e730: 6163 6375 6d5f 6c6f 7373 2c20 6175 785f  accum_loss, aux_
+0002e740: 6c6f 7373 290a 2020 2020 2020 2020 2020  loss).          
+0002e750: 2020 7265 7475 726e 2068 6964 6465 6e5f    return hidden_
+0002e760: 7374 6174 6573 2c20 7072 6573 656e 745f  states, present_
+0002e770: 6c61 7965 722c 2061 6363 756d 5f6c 6f73  layer, accum_los
+0002e780: 730a 0a20 2020 2020 2020 2023 204c 6f6f  s..        # Loo
+0002e790: 7020 7468 726f 7567 6820 6561 6368 2073  p through each s
+0002e7a0: 656c 662d 6174 7465 6e74 696f 6e20 6c61  elf-attention la
+0002e7b0: 7965 720a 2020 2020 2020 2020 666f 7220  yer.        for 
+0002e7c0: 6920 696e 2072 616e 6765 2873 656c 662e  i in range(self.
+0002e7d0: 6e75 6d5f 6c61 7965 7273 293a 0a20 2020  num_layers):.   
+0002e7e0: 2020 2020 2020 2020 2068 6964 6465 6e5f           hidden_
+0002e7f0: 7374 6174 6573 2c20 7072 6573 656e 7420  states, present 
+0002e800: 3d20 7365 6c66 2e62 6c6f 636b 735b 695d  = self.blocks[i]
+0002e810: 2868 6964 6465 6e5f 7374 6174 6573 2c0a  (hidden_states,.
+0002e820: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e830: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e840: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e850: 2020 2020 6174 7465 6e74 696f 6e5f 6d61      attention_ma
+0002e860: 736b 2c0a 2020 2020 2020 2020 2020 2020  sk,.            
+0002e870: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e880: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e890: 2020 2020 2020 2020 656e 636f 6465 725f          encoder_
+0002e8a0: 6f75 7470 7574 2c0a 2020 2020 2020 2020  output,.        
+0002e8b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e8c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e8d0: 2020 2020 2020 2020 2020 2020 6d65 6d6f              memo
+0002e8e0: 7279 5f6d 6173 6b2c 0a20 2020 2020 2020  ry_mask,.       
+0002e8f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e900: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e910: 2020 2020 2020 2020 2020 2020 2069 6e69               ini
+0002e920: 745f 7265 7365 742c 0a20 2020 2020 2020  t_reset,.       
+0002e930: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e940: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002e950: 2020 2020 2020 2020 2020 2020 2062 6174               bat
+0002e960: 6368 5f76 616c 6964 5f6c 656e 6774 6829  ch_valid_length)
+0002e970: 0a20 2020 2020 2020 2020 2020 2070 7265  .            pre
+0002e980: 7365 6e74 5f6c 6179 6572 203d 2070 7265  sent_layer = pre
+0002e990: 7365 6e74 5f6c 6179 6572 202b 2028 7072  sent_layer + (pr
+0002e9a0: 6573 656e 742c 290a 0a20 2020 2020 2020  esent,)..       
+0002e9b0: 2072 6574 7572 6e20 6869 6464 656e 5f73   return hidden_s
+0002e9c0: 7461 7465 732c 2070 7265 7365 6e74 5f6c  tates, present_l
+0002e9d0: 6179 6572 0a0a 0a63 6c61 7373 2054 7261  ayer...class Tra
+0002e9e0: 6e73 666f 726d 6572 2843 656c 6c29 3a0a  nsformer(Cell):.
+0002e9f0: 2020 2020 7222 2222 0a20 2020 2020 2020      r""".       
+0002ea00: 2054 7261 6e73 666f 726d 6572 206d 6f64   Transformer mod
+0002ea10: 756c 6520 696e 636c 7564 696e 6720 656e  ule including en
+0002ea20: 636f 6465 7220 616e 6420 6465 636f 6465  coder and decode
+0002ea30: 722e 2054 6865 2064 6966 6665 7265 6e63  r. The differenc
+0002ea40: 6520 7769 7468 2074 6865 206f 7269 6769  e with the origi
+0002ea50: 6e61 6c20 696d 706c 656d 656e 7473 2069  nal implements i
+0002ea60: 7320 7468 6520 6d6f 6475 6c65 2075 7365  s the module use
+0002ea70: 0a20 2020 2020 2020 2074 6865 2072 6573  .        the res
+0002ea80: 6964 7561 6c20 6164 6469 7469 6f6e 2062  idual addition b
+0002ea90: 6566 6f72 6520 7468 6520 6c61 7965 7220  efore the layer 
+0002eaa0: 6e6f 726d 616c 697a 6174 696f 6e2e 2041  normalization. A
+0002eab0: 6e64 2074 6865 2064 6566 6175 6c74 2068  nd the default h
+0002eac0: 6964 6465 6e20 6163 7420 6973 2060 6765  idden act is `ge
+0002ead0: 6c75 602e 0a20 2020 2020 2020 2054 6865  lu`..        The
+0002eae0: 2064 6574 6169 6c73 2063 616e 2062 6520   details can be 
+0002eaf0: 666f 756e 6420 696e 2060 4174 7465 6e74  found in `Attent
+0002eb00: 696f 6e20 6973 2061 6c6c 2079 6f75 206e  ion is all you n
+0002eb10: 6565 6420 3c68 7474 7073 3a2f 2f61 7278  eed <https://arx
+0002eb20: 6976 2e6f 7267 2f70 6466 2f31 3730 362e  iv.org/pdf/1706.
+0002eb30: 3033 3736 3276 352e 7064 663e 605f 2e0a  03762v5.pdf>`_..
+0002eb40: 0a20 2020 2020 2020 204e 6f74 653a 0a20  .        Note:. 
+0002eb50: 2020 2020 2020 2020 2020 2054 6869 7320             This 
+0002eb60: 6973 2061 6e20 6578 7065 7269 6d65 6e74  is an experiment
+0002eb70: 616c 2069 6e74 6572 6661 6365 2074 6861  al interface tha
+0002eb80: 7420 6973 2073 7562 6a65 6374 2074 6f20  t is subject to 
+0002eb90: 6368 616e 6765 206f 7220 6465 6c65 7469  change or deleti
+0002eba0: 6f6e 2e0a 0a20 2020 2020 2020 2041 7267  on...        Arg
+0002ebb0: 733a 0a20 2020 2020 2020 2020 2020 2068  s:.            h
+0002ebc0: 6964 6465 6e5f 7369 7a65 2869 6e74 293a  idden_size(int):
+0002ebd0: 2054 6865 2068 6964 6465 6e20 7369 7a65   The hidden size
+0002ebe0: 206f 6620 7468 6520 696e 7075 742e 0a20   of the input.. 
+0002ebf0: 2020 2020 2020 2020 2020 2062 6174 6368             batch
+0002ec00: 5f73 697a 6528 696e 7429 3a20 5468 6520  _size(int): The 
+0002ec10: 6261 7463 6820 7369 7a65 206f 6620 7468  batch size of th
+0002ec20: 6520 696e 7075 7420 7465 6e73 6f72 2077  e input tensor w
+0002ec30: 6865 6e20 646f 2069 6e63 7265 6e6d 656e  hen do increnmen
+0002ec40: 7461 6c20 7072 6564 6963 7469 6f6e 2e20  tal prediction. 
+0002ec50: 5368 6f75 6c64 2062 6520 6120 706f 7369  Should be a posi
+0002ec60: 7469 7665 0a20 2020 2020 2020 2020 2020  tive.           
+0002ec70: 2020 2020 2076 616c 7565 2e20 5768 656e       value. When
+0002ec80: 2064 6f20 7472 6169 6e69 6e67 206f 7220   do training or 
+0002ec90: 7072 6564 6963 7469 6f6e 2c20 7468 6520  prediction, the 
+0002eca0: 6172 6775 6d65 6e74 2077 696c 6c20 6e6f  argument will no
+0002ecb0: 7420 776f 726b 2061 6e64 2074 6865 2075  t work and the u
+0002ecc0: 7365 7220 6361 6e20 6a75 7374 2070 6173  ser can just pas
+0002ecd0: 7320 4e6f 6e65 2074 6f0a 2020 2020 2020  s None to.      
+0002ece0: 2020 2020 2020 2020 2020 7468 6520 6172            the ar
+0002ecf0: 6775 6d65 6e74 2e0a 2020 2020 2020 2020  gument..        
+0002ed00: 2020 2020 6666 6e5f 6869 6464 656e 5f73      ffn_hidden_s
+0002ed10: 697a 6528 696e 7429 3a20 5468 6520 6869  ize(int): The hi
+0002ed20: 6464 656e 2073 697a 6520 6f66 2062 6f74  dden size of bot
+0002ed30: 746c 656e 6563 6b20 696e 2074 6865 2066  tleneck in the f
+0002ed40: 6565 6466 6f72 7761 7264 206c 6179 6572  eedforward layer
+0002ed50: 2e0a 2020 2020 2020 2020 2020 2020 7372  ..            sr
+0002ed60: 635f 7365 715f 6c65 6e67 7468 2869 6e74  c_seq_length(int
+0002ed70: 293a 2054 6865 2073 6571 5f6c 656e 6774  ): The seq_lengt
+0002ed80: 6820 6f66 2074 6865 2065 6e63 6f64 6572  h of the encoder
+0002ed90: 2773 2069 6e70 7574 2074 656e 736f 722e  's input tensor.
+0002eda0: 0a20 2020 2020 2020 2020 2020 2074 6774  .            tgt
+0002edb0: 5f73 6571 5f6c 656e 6774 6828 696e 7429  _seq_length(int)
+0002edc0: 3a20 5468 6520 7365 715f 6c65 6e67 7468  : The seq_length
+0002edd0: 206f 6620 7468 6520 6465 636f 6465 7227   of the decoder'
+0002ede0: 7320 696e 7075 7420 7465 6e73 6f72 2e0a  s input tensor..
+0002edf0: 2020 2020 2020 2020 2020 2020 656e 636f              enco
+0002ee00: 6465 725f 6c61 7965 7273 2869 6e74 293a  der_layers(int):
+0002ee10: 2054 6865 206c 6179 6572 7320 6f66 2074   The layers of t
+0002ee20: 6865 2060 5472 616e 7366 6f72 6d65 7245  he `TransformerE
+0002ee30: 6e63 6f64 6572 4c61 7965 7260 2e20 4465  ncoderLayer`. De
+0002ee40: 6661 756c 7420 332e 0a20 2020 2020 2020  fault 3..       
+0002ee50: 2020 2020 2064 6563 6f64 6572 5f6c 6179       decoder_lay
+0002ee60: 6572 7328 696e 7429 3a20 5468 6520 6c61  ers(int): The la
+0002ee70: 7965 7273 206f 6620 7468 6520 6054 7261  yers of the `Tra
+0002ee80: 6e73 666f 726d 6572 4465 636f 6465 724c  nsformerDecoderL
+0002ee90: 6179 6572 602e 2044 6566 6175 6c74 2033  ayer`. Default 3
+0002eea0: 2e0a 2020 2020 2020 2020 2020 2020 6e75  ..            nu
+0002eeb0: 6d5f 6865 6164 7328 696e 7429 3a20 5468  m_heads(int): Th
+0002eec0: 6520 6e75 6d62 6572 206f 6620 7468 6520  e number of the 
+0002eed0: 6865 6164 732e 2044 6566 6175 6c74 3a20  heads. Default: 
+0002eee0: 322e 0a20 2020 2020 2020 2020 2020 2061  2..            a
+0002eef0: 7474 656e 7469 6f6e 5f64 726f 706f 7574  ttention_dropout
+0002ef00: 5f72 6174 6528 666c 6f61 7429 3a20 5468  _rate(float): Th
+0002ef10: 6520 6472 6f70 6f75 7420 7261 7465 206f  e dropout rate o
+0002ef20: 6620 7468 6520 6174 7465 6e74 696f 6e20  f the attention 
+0002ef30: 7363 6f72 6573 2e20 4465 6661 756c 743a  scores. Default:
+0002ef40: 302e 312e 0a20 2020 2020 2020 2020 2020  0.1..           
+0002ef50: 2068 6964 6465 6e5f 6472 6f70 6f75 745f   hidden_dropout_
+0002ef60: 7261 7465 2866 6c6f 6174 293a 2054 6865  rate(float): The
+0002ef70: 2064 726f 706f 7574 2072 6174 6520 6f66   dropout rate of
+0002ef80: 2074 6865 2066 696e 616c 206f 7574 7075   the final outpu
+0002ef90: 7420 6f66 2074 6865 206c 6179 6572 2e20  t of the layer. 
+0002efa0: 4465 6661 756c 743a 302e 312e 0a20 2020  Default:0.1..   
+0002efb0: 2020 2020 2020 2020 2068 6964 6465 6e5f           hidden_
+0002efc0: 6163 7420 2873 7472 2c20 6e6e 2e43 656c  act (str, nn.Cel
+0002efd0: 6c29 3a20 5468 6520 6163 7469 7661 7469  l): The activati
+0002efe0: 6f6e 206f 6620 7468 6520 696e 7465 726e  on of the intern
+0002eff0: 616c 2066 6565 6466 6f72 7761 7264 206c  al feedforward l
+0002f000: 6179 6572 2e20 5375 7070 6f72 7473 2027  ayer. Supports '
+0002f010: 7265 6c75 272c 0a20 2020 2020 2020 2020  relu',.         
+0002f020: 2020 2020 2020 2027 7265 6c75 3627 2c20         'relu6', 
+0002f030: 2774 616e 6827 2c20 2767 656c 7527 2c20  'tanh', 'gelu', 
+0002f040: 2766 6173 745f 6765 6c75 272c 2027 656c  'fast_gelu', 'el
+0002f050: 7527 2c20 2773 6967 6d6f 6964 272c 2027  u', 'sigmoid', '
+0002f060: 7072 656c 7527 2c20 276c 6561 6b79 7265  prelu', 'leakyre
+0002f070: 6c75 272c 2027 6873 7769 7368 272c 0a20  lu', 'hswish',. 
+0002f080: 2020 2020 2020 2020 2020 2020 2020 2027                 '
+0002f090: 6873 6967 6d6f 6964 272c 2027 6c6f 6773  hsigmoid', 'logs
+0002f0a0: 6967 6d6f 6964 2720 616e 6420 736f 206f  igmoid' and so o
+0002f0b0: 6e2e 2055 7365 7220 6361 6e20 7072 6f76  n. User can prov
+0002f0c0: 6964 6520 6375 7374 6f6d 2061 6374 6976  ide custom activ
+0002f0d0: 6974 696f 6e20 746f 2074 6865 2061 7267  ition to the arg
+0002f0e0: 756d 656e 742e 0a20 2020 2020 2020 2020  ument..         
+0002f0f0: 2020 2020 2020 2049 6620 7573 6572 2077         If user w
+0002f100: 616e 7473 2074 6f20 7275 6e20 7468 6520  ants to run the 
+0002f110: 6e65 7420 696e 2074 6865 2070 6172 616c  net in the paral
+0002f120: 6c65 6c20 6d6f 6465 2c20 7468 6520 6375  lel mode, the cu
+0002f130: 7374 6f6d 2061 6374 6976 6174 696f 6e20  stom activation 
+0002f140: 6d75 7374 2061 6c73 6f20 7072 6f76 6964  must also provid
+0002f150: 650a 2020 2020 2020 2020 2020 2020 2020  e.              
+0002f160: 2020 7468 6520 6061 6374 6976 6174 696f    the `activatio
+0002f170: 6e5f 7368 6172 6460 2066 756e 6374 696f  n_shard` functio
+0002f180: 6e2e 2050 6c65 6173 6520 7365 6520 7468  n. Please see th
+0002f190: 6520 6578 616d 706c 6573 206f 6620 7468  e examples of th
+0002f1a0: 650a 2020 2020 2020 2020 2020 2020 2020  e.              
+0002f1b0: 2020 636c 6173 733a 606d 696e 6466 6f72    class:`mindfor
+0002f1c0: 6d65 7273 2e6d 6f64 756c 6573 2e74 7261  mers.modules.tra
+0002f1d0: 6e73 666f 726d 6572 2e46 6565 6446 6f72  nsformer.FeedFor
+0002f1e0: 7761 7264 602e 2044 6566 6175 6c74 3a20  ward`. Default: 
+0002f1f0: 6765 6c75 2e0a 2020 2020 2020 2020 2020  gelu..          
+0002f200: 2020 706f 7374 5f6c 6179 6572 6e6f 726d    post_layernorm
+0002f210: 5f72 6573 6964 7561 6c28 626f 6f6c 293a  _residual(bool):
+0002f220: 2044 6f20 7265 7369 6475 616c 7320 6164   Do residuals ad
+0002f230: 6473 2062 6566 6f72 6520 7468 6520 6c61  ds before the la
+0002f240: 7965 726e 6f72 6d2e 2044 6566 6175 6c74  yernorm. Default
+0002f250: 2046 616c 7365 2e0a 2020 2020 2020 2020   False..        
+0002f260: 2020 2020 6c61 7965 726e 6f72 6d5f 636f      layernorm_co
+0002f270: 6d70 7574 655f 7479 7065 2864 7479 7065  mpute_type(dtype
+0002f280: 2e4e 756d 6265 7229 3a20 5468 6520 636f  .Number): The co
+0002f290: 6d70 7574 6174 696f 6e20 7479 7065 206f  mputation type o
+0002f2a0: 6620 7468 6520 6c61 7965 726e 6f72 6d2e  f the layernorm.
+0002f2b0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0002f2c0: 2053 686f 756c 6420 6265 2064 7479 7065   Should be dtype
+0002f2d0: 2e66 6c6f 6174 3332 206f 7220 6474 7970  .float32 or dtyp
+0002f2e0: 652e 666c 6f61 7431 362e 2044 6566 6175  e.float16. Defau
+0002f2f0: 6c74 2064 7479 7065 2e66 6c6f 6174 3332  lt dtype.float32
+0002f300: 2e0a 2020 2020 2020 2020 2020 2020 736f  ..            so
+0002f310: 6674 6d61 785f 636f 6d70 7574 655f 7479  ftmax_compute_ty
+0002f320: 7065 2864 7479 7065 2e4e 756d 6265 7229  pe(dtype.Number)
+0002f330: 3a20 5468 6520 636f 6d70 7574 6174 696f  : The computatio
+0002f340: 6e20 7479 7065 206f 6620 7468 6520 736f  n type of the so
+0002f350: 6674 6d61 7820 696e 2074 6865 2061 7474  ftmax in the att
+0002f360: 656e 7469 6f6e 2e0a 2020 2020 2020 2020  ention..        
+0002f370: 2020 2020 2020 2020 5368 6f75 6c64 2062          Should b
+0002f380: 6520 6474 7970 652e 666c 6f61 7433 3220  e dtype.float32 
+0002f390: 6f72 2064 7479 7065 2e66 6c6f 6174 3136  or dtype.float16
+0002f3a0: 2e20 4465 6661 756c 7420 6d73 7479 7065  . Default mstype
+0002f3b0: 2e66 6c6f 6174 3332 2e0a 2020 2020 2020  .float32..      
+0002f3c0: 2020 2020 2020 7061 7261 6d5f 696e 6974        param_init
+0002f3d0: 5f74 7970 6528 6474 7970 652e 4e75 6d62  _type(dtype.Numb
+0002f3e0: 6572 293a 2054 6865 2070 6172 616d 6574  er): The paramet
+0002f3f0: 6572 2069 6e69 7469 616c 697a 6174 696f  er initializatio
+0002f400: 6e20 7479 7065 206f 6620 7468 6520 6d6f  n type of the mo
+0002f410: 6475 6c65 2e0a 2020 2020 2020 2020 2020  dule..          
+0002f420: 2020 2020 2020 5368 6f75 6c64 2062 6520        Should be 
+0002f430: 6474 7970 652e 666c 6f61 7433 3220 6f72  dtype.float32 or
+0002f440: 2064 7479 7065 2e66 6c6f 6174 3136 2e20   dtype.float16. 
+0002f450: 4465 6661 756c 7420 6474 7970 652e 666c  Default dtype.fl
+0002f460: 6f61 7433 322e 0a20 2020 2020 2020 2020  oat32..         
+0002f470: 2020 206c 616d 6264 615f 6675 6e63 3a20     lambda_func: 
+0002f480: 4120 6675 6e63 7469 6f6e 2063 616e 2064  A function can d
+0002f490: 6574 6572 6d69 6e65 2074 6865 2066 7573  etermine the fus
+0002f4a0: 696f 6e20 696e 6465 782c 2070 6970 656c  ion index, pipel
+0002f4b0: 696e 6520 7374 6167 6573 2061 6e64 2072  ine stages and r
+0002f4c0: 6563 6f6d 7075 7465 2061 7474 7269 6275  ecompute attribu
+0002f4d0: 7465 2e20 4966 2074 6865 2075 7365 720a  te. If the user.
+0002f4e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0002f4f0: 7761 6e74 7320 746f 2064 6574 6572 6d69  wants to determi
+0002f500: 6e65 2074 6865 2070 6970 656c 696e 6520  ne the pipeline 
+0002f510: 7374 6167 6520 616e 6420 6772 6164 6965  stage and gradie
+0002f520: 6e74 2061 6767 7265 6761 7469 6f6e 2066  nt aggregation f
+0002f530: 7573 696f 6e2c 2074 6865 2075 7365 7220  usion, the user 
+0002f540: 6361 6e20 7061 7373 2061 2066 756e 6374  can pass a funct
+0002f550: 696f 6e0a 2020 2020 2020 2020 2020 2020  ion.            
+0002f560: 2020 2020 7468 6174 2061 6363 6570 7473      that accepts
+0002f570: 2060 6e65 7477 6f72 6b60 2c20 606c 6179   `network`, `lay
+0002f580: 6572 5f69 6460 2c20 606f 6666 7365 7460  er_id`, `offset`
+0002f590: 2c20 6070 6172 616c 6c65 6c5f 636f 6e66  , `parallel_conf
+0002f5a0: 6967 602c 2060 6c61 7965 7273 602e 2054  ig`, `layers`. T
+0002f5b0: 6865 2060 6e65 7477 6f72 6b28 4365 6c6c  he `network(Cell
+0002f5c0: 2960 0a20 2020 2020 2020 2020 2020 2020  )`.             
+0002f5d0: 2020 2072 6570 7265 7365 6e74 7320 7468     represents th
+0002f5e0: 6520 7472 616e 7366 6f72 6d65 7220 626c  e transformer bl
+0002f5f0: 6f63 6b2c 2060 6c61 7965 725f 6964 2869  ock, `layer_id(i
+0002f600: 6e74 2960 206d 6561 6e73 2074 6865 206c  nt)` means the l
+0002f610: 6179 6572 2069 6e64 6578 2066 6f72 2074  ayer index for t
+0002f620: 6865 2063 7572 7265 6e74 206d 6f64 756c  he current modul
+0002f630: 652c 2063 6f75 6e74 730a 2020 2020 2020  e, counts.      
+0002f640: 2020 2020 2020 2020 2020 6672 6f6d 207a            from z
+0002f650: 6572 6f2c 2060 6f66 6673 6574 2869 6e74  ero, `offset(int
+0002f660: 2960 206d 6561 6e73 2074 6865 206c 6179  )` means the lay
+0002f670: 6572 5f69 6e64 6578 206e 6565 6473 2061  er_index needs a
+0002f680: 6e20 6f66 6673 6574 2c20 6966 2074 6865  n offset, if the
+0002f690: 7265 2061 7265 206f 7468 6572 206d 6f64  re are other mod
+0002f6a0: 756c 6573 2069 6e20 7468 6520 6e65 742e  ules in the net.
+0002f6b0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0002f6c0: 2054 6865 2064 6566 6175 6c74 2073 6574   The default set
+0002f6d0: 7469 6e67 2066 6f72 2074 6865 2070 6970  ting for the pip
+0002f6e0: 656c 696e 6520 6973 3a20 6028 6c61 7965  eline is: `(laye
+0002f6f0: 725f 6964 202b 206f 6666 7365 7429 202f  r_id + offset) /
+0002f700: 2f20 2828 656e 636f 6465 725f 6c61 7965  / ((encoder_laye
+0002f710: 7273 202b 2064 6563 6f64 6572 5f6c 6179  rs + decoder_lay
+0002f720: 6572 7329 0a20 2020 2020 2020 2020 2020  ers).           
+0002f730: 2020 2020 202f 2070 6970 656c 696e 655f       / pipeline_
+0002f740: 7374 6167 6529 602e 2044 6566 6175 6c74  stage)`. Default
+0002f750: 204e 6f6e 652e 0a20 2020 2020 2020 2020   None..         
+0002f760: 2020 2075 7365 5f70 6173 7428 626f 6f6c     use_past(bool
+0002f770: 293a 2055 7365 2074 6865 2070 6173 7420  ): Use the past 
+0002f780: 7374 6174 6520 746f 2063 6f6d 7075 7465  state to compute
+0002f790: 2c20 7573 6564 2066 6f72 2069 6e63 7265  , used for incre
+0002f7a0: 6d65 6e74 616c 2070 7265 6469 6374 696f  mental predictio
+0002f7b0: 6e2e 2044 6566 6175 6c74 2046 616c 7365  n. Default False
+0002f7c0: 2e0a 2020 2020 2020 2020 2020 2020 6d6f  ..            mo
+0002f7d0: 655f 636f 6e66 6967 284d 6f45 436f 6e66  e_config(MoEConf
+0002f7e0: 6967 293a 2054 6865 2063 6f6e 6669 6775  ig): The configu
+0002f7f0: 7261 7469 6f6e 206f 6620 4d6f 4520 284d  ration of MoE (M
+0002f800: 6978 7475 7265 206f 6620 4578 7065 7274  ixture of Expert
+0002f810: 292e 2044 6566 6175 6c74 2069 7320 616e  ). Default is an
+0002f820: 2069 6e73 7461 6e63 6520 6f66 204d 6f45   instance of MoE
+0002f830: 436f 6e66 6967 0a20 2020 2020 2020 2020  Config.         
+0002f840: 2020 2020 2020 2077 6974 6820 6465 6661         with defa
+0002f850: 756c 7420 7661 6c75 6573 2e20 506c 6561  ult values. Plea
+0002f860: 7365 2073 6565 2060 4d6f 4543 6f6e 6669  se see `MoEConfi
+0002f870: 6760 2e0a 2020 2020 2020 2020 2020 2020  g`..            
+0002f880: 7061 7261 6c6c 656c 5f63 6f6e 6669 6728  parallel_config(
+0002f890: 5472 616e 7366 6f72 6d65 724f 7050 6172  TransformerOpPar
+0002f8a0: 616c 6c65 6c43 6f6e 6669 6729 3a20 5468  allelConfig): Th
+0002f8b0: 6520 7061 7261 6c6c 656c 2063 6f6e 6669  e parallel confi
+0002f8c0: 6775 7265 2e20 4465 6661 756c 7420 6064  gure. Default `d
+0002f8d0: 6566 6175 6c74 5f74 7261 6e73 666f 726d  efault_transform
+0002f8e0: 6572 5f63 6f6e 6669 6760 2c0a 2020 2020  er_config`,.    
+0002f8f0: 2020 2020 2020 2020 2020 2020 616e 2069              an i
+0002f900: 6e73 7461 6e63 6520 6f66 2060 5472 616e  nstance of `Tran
+0002f910: 7366 6f72 6d65 724f 7050 6172 616c 6c65  sformerOpParalle
+0002f920: 6c43 6f6e 6669 6760 2077 6974 6820 6465  lConfig` with de
+0002f930: 6661 756c 7420 6172 6773 2e0a 0a20 2020  fault args...   
+0002f940: 2020 2020 2049 6e70 7574 733a 0a20 2020       Inputs:.   
+0002f950: 2020 2020 2020 2020 202d 202a 2a65 6e63           - **enc
+0002f960: 6f64 6572 5f69 6e70 7574 732a 2a20 2854  oder_inputs** (T
+0002f970: 656e 736f 7229 202d 2054 6865 2069 6e70  ensor) - The inp
+0002f980: 7574 2074 656e 736f 7220 7769 7468 2073  ut tensor with s
+0002f990: 6861 7065 205b 6261 7463 685f 7369 7a65  hape [batch_size
+0002f9a0: 2c20 7365 715f 6c65 6e67 7468 2c20 6869  , seq_length, hi
+0002f9b0: 6464 656e 5f73 697a 655d 206f 720a 2020  dden_size] or.  
+0002f9c0: 2020 2020 2020 2020 2020 2020 5b62 6174              [bat
+0002f9d0: 6368 5f73 697a 6520 2a20 7365 715f 6c65  ch_size * seq_le
+0002f9e0: 6e67 7468 2c20 6869 6464 656e 5f73 697a  ngth, hidden_siz
+0002f9f0: 655d 2e0a 2020 2020 2020 2020 2020 2020  e]..            
+0002fa00: 2d20 2a2a 656e 636f 6465 725f 6d61 736b  - **encoder_mask
+0002fa10: 732a 2a20 2854 656e 736f 7229 202d 2054  s** (Tensor) - T
+0002fa20: 6865 2061 7474 656e 7469 6f6e 206d 6173  he attention mas
+0002fa30: 6b20 666f 7220 6465 636f 6465 7220 7769  k for decoder wi
+0002fa40: 7468 2073 6861 7065 0a20 2020 2020 2020  th shape.       
+0002fa50: 2020 2020 2020 205b 6261 7463 685f 7369         [batch_si
+0002fa60: 7a65 2c20 7365 715f 6c65 6e67 7468 2c20  ze, seq_length, 
+0002fa70: 7365 715f 6c65 6e67 7468 5d20 6f72 204e  seq_length] or N
+0002fa80: 6f6e 652e 204e 6f6e 6520 6d65 616e 7320  one. None means 
+0002fa90: 7468 6572 6520 7769 6c6c 2062 6520 6e6f  there will be no
+0002faa0: 206d 6173 6b20 696e 2073 6f66 746d 6178   mask in softmax
+0002fab0: 2063 6f6d 7075 7461 7469 6f6e 0a20 2020   computation.   
+0002fac0: 2020 2020 2020 2020 2020 2069 6e20 7365             in se
+0002fad0: 6c66 2061 7474 656e 7469 6f6e 206f 6620  lf attention of 
+0002fae0: 7468 6520 656e 636f 6465 7220 6d6f 6475  the encoder modu
+0002faf0: 6c65 2e0a 2020 2020 2020 2020 2020 2020  le..            
+0002fb00: 2d20 2a2a 6465 636f 6465 725f 696e 7075  - **decoder_inpu
+0002fb10: 7473 2a2a 2028 5465 6e73 6f72 2920 2d20  ts** (Tensor) - 
+0002fb20: 5468 6520 6f75 7470 7574 206f 6620 7468  The output of th
+0002fb30: 6520 656e 636f 6465 7220 7769 7468 2073  e encoder with s
+0002fb40: 6861 7065 205b 6261 7463 685f 7369 7a65  hape [batch_size
+0002fb50: 2c20 7365 715f 6c65 6e67 7468 2c20 6869  , seq_length, hi
+0002fb60: 6464 656e 5f73 697a 655d 0a20 2020 2020  dden_size].     
+0002fb70: 2020 2020 2020 2020 206f 7220 5b62 6174           or [bat
+0002fb80: 6368 5f73 697a 6520 2a20 7365 715f 6c65  ch_size * seq_le
+0002fb90: 6e67 7468 2c20 6869 6464 656e 5f73 697a  ngth, hidden_siz
+0002fba0: 655d 2c20 7468 6973 2073 686f 756c 6420  e], this should 
+0002fbb0: 6265 206e 6f6e 6520 6966 2074 6865 2064  be none if the d
+0002fbc0: 6563 6f64 6572 206c 6179 6572 2069 7320  ecoder layer is 
+0002fbd0: 302e 0a20 2020 2020 2020 2020 2020 202d  0..            -
+0002fbe0: 202a 2a64 6563 6f64 6572 5f6d 6173 6b73   **decoder_masks
+0002fbf0: 2a2a 2028 5465 6e73 6f72 2920 2d20 5468  ** (Tensor) - Th
+0002fc00: 6520 6174 7465 6e74 696f 6e20 6d61 736b  e attention mask
+0002fc10: 2066 6f72 2064 6563 6f64 6572 2077 6974   for decoder wit
+0002fc20: 6820 7368 6170 650a 2020 2020 2020 2020  h shape.        
+0002fc30: 2020 2020 2020 5b62 6174 6368 5f73 697a        [batch_siz
+0002fc40: 652c 2073 6571 5f6c 656e 6774 682c 2073  e, seq_length, s
+0002fc50: 6571 5f6c 656e 6774 685d 206f 7220 4e6f  eq_length] or No
+0002fc60: 6e65 2e20 4e6f 6e65 206d 6561 6e73 2074  ne. None means t
+0002fc70: 6865 7265 2077 696c 6c20 6265 206e 6f20  here will be no 
+0002fc80: 6d61 736b 2069 6e20 736f 6674 6d61 7820  mask in softmax 
+0002fc90: 636f 6d70 7574 6174 696f 6e0a 2020 2020  computation.    
+0002fca0: 2020 2020 2020 2020 2020 696e 2073 656c            in sel
+0002fcb0: 6620 6174 7465 6e74 696f 6e20 6f66 2074  f attention of t
+0002fcc0: 6865 2064 6563 6f64 6572 206d 6f64 756c  he decoder modul
+0002fcd0: 652e 0a20 2020 2020 2020 2020 2020 202d  e..            -
+0002fce0: 202a 2a6d 656d 6f72 795f 6d61 736b 2a2a   **memory_mask**
+0002fcf0: 2028 5465 6e73 6f72 2920 2d20 5468 6520   (Tensor) - The 
+0002fd00: 6d65 6d6f 7279 206d 6173 6b20 6f66 2074  memory mask of t
+0002fd10: 6865 2063 726f 7373 2061 7474 656e 7469  he cross attenti
+0002fd20: 6f6e 2077 6974 6820 7368 6170 6520 5b62  on with shape [b
+0002fd30: 6174 6368 2c20 7467 745f 7365 715f 6c65  atch, tgt_seq_le
+0002fd40: 6e67 7468 2c0a 2020 2020 2020 2020 2020  ngth,.          
+0002fd50: 2020 2020 7372 635f 7365 715f 6c65 6e67      src_seq_leng
+0002fd60: 7468 5d0a 2020 2020 2020 2020 2020 2020  th].            
+0002fd70: 2020 7768 6572 6520 7467 745f 7365 715f    where tgt_seq_
+0002fd80: 6c65 6e67 7468 2069 7320 7468 6520 6c65  length is the le
+0002fd90: 6e67 7468 206f 6620 7468 6520 6465 636f  ngth of the deco
+0002fda0: 6465 722e 2054 6865 206f 7574 7075 7420  der. The output 
+0002fdb0: 6f66 2074 6865 2065 6e63 6f64 6572 2077  of the encoder w
+0002fdc0: 6974 6820 7368 6170 6520 5b62 6174 6368  ith shape [batch
+0002fdd0: 5f73 697a 652c 0a20 2020 2020 2020 2020  _size,.         
+0002fde0: 2020 2020 2073 6571 5f6c 656e 6774 682c       seq_length,
+0002fdf0: 2068 6964 6465 6e5f 7369 7a65 5d2c 2074   hidden_size], t
+0002fe00: 6869 7320 7368 6f75 6c64 2062 6520 6e6f  his should be no
+0002fe10: 6e65 2069 6620 7468 6520 6465 636f 6465  ne if the decode
+0002fe20: 7220 6c61 7965 7220 6973 2030 206f 7220  r layer is 0 or 
+0002fe30: 7468 6520 7573 6572 2077 616e 7473 206e  the user wants n
+0002fe40: 6f20 6d61 736b 2e0a 2020 2020 2020 2020  o mask..        
+0002fe50: 2020 2020 2d20 2a2a 696e 6974 5f72 6573      - **init_res
+0002fe60: 6574 2a2a 2028 5465 6e73 6f72 2920 2d20  et** (Tensor) - 
+0002fe70: 4120 626f 6f6c 2074 656e 736f 7220 7769  A bool tensor wi
+0002fe80: 7468 2073 6861 7065 205b 315d 2c20 7573  th shape [1], us
+0002fe90: 6564 2074 6f20 636c 6561 7220 7468 6520  ed to clear the 
+0002fea0: 7061 7374 206b 6579 2070 6172 616d 6574  past key paramet
+0002feb0: 6572 2061 6e64 0a20 2020 2020 2020 2020  er and.         
+0002fec0: 2020 2020 2070 6173 7420 7661 6c75 6520       past value 
+0002fed0: 7061 7261 6d65 7465 7220 7573 6564 2069  parameter used i
+0002fee0: 6e20 7468 6520 696e 6372 656d 656e 7461  n the incrementa
+0002fef0: 6c20 7072 6564 6963 7469 6f6e 2e20 4f6e  l prediction. On
+0002ff00: 6c79 2076 616c 6964 2077 6865 6e20 7573  ly valid when us
+0002ff10: 655f 7061 7374 2069 7320 5472 7565 2e20  e_past is True. 
+0002ff20: 4465 6661 756c 7420 5472 7565 2e0a 2020  Default True..  
+0002ff30: 2020 2020 2020 2020 2020 2d20 2a2a 6261            - **ba
+0002ff40: 7463 685f 7661 6c69 645f 6c65 6e67 7468  tch_valid_length
+0002ff50: 2a2a 2028 5465 6e73 6f72 2920 2d20 496e  ** (Tensor) - In
+0002ff60: 7433 3220 7465 6e73 6f72 2077 6974 6820  t32 tensor with 
+0002ff70: 7368 6170 6520 5b62 6174 6368 5f73 697a  shape [batch_siz
+0002ff80: 655d 2074 6865 2070 6173 7420 6361 6c63  e] the past calc
+0002ff90: 756c 6174 6564 2074 6865 2069 6e64 6578  ulated the index
+0002ffa0: 2e0a 2020 2020 2020 2020 2020 2020 2020  ..              
+0002ffb0: 5573 6564 2066 6f72 2069 6e63 7265 6d65  Used for increme
+0002ffc0: 6e74 616c 2070 7265 6469 6374 696f 6e20  ntal prediction 
+0002ffd0: 7768 656e 2074 6865 2075 7365 5f70 6173  when the use_pas
+0002ffe0: 7420 6973 2054 7275 652e 2044 6566 6175  t is True. Defau
+0002fff0: 6c74 204e 6f6e 652e 0a0a 2020 2020 2020  lt None...      
+00030000: 2020 4f75 7470 7574 733a 0a20 2020 2020    Outputs:.     
+00030010: 2020 2020 2020 2054 7570 6c65 2c20 6120         Tuple, a 
+00030020: 7475 706c 6520 636f 6e74 6169 6e73 2860  tuple contains(`
+00030030: 6f75 7470 7574 602c 2060 656e 636f 6465  output`, `encode
+00030040: 725f 6c61 7965 725f 7072 6573 656e 7460  r_layer_present`
+00030050: 2c20 6064 6563 6f64 6572 5f6c 6179 6572  , `decoder_layer
+00030060: 5f70 7265 7365 6e74 602c 2060 6163 6375  _present`, `accu
+00030070: 6d5f 6c6f 7373 6029 0a0a 2020 2020 2020  m_loss`)..      
+00030080: 2020 2020 2020 2d20 2a2a 6f75 7470 7574        - **output
+00030090: 2a2a 2028 5465 6e73 6f72 2920 2d20 4966  ** (Tensor) - If
+000300a0: 2074 6865 7265 2069 7320 6f6e 6c79 2065   there is only e
+000300b0: 6e63 6f64 6572 2c20 7468 6520 6f75 7470  ncoder, the outp
+000300c0: 7574 206c 6f67 6974 206f 6620 7468 6520  ut logit of the 
+000300d0: 656e 636f 6465 7220 6c61 7965 722e 2054  encoder layer. T
+000300e0: 6865 2073 6861 7065 2069 730a 2020 2020  he shape is.    
+000300f0: 2020 2020 2020 2020 2020 5b62 6174 6368            [batch
+00030100: 2c20 7372 635f 7365 715f 6c65 6e67 7468  , src_seq_length
+00030110: 2c20 6869 6464 656e 5f73 697a 655d 206f  , hidden_size] o
+00030120: 7220 5b62 6174 6368 202a 2073 7263 5f73  r [batch * src_s
+00030130: 6571 5f6c 656e 6774 682c 2068 6964 6465  eq_length, hidde
+00030140: 6e5f 7369 7a65 5d2c 2069 6620 7468 6572  n_size], if ther
+00030150: 6520 6172 6520 656e 636f 6465 7220 616e  e are encoder an
+00030160: 640a 2020 2020 2020 2020 2020 2020 2020  d.              
+00030170: 6465 636f 6465 7273 2c20 7468 6520 6f75  decoders, the ou
+00030180: 7470 7574 2069 7320 6672 6f6d 2074 6865  tput is from the
+00030190: 2064 6563 6f64 6572 206c 6179 6572 2e20   decoder layer. 
+000301a0: 5468 6520 7368 6170 6520 6973 205b 6261  The shape is [ba
+000301b0: 7463 682c 2074 6774 5f73 6571 5f6c 656e  tch, tgt_seq_len
+000301c0: 6774 682c 2068 6964 6465 6e5f 7369 7a65  gth, hidden_size
+000301d0: 5d20 6f72 0a20 2020 2020 2020 2020 2020  ] or.           
+000301e0: 2020 205b 6261 7463 6820 2a20 7467 745f     [batch * tgt_
+000301f0: 7365 715f 6c65 6e67 7468 2c20 6869 6464  seq_length, hidd
+00030200: 656e 5f73 697a 655d 2e0a 2020 2020 2020  en_size]..      
+00030210: 2020 2020 2020 2d20 2a2a 656e 636f 6465        - **encode
+00030220: 725f 6c61 7965 725f 7072 6573 656e 742a  r_layer_present*
+00030230: 2a20 2854 7570 6c65 2920 2d20 4120 7475  * (Tuple) - A tu
+00030240: 706c 6520 7769 7468 2073 697a 6520 6f66  ple with size of
+00030250: 206e 756d 5f6c 6179 6572 732c 2077 6865   num_layers, whe
+00030260: 7265 2065 6163 6820 7475 706c 6520 6973  re each tuple is
+00030270: 2074 6865 2074 656e 736f 7220 7468 650a   the tensor the.
+00030280: 2020 2020 2020 2020 2020 2020 2020 7072                pr
+00030290: 6f6a 6563 7465 6420 6b65 7920 616e 6420  ojected key and 
+000302a0: 7661 6c75 6520 7665 6374 6f72 2069 6e20  value vector in 
+000302b0: 7365 6c66 2061 7474 656e 7469 6f6e 2077  self attention w
+000302c0: 6974 6820 7368 6170 6520 2828 6261 7463  ith shape ((batc
+000302d0: 685f 7369 7a65 2c20 6e75 6d5f 6865 6164  h_size, num_head
+000302e0: 732c 2073 697a 655f 7065 725f 6865 6164  s, size_per_head
+000302f0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00030300: 7372 635f 7365 715f 6c65 6e67 7468 292c  src_seq_length),
+00030310: 2028 6261 7463 685f 7369 7a65 2c20 6e75   (batch_size, nu
+00030320: 6d5f 6865 6164 732c 2073 7263 5f73 6571  m_heads, src_seq
+00030330: 5f6c 656e 6774 682c 2073 697a 655f 7065  _length, size_pe
+00030340: 725f 6865 6164 2929 2e0a 2020 2020 2020  r_head))..      
+00030350: 2020 2020 2020 2d20 2a2a 6465 636f 6465        - **decode
+00030360: 725f 6c61 7965 725f 7072 6573 656e 742a  r_layer_present*
+00030370: 2a20 2854 7570 6c65 2920 2d20 4120 7475  * (Tuple) - A tu
+00030380: 706c 6520 7769 7468 2073 697a 6520 6f66  ple with size of
+00030390: 206e 756d 5f6c 6179 6572 732c 2077 6865   num_layers, whe
+000303a0: 7265 2065 6163 6820 7475 706c 6520 6973  re each tuple is
+000303b0: 2074 6865 2074 656e 736f 720a 2020 2020   the tensor.    
+000303c0: 2020 2020 2020 2020 2020 6f66 2074 6865            of the
+000303d0: 2070 726f 6a65 6374 6564 206b 6579 2061   projected key a
+000303e0: 6e64 2076 616c 7565 2076 6563 746f 7220  nd value vector 
+000303f0: 696e 2073 656c 6620 6174 7465 6e74 696f  in self attentio
+00030400: 6e20 7769 7468 2073 6861 7065 2028 2862  n with shape ((b
+00030410: 6174 6368 5f73 697a 652c 206e 756d 5f68  atch_size, num_h
+00030420: 6561 6473 2c20 7369 7a65 5f70 6572 5f68  eads, size_per_h
+00030430: 6561 642c 0a20 2020 2020 2020 2020 2020  ead,.           
+00030440: 2020 2074 6774 5f73 6571 5f6c 656e 6774     tgt_seq_lengt
+00030450: 6829 2c20 2862 6174 6368 5f73 697a 652c  h), (batch_size,
+00030460: 206e 756d 5f68 6561 6473 2c20 7467 745f   num_heads, tgt_
+00030470: 7365 715f 6c65 6e67 7468 2c20 7369 7a65  seq_length, size
+00030480: 5f70 6572 5f68 6561 6429 292c 2061 6e64  _per_head)), and
+00030490: 2074 6865 0a20 2020 2020 2020 2020 2020   the.           
+000304a0: 2020 2070 726f 6a65 6374 6564 206b 6579     projected key
+000304b0: 2061 6e64 2076 616c 7565 2076 6563 746f   and value vecto
+000304c0: 7220 696e 2063 726f 7373 2061 7474 656e  r in cross atten
+000304d0: 7469 6f6e 2077 6974 6820 7368 6170 650a  tion with shape.
+000304e0: 2020 2020 2020 2020 2020 2020 2020 2828                ((
+000304f0: 6261 7463 685f 7369 7a65 2c20 6e75 6d5f  batch_size, num_
+00030500: 6865 6164 732c 2073 697a 655f 7065 725f  heads, size_per_
+00030510: 6865 6164 2c20 7372 635f 7365 715f 6c65  head, src_seq_le
+00030520: 6e67 7468 292c 0a20 2020 2020 2020 2020  ngth),.         
+00030530: 2020 2020 2028 6261 7463 685f 7369 7a65       (batch_size
+00030540: 2c20 6e75 6d5f 6865 6164 732c 2073 7263  , num_heads, src
+00030550: 5f73 6571 5f6c 656e 6774 682c 2073 697a  _seq_length, siz
+00030560: 655f 7065 725f 6865 6164 2929 2e20 4966  e_per_head)). If
+00030570: 2074 6865 2064 6563 6f64 6572 2069 7320   the decoder is 
+00030580: 6e6f 7420 7365 742c 2074 6865 0a20 2020  not set, the.   
+00030590: 2020 2020 2020 2020 2020 2072 6574 7572             retur
+000305a0: 6e65 6420 7661 6c75 6520 7769 6c6c 2062  ned value will b
+000305b0: 6520 4e6f 6e65 2e0a 2020 2020 2020 2020  e None..        
+000305c0: 2020 2020 2d20 2a2a 6163 6375 6d5f 6c6f      - **accum_lo
+000305d0: 7373 2a2a 2028 5465 6e73 6f72 2920 2d20  ss** (Tensor) - 
+000305e0: 4120 5465 6e73 6f72 2069 6e64 6963 6174  A Tensor indicat
+000305f0: 6573 2061 6e20 6175 7869 6c69 6172 7920  es an auxiliary 
+00030600: 6c6f 7373 2074 6f20 6d69 6e69 6d69 7a65  loss to minimize
+00030610: 2074 6865 206d 6561 6e20 7371 7561 7265   the mean square
+00030620: 206f 6620 7468 6520 6461 7461 0a20 2020   of the data.   
+00030630: 2020 2020 2020 2020 2020 2070 6172 7420             part 
+00030640: 726f 7574 6564 2074 6f20 6561 6368 2065  routed to each e
+00030650: 7870 6572 742c 2061 6e64 206f 6e6c 7920  xpert, and only 
+00030660: 7265 7475 726e 6564 2069 6620 7468 6520  returned if the 
+00030670: 6e75 6d62 6572 206f 6620 6578 7065 7274  number of expert
+00030680: 7320 6973 2067 7265 6174 6572 2074 6861  s is greater tha
+00030690: 6e20 312e 0a0a 2020 2020 2020 2020 5375  n 1...        Su
+000306a0: 7070 6f72 7465 6420 506c 6174 666f 726d  pported Platform
+000306b0: 733a 0a20 2020 2020 2020 2020 2020 2060  s:.            `
+000306c0: 6041 7363 656e 6460 6020 6060 4750 5560  `Ascend`` ``GPU`
+000306d0: 600a 0a20 2020 2020 2020 2045 7861 6d70  `..        Examp
+000306e0: 6c65 733a 0a20 2020 2020 2020 2020 2020  les:.           
+000306f0: 203e 3e3e 2069 6d70 6f72 7420 6e75 6d70   >>> import nump
+00030700: 7920 6173 206e 700a 2020 2020 2020 2020  y as np.        
+00030710: 2020 2020 3e3e 3e20 6672 6f6d 206d 696e      >>> from min
+00030720: 6473 706f 7265 2069 6d70 6f72 7420 6474  dspore import dt
+00030730: 7970 6520 6173 206d 7374 7970 650a 2020  ype as mstype.  
+00030740: 2020 2020 2020 2020 2020 3e3e 3e20 6672            >>> fr
+00030750: 6f6d 206d 696e 6466 6f72 6d65 7273 2e6d  om mindformers.m
+00030760: 6f64 756c 6573 2e74 7261 6e73 666f 726d  odules.transform
+00030770: 6572 2069 6d70 6f72 7420 5472 616e 7366  er import Transf
+00030780: 6f72 6d65 720a 2020 2020 2020 2020 2020  ormer.          
+00030790: 2020 3e3e 3e20 6672 6f6d 206d 696e 6473    >>> from minds
+000307a0: 706f 7265 2069 6d70 6f72 7420 5465 6e73  pore import Tens
+000307b0: 6f72 0a20 2020 2020 2020 2020 2020 203e  or.            >
+000307c0: 3e3e 206d 6f64 656c 203d 2054 7261 6e73  >> model = Trans
+000307d0: 666f 726d 6572 2862 6174 6368 5f73 697a  former(batch_siz
+000307e0: 653d 322c 2065 6e63 6f64 6572 5f6c 6179  e=2, encoder_lay
+000307f0: 6572 733d 312c 2064 6563 6f64 6572 5f6c  ers=1, decoder_l
+00030800: 6179 6572 733d 322c 2068 6964 6465 6e5f  ayers=2, hidden_
+00030810: 7369 7a65 3d36 342c 0a20 2020 2020 2020  size=64,.       
+00030820: 2020 2020 202e 2e2e 2020 2020 2020 2020       ...        
+00030830: 2020 2020 2020 2020 2020 2020 2066 666e               ffn
+00030840: 5f68 6964 6465 6e5f 7369 7a65 3d36 342c  _hidden_size=64,
+00030850: 2073 7263 5f73 6571 5f6c 656e 6774 683d   src_seq_length=
+00030860: 3230 2c20 7467 745f 7365 715f 6c65 6e67  20, tgt_seq_leng
+00030870: 7468 3d31 3029 0a20 2020 2020 2020 2020  th=10).         
+00030880: 2020 203e 3e3e 2065 6e63 6f64 6572 5f69     >>> encoder_i
+00030890: 6e70 7574 5f76 616c 7565 203d 2054 656e  nput_value = Ten
+000308a0: 736f 7228 6e70 2e6f 6e65 7328 2832 2c20  sor(np.ones((2, 
+000308b0: 3230 2c20 3634 2929 2c20 6d73 7479 7065  20, 64)), mstype
+000308c0: 2e66 6c6f 6174 3332 290a 2020 2020 2020  .float32).      
+000308d0: 2020 2020 2020 3e3e 3e20 656e 636f 6465        >>> encode
+000308e0: 725f 696e 7075 745f 6d61 736b 203d 2054  r_input_mask = T
+000308f0: 656e 736f 7228 6e70 2e6f 6e65 7328 2832  ensor(np.ones((2
+00030900: 2c20 3230 2c20 3230 2929 2c20 6d73 7479  , 20, 20)), msty
+00030910: 7065 2e66 6c6f 6174 3136 290a 2020 2020  pe.float16).    
+00030920: 2020 2020 2020 2020 3e3e 3e20 6465 636f          >>> deco
+00030930: 6465 725f 696e 7075 745f 7661 6c75 6520  der_input_value 
+00030940: 3d20 5465 6e73 6f72 286e 702e 6f6e 6573  = Tensor(np.ones
+00030950: 2828 322c 2031 302c 2036 3429 292c 206d  ((2, 10, 64)), m
+00030960: 7374 7970 652e 666c 6f61 7433 3229 0a20  stype.float32). 
+00030970: 2020 2020 2020 2020 2020 203e 3e3e 2064             >>> d
+00030980: 6563 6f64 6572 5f69 6e70 7574 5f6d 6173  ecoder_input_mas
+00030990: 6b20 3d20 5465 6e73 6f72 286e 702e 6f6e  k = Tensor(np.on
+000309a0: 6573 2828 322c 2031 302c 2031 3029 292c  es((2, 10, 10)),
+000309b0: 206d 7374 7970 652e 666c 6f61 7431 3629   mstype.float16)
+000309c0: 0a20 2020 2020 2020 2020 2020 203e 3e3e  .            >>>
+000309d0: 206d 656d 6f72 795f 6d61 736b 203d 2054   memory_mask = T
+000309e0: 656e 736f 7228 6e70 2e6f 6e65 7328 2832  ensor(np.ones((2
+000309f0: 2c20 3130 2c20 3230 2929 2c20 6d73 7479  , 10, 20)), msty
+00030a00: 7065 2e66 6c6f 6174 3136 290a 2020 2020  pe.float16).    
+00030a10: 2020 2020 2020 2020 3e3e 3e20 6f75 7470          >>> outp
+00030a20: 7574 2c20 656e 5f70 6173 742c 2064 655f  ut, en_past, de_
+00030a30: 7061 7374 203d 206d 6f64 656c 2865 6e63  past = model(enc
+00030a40: 6f64 6572 5f69 6e70 7574 5f76 616c 7565  oder_input_value
+00030a50: 2c20 656e 636f 6465 725f 696e 7075 745f  , encoder_input_
+00030a60: 6d61 736b 2c20 6465 636f 6465 725f 696e  mask, decoder_in
+00030a70: 7075 745f 7661 6c75 652c 0a20 2020 2020  put_value,.     
+00030a80: 2020 2020 2020 202e 2e2e 2020 2020 2020         ...      
+00030a90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030aa0: 2020 2020 2020 2020 2020 2020 6465 636f              deco
+00030ab0: 6465 725f 696e 7075 745f 6d61 736b 2c20  der_input_mask, 
+00030ac0: 6d65 6d6f 7279 5f6d 6173 6b29 0a20 2020  memory_mask).   
+00030ad0: 2020 2020 2020 2020 203e 3e3e 2070 7269           >>> pri
+00030ae0: 6e74 286f 7574 7075 742e 7368 6170 6529  nt(output.shape)
+00030af0: 0a20 2020 2020 2020 2020 2020 2028 322c  .            (2,
+00030b00: 2031 302c 2036 3429 0a20 2020 2020 2020   10, 64).       
+00030b10: 2020 2020 203e 3e3e 2070 7269 6e74 286c       >>> print(l
+00030b20: 656e 2865 6e5f 7061 7374 2929 0a20 2020  en(en_past)).   
+00030b30: 2020 2020 2020 2020 2031 0a20 2020 2020           1.     
+00030b40: 2020 2020 2020 203e 3e3e 2070 7269 6e74         >>> print
+00030b50: 286c 656e 2864 655f 7061 7374 2929 0a20  (len(de_past)). 
+00030b60: 2020 2020 2020 2020 2020 2032 0a20 2020             2.   
+00030b70: 2020 2020 2020 2020 203e 3e3e 2070 7269           >>> pri
+00030b80: 6e74 2865 6e5f 7061 7374 5b30 5d5b 305d  nt(en_past[0][0]
+00030b90: 2e73 6861 7065 290a 2020 2020 2020 2020  .shape).        
+00030ba0: 2020 2020 2832 2c20 322c 2033 322c 2032      (2, 2, 32, 2
+00030bb0: 3029 0a20 2020 2020 2020 2020 2020 203e  0).            >
+00030bc0: 3e3e 2070 7269 6e74 2865 6e5f 7061 7374  >> print(en_past
+00030bd0: 5b30 5d5b 315d 2e73 6861 7065 290a 2020  [0][1].shape).  
+00030be0: 2020 2020 2020 2020 2020 2832 2c20 322c            (2, 2,
+00030bf0: 2032 302c 2033 3229 0a20 2020 2020 2020   20, 32).       
+00030c00: 2020 2020 203e 3e3e 2070 7269 6e74 2864       >>> print(d
+00030c10: 655f 7061 7374 5b30 5d5b 305d 2e73 6861  e_past[0][0].sha
+00030c20: 7065 290a 2020 2020 2020 2020 2020 2020  pe).            
+00030c30: 2832 2c20 322c 2033 322c 2031 3029 0a20  (2, 2, 32, 10). 
+00030c40: 2020 2020 2020 2020 2020 203e 3e3e 2070             >>> p
+00030c50: 7269 6e74 2864 655f 7061 7374 5b30 5d5b  rint(de_past[0][
+00030c60: 315d 2e73 6861 7065 290a 2020 2020 2020  1].shape).      
+00030c70: 2020 2020 2020 2832 2c20 322c 2031 302c        (2, 2, 10,
+00030c80: 2033 3229 0a20 2020 2020 2020 2020 2020   32).           
+00030c90: 203e 3e3e 2070 7269 6e74 2864 655f 7061   >>> print(de_pa
+00030ca0: 7374 5b30 5d5b 325d 2e73 6861 7065 290a  st[0][2].shape).
+00030cb0: 2020 2020 2020 2020 2020 2020 2832 2c20              (2, 
+00030cc0: 322c 2033 322c 2032 3029 0a20 2020 2020  2, 32, 20).     
+00030cd0: 2020 2020 2020 203e 3e3e 2070 7269 6e74         >>> print
+00030ce0: 2864 655f 7061 7374 5b30 5d5b 335d 2e73  (de_past[0][3].s
+00030cf0: 6861 7065 290a 2020 2020 2020 2020 2020  hape).          
+00030d00: 2020 2832 2c20 322c 2032 302c 2033 3229    (2, 2, 20, 32)
+00030d10: 0a20 2020 2022 2222 0a0a 2020 2020 405f  .    """..    @_
+00030d20: 4c6f 6741 6374 696f 6e4f 6e63 6528 6d5f  LogActionOnce(m_
+00030d30: 6c6f 6767 6572 3d6c 6f67 6765 722c 206b  logger=logger, k
+00030d40: 6579 3d27 5472 616e 7366 6f72 6d65 7227  ey='Transformer'
+00030d50: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00030d60: 2020 2020 2020 6e6f 5f77 6172 6e69 6e67        no_warning
+00030d70: 3d5f 6765 745f 7061 7261 6c6c 656c 5f6d  =_get_parallel_m
+00030d80: 6f64 6528 2920 696e 2028 5061 7261 6c6c  ode() in (Parall
+00030d90: 656c 4d6f 6465 2e53 5441 4e44 5f41 4c4f  elMode.STAND_ALO
+00030da0: 4e45 2c29 290a 2020 2020 405f 6172 6773  NE,)).    @_args
+00030db0: 5f74 7970 655f 7661 6c69 6461 746f 725f  _type_validator_
+00030dc0: 6368 6563 6b28 6869 6464 656e 5f73 697a  check(hidden_siz
+00030dd0: 653d 5661 6c69 6461 746f 722e 6368 6563  e=Validator.chec
+00030de0: 6b5f 706f 7369 7469 7665 5f69 6e74 2c0a  k_positive_int,.
+00030df0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030e00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030e10: 6e75 6d5f 6865 6164 733d 5661 6c69 6461  num_heads=Valida
+00030e20: 746f 722e 6368 6563 6b5f 706f 7369 7469  tor.check_positi
+00030e30: 7665 5f69 6e74 2c0a 2020 2020 2020 2020  ve_int,.        
+00030e40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030e50: 2020 2020 2020 2020 6666 6e5f 6869 6464          ffn_hidd
+00030e60: 656e 5f73 697a 653d 5661 6c69 6461 746f  en_size=Validato
+00030e70: 722e 6368 6563 6b5f 706f 7369 7469 7665  r.check_positive
+00030e80: 5f69 6e74 2c0a 2020 2020 2020 2020 2020  _int,.          
+00030e90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030ea0: 2020 2020 2020 7372 635f 7365 715f 6c65        src_seq_le
+00030eb0: 6e67 7468 3d56 616c 6964 6174 6f72 2e63  ngth=Validator.c
+00030ec0: 6865 636b 5f70 6f73 6974 6976 655f 696e  heck_positive_in
+00030ed0: 742c 0a20 2020 2020 2020 2020 2020 2020  t,.             
 00030ee0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030ef0: 2020 2020 2020 6d73 7479 7065 2e66 6c6f        mstype.flo
-00030f00: 6174 3136 2c20 6d73 7479 7065 2e62 666c  at16, mstype.bfl
-00030f10: 6f61 7431 365d 2c20 2254 7261 6e73 666f  oat16], "Transfo
-00030f20: 726d 6572 2229 2c0a 2020 2020 2020 2020  rmer"),.        
+00030ef0: 2020 2065 6e63 6f64 6572 5f6c 6179 6572     encoder_layer
+00030f00: 733d 5661 6c69 6461 746f 722e 6368 6563  s=Validator.chec
+00030f10: 6b5f 706f 7369 7469 7665 5f69 6e74 2c0a  k_positive_int,.
+00030f20: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00030f30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030f40: 2020 2020 2020 2020 7061 7261 6c6c 656c          parallel
-00030f50: 5f63 6f6e 6669 673d 5f76 616c 6964 5f74  _config=_valid_t
-00030f60: 7970 655f 6368 6563 6b73 285b 5472 616e  ype_checks([Tran
-00030f70: 7366 6f72 6d65 724f 7050 6172 616c 6c65  sformerOpParalle
-00030f80: 6c43 6f6e 6669 675d 2c20 2254 7261 6e73  lConfig], "Trans
-00030f90: 666f 726d 6572 2229 2c0a 2020 2020 2020  former"),.      
-00030fa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00030fb0: 2020 2020 2020 2020 2020 7573 655f 7061            use_pa
-00030fc0: 7374 3d56 616c 6964 6174 6f72 2e63 6865  st=Validator.che
-00030fd0: 636b 5f62 6f6f 6c29 0a20 2020 2064 6566  ck_bool).    def
-00030fe0: 205f 5f69 6e69 745f 5f28 7365 6c66 2c0a   __init__(self,.
-00030ff0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00031000: 2068 6964 6465 6e5f 7369 7a65 2c0a 2020   hidden_size,.  
-00031010: 2020 2020 2020 2020 2020 2020 2020 2062                 b
-00031020: 6174 6368 5f73 697a 652c 0a20 2020 2020  atch_size,.     
-00031030: 2020 2020 2020 2020 2020 2020 6666 6e5f              ffn_
-00031040: 6869 6464 656e 5f73 697a 652c 0a20 2020  hidden_size,.   
-00031050: 2020 2020 2020 2020 2020 2020 2020 7372                sr
-00031060: 635f 7365 715f 6c65 6e67 7468 2c0a 2020  c_seq_length,.  
-00031070: 2020 2020 2020 2020 2020 2020 2020 2074                 t
-00031080: 6774 5f73 6571 5f6c 656e 6774 682c 0a20  gt_seq_length,. 
-00031090: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000310a0: 656e 636f 6465 725f 6c61 7965 7273 3d33  encoder_layers=3
-000310b0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-000310c0: 2020 2064 6563 6f64 6572 5f6c 6179 6572     decoder_layer
-000310d0: 733d 332c 0a20 2020 2020 2020 2020 2020  s=3,.           
-000310e0: 2020 2020 2020 6e75 6d5f 6865 6164 733d        num_heads=
-000310f0: 322c 0a20 2020 2020 2020 2020 2020 2020  2,.             
-00031100: 2020 2020 6174 7465 6e74 696f 6e5f 6472      attention_dr
-00031110: 6f70 6f75 745f 7261 7465 3d30 2e31 2c0a  opout_rate=0.1,.
+00030f40: 6465 636f 6465 725f 6c61 7965 7273 3d56  decoder_layers=V
+00030f50: 616c 6964 6174 6f72 2e63 6865 636b 5f6e  alidator.check_n
+00030f60: 6f6e 5f6e 6567 6174 6976 655f 696e 742c  on_negative_int,
+00030f70: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00030f80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030f90: 2074 6774 5f73 6571 5f6c 656e 6774 683d   tgt_seq_length=
+00030fa0: 5661 6c69 6461 746f 722e 6368 6563 6b5f  Validator.check_
+00030fb0: 706f 7369 7469 7665 5f69 6e74 2c0a 2020  positive_int,.  
+00030fc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00030fd0: 2020 2020 2020 2020 2020 2020 2020 6174                at
+00030fe0: 7465 6e74 696f 6e5f 6472 6f70 6f75 745f  tention_dropout_
+00030ff0: 7261 7465 3d56 616c 6964 6174 6f72 2e63  rate=Validator.c
+00031000: 6865 636b 5f6e 6f6e 5f6e 6567 6174 6976  heck_non_negativ
+00031010: 655f 666c 6f61 742c 0a20 2020 2020 2020  e_float,.       
+00031020: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00031030: 2020 2020 2020 2020 2068 6964 6465 6e5f           hidden_
+00031040: 6472 6f70 6f75 745f 7261 7465 3d56 616c  dropout_rate=Val
+00031050: 6964 6174 6f72 2e63 6865 636b 5f6e 6f6e  idator.check_non
+00031060: 5f6e 6567 6174 6976 655f 666c 6f61 742c  _negative_float,
+00031070: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00031080: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00031090: 2070 6f73 745f 6c61 7965 726e 6f72 6d5f   post_layernorm_
+000310a0: 7265 7369 6475 616c 3d56 616c 6964 6174  residual=Validat
+000310b0: 6f72 2e63 6865 636b 5f62 6f6f 6c2c 0a20  or.check_bool,. 
+000310c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000310d0: 2020 2020 2020 2020 2020 2020 2020 206c                 l
+000310e0: 6179 6572 6e6f 726d 5f63 6f6d 7075 7465  ayernorm_compute
+000310f0: 5f74 7970 653d 5f76 616c 6964 5f76 616c  _type=_valid_val
+00031100: 7565 5f63 6865 636b 7328 5b6d 7374 7970  ue_checks([mstyp
+00031110: 652e 666c 6f61 7433 322c 0a20 2020 2020  e.float32,.     
 00031120: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00031130: 2068 6964 6465 6e5f 6472 6f70 6f75 745f   hidden_dropout_
-00031140: 7261 7465 3d30 2e31 2c0a 2020 2020 2020  rate=0.1,.      
-00031150: 2020 2020 2020 2020 2020 2068 6964 6465             hidde
-00031160: 6e5f 6163 743d 2767 656c 7527 2c0a 2020  n_act='gelu',.  
-00031170: 2020 2020 2020 2020 2020 2020 2020 2070                 p
-00031180: 6f73 745f 6c61 7965 726e 6f72 6d5f 7265  ost_layernorm_re
-00031190: 7369 6475 616c 3d46 616c 7365 2c0a 2020  sidual=False,.  
-000311a0: 2020 2020 2020 2020 2020 2020 2020 206c                 l
-000311b0: 6179 6572 6e6f 726d 5f63 6f6d 7075 7465  ayernorm_compute
-000311c0: 5f74 7970 653d 6d73 7479 7065 2e66 6c6f  _type=mstype.flo
-000311d0: 6174 3332 2c0a 2020 2020 2020 2020 2020  at32,.          
-000311e0: 2020 2020 2020 2073 6f66 746d 6178 5f63         softmax_c
-000311f0: 6f6d 7075 7465 5f74 7970 653d 6d73 7479  ompute_type=msty
-00031200: 7065 2e66 6c6f 6174 3332 2c0a 2020 2020  pe.float32,.    
-00031210: 2020 2020 2020 2020 2020 2020 2070 6172               par
-00031220: 616d 5f69 6e69 745f 7479 7065 3d6d 7374  am_init_type=mst
-00031230: 7970 652e 666c 6f61 7433 322c 0a20 2020  ype.float32,.   
-00031240: 2020 2020 2020 2020 2020 2020 2020 6c61                la
-00031250: 6d62 6461 5f66 756e 633d 4e6f 6e65 2c0a  mbda_func=None,.
+00031130: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00031140: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00031150: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00031160: 2020 2020 2020 206d 7374 7970 652e 666c         mstype.fl
+00031170: 6f61 7431 362c 206d 7374 7970 652e 6266  oat16, mstype.bf
+00031180: 6c6f 6174 3136 5d2c 0a20 2020 2020 2020  loat16],.       
+00031190: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000311a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000311b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000311c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000311d0: 2020 2020 2254 7261 6e73 666f 726d 6572      "Transformer
+000311e0: 2229 2c0a 2020 2020 2020 2020 2020 2020  "),.            
+000311f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00031200: 2020 2020 736f 6674 6d61 785f 636f 6d70      softmax_comp
+00031210: 7574 655f 7479 7065 3d5f 7661 6c69 645f  ute_type=_valid_
+00031220: 7661 6c75 655f 6368 6563 6b73 285b 6d73  value_checks([ms
+00031230: 7479 7065 2e66 6c6f 6174 3332 2c0a 2020  type.float32,.  
+00031240: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00031250: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00031260: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00031270: 2075 7365 5f70 6173 743d 4661 6c73 652c   use_past=False,
-00031280: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00031290: 2020 6d6f 655f 636f 6e66 6967 3d64 6566    moe_config=def
-000312a0: 6175 6c74 5f6d 6f65 5f63 6f6e 6669 672c  ault_moe_config,
-000312b0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-000312c0: 2020 7061 7261 6c6c 656c 5f63 6f6e 6669    parallel_confi
-000312d0: 673d 6465 6661 756c 745f 7472 616e 7366  g=default_transf
-000312e0: 6f72 6d65 725f 636f 6e66 6967 293a 0a20  ormer_config):. 
-000312f0: 2020 2020 2020 2073 7570 6572 2854 7261         super(Tra
-00031300: 6e73 666f 726d 6572 2c20 7365 6c66 292e  nsformer, self).
-00031310: 5f5f 696e 6974 5f5f 2829 0a20 2020 2020  __init__().     
-00031320: 2020 2069 6620 6261 7463 685f 7369 7a65     if batch_size
-00031330: 206f 7220 7573 655f 7061 7374 3a0a 2020   or use_past:.  
-00031340: 2020 2020 2020 2020 2020 5661 6c69 6461            Valida
-00031350: 746f 722e 6368 6563 6b5f 706f 7369 7469  tor.check_positi
-00031360: 7665 5f69 6e74 2862 6174 6368 5f73 697a  ve_int(batch_siz
-00031370: 6529 0a20 2020 2020 2020 2069 6620 5f67  e).        if _g
-00031380: 6574 5f70 6172 616c 6c65 6c5f 6d6f 6465  et_parallel_mode
-00031390: 2829 2069 6e20 2850 6172 616c 6c65 6c4d  () in (ParallelM
-000313a0: 6f64 652e 4155 544f 5f50 4152 414c 4c45  ode.AUTO_PARALLE
-000313b0: 4c2c 293a 0a20 2020 2020 2020 2020 2020  L,):.           
-000313c0: 205f 6368 6563 6b5f 636f 6e66 6967 2870   _check_config(p
-000313d0: 6172 616c 6c65 6c5f 636f 6e66 6967 290a  arallel_config).
-000313e0: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-000313f0: 2e62 6174 6368 5f73 697a 6520 3d20 6261  .batch_size = ba
-00031400: 7463 685f 7369 7a65 0a20 2020 2020 2020  tch_size.       
-00031410: 2020 2020 2073 656c 662e 6869 6464 656e       self.hidden
-00031420: 5f73 697a 6520 3d20 6869 6464 656e 5f73  _size = hidden_s
-00031430: 697a 650a 2020 2020 2020 2020 2020 2020  ize.            
-00031440: 7365 6c66 2e73 7263 5f73 6571 5f6c 656e  self.src_seq_len
-00031450: 6774 6820 3d20 7372 635f 7365 715f 6c65  gth = src_seq_le
-00031460: 6e67 7468 0a20 2020 2020 2020 2020 2020  ngth.           
-00031470: 2073 656c 662e 7467 745f 7365 715f 6c65   self.tgt_seq_le
-00031480: 6e67 7468 203d 2074 6774 5f73 6571 5f6c  ngth = tgt_seq_l
-00031490: 656e 6774 680a 2020 2020 2020 2020 2020  ength.          
-000314a0: 2020 7365 6c66 2e75 7365 5f70 6173 7420    self.use_past 
-000314b0: 3d20 7573 655f 7061 7374 0a20 2020 2020  = use_past.     
-000314c0: 2020 2020 2020 2069 6620 656e 636f 6465         if encode
-000314d0: 725f 6c61 7965 7273 203c 3d20 3020 3c20  r_layers <= 0 < 
-000314e0: 6465 636f 6465 725f 6c61 7965 7273 3a0a  decoder_layers:.
-000314f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00031500: 7261 6973 6520 5661 6c75 6545 7272 6f72  raise ValueError
-00031510: 2866 2254 7261 6e73 666f 726d 6572 2064  (f"Transformer d
-00031520: 6f65 7374 2073 7570 706f 7274 2065 6e63  oest support enc
-00031530: 6f64 6572 206c 6179 6572 207b 656e 636f  oder layer {enco
-00031540: 6465 725f 6c61 7965 7273 7d20 616e 6420  der_layers} and 
-00031550: 6465 636f 6465 7222 0a20 2020 2020 2020  decoder".       
-00031560: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00031570: 2020 2020 2020 2020 2020 6622 6c61 7965            f"laye
-00031580: 7220 7b64 6563 6f64 6572 5f6c 6179 6572  r {decoder_layer
-00031590: 737d 2c20 706c 6561 7365 2075 7365 2054  s}, please use T
-000315a0: 7261 6e73 666f 726d 6572 4465 636f 6465  ransformerDecode
-000315b0: 7222 290a 2020 2020 2020 2020 2020 2020  r").            
-000315c0: 6966 2065 6e63 6f64 6572 5f6c 6179 6572  if encoder_layer
-000315d0: 7320 3e20 3020 616e 6420 6465 636f 6465  s > 0 and decode
-000315e0: 725f 6c61 7965 7273 203e 2030 2061 6e64  r_layers > 0 and
-000315f0: 2075 7365 5f70 6173 743a 0a20 2020 2020   use_past:.     
-00031600: 2020 2020 2020 2020 2020 2072 6169 7365             raise
-00031610: 2056 616c 7565 4572 726f 7228 6622 5468   ValueError(f"Th
-00031620: 6520 7b73 656c 662e 636c 735f 6e61 6d65  e {self.cls_name
-00031630: 7d20 7769 7468 2065 6e63 6f64 6572 2061  } with encoder a
-00031640: 6e64 2064 6563 6f64 6572 2064 6f65 7320  nd decoder does 
-00031650: 6e6f 7420 7375 7070 6f72 7420 7573 655f  not support use_
-00031660: 7061 7374 3d54 7275 652e 2229 0a20 2020  past=True.").   
-00031670: 2020 2020 2020 2020 2023 2054 6865 2073           # The s
-00031680: 6861 7264 2073 6574 7469 6e67 206f 6620  hard setting of 
-00031690: 5472 616e 7366 6f72 6d65 7220 6973 2073  Transformer is s
-000316a0: 6574 2077 6974 6869 6e20 7468 6520 5472  et within the Tr
-000316b0: 616e 7366 6f72 6d65 7245 6e63 6f64 6572  ansformerEncoder
-000316c0: 4c61 7965 720a 2020 2020 2020 2020 2020  Layer.          
-000316d0: 2020 6966 206e 6f74 206c 616d 6264 615f    if not lambda_
-000316e0: 6675 6e63 3a0a 2020 2020 2020 2020 2020  func:.          
-000316f0: 2020 2020 2020 6c61 6d62 6461 5f66 756e        lambda_fun
-00031700: 6320 3d20 5f67 6574 5f6c 616d 6264 615f  c = _get_lambda_
-00031710: 6675 6e63 2874 6f74 616c 5f6c 6179 6572  func(total_layer
-00031720: 3d65 6e63 6f64 6572 5f6c 6179 6572 7320  =encoder_layers 
-00031730: 2b20 6465 636f 6465 725f 6c61 7965 7273  + decoder_layers
-00031740: 290a 2020 2020 2020 2020 2020 2020 5f63  ).            _c
-00031750: 6865 636b 5f6d 6f65 5f63 6f6e 6669 6728  heck_moe_config(
-00031760: 6d6f 655f 636f 6e66 6967 2c20 7061 7261  moe_config, para
-00031770: 6c6c 656c 5f63 6f6e 6669 6729 0a20 2020  llel_config).   
-00031780: 2020 2020 2020 2020 2073 656c 662e 7573           self.us
-00031790: 655f 6d6f 6520 3d20 286d 6f65 5f63 6f6e  e_moe = (moe_con
-000317a0: 6669 672e 6578 7065 7274 5f6e 756d 203e  fig.expert_num >
-000317b0: 2031 290a 2020 2020 2020 2020 2020 2020   1).            
-000317c0: 7365 6c66 2e61 6464 203d 2050 2e41 6464  self.add = P.Add
-000317d0: 2829 0a20 2020 2020 2020 2020 2020 2073  ().            s
-000317e0: 656c 662e 6175 785f 6c6f 7373 203d 2054  elf.aux_loss = T
-000317f0: 656e 736f 7228 302e 302c 206d 7374 7970  ensor(0.0, mstyp
-00031800: 652e 666c 6f61 7433 3229 0a20 2020 2020  e.float32).     
-00031810: 2020 2020 2020 2069 6620 656e 636f 6465         if encode
-00031820: 725f 6c61 7965 7273 203e 2030 3a0a 2020  r_layers > 0:.  
-00031830: 2020 2020 2020 2020 2020 2020 2020 7365                se
-00031840: 6c66 2e65 6e63 6f64 6572 203d 2054 7261  lf.encoder = Tra
-00031850: 6e73 666f 726d 6572 456e 636f 6465 7228  nsformerEncoder(
-00031860: 6e75 6d5f 6c61 7965 7273 3d65 6e63 6f64  num_layers=encod
-00031870: 6572 5f6c 6179 6572 732c 0a20 2020 2020  er_layers,.     
-00031880: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00031890: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000318a0: 2020 2020 2020 2020 2020 2020 2062 6174               bat
-000318b0: 6368 5f73 697a 653d 6261 7463 685f 7369  ch_size=batch_si
-000318c0: 7a65 2c0a 2020 2020 2020 2020 2020 2020  ze,.            
-000318d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000318e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000318f0: 2020 2020 2020 6869 6464 656e 5f73 697a        hidden_siz
-00031900: 653d 6869 6464 656e 5f73 697a 652c 0a20  e=hidden_size,. 
-00031910: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00031920: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00031930: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00031940: 2066 666e 5f68 6964 6465 6e5f 7369 7a65   ffn_hidden_size
-00031950: 3d66 666e 5f68 6964 6465 6e5f 7369 7a65  =ffn_hidden_size
-00031960: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00031970: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00031980: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00031990: 2020 2020 6e75 6d5f 6865 6164 733d 6e75      num_heads=nu
-000319a0: 6d5f 6865 6164 732c 0a20 2020 2020 2020  m_heads,.       
-000319b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000319c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000319d0: 2020 2020 2020 2020 2020 2073 6571 5f6c             seq_l
-000319e0: 656e 6774 683d 7372 635f 7365 715f 6c65  ength=src_seq_le
-000319f0: 6e67 7468 2c0a 2020 2020 2020 2020 2020  ngth,.          
+00031270: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00031280: 2020 2020 2020 2020 6d73 7479 7065 2e66          mstype.f
+00031290: 6c6f 6174 3136 2c20 6d73 7479 7065 2e62  loat16, mstype.b
+000312a0: 666c 6f61 7431 365d 2c0a 2020 2020 2020  float16],.      
+000312b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000312c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000312d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000312e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000312f0: 2020 2022 5472 616e 7366 6f72 6d65 7222     "Transformer"
+00031300: 292c 0a20 2020 2020 2020 2020 2020 2020  ),.             
+00031310: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00031320: 2020 2070 6172 616d 5f69 6e69 745f 7479     param_init_ty
+00031330: 7065 3d5f 7661 6c69 645f 7661 6c75 655f  pe=_valid_value_
+00031340: 6368 6563 6b73 285b 6d73 7479 7065 2e66  checks([mstype.f
+00031350: 6c6f 6174 3332 2c0a 2020 2020 2020 2020  loat32,.        
+00031360: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00031370: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00031380: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00031390: 2020 2020 2020 2020 2020 2020 206d 7374               mst
+000313a0: 7970 652e 666c 6f61 7431 362c 206d 7374  ype.float16, mst
+000313b0: 7970 652e 6266 6c6f 6174 3136 5d2c 2022  ype.bfloat16], "
+000313c0: 5472 616e 7366 6f72 6d65 7222 292c 0a20  Transformer"),. 
+000313d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000313e0: 2020 2020 2020 2020 2020 2020 2020 2070                 p
+000313f0: 6172 616c 6c65 6c5f 636f 6e66 6967 3d5f  arallel_config=_
+00031400: 7661 6c69 645f 7479 7065 5f63 6865 636b  valid_type_check
+00031410: 7328 5b54 7261 6e73 666f 726d 6572 4f70  s([TransformerOp
+00031420: 5061 7261 6c6c 656c 436f 6e66 6967 5d2c  ParallelConfig],
+00031430: 2022 5472 616e 7366 6f72 6d65 7222 292c   "Transformer"),
+00031440: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00031450: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00031460: 2075 7365 5f70 6173 743d 5661 6c69 6461   use_past=Valida
+00031470: 746f 722e 6368 6563 6b5f 626f 6f6c 290a  tor.check_bool).
+00031480: 2020 2020 6465 6620 5f5f 696e 6974 5f5f      def __init__
+00031490: 2873 656c 662c 0a20 2020 2020 2020 2020  (self,.         
+000314a0: 2020 2020 2020 2020 6869 6464 656e 5f73          hidden_s
+000314b0: 697a 652c 0a20 2020 2020 2020 2020 2020  ize,.           
+000314c0: 2020 2020 2020 6261 7463 685f 7369 7a65        batch_size
+000314d0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+000314e0: 2020 2066 666e 5f68 6964 6465 6e5f 7369     ffn_hidden_si
+000314f0: 7a65 2c0a 2020 2020 2020 2020 2020 2020  ze,.            
+00031500: 2020 2020 2073 7263 5f73 6571 5f6c 656e       src_seq_len
+00031510: 6774 682c 0a20 2020 2020 2020 2020 2020  gth,.           
+00031520: 2020 2020 2020 7467 745f 7365 715f 6c65        tgt_seq_le
+00031530: 6e67 7468 2c0a 2020 2020 2020 2020 2020  ngth,.          
+00031540: 2020 2020 2020 2065 6e63 6f64 6572 5f6c         encoder_l
+00031550: 6179 6572 733d 332c 0a20 2020 2020 2020  ayers=3,.       
+00031560: 2020 2020 2020 2020 2020 6465 636f 6465            decode
+00031570: 725f 6c61 7965 7273 3d33 2c0a 2020 2020  r_layers=3,.    
+00031580: 2020 2020 2020 2020 2020 2020 206e 756d               num
+00031590: 5f68 6561 6473 3d32 2c0a 2020 2020 2020  _heads=2,.      
+000315a0: 2020 2020 2020 2020 2020 2061 7474 656e             atten
+000315b0: 7469 6f6e 5f64 726f 706f 7574 5f72 6174  tion_dropout_rat
+000315c0: 653d 302e 312c 0a20 2020 2020 2020 2020  e=0.1,.         
+000315d0: 2020 2020 2020 2020 6869 6464 656e 5f64          hidden_d
+000315e0: 726f 706f 7574 5f72 6174 653d 302e 312c  ropout_rate=0.1,
+000315f0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00031600: 2020 6869 6464 656e 5f61 6374 3d27 6765    hidden_act='ge
+00031610: 6c75 272c 0a20 2020 2020 2020 2020 2020  lu',.           
+00031620: 2020 2020 2020 706f 7374 5f6c 6179 6572        post_layer
+00031630: 6e6f 726d 5f72 6573 6964 7561 6c3d 4661  norm_residual=Fa
+00031640: 6c73 652c 0a20 2020 2020 2020 2020 2020  lse,.           
+00031650: 2020 2020 2020 6c61 7965 726e 6f72 6d5f        layernorm_
+00031660: 636f 6d70 7574 655f 7479 7065 3d6d 7374  compute_type=mst
+00031670: 7970 652e 666c 6f61 7433 322c 0a20 2020  ype.float32,.   
+00031680: 2020 2020 2020 2020 2020 2020 2020 736f                so
+00031690: 6674 6d61 785f 636f 6d70 7574 655f 7479  ftmax_compute_ty
+000316a0: 7065 3d6d 7374 7970 652e 666c 6f61 7433  pe=mstype.float3
+000316b0: 322c 0a20 2020 2020 2020 2020 2020 2020  2,.             
+000316c0: 2020 2020 7061 7261 6d5f 696e 6974 5f74      param_init_t
+000316d0: 7970 653d 6d73 7479 7065 2e66 6c6f 6174  ype=mstype.float
+000316e0: 3332 2c0a 2020 2020 2020 2020 2020 2020  32,.            
+000316f0: 2020 2020 206c 616d 6264 615f 6675 6e63       lambda_func
+00031700: 3d4e 6f6e 652c 0a20 2020 2020 2020 2020  =None,.         
+00031710: 2020 2020 2020 2020 7573 655f 7061 7374          use_past
+00031720: 3d46 616c 7365 2c0a 2020 2020 2020 2020  =False,.        
+00031730: 2020 2020 2020 2020 206d 6f65 5f63 6f6e           moe_con
+00031740: 6669 673d 6465 6661 756c 745f 6d6f 655f  fig=default_moe_
+00031750: 636f 6e66 6967 2c0a 2020 2020 2020 2020  config,.        
+00031760: 2020 2020 2020 2020 2070 6172 616c 6c65           paralle
+00031770: 6c5f 636f 6e66 6967 3d64 6566 6175 6c74  l_config=default
+00031780: 5f74 7261 6e73 666f 726d 6572 5f63 6f6e  _transformer_con
+00031790: 6669 6729 3a0a 2020 2020 2020 2020 7375  fig):.        su
+000317a0: 7065 7228 5472 616e 7366 6f72 6d65 722c  per(Transformer,
+000317b0: 2073 656c 6629 2e5f 5f69 6e69 745f 5f28   self).__init__(
+000317c0: 290a 2020 2020 2020 2020 6966 2062 6174  ).        if bat
+000317d0: 6368 5f73 697a 6520 6f72 2075 7365 5f70  ch_size or use_p
+000317e0: 6173 743a 0a20 2020 2020 2020 2020 2020  ast:.           
+000317f0: 2056 616c 6964 6174 6f72 2e63 6865 636b   Validator.check
+00031800: 5f70 6f73 6974 6976 655f 696e 7428 6261  _positive_int(ba
+00031810: 7463 685f 7369 7a65 290a 2020 2020 2020  tch_size).      
+00031820: 2020 6966 205f 6765 745f 7061 7261 6c6c    if _get_parall
+00031830: 656c 5f6d 6f64 6528 2920 696e 2028 5061  el_mode() in (Pa
+00031840: 7261 6c6c 656c 4d6f 6465 2e41 5554 4f5f  rallelMode.AUTO_
+00031850: 5041 5241 4c4c 454c 2c29 3a0a 2020 2020  PARALLEL,):.    
+00031860: 2020 2020 2020 2020 5f63 6865 636b 5f63          _check_c
+00031870: 6f6e 6669 6728 7061 7261 6c6c 656c 5f63  onfig(parallel_c
+00031880: 6f6e 6669 6729 0a20 2020 2020 2020 2020  onfig).         
+00031890: 2020 2073 656c 662e 6261 7463 685f 7369     self.batch_si
+000318a0: 7a65 203d 2062 6174 6368 5f73 697a 650a  ze = batch_size.
+000318b0: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+000318c0: 2e68 6964 6465 6e5f 7369 7a65 203d 2068  .hidden_size = h
+000318d0: 6964 6465 6e5f 7369 7a65 0a20 2020 2020  idden_size.     
+000318e0: 2020 2020 2020 2073 656c 662e 7372 635f         self.src_
+000318f0: 7365 715f 6c65 6e67 7468 203d 2073 7263  seq_length = src
+00031900: 5f73 6571 5f6c 656e 6774 680a 2020 2020  _seq_length.    
+00031910: 2020 2020 2020 2020 7365 6c66 2e74 6774          self.tgt
+00031920: 5f73 6571 5f6c 656e 6774 6820 3d20 7467  _seq_length = tg
+00031930: 745f 7365 715f 6c65 6e67 7468 0a20 2020  t_seq_length.   
+00031940: 2020 2020 2020 2020 2073 656c 662e 7573           self.us
+00031950: 655f 7061 7374 203d 2075 7365 5f70 6173  e_past = use_pas
+00031960: 740a 2020 2020 2020 2020 2020 2020 6966  t.            if
+00031970: 2065 6e63 6f64 6572 5f6c 6179 6572 7320   encoder_layers 
+00031980: 3c3d 2030 203c 2064 6563 6f64 6572 5f6c  <= 0 < decoder_l
+00031990: 6179 6572 733a 0a20 2020 2020 2020 2020  ayers:.         
+000319a0: 2020 2020 2020 2072 6169 7365 2056 616c         raise Val
+000319b0: 7565 4572 726f 7228 6622 5472 616e 7366  ueError(f"Transf
+000319c0: 6f72 6d65 7220 646f 6573 7420 7375 7070  ormer doest supp
+000319d0: 6f72 7420 656e 636f 6465 7220 6c61 7965  ort encoder laye
+000319e0: 7220 7b65 6e63 6f64 6572 5f6c 6179 6572  r {encoder_layer
+000319f0: 737d 2061 6e64 2064 6563 6f64 6572 220a  s} and decoder".
 00031a00: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00031a10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00031a20: 2020 2020 2020 2020 6174 7465 6e74 696f          attentio
-00031a30: 6e5f 6472 6f70 6f75 745f 7261 7465 3d61  n_dropout_rate=a
-00031a40: 7474 656e 7469 6f6e 5f64 726f 706f 7574  ttention_dropout
-00031a50: 5f72 6174 652c 0a20 2020 2020 2020 2020  _rate,.         
-00031a60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00031a70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00031a80: 2020 2020 2020 2020 2068 6964 6465 6e5f           hidden_
-00031a90: 6472 6f70 6f75 745f 7261 7465 3d68 6964  dropout_rate=hid
-00031aa0: 6465 6e5f 6472 6f70 6f75 745f 7261 7465  den_dropout_rate
-00031ab0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00031ac0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00031ad0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00031ae0: 2020 2020 6869 6464 656e 5f61 6374 3d68      hidden_act=h
-00031af0: 6964 6465 6e5f 6163 742c 0a20 2020 2020  idden_act,.     
-00031b00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00031b10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00031b20: 2020 2020 2020 2020 2020 2020 206c 6179               lay
-00031b30: 6572 6e6f 726d 5f63 6f6d 7075 7465 5f74  ernorm_compute_t
-00031b40: 7970 653d 6c61 7965 726e 6f72 6d5f 636f  ype=layernorm_co
-00031b50: 6d70 7574 655f 7479 7065 2c0a 2020 2020  mpute_type,.    
-00031b60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00031b70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00031b80: 2020 2020 2020 2020 2020 2020 2020 736f                so
-00031b90: 6674 6d61 785f 636f 6d70 7574 655f 7479  ftmax_compute_ty
-00031ba0: 7065 3d73 6f66 746d 6178 5f63 6f6d 7075  pe=softmax_compu
-00031bb0: 7465 5f74 7970 652c 0a20 2020 2020 2020  te_type,.       
-00031bc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00031bd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00031be0: 2020 2020 2020 2020 2020 2070 6f73 745f             post_
-00031bf0: 6c61 7965 726e 6f72 6d5f 7265 7369 6475  layernorm_residu
-00031c00: 616c 3d70 6f73 745f 6c61 7965 726e 6f72  al=post_layernor
-00031c10: 6d5f 7265 7369 6475 616c 2c0a 2020 2020  m_residual,.    
-00031c20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00031c30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00031c40: 2020 2020 2020 2020 2020 2020 2020 7061                pa
-00031c50: 7261 6d5f 696e 6974 5f74 7970 653d 7061  ram_init_type=pa
-00031c60: 7261 6d5f 696e 6974 5f74 7970 652c 0a20  ram_init_type,. 
-00031c70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00031c80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00031c90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00031ca0: 206c 616d 6264 615f 6675 6e63 3d6c 616d   lambda_func=lam
-00031cb0: 6264 615f 6675 6e63 2c0a 2020 2020 2020  bda_func,.      
-00031cc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00031cd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00031ce0: 2020 2020 2020 2020 2020 2020 7573 655f              use_
-00031cf0: 7061 7374 3d75 7365 5f70 6173 742c 0a20  past=use_past,. 
-00031d00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00031d10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00031d20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00031d30: 206d 6f65 5f63 6f6e 6669 673d 6d6f 655f   moe_config=moe_
-00031d40: 636f 6e66 6967 2c0a 2020 2020 2020 2020  config,.        
-00031d50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00031d60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00031d70: 2020 2020 2020 2020 2020 7061 7261 6c6c            parall
-00031d80: 656c 5f63 6f6e 6669 673d 7061 7261 6c6c  el_config=parall
-00031d90: 656c 5f63 6f6e 6669 6729 0a20 2020 2020  el_config).     
-00031da0: 2020 2020 2020 2065 6c73 653a 0a20 2020         else:.   
-00031db0: 2020 2020 2020 2020 2020 2020 2073 656c               sel
-00031dc0: 662e 656e 636f 6465 7220 3d20 4e6f 6e65  f.encoder = None
-00031dd0: 0a0a 2020 2020 2020 2020 2020 2020 2320  ..            # 
-00031de0: 4f66 6673 6574 2069 7320 6e65 6564 6564  Offset is needed
-00031df0: 2061 7320 7468 6520 656e 636f 6465 7220   as the encoder 
-00031e00: 6861 7320 636f 6e73 756d 6564 2073 6f6d  has consumed som
-00031e10: 6520 666c 6167 732e 0a20 2020 2020 2020  e flags..       
-00031e20: 2020 2020 2023 2073 6f20 7468 6520 6465       # so the de
-00031e30: 636f 6465 7220 6e65 6564 2074 6f20 696e  coder need to in
-00031e40: 6372 6561 7365 2074 6865 2066 6c61 6773  crease the flags
-00031e50: 2062 6173 6564 206f 6e20 7468 6520 656e   based on the en
-00031e60: 636f 6465 7220 6c61 7965 720a 2020 2020  coder layer.    
-00031e70: 2020 2020 2020 2020 7365 6c66 2e64 6563          self.dec
-00031e80: 6f64 6572 203d 204e 6f6e 650a 2020 2020  oder = None.    
-00031e90: 2020 2020 2020 2020 6966 2064 6563 6f64          if decod
-00031ea0: 6572 5f6c 6179 6572 7320 3e20 303a 0a20  er_layers > 0:. 
-00031eb0: 2020 2020 2020 2020 2020 2020 2020 2073                 s
-00031ec0: 656c 662e 6465 636f 6465 7220 3d20 5472  elf.decoder = Tr
-00031ed0: 616e 7366 6f72 6d65 7244 6563 6f64 6572  ansformerDecoder
-00031ee0: 286e 756d 5f6c 6179 6572 733d 6465 636f  (num_layers=deco
-00031ef0: 6465 725f 6c61 7965 7273 2c0a 2020 2020  der_layers,.    
+00031a20: 2066 226c 6179 6572 207b 6465 636f 6465   f"layer {decode
+00031a30: 725f 6c61 7965 7273 7d2c 2070 6c65 6173  r_layers}, pleas
+00031a40: 6520 7573 6520 5472 616e 7366 6f72 6d65  e use Transforme
+00031a50: 7244 6563 6f64 6572 2229 0a20 2020 2020  rDecoder").     
+00031a60: 2020 2020 2020 2069 6620 656e 636f 6465         if encode
+00031a70: 725f 6c61 7965 7273 203e 2030 2061 6e64  r_layers > 0 and
+00031a80: 2064 6563 6f64 6572 5f6c 6179 6572 7320   decoder_layers 
+00031a90: 3e20 3020 616e 6420 7573 655f 7061 7374  > 0 and use_past
+00031aa0: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
+00031ab0: 2020 7261 6973 6520 5661 6c75 6545 7272    raise ValueErr
+00031ac0: 6f72 2866 2254 6865 207b 7365 6c66 2e63  or(f"The {self.c
+00031ad0: 6c73 5f6e 616d 657d 2077 6974 6820 656e  ls_name} with en
+00031ae0: 636f 6465 7220 616e 6420 6465 636f 6465  coder and decode
+00031af0: 7220 646f 6573 206e 6f74 2073 7570 706f  r does not suppo
+00031b00: 7274 2075 7365 5f70 6173 743d 5472 7565  rt use_past=True
+00031b10: 2e22 290a 2020 2020 2020 2020 2020 2020  .").            
+00031b20: 2320 5468 6520 7368 6172 6420 7365 7474  # The shard sett
+00031b30: 696e 6720 6f66 2054 7261 6e73 666f 726d  ing of Transform
+00031b40: 6572 2069 7320 7365 7420 7769 7468 696e  er is set within
+00031b50: 2074 6865 2054 7261 6e73 666f 726d 6572   the Transformer
+00031b60: 456e 636f 6465 724c 6179 6572 0a20 2020  EncoderLayer.   
+00031b70: 2020 2020 2020 2020 2069 6620 6e6f 7420           if not 
+00031b80: 6c61 6d62 6461 5f66 756e 633a 0a20 2020  lambda_func:.   
+00031b90: 2020 2020 2020 2020 2020 2020 206c 616d               lam
+00031ba0: 6264 615f 6675 6e63 203d 205f 6765 745f  bda_func = _get_
+00031bb0: 6c61 6d62 6461 5f66 756e 6328 746f 7461  lambda_func(tota
+00031bc0: 6c5f 6c61 7965 723d 656e 636f 6465 725f  l_layer=encoder_
+00031bd0: 6c61 7965 7273 202b 2064 6563 6f64 6572  layers + decoder
+00031be0: 5f6c 6179 6572 7329 0a20 2020 2020 2020  _layers).       
+00031bf0: 2020 2020 205f 6368 6563 6b5f 6d6f 655f       _check_moe_
+00031c00: 636f 6e66 6967 286d 6f65 5f63 6f6e 6669  config(moe_confi
+00031c10: 672c 2070 6172 616c 6c65 6c5f 636f 6e66  g, parallel_conf
+00031c20: 6967 290a 2020 2020 2020 2020 2020 2020  ig).            
+00031c30: 7365 6c66 2e75 7365 5f6d 6f65 203d 2028  self.use_moe = (
+00031c40: 6d6f 655f 636f 6e66 6967 2e65 7870 6572  moe_config.exper
+00031c50: 745f 6e75 6d20 3e20 3129 0a20 2020 2020  t_num > 1).     
+00031c60: 2020 2020 2020 2073 656c 662e 6164 6420         self.add 
+00031c70: 3d20 502e 4164 6428 290a 2020 2020 2020  = P.Add().      
+00031c80: 2020 2020 2020 7365 6c66 2e61 7578 5f6c        self.aux_l
+00031c90: 6f73 7320 3d20 5465 6e73 6f72 2830 2e30  oss = Tensor(0.0
+00031ca0: 2c20 6d73 7479 7065 2e66 6c6f 6174 3332  , mstype.float32
+00031cb0: 290a 2020 2020 2020 2020 2020 2020 6966  ).            if
+00031cc0: 2065 6e63 6f64 6572 5f6c 6179 6572 7320   encoder_layers 
+00031cd0: 3e20 303a 0a20 2020 2020 2020 2020 2020  > 0:.           
+00031ce0: 2020 2020 2073 656c 662e 656e 636f 6465       self.encode
+00031cf0: 7220 3d20 5472 616e 7366 6f72 6d65 7245  r = TransformerE
+00031d00: 6e63 6f64 6572 286e 756d 5f6c 6179 6572  ncoder(num_layer
+00031d10: 733d 656e 636f 6465 725f 6c61 7965 7273  s=encoder_layers
+00031d20: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00031d30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00031d40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00031d50: 2020 2020 6261 7463 685f 7369 7a65 3d62      batch_size=b
+00031d60: 6174 6368 5f73 697a 652c 0a20 2020 2020  atch_size,.     
+00031d70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00031d80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00031d90: 2020 2020 2020 2020 2020 2020 2068 6964               hid
+00031da0: 6465 6e5f 7369 7a65 3d68 6964 6465 6e5f  den_size=hidden_
+00031db0: 7369 7a65 2c0a 2020 2020 2020 2020 2020  size,.          
+00031dc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00031dd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00031de0: 2020 2020 2020 2020 6666 6e5f 6869 6464          ffn_hidd
+00031df0: 656e 5f73 697a 653d 6666 6e5f 6869 6464  en_size=ffn_hidd
+00031e00: 656e 5f73 697a 652c 0a20 2020 2020 2020  en_size,.       
+00031e10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00031e20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00031e30: 2020 2020 2020 2020 2020 206e 756d 5f68             num_h
+00031e40: 6561 6473 3d6e 756d 5f68 6561 6473 2c0a  eads=num_heads,.
+00031e50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00031e60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00031e70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00031e80: 2020 7365 715f 6c65 6e67 7468 3d73 7263    seq_length=src
+00031e90: 5f73 6571 5f6c 656e 6774 682c 0a20 2020  _seq_length,.   
+00031ea0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00031eb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00031ec0: 2020 2020 2020 2020 2020 2020 2020 2061                 a
+00031ed0: 7474 656e 7469 6f6e 5f64 726f 706f 7574  ttention_dropout
+00031ee0: 5f72 6174 653d 6174 7465 6e74 696f 6e5f  _rate=attention_
+00031ef0: 6472 6f70 6f75 745f 7261 7465 2c0a 2020  dropout_rate,.  
 00031f00: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00031f10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00031f20: 2020 2020 2020 2020 2020 2020 2020 6261                ba
-00031f30: 7463 685f 7369 7a65 3d62 6174 6368 5f73  tch_size=batch_s
-00031f40: 697a 652c 0a20 2020 2020 2020 2020 2020  ize,.           
-00031f50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00031f20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00031f30: 6869 6464 656e 5f64 726f 706f 7574 5f72  hidden_dropout_r
+00031f40: 6174 653d 6869 6464 656e 5f64 726f 706f  ate=hidden_dropo
+00031f50: 7574 5f72 6174 652c 0a20 2020 2020 2020  ut_rate,.       
 00031f60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00031f70: 2020 2020 2020 2068 6964 6465 6e5f 7369         hidden_si
-00031f80: 7a65 3d68 6964 6465 6e5f 7369 7a65 2c0a  ze=hidden_size,.
-00031f90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00031fa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00031f70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00031f80: 2020 2020 2020 2020 2020 2068 6964 6465             hidde
+00031f90: 6e5f 6163 743d 6869 6464 656e 5f61 6374  n_act=hidden_act
+00031fa0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
 00031fb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00031fc0: 2020 6666 6e5f 6869 6464 656e 5f73 697a    ffn_hidden_siz
-00031fd0: 653d 6666 6e5f 6869 6464 656e 5f73 697a  e=ffn_hidden_siz
-00031fe0: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
-00031ff0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032000: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032010: 2020 2020 206e 756d 5f68 6561 6473 3d6e       num_heads=n
-00032020: 756d 5f68 6561 6473 2c0a 2020 2020 2020  um_heads,.      
-00032030: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032040: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032050: 2020 2020 2020 2020 2020 2020 7372 635f              src_
-00032060: 7365 715f 6c65 6e67 7468 3d73 7263 5f73  seq_length=src_s
-00032070: 6571 5f6c 656e 6774 682c 0a20 2020 2020  eq_length,.     
+00031fc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00031fd0: 2020 2020 6c61 7965 726e 6f72 6d5f 636f      layernorm_co
+00031fe0: 6d70 7574 655f 7479 7065 3d6c 6179 6572  mpute_type=layer
+00031ff0: 6e6f 726d 5f63 6f6d 7075 7465 5f74 7970  norm_compute_typ
+00032000: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
+00032010: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00032020: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00032030: 2020 2020 2073 6f66 746d 6178 5f63 6f6d       softmax_com
+00032040: 7075 7465 5f74 7970 653d 736f 6674 6d61  pute_type=softma
+00032050: 785f 636f 6d70 7574 655f 7479 7065 2c0a  x_compute_type,.
+00032060: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00032070: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00032080: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032090: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000320a0: 2020 2020 2020 2020 2020 2020 2074 6774               tgt
-000320b0: 5f73 6571 5f6c 656e 6774 683d 7467 745f  _seq_length=tgt_
-000320c0: 7365 715f 6c65 6e67 7468 2c0a 2020 2020  seq_length,.    
+00032090: 2020 706f 7374 5f6c 6179 6572 6e6f 726d    post_layernorm
+000320a0: 5f72 6573 6964 7561 6c3d 706f 7374 5f6c  _residual=post_l
+000320b0: 6179 6572 6e6f 726d 5f72 6573 6964 7561  ayernorm_residua
+000320c0: 6c2c 0a20 2020 2020 2020 2020 2020 2020  l,.             
 000320d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 000320e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000320f0: 2020 2020 2020 2020 2020 2020 2020 6174                at
-00032100: 7465 6e74 696f 6e5f 6472 6f70 6f75 745f  tention_dropout_
-00032110: 7261 7465 3d61 7474 656e 7469 6f6e 5f64  rate=attention_d
-00032120: 726f 706f 7574 5f72 6174 652c 0a20 2020  ropout_rate,.   
+000320f0: 2020 2020 2070 6172 616d 5f69 6e69 745f       param_init_
+00032100: 7479 7065 3d70 6172 616d 5f69 6e69 745f  type=param_init_
+00032110: 7479 7065 2c0a 2020 2020 2020 2020 2020  type,.          
+00032120: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00032130: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032140: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032150: 2020 2020 2020 2020 2020 2020 2020 2068                 h
-00032160: 6964 6465 6e5f 6472 6f70 6f75 745f 7261  idden_dropout_ra
-00032170: 7465 3d68 6964 6465 6e5f 6472 6f70 6f75  te=hidden_dropou
-00032180: 745f 7261 7465 2c0a 2020 2020 2020 2020  t_rate,.        
-00032190: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000321a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000321b0: 2020 2020 2020 2020 2020 6869 6464 656e            hidden
-000321c0: 5f61 6374 3d68 6964 6465 6e5f 6163 742c  _act=hidden_act,
-000321d0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-000321e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00032140: 2020 2020 2020 2020 6c61 6d62 6461 5f66          lambda_f
+00032150: 756e 633d 6c61 6d62 6461 5f66 756e 632c  unc=lambda_func,
+00032160: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00032170: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00032180: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00032190: 2020 2075 7365 5f70 6173 743d 7573 655f     use_past=use_
+000321a0: 7061 7374 2c0a 2020 2020 2020 2020 2020  past,.          
+000321b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000321c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000321d0: 2020 2020 2020 2020 6d6f 655f 636f 6e66          moe_conf
+000321e0: 6967 3d6d 6f65 5f63 6f6e 6669 672c 0a20  ig=moe_config,. 
 000321f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032200: 2020 2070 6f73 745f 6c61 7965 726e 6f72     post_layernor
-00032210: 6d5f 7265 7369 6475 616c 3d70 6f73 745f  m_residual=post_
-00032220: 6c61 7965 726e 6f72 6d5f 7265 7369 6475  layernorm_residu
-00032230: 616c 2c0a 2020 2020 2020 2020 2020 2020  al,.            
-00032240: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032250: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032260: 2020 2020 2020 6c61 7965 726e 6f72 6d5f        layernorm_
-00032270: 636f 6d70 7574 655f 7479 7065 3d6c 6179  compute_type=lay
-00032280: 6572 6e6f 726d 5f63 6f6d 7075 7465 5f74  ernorm_compute_t
-00032290: 7970 652c 0a20 2020 2020 2020 2020 2020  ype,.           
-000322a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000322b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000322c0: 2020 2020 2020 2073 6f66 746d 6178 5f63         softmax_c
-000322d0: 6f6d 7075 7465 5f74 7970 653d 736f 6674  ompute_type=soft
-000322e0: 6d61 785f 636f 6d70 7574 655f 7479 7065  max_compute_type
-000322f0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00032300: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032310: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032320: 2020 2020 6c61 6d62 6461 5f66 756e 633d      lambda_func=
-00032330: 6c61 6d62 6461 5f66 756e 632c 0a20 2020  lambda_func,.   
-00032340: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032350: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032360: 2020 2020 2020 2020 2020 2020 2020 2075                 u
-00032370: 7365 5f70 6173 743d 7573 655f 7061 7374  se_past=use_past
-00032380: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00032390: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000323a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000323b0: 2020 2020 7061 7261 6d5f 696e 6974 5f74      param_init_t
-000323c0: 7970 653d 7061 7261 6d5f 696e 6974 5f74  ype=param_init_t
-000323d0: 7970 652c 0a20 2020 2020 2020 2020 2020  ype,.           
-000323e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00032200: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00032210: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00032220: 2070 6172 616c 6c65 6c5f 636f 6e66 6967   parallel_config
+00032230: 3d70 6172 616c 6c65 6c5f 636f 6e66 6967  =parallel_config
+00032240: 290a 2020 2020 2020 2020 2020 2020 656c  ).            el
+00032250: 7365 3a0a 2020 2020 2020 2020 2020 2020  se:.            
+00032260: 2020 2020 7365 6c66 2e65 6e63 6f64 6572      self.encoder
+00032270: 203d 204e 6f6e 650a 0a20 2020 2020 2020   = None..       
+00032280: 2020 2020 2023 204f 6666 7365 7420 6973       # Offset is
+00032290: 206e 6565 6465 6420 6173 2074 6865 2065   needed as the e
+000322a0: 6e63 6f64 6572 2068 6173 2063 6f6e 7375  ncoder has consu
+000322b0: 6d65 6420 736f 6d65 2066 6c61 6773 2e0a  med some flags..
+000322c0: 2020 2020 2020 2020 2020 2020 2320 736f              # so
+000322d0: 2074 6865 2064 6563 6f64 6572 206e 6565   the decoder nee
+000322e0: 6420 746f 2069 6e63 7265 6173 6520 7468  d to increase th
+000322f0: 6520 666c 6167 7320 6261 7365 6420 6f6e  e flags based on
+00032300: 2074 6865 2065 6e63 6f64 6572 206c 6179   the encoder lay
+00032310: 6572 0a20 2020 2020 2020 2020 2020 2073  er.            s
+00032320: 656c 662e 6465 636f 6465 7220 3d20 4e6f  elf.decoder = No
+00032330: 6e65 0a20 2020 2020 2020 2020 2020 2069  ne.            i
+00032340: 6620 6465 636f 6465 725f 6c61 7965 7273  f decoder_layers
+00032350: 203e 2030 3a0a 2020 2020 2020 2020 2020   > 0:.          
+00032360: 2020 2020 2020 7365 6c66 2e64 6563 6f64        self.decod
+00032370: 6572 203d 2054 7261 6e73 666f 726d 6572  er = Transformer
+00032380: 4465 636f 6465 7228 6e75 6d5f 6c61 7965  Decoder(num_laye
+00032390: 7273 3d64 6563 6f64 6572 5f6c 6179 6572  rs=decoder_layer
+000323a0: 732c 0a20 2020 2020 2020 2020 2020 2020  s,.             
+000323b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000323c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000323d0: 2020 2020 2062 6174 6368 5f73 697a 653d       batch_size=
+000323e0: 6261 7463 685f 7369 7a65 2c0a 2020 2020  batch_size,.    
 000323f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032400: 2020 2020 2020 206f 6666 7365 743d 656e         offset=en
-00032410: 636f 6465 725f 6c61 7965 7273 2c0a 2020  coder_layers,.  
-00032420: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032430: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00032400: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00032410: 2020 2020 2020 2020 2020 2020 2020 6869                hi
+00032420: 6464 656e 5f73 697a 653d 6869 6464 656e  dden_size=hidden
+00032430: 5f73 697a 652c 0a20 2020 2020 2020 2020  _size,.         
 00032440: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032450: 6d6f 655f 636f 6e66 6967 3d6d 6f65 5f63  moe_config=moe_c
-00032460: 6f6e 6669 672c 0a20 2020 2020 2020 2020  onfig,.         
-00032470: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032480: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032490: 2020 2020 2020 2020 2070 6172 616c 6c65           paralle
-000324a0: 6c5f 636f 6e66 6967 3d70 6172 616c 6c65  l_config=paralle
-000324b0: 6c5f 636f 6e66 6967 290a 2020 2020 2020  l_config).      
-000324c0: 2020 656c 6966 205f 6765 745f 7061 7261    elif _get_para
-000324d0: 6c6c 656c 5f6d 6f64 6528 2920 6e6f 7420  llel_mode() not 
-000324e0: 696e 2028 5061 7261 6c6c 656c 4d6f 6465  in (ParallelMode
-000324f0: 2e41 5554 4f5f 5041 5241 4c4c 454c 2c29  .AUTO_PARALLEL,)
-00032500: 3a0a 2020 2020 2020 2020 2020 2020 5f63  :.            _c
-00032510: 6865 636b 5f63 6f6e 6669 6728 7061 7261  heck_config(para
-00032520: 6c6c 656c 5f63 6f6e 6669 6729 0a20 2020  llel_config).   
-00032530: 2020 2020 2020 2020 2073 656c 662e 6261           self.ba
-00032540: 7463 685f 7369 7a65 203d 2062 6174 6368  tch_size = batch
-00032550: 5f73 697a 650a 2020 2020 2020 2020 2020  _size.          
-00032560: 2020 7365 6c66 2e68 6964 6465 6e5f 7369    self.hidden_si
-00032570: 7a65 203d 2068 6964 6465 6e5f 7369 7a65  ze = hidden_size
-00032580: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-00032590: 662e 7372 635f 7365 715f 6c65 6e67 7468  f.src_seq_length
-000325a0: 203d 2073 7263 5f73 6571 5f6c 656e 6774   = src_seq_lengt
-000325b0: 680a 2020 2020 2020 2020 2020 2020 7365  h.            se
-000325c0: 6c66 2e74 6774 5f73 6571 5f6c 656e 6774  lf.tgt_seq_lengt
-000325d0: 6820 3d20 7467 745f 7365 715f 6c65 6e67  h = tgt_seq_leng
-000325e0: 7468 0a20 2020 2020 2020 2020 2020 2073  th.            s
-000325f0: 656c 662e 7573 655f 7061 7374 203d 2075  elf.use_past = u
-00032600: 7365 5f70 6173 740a 2020 2020 2020 2020  se_past.        
-00032610: 2020 2020 6966 2065 6e63 6f64 6572 5f6c      if encoder_l
-00032620: 6179 6572 7320 3c3d 2030 203c 2064 6563  ayers <= 0 < dec
-00032630: 6f64 6572 5f6c 6179 6572 733a 0a20 2020  oder_layers:.   
-00032640: 2020 2020 2020 2020 2020 2020 2072 6169               rai
-00032650: 7365 2056 616c 7565 4572 726f 7228 6622  se ValueError(f"
-00032660: 5472 616e 7366 6f72 6d65 7220 646f 6573  Transformer does
-00032670: 7420 7375 7070 6f72 7420 656e 636f 6465  t support encode
-00032680: 7220 6c61 7965 7220 7b65 6e63 6f64 6572  r layer {encoder
-00032690: 5f6c 6179 6572 737d 2061 6e64 2064 6563  _layers} and dec
-000326a0: 6f64 6572 220a 2020 2020 2020 2020 2020  oder".          
-000326b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000326c0: 2020 2020 2020 2066 226c 6179 6572 207b         f"layer {
-000326d0: 6465 636f 6465 725f 6c61 7965 7273 7d2c  decoder_layers},
-000326e0: 2070 6c65 6173 6520 7573 6520 5472 616e   please use Tran
-000326f0: 7366 6f72 6d65 7244 6563 6f64 6572 2229  sformerDecoder")
-00032700: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if 
-00032710: 656e 636f 6465 725f 6c61 7965 7273 203e  encoder_layers >
-00032720: 2030 2061 6e64 2064 6563 6f64 6572 5f6c   0 and decoder_l
-00032730: 6179 6572 7320 3e20 3020 616e 6420 7573  ayers > 0 and us
-00032740: 655f 7061 7374 3a0a 2020 2020 2020 2020  e_past:.        
-00032750: 2020 2020 2020 2020 7261 6973 6520 5661          raise Va
-00032760: 6c75 6545 7272 6f72 2866 2254 6865 207b  lueError(f"The {
-00032770: 7365 6c66 2e63 6c73 5f6e 616d 657d 2077  self.cls_name} w
-00032780: 6974 6820 656e 636f 6465 7220 616e 6420  ith encoder and 
-00032790: 6465 636f 6465 7220 646f 6573 206e 6f74  decoder does not
-000327a0: 2073 7570 706f 7274 2075 7365 5f70 6173   support use_pas
-000327b0: 743d 5472 7565 2e22 290a 2020 2020 2020  t=True.").      
-000327c0: 2020 2020 2020 6c6f 6767 6572 2e77 6172        logger.war
-000327d0: 6e69 6e67 2822 466f 7220 7061 7261 6c6c  ning("For parall
-000327e0: 656c 206d 6f64 652c 2073 6861 7264 696e  el mode, shardin
-000327f0: 6720 7072 6f70 6167 6174 696f 6e20 6973  g propagation is
-00032800: 2072 6563 6f6d 6d65 6e64 6564 2c20 796f   recommended, yo
-00032810: 7520 6361 6e20 7573 6520 6974 2062 7920  u can use it by 
-00032820: 7365 7474 696e 6720 220a 2020 2020 2020  setting ".      
+00032450: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00032460: 2020 2020 2020 2020 2066 666e 5f68 6964           ffn_hid
+00032470: 6465 6e5f 7369 7a65 3d66 666e 5f68 6964  den_size=ffn_hid
+00032480: 6465 6e5f 7369 7a65 2c0a 2020 2020 2020  den_size,.      
+00032490: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000324a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000324b0: 2020 2020 2020 2020 2020 2020 6e75 6d5f              num_
+000324c0: 6865 6164 733d 6e75 6d5f 6865 6164 732c  heads=num_heads,
+000324d0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+000324e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000324f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00032500: 2020 2073 7263 5f73 6571 5f6c 656e 6774     src_seq_lengt
+00032510: 683d 7372 635f 7365 715f 6c65 6e67 7468  h=src_seq_length
+00032520: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00032530: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00032540: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00032550: 2020 2020 7467 745f 7365 715f 6c65 6e67      tgt_seq_leng
+00032560: 7468 3d74 6774 5f73 6571 5f6c 656e 6774  th=tgt_seq_lengt
+00032570: 682c 0a20 2020 2020 2020 2020 2020 2020  h,.             
+00032580: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00032590: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000325a0: 2020 2020 2061 7474 656e 7469 6f6e 5f64       attention_d
+000325b0: 726f 706f 7574 5f72 6174 653d 6174 7465  ropout_rate=atte
+000325c0: 6e74 696f 6e5f 6472 6f70 6f75 745f 7261  ntion_dropout_ra
+000325d0: 7465 2c0a 2020 2020 2020 2020 2020 2020  te,.            
+000325e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000325f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00032600: 2020 2020 2020 6869 6464 656e 5f64 726f        hidden_dro
+00032610: 706f 7574 5f72 6174 653d 6869 6464 656e  pout_rate=hidden
+00032620: 5f64 726f 706f 7574 5f72 6174 652c 0a20  _dropout_rate,. 
+00032630: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00032640: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00032650: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00032660: 2068 6964 6465 6e5f 6163 743d 6869 6464   hidden_act=hidd
+00032670: 656e 5f61 6374 2c0a 2020 2020 2020 2020  en_act,.        
+00032680: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00032690: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000326a0: 2020 2020 2020 2020 2020 706f 7374 5f6c            post_l
+000326b0: 6179 6572 6e6f 726d 5f72 6573 6964 7561  ayernorm_residua
+000326c0: 6c3d 706f 7374 5f6c 6179 6572 6e6f 726d  l=post_layernorm
+000326d0: 5f72 6573 6964 7561 6c2c 0a20 2020 2020  _residual,.     
+000326e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000326f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00032700: 2020 2020 2020 2020 2020 2020 206c 6179               lay
+00032710: 6572 6e6f 726d 5f63 6f6d 7075 7465 5f74  ernorm_compute_t
+00032720: 7970 653d 6c61 7965 726e 6f72 6d5f 636f  ype=layernorm_co
+00032730: 6d70 7574 655f 7479 7065 2c0a 2020 2020  mpute_type,.    
+00032740: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00032750: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00032760: 2020 2020 2020 2020 2020 2020 2020 736f                so
+00032770: 6674 6d61 785f 636f 6d70 7574 655f 7479  ftmax_compute_ty
+00032780: 7065 3d73 6f66 746d 6178 5f63 6f6d 7075  pe=softmax_compu
+00032790: 7465 5f74 7970 652c 0a20 2020 2020 2020  te_type,.       
+000327a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000327b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000327c0: 2020 2020 2020 2020 2020 206c 616d 6264             lambd
+000327d0: 615f 6675 6e63 3d6c 616d 6264 615f 6675  a_func=lambda_fu
+000327e0: 6e63 2c0a 2020 2020 2020 2020 2020 2020  nc,.            
+000327f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00032800: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00032810: 2020 2020 2020 7573 655f 7061 7374 3d75        use_past=u
+00032820: 7365 5f70 6173 742c 0a20 2020 2020 2020  se_past,.       
 00032830: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032840: 2020 2020 2022 2773 6574 5f61 7574 6f5f       "'set_auto_
-00032850: 7061 7261 6c6c 656c 5f63 6f6e 7465 7874  parallel_context
-00032860: 2870 6172 616c 6c65 6c5f 6d6f 6465 3d50  (parallel_mode=P
-00032870: 6172 616c 6c65 6c4d 6f64 652e 4155 544f  arallelMode.AUTO
-00032880: 5f50 4152 414c 4c45 4c2c 2022 0a20 2020  _PARALLEL, ".   
+00032840: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00032850: 2020 2020 2020 2020 2020 2070 6172 616d             param
+00032860: 5f69 6e69 745f 7479 7065 3d70 6172 616d  _init_type=param
+00032870: 5f69 6e69 745f 7479 7065 2c0a 2020 2020  _init_type,.    
+00032880: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00032890: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000328a0: 2020 2020 2020 2020 2273 6561 7263 685f          "search_
-000328b0: 6d6f 6465 3d5c 2273 6861 7264 696e 675f  mode=\"sharding_
-000328c0: 7072 6f70 6167 6174 696f 6e5c 2229 2720  propagation\")' 
-000328d0: 616e 6420 220a 2020 2020 2020 2020 2020  and ".          
+000328a0: 2020 2020 2020 2020 2020 2020 2020 6f66                of
+000328b0: 6673 6574 3d65 6e63 6f64 6572 5f6c 6179  fset=encoder_lay
+000328c0: 6572 732c 0a20 2020 2020 2020 2020 2020  ers,.           
+000328d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 000328e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000328f0: 2022 2773 6574 5f61 6c67 6f5f 7061 7261   "'set_algo_para
-00032900: 6d65 7465 7273 2865 6c65 6d65 6e74 7769  meters(elementwi
-00032910: 7365 5f6f 705f 7374 7261 7465 6779 5f66  se_op_strategy_f
-00032920: 6f6c 6c6f 773d 4661 6c73 652c 2066 756c  ollow=False, ful
-00032930: 6c79 5f75 7365 5f64 6576 6963 6573 3d46  ly_use_devices=F
-00032940: 616c 7365 2927 2229 0a20 2020 2020 2020  alse)'").       
-00032950: 2020 2020 2023 2054 6865 2073 6861 7264       # The shard
-00032960: 2073 6574 7469 6e67 206f 6620 5472 616e   setting of Tran
-00032970: 7366 6f72 6d65 7220 6973 2073 6574 2077  sformer is set w
-00032980: 6974 6869 6e20 7468 6520 5472 616e 7366  ithin the Transf
-00032990: 6f72 6d65 7245 6e63 6f64 6572 4c61 7965  ormerEncoderLaye
-000329a0: 720a 2020 2020 2020 2020 2020 2020 6966  r.            if
-000329b0: 206e 6f74 206c 616d 6264 615f 6675 6e63   not lambda_func
-000329c0: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
-000329d0: 2020 6c61 6d62 6461 5f66 756e 6320 3d20    lambda_func = 
-000329e0: 5f67 6574 5f6c 616d 6264 615f 6675 6e63  _get_lambda_func
-000329f0: 2874 6f74 616c 5f6c 6179 6572 3d65 6e63  (total_layer=enc
-00032a00: 6f64 6572 5f6c 6179 6572 7320 2b20 6465  oder_layers + de
-00032a10: 636f 6465 725f 6c61 7965 7273 290a 2020  coder_layers).  
-00032a20: 2020 2020 2020 2020 2020 5f63 6865 636b            _check
-00032a30: 5f6d 6f65 5f63 6f6e 6669 6728 6d6f 655f  _moe_config(moe_
-00032a40: 636f 6e66 6967 2c20 7061 7261 6c6c 656c  config, parallel
-00032a50: 5f63 6f6e 6669 6729 0a20 2020 2020 2020  _config).       
-00032a60: 2020 2020 2073 656c 662e 7573 655f 6d6f       self.use_mo
-00032a70: 6520 3d20 286d 6f65 5f63 6f6e 6669 672e  e = (moe_config.
-00032a80: 6578 7065 7274 5f6e 756d 203e 2031 290a  expert_num > 1).
-00032a90: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-00032aa0: 2e61 6464 203d 2050 2e41 6464 2829 2e73  .add = P.Add().s
-00032ab0: 6861 7264 2828 2829 2c20 2829 2929 0a20  hard(((), ())). 
-00032ac0: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-00032ad0: 6175 785f 6c6f 7373 203d 2054 656e 736f  aux_loss = Tenso
-00032ae0: 7228 302e 302c 206d 7374 7970 652e 666c  r(0.0, mstype.fl
-00032af0: 6f61 7433 3229 0a20 2020 2020 2020 2020  oat32).         
-00032b00: 2020 2069 6620 656e 636f 6465 725f 6c61     if encoder_la
-00032b10: 7965 7273 203e 2030 3a0a 2020 2020 2020  yers > 0:.      
-00032b20: 2020 2020 2020 2020 2020 7365 6c66 2e65            self.e
-00032b30: 6e63 6f64 6572 203d 2054 7261 6e73 666f  ncoder = Transfo
-00032b40: 726d 6572 456e 636f 6465 7228 6e75 6d5f  rmerEncoder(num_
-00032b50: 6c61 7965 7273 3d65 6e63 6f64 6572 5f6c  layers=encoder_l
-00032b60: 6179 6572 732c 0a20 2020 2020 2020 2020  ayers,.         
-00032b70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032b80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032b90: 2020 2020 2020 2020 2062 6174 6368 5f73           batch_s
-00032ba0: 697a 653d 6261 7463 685f 7369 7a65 2c0a  ize=batch_size,.
-00032bb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032bc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032bd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032be0: 2020 6869 6464 656e 5f73 697a 653d 6869    hidden_size=hi
-00032bf0: 6464 656e 5f73 697a 652c 0a20 2020 2020  dden_size,.     
-00032c00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032c10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032c20: 2020 2020 2020 2020 2020 2020 2066 666e               ffn
-00032c30: 5f68 6964 6465 6e5f 7369 7a65 3d66 666e  _hidden_size=ffn
-00032c40: 5f68 6964 6465 6e5f 7369 7a65 2c0a 2020  _hidden_size,.  
-00032c50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032c60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032c70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032c80: 6e75 6d5f 6865 6164 733d 6e75 6d5f 6865  num_heads=num_he
-00032c90: 6164 732c 0a20 2020 2020 2020 2020 2020  ads,.           
-00032ca0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032cb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032cc0: 2020 2020 2020 2073 6571 5f6c 656e 6774         seq_lengt
-00032cd0: 683d 7372 635f 7365 715f 6c65 6e67 7468  h=src_seq_length
-00032ce0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00032cf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032d00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032d10: 2020 2020 6174 7465 6e74 696f 6e5f 6472      attention_dr
-00032d20: 6f70 6f75 745f 7261 7465 3d61 7474 656e  opout_rate=atten
-00032d30: 7469 6f6e 5f64 726f 706f 7574 5f72 6174  tion_dropout_rat
-00032d40: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
-00032d50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032d60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032d70: 2020 2020 2068 6964 6465 6e5f 6472 6f70       hidden_drop
-00032d80: 6f75 745f 7261 7465 3d68 6964 6465 6e5f  out_rate=hidden_
-00032d90: 6472 6f70 6f75 745f 7261 7465 2c0a 2020  dropout_rate,.  
-00032da0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032db0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032dc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032dd0: 6869 6464 656e 5f61 6374 3d68 6964 6465  hidden_act=hidde
-00032de0: 6e5f 6163 742c 0a20 2020 2020 2020 2020  n_act,.         
-00032df0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032e00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032e10: 2020 2020 2020 2020 206c 6179 6572 6e6f           layerno
-00032e20: 726d 5f63 6f6d 7075 7465 5f74 7970 653d  rm_compute_type=
-00032e30: 6c61 7965 726e 6f72 6d5f 636f 6d70 7574  layernorm_comput
-00032e40: 655f 7479 7065 2c0a 2020 2020 2020 2020  e_type,.        
-00032e50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032e60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032e70: 2020 2020 2020 2020 2020 736f 6674 6d61            softma
-00032e80: 785f 636f 6d70 7574 655f 7479 7065 3d73  x_compute_type=s
-00032e90: 6f66 746d 6178 5f63 6f6d 7075 7465 5f74  oftmax_compute_t
-00032ea0: 7970 652c 0a20 2020 2020 2020 2020 2020  ype,.           
-00032eb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032ec0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032ed0: 2020 2020 2020 2070 6f73 745f 6c61 7965         post_laye
-00032ee0: 726e 6f72 6d5f 7265 7369 6475 616c 3d70  rnorm_residual=p
-00032ef0: 6f73 745f 6c61 7965 726e 6f72 6d5f 7265  ost_layernorm_re
-00032f00: 7369 6475 616c 2c0a 2020 2020 2020 2020  sidual,.        
-00032f10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032f20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032f30: 2020 2020 2020 2020 2020 7061 7261 6d5f            param_
-00032f40: 696e 6974 5f74 7970 653d 7061 7261 6d5f  init_type=param_
-00032f50: 696e 6974 5f74 7970 652c 0a20 2020 2020  init_type,.     
-00032f60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032f70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032f80: 2020 2020 2020 2020 2020 2020 206c 616d               lam
-00032f90: 6264 615f 6675 6e63 3d6c 616d 6264 615f  bda_func=lambda_
-00032fa0: 6675 6e63 2c0a 2020 2020 2020 2020 2020  func,.          
-00032fb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032fc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00032fd0: 2020 2020 2020 2020 7573 655f 7061 7374          use_past
-00032fe0: 3d75 7365 5f70 6173 742c 0a20 2020 2020  =use_past,.     
-00032ff0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033000: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033010: 2020 2020 2020 2020 2020 2020 206d 6f65               moe
-00033020: 5f63 6f6e 6669 673d 6d6f 655f 636f 6e66  _config=moe_conf
-00033030: 6967 2c0a 2020 2020 2020 2020 2020 2020  ig,.            
-00033040: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033050: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033060: 2020 2020 2020 7061 7261 6c6c 656c 5f63        parallel_c
-00033070: 6f6e 6669 673d 7061 7261 6c6c 656c 5f63  onfig=parallel_c
-00033080: 6f6e 6669 6729 0a20 2020 2020 2020 2020  onfig).         
-00033090: 2020 2065 6c73 653a 0a20 2020 2020 2020     else:.       
-000330a0: 2020 2020 2020 2020 2073 656c 662e 656e           self.en
-000330b0: 636f 6465 7220 3d20 4e6f 6e65 0a0a 2020  coder = None..  
-000330c0: 2020 2020 2020 2020 2020 2320 4f66 6673            # Offs
-000330d0: 6574 2069 7320 6e65 6564 6564 2061 7320  et is needed as 
-000330e0: 7468 6520 656e 636f 6465 7220 6861 7320  the encoder has 
-000330f0: 636f 6e73 756d 6564 2073 6f6d 6520 666c  consumed some fl
-00033100: 6167 732e 0a20 2020 2020 2020 2020 2020  ags..           
-00033110: 2023 2073 6f20 7468 6520 6465 636f 6465   # so the decode
-00033120: 7220 6e65 6564 2074 6f20 696e 6372 6561  r need to increa
-00033130: 7365 2074 6865 2066 6c61 6773 2062 6173  se the flags bas
-00033140: 6564 206f 6e20 7468 6520 656e 636f 6465  ed on the encode
-00033150: 7220 6c61 7965 720a 2020 2020 2020 2020  r layer.        
-00033160: 2020 2020 7365 6c66 2e64 6563 6f64 6572      self.decoder
-00033170: 203d 204e 6f6e 650a 2020 2020 2020 2020   = None.        
-00033180: 2020 2020 6966 2064 6563 6f64 6572 5f6c      if decoder_l
-00033190: 6179 6572 7320 3e20 303a 0a20 2020 2020  ayers > 0:.     
-000331a0: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-000331b0: 6465 636f 6465 7220 3d20 5472 616e 7366  decoder = Transf
-000331c0: 6f72 6d65 7244 6563 6f64 6572 286e 756d  ormerDecoder(num
-000331d0: 5f6c 6179 6572 733d 6465 636f 6465 725f  _layers=decoder_
-000331e0: 6c61 7965 7273 2c0a 2020 2020 2020 2020  layers,.        
+000328f0: 2020 2020 2020 206d 6f65 5f63 6f6e 6669         moe_confi
+00032900: 673d 6d6f 655f 636f 6e66 6967 2c0a 2020  g=moe_config,.  
+00032910: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00032920: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00032930: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00032940: 7061 7261 6c6c 656c 5f63 6f6e 6669 673d  parallel_config=
+00032950: 7061 7261 6c6c 656c 5f63 6f6e 6669 6729  parallel_config)
+00032960: 0a20 2020 2020 2020 2065 6c69 6620 5f67  .        elif _g
+00032970: 6574 5f70 6172 616c 6c65 6c5f 6d6f 6465  et_parallel_mode
+00032980: 2829 206e 6f74 2069 6e20 2850 6172 616c  () not in (Paral
+00032990: 6c65 6c4d 6f64 652e 4155 544f 5f50 4152  lelMode.AUTO_PAR
+000329a0: 414c 4c45 4c2c 293a 0a20 2020 2020 2020  ALLEL,):.       
+000329b0: 2020 2020 205f 6368 6563 6b5f 636f 6e66       _check_conf
+000329c0: 6967 2870 6172 616c 6c65 6c5f 636f 6e66  ig(parallel_conf
+000329d0: 6967 290a 2020 2020 2020 2020 2020 2020  ig).            
+000329e0: 7365 6c66 2e62 6174 6368 5f73 697a 6520  self.batch_size 
+000329f0: 3d20 6261 7463 685f 7369 7a65 0a20 2020  = batch_size.   
+00032a00: 2020 2020 2020 2020 2073 656c 662e 6869           self.hi
+00032a10: 6464 656e 5f73 697a 6520 3d20 6869 6464  dden_size = hidd
+00032a20: 656e 5f73 697a 650a 2020 2020 2020 2020  en_size.        
+00032a30: 2020 2020 7365 6c66 2e73 7263 5f73 6571      self.src_seq
+00032a40: 5f6c 656e 6774 6820 3d20 7372 635f 7365  _length = src_se
+00032a50: 715f 6c65 6e67 7468 0a20 2020 2020 2020  q_length.       
+00032a60: 2020 2020 2073 656c 662e 7467 745f 7365       self.tgt_se
+00032a70: 715f 6c65 6e67 7468 203d 2074 6774 5f73  q_length = tgt_s
+00032a80: 6571 5f6c 656e 6774 680a 2020 2020 2020  eq_length.      
+00032a90: 2020 2020 2020 7365 6c66 2e75 7365 5f70        self.use_p
+00032aa0: 6173 7420 3d20 7573 655f 7061 7374 0a20  ast = use_past. 
+00032ab0: 2020 2020 2020 2020 2020 2069 6620 656e             if en
+00032ac0: 636f 6465 725f 6c61 7965 7273 203c 3d20  coder_layers <= 
+00032ad0: 3020 3c20 6465 636f 6465 725f 6c61 7965  0 < decoder_laye
+00032ae0: 7273 3a0a 2020 2020 2020 2020 2020 2020  rs:.            
+00032af0: 2020 2020 7261 6973 6520 5661 6c75 6545      raise ValueE
+00032b00: 7272 6f72 2866 2254 7261 6e73 666f 726d  rror(f"Transform
+00032b10: 6572 2064 6f65 7374 2073 7570 706f 7274  er doest support
+00032b20: 2065 6e63 6f64 6572 206c 6179 6572 207b   encoder layer {
+00032b30: 656e 636f 6465 725f 6c61 7965 7273 7d20  encoder_layers} 
+00032b40: 616e 6420 6465 636f 6465 7222 0a20 2020  and decoder".   
+00032b50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00032b60: 2020 2020 2020 2020 2020 2020 2020 6622                f"
+00032b70: 6c61 7965 7220 7b64 6563 6f64 6572 5f6c  layer {decoder_l
+00032b80: 6179 6572 737d 2c20 706c 6561 7365 2075  ayers}, please u
+00032b90: 7365 2054 7261 6e73 666f 726d 6572 4465  se TransformerDe
+00032ba0: 636f 6465 7222 290a 2020 2020 2020 2020  coder").        
+00032bb0: 2020 2020 6966 2065 6e63 6f64 6572 5f6c      if encoder_l
+00032bc0: 6179 6572 7320 3e20 3020 616e 6420 6465  ayers > 0 and de
+00032bd0: 636f 6465 725f 6c61 7965 7273 203e 2030  coder_layers > 0
+00032be0: 2061 6e64 2075 7365 5f70 6173 743a 0a20   and use_past:. 
+00032bf0: 2020 2020 2020 2020 2020 2020 2020 2072                 r
+00032c00: 6169 7365 2056 616c 7565 4572 726f 7228  aise ValueError(
+00032c10: 6622 5468 6520 7b73 656c 662e 636c 735f  f"The {self.cls_
+00032c20: 6e61 6d65 7d20 7769 7468 2065 6e63 6f64  name} with encod
+00032c30: 6572 2061 6e64 2064 6563 6f64 6572 2064  er and decoder d
+00032c40: 6f65 7320 6e6f 7420 7375 7070 6f72 7420  oes not support 
+00032c50: 7573 655f 7061 7374 3d54 7275 652e 2229  use_past=True.")
+00032c60: 0a20 2020 2020 2020 2020 2020 206c 6f67  .            log
+00032c70: 6765 722e 7761 726e 696e 6728 2246 6f72  ger.warning("For
+00032c80: 2070 6172 616c 6c65 6c20 6d6f 6465 2c20   parallel mode, 
+00032c90: 7368 6172 6469 6e67 2070 726f 7061 6761  sharding propaga
+00032ca0: 7469 6f6e 2069 7320 7265 636f 6d6d 656e  tion is recommen
+00032cb0: 6465 642c 2079 6f75 2063 616e 2075 7365  ded, you can use
+00032cc0: 2069 7420 6279 2073 6574 7469 6e67 2022   it by setting "
+00032cd0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00032ce0: 2020 2020 2020 2020 2020 2020 2227 7365              "'se
+00032cf0: 745f 6175 746f 5f70 6172 616c 6c65 6c5f  t_auto_parallel_
+00032d00: 636f 6e74 6578 7428 7061 7261 6c6c 656c  context(parallel
+00032d10: 5f6d 6f64 653d 5061 7261 6c6c 656c 4d6f  _mode=ParallelMo
+00032d20: 6465 2e41 5554 4f5f 5041 5241 4c4c 454c  de.AUTO_PARALLEL
+00032d30: 2c20 220a 2020 2020 2020 2020 2020 2020  , ".            
+00032d40: 2020 2020 2020 2020 2020 2020 2020 2022                 "
+00032d50: 7365 6172 6368 5f6d 6f64 653d 5c22 7368  search_mode=\"sh
+00032d60: 6172 6469 6e67 5f70 726f 7061 6761 7469  arding_propagati
+00032d70: 6f6e 5c22 2927 2061 6e64 2022 0a20 2020  on\")' and ".   
+00032d80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00032d90: 2020 2020 2020 2020 2227 7365 745f 616c          "'set_al
+00032da0: 676f 5f70 6172 616d 6574 6572 7328 656c  go_parameters(el
+00032db0: 656d 656e 7477 6973 655f 6f70 5f73 7472  ementwise_op_str
+00032dc0: 6174 6567 795f 666f 6c6c 6f77 3d46 616c  ategy_follow=Fal
+00032dd0: 7365 2c20 6675 6c6c 795f 7573 655f 6465  se, fully_use_de
+00032de0: 7669 6365 733d 4661 6c73 6529 2722 290a  vices=False)'").
+00032df0: 2020 2020 2020 2020 2020 2020 2320 5468              # Th
+00032e00: 6520 7368 6172 6420 7365 7474 696e 6720  e shard setting 
+00032e10: 6f66 2054 7261 6e73 666f 726d 6572 2069  of Transformer i
+00032e20: 7320 7365 7420 7769 7468 696e 2074 6865  s set within the
+00032e30: 2054 7261 6e73 666f 726d 6572 456e 636f   TransformerEnco
+00032e40: 6465 724c 6179 6572 0a20 2020 2020 2020  derLayer.       
+00032e50: 2020 2020 2069 6620 6e6f 7420 6c61 6d62       if not lamb
+00032e60: 6461 5f66 756e 633a 0a20 2020 2020 2020  da_func:.       
+00032e70: 2020 2020 2020 2020 206c 616d 6264 615f           lambda_
+00032e80: 6675 6e63 203d 205f 6765 745f 6c61 6d62  func = _get_lamb
+00032e90: 6461 5f66 756e 6328 746f 7461 6c5f 6c61  da_func(total_la
+00032ea0: 7965 723d 656e 636f 6465 725f 6c61 7965  yer=encoder_laye
+00032eb0: 7273 202b 2064 6563 6f64 6572 5f6c 6179  rs + decoder_lay
+00032ec0: 6572 7329 0a20 2020 2020 2020 2020 2020  ers).           
+00032ed0: 205f 6368 6563 6b5f 6d6f 655f 636f 6e66   _check_moe_conf
+00032ee0: 6967 286d 6f65 5f63 6f6e 6669 672c 2070  ig(moe_config, p
+00032ef0: 6172 616c 6c65 6c5f 636f 6e66 6967 290a  arallel_config).
+00032f00: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+00032f10: 2e75 7365 5f6d 6f65 203d 2028 6d6f 655f  .use_moe = (moe_
+00032f20: 636f 6e66 6967 2e65 7870 6572 745f 6e75  config.expert_nu
+00032f30: 6d20 3e20 3129 0a20 2020 2020 2020 2020  m > 1).         
+00032f40: 2020 2073 656c 662e 6164 6420 3d20 502e     self.add = P.
+00032f50: 4164 6428 292e 7368 6172 6428 2828 292c  Add().shard(((),
+00032f60: 2028 2929 290a 2020 2020 2020 2020 2020   ())).          
+00032f70: 2020 7365 6c66 2e61 7578 5f6c 6f73 7320    self.aux_loss 
+00032f80: 3d20 5465 6e73 6f72 2830 2e30 2c20 6d73  = Tensor(0.0, ms
+00032f90: 7479 7065 2e66 6c6f 6174 3332 290a 2020  type.float32).  
+00032fa0: 2020 2020 2020 2020 2020 6966 2065 6e63            if enc
+00032fb0: 6f64 6572 5f6c 6179 6572 7320 3e20 303a  oder_layers > 0:
+00032fc0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00032fd0: 2073 656c 662e 656e 636f 6465 7220 3d20   self.encoder = 
+00032fe0: 5472 616e 7366 6f72 6d65 7245 6e63 6f64  TransformerEncod
+00032ff0: 6572 286e 756d 5f6c 6179 6572 733d 656e  er(num_layers=en
+00033000: 636f 6465 725f 6c61 7965 7273 2c0a 2020  coder_layers,.  
+00033010: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00033020: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00033030: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00033040: 6261 7463 685f 7369 7a65 3d62 6174 6368  batch_size=batch
+00033050: 5f73 697a 652c 0a20 2020 2020 2020 2020  _size,.         
+00033060: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00033070: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00033080: 2020 2020 2020 2020 2068 6964 6465 6e5f           hidden_
+00033090: 7369 7a65 3d68 6964 6465 6e5f 7369 7a65  size=hidden_size
+000330a0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+000330b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000330c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000330d0: 2020 2020 6666 6e5f 6869 6464 656e 5f73      ffn_hidden_s
+000330e0: 697a 653d 6666 6e5f 6869 6464 656e 5f73  ize=ffn_hidden_s
+000330f0: 697a 652c 0a20 2020 2020 2020 2020 2020  ize,.           
+00033100: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00033110: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00033120: 2020 2020 2020 206e 756d 5f68 6561 6473         num_heads
+00033130: 3d6e 756d 5f68 6561 6473 2c0a 2020 2020  =num_heads,.    
+00033140: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00033150: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00033160: 2020 2020 2020 2020 2020 2020 2020 7365                se
+00033170: 715f 6c65 6e67 7468 3d73 7263 5f73 6571  q_length=src_seq
+00033180: 5f6c 656e 6774 682c 0a20 2020 2020 2020  _length,.       
+00033190: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000331a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000331b0: 2020 2020 2020 2020 2020 2061 7474 656e             atten
+000331c0: 7469 6f6e 5f64 726f 706f 7574 5f72 6174  tion_dropout_rat
+000331d0: 653d 6174 7465 6e74 696f 6e5f 6472 6f70  e=attention_drop
+000331e0: 6f75 745f 7261 7465 2c0a 2020 2020 2020  out_rate,.      
 000331f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00033200: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033210: 2020 2020 2020 2020 2020 6261 7463 685f            batch_
-00033220: 7369 7a65 3d62 6174 6368 5f73 697a 652c  size=batch_size,
-00033230: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00033240: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00033210: 2020 2020 2020 2020 2020 2020 6869 6464              hidd
+00033220: 656e 5f64 726f 706f 7574 5f72 6174 653d  en_dropout_rate=
+00033230: 6869 6464 656e 5f64 726f 706f 7574 5f72  hidden_dropout_r
+00033240: 6174 652c 0a20 2020 2020 2020 2020 2020  ate,.           
 00033250: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033260: 2020 2068 6964 6465 6e5f 7369 7a65 3d68     hidden_size=h
-00033270: 6964 6465 6e5f 7369 7a65 2c0a 2020 2020  idden_size,.    
-00033280: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00033260: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00033270: 2020 2020 2020 2068 6964 6465 6e5f 6163         hidden_ac
+00033280: 743d 6869 6464 656e 5f61 6374 2c0a 2020  t=hidden_act,.  
 00033290: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000332a0: 2020 2020 2020 2020 2020 2020 2020 6666                ff
-000332b0: 6e5f 6869 6464 656e 5f73 697a 653d 6666  n_hidden_size=ff
-000332c0: 6e5f 6869 6464 656e 5f73 697a 652c 0a20  n_hidden_size,. 
-000332d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000332e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000332a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000332b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000332c0: 6c61 7965 726e 6f72 6d5f 636f 6d70 7574  layernorm_comput
+000332d0: 655f 7479 7065 3d6c 6179 6572 6e6f 726d  e_type=layernorm
+000332e0: 5f63 6f6d 7075 7465 5f74 7970 652c 0a20  _compute_type,. 
 000332f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033300: 206e 756d 5f68 6561 6473 3d6e 756d 5f68   num_heads=num_h
-00033310: 6561 6473 2c0a 2020 2020 2020 2020 2020  eads,.          
-00033320: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033330: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033340: 2020 2020 2020 2020 7372 635f 7365 715f          src_seq_
-00033350: 6c65 6e67 7468 3d73 7263 5f73 6571 5f6c  length=src_seq_l
-00033360: 656e 6774 682c 0a20 2020 2020 2020 2020  ength,.         
-00033370: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033380: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033390: 2020 2020 2020 2020 2074 6774 5f73 6571           tgt_seq
-000333a0: 5f6c 656e 6774 683d 7467 745f 7365 715f  _length=tgt_seq_
-000333b0: 6c65 6e67 7468 2c0a 2020 2020 2020 2020  length,.        
+00033300: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00033310: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00033320: 2073 6f66 746d 6178 5f63 6f6d 7075 7465   softmax_compute
+00033330: 5f74 7970 653d 736f 6674 6d61 785f 636f  _type=softmax_co
+00033340: 6d70 7574 655f 7479 7065 2c0a 2020 2020  mpute_type,.    
+00033350: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00033360: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00033370: 2020 2020 2020 2020 2020 2020 2020 706f                po
+00033380: 7374 5f6c 6179 6572 6e6f 726d 5f72 6573  st_layernorm_res
+00033390: 6964 7561 6c3d 706f 7374 5f6c 6179 6572  idual=post_layer
+000333a0: 6e6f 726d 5f72 6573 6964 7561 6c2c 0a20  norm_residual,. 
+000333b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 000333c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 000333d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000333e0: 2020 2020 2020 2020 2020 6174 7465 6e74            attent
-000333f0: 696f 6e5f 6472 6f70 6f75 745f 7261 7465  ion_dropout_rate
-00033400: 3d61 7474 656e 7469 6f6e 5f64 726f 706f  =attention_dropo
-00033410: 7574 5f72 6174 652c 0a20 2020 2020 2020  ut_rate,.       
+000333e0: 2070 6172 616d 5f69 6e69 745f 7479 7065   param_init_type
+000333f0: 3d70 6172 616d 5f69 6e69 745f 7479 7065  =param_init_type
+00033400: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00033410: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00033420: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033430: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033440: 2020 2020 2020 2020 2020 2068 6964 6465             hidde
-00033450: 6e5f 6472 6f70 6f75 745f 7261 7465 3d68  n_dropout_rate=h
-00033460: 6964 6465 6e5f 6472 6f70 6f75 745f 7261  idden_dropout_ra
-00033470: 7465 2c0a 2020 2020 2020 2020 2020 2020  te,.            
-00033480: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033490: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000334a0: 2020 2020 2020 6869 6464 656e 5f61 6374        hidden_act
-000334b0: 3d68 6964 6465 6e5f 6163 742c 0a20 2020  =hidden_act,.   
-000334c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000334d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000334e0: 2020 2020 2020 2020 2020 2020 2020 2070                 p
-000334f0: 6f73 745f 6c61 7965 726e 6f72 6d5f 7265  ost_layernorm_re
-00033500: 7369 6475 616c 3d70 6f73 745f 6c61 7965  sidual=post_laye
-00033510: 726e 6f72 6d5f 7265 7369 6475 616c 2c0a  rnorm_residual,.
-00033520: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033530: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00033430: 2020 2020 6c61 6d62 6461 5f66 756e 633d      lambda_func=
+00033440: 6c61 6d62 6461 5f66 756e 632c 0a20 2020  lambda_func,.   
+00033450: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00033460: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00033470: 2020 2020 2020 2020 2020 2020 2020 2075                 u
+00033480: 7365 5f70 6173 743d 7573 655f 7061 7374  se_past=use_past
+00033490: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+000334a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000334b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000334c0: 2020 2020 6d6f 655f 636f 6e66 6967 3d6d      moe_config=m
+000334d0: 6f65 5f63 6f6e 6669 672c 0a20 2020 2020  oe_config,.     
+000334e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000334f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00033500: 2020 2020 2020 2020 2020 2020 2070 6172               par
+00033510: 616c 6c65 6c5f 636f 6e66 6967 3d70 6172  allel_config=par
+00033520: 616c 6c65 6c5f 636f 6e66 6967 290a 2020  allel_config).  
+00033530: 2020 2020 2020 2020 2020 656c 7365 3a0a            else:.
 00033540: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033550: 2020 6c61 7965 726e 6f72 6d5f 636f 6d70    layernorm_comp
-00033560: 7574 655f 7479 7065 3d6c 6179 6572 6e6f  ute_type=layerno
-00033570: 726d 5f63 6f6d 7075 7465 5f74 7970 652c  rm_compute_type,
-00033580: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00033590: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000335a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000335b0: 2020 2073 6f66 746d 6178 5f63 6f6d 7075     softmax_compu
-000335c0: 7465 5f74 7970 653d 736f 6674 6d61 785f  te_type=softmax_
-000335d0: 636f 6d70 7574 655f 7479 7065 2c0a 2020  compute_type,.  
-000335e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000335f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033600: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033610: 6c61 6d62 6461 5f66 756e 633d 6c61 6d62  lambda_func=lamb
-00033620: 6461 5f66 756e 632c 0a20 2020 2020 2020  da_func,.       
-00033630: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033640: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033650: 2020 2020 2020 2020 2020 2075 7365 5f70             use_p
-00033660: 6173 743d 7573 655f 7061 7374 2c0a 2020  ast=use_past,.  
-00033670: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033680: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00033550: 7365 6c66 2e65 6e63 6f64 6572 203d 204e  self.encoder = N
+00033560: 6f6e 650a 0a20 2020 2020 2020 2020 2020  one..           
+00033570: 2023 204f 6666 7365 7420 6973 206e 6565   # Offset is nee
+00033580: 6465 6420 6173 2074 6865 2065 6e63 6f64  ded as the encod
+00033590: 6572 2068 6173 2063 6f6e 7375 6d65 6420  er has consumed 
+000335a0: 736f 6d65 2066 6c61 6773 2e0a 2020 2020  some flags..    
+000335b0: 2020 2020 2020 2020 2320 736f 2074 6865          # so the
+000335c0: 2064 6563 6f64 6572 206e 6565 6420 746f   decoder need to
+000335d0: 2069 6e63 7265 6173 6520 7468 6520 666c   increase the fl
+000335e0: 6167 7320 6261 7365 6420 6f6e 2074 6865  ags based on the
+000335f0: 2065 6e63 6f64 6572 206c 6179 6572 0a20   encoder layer. 
+00033600: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+00033610: 6465 636f 6465 7220 3d20 4e6f 6e65 0a20  decoder = None. 
+00033620: 2020 2020 2020 2020 2020 2069 6620 6465             if de
+00033630: 636f 6465 725f 6c61 7965 7273 203e 2030  coder_layers > 0
+00033640: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
+00033650: 2020 7365 6c66 2e64 6563 6f64 6572 203d    self.decoder =
+00033660: 2054 7261 6e73 666f 726d 6572 4465 636f   TransformerDeco
+00033670: 6465 7228 6e75 6d5f 6c61 7965 7273 3d64  der(num_layers=d
+00033680: 6563 6f64 6572 5f6c 6179 6572 732c 0a20  ecoder_layers,. 
 00033690: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000336a0: 7061 7261 6d5f 696e 6974 5f74 7970 653d  param_init_type=
-000336b0: 7061 7261 6d5f 696e 6974 5f74 7970 652c  param_init_type,
-000336c0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-000336d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000336a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000336b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000336c0: 2062 6174 6368 5f73 697a 653d 6261 7463   batch_size=batc
+000336d0: 685f 7369 7a65 2c0a 2020 2020 2020 2020  h_size,.        
 000336e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000336f0: 2020 206f 6666 7365 743d 656e 636f 6465     offset=encode
-00033700: 725f 6c61 7965 7273 2c0a 2020 2020 2020  r_layers,.      
-00033710: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033720: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033730: 2020 2020 2020 2020 2020 2020 6d6f 655f              moe_
-00033740: 636f 6e66 6967 3d6d 6f65 5f63 6f6e 6669  config=moe_confi
-00033750: 672c 0a20 2020 2020 2020 2020 2020 2020  g,.             
-00033760: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033770: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033780: 2020 2020 2070 6172 616c 6c65 6c5f 636f       parallel_co
-00033790: 6e66 6967 3d70 6172 616c 6c65 6c5f 636f  nfig=parallel_co
-000337a0: 6e66 6967 290a 2020 2020 2020 2020 656c  nfig).        el
-000337b0: 7365 3a0a 2020 2020 2020 2020 2020 2020  se:.            
-000337c0: 7261 6973 6520 5275 6e74 696d 6545 7272  raise RuntimeErr
-000337d0: 6f72 2866 2254 6865 207b 7365 6c66 2e63  or(f"The {self.c
-000337e0: 6c73 5f6e 616d 657d 206f 6e6c 7920 7375  ls_name} only su
-000337f0: 7070 6f72 7420 7368 6172 6469 6e67 2070  pport sharding p
-00033800: 726f 7061 6761 7469 6f6e 206f 7220 220a  ropagation or ".
+000336f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00033700: 2020 2020 2020 2020 2020 6869 6464 656e            hidden
+00033710: 5f73 697a 653d 6869 6464 656e 5f73 697a  _size=hidden_siz
+00033720: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             
+00033730: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00033740: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00033750: 2020 2020 2066 666e 5f68 6964 6465 6e5f       ffn_hidden_
+00033760: 7369 7a65 3d66 666e 5f68 6964 6465 6e5f  size=ffn_hidden_
+00033770: 7369 7a65 2c0a 2020 2020 2020 2020 2020  size,.          
+00033780: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00033790: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000337a0: 2020 2020 2020 2020 6e75 6d5f 6865 6164          num_head
+000337b0: 733d 6e75 6d5f 6865 6164 732c 0a20 2020  s=num_heads,.   
+000337c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000337d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000337e0: 2020 2020 2020 2020 2020 2020 2020 2073                 s
+000337f0: 7263 5f73 6571 5f6c 656e 6774 683d 7372  rc_seq_length=sr
+00033800: 635f 7365 715f 6c65 6e67 7468 2c0a 2020  c_seq_length,.  
 00033810: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033820: 2020 2020 2020 2020 2020 2020 2020 2066                 f
-00033830: 2273 656d 692d 6175 746f 2070 6172 616c  "semi-auto paral
-00033840: 6c65 6c20 6d6f 6465 206e 6f77 2e22 290a  lel mode now.").
-00033850: 0a20 2020 2064 6566 2063 6f6e 7374 7275  .    def constru
-00033860: 6374 2873 656c 662c 2065 6e63 6f64 6572  ct(self, encoder
-00033870: 5f69 6e70 7574 732c 0a20 2020 2020 2020  _inputs,.       
-00033880: 2020 2020 2020 2020 2020 2065 6e63 6f64             encod
-00033890: 6572 5f6d 6173 6b73 2c0a 2020 2020 2020  er_masks,.      
-000338a0: 2020 2020 2020 2020 2020 2020 6465 636f              deco
-000338b0: 6465 725f 696e 7075 7473 3d4e 6f6e 652c  der_inputs=None,
-000338c0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-000338d0: 2020 2064 6563 6f64 6572 5f6d 6173 6b73     decoder_masks
-000338e0: 3d4e 6f6e 652c 0a20 2020 2020 2020 2020  =None,.         
-000338f0: 2020 2020 2020 2020 206d 656d 6f72 795f           memory_
-00033900: 6d61 736b 3d4e 6f6e 652c 0a20 2020 2020  mask=None,.     
-00033910: 2020 2020 2020 2020 2020 2020 2069 6e69               ini
-00033920: 745f 7265 7365 743d 5472 7565 2c0a 2020  t_reset=True,.  
+00033820: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00033830: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00033840: 7467 745f 7365 715f 6c65 6e67 7468 3d74  tgt_seq_length=t
+00033850: 6774 5f73 6571 5f6c 656e 6774 682c 0a20  gt_seq_length,. 
+00033860: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00033870: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00033880: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00033890: 2061 7474 656e 7469 6f6e 5f64 726f 706f   attention_dropo
+000338a0: 7574 5f72 6174 653d 6174 7465 6e74 696f  ut_rate=attentio
+000338b0: 6e5f 6472 6f70 6f75 745f 7261 7465 2c0a  n_dropout_rate,.
+000338c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000338d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000338e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000338f0: 2020 6869 6464 656e 5f64 726f 706f 7574    hidden_dropout
+00033900: 5f72 6174 653d 6869 6464 656e 5f64 726f  _rate=hidden_dro
+00033910: 706f 7574 5f72 6174 652c 0a20 2020 2020  pout_rate,.     
+00033920: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00033930: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033940: 6261 7463 685f 7661 6c69 645f 6c65 6e67  batch_valid_leng
-00033950: 7468 3d4e 6f6e 6529 3a0a 2020 2020 2020  th=None):.      
-00033960: 2020 2222 2266 6f72 7761 7264 2070 726f    """forward pro
-00033970: 6365 7373 2222 220a 2020 2020 2020 2020  cess""".        
-00033980: 656e 636f 6465 725f 6f75 7470 7574 203d  encoder_output =
-00033990: 204e 6f6e 650a 2020 2020 2020 2020 6f75   None.        ou
-000339a0: 7470 7574 203d 204e 6f6e 650a 2020 2020  tput = None.    
-000339b0: 2020 2020 656e 636f 6465 725f 6c61 7965      encoder_laye
-000339c0: 725f 7072 6573 656e 7420 3d20 4e6f 6e65  r_present = None
-000339d0: 0a20 2020 2020 2020 2064 6563 6f64 6572  .        decoder
-000339e0: 5f6c 6179 6572 5f70 7265 7365 6e74 203d  _layer_present =
-000339f0: 204e 6f6e 650a 2020 2020 2020 2020 6163   None.        ac
-00033a00: 6375 6d5f 6c6f 7373 203d 2073 656c 662e  cum_loss = self.
-00033a10: 6175 785f 6c6f 7373 0a20 2020 2020 2020  aux_loss.       
-00033a20: 2069 6620 7365 6c66 2e65 6e63 6f64 6572   if self.encoder
-00033a30: 2069 7320 6e6f 7420 4e6f 6e65 3a0a 2020   is not None:.  
-00033a40: 2020 2020 2020 2020 2020 6966 2073 656c            if sel
-00033a50: 662e 7573 655f 6d6f 653a 0a20 2020 2020  f.use_moe:.     
-00033a60: 2020 2020 2020 2020 2020 2065 6e63 6f64             encod
-00033a70: 6572 5f6f 7574 7075 742c 2065 6e63 6f64  er_output, encod
-00033a80: 6572 5f6c 6179 6572 5f70 7265 7365 6e74  er_layer_present
-00033a90: 2c20 656e 636f 6465 725f 6175 785f 6c6f  , encoder_aux_lo
-00033aa0: 7373 203d 2073 656c 662e 656e 636f 6465  ss = self.encode
-00033ab0: 7228 656e 636f 6465 725f 696e 7075 7473  r(encoder_inputs
-00033ac0: 2c20 656e 636f 6465 725f 6d61 736b 732c  , encoder_masks,
-00033ad0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00033940: 2020 2020 2020 2020 2020 2020 2068 6964               hid
+00033950: 6465 6e5f 6163 743d 6869 6464 656e 5f61  den_act=hidden_a
+00033960: 6374 2c0a 2020 2020 2020 2020 2020 2020  ct,.            
+00033970: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00033980: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00033990: 2020 2020 2020 706f 7374 5f6c 6179 6572        post_layer
+000339a0: 6e6f 726d 5f72 6573 6964 7561 6c3d 706f  norm_residual=po
+000339b0: 7374 5f6c 6179 6572 6e6f 726d 5f72 6573  st_layernorm_res
+000339c0: 6964 7561 6c2c 0a20 2020 2020 2020 2020  idual,.         
+000339d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000339e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000339f0: 2020 2020 2020 2020 206c 6179 6572 6e6f           layerno
+00033a00: 726d 5f63 6f6d 7075 7465 5f74 7970 653d  rm_compute_type=
+00033a10: 6c61 7965 726e 6f72 6d5f 636f 6d70 7574  layernorm_comput
+00033a20: 655f 7479 7065 2c0a 2020 2020 2020 2020  e_type,.        
+00033a30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00033a40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00033a50: 2020 2020 2020 2020 2020 736f 6674 6d61            softma
+00033a60: 785f 636f 6d70 7574 655f 7479 7065 3d73  x_compute_type=s
+00033a70: 6f66 746d 6178 5f63 6f6d 7075 7465 5f74  oftmax_compute_t
+00033a80: 7970 652c 0a20 2020 2020 2020 2020 2020  ype,.           
+00033a90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00033aa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00033ab0: 2020 2020 2020 206c 616d 6264 615f 6675         lambda_fu
+00033ac0: 6e63 3d6c 616d 6264 615f 6675 6e63 2c0a  nc=lambda_func,.
+00033ad0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00033ae0: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00033af0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033b00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033b10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033b20: 2020 2020 2020 2020 696e 6974 5f72 6573          init_res
-00033b30: 6574 2c20 6261 7463 685f 7661 6c69 645f  et, batch_valid_
-00033b40: 6c65 6e67 7468 290a 2020 2020 2020 2020  length).        
-00033b50: 2020 2020 2020 2020 6163 6375 6d5f 6c6f          accum_lo
-00033b60: 7373 203d 2073 656c 662e 6164 6428 6163  ss = self.add(ac
-00033b70: 6375 6d5f 6c6f 7373 2c20 656e 636f 6465  cum_loss, encode
-00033b80: 725f 6175 785f 6c6f 7373 290a 2020 2020  r_aux_loss).    
-00033b90: 2020 2020 2020 2020 656c 7365 3a0a 2020          else:.  
-00033ba0: 2020 2020 2020 2020 2020 2020 2020 656e                en
-00033bb0: 636f 6465 725f 6f75 7470 7574 2c20 656e  coder_output, en
-00033bc0: 636f 6465 725f 6c61 7965 725f 7072 6573  coder_layer_pres
-00033bd0: 656e 7420 3d20 7365 6c66 2e65 6e63 6f64  ent = self.encod
-00033be0: 6572 2865 6e63 6f64 6572 5f69 6e70 7574  er(encoder_input
-00033bf0: 732c 2065 6e63 6f64 6572 5f6d 6173 6b73  s, encoder_masks
-00033c00: 2c20 696e 6974 5f72 6573 6574 2c0a 2020  , init_reset,.  
+00033b00: 2020 7573 655f 7061 7374 3d75 7365 5f70    use_past=use_p
+00033b10: 6173 742c 0a20 2020 2020 2020 2020 2020  ast,.           
+00033b20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00033b30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00033b40: 2020 2020 2020 2070 6172 616d 5f69 6e69         param_ini
+00033b50: 745f 7479 7065 3d70 6172 616d 5f69 6e69  t_type=param_ini
+00033b60: 745f 7479 7065 2c0a 2020 2020 2020 2020  t_type,.        
+00033b70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00033b80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00033b90: 2020 2020 2020 2020 2020 6f66 6673 6574            offset
+00033ba0: 3d65 6e63 6f64 6572 5f6c 6179 6572 732c  =encoder_layers,
+00033bb0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00033bc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00033bd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00033be0: 2020 206d 6f65 5f63 6f6e 6669 673d 6d6f     moe_config=mo
+00033bf0: 655f 636f 6e66 6967 2c0a 2020 2020 2020  e_config,.      
+00033c00: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00033c10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033c20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033c30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033c40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033c50: 2020 2062 6174 6368 5f76 616c 6964 5f6c     batch_valid_l
-00033c60: 656e 6774 6829 0a20 2020 2020 2020 2020  ength).         
-00033c70: 2020 206f 7574 7075 7420 3d20 656e 636f     output = enco
-00033c80: 6465 725f 6f75 7470 7574 0a0a 2020 2020  der_output..    
-00033c90: 2020 2020 6966 2073 656c 662e 6465 636f      if self.deco
-00033ca0: 6465 7220 6973 206e 6f74 204e 6f6e 653a  der is not None:
-00033cb0: 0a20 2020 2020 2020 2020 2020 2023 2064  .            # d
-00033cc0: 6563 6f64 6572 206d 6173 6b20 7368 6f75  ecoder mask shou
-00033cd0: 6c64 2062 6520 6372 6561 7465 6420 6f75  ld be created ou
-00033ce0: 7473 6964 6520 7468 6520 6d6f 6465 6c0a  tside the model.
-00033cf0: 2020 2020 2020 2020 2020 2020 6966 2073              if s
-00033d00: 656c 662e 7573 655f 6d6f 653a 0a20 2020  elf.use_moe:.   
-00033d10: 2020 2020 2020 2020 2020 2020 2064 6563               dec
-00033d20: 6f64 6572 5f6f 7574 7075 742c 2064 6563  oder_output, dec
-00033d30: 6f64 6572 5f6c 6179 6572 5f70 7265 7365  oder_layer_prese
-00033d40: 6e74 2c20 6465 636f 6465 725f 6175 785f  nt, decoder_aux_
-00033d50: 6c6f 7373 203d 2073 656c 662e 6465 636f  loss = self.deco
-00033d60: 6465 7228 6465 636f 6465 725f 696e 7075  der(decoder_inpu
-00033d70: 7473 2c20 6465 636f 6465 725f 6d61 736b  ts, decoder_mask
-00033d80: 732c 0a20 2020 2020 2020 2020 2020 2020  s,.             
+00033c20: 2020 2020 2020 2020 2020 2020 7061 7261              para
+00033c30: 6c6c 656c 5f63 6f6e 6669 673d 7061 7261  llel_config=para
+00033c40: 6c6c 656c 5f63 6f6e 6669 6729 0a20 2020  llel_config).   
+00033c50: 2020 2020 2065 6c73 653a 0a20 2020 2020       else:.     
+00033c60: 2020 2020 2020 2072 6169 7365 2052 756e         raise Run
+00033c70: 7469 6d65 4572 726f 7228 6622 5468 6520  timeError(f"The 
+00033c80: 7b73 656c 662e 636c 735f 6e61 6d65 7d20  {self.cls_name} 
+00033c90: 6f6e 6c79 2073 7570 706f 7274 2073 6861  only support sha
+00033ca0: 7264 696e 6720 7072 6f70 6167 6174 696f  rding propagatio
+00033cb0: 6e20 6f72 2022 0a20 2020 2020 2020 2020  n or ".         
+00033cc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00033cd0: 2020 2020 2020 6622 7365 6d69 2d61 7574        f"semi-aut
+00033ce0: 6f20 7061 7261 6c6c 656c 206d 6f64 6520  o parallel mode 
+00033cf0: 6e6f 772e 2229 0a0a 2020 2020 6465 6620  now.")..    def 
+00033d00: 636f 6e73 7472 7563 7428 7365 6c66 2c20  construct(self, 
+00033d10: 656e 636f 6465 725f 696e 7075 7473 2c0a  encoder_inputs,.
+00033d20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00033d30: 2020 656e 636f 6465 725f 6d61 736b 732c    encoder_masks,
+00033d40: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00033d50: 2020 2064 6563 6f64 6572 5f69 6e70 7574     decoder_input
+00033d60: 733d 4e6f 6e65 2c0a 2020 2020 2020 2020  s=None,.        
+00033d70: 2020 2020 2020 2020 2020 6465 636f 6465            decode
+00033d80: 725f 6d61 736b 733d 4e6f 6e65 2c0a 2020  r_masks=None,.  
 00033d90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033da0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033db0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033dc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033dd0: 2020 2020 2020 2020 2020 656e 636f 6465            encode
-00033de0: 725f 6f75 7470 7574 2c20 6d65 6d6f 7279  r_output, memory
-00033df0: 5f6d 6173 6b2c 0a20 2020 2020 2020 2020  _mask,.         
-00033e00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033e10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033e20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033e30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033e40: 2020 2020 2020 2020 2020 2020 2020 696e                in
-00033e50: 6974 5f72 6573 6574 2c20 6261 7463 685f  it_reset, batch_
-00033e60: 7661 6c69 645f 6c65 6e67 7468 290a 2020  valid_length).  
-00033e70: 2020 2020 2020 2020 2020 2020 2020 6163                ac
-00033e80: 6375 6d5f 6c6f 7373 203d 2073 656c 662e  cum_loss = self.
-00033e90: 6164 6428 6163 6375 6d5f 6c6f 7373 2c20  add(accum_loss, 
-00033ea0: 6465 636f 6465 725f 6175 785f 6c6f 7373  decoder_aux_loss
-00033eb0: 290a 2020 2020 2020 2020 2020 2020 656c  ).            el
-00033ec0: 7365 3a0a 2020 2020 2020 2020 2020 2020  se:.            
-00033ed0: 2020 2020 6465 636f 6465 725f 6f75 7470      decoder_outp
-00033ee0: 7574 2c20 6465 636f 6465 725f 6c61 7965  ut, decoder_laye
-00033ef0: 725f 7072 6573 656e 7420 3d20 7365 6c66  r_present = self
-00033f00: 2e64 6563 6f64 6572 2864 6563 6f64 6572  .decoder(decoder
-00033f10: 5f69 6e70 7574 732c 0a20 2020 2020 2020  _inputs,.       
-00033f20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033f30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033f40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033f50: 2020 2020 2020 2020 2020 2020 2020 6465                de
-00033f60: 636f 6465 725f 6d61 736b 732c 0a20 2020  coder_masks,.   
-00033f70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00033da0: 6d65 6d6f 7279 5f6d 6173 6b3d 4e6f 6e65  memory_mask=None
+00033db0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00033dc0: 2020 2020 696e 6974 5f72 6573 6574 3d54      init_reset=T
+00033dd0: 7275 652c 0a20 2020 2020 2020 2020 2020  rue,.           
+00033de0: 2020 2020 2020 2062 6174 6368 5f76 616c         batch_val
+00033df0: 6964 5f6c 656e 6774 683d 4e6f 6e65 293a  id_length=None):
+00033e00: 0a20 2020 2020 2020 2022 2222 7072 6f63  .        """proc
+00033e10: 6573 7320 7072 6f63 6573 7322 2222 0a20  ess process""". 
+00033e20: 2020 2020 2020 2065 6e63 6f64 6572 5f6f         encoder_o
+00033e30: 7574 7075 7420 3d20 4e6f 6e65 0a20 2020  utput = None.   
+00033e40: 2020 2020 206f 7574 7075 7420 3d20 4e6f       output = No
+00033e50: 6e65 0a20 2020 2020 2020 2065 6e63 6f64  ne.        encod
+00033e60: 6572 5f6c 6179 6572 5f70 7265 7365 6e74  er_layer_present
+00033e70: 203d 204e 6f6e 650a 2020 2020 2020 2020   = None.        
+00033e80: 6465 636f 6465 725f 6c61 7965 725f 7072  decoder_layer_pr
+00033e90: 6573 656e 7420 3d20 4e6f 6e65 0a20 2020  esent = None.   
+00033ea0: 2020 2020 2061 6363 756d 5f6c 6f73 7320       accum_loss 
+00033eb0: 3d20 7365 6c66 2e61 7578 5f6c 6f73 730a  = self.aux_loss.
+00033ec0: 2020 2020 2020 2020 6966 2073 656c 662e          if self.
+00033ed0: 656e 636f 6465 7220 6973 206e 6f74 204e  encoder is not N
+00033ee0: 6f6e 653a 0a20 2020 2020 2020 2020 2020  one:.           
+00033ef0: 2069 6620 7365 6c66 2e75 7365 5f6d 6f65   if self.use_moe
+00033f00: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
+00033f10: 2020 656e 636f 6465 725f 6f75 7470 7574    encoder_output
+00033f20: 2c20 656e 636f 6465 725f 6c61 7965 725f  , encoder_layer_
+00033f30: 7072 6573 656e 742c 2065 6e63 6f64 6572  present, encoder
+00033f40: 5f61 7578 5f6c 6f73 7320 3d20 7365 6c66  _aux_loss = self
+00033f50: 2e65 6e63 6f64 6572 2865 6e63 6f64 6572  .encoder(encoder
+00033f60: 5f69 6e70 7574 732c 2065 6e63 6f64 6572  _inputs, encoder
+00033f70: 5f6d 6173 6b73 2c0a 2020 2020 2020 2020  _masks,.        
 00033f80: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00033f90: 2020 2020 2020 2020 2020 2020 2020 2020                  
 00033fa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033fb0: 2020 656e 636f 6465 725f 6f75 7470 7574    encoder_output
-00033fc0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00033fd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033fe0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00033ff0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00034000: 2020 2020 2020 206d 656d 6f72 795f 6d61         memory_ma
-00034010: 736b 2c20 696e 6974 5f72 6573 6574 2c0a  sk, init_reset,.
-00034020: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00034030: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00034040: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00034050: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00034060: 2020 2020 2062 6174 6368 5f76 616c 6964       batch_valid
-00034070: 5f6c 656e 6774 6829 0a20 2020 2020 2020  _length).       
-00034080: 2020 2020 206f 7574 7075 7420 3d20 6465       output = de
-00034090: 636f 6465 725f 6f75 7470 7574 0a20 2020  coder_output.   
-000340a0: 2020 2020 2069 6620 7365 6c66 2e75 7365       if self.use
-000340b0: 5f6d 6f65 3a0a 2020 2020 2020 2020 2020  _moe:.          
-000340c0: 2020 7265 7475 726e 206f 7574 7075 742c    return output,
-000340d0: 2065 6e63 6f64 6572 5f6c 6179 6572 5f70   encoder_layer_p
-000340e0: 7265 7365 6e74 2c20 6465 636f 6465 725f  resent, decoder_
-000340f0: 6c61 7965 725f 7072 6573 656e 742c 2061  layer_present, a
-00034100: 6363 756d 5f6c 6f73 730a 2020 2020 2020  ccum_loss.      
-00034110: 2020 7265 7475 726e 206f 7574 7075 742c    return output,
-00034120: 2065 6e63 6f64 6572 5f6c 6179 6572 5f70   encoder_layer_p
-00034130: 7265 7365 6e74 2c20 6465 636f 6465 725f  resent, decoder_
-00034140: 6c61 7965 725f 7072 6573 656e 740a       layer_present.
+00033fb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00033fc0: 2020 2020 2020 2020 2020 2020 2020 2069                 i
+00033fd0: 6e69 745f 7265 7365 742c 2062 6174 6368  nit_reset, batch
+00033fe0: 5f76 616c 6964 5f6c 656e 6774 6829 0a20  _valid_length). 
+00033ff0: 2020 2020 2020 2020 2020 2020 2020 2061                 a
+00034000: 6363 756d 5f6c 6f73 7320 3d20 7365 6c66  ccum_loss = self
+00034010: 2e61 6464 2861 6363 756d 5f6c 6f73 732c  .add(accum_loss,
+00034020: 2065 6e63 6f64 6572 5f61 7578 5f6c 6f73   encoder_aux_los
+00034030: 7329 0a20 2020 2020 2020 2020 2020 2065  s).            e
+00034040: 6c73 653a 0a20 2020 2020 2020 2020 2020  lse:.           
+00034050: 2020 2020 2065 6e63 6f64 6572 5f6f 7574       encoder_out
+00034060: 7075 742c 2065 6e63 6f64 6572 5f6c 6179  put, encoder_lay
+00034070: 6572 5f70 7265 7365 6e74 203d 2073 656c  er_present = sel
+00034080: 662e 656e 636f 6465 7228 656e 636f 6465  f.encoder(encode
+00034090: 725f 696e 7075 7473 2c20 656e 636f 6465  r_inputs, encode
+000340a0: 725f 6d61 736b 732c 2069 6e69 745f 7265  r_masks, init_re
+000340b0: 7365 742c 0a20 2020 2020 2020 2020 2020  set,.           
+000340c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000340d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000340e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000340f0: 2020 2020 2020 2020 2020 6261 7463 685f            batch_
+00034100: 7661 6c69 645f 6c65 6e67 7468 290a 2020  valid_length).  
+00034110: 2020 2020 2020 2020 2020 6f75 7470 7574            output
+00034120: 203d 2065 6e63 6f64 6572 5f6f 7574 7075   = encoder_outpu
+00034130: 740a 0a20 2020 2020 2020 2069 6620 7365  t..        if se
+00034140: 6c66 2e64 6563 6f64 6572 2069 7320 6e6f  lf.decoder is no
+00034150: 7420 4e6f 6e65 3a0a 2020 2020 2020 2020  t None:.        
+00034160: 2020 2020 2320 6465 636f 6465 7220 6d61      # decoder ma
+00034170: 736b 2073 686f 756c 6420 6265 2063 7265  sk should be cre
+00034180: 6174 6564 206f 7574 7369 6465 206f 6620  ated outside of 
+00034190: 7468 6520 6d6f 6465 6c0a 2020 2020 2020  the model.      
+000341a0: 2020 2020 2020 6966 2073 656c 662e 7573        if self.us
+000341b0: 655f 6d6f 653a 0a20 2020 2020 2020 2020  e_moe:.         
+000341c0: 2020 2020 2020 2064 6563 6f64 6572 5f6f         decoder_o
+000341d0: 7574 7075 742c 2064 6563 6f64 6572 5f6c  utput, decoder_l
+000341e0: 6179 6572 5f70 7265 7365 6e74 2c20 6465  ayer_present, de
+000341f0: 636f 6465 725f 6175 785f 6c6f 7373 203d  coder_aux_loss =
+00034200: 2073 656c 662e 6465 636f 6465 7228 6465   self.decoder(de
+00034210: 636f 6465 725f 696e 7075 7473 2c20 6465  coder_inputs, de
+00034220: 636f 6465 725f 6d61 736b 732c 0a20 2020  coder_masks,.   
+00034230: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00034240: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00034250: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00034260: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00034270: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00034280: 2020 2020 656e 636f 6465 725f 6f75 7470      encoder_outp
+00034290: 7574 2c20 6d65 6d6f 7279 5f6d 6173 6b2c  ut, memory_mask,
+000342a0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+000342b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000342c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000342d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000342e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000342f0: 2020 2020 2020 2020 696e 6974 5f72 6573          init_res
+00034300: 6574 2c20 6261 7463 685f 7661 6c69 645f  et, batch_valid_
+00034310: 6c65 6e67 7468 290a 2020 2020 2020 2020  length).        
+00034320: 2020 2020 2020 2020 6163 6375 6d5f 6c6f          accum_lo
+00034330: 7373 203d 2073 656c 662e 6164 6428 6163  ss = self.add(ac
+00034340: 6375 6d5f 6c6f 7373 2c20 6465 636f 6465  cum_loss, decode
+00034350: 725f 6175 785f 6c6f 7373 290a 2020 2020  r_aux_loss).    
+00034360: 2020 2020 2020 2020 656c 7365 3a0a 2020          else:.  
+00034370: 2020 2020 2020 2020 2020 2020 2020 6465                de
+00034380: 636f 6465 725f 6f75 7470 7574 2c20 6465  coder_output, de
+00034390: 636f 6465 725f 6c61 7965 725f 7072 6573  coder_layer_pres
+000343a0: 656e 7420 3d20 7365 6c66 2e64 6563 6f64  ent = self.decod
+000343b0: 6572 2864 6563 6f64 6572 5f69 6e70 7574  er(decoder_input
+000343c0: 732c 0a20 2020 2020 2020 2020 2020 2020  s,.             
+000343d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000343e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000343f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00034400: 2020 2020 2020 2020 6465 636f 6465 725f          decoder_
+00034410: 6d61 736b 732c 0a20 2020 2020 2020 2020  masks,.         
+00034420: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00034430: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00034440: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00034450: 2020 2020 2020 2020 2020 2020 656e 636f              enco
+00034460: 6465 725f 6f75 7470 7574 2c0a 2020 2020  der_output,.    
+00034470: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00034480: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00034490: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000344a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000344b0: 206d 656d 6f72 795f 6d61 736b 2c20 696e   memory_mask, in
+000344c0: 6974 5f72 6573 6574 2c0a 2020 2020 2020  it_reset,.      
+000344d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000344e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000344f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00034500: 2020 2020 2020 2020 2020 2020 2020 2062                 b
+00034510: 6174 6368 5f76 616c 6964 5f6c 656e 6774  atch_valid_lengt
+00034520: 6829 0a20 2020 2020 2020 2020 2020 206f  h).            o
+00034530: 7574 7075 7420 3d20 6465 636f 6465 725f  utput = decoder_
+00034540: 6f75 7470 7574 0a20 2020 2020 2020 2069  output.        i
+00034550: 6620 7365 6c66 2e75 7365 5f6d 6f65 3a0a  f self.use_moe:.
+00034560: 2020 2020 2020 2020 2020 2020 7265 7475              retu
+00034570: 726e 206f 7574 7075 742c 2065 6e63 6f64  rn output, encod
+00034580: 6572 5f6c 6179 6572 5f70 7265 7365 6e74  er_layer_present
+00034590: 2c20 6465 636f 6465 725f 6c61 7965 725f  , decoder_layer_
+000345a0: 7072 6573 656e 742c 2061 6363 756d 5f6c  present, accum_l
+000345b0: 6f73 730a 2020 2020 2020 2020 7265 7475  oss.        retu
+000345c0: 726e 206f 7574 7075 742c 2065 6e63 6f64  rn output, encod
+000345d0: 6572 5f6c 6179 6572 5f70 7265 7365 6e74  er_layer_present
+000345e0: 2c20 6465 636f 6465 725f 6c61 7965 725f  , decoder_layer_
+000345f0: 7072 6573 656e 740a                      present.
```

## mindformers/pet/pet_model.py

```diff
@@ -20,34 +20,35 @@
 from mindformers.models.modeling_utils import PreTrainedModel
 from mindformers.pet.constants import PetType
 from mindformers.pet.models.lora import LoraModel
 from mindformers.pet.pet_config import LoraConfig, PetConfig
 from mindformers.pet.tuners.pet_adapter import PetAdapter
 from mindformers.tools import logger
 
+
 # Mapping of pet models.
 PET_TYPE_TO_MODEL_MAPPING = {
     PetType.LORA.value: LoraModel,
 }
 
+
 # Mapping of pet configs.
 PET_TYPE_TO_CONFIG_MAPPING = {
     PetType.LORA.value: LoraConfig,
 }
 
 
 class PetModel(PreTrainedModel):
     """
     PetModel define parameter efficient tuning model for LLM model.
 
     Args:
         config(PetConfig): pet config,define parameters efficient tuning algorithm.
         base_model(PreTrainedModel): pretrained model for tuning.
     """
-
     @args_type_check(config=(dict, PetConfig))
     def __init__(self, config: Union[dict, PetConfig], base_model: PreTrainedModel):
         super().__init__(base_model.config, auto_prefix=False)
         if not isinstance(config, PetConfig):
             pet_type = config.pop("pet_type")
             pet_config = PET_TYPE_TO_CONFIG_MAPPING[pet_type](**config)
         else:
@@ -63,26 +64,24 @@
 
     def update_model_kwargs_before_generate(self, input_ids, model_kwargs: dict):
         return self.pet_model.update_model_kwargs_before_generate(input_ids, model_kwargs)
 
     def prepare_inputs_for_generation(self, input_ids, **kwargs):
         return self.pet_model.prepare_inputs_for_generation(input_ids, **kwargs)
 
-    def prepare_inputs_for_predict_layout(self, input_ids, **kwargs):
-        return self.pet_model.prepare_inputs_for_predict_layout(input_ids, **kwargs)
+    def prepare_inputs_for_export(self, full_model=True):
+        return self.pet_model.prepare_inputs_for_export(full_model)
 
     def slice_incremental_inputs(self, model_inputs: dict, current_index):
         return self.pet_model.slice_incremental_inputs(model_inputs, current_index)
 
     def construct(self, input_ids, labels=None, position_ids=None, attention_mask=None, input_position=None,
-                  input_embeds=None, init_reset=True, batch_valid_length=None, batch_index=None,
-                  zactivate_len=None, block_tables=None, slot_mapping=None):
-        return self.pet_model(input_ids, labels, input_position, position_ids, attention_mask, input_embeds,
-                              init_reset, batch_valid_length, batch_index, zactivate_len, block_tables, slot_mapping)
-
+                  input_embeds=None, init_reset=True, batch_valid_length=None):
+        return self.pet_model(input_ids, labels, input_position, position_ids,
+                              attention_mask, input_embeds, init_reset, batch_valid_length)
 
 @args_type_check(config=(dict, PetConfig))
 def get_pet_model(base_model: PreTrainedModel, config: Union[dict, PetConfig]):
     """
     Get model with pet model.
 
     Args:
@@ -96,14 +95,13 @@
     if not PET_TYPE_TO_MODEL_MAPPING.get(pet_type):
         logger.warning("%s doesn't have pet model currently.", pet_type)
         return base_model
 
     # return pet model.
     return PetModel(config=config, base_model=base_model)
 
-
 def is_supported_pet_type(pet_type: str):
     """
     Return `pet_type` is supported or not.
     """
 
     return pet_type in PET_TYPE_TO_MODEL_MAPPING
```

## mindformers/pet/models/lora.py

```diff
@@ -25,15 +25,14 @@
     """
     Lora Model for llm model.
 
     Args:
         config(LoraConfig): pet config,define parameters efficient tuning algorithm.
         base_model(PreTrainedModel): pretrained model for tuning.
     """
-
     def __init__(self, config: LoraConfig, base_model: PreTrainedModel):
         super().__init__(base_model.config, auto_prefix=False)
         self.config.pet_config = config
         self._check_config()
         # add lora layer.
         self.lora_model = self.add_adapter(base_model)
 
@@ -56,28 +55,23 @@
 
     def update_model_kwargs_before_generate(self, input_ids, model_kwargs: dict):
         return self.lora_model.update_model_kwargs_before_generate(input_ids, model_kwargs)
 
     def prepare_inputs_for_generation(self, input_ids, **kwargs):
         return self.lora_model.prepare_inputs_for_generation(input_ids, **kwargs)
 
-    def prepare_inputs_for_predict_layout(self, input_ids, **kwargs):
-        return self.lora_model.prepare_inputs_for_predict_layout(input_ids, **kwargs)
+    def prepare_inputs_for_export(self, full_model=True):
+        return self.lora_model.prepare_inputs_for_export(full_model)
 
     def slice_incremental_inputs(self, model_inputs: dict, current_index):
         return self.lora_model.slice_incremental_inputs(model_inputs, current_index)
 
     def construct(self, input_ids, labels=None, input_position=None, position_ids=None, attention_mask=None,
-                  input_embeds=None, init_reset=True, batch_valid_length=None, batch_index=None,
-                  zactivate_len=None, block_tables=None, slot_mapping=None):
-        return self.lora_model(input_ids=input_ids,
-                               labels=labels,
-                               input_position=input_position,
-                               position_ids=position_ids,
-                               attention_mask=attention_mask,
-                               input_embeds=input_embeds,
-                               init_reset=init_reset,
-                               batch_valid_length=batch_valid_length,
-                               batch_index=batch_index,
-                               zactivate_len=zactivate_len,
-                               block_tables=block_tables,
-                               slot_mapping=slot_mapping)
+                  input_embeds=None, init_reset=True, batch_valid_length=None):
+        return  self.lora_model(input_ids=input_ids,
+                                labels=labels,
+                                input_position=input_position,
+                                position_ids=position_ids,
+                                attention_mask=attention_mask,
+                                input_embeds=input_embeds,
+                                init_reset=init_reset,
+                                batch_valid_length=batch_valid_length)
```

## mindformers/pet/tuners/ptuning2_adapter.py

```diff
@@ -31,13 +31,11 @@
         Add p-tuning v2 prefix for key, vale.
         """
 
         if prefix_key_value is not None:
             prefix_key = prefix_key_value[0]
             prefix_value = prefix_key_value[1]
             cat = P.Concat(seq_len_dim)
-            prefix_key = P.Cast()(prefix_key, key.dtype)
             key = cat([prefix_key, key])
-            prefix_value = P.Cast()(prefix_value, value.dtype)
             value = cat([prefix_value, value])
 
         return key, value
```

## mindformers/pipeline/pipeline.py

```diff
@@ -26,18 +26,19 @@
 
 from mindspore import Model, set_context
 
 from mindformers.models.auto import AutoConfig, AutoModel, AutoTokenizer
 from mindformers.mindformer_book import MindFormerBook
 from mindformers.models import (BaseAudioProcessor, BaseImageProcessor,
                                 PreTrainedModel, PreTrainedTokenizerBase,
-                                build_processor, build_tokenizer, build_network)
+                                build_model, build_processor, build_tokenizer)
 from mindformers.models.auto import TOKENIZER_MAPPING, IMAGE_PROCESSOR_MAPPING
 from mindformers.models.configuration_utils import PretrainedConfig
 from mindformers.models.utils import CONFIG_NAME
+from mindformers.pet import get_pet_model, is_supported_pet_type
 from mindformers.tools import logger
 from mindformers.tools.hub.dynamic_module_utils import \
     get_class_from_dynamic_module
 from mindformers.tools.hub.hub import cached_file, extract_commit_hash
 from mindformers.tools.register import MindFormerConfig
 
 from .build_pipeline import build_pipeline
@@ -139,15 +140,22 @@
     if model is None:
         batch_size = kwargs.get("batch_size", None)
         build_names = ["batch_size", "use_past", "seq_length"]
         build_args = {}
         for build_name in build_names:
             if build_name in kwargs:
                 build_args[build_name] = kwargs.pop(build_name)
-        model = build_network(pipeline_config.model, default_args=build_args)
+        ckpt_cfg = pipeline_config.model.model_config.checkpoint_name_or_path
+        pet_config = pipeline_config.model.model_config.pet_config
+        if pet_config and is_supported_pet_type(pet_config.pet_type):
+            pipeline_config.model.model_config.checkpoint_name_or_path = None
+        model = build_model(pipeline_config.model, default_args=build_args)
+        if pet_config:
+            model.config.checkpoint_name_or_path = ckpt_cfg
+            model = get_pet_model(model, pet_config)
         if batch_size is not None:
             kwargs["batch_size"] = batch_size
     if image_processor is None and hasattr(pipeline_config.processor, 'image_processor'):
         image_processor = build_processor(pipeline_config.processor.image_processor)
     if audio_processor is None and hasattr(pipeline_config.processor, 'audio_processor'):
         audio_processor = build_processor(pipeline_config.processor.audio_processor)
     if tokenizer is None:
```

## mindformers/pipeline/text_generation_pipeline.py

```diff
@@ -125,14 +125,17 @@
                 The text to be classified.
             preprocess_params (dict):
                 The parameter dict for preprocess.
 
         Return:
             Processed text.
         """
+        if self.model_name is not None and self.model_name.startswith("glm32k"):
+            return self.tokenizer.build_chat_input(inputs)
+
         add_special_tokens = preprocess_params.get('add_special_tokens', True)
         if isinstance(inputs, dict):
             keys = preprocess_params.get('keys', None)
             default_src_language_name = 'text'
             feature_name = keys.get('src_language', default_src_language_name) if keys else default_src_language_name
 
             inputs = inputs[feature_name]
@@ -170,15 +173,15 @@
                              f"the batch size of input list {batch_size} should be consistent with "
                              f"model batch size {self._batch_size}. Please check your inputs.")
         if len(inputs) % batch_size != 0:
             raise ValueError(f"When running multi input pipeline, the length of inputs {len(inputs)}"
                              f" should be multiple of batch size {batch_size}. Please check yout inputs.")
         outputs = []
         if batch_size > 1:
-            batch_inputs = [inputs[i:i + batch_size] for i in range(0, len(inputs), batch_size)]
+            batch_inputs = [inputs[i:i+batch_size] for i in range(0, len(inputs), batch_size)]
         else:
             batch_inputs = inputs
         for item in batch_inputs:
             outputs.extend(self.run_single(item, preprocess_params,
                                            forward_params, postprocess_params))
         return outputs
```

## mindformers/tools/check_rules.py

```diff
@@ -47,35 +47,42 @@
             config.model.model_config.use_past = False
             logger.warning("use_past could not be used in train mode, "
                            "it has been forced to False")
         if config.metric:
             _check_keyword_gen_dataset(config, mode, **kwargs)
     elif mode == 'predict':
         _restore_net_type(config)
+        _rule_fa_only_for_train(config, mode)
+        _rule_pp_only_for_train(config, mode)
         _rule_bs_divisible_by_dp(config, **kwargs)
     elif mode == 'eval':
         _restore_net_type(config)
         _rule_fa_only_for_train(config, mode)
         _rule_pp_only_for_train(config, mode)
 
         if config.metric:
             _check_keyword_gen_dataset(config, mode, **kwargs)
 
         _rule_bs_divisible_by_dp(config, **kwargs)
+    elif mode == 'export':
+        _restore_net_type(config)
+        _rule_fa_only_for_train(config, mode)
+        _rule_pp_only_for_train(config, mode)
+        _rule_bs_divisible_by_dp(config, **kwargs)
     else:
-        raise ValueError(f"mode should be in ['train', 'predict', 'eval'], but get {mode}")
+        raise ValueError(f"mode should be in ['train', 'predict', 'eval', 'export'], but get {mode}")
 
 
 def _restore_net_type(config):
     """net data type with different mode for llama2 7b"""
     if config.model.model_config.compute_dtype == 'bfloat16' and \
         config.model.model_config.param_init_type == 'float32':
         config.model.model_config.compute_dtype = 'float16'
         config.model.model_config.param_init_type = 'float16'
-        logger.warning("cast compute_dtype and param_init_type to float16 for predict/eval performance")
+        logger.warning("cast compute_dtype and param_init_type to float16 for predict/eval/export performance")
 
 
 def _rule_bs_divisible_by_dp(config, **kwargs):
     """check bs % dp == 0 when task is text_generation"""
     network = kwargs.get("network", None)
     task = kwargs.get("task", None)
     dataset = kwargs.get("dataset", None)
@@ -103,15 +110,15 @@
                        f"disable use_flash_attention in {mode} mode.")
 
 
 def _rule_pp_only_for_train(config, mode):
     """pp only support training for now"""
     _, _, pp = get_parallel_strategy(config)
     if pp > 1:
-        raise ValueError(f"pipeline stage only support training process for now, set pipeline stage=1 to {mode} ")
+        raise ValueError(f"pp only support training process for now, set pp=1 to {mode} ")
 
 
 def _check_full_batch():
     """check full_batch"""
     parallel_mode = ms.get_auto_parallel_context("parallel_mode")
     full_batch = ms.get_auto_parallel_context("full_batch")
     if parallel_mode not in ["semi_auto_parallel", "auto_parallel"] and full_batch:
@@ -210,22 +217,13 @@
         # when do_eval == True, eval_dataset should be in train mode
         if metric_config['type'] == "PerplexityMetric" and \
             eval_dataset and eval_dataset.data_loader.phase != 'train':
             logger.warning("when using 'PerplexityMetric', eval_dataset.data_loader.phase would be set to 'train'.")
             eval_dataset.data_loader.phase = 'train'
             config.eval_dataset_task.dataset_config.data_loader.phase = eval_dataset.data_loader.phase
 
-def _check_env(config):
-    """check environment"""
-    fine_grain_interleave = config.model.model_config.fine_grain_interleave
-    if fine_grain_interleave and fine_grain_interleave > 1:
-        if os.getenv("ENABLE_CELL_REUSE", "0") == 0:
-            os.environ["ENABLE_CELL_REUSE"] = '1'
-            logger.warning(f"ENABLE_CELL_REUSE must be set in environment when use fine_grain_interleave"
-                           f" (export ENABLE_CELL_REUSE=1)")
 
 def check_rules(config, mode='train', **kwargs):
     """check rules"""
     _check_mode(config, mode, **kwargs)
     _check_full_batch()
     _check_parallel(config)
-    _check_env(config)
```

## mindformers/tools/generic.py

```diff
@@ -47,23 +47,25 @@
     """Raise RuntimeError when detecting exception in decorated function.
 
     :param: func: decorated function
     :return: decorator
     """
 
     def wrapper(func):
-        def inner_wrapper(cls, *args, **kwargs):
+        def inner_wrapper(*args, **kwargs):
             try:
-                return func(cls, *args, **kwargs)
-            except Exception:
+                return func(*args, **kwargs)
+            except Exception as ex:
                 error_msg = f"Error occurred when executing function {func.__name__}."
 
                 if custom_err_msg:
-                    error_msg += f"\n You are using {cls.__name__} in experimental mode, which is not available" \
-                                 f" now. Check the error message: {custom_err_msg}"
+                    error_msg += f"\nCustom error message: {custom_err_msg}"
+
+                if str(ex):
+                    error_msg += f"\nOriginal error message: {ex}"
 
                 raise RuntimeError(error_msg)
 
         return inner_wrapper
 
     return wrapper
```

## mindformers/tools/logger.py

```diff
@@ -174,38 +174,14 @@
             self.latest_log = log_message
             self._emit(record)
 
     def _emit(self, record):
         super().emit(record)
 
 
-class LimitedRepeatFileHandler(logging.handlers.RotatingFileHandler):
-    """Limited Repeat File Handler"""
-    def __init__(self, max_repeats=10, **kwargs):
-        super().__init__(**kwargs)
-        self.max_repeats = max_repeats
-        self.latest_log = ''
-        self.count = 1
-
-    def emit(self, record):
-        """emit"""
-        log_message = record.getMessage()
-        if log_message == self.latest_log:
-            self.count += 1
-            if self.count <= self.max_repeats:
-                self._emit(record)
-        else:
-            self.count = 1
-            self.latest_log = log_message
-            self._emit(record)
-
-    def _emit(self, record):
-        super().emit(record)
-
-
 class StreamRedirector:
     """Stream Re-director for Log."""
 
     def __init__(self, source_stream, target_stream):
         """Redirects the source stream to the target stream.
 
         Args:
@@ -550,17 +526,17 @@
             os.makedirs(base_dir, exist_ok=True)
         file_path.append(path)
 
     max_file_size = max_file_size * 1024 * 1024
 
     file_formatter = _DataFormatter(DEFAULT_FILEHANDLER_FORMAT)
     for i, level in enumerate(file_level):
-        file_handler = LimitedRepeatFileHandler(filename=file_path[i],
-                                                maxBytes=max_file_size,
-                                                backupCount=max_num_of_files)
+        file_handler = logging.handlers.RotatingFileHandler(filename=file_path[i],
+                                                            maxBytes=max_file_size,
+                                                            backupCount=max_num_of_files)
         file_handler.setLevel(level)
         file_handler.setFormatter(file_formatter)
         mf_logger.addHandler(file_handler)
 
     mf_logger.setLevel(_convert_level('INFO'))
 
     mf_logger.propagate = False
```

## mindformers/tools/hub/dynamic_module_utils.py

```diff
@@ -255,14 +255,16 @@
     Passing `token=True` is required when you want to use a private model.
 
     </Tip>
 
     Returns:
         `str`: The path to the module inside the cache.
     """
+    from openmind_hub import try_to_load_from_cache
+
     use_auth_token = deprecated_kwargs.pop("use_auth_token", None)
     if use_auth_token is not None:
         warnings.warn(
             "The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. "
             "Please use `token` instead.",
             FutureWarning,
         )
@@ -277,15 +279,14 @@
     # Download and cache module_file from the repo `pretrained_model_name_or_path` of grab it if it's a local file.
     pretrained_model_name_or_path = str(pretrained_model_name_or_path)
     is_local = os.path.isdir(pretrained_model_name_or_path)
     if is_local:
         submodule = os.path.basename(pretrained_model_name_or_path)
     else:
         submodule = pretrained_model_name_or_path.replace("/", os.path.sep)
-        from openmind_hub import try_to_load_from_cache
         cached_module = try_to_load_from_cache(
             pretrained_model_name_or_path, module_file, cache_dir=cache_dir, revision=_commit_hash
         )
 
     new_files = []
     try:
         # Load from URL or cache if already cached
```

## mindformers/tools/hub/hub.py

 * *Ordering differences only*

```diff
@@ -137,14 +137,17 @@
         Hub
     - returns the list of paths to all the shards, as well as some metadata.
 
     For the description of each arg, see [`PreTrainedModel.from_pretrained`]. `index_filename` is the full
     path to the
     index (downloaded and cached if `pretrained_model_name_or_path` is a model ID on the Hub).
     """
+    from openmind_hub.utils import EntryNotFoundError, OmHubHTTPError
+    from openmind_hub import try_to_load_from_cache
+
     import json
     from tqdm import tqdm
     if not os.path.isfile(index_filename):
         raise ValueError(f"Can't find a checkpoint index ({index_filename}) in {pretrained_model_name_or_path}.")
     with open(index_filename, "r") as f:
         index = json.loads(f.read())
 
@@ -154,17 +157,14 @@
     sharded_metadata["weight_map"] = index["weight_map"].copy()
 
     # First, let's deal with local folder.
     if os.path.isdir(pretrained_model_name_or_path):
         shard_filenames = [os.path.join(pretrained_model_name_or_path, subfolder, f) for f in shard_filenames]
         return shard_filenames, sharded_metadata
 
-    from openmind_hub.utils import EntryNotFoundError, OmHubHTTPError
-    from openmind_hub import try_to_load_from_cache
-
     # At this stage pretrained_model_name_or_path is a model identifier on the Hub
     cached_filenames = []
     # Check if the model is already cached or not. We only try the last checkpoint, this should cover most cases of
     # downloaded (if interrupted).
     last_shard = try_to_load_from_cache(
         pretrained_model_name_or_path, shard_filenames[-1], cache_dir=cache_dir, revision=_commit_hash
     )
@@ -270,14 +270,26 @@
     Examples:
 
     ```python
     # Download a model weight from the Hub and cache it.
     model_weights_file = cached_file("bert-base-uncased", "pytorch_model.bin")
     ```
     """
+    from openmind_hub.utils import (
+        GatedRepoError,
+        RepositoryNotFoundError,
+        RevisionNotFoundError,
+        LocalEntryNotFoundError,
+        EntryNotFoundError,
+        OmHubHTTPError,
+        OMValidationError
+    )
+
+    from openmind_hub import _CACHED_NO_EXIST, om_hub_download, try_to_load_from_cache
+
     if is_offline_mode() and not local_files_only:
         logger.info("Offline mode: forcing local_files_only=True")
         local_files_only = True
     if subfolder is None:
         subfolder = ""
 
     path_or_repo_id = str(path_or_repo_id)
@@ -289,27 +301,14 @@
                 raise EnvironmentError(
                     f"{path_or_repo_id} does not appear to have a file named {full_filename}. Checkout "
                     f"'https://openmind.cn/{path_or_repo_id}/{revision}' for available files."
                 )
             return None
         return resolved_file
 
-    from openmind_hub.utils import (
-        GatedRepoError,
-        RepositoryNotFoundError,
-        RevisionNotFoundError,
-        LocalEntryNotFoundError,
-        EntryNotFoundError,
-        OmHubHTTPError,
-        OMValidationError
-    )
-
-    from openmind_hub import _CACHED_NO_EXIST, om_hub_download, try_to_load_from_cache
-
-
     if cache_dir is None:
         cache_dir = HubConstants.OPENMIND_CACHE
     if isinstance(cache_dir, Path):
         cache_dir = str(cache_dir)
 
     if _commit_hash is not None and not force_download:
         # If the file is cached under that commit hash, we return it directly.
@@ -438,22 +437,23 @@
     return tmp_file
 
 
 def extract_commit_hash(resolved_file: Optional[str], commit_hash: Optional[str]) -> Optional[str]:
     """
     Extracts the commit hash from a resolved filename toward a cache file.
     """
+    from openmind_hub import REGEX_COMMIT_HASH
+
     if resolved_file is None or commit_hash is not None:
         return commit_hash
     resolved_file = str(Path(resolved_file).as_posix())
     search = re.search(r"snapshots/([^/]+)/", resolved_file)
     if search is None:
         return None
     commit_hash = search.groups()[0]
-    from openmind_hub import REGEX_COMMIT_HASH
     return commit_hash if REGEX_COMMIT_HASH.match(commit_hash) else None
 
 
 def get_file_from_repo(
         path_or_repo: Union[str, os.PathLike],
         filename: str,
         cache_dir: Optional[Union[str, os.PathLike]] = None,
```

## mindformers/trainer/base_trainer.py

```diff
@@ -30,30 +30,32 @@
 from mindspore.nn import Optimizer, Cell, PipelineCell, MicroBatchInterleaved
 try:
     # new interface in ms2.1.1
     from mindspore.nn.wrap.cell_wrapper import GradAccumulationCell
     GRAD_ACCUMULATION_VALID = True
 except ImportError:
     GRAD_ACCUMULATION_VALID = False
+from mindspore.common import initializer as init
 
 from mindformers.mindformer_book import MindFormerBook
 from mindformers.core import build_lr, build_optim, build_callback, build_metric
 from mindformers.core.callback.callback import EvalCallBack
 from mindformers.core.parallel_config import build_parallel_config
 from mindformers.dataset import build_dataset, check_dataset_config, \
     check_dataset_iterable, BaseDataset
-from mindformers.models import build_network, build_processor, build_tokenizer, \
+from mindformers.models import build_model, build_processor, build_tokenizer, \
     PreTrainedModel, PreTrainedTokenizerBase, BaseImageProcessor
 from mindformers.pipeline import pipeline
 from mindformers.wrapper import build_wrapper
 from mindformers.tools.register import MindFormerConfig
 from mindformers.tools.logger import logger
-from mindformers.tools.utils import count_params
+from mindformers.tools.utils import count_params, get_output_subpath
 from mindformers.tools.check_rules import check_rules
 from mindformers.models.auto import AutoModel
+from mindformers.pet import get_pet_model, is_supported_pet_type
 from mindformers.tools.utils import get_real_rank, get_real_group_size
 from mindformers.core.callback.callback import ColdHotExpertMointor
 from .config_args import ConfigArguments
 from .training_args import TrainingArguments
 from .utils import check_runner_config, transform_and_load_checkpoint, load_resume_context_from_checkpoint
 from .optimizer_grouped_parameters import get_optimizer_grouped_parameters
 from .utils import set_seed, check_train_data_loader_type, \
@@ -381,15 +383,23 @@
                     self.config.eval_dataset.batch_size)
         eval_dataset = self.create_dataset(is_train=False, default_args=default_args)
         return eval_dataset
 
     def create_network(self, default_args: dict = None):
         """Create the network for task trainer."""
         logger.info(".........Build Network From Config..........")
-        return build_network(self.config.model, default_args=default_args)
+        ckpt_cfg = self.config.model.model_config.checkpoint_name_or_path
+        pet_config = self.config.model.model_config.pet_config
+        if pet_config and is_supported_pet_type(pet_config.pet_type):
+            self.config.model.model_config.checkpoint_name_or_path = None
+        network = build_model(self.config.model, default_args=default_args)
+        if pet_config:
+            network.config.checkpoint_name_or_path = ckpt_cfg
+            network = get_pet_model(network, pet_config)
+        return network
 
     def wrap_network_with_tool_cells(self, network):
         """For training process, warp the network with some tool cells."""
         micro_batch_interleave_num = self.config.micro_batch_interleave_num
         gradient_accumulation_steps = self.config.runner_config.gradient_accumulation_steps
         parallel_mode = ms.context.get_auto_parallel_context("parallel_mode")
         pp = self.get_pipeline_stages()
@@ -832,15 +842,15 @@
         model = Model(network, metrics=compute_metrics, eval_network=network)
 
         if config.load_checkpoint or config.only_save_strategy:
             if config.load_checkpoint in SUPPORT_MODEL_NAMES:
                 config.load_checkpoint = \
                     AutoModel.from_pretrained(config.load_checkpoint).default_checkpoint_download_path
             logger.info(".............Start load checkpoint for eval..................")
-            transform_and_load_checkpoint(config, model, network, next(dataset.create_tuple_iterator()), do_eval=True)
+            transform_and_load_checkpoint(config, model, network, dataset, do_eval=True)
 
         logger.info(".........Starting Evaluate Model..........")
         if get_real_rank() % 8 == 0:
             pprint(config)
         output = model.eval(dataset,
                             callbacks=callbacks,
                             dataset_sink_mode=config.runner_config.sink_mode)
@@ -875,14 +885,15 @@
 
             # build network
             if network is None:
                 network = self.create_network(
                     default_args={"parallel_config": config.parallel_config,
                                   "moe_config": config.moe_config})
             self.set_network(network, is_train=False)
+
             self.count_parameters()
 
             if tokenizer is None and config.processor.tokenizer:
                 tokenizer = build_tokenizer(config.processor.tokenizer, tokenizer_name=config.trainer.model_name)
 
             if image_processor is None and config.processor.image_processor:
                 image_processor = build_processor(config.processor.image_processor)
@@ -897,16 +908,15 @@
                         ['semi_auto_parallel', 'auto_parallel', 'hybrid_parallel']:
                     if network.config:
                         batch_size = network.config.batch_size
                         seq_length = network.config.seq_length
                     else:
                         batch_size = config.model.model_config.batch_size
                         seq_length = config.model.model_config.seq_length
-                    input_ids = np.ones(shape=tuple([batch_size, seq_length]))
-                    infer_data = network.prepare_inputs_for_predict_layout(input_ids)
+                    infer_data = Tensor(shape=(batch_size, seq_length), dtype=ms.int32, init=init.One())
                     transform_and_load_checkpoint(config, model, network, infer_data, do_predict=True)
                 else:
                     transform_and_load_checkpoint(config, model, network, None, do_predict=True)
 
             self.pipeline_task = pipeline(
                 task=task,
                 model=network,
@@ -949,14 +959,68 @@
         file.close()
 
         logger.info("output result is: %s", str(output_info))
         logger.info("output result is saved at: %s", save_file)
         logger.info(".........Predict Over!.............")
         return output_results
 
+    def export_process(self, config: Optional[Union[dict, MindFormerConfig, ConfigArguments, TrainingArguments]] = None,
+                       network: Optional[Union[Cell, PreTrainedModel]] = None,
+                       **kwargs):
+        '''Export for BaseTrainer in MindFormers'''
+        is_full_config = kwargs.get("is_full_config", False)
+        config = self.set_config(config, is_full_config)
+
+        # check rules
+        check_rules(config, mode='export', network=network, task=self.task)
+
+        # build network
+        if network is None:
+            network = self.create_network(
+                default_args={"parallel_config": config.parallel_config,
+                              "moe_config": config.moe_config})
+        self.set_network(network, is_train=False)
+
+        network.model_name = kwargs.get("model_name", None)
+        self.count_parameters()
+        model = Model(network)
+
+        if config.load_checkpoint or config.only_save_strategy:
+            if ms.context.get_auto_parallel_context('parallel_mode') in \
+                    ['semi_auto_parallel', 'auto_parallel', 'hybrid_parallel']:
+                if network.config:
+                    batch_size = network.config.batch_size
+                    seq_length = network.config.seq_length
+                else:
+                    batch_size = config.model.model_config.batch_size
+                    seq_length = config.model.model_config.seq_length
+                infer_data = Tensor(shape=(batch_size, seq_length), dtype=ms.int32, init=init.One())
+                transform_and_load_checkpoint(config, model, network, infer_data, do_predict=True)
+            else:
+                transform_and_load_checkpoint(config, model, network, None, do_predict=True)
+
+        rank_id = get_real_rank()
+
+        # 
+        logger.info("Start export full model...")
+        network.add_flags_recursive(is_first_iteration=True)
+        full_inputs = network.prepare_inputs_for_export(full_model=True)
+        save_path = get_output_subpath("mindir_full_checkpoint", rank_id)
+        ms.export(network, *full_inputs, file_name=save_path, file_format='MINDIR')
+
+        if network.config.use_past:
+            # 
+            logger.info("Start export inc model...")
+            network.add_flags_recursive(is_first_iteration=False)
+            inc_inputs = network.prepare_inputs_for_export(full_model=False)
+            save_path = get_output_subpath("mindir_inc_checkpoint", rank_id)
+            ms.export(network, *inc_inputs, file_name=save_path, file_format='MINDIR')
+
+        logger.info(".........Export Over!.............")
+
     def _evaluate_in_training(self, model: Model, eval_dataset: BaseDataset):
         origin_phase = model.eval_network.phase
         model.eval_network.set_train(False)
         output = model.eval(
             eval_dataset, dataset_sink_mode=self.config.runner_config.sink_mode
         )
         model.eval_network.set_train(origin_phase)
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## mindformers/trainer/trainer.py

```diff
@@ -201,15 +201,16 @@
                  optimizers: Optional[Optimizer] = None,
                  compute_metrics: Optional[Union[dict, set]] = None,
                  callbacks: Optional[Union[Callback, List[Callback]]] = None,
                  eval_callbacks: Optional[Union[Callback, List[Callback]]] = None,
                  pet_method: Optional[str] = '',
                  image_processor: Optional[BaseImageProcessor] = None,
                  audio_processor: Optional[BaseAudioProcessor] = None,
-                 save_config: bool = False):
+                 save_config: bool = False,
+                 reset_model: bool = False):
         self.args = args
         self.task = task
         self.model = model
         self.model_name = model_name
         self.tokenizer = tokenizer
         self.train_dataset = train_dataset
         self.eval_dataset = eval_dataset
@@ -217,16 +218,18 @@
         self.optimizers = optimizers
         self.compute_metrics = compute_metrics
         self.callbacks = callbacks
         self.eval_callbacks = eval_callbacks
         self.pet_method = pet_method
         self.image_processor = image_processor
         self.audio_processor = audio_processor
+        self.reset_model = reset_model
         self.default_checkpoint_name_or_path = None
         self.configs_directory = os.path.join('.', DEFAULT_CONFIG_DIR)
+        self.reset_use_past_to_false = False
 
         # check_task_and_model
         if self.task not in SUPPORT_TASKS.keys():
             raise ValueError(
                 "The value of task must be in {}, but get {}".format(SUPPORT_TASKS.keys(), self.task))
 
         if isinstance(self.model, (Cell, PreTrainedModel)):
@@ -257,22 +260,14 @@
                     "it is recommended to select a model configuration that corresponds "
                     "to the support of MindFormers based on the instance model and set model_name.")
                 logger.warning(
                     "Otherwise, they will default to a general configuration."
                     "You are advised to pass instances such as optimizers, metric, tokenizer, and processor")
             self.model_name = 'common'
 
-        if (isinstance(self.args, (str, MindFormerConfig)) or \
-            (isinstance(self.args, TrainingArguments) and self.is_model_instance)) and \
-            self.task == 'general' and self.model_name != 'common':
-            logger.warning("When (`args` is MindformerConfig) or \
-                (`args` is TrainingArguments and a model instance is passed), \
-                    The `model_name` is invalid and set to 'common'.")
-            self.model_name = 'common'
-
         self._check_args_task_and_model()
 
         # config init
         task_config = self.get_task_config(self.task, self.model_name)
 
         self.config = self._config_init(args, task_config)
 
@@ -694,14 +689,62 @@
             network=self.model, image_processor=self.image_processor,
             audio_processor=self.audio_processor,
             tokenizer=self.tokenizer,
             is_full_config=True,
             **kwargs)
         return output_result
 
+    @args_type_check(predict_checkpoint=(str, bool), auto_trans_ckpt=bool)
+    def export(self,
+               predict_checkpoint: Optional[Union[str, bool]] = None,
+               auto_trans_ckpt: Optional[bool] = None,
+               src_strategy: Optional[str] = None):
+        """
+        The export API of Trainer. After setting custom settings, implement export by calling the
+        export method of task-trainer instance.
+
+        Args:
+            predict_checkpoint (Optional[Union[str, bool]]):
+                Used to predict the weight of the network.
+                It supports real checkpoint path or valid model name of mindformers or bool value.
+                if it's true, the last checkpoint file saved from the previous training round is automatically used.
+                Default: False.
+            auto_trans_ckpt:
+                auto transform checkpoint to load in distributed model
+            src_strategy (Optionalp[str]):
+                The strategy of `resume_from_checkpoint`. Effective only when auto_trans_ckpt is set to True,
+                used for automatic checkpoint transform.
+
+        Return:
+            None
+
+        Raises:
+            TypeError: if predict_checkpoint is not bool or str type.
+        """
+        if predict_checkpoint is not None and not isinstance(predict_checkpoint, (bool, str)):
+            raise TypeError(f"predict_checkpoint must be one of [None, string, bool], "
+                            f"but get {predict_checkpoint}")
+
+        if predict_checkpoint:
+            self.config.load_checkpoint = predict_checkpoint
+        if self.config.load_checkpoint is not None:
+            if self.config.load_checkpoint is True:
+                self.config.load_checkpoint = self.get_last_checkpoint()
+
+        if auto_trans_ckpt is not None:
+            self.config.auto_trans_ckpt = auto_trans_ckpt
+        if src_strategy is not None:
+            self.config.src_strategy_path_or_dir = src_strategy
+
+        self._check_config_rules()
+
+        self.trainer.export(config=self.config,
+                            network=self.model,
+                            is_full_config=True)
+
     def add_callback(self, callback):
         """add callback."""
         cb = callback() if isinstance(callback, type) else callback
         cb_class = callback if isinstance(callback, type) else callback.__class__
         if cb_class in [c.__class__ for c in self.callbacks]:
             logger.warning(
                 f"You are adding a {cb_class.__name__} to the callbacks of this Trainer, but there is already one.\n"
@@ -873,15 +916,15 @@
         self.config.moe_config.expert_group_size = expert_group_size
         self.config.moe_config.group_wise_a2a = group_wise_a2a
         self.config.moe_config.comp_comm_parallel = comp_comm_parallel
         self.config.moe_config.comp_comm_parallel_degree = comp_comm_parallel_degree
 
         self.is_set_moe_config = True
 
-    def _reset_model_instance(self, is_train=False):
+    def _reset_model_instance(self):
         """Reset model instance for new model config."""
         if self.is_set_parallel_config:
             logger.info("The incoming model will be configured in parallel.")
 
         if self.is_set_recompute_config:
             logger.info("The incoming model will be configured in recompute.")
 
@@ -889,27 +932,35 @@
             logger.info("The incoming model will be configured in moe.")
 
         if not isinstance(self.model, PreTrainedModel):
             raise NotImplementedError("Currently only the integrated model structure in MindFormers is supported.")
 
         build_parallel_config(self.config)
         model_config = self.model.config
-        if True in [self.is_set_parallel_config, self.is_set_moe_config, self.is_set_recompute_config] or \
-            (is_train and hasattr(model_config, 'use_past') and model_config.use_past):
+        if True in [self.is_set_parallel_config, self.is_set_moe_config, self.is_set_recompute_config]:
             logger.info("..........Reinit Model..........")
-            if is_train and hasattr(model_config, 'use_past') and model_config.use_past:
-                model_config.use_past = False
-                logger.warning("The `use_past` is set to False.")
             model_config.parallel_config = self.config.parallel_config
             model_config.moe_config = self.config.moe_config
             self.model.__init__(model_config)
             self.is_set_parallel_config = False
             self.is_set_recompute_config = False
             self.is_set_moe_config = False
 
+    def _reset_use_past(self, is_train=False):
+        """Reset use_past to false when training."""
+        model_config = self.model.config
+        if hasattr(model_config, 'use_past'):
+            if is_train and model_config.use_past:
+                self.reset_use_past_to_false = True
+                self.model.add_flags_recursive(use_past=False)
+                logger.info("The `use_past` is set to `false` before training.")
+            elif not is_train and self.reset_use_past_to_false:
+                self.model.add_flags_recursive(use_past=True)
+                logger.info("The `use_past` is restored to `true`.")
+
     @staticmethod
     def get_task_config(task, model_name):
         """"get task config based on task and model_name."""
         default_config_path = SUPPORT_TASKS.get(task).get(model_name)
         relative_config_path = default_config_path[default_config_path.rfind("configs/"):]
         current_config_path = os.path.join(os.getcwd(), relative_config_path)
         if os.path.exists(current_config_path):
@@ -1006,15 +1057,17 @@
         if isinstance(self.model, (Cell, PreTrainedModel)):
             self.is_model_instance = True
         else:
             assert self.config.model is not None, \
                 "When `model` is not instance, `self.config.model` must not be None."
 
         if self.is_model_instance:
-            self._reset_model_instance(is_train)
+            self._reset_use_past(is_train)
+            if self.reset_model:
+                self._reset_model_instance()
 
     def _init_tokenizer(self):
         """init tokenizer"""
         if self.tokenizer is not None:
             logger.info("..........Init Tokenizer..........")
             if self.config.train_dataset is not None:
                 self.config.train_dataset.tokenizer = self.tokenizer
@@ -1216,52 +1269,44 @@
         if self.config.load_checkpoint and self.config.model \
             and self.config.model.model_config.checkpoint_name_or_path:
             self.config.model.model_config.checkpoint_name_or_path = None
             logger.info("The `load_checkpoint` is set, the `checkpoint_name_or_path` will be set to None.")
 
     def _check_args_task_and_model(self):
         """Check args, task and model."""
-        # get support model names of task
-        model_name_support_list = list(MindFormerBook().get_model_name_support_list_for_task(self.task))
-        model_name_support_list.sort()
-        # if task is not general, model_name should be supported by task
-        if self.task != 'general' and self.model_name not in model_name_support_list:
-            raise ValueError(f"The `model_name`={self.model_name} is not support in task: {self.task},\n"
-                             f"Support model name of {self.task}: {model_name_support_list}.")
-
         if isinstance(self.args, (str, MindFormerConfig)) or \
             (isinstance(self.args, TrainingArguments) and self.is_model_instance):
+            if self.task == 'general' and self.model_name != 'common':
+                logger.warning("\n===================================================================\n"
+                               "The `model_name` is invalid.\n"
+                               "===================================================================\n")
+                self.model_name = 'common'
             return
 
         if self.task == 'general':
             if self.model_name != 'common':
-                # task is not defined but model name is defined, raise error
-                task_name_support_list = []
-                for task in list(SUPPORT_TASKS.keys()):
-                    if self.model_name in list(MindFormerBook().get_model_name_support_list_for_task(task)):
-                        task_name_support_list.append(task)
-                task_name_support_list.sort()
-                raise ValueError(f"The `task` is needed, \
-                    please select an appropriate task from {task_name_support_list}.")
+                task_list = list(SUPPORT_TASKS.keys())
+                task_list.sort()
+                raise ValueError(f"The `task` is needed, please select an appropriate task from {task_list}.")
             if self.args is None:
                 if self.is_model_instance:
-                    # only model instance is defined, need train args.
                     raise ValueError("The `args` is needed, it could be an instance of "
                                      "`TrainingArguments` or `MindFormerConfig` or `yaml path`.")
                 raise ValueError("Neither `task`, `model`, `model_name`, nor `args` are configured.\n")
             if isinstance(self.args, TrainingArguments) and not self.is_model_instance:
-                # only train args is defined, need model instance.
                 raise ValueError("A model instance is needed, which is passed through the `model` parameter.")
-        elif self.model_name == 'common':
+        if self.task != 'general' and self.model_name == 'common':
+            model_name_support_list = list(MindFormerBook().get_model_name_support_list_for_task(self.task))
+            if 'common' in model_name_support_list:
+                model_name_support_list.remove('common')
+            model_name_support_list.sort()
             if not self.is_model_instance:
-                # only task is defined, need model_name by raise error.
                 raise ValueError("A model name is needed, which is passed through the `model` or `model_name`.\n"
                                  f"Support model name of {self.task}: {model_name_support_list}.")
             if self.args is None:
-                # only task and model instance is defined, need model_name by warning.
                 logger.warning("\n===================================================================\n"
                                "Note that the `model_name` is not passed and it is defined as 'common'. "
                                "You'd better choose a suitable model name, otherwise you may end up using "
                                "an inappropriate YAML file as the task configuration.\n"
                                f"Support model name of {self.task}: {model_name_support_list}.\n"
                                "===================================================================\n")
                 return
```

## mindformers/trainer/training_args.py

```diff
@@ -21,14 +21,15 @@
 import math
 import json
 from enum import Enum
 from collections import OrderedDict
 from typing import Optional, Union, List
 from dataclasses import asdict, dataclass, field, fields
 
+from mindformers.modules.transformer import TransformerRecomputeConfig, TransformerOpParallelConfig, MoEConfig
 from mindformers.tools.register import MindFormerConfig
 from mindformers.tools import logger
 from mindformers.tools.utils import get_real_rank, get_real_group_size
 from .utils import (
     LrSchedulerType,
     OptimizerType,
     IntervalStrategy,
@@ -607,22 +608,14 @@
         default=True,
         metadata={"help": "Whether enable gradient clipping. Default: False."}
     )
     max_grad_norm: float = field(
         default=1.0,
         metadata={"help": "Max gradient norm."}
     )
-    max_scale_window: int = field(
-        default=1000,
-        metadata={"help": "Maximum scale_window of the automatic scale window list. The default value is 1000."}
-    )
-    min_scale_window: int = field(
-        default=20,
-        metadata={"help": "Minimum scale_window of the automatic scale window list. The default value is 20."}
-    )
 
     # metric config
     metric_type: Optional[Union[List[str], str]] = field(
         default=None,
         metadata={"help": "Whether enable gradient clipping. Default: False."}
     )
 
@@ -638,22 +631,14 @@
             "help": (
                 "Log every X updates steps. Should be an integer or a float in range `[0,1)`. "
                 "If smaller than 1, will be interpreted as ratio of total training steps."
             )
         },
     )
     # checkpoint
-    save_prefix: str = field(
-        default='CKP',
-        metadata={"help": "The prefix name of checkpoint files. Default: 'CKP'."}
-    )
-    save_directory: Optional[str] = field(
-        default=None,
-        metadata={"help": "The path of the folder which will be saved in the checkpoint file. Default: None."}
-    )
     save_strategy: Union[SaveIntervalStrategy, str] = field(
         default='steps',
         metadata={"help": "The checkpoint save strategy to use. Default: 'steps'."},
     )
     save_steps: float = field(
         default=500,
         metadata={
@@ -677,23 +662,14 @@
                 " for `save_total_limit=5` and `load_best_model_at_end=True`, the four last checkpoints will always be"
                 " retained alongside the best model. When `save_total_limit=1` and `load_best_model_at_end=True`,"
                 " it is possible that two checkpoints are saved: the last one and the best one (if they are different)."
                 " Default is unlimited checkpoints. Default: 5."
             )
         },
     )
-    keep_checkpoint_per_n_minutes: int = field(
-        default=0,
-        metadata={
-            "help": (
-                "Save the checkpoint file every `keep_checkpoint_per_n_minutes` minutes."
-                "Can't be used with keep_checkpoint_max at the same time. Default: 0."
-            )
-        },
-    )
     save_on_each_node: bool = field(
         default=True,
         metadata={
             "help": (
                 "When doing multi-node distributed training, whether to save models and checkpoints on each node, or"
                 " only on the main one. Default: True."
             )
@@ -704,26 +680,14 @@
             "help": (
                 "Whether to merge and save the split Tensor in the automatic parallel scenario. "
                 "Integrated save function is only supported in automatic parallel scene, not supported"
                 "in manual parallel. If set, `save_on_each_node` will become invalid. Default: None."
             )
         }
     )
-    save_network_params: bool = field(
-        default=True,
-        metadata={"help": "Whether to only save network weights additionally. Default: True."}
-    )
-    save_trainable_params: bool = field(
-        default=False,
-        metadata={"help": "Whether to save fine-tuned weights additionally. Default: False."}
-    )
-    async_save: bool = field(
-        default=False,
-        metadata={"help": "Whether asynchronous execution saves the checkpoint to a file. Default: False."}
-    )
     # evaluate
     evaluation_strategy: Union[IntervalStrategy, str] = field(
         default="no",
         metadata={"help": "The evaluation strategy to use. Default: 'no'."},
     )
     eval_steps: Optional[float] = field(
         default=None,
@@ -968,14 +932,50 @@
         Get number of steps used for a linear warmup.
         """
         warmup_steps = (
             self.warmup_steps if self.warmup_steps > 0 else math.ceil(num_training_steps * self.warmup_ratio)
         )
         return warmup_steps
 
+    def get_recompute_config(self):
+        """get recompute config"""
+        recompute_config = TransformerRecomputeConfig(
+            recompute=self.recompute,
+            select_recompute=self.select_recompute,
+            parallel_optimizer_comm_recompute=self.parallel_optimizer_comm_recompute,
+            mp_comm_recompute=self.mp_comm_recompute,
+            recompute_slice_activation=self.recompute_slice_activation
+        )
+        return recompute_config
+
+    def get_parallel_config(self):
+        """get parallel config"""
+        parallel_config = TransformerOpParallelConfig(
+            data_parallel=self.data_parallel,
+            model_parallel=self.model_parallel,
+            expert_parallel=self.expert_parallel,
+            pipeline_stage=self.pipeline_stage,
+            micro_batch_num=self.micro_batch_num,
+            recompute=self.get_recompute_config(),
+            use_seq_parallel=self.use_seq_parallel,
+            gradient_aggregation_group=self.gradient_aggregation_group,
+            vocab_emb_dp=self.vocab_emb_dp,
+        )
+        return parallel_config
+
+    def get_moe_config(self):
+        """get moe config"""
+        moe_config = MoEConfig(
+            expert_num=self.expert_num,
+            capacity_factor=self.capacity_factor,
+            aux_loss_factor=self.aux_loss_factor,
+            num_experts_chosen=self.num_experts_chosen
+        )
+        return moe_config
+
     def to_dict(self):
         """
         Serializes this instance while replace `Enum` by their values (for JSON serialization support). It obfuscates
         the token values by removing their value.
         """
         # filter out fields that are defined as field(init=False)
         d = {field.name: getattr(self, field.name) for field in fields(self) if field.init}
@@ -1000,14 +1000,15 @@
             self,
             learning_rate: float = 5e-5,
             batch_size: int = 8,
             weight_decay: float = 0,
             num_epochs: float = 3,
             gradient_accumulation_steps: int = 1,
             seed: int = 42,
+            **kwargs
     ):
         """
         A method that regroups all basic arguments linked to the training.
 
         <Tip>
 
         Calling this method will automatically set `self.do_train` to `True`.
@@ -1055,21 +1056,23 @@
         self.do_train = True
         self.learning_rate = learning_rate
         self.per_device_train_batch_size = batch_size
         self.weight_decay = weight_decay
         self.num_train_epochs = num_epochs
         self.gradient_accumulation_steps = gradient_accumulation_steps
         self.seed = seed
+        self.print_kwargs_unused(**kwargs)
         return self
 
     def set_evaluate(
             self,
             strategy: Union[str, IntervalStrategy] = "no",
             steps: int = 500,
-            batch_size: int = 8
+            batch_size: int = 8,
+            **kwargs
     ):
         """
         A method that regroups all arguments linked to the evaluation.
 
         Args:
             strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `"no"`):
                 The evaluation strategy to adopt during training. Possible values are:
@@ -1097,20 +1100,22 @@
         """
         self.evaluation_strategy = IntervalStrategy(strategy).value
         if self.evaluation_strategy == IntervalStrategy.STEPS and steps == 0:
             raise ValueError("Setting `strategy` as 'steps' requires a positive value for `steps`.")
         self.do_eval = self.evaluation_strategy != IntervalStrategy.NO
         self.eval_steps = steps
         self.per_device_eval_batch_size = batch_size
+        self.print_kwargs_unused(**kwargs)
         return self
 
     def set_testing(
             self,
             batch_size: int = 8,
             loss_only: bool = False,
+            **kwargs
     ):
         """
         A method that regroups all basic arguments linked to testing on a held-out dataset.
 
         <Tip>
 
         Calling this method will automatically set `self.do_predict` to `True`.
@@ -1133,22 +1138,24 @@
         >>> args.per_device_eval_batch_size
         32
         ```
         """
         self.do_predict = True
         self.per_device_eval_batch_size = batch_size
         self.prediction_loss_only = loss_only
+        self.print_kwargs_unused(**kwargs)
         return self
 
     def set_save(
             self,
             strategy: Union[str, IntervalStrategy] = "steps",
             steps: int = 500,
             total_limit: Optional[int] = None,
             on_each_node: bool = True,
+            **kwargs
     ):
         """
         A method that regroups all arguments linked to the evaluation.
 
         Args:
             strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `"steps"`):
                 The checkpoint save strategy to adopt during training. Possible values are:
@@ -1182,20 +1189,22 @@
         """
         self.save_strategy = IntervalStrategy(strategy).value
         if self.save_strategy == IntervalStrategy.STEPS and steps == 0:
             raise ValueError("Setting `strategy` as 'steps' requires a positive value for `steps`.")
         self.save_steps = steps
         self.save_total_limit = total_limit
         self.save_on_each_node = on_each_node
+        self.print_kwargs_unused(**kwargs)
         return self
 
     def set_logging(
             self,
             strategy: Union[str, IntervalStrategy] = "steps",
             steps: int = 500,
+            **kwargs
     ):
         """
         A method that regroups all arguments linked to the evaluation.
 
         Args:
             strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `"steps"`):
                 The logging strategy to adopt during training. Possible values are:
@@ -1217,23 +1226,25 @@
         100
         ```
         """
         self.logging_strategy = IntervalStrategy(strategy).value
         if self.logging_strategy == IntervalStrategy.STEPS and steps == 0:
             raise ValueError("Setting `strategy` as 'steps' requires a positive value for `steps`.")
         self.logging_steps = steps
+        self.print_kwargs_unused(**kwargs)
         return self
 
     def set_push_to_hub(
             self,
             model_id: str,
             strategy: Union[str, HubStrategy] = "every_save",
             token: Optional[str] = None,
             private_repo: bool = False,
             always_push: bool = False,
+            **kwargs
     ):
         """
         A method that regroups all arguments linked to synchronizing checkpoints with the Hub.
 
         <Tip>
 
         Calling this method will set `self.push_to_hub` to `True`, which means the `output_dir` will begin a git
@@ -1286,25 +1297,27 @@
         """
         self.push_to_hub = True
         self.hub_model_id = model_id
         self.hub_strategy = HubStrategy(strategy)
         self.hub_token = token
         self.hub_private_repo = private_repo
         self.hub_always_push = always_push
+        self.print_kwargs_unused(**kwargs)
         return self
 
     def set_optimizer(
             self,
             name: Union[str, OptimizerType] = "adamw",
             learning_rate: float = 5e-5,
             lr_end: float = 1e-6,
             weight_decay: float = 0,
             beta1: float = 0.9,
             beta2: float = 0.999,
             epsilon: float = 1e-8,
+            **kwargs
     ):
         """
         A method that regroups all arguments linked to the optimizer and its hyperparameters.
 
         Args:
             name (`str` or [`training_args.OptimizerNames`], *optional*, defaults to `"adamw"`):
                 The optimizer to use: `"AdamWeightDecay"`, `"adamw"`, `"adam"`, `"sgd"`,
@@ -1336,24 +1349,26 @@
         self.optim = OptimizerType(name).value
         self.learning_rate = learning_rate
         self.lr_end = lr_end
         self.weight_decay = weight_decay
         self.adam_beta1 = beta1
         self.adam_beta2 = beta2
         self.adam_epsilon = epsilon
+        self.print_kwargs_unused(**kwargs)
         return self
 
     def set_lr_scheduler(
             self,
             name: Union[str, LrSchedulerType] = "linear",
             num_epochs: float = 3.0,
             warmup_lr_init: float = 0.0,
             warmup_epochs: Optional[int] = None,
             warmup_ratio: Optional[float] = None,
             warmup_steps: int = 0,
+            **kwargs
     ):
         """
         A method that regroups all arguments linked to the learning rate scheduler and its hyperparameters.
 
         Args:
             name (`str` or [`LrSchedulerType`], *optional*, defaults to `"linear"`):
                 The scheduler type to use. See the documentation of [`LrSchedulerType`] for all possible values.
@@ -1381,24 +1396,26 @@
         """
         self.lr_scheduler_type = LrSchedulerType(name).value
         self.num_train_epochs = num_epochs
         self.warmup_lr_init = warmup_lr_init
         self.warmup_epochs = warmup_epochs
         self.warmup_ratio = warmup_ratio
         self.warmup_steps = warmup_steps
+        self.print_kwargs_unused(**kwargs)
         return self
 
     def set_dataloader(
             self,
             train_batch_size: int = 8,
             eval_batch_size: int = 8,
             drop_last: bool = False,
             num_workers: int = 0,
             ignore_data_skip: bool = False,
             sampler_seed: Optional[int] = None,
+            **kwargs
     ):
         """
         A method that regroups all arguments linked to the dataloaders creation.
 
         Args:
             train_batch_size (`int`, defaults to 8):
                 Batch size per GPU/NPU core/CPU for training.
@@ -1433,16 +1450,23 @@
         """
         self.per_device_train_batch_size = train_batch_size
         self.per_device_eval_batch_size = eval_batch_size
         self.dataloader_drop_last = drop_last
         self.dataloader_num_workers = num_workers
         self.ignore_data_skip = ignore_data_skip
         self.data_seed = sampler_seed
+        self.print_kwargs_unused(**kwargs)
         return self
 
+    @staticmethod
+    def print_kwargs_unused(**kwargs):
+        if kwargs:
+            for k in kwargs:
+                logger.warning("the `%s` parameter is not supported.", k)
+
     def convert_args_to_mindformers_config(self, task_config: MindFormerConfig = None):
         """convert training arguments to mindformer config type for adapting hugging-face."""
         if task_config is None:
             task_config = MindFormerConfig()
 
         self._adapt_common_config(task_config)
         self._adapt_runner_config(task_config)
@@ -1453,15 +1477,15 @@
         self._adapt_optimizer_config(task_config)
         self._adapt_lr_schedule_config(task_config)
         self._adapt_dataset_config(task_config)
         self._adapt_wrapper_config(task_config)
         self._adapt_metric_config(task_config)
         self._adapt_callback_config(task_config)
         self._adapt_eval_config(task_config)
-        self._adapt_profile_config(task_config)
+        self._adapt_profile_configl(task_config)
         self._adapt_auto_tune_config(task_config)
         self._adapt_hub_config(task_config)
 
         return task_config
 
     def _adapt_common_config(self, task_config):
         """adapt common config."""
@@ -1470,15 +1494,14 @@
             task_config.overwrite_output_dir, self.overwrite_output_dir)
         task_config.seed = _check_training_args(task_config.seed, self.seed)
         task_config.data_seed = _check_training_args(task_config.data_seed, self.data_seed)
         task_config.only_save_strategy = _check_training_args(task_config.only_save_strategy, self.only_save_strategy)
         task_config.auto_trans_ckpt = _check_training_args(task_config.auto_trans_ckpt, self.auto_trans_ckpt)
         task_config.src_strategy_path_or_dir = _check_training_args(task_config.src_strategy, self.src_strategy)
         task_config.load_checkpoint = _check_training_args(task_config.load_checkpoint, self.resume_from_checkpoint)
-        task_config.resume_training = _check_training_args(task_config.resume_training, self.resume_training)
         task_config.ignore_data_skip = _check_training_args(task_config.ignore_data_skip, self.ignore_data_skip)
         task_config.do_train = _check_training_args(task_config.do_train, self.do_train)
         task_config.do_eval = _check_training_args(task_config.do_eval, self.do_eval)
         task_config.do_predict = _check_training_args(task_config.do_predict, self.do_predict)
         task_config.remote_save_url = _check_training_args(task_config.remote_save_url, self.remote_save_url)
 
     def _adapt_runner_config(self, task_config):
@@ -1741,19 +1764,14 @@
                 task_config.runner_wrapper.scale_sense.type, self.scale_sense)
             task_config.runner_wrapper.scale_sense.loss_scale_value = _check_training_args(
                 task_config.runner_wrapper.scale_sense.loss_scale_value, self.loss_scale_value)
             task_config.runner_wrapper.scale_sense.scale_factor = _check_training_args(
                 task_config.runner_wrapper.scale_sense.scale_factor, self.loss_scale_factor)
             task_config.runner_wrapper.scale_sense.scale_window = _check_training_args(
                 task_config.runner_wrapper.scale_sense.scale_window, self.loss_scale_window)
-            if self.scale_sense == "AdaptiveLossScaleUpdateCell":
-                task_config.runner_wrapper.scale_sense.max_scale_window = _check_training_args(
-                    task_config.runner_wrapper.scale_sense.max_scale_window, self.max_scale_window)
-                task_config.runner_wrapper.scale_sense.min_scale_window = _check_training_args(
-                    task_config.runner_wrapper.scale_sense.min_scale_window, self.min_scale_window)
         elif isinstance(self.scale_sense, (float, int)):
             task_config.runner_wrapper.scale_sense = int(self.scale_sense)
 
     def _adapt_metric_config(self, task_config):
         """adapt metric config."""
         if not _check_task_config(task_config.metric):
             task_config.metric = []
@@ -1793,42 +1811,36 @@
             """adapt logging callback"""
             if self.logging_strategy == LoggingIntervalStrategy.STEPS:
                 _adapt_dict_args(callback, 'per_print_times', self.logging_steps)
             return callback
 
         def _adapt_save_checkpoint_callback(callback):
             """adapt save checkpoint callback"""
-            _adapt_dict_args(callback, 'prefix', self.save_prefix)
-            _adapt_dict_args(callback, 'directory', self.save_directory)
             if self.save_strategy == SaveIntervalStrategy.STEPS:
                 _adapt_dict_args(callback, 'save_checkpoint_steps', self.save_steps)
             elif self.save_strategy == SaveIntervalStrategy.SECONDS:
                 _adapt_dict_args(callback, 'save_checkpoint_seconds', self.save_seconds)
             _adapt_dict_args(callback, 'keep_checkpoint_max', self.save_total_limit)
-            _adapt_dict_args(callback, 'keep_checkpoint_per_n_minutes', self.keep_checkpoint_per_n_minutes)
             integrated_save = self.integrated_save if self.integrated_save is not None \
                 else not self.save_on_each_node
             _adapt_dict_args(callback, 'integrated_save', integrated_save)
-            _adapt_dict_args(callback, 'save_network_params', self.save_network_params)
-            _adapt_dict_args(callback, 'save_trainable_params', self.save_trainable_params)
-            _adapt_dict_args(callback, 'async_save', self.async_save)
             return callback
 
         for i, callback in enumerate(task_config.callbacks):
             if callback['type'] == "MFLossMonitor":
                 task_config.callbacks[i] = _adapt_logging_callback(task_config.callbacks[i])
             if callback['type'] == "CheckpointMointor":
                 task_config.callbacks[i] = _adapt_save_checkpoint_callback(task_config.callbacks[i])
 
     def _adapt_eval_config(self, task_config):
         """adapt eval config"""
         task_config.eval_step_interval = _check_training_args(task_config.eval_step_interval, self.eval_steps)
         task_config.eval_epoch_interval = _check_training_args(task_config.eval_epoch_interval, self.eval_epochs)
 
-    def _adapt_profile_config(self, task_config):
+    def _adapt_profile_configl(self, task_config):
         """adapt profile config."""
         task_config.profile = _check_training_args(task_config.profile, self.profile)
         task_config.profile_start_step = _check_training_args(task_config.profile_start_step, self.profile_start_step)
         task_config.profile_end_step = _check_training_args(task_config.profile_end_step, self.profile_end_step)
         task_config.init_start_profile = _check_training_args(task_config.init_start_profile, self.init_start_profile)
         task_config.profile_communication = _check_training_args(task_config.profile_communication,
                                                                  self.profile_communication)
```

## mindformers/trainer/utils.py

```diff
@@ -257,47 +257,45 @@
     for key, value in config.items():
         if isinstance(value, MindFormerConfig):
             value = config2dict(value)
         new_dict.setdefault(key, value)
     return new_dict
 
 
-def load_distributed_checkpoint(checkpoint_dir, choice_func=None):
+def load_distributed_checkpoint(checkpoint_dir, specify_prefix=None):
     """Load Checkpoint in Parallel Mode."""
     if os.path.isdir(checkpoint_dir):
         logger.info(
             "When distributed loads are sliced weights,"
             "load_checkpoint should be a checkpoint directory containing the directory of rank_{0-*},"
             "The directory structure is as follows: **checkpoint_root_dir/rank_{0-*}/**.ckpt")
         distribute_checkpoint_dir = os.path.join(
             checkpoint_dir, "rank_{}".format(get_real_rank()))
         distribute_checkpoint_path = get_last_checkpoint(distribute_checkpoint_dir)
     elif os.path.isfile(checkpoint_dir):
         logger.info("Your load_checkpoint is file, it will be load in network.")
         distribute_checkpoint_path = checkpoint_dir
     else:
         raise FileNotFoundError(f"{checkpoint_dir} is not found.")
-    checkpoint_dict = load_checkpoint(distribute_checkpoint_path, choice_func=choice_func)
+    checkpoint_dict = load_checkpoint(distribute_checkpoint_path, specify_prefix=specify_prefix)
     logger.info("Distribute load is success.")
     return checkpoint_dict
 
 
 def load_resume_context_from_checkpoint(config, dataset):
     """resume training, load training info from checkpoint to config"""
     if not os.path.realpath(config.load_checkpoint) or \
             not os.path.exists(config.load_checkpoint):
         raise FileNotFoundError(f"The load_checkpoint must be correct, "
                                 f"but get {config.load_checkpoint}")
 
     if os.path.isdir(config.load_checkpoint):
-        resume_dict = load_distributed_checkpoint(config.load_checkpoint,
-                                                  choice_func=lambda x: x in ["loss_scale", "epoch_num", "step_num"])
+        resume_dict = load_distributed_checkpoint(config.load_checkpoint, ["loss_scale", "epoch_num", "step_num"])
     else:
-        resume_dict = load_checkpoint(config.load_checkpoint,
-                                      choice_func=lambda x: x in ["loss_scale", "epoch_num", "step_num"])
+        resume_dict = load_checkpoint(config.load_checkpoint, specify_prefix=["loss_scale", "epoch_num", "step_num"])
 
     if "step_num" in resume_dict:
         config.runner_config.initial_step = int(resume_dict["step_num"])
     else:
         config.runner_config.initial_step = 0
 
     if "epoch_num" in resume_dict:
@@ -330,16 +328,16 @@
     5. load ckpt
     """
     if not config.only_save_strategy and (not os.path.realpath(config.load_checkpoint) or
                                           not os.path.exists(config.load_checkpoint)):
         raise FileNotFoundError(f"The load_checkpoint must be correct, "
                                 f"but get {config.load_checkpoint}")
 
-    if not config.auto_trans_ckpt and not config.only_save_strategy and \
-        check_path_include_total_ckpt(config.load_checkpoint):
+    if check_path_include_total_ckpt(config.load_checkpoint) and not config.auto_trans_ckpt and \
+                                                                 not config.only_save_strategy:
         load_ckpt(config, network, optimizer=optimizer)
         return
 
     if context.get_auto_parallel_context('parallel_mode') in ['semi_auto_parallel', 'auto_parallel',
                                                               'hybrid_parallel']:
         # 1. build net if parallel mode is auto_parallel
         logger.info(".........Building model.........")
@@ -375,22 +373,14 @@
     load_ckpt(config, network, optimizer=optimizer)
 
 
 def check_ckpt_for_transform(ckpt_dir):
     """check input ckpt_dir and transform it by using softlink"""
     soft_link_dir = os.path.join(get_output_root_path(), "softlink_ckpt")
     rank_id = get_real_rank()
-
-    if os.path.isdir(ckpt_dir) and not check_rank_folders(ckpt_dir, 0) and \
-        not check_ckpt_file_exist(ckpt_dir):
-        raise ValueError(f"No rank_0 folder or ckpt files are found under {ckpt_dir}.")
-    if os.path.isfile(ckpt_dir) and not ckpt_dir.endswith('.ckpt'):
-        raise ValueError(f"The value of load_checkpoint must be a folder or a file with suffix '.ckpt', "
-                         f"but got {ckpt_dir}")
-
     if (not rank_id) or (rank_id % 8 == 0 and check_in_modelarts()):
         if os.path.exists(soft_link_dir):
             shutil.rmtree(soft_link_dir)
             logger.info("Find exist softlink dir %s and delete it.", os.path.join(os.getcwd(), soft_link_dir))
         if os.path.isdir(ckpt_dir):
             if check_rank_folders(ckpt_dir, 0):
                 if check_ckpt_file_exist(ckpt_dir):
@@ -402,24 +392,30 @@
                 soft_link = os.path.join(soft_link_dir, os.path.basename(ckpt_dir))
                 logger.info("Make soft link of checkpoint file from %s to %s", ckpt_dir, soft_link)
                 if not os.path.exists(soft_link):
                     os.symlink(ckpt_dir, soft_link)
                 else:
                     os.remove(soft_link)
                     os.symlink(ckpt_dir, soft_link)
-            else:
+            elif check_ckpt_file_exist(ckpt_dir):
                 for ckpt_file in os.listdir(ckpt_dir):
                     if ckpt_file.endswith('.ckpt'):
                         soft_link = os.path.join(soft_link_dir, os.path.splitext(ckpt_file)[0])
                         ckpt_file = os.path.join(ckpt_dir, ckpt_file)
                         make_softlink(soft_link, ckpt_file)
+            else:
+                raise ValueError(f"No rank_0 folder or ckpt files are found under {ckpt_dir}.")
         else:
-            ckpt_file = ckpt_dir
-            soft_link = os.path.join(soft_link_dir, os.path.splitext(os.path.basename(ckpt_file))[0])
-            make_softlink(soft_link, ckpt_file)
+            if ckpt_dir.endswith('.ckpt'):
+                ckpt_file = ckpt_dir
+                soft_link = os.path.join(soft_link_dir, os.path.splitext(os.path.basename(ckpt_file))[0])
+                make_softlink(soft_link, ckpt_file)
+            else:
+                raise ValueError(f"The value of load_checkpoint must be a folder or a file with suffix '.ckpt', "
+                                 f"but got {ckpt_dir}")
 
     wait_create_softlink(soft_link_dir)
 
     return soft_link_dir
 
 
 def check_rank_folders(path, rank_id):
@@ -436,16 +432,14 @@
         if file_name.endswith('.ckpt'):
             return True
     return False
 
 
 def check_path_include_total_ckpt(path):
     """check if the input path is total, not split."""
-    if path is None:
-        return False
     if os.path.isdir(path):
         if check_ckpt_file_exist(path):
             return True
     elif path.endswith('.ckpt'):
         return True
     return False
 
@@ -463,34 +457,38 @@
 def build_model(config, model, dataset, do_eval=False, do_predict=False):
     """build model, generate strategy file"""
     if context.get_auto_parallel_context('parallel_mode') in ['semi_auto_parallel', 'auto_parallel',
                                                               'hybrid_parallel']:
         if not config.runner_config.sink_mode:
             raise ValueError("When distributed loads are sliced weights, sink_mode must be set True.")
         if do_eval:
-            model.infer_predict_layout(*dataset)
+            model.infer_predict_layout(*next(dataset.create_tuple_iterator()))
         elif do_predict:
-            model.infer_predict_layout(*dataset)
+            model.infer_predict_layout(dataset)
         else:
             model.build(train_dataset=dataset, epoch=config.runner_config.epochs,
                         sink_size=config.runner_config.sink_size)
 
 
 def get_src_and_dst_strategy(config):
     """get strategy"""
     rank_id = get_real_rank()
     world_size = get_real_group_size()
 
-    if config.src_strategy_path_or_dir:
-        assert os.path.exists(config.src_strategy_path_or_dir), \
-            f'{config.src_strategy_path_or_dir} not found!'
-
     dst_strategy_path = None
     if (not rank_id) or (rank_id % 8 == 0 and check_in_modelarts()):
-        src_strategy_path = get_strategy(config.src_strategy_path_or_dir)
+        if config.src_strategy_path_or_dir and os.path.isdir(config.src_strategy_path_or_dir):
+            if config.parallel_config.pipeline_stage > 1:
+                src_strategy_path = get_strategy(config.src_strategy_path_or_dir)
+            elif config.parallel_config.pipeline_stage == 1:
+                src_strategy_paths = glob(os.path.join(config.src_strategy_path_or_dir, "*_rank_*.ckpt"))
+                src_strategy_paths.sort()
+                src_strategy_path = src_strategy_paths[0]
+        else:
+            src_strategy_path = get_strategy(config.src_strategy_path_or_dir)
     else:
         src_strategy_path = None
 
     if world_size == 1:
         return src_strategy_path, dst_strategy_path
 
     if check_in_modelarts():
```

## mindformers/trainer/causal_language_modeling/causal_language_modeling.py

```diff
@@ -212,30 +212,25 @@
             compute_metrics = self.create_metrics(metric_name=metric_name)
             for metric_name in compute_metrics:
                 compute_metrics[metric_name].clear()
 
         # build tokenizer
         logger.info(".........Build tokenizer For Evaluate..........")
         if tokenizer is None and config.processor.tokenizer:
-            tokenizer = build_tokenizer(config.processor.tokenizer)
+            tokenizer = build_tokenizer(config.processor.tokenizer, tokenizer_name=config.trainer.model_name)
 
         logger.info(".........Starting Init Evaluate Model..........")
         model = Model(network, eval_network=network)
 
         if config.load_checkpoint or config.only_save_strategy:
             if config.load_checkpoint in SUPPORT_MODEL_NAMES:
                 config.load_checkpoint = \
                     AutoModel.from_pretrained(config.load_checkpoint).default_checkpoint_download_path
             logger.info(".............Start load checkpoint for eval..................")
-
-            dataset_dict = next(dataset.create_dict_iterator())
-            input_ids = dataset_dict['input_ids'].asnumpy()
-            labels = dataset_dict['labels'].asnumpy()
-            infer_data = network.prepare_inputs_for_predict_layout(input_ids, labels=labels)
-            transform_and_load_checkpoint(config, model, network, infer_data, do_eval=True)
+            transform_and_load_checkpoint(config, model, network, dataset, do_eval=True)
 
         logger.info('.........Starting Evaluate Model..........')
         if get_real_rank() % 8 == 0:
             pprint(config)
         # generate config
         do_sample = config.model.model_config.do_sample
         top_p = config.model.model_config.top_p
@@ -343,14 +338,23 @@
         return self.predict_process(config=config,
                                     input_data=input_data,
                                     task='text_generation',
                                     network=network,
                                     tokenizer=tokenizer,
                                     **kwargs)
 
+    def export(self,
+               config: Optional[Union[dict, MindFormerConfig, ConfigArguments, TrainingArguments]] = None,
+               network: Optional[Union[Cell, PreTrainedModel]] = None,
+               **kwargs):
+
+        return self.export_process(config=config,
+                                   network=network,
+                                   **kwargs)
+
     def _evaluate_in_training(self, model, eval_dataset):
         logger.info('Starting Evaluate Model')
         metric_name_list = list(self.compute_metrics.keys())
         use_generate_evaluate = self.check_generate_evaluate(metric_name_list)
 
         if use_generate_evaluate:
             config = self.config
```

## mindformers/trainer/contrastive_language_image_pretrain/contrastive_language_image_pretrain.py

```diff
@@ -94,7 +94,11 @@
     def evaluate(self, **kwargs):
         raise NotImplementedError(
             "The contrastive language image pretrain task does not support evaluate.")
 
     def predict(self, **kwargs):
         raise NotImplementedError(
             "The contrastive language image pretrain task does not support predict.")
+
+    def export(self, **kwargs):
+        raise NotImplementedError(
+            "The contrastive language image pretrain task does not support export.")
```

## mindformers/trainer/general_task_trainer/general_task_trainer.py

```diff
@@ -168,7 +168,11 @@
             callbacks=callbacks,
             **kwargs
         )
 
     def predict(self, **kwargs):
         raise NotImplementedError(
             "The general task does not support predict, please customize pipeline inference.")
+
+    def export(self, **kwargs):
+        raise NotImplementedError(
+            "The general task does not support export, please customize pipeline inference.")
```

## mindformers/trainer/image_classification/image_classification.py

```diff
@@ -183,7 +183,11 @@
 
         return self.predict_process(config=config,
                                     input_data=batch_input_data,
                                     task='image_classification',
                                     network=network,
                                     image_processor=image_processor,
                                     **kwargs)
+
+    def export(self, **kwargs):
+        raise NotImplementedError(
+            "The image classification task does not support export, please customize pipeline inference.")
```

## mindformers/trainer/image_classification/zero_shot_image_classification.py

```diff
@@ -159,7 +159,11 @@
                                     task='zero_shot_image_classification',
                                     network=network,
                                     tokenizer=tokenizer,
                                     image_processor=image_processor,
                                     candidate_labels=candidate_labels,
                                     hypothesis_template=hypothesis_template,
                                     **kwargs)
+
+    def export(self, **kwargs):
+        raise NotImplementedError(
+            "The image classification task does not support export, please customize pipeline inference.")
```

## mindformers/trainer/image_to_text_generation/image_to_text_generation.py

```diff
@@ -125,7 +125,11 @@
                                     network=network,
                                     tokenizer=tokenizer,
                                     image_processor=image_processor,
                                     max_length=max_length,
                                     padding=padding,
                                     hypothesis_template=hypothesis_template,
                                     **kwargs)
+
+    def export(self, **kwargs):
+        raise NotImplementedError(
+            "The image to text generation task does not support export, please customize pipeline inference.")
```

## mindformers/trainer/image_to_text_retrieval/image_to_text_retrieval.py

```diff
@@ -17,15 +17,15 @@
 from pprint import pprint
 
 import numpy as np
 from mindspore import dtype as mstype
 from mindspore.train import Callback
 
 from mindformers.dataset import build_dataset, check_dataset_config, BaseDataset
-from mindformers.models import build_network, PreTrainedModel
+from mindformers.models import build_model, PreTrainedModel
 from mindformers.core.callback import build_callback
 from mindformers.tools.logger import logger
 from mindformers.tools.utils import count_params, get_real_rank
 from mindformers.tools.register import MindFormerRegister, MindFormerModuleType
 from mindformers.tools.check_rules import check_rules
 from .eval_utils import compute_itm_scores, extract_image_text_mapping, \
     prepare_inputs_for_itm_eval, report_metrics
@@ -36,14 +36,20 @@
 @MindFormerRegister.register(MindFormerModuleType.TRAINER, alias="image_to_text_retrieval")
 class ImageToTextRetrievalTrainer(BaseTrainer):
     """
     Image-to-text Retrieval Trainer.
 
     Args:
         model_name (str): The model name of Task-Trainer. Default: None
+
+    Examples:
+        >>> from mindformers.trainer import ImageToTextRetrievalTrainer
+        >>> trainer = ImageToTextRetrievalTrainer(model_name="blip2_stage1_vit_g")
+        >>> type(trainer)
+        <class 'mindformers.trainer.image_to_text_retrieval.image_to_text_retrieval.ImageToTextRetrievalTrainer'>
     """
     def __init__(self, model_name: str = None):
         super(ImageToTextRetrievalTrainer, self).__init__("image_to_text_retrieval", model_name)
         self.model_name = model_name
         self.kwargs = None
 
     def train(self, **kwargs):
@@ -83,15 +89,15 @@
         if dataset is None:
             dataset = build_dataset(config.eval_dataset_task)
         logger.info("Create eval dataset finish, dataset size:%d", dataset.get_dataset_size())
 
         # build network
         logger.info(".........Build Net..........")
         if network is None:
-            network = build_network(config.model, default_args={
+            network = build_model(config.model, default_args={
                 "parallel_config": config.parallel_config,
                 "moe_config": config.moe_config,
                 "is_training": False})
 
         network = network.to_float(mstype.float16)
 
         # checkpoint
@@ -158,7 +164,11 @@
             img2txt,
             txt2img,
         )
 
         logger.info(eval_result)
         logger.info(".........Evaluate Over!.............")
         return eval_result
+
+    def export(self, **kwargs):
+        raise NotImplementedError(
+            "The image to text retrieval task does not support export, please customize pipeline inference.")
```

## mindformers/utils/convert_utils.py

```diff
@@ -13,39 +13,35 @@
 # limitations under the License.
 # ============================================================================
 """
 Convert utils.
 """
 import torch
 import mindspore as ms
-from mindspore.ops.operations import Cast
-
-cpu_cast = Cast().set_device("CPU")
 
 
 def pt2ms(value: torch.Tensor, dtype) -> ms.Tensor:
     """
     convert torch.Tensor to ms.Tensor with specified dtype
     """
     if value.dtype == torch.bfloat16:
-        np_value = value.detach().cpu().to(torch.float32).numpy()
+        np_value = value.to(torch.float32).numpy()
     else:
         np_value = value.detach().numpy()
 
     if dtype:
         return ms.Tensor(np_value, dtype=dtype)
     return ms.Tensor(np_value, dtype=ms.bfloat16) if value.dtype == torch.bfloat16 else ms.Tensor(np_value)
 
 
 def ms2pt(value: ms.Tensor, dtype) -> torch.Tensor:
     """
     convert ms.Tensor to torch.Tensor with specified dtype
     """
     if value.dtype == ms.bfloat16:
-        np_value = cpu_cast(value, ms.float32).asnumpy()
+        np_value = value.data.astype(ms.float32).asnumpy()
     else:
-        np_value = value.asnumpy()
+        np_value = value.data.asnumpy()
 
     if dtype:
-        return torch.from_numpy(np_value).cpu().to(dtype)
-    return torch.from_numpy(np_value).cpu().to(torch.bfloat16) if value.dtype == ms.bfloat16 else torch.from_numpy(
-        np_value)
+        return torch.from_numpy(np_value).to(dtype)
+    return torch.from_numpy(np_value).to(torch.bfloat16) if value.dtype == ms.bfloat16 else torch.from_numpy(np_value)
```

## Comparing `configs/codellama/finetune_codellama_34b_32p.yaml` & `configs/codellama/run_codellama_34b_910b_32p.yaml`

 * *Files 3% similar despite different names*

```diff
@@ -39,15 +39,15 @@
 
 # dataset
 train_dataset: &train_dataset
   data_loader:
     type: MindDataset
     dataset_dir: ""
     shuffle: True
-  input_columns: ["input_ids", "labels"]  # "input_ids", "labels" , labels are used in instruction finetune.
+  input_columns: ["input_ids"]  # "input_ids", "labels" , labels are used in instruction finetune.
   num_parallel_workers: 8
   python_multiprocessing: False
   drop_remainder: True
   repeat: 1
   numa_enable: False
   prefetch_size: 1
 train_dataset_task:
@@ -147,18 +147,21 @@
     theta: 1000000.0
     compute_dtype: "float16"
     layernorm_compute_type: "float32"
     softmax_compute_type: "float16"
     rotary_dtype: "float16"
     param_init_type: "float16"
     use_past: False
+    pretrain_seqlen: 4096 # seqlen of the pretrain checkpoint
     extend_method: "None" # support "None", "PI", "NTK"
+    compute_in_2d: True
     use_flash_attention: True
     fine_grain_interleave: 2
     offset: 0
+    use_past_shard: False
     checkpoint_name_or_path: ""
     repetition_penalty: 1
     max_decode_length: 512
     top_k: 3
     top_p: 1
     do_sample: False
   arch:
```

## Comparing `configs/glm2/predict_glm2_6b.yaml` & `configs/blip2/run_blip2_stage2_vit_g_llama_7b_910b.yaml`

 * *Files 14% similar despite different names*

```diff
@@ -1,253 +1,260 @@
-seed: 0
-run_mode: 'predict'
+seed: 42
+run_mode: 'train'
 output_dir: './output' # path to save checkpoint/strategy
-load_checkpoint: ''
+load_checkpoint: 'blip2_stage1_pretrained'
 src_strategy_path_or_dir: ''
 auto_trans_ckpt: False  # If true, auto transform load_checkpoint to load in distributed model
 only_save_strategy: False
 resume_training: False
 
-# ==== context config ====
+# context
 context:
   mode: 0 #0--Graph Mode; 1--Pynative Mode
   device_target: "Ascend"
   enable_graph_kernel: False
-  graph_kernel_flags: "--disable_expand_ops=Softmax,Dropout --enable_parallel_fusion=true --reduce_fuse_depth=8 --enable_auto_tensor_inplace=true"
+  graph_kernel_flags: str = "--disable_expand_ops=Softmax,Dropout " \
+                              "--enable_parallel_fusion=true --reduce_fuse_depth=8 --enable_auto_tensor_inplace=true"
+  max_device_memory: "55GB"
   max_call_depth: 10000
-  max_device_memory: "30GB" # 59GB for Atlas 800T A2
   save_graphs: False
+  save_graphs_path: "./graph"
   device_id: 0
 
 # aicc
 remote_save_url: "Please input obs url on AICC platform."
 
-# ==== model config ====
-model:
-  model_config:
-    type: ChatGLM2Config
-    batch_size: 1   # only for incremental infer
-    num_layers: 28
-    padded_vocab_size: 65024
-    hidden_size: 4096
-    ffn_hidden_size: 13696
-    kv_channels: 128
-    num_attention_heads: 32
-    seq_length: 193
-    hidden_dropout: 0.0
-    attention_dropout: 0.0
-    layernorm_epsilon: 1e-5
-    rmsnorm: True
-    apply_residual_connection_post_layernorm: False
-    post_layer_norm: True
-    add_bias_linear: False
-    add_qkv_bias: True
-    bias_dropout_fusion: True
-    multi_query_attention: True
-    multi_query_group_num: 2
-    apply_query_key_layer_scaling: True
-    attention_softmax_in_fp32: True
-    fp32_residual_connection: False
-    quantization_bit: 0
-    pre_seq_len: None
-    prefix_projection: False
-    param_init_type: "float16"
-    compute_dtype: "float16"
-    layernorm_compute_type: "float32"
-    use_past: True
-    is_dynamic: True
-    use_flash_attention: False # when use FlashAttention, seq_length should be multiple of 16
-    use_prompt_flash_attention: False
-    use_incre_flash_attention: False
-    eos_token_id: 2
-    pad_token_id: 0
-    repetition_penalty: 1.0
-    max_decode_length: 256
-    checkpoint_name_or_path: "glm2_6b"
-    top_k: 1
-    top_p: 1
-    do_sample: True
-  arch:
-    type: ChatGLM2ForConditionalGeneration
-
-trainer:
-  type: CausalLanguageModelingTrainer
-  model_name: 'glm2_6b'
-# if True do, evaluate during the training process. if false, do nothing.
-# note that the task trainer should support _evaluate_in_training function.
-do_eval: False
-eval_step_interval: 500
-eval_epoch_interval: -1
-
-metric:
-  type: ADGENMetric
-
-processor:
-  return_tensors: ms
-  tokenizer:
-    type: ChatGLM2Tokenizer
-    bos_token: '<sop>'
-    eos_token: '<eop>'
-    end_token: '</s>'
-    mask_token: '[MASK]'
-    gmask_token: '[gMASK]'
-    pad_token: '<pad>'
-    unk_token: '<unk>'
-    # vocab_file: "/path/to/tokenizer.model"
-  type: GLMProcessor
-
-# ==== dataset config ====
-train_dataset: &train_dataset
-  data_loader:
-    type: ADGenDataLoader
-    dataset_dir: "/path/to/AdvertiseGen/train.json"
-    shuffle: True
-    phase: "train"
-    version: 2
-    origin_columns: ["content", "summary"]
-  tokenizer:
-    type: ChatGLM2Tokenizer
-    vocab_file: "/path/to/tokenizer.model"
-  input_columns: ["input_ids", "labels"]
-  max_source_length: 64
-  max_target_length: 128
-  ignore_pad_token_for_loss: True
-  num_parallel_workers: 8
-  python_multiprocessing: False
-  drop_remainder: True
-  batch_size: 1
-  repeat: 1
-  numa_enable: False
-  prefetch_size: 1
-  seed: 0
-
-train_dataset_task:
-  type: KeyWordGenDataset
-  dataset_config: *train_dataset
-
-eval_dataset: &eval_dataset
-  data_loader:
-    type: ADGenDataLoader
-    dataset_dir: "/path/to/AdvertiseGen/dev.json"
-    shuffle: False
-    phase: "eval"
-    version: 2
-    origin_columns: ["content", "summary"]
-  tokenizer:
-    type: ChatGLM2Tokenizer
-    vocab_file: "/path/to/tokenizer.model"
-  max_source_length: 256
-  max_target_length: 256
-  ignore_pad_token_for_loss: True
-  input_columns: ["input_ids", "labels"]
-  num_parallel_workers: 8
-  python_multiprocessing: False
-  drop_remainder: True
-  batch_size: 1
-  repeat: 1
-  numa_enable: False
-  prefetch_size: 1
-  seed: 0
-
-eval_dataset_task:
-  type: KeyWordGenDataset
-  dataset_config: *eval_dataset
-
-# ==== runner config ====
+# runner
 runner_config:
   epochs: 1
-  batch_size: 8
+  batch_size: &batch_size 64
+  sink_size: 2
+  image_size: 224
   sink_mode: True
-  sink_size: 4
-
+  initial_epoch: 0
+  has_trained_epoches: 0
+  has_trained_steps: 0
 runner_wrapper:
-  type: MFTrainOneStepCell
-  scale_sense:
-    type: DynamicLossScaleUpdateCell
-    loss_scale_value: 65536
-    scale_factor: 2
-    scale_window: 1000
-  use_clip_grad: True
-
-# lr sechdule
-lr_schedule:
-  type: polynomial
-  learning_rate: 5.e-5
-  lr_end: 1.e-6
-  warmup_steps: 0
-  total_steps: -1 # -1 means it will load the total steps of the dataset
-layer_scale: False
-layer_decay: 0.65
-
-# optimizer
-optimizer:
-  type: FP32StateAdamWeightDecay
-  beta1: 0.9
-  beta2: 0.95
-  eps: 1.e-8
-  weight_decay: 0.1
-lr_scale: False
-lr_scale_factor: 256
+  type: TrainOneStepCell
+  sens: 1024
 
-# parallel config
+# parallel
 use_parallel: False
 parallel:
-  parallel_mode: 1 # 0-dataset, 1-semi, 2-auto, 3-hybrid
-  gradients_mean: False
-  loss_repeated_mean: True
-  enable_alltoall: False
-  full_batch: True
+  parallel_mode: 0 # 0-dataset, 1-semi, 2-auto, 3-hybrid
+  gradients_mean: True
   search_mode: "sharding_propagation"
-  enable_parallel_optimizer: True  # optimizer shard
-  strategy_ckpt_config:
-    save_file: "./ckpt_strategy.ckpt"
+  enable_parallel_optimizer: False
+  full_batch: False
 parallel_config:
-  data_parallel: 8
+  data_parallel: 1
   model_parallel: 1
   pipeline_stage: 1
-  expert_parallel: 1
   micro_batch_num: 1
   vocab_emb_dp: True
   gradient_aggregation_group: 4
-micro_batch_interleave_num: 1
-
-# moe
-moe_config:
-  expert_num: 1
-  capacity_factor: 1.05
-  aux_loss_factor: 0.05
-  num_experts_chosen: 1
+micro_batch_interleave_num: &micro_batch_interleave_num 1
 
 # recompute
 recompute_config:
-  recompute: True
+  recompute: False
   parallel_optimizer_comm_recompute: False
   mp_comm_recompute: True
   recompute_slice_activation: False
 
 # autotune
 auto_tune: False
 filepath_prefix: './autotune'
 autotune_per_step: 10
 
 # profile
 profile: False
 profile_start_step: 1
 profile_stop_step: 10
-init_start_profile: True
-profile_communication: True
+init_start_profile: False
+profile_communication: False
 profile_memory: True
 
+# Trainer
+trainer:
+  type: ContrastiveLanguageImagePretrainTrainer
+  model_name: 'blip2_stage2_vit_g_llama_7b'
+
+# train dataset
+train_dataset: &train_dataset
+  data_loader:
+    type: MultiImgCapDataLoader
+    dataset_dir: "/data"
+    annotation_files: [
+      "coco2014/coco/annotations/coco_karpathy_train.json"
+    ]
+    image_dirs: [
+      "coco2014/coco/images"
+    ]
+    stage: "train"
+    column_names: [ "image", "text" ]
+    shuffle: True
+  transforms:
+    - type: RandomResizedCrop
+      size: 224
+      scale: [ 0.5, 1.0 ]
+      interpolation: "bicubic"
+    - type: RandomHorizontalFlip
+    - type: ToTensor
+    - type: Normalize
+      mean: [ 0.48145466, 0.4578275, 0.40821073 ]
+      std: [ 0.26862954, 0.26130258, 0.27577711 ]
+      is_hwc: False
+  text_transforms:
+    type: CaptionTransform
+    prompt: ""
+    max_words: 50
+    max_length: 33  # it equals to Blip2Config.max_txt_len + 1 for constructing labels
+    padding: 'max_length'
+    random_seed: 2022
+    truncation: True
+    add_special_tokens: True
+  tokenizer:
+    type: LlamaTokenizer
+    unk_token: '<unk>'
+    bos_token: '<s>'
+    eos_token: '</s>'
+    pad_token: '<unk>'
+    add_special_tokens: False
+    padding: 'max_length'
+    truncation: True
+    max_length: 33   # it equals to Blip2Config.max_txt_len + 1 for constructing labels
+    add_bos_token: False
+    add_eos_token: True
+    checkpoint_name_or_path: 'llama_7b'
+
+  num_parallel_workers: 8
+  python_multiprocessing: False
+  drop_remainder: True
+  batch_size: 4
+  repeat: 1
+  numa_enable: False
+  prefetch_size: 30
+  seed: 2022
+train_dataset_task:
+  type: ContrastiveLanguageImagePretrainDataset
+  dataset_config: *train_dataset
+# model
+model:
+  model_config:
+    type: Blip2Config
+    batch_size: *batch_size
+    micro_batch_interleave_num: *micro_batch_interleave_num
+    freeze_vision: True
+    freeze_text: True
+    max_txt_len: 32
+    checkpoint_name_or_path: "blip2_stage1_pretrained"
+    dtype: "float32"
+    compute_dtype: "float16"
+    layernorm_dtype: "float32"
+    softmax_dtype: "float32"
+    prompt: False
+    prompt_length: 0
+    vision_config:
+      type: ViTConfig
+      image_size: 224
+      patch_size: 14
+      num_channels: 3
+      initializer_range: 0.001
+      hidden_size: 1408
+      num_hidden_layers: 39
+      num_attention_heads: 16
+      intermediate_size: 6144
+      qkv_bias: True
+      hidden_act: gelu
+      post_layernorm_residual: false
+      layer_norm_eps: 1.0e-6
+      attention_probs_dropout_prob: 0.0
+      hidden_dropout_prob: 0.0
+      drop_path_rate: 0.0
+      use_mean_pooling: false
+      encoder_stride: 16
+      checkpoint_name_or_path: "vit_g_p16"
+
+    qformer_config:
+      vocab_size: 44728
+
+    text_config:
+      type: LlamaConfig
+      seq_length: 64  # sum of max_txt_len and num_query_token
+      hidden_size: 4096
+      num_layers: 32
+      num_heads: 32
+      vocab_size: 32000
+      multiple_of: 256
+      rms_norm_eps: 1.0e-6
+      bos_token_id: 1
+      eos_token_id: 2
+      pad_token_id: 0
+      ignore_token_id: -100
+      compute_dtype: "float16"
+      layernorm_compute_type: "float32"
+      softmax_compute_type: "float32"
+      rotary_dtype: "float16"
+      param_init_type: "float16"
+      use_past: False
+      offset: 0
+      repetition_penalty: 1
+      max_decode_length: 512
+      top_k: 3
+      top_p: 1
+      do_sample: False
+      checkpoint_name_or_path: "llama_7b"
+  arch:
+    type: Blip2Llm
+
+# lr schedule
+lr_schedule:
+  type: CosineWithWarmUpLR
+  learning_rate: 1.e-4
+  lr_end: 5.e-5
+  warmup_lr_init: 1.e-6
+  warmup_steps: 2000
+  total_steps: -1 # -1 means it will load the total steps of the dataset
+layer_scale: False
+layer_decay: 0.65
+lr_scale: False
+lr_scale_factor: 256
+
+# optimizer
+optimizer:
+  type: adamw
+  beta1: 0.9
+  beta2: 0.98
+  eps: 1.e-8 # 1e-8
+  weight_decay: 0.05
+  learning_rate: 1.e-4
+
 # callbacks
 callbacks:
   - type: MFLossMonitor
   - type: CheckpointMointor
-    prefix: "glm2-6b"
-    save_checkpoint_steps: 1000
-    keep_checkpoint_max: 2
-    integrated_save: False
+    prefix: "blip2_stage2_vig_g_llama_7b"
+    save_checkpoint_steps: 5000
+    integrated_save: True
     async_save: False
   - type: ObsMonitor
-    keep_last: False
 eval_callbacks:
   - type: ObsMonitor
-    keep_last: False
+
+
+# processor
+processor:
+  type: Blip2Processor
+  image_processor:
+    type: Blip2ImageProcessor
+    image_size: 224  # input image size
+  tokenizer:
+    type: LlamaTokenizer
+    pad_token: '<unk>'
+    bos_token: '<s>'
+    unk_token: '</s>'
+    eos_token: '</s>'
+    add_special_tokens: False
+    padding: 'max_length'
+    truncation: True
+    max_length: 32
```

## Comparing `configs/glm3/predict_glm3_6b.yaml` & `configs/blip2/run_blip2_stage1_vit_g_qformer_pretrain.yaml`

 * *Files 14% similar despite different names*

```diff
@@ -1,255 +1,252 @@
-seed: 0
-run_mode: 'predict'
+seed: 42
+run_mode: 'train'
 output_dir: './output' # path to save checkpoint/strategy
 load_checkpoint: ''
 src_strategy_path_or_dir: ''
 auto_trans_ckpt: False  # If true, auto transform load_checkpoint to load in distributed model
 only_save_strategy: False
 resume_training: False
 
-# ==== context config ====
+# context
 context:
   mode: 0 #0--Graph Mode; 1--Pynative Mode
   device_target: "Ascend"
   enable_graph_kernel: False
-  graph_kernel_flags: "--disable_expand_ops=Softmax,Dropout --enable_parallel_fusion=true --reduce_fuse_depth=8 --enable_auto_tensor_inplace=true"
+  graph_kernel_flags: str = "--disable_expand_ops=Softmax,Dropout " \
+                              "--enable_parallel_fusion=true --reduce_fuse_depth=8 --enable_auto_tensor_inplace=true"
   max_call_depth: 10000
-  max_device_memory: "59GB" # 59GB for Atlas 800T A2
   save_graphs: False
+  save_graphs_path: "./graph"
   device_id: 0
 
 # aicc
 remote_save_url: "Please input obs url on AICC platform."
 
-# ==== model config ====
-model:
-  model_config:
-    type: ChatGLM2Config
-    batch_size: 1   # only for incremental infer
-    num_layers: 28
-    padded_vocab_size: 65024
-    hidden_size: 4096
-    ffn_hidden_size: 13696
-    kv_channels: 128
-    num_attention_heads: 32
-    seq_length: 2048
-    hidden_dropout: 0.0
-    attention_dropout: 0.0
-    rmsnorm: True
-    layernorm_epsilon: 0.00001
-    apply_residual_connection_post_layernorm: False
-    post_layer_norm: True
-    add_bias_linear: False
-    add_qkv_bias: True
-    bias_dropout_fusion: True
-    multi_query_attention: True
-    multi_query_group_num: 2
-    apply_query_key_layer_scaling: True
-    attention_softmax_in_fp32: True
-    fp32_residual_connection: False
-    quantization_bit: 0
-    pre_seq_len: None
-    prefix_projection: False
-    param_init_type: "float16"
-    compute_dtype: "float16"
-    layernorm_compute_type: "float32"
-    use_past: True
-    use_flash_attention: True  # when use FlashAttention, seq_length should be multiple of 16
-    max_length: 256
-    block_size: 16
-    num_blocks: 128
-    is_dynamic: True
-    eos_token_id: 2
-    pad_token_id: 0
-    repetition_penalty: 1.0
-    max_decode_length: 256
-    checkpoint_name_or_path: "glm3_6b"
-    top_k: 1
-    top_p: 1
-    do_sample: False
-  arch:
-    type: ChatGLM2ForConditionalGeneration
-
-trainer:
-  type: CausalLanguageModelingTrainer
-  model_name: 'glm3_6b'
-# if True do, evaluate during the training process. if false, do nothing.
-# note that the task trainer should support _evaluate_in_training function.
-do_eval: False
-eval_step_interval: 500
-eval_epoch_interval: -1
-
-metric:
-  type: ADGENMetric
-
-processor:
-  return_tensors: ms
-  tokenizer:
-    type: ChatGLM3Tokenizer
-    bos_token: '<sop>'
-    eos_token: '<eop>'
-    end_token: '</s>'
-    mask_token: '[MASK]'
-    gmask_token: '[gMASK]'
-    pad_token: '<pad>'
-    unk_token: '<unk>'
-    vocab_file: "path/to/tokenizer.model"
-  type: GLMProcessor
-
-# ==== dataset config ====
-train_dataset: &train_dataset
-  data_loader:
-    type: ADGenDataLoader
-    dataset_dir: "/path/to/AdvertiseGen/train.json"
-    shuffle: True
-    phase: "train"
-    version: 3
-    origin_columns: ["content", "summary"]
-  tokenizer:
-    type: ChatGLM3Tokenizer
-    vocab_file: "/path/to/tokenizer.model"
-  input_columns: ["input_ids", "labels"]
-  max_source_length: 64
-  max_target_length: 127
-  ignore_pad_token_for_loss: True
-  num_parallel_workers: 8
-  python_multiprocessing: False
-  drop_remainder: True
-  batch_size: 8
-  repeat: 1
-  numa_enable: False
-  prefetch_size: 1
-  seed: 0
-
-train_dataset_task:
-  type: KeyWordGenDataset
-  dataset_config: *train_dataset
-
-eval_dataset: &eval_dataset
-  data_loader:
-    type: ADGenDataLoader
-    dataset_dir: "/path/to/AdvertiseGen/dev.json"
-    shuffle: False
-    phase: "eval"
-    version: 2
-    origin_columns: ["content", "summary"]
-  tokenizer:
-    type: ChatGLM3Tokenizer
-    vocab_file: "/path/to/tokenizer.model"
-  max_source_length: 256
-  max_target_length: 256
-  ignore_pad_token_for_loss: True
-  input_columns: ["input_ids", "labels"]
-  num_parallel_workers: 8
-  python_multiprocessing: False
-  drop_remainder: True
-  batch_size: 8
-  repeat: 1
-  numa_enable: False
-  prefetch_size: 1
-  seed: 0
-
-eval_dataset_task:
-  type: KeyWordGenDataset
-  dataset_config: *eval_dataset
-
-# ==== runner config ====
+# runner
 runner_config:
-  epochs: 1
-  batch_size: 8
-  sink_mode: True
-  sink_size: 4
-
+  epochs: 10
+  batch_size: 80
+  sink_size: 2
+  image_size: 224
+  sink_mode: False
+  initial_epoch: 0
+  has_trained_epoches: 0
+  has_trained_steps: 0
 runner_wrapper:
-  type: MFTrainOneStepCell
-  scale_sense:
-    type: DynamicLossScaleUpdateCell
-    loss_scale_value: 65536
-    scale_factor: 2
-    scale_window: 1000
-  use_clip_grad: True
-
-# lr sechdule
-lr_schedule:
-  type: polynomial
-  learning_rate: 1.e-4
-  lr_end: 1.e-6
-  warmup_steps: 0
-  total_steps: -1 # -1 means it will load the total steps of the dataset
-layer_scale: False
-layer_decay: 0.65
+  type: TrainOneStepCell
+  sens: 1024
 
-# optimizer
-optimizer:
-  type: FP32StateAdamWeightDecay
-  beta1: 0.9
-  beta2: 0.95
-  eps: 1.e-8
-  weight_decay: 0.1
-lr_scale: False
-lr_scale_factor: 256
-
-# parallel config
-use_parallel: False
+# parallel
+use_parallel: True
 parallel:
-  parallel_mode: 1 # 0-dataset, 1-semi, 2-auto, 3-hybrid
-  gradients_mean: False
-  loss_repeated_mean: True
-  enable_alltoall: False
-  full_batch: True
+  parallel_mode: 0 # 0-dataset, 1-semi, 2-auto, 3-hybrid
+  gradients_mean: True
   search_mode: "sharding_propagation"
-  enable_parallel_optimizer: False  # optimizer shard
-  strategy_ckpt_config:
-    save_file: "./ckpt_strategy.ckpt"
-    only_trainable_params: False
+  enable_parallel_optimizer: False
+  full_batch: False
 parallel_config:
-  data_parallel: 8
+  data_parallel: 2
   model_parallel: 1
   pipeline_stage: 1
-  expert_parallel: 1
   micro_batch_num: 1
   vocab_emb_dp: True
   gradient_aggregation_group: 4
 micro_batch_interleave_num: 1
 
-# moe
-moe_config:
-  expert_num: 1
-  capacity_factor: 1.05
-  aux_loss_factor: 0.05
-  num_experts_chosen: 1
-
 # recompute
 recompute_config:
   recompute: False
   parallel_optimizer_comm_recompute: False
   mp_comm_recompute: True
-  recompute_slice_activation: True
+  recompute_slice_activation: False
 
 # autotune
 auto_tune: False
 filepath_prefix: './autotune'
 autotune_per_step: 10
 
 # profile
 profile: False
 profile_start_step: 1
 profile_stop_step: 10
-init_start_profile: True
-profile_communication: True
+init_start_profile: False
+profile_communication: False
 profile_memory: True
 
+# Trainer
+trainer:
+  type: ContrastiveLanguageImagePretrainTrainer
+  model_name: 'blip2_stage1_vit_g'
+
+# train dataset
+train_dataset: &train_dataset
+  data_loader:
+    type: MultiImgCapDataLoader
+    dataset_dir: "/data"
+    annotation_files: [
+      "vg/annotations/vg_caption.json",
+      "coco2014/coco/annotations/coco_karpathy_train.json"
+    ]
+    image_dirs: [
+      "vg/images",
+      "coco2014/coco/images"
+    ]
+    stage: "train"
+    column_names: ["image", "text"]
+  transforms:
+    - type: RandomResizedCrop
+      size: 224
+      scale: [0.5, 1.0]
+      interpolation: "bicubic"
+    - type: RandomHorizontalFlip
+    - type: ToTensor
+    - type: Normalize
+      mean: [0.48145466, 0.4578275, 0.40821073]
+      std: [0.26862954, 0.26130258, 0.27577711]
+      is_hwc: False
+  text_transforms:
+    type: CaptionTransform
+    prompt: ""
+    max_words: 50
+    max_length: 32
+    padding: 'max_length'
+    random_seed: 2022
+    truncation: True
+  tokenizer:
+    type: BertTokenizer
+    pad_token: '[PAD]'
+    bos_token: '[DEC]'
+    add_special_tokens: True
+    padding: 'max_length'
+    truncation: True
+    max_length: 32
+  num_parallel_workers: 8
+  python_multiprocessing: False
+  drop_remainder: True
+  batch_size: 1
+  repeat: 1
+  numa_enable: False
+  prefetch_size: 30
+  seed: 2022
+  return_attention_mask: True
+train_dataset_task:
+  type: ContrastiveLanguageImagePretrainDataset
+  dataset_config: *train_dataset
+
+# model
+model:
+  model_config:
+    type: Blip2Config
+    freeze_vision: True
+    max_txt_len: 32
+    checkpoint_name_or_path: ""
+    dtype: "float32"
+    compute_dtype: "float16"
+    layernorm_dtype: "float32"
+    softmax_dtype: "float32"
+    vision_config:
+      type: ViTConfig
+      image_size: 224
+      patch_size: 14
+      num_channels: 3
+      initializer_range: 0.001
+      hidden_size: 1408
+      num_hidden_layers: 39
+      num_attention_heads: 16
+      intermediate_size: 6144
+      qkv_bias: true
+      hidden_act: gelu
+      post_layernorm_residual: false
+      layer_norm_eps: 1.0e-6
+      attention_probs_dropout_prob: 0.0
+      hidden_dropout_prob: 0.0
+      drop_path_rate: 0.0
+      use_mean_pooling: false
+      encoder_stride: 16
+      checkpoint_name_or_path: "vit_g_p16"
+
+    qformer_config:
+      num_hidden_layers: 12
+      num_attention_heads: 12
+      query_length: 32
+      resize_token_embeddings: True # if run on Atlas 800T A2, turn it to False
+      special_token_nums: 1
+      vocab_size: 30522
+      hidden_size: 768
+      encoder_width: 1408
+      bos_token_id: 30522
+      sep_token_id: 102
+      pad_token_id: 0
+      max_position_embeddings: 512
+      layer_norm_eps: 1.e-12
+      hidden_dropout_prob: 0.1
+      attention_probs_dropout_prob: 0.1
+      chunk_size_feed_forward: 0
+      cross_attention_freq: 2
+      intermediate_size: 3072
+      initializer_range: 0.02
+      hidden_act: "gelu"
+      dtype: "float32"
+      layernorm_dtype: "float32"
+      softmax_dtype: "float32"
+      compute_dtype: "float16"
+      add_cross_attention: True
+      use_relative_positions: False
+      tie_word_embeddings: True
+      output_attentions: False
+      output_hidden_states: False
+      use_return_dict: False
+      convert_param_from_bert: True
+      checkpoint_name_or_path: "bert_base_uncased"
+  arch:
+    type: Blip2Qformer
+
+# lr sechdule
+lr_schedule:
+  type: cosine
+  learning_rate: 1.e-4
+  lr_end: 1.e-5
+  warmup_lr_init: 1.e-6
+  warmup_steps: 5000
+  total_steps: -1 # -1 means it will load the total steps of the dataset
+layer_scale: False
+lr_scale: False
+
+# optimizer
+optimizer:
+  type: adamw
+  beta1: 0.9
+  beta2: 0.98
+  eps: 1.e-8
+  weight_decay: 0.05
+
 # callbacks
 callbacks:
   - type: MFLossMonitor
   - type: CheckpointMointor
-    prefix: "glm3-6b"
-    save_checkpoint_steps: 1000
-    keep_checkpoint_max: 1
-    integrated_save: False
+    prefix: "blip2_qformer"
+    save_checkpoint_steps: 7084
+    integrated_save: True
     async_save: False
   - type: ObsMonitor
-    keep_last: False
+    step_upload_frequence: 1000
 eval_callbacks:
   - type: ObsMonitor
-    keep_last: False
+
+# image processor, tokenizer for prediction
+processor:
+  type: Blip2Processor
+  image_processor:
+    type: Blip2ImageProcessor
+    image_size: 224
+    mean: [0.48145466, 0.4578275, 0.40821073]
+    std: [0.26862954, 0.26130258, 0.27577711]
+    is_hwc: False
+  tokenizer:
+    type: BertTokenizer
+    pad_token: '[PAD]'
+    bos_token: '[DEC]'
+    add_special_tokens: True
+    padding: 'max_length'
+    truncation: True
+    max_length: 32
```

## Comparing `configs/llama2/predict_llama2_13b.yaml` & `configs/llama2/predict_llama2_70b_910b.yaml`

 * *Files 10% similar despite different names*

```diff
@@ -6,112 +6,114 @@
 only_save_strategy: False
 resume_training: False
 run_mode: 'predict'
 
 # trainer config
 trainer:
   type: CausalLanguageModelingTrainer
-  model_name: 'llama2_13b'
+  model_name: 'llama_70b'
 # if True, do evaluate during the training process. if false, do nothing.
 # note that the task trainer should support _evaluate_in_training function.
 do_eval: False
+eval_step_interval: -1        # num of step intervals between each eval, -1 means no step end eval.
+eval_epoch_interval: 50        # num of epoch intervals between each eval, 1 means eval on every epoch end.
 
 # runner config
 runner_config:
   epochs: 2
   batch_size: 1
   sink_mode: True
   sink_size: 2
 
-# eval dataset
-eval_dataset: &eval_dataset
-  data_loader:
-    type: MindDataset
-    dataset_dir: ""
-    shuffle: False
-  input_columns: ["input_ids"]
-  num_parallel_workers: 8
-  python_multiprocessing: False
-  drop_remainder: False
-  repeat: 1
-  numa_enable: False
-  prefetch_size: 1
-eval_dataset_task:
-  type: CausalLanguageModelDataset
-  dataset_config: *eval_dataset
-
 use_parallel: True
 # parallel context config
 parallel:
   parallel_mode: 1 # 0-data parallel, 1-semi-auto parallel, 2-auto parallel, 3-hybrid parallel
   gradients_mean: False
   enable_alltoall: False
   full_batch: True
   search_mode: "sharding_propagation"
-  enable_parallel_optimizer: False
+  enable_parallel_optimizer: True
   strategy_ckpt_save_file: "./ckpt_strategy.ckpt"
   parallel_optimizer_config:
     gradient_accumulation_shard: False
     parallel_optimizer_threshold: 64
-# default parallel of device num = 16 for Atlas 800T A2
+# default parallel of device num = 32 for Atlas 800T A2
 parallel_config:
-  data_parallel: 8
-  model_parallel: 1 # write npu num as much as you need to export
+  data_parallel: 1
+  model_parallel: 8
   pipeline_stage: 1
   use_seq_parallel: False
-  micro_batch_num: 16
+  micro_batch_num: 1
   vocab_emb_dp: True
   gradient_aggregation_group: 4
 # when model parallel is greater than 1, we can set micro_batch_interleave_num=2, that may accelerate the train process.
 micro_batch_interleave_num: 1
 
+# callbacks
+callbacks:
+  - type: MFLossMonitor
+  - type: CheckpointMointor
+    prefix: "llama_70b"
+    save_checkpoint_steps: 1000
+    integrated_save: False
+    async_save: False
+  - type: ObsMonitor
+
 # mindspore context init config
 context:
   mode: 0 #0--Graph Mode; 1--Pynative Mode
   device_target: "Ascend"
   enable_graph_kernel: False
   graph_kernel_flags: "--disable_expand_ops=Softmax,Dropout --enable_parallel_fusion=true --reduce_fuse_depth=8 --enable_auto_tensor_inplace=true"
+  ascend_config:
+    precision_mode: "must_keep_origin_dtype"
   max_call_depth: 10000
   max_device_memory: "58GB"
   save_graphs: False
   save_graphs_path: "./graph"
   device_id: 0
-  ascend_config:
-    precision_mode: "must_keep_origin_dtype"
 
 # model config
 model:
   model_config:
     type: LlamaConfig
     batch_size: 1 # add for increase predict
     seq_length: 4096
-    hidden_size: 5120
-    num_layers: 40
-    num_heads: 40
+    hidden_size: 8192
+    num_layers: 80
+    num_heads: 64
     max_position_embedding: 4096
     vocab_size: 32000
     multiple_of: 256
+    n_kv_heads: 8
+    ffn_dim_multiplier: 1.3
     rms_norm_eps: 1.0e-5
     bos_token_id: 1
     eos_token_id: 2
     pad_token_id: 0
     ignore_token_id: -100
     compute_dtype: "float16"
-    layernorm_compute_type: "float16"
+    layernorm_compute_type: "float32"
     softmax_compute_type: "float16"
     rotary_dtype: "float16"
     param_init_type: "float16"
     use_past: True
+    scaling_factor: 1.0
     extend_method: "None" # support "None", "PI", "NTK"
-    use_flash_attention: True
+    use_flash_attention: False
+    use_paged_attention: False  # PA only supported in inference
     block_size: 16
     num_blocks: 512
-    is_dynamic: True
+    is_dynamic: False
+    use_kvcache_op: False
+    is_flexible_shape: False
     offset: 0
-    checkpoint_name_or_path: ""
+    use_rope_slice: False
+    checkpoint_name_or_path: "llama2_70b"
     repetition_penalty: 1
     max_decode_length: 512
     top_k: 3
     top_p: 1
     do_sample: False
   arch:
     type: LlamaForCausalLM
@@ -120,29 +122,20 @@
   return_tensors: ms
   tokenizer:
     unk_token: '<unk>'
     bos_token: '<s>'
     eos_token: '</s>'
     pad_token: '<unk>'
     type: LlamaTokenizer
+    vocab_file: ""
   type: LlamaProcessor
 
 # metric
 metric:
-  type: EmF1Metric
-
-# wrapper cell config
-runner_wrapper:
-  type: MFTrainOneStepCell
-  scale_sense:
-    type: DynamicLossScaleUpdateCell
-    loss_scale_value: 65536
-    scale_factor: 2
-    scale_window: 1000
-  use_clip_grad: True
+  type: PerplexityMetric
 
 eval_callbacks:
   - type: ObsMonitor
 
 auto_tune: False
 filepath_prefix: './autotune'
 autotune_per_step: 10
@@ -154,8 +147,8 @@
 profile_communication: False
 profile_memory: True
 layer_scale: False
 layer_decay: 0.65
 lr_scale_factor: 256
 
 # aicc
-remote_save_url: "Please input obs url on AICC platform."
+remote_save_url: "Please input obs url on AICC platform."
```

## Comparing `configs/llama2/predict_llama2_70b.yaml` & `configs/blip2/run_blip2_stage1_vit_g_zero_shot_image_classification_cifar100.yaml`

 * *Files 23% similar despite different names*

```diff
@@ -1,166 +1,213 @@
 seed: 0
+run_mode: 'predict'
 output_dir: './output' # path to save checkpoint/strategy
 load_checkpoint: ''
 src_strategy_path_or_dir: ''
 auto_trans_ckpt: False  # If true, auto transform load_checkpoint to load in distributed model
 only_save_strategy: False
 resume_training: False
-run_mode: 'predict'
 
-# trainer config
-trainer:
-  type: CausalLanguageModelingTrainer
-  model_name: 'llama2_70b'
-# if True, do evaluate during the training process. if false, do nothing.
-# note that the task trainer should support _evaluate_in_training function.
-do_eval: False
-eval_step_interval: -1        # num of step intervals between each eval, -1 means no step end eval.
-eval_epoch_interval: 50        # num of epoch intervals between each eval, 1 means eval on every epoch end.
+# context
+context:
+  mode: 0 #0--Graph Mode; 1--Pynative Mode
+  device_target: "Ascend"
+  enable_graph_kernel: False
+  graph_kernel_flags: "--opt_level=0"
+  max_call_depth: 10000
+  save_graphs: False
+  device_id: 0
 
-# runner config
+# aicc
+remote_save_url: "Please input obs url on AICC platform."
+
+# runner
 runner_config:
-  epochs: 2
-  batch_size: 1
-  sink_mode: True
+  epochs: 5
+  batch_size: 40
+  image_size: 224
+  sink_mode: False
   sink_size: 2
 
-# eval dataset
-eval_dataset: &eval_dataset
-  data_loader:
-    type: MindDataset
-    dataset_dir: ""
-    shuffle: False
-  input_columns: ["input_ids"]
-  num_parallel_workers: 8
-  python_multiprocessing: False
-  drop_remainder: False
-  repeat: 1
-  numa_enable: False
-  prefetch_size: 1
-eval_dataset_task:
-  type: CausalLanguageModelDataset
-  dataset_config: *eval_dataset
-
-use_parallel: True
-# parallel context config
+# parallel
+use_parallel: False
 parallel:
-  parallel_mode: 1 # 0-data parallel, 1-semi-auto parallel, 2-auto parallel, 3-hybrid parallel
-  gradients_mean: False
+  parallel_mode: 0 # 0-dataset, 1-semi, 2-auto, 3-hybrid
+  gradients_mean: True
   enable_alltoall: False
-  full_batch: True
+  full_batch: False
   search_mode: "sharding_propagation"
-  enable_parallel_optimizer: True
+  enable_parallel_optimizer: False
   strategy_ckpt_save_file: "./ckpt_strategy.ckpt"
-  parallel_optimizer_config:
-    gradient_accumulation_shard: False
-    parallel_optimizer_threshold: 64
-# default parallel of device num = 32 for Atlas 800T A2
 parallel_config:
   data_parallel: 1
-  model_parallel: 8
+  model_parallel: 1
+  expert_parallel: 1
   pipeline_stage: 1
-  use_seq_parallel: False
   micro_batch_num: 1
-  vocab_emb_dp: True
   gradient_aggregation_group: 4
-# when model parallel is greater than 1, we can set micro_batch_interleave_num=2, that may accelerate the train process.
 micro_batch_interleave_num: 1
 
-# callbacks
-callbacks:
-  - type: MFLossMonitor
-  - type: CheckpointMointor
-    prefix: "llama2_70b"
-    save_checkpoint_steps: 1000
-    integrated_save: False
-    async_save: False
-  - type: ObsMonitor
-
-# mindspore context init config
-context:
-  mode: 0 #0--Graph Mode; 1--Pynative Mode
-  device_target: "Ascend"
-  enable_graph_kernel: False
-  graph_kernel_flags: "--disable_expand_ops=Softmax,Dropout --enable_parallel_fusion=true --reduce_fuse_depth=8 --enable_auto_tensor_inplace=true"
-  ascend_config:
-    precision_mode: "must_keep_origin_dtype"
-  max_call_depth: 10000
-  max_device_memory: "58GB"
-  save_graphs: False
-  save_graphs_path: "./graph"
-  device_id: 0
-
-# model config
-model:
-  model_config:
-    type: LlamaConfig
-    batch_size: 1 # add for increase predict
-    seq_length: 4096
-    hidden_size: 8192
-    num_layers: 80
-    num_heads: 64
-    max_position_embedding: 4096
-    vocab_size: 32000
-    multiple_of: 256
-    n_kv_heads: 8
-    ffn_dim_multiplier: 1.3
-    rms_norm_eps: 1.0e-5
-    bos_token_id: 1
-    eos_token_id: 2
-    pad_token_id: 0
-    ignore_token_id: -100
-    compute_dtype: "float16"
-    layernorm_compute_type: "float16"
-    softmax_compute_type: "float16"
-    rotary_dtype: "float16"
-    param_init_type: "float16"
-    use_past: True
-    scaling_factor: 1.0
-    extend_method: "None" # support "None", "PI", "NTK"
-    use_flash_attention: True
-    block_size: 16
-    num_blocks: 512
-    is_dynamic: True
-    offset: 0
-    checkpoint_name_or_path: ""
-    repetition_penalty: 1
-    max_decode_length: 512
-    top_k: 3
-    top_p: 1
-    do_sample: False
-  arch:
-    type: LlamaForCausalLM
-
-processor:
-  return_tensors: ms
-  tokenizer:
-    unk_token: '<unk>'
-    bos_token: '<s>'
-    eos_token: '</s>'
-    pad_token: '<unk>'
-    type: LlamaTokenizer
-  type: LlamaProcessor
-
-# metric
-metric:
-  type: EmF1Metric
-
-eval_callbacks:
-  - type: ObsMonitor
+# recompute
+recompute_config:
+  recompute: False
+  parallel_optimizer_comm_recompute: False
+  mp_comm_recompute: True
+  recompute_slice_activation: False
 
+# autotune
 auto_tune: False
 filepath_prefix: './autotune'
 autotune_per_step: 10
 
+# profile
 profile: False
 profile_start_step: 1
 profile_stop_step: 10
 init_start_profile: False
 profile_communication: False
 profile_memory: True
+
+# Trainer
+trainer:
+  type: ZeroShotImageClassificationTrainer
+  model_name: 'blip2_stage1_classification'
+
+# eval dataset
+eval_dataset:  &eval_dataset
+  data_loader:
+    type: Cifar100DataLoader
+    dataset_dir: 'cifar-100-python'
+    column_names: ["image", "text", "label"]
+    stage: 'test'
+    fine_label: True
+    shuffle: False
+    hypothesis_template: "a picture of {}"
+  num_parallel_workers: 1
+  python_multiprocessing: False
+  drop_remainder: True
+  batch_size: 40
+  repeat: 1
+  numa_enable: False
+  prefetch_size: 30
+  seed: 2022
+eval_dataset_task:
+  type: ZeroShotImageClassificationDataset
+  dataset_config: *eval_dataset
+
+# model
+model:
+  model_config:
+    type: Blip2Config
+    freeze_vision: True
+    max_txt_len: 32
+    checkpoint_name_or_path: "blip2_stage1_classification"
+    dtype: "float32"
+    compute_dtype: "float16"
+    layernorm_dtype: "float32"
+    softmax_dtype: "float32"
+    vision_config:
+      type: ViTConfig
+      image_size: 224
+      patch_size: 14
+      num_channels: 3
+      initializer_range: 0.001
+      hidden_size: 1408
+      num_hidden_layers: 39
+      num_attention_heads: 16
+      intermediate_size: 6144
+      qkv_bias: true
+      hidden_act: gelu
+      post_layernorm_residual: false
+      layer_norm_eps: 1.0e-6
+      attention_probs_dropout_prob: 0.0
+      hidden_dropout_prob: 0.0
+      drop_path_rate: 0.0
+      use_mean_pooling: false
+      encoder_stride: 16
+      checkpoint_name_or_path: "vit_g_p16"
+
+    qformer_config:
+      num_hidden_layers: 12
+      num_attention_heads: 12
+      query_length: 32
+      vocab_size: 30522
+      hidden_size: 768
+      encoder_width: 1408
+      bos_token_id: 30522
+      sep_token_id: 102
+      pad_token_id: 0
+      max_position_embeddings: 512
+      layer_norm_eps: 1.e-12
+      hidden_dropout_prob: 0.1
+      attention_probs_dropout_prob: 0.1
+      chunk_size_feed_forward: 0
+      cross_attention_freq: 2
+      intermediate_size: 3072
+      initializer_range: 0.02
+      hidden_act: "gelu"
+      dtype: "float32"
+      layernorm_dtype: "float32"
+      softmax_dtype: "float32"
+      compute_dtype: "float16"
+      add_cross_attention: True
+      use_relative_positions: False
+      tie_word_embeddings: True
+      output_attentions: False
+      output_hidden_states: False
+      use_return_dict: False
+  arch:
+    type: Blip2Classifier
+
+# metric
+metric:
+  type: 'Accuracy'
+
+# lr sechdule
+lr_schedule:
+  type: cosine
+  learning_rate: 1.e-4
+  lr_end: 1.e-5
+  warmup_lr_init: 1.e-6
+  warmup_steps: 5000
+  total_steps: -1 # -1 means it will load the total steps of the dataset
 layer_scale: False
-layer_decay: 0.65
-lr_scale_factor: 256
+lr_scale: False
 
-# aicc
-remote_save_url: "Please input obs url on AICC platform."
+# optimizer
+optimizer:
+  type: adamw
+  beta1: 0.9
+  beta2: 0.98
+  eps: 1.e-8
+  weight_decay: 0.05
+
+# callbacks
+callbacks:
+  - type: MFLossMonitor
+  - type: CheckpointMointor
+    prefix: "blip2_stage1_classification"
+    save_checkpoint_steps: 10000
+    integrated_save: True
+    async_save: False
+  - type: ObsMonitor
+eval_callbacks:
+  - type: ObsMonitor
+
+# image processor, tokenizer for prediction
+processor:
+  type: Blip2Processor
+  image_processor:
+    type: Blip2ImageProcessor
+    image_size: [224, 224]
+    interpolation: "bicubic"
+    mean: [0.48145466, 0.4578275, 0.40821073]
+    std: [0.26862954, 0.26130258, 0.27577711]
+    is_hwc: False
+  tokenizer:
+    type: BertTokenizer
+    pad_token: '[PAD]'
+    bos_token: '[DEC]'
+    add_special_tokens: True
+    padding: 'max_length'
+    truncation: True
+    max_length: 32
```

## Comparing `configs/llama2/predict_llama2_7b.yaml` & `configs/blip2/run_blip2_stage2_vit_g_baichuan_7b_image_to_text_generation.yaml`

 * *Files 14% similar despite different names*

```diff
@@ -1,212 +1,198 @@
-seed: 0
+seed: 42
+run_mode: 'predict'
 output_dir: './output' # path to save checkpoint/strategy
 load_checkpoint: ''
 src_strategy_path_or_dir: ''
 auto_trans_ckpt: False  # If true, auto transform load_checkpoint to load in distributed model
 only_save_strategy: False
 resume_training: False
-run_mode: 'predict'
 
-# trainer config
-trainer:
-  type: CausalLanguageModelingTrainer
-  model_name: 'llama2_7b'
+# context
+context:
+  mode: 0 #0--Graph Mode; 1--Pynative Mode
+  device_target: "Ascend"
+  enable_graph_kernel: False
+  graph_kernel_flags: str = "--disable_expand_ops=Softmax,Dropout " \
+                              "--enable_parallel_fusion=true --reduce_fuse_depth=8 --enable_auto_tensor_inplace=true"
+  max_call_depth: 10000
+  save_graphs: False
+  save_graphs_path: "./graph"
+  device_id: 0
 
-# runner config
+# aicc
+remote_save_url: "Please input obs url on AICC platform."
+
+# runner
 runner_config:
-  epochs: 2
-  batch_size: 1
-  sink_mode: True
+  epochs: 10
+  batch_size: &batch_size 1
   sink_size: 2
-  gradient_accumulation_steps: 8
-
-# optimizer
-optimizer:
-  type: FP32StateAdamWeightDecay
-  beta1: 0.9
-  beta2: 0.95
-  eps: 1.e-8
-  learning_rate: 5.e-5
-
-# lr sechdule
-lr_schedule:
-  type: CosineWithWarmUpLR
-  learning_rate: 5.e-5
-  lr_end: 0
-  warmup_ratio: 0.03
-  total_steps: -1 # -1 means it will load the total steps of the dataset
-
-# dataset
-train_dataset: &train_dataset
-  data_loader:
-    type: MindDataset
-    dataset_dir: ""
-    shuffle: True
-  input_columns: ["input_ids"]  # "input_ids", "labels" , labels are used in instruction finetune.
-  num_parallel_workers: 8
-  python_multiprocessing: False
-  drop_remainder: True
-  batch_size: 6
-  repeat: 1
-  numa_enable: False
-  prefetch_size: 1
-train_dataset_task:
-  type: CausalLanguageModelDataset
-  dataset_config: *train_dataset
-# if True, do evaluate during the training process. if false, do nothing.
-# note that the task trainer should support _evaluate_in_training function.
-do_eval: False
-
-# eval dataset
-eval_dataset: &eval_dataset
-  data_loader:
-    type: MindDataset
-    dataset_dir: ""
-    shuffle: False
-  input_columns: ["input_ids"]
-  num_parallel_workers: 8
-  python_multiprocessing: False
-  drop_remainder: False
-  repeat: 1
-  numa_enable: False
-  prefetch_size: 1
-eval_dataset_task:
-  type: CausalLanguageModelDataset
-  dataset_config: *eval_dataset
+  image_size: 224
 
+# parallel
 use_parallel: False
-# parallel context config
 parallel:
-  parallel_mode: 1 # 0-data parallel, 1-semi-auto parallel, 2-auto parallel, 3-hybrid parallel
-  gradients_mean: False
-  enable_alltoall: False
-  full_batch: True
+  parallel_mode: 0 # 0-dataset, 1-semi, 2-auto, 3-hybrid
+  gradients_mean: True
   search_mode: "sharding_propagation"
-  enable_parallel_optimizer: True
-  strategy_ckpt_save_file: "./ckpt_strategy.ckpt"
-  parallel_optimizer_config:
-    gradient_accumulation_shard: False
-    parallel_optimizer_threshold: 64
-# default parallel of device num = 8 for Atlas 800T A2
+  enable_parallel_optimizer: False
+  full_batch: False
 parallel_config:
-  data_parallel: 8
+  data_parallel: 1
   model_parallel: 1
   pipeline_stage: 1
-  use_seq_parallel: False
   micro_batch_num: 1
   vocab_emb_dp: True
   gradient_aggregation_group: 4
-# when model parallel is greater than 1, we can set micro_batch_interleave_num=2, that may accelerate the train process.
 micro_batch_interleave_num: 1
 
-# recompute config
+# recompute
 recompute_config:
   recompute: False
-  select_recompute: False
   parallel_optimizer_comm_recompute: False
   mp_comm_recompute: True
-  recompute_slice_activation: True
+  recompute_slice_activation: False
 
-# callbacks
-callbacks:
-  - type: MFLossMonitor
-  - type: CheckpointMointor
-    prefix: "llama2_7b"
-    save_checkpoint_steps: 100
-    integrated_save: False
-    async_save: False
-  - type: ObsMonitor
+# autotune
+auto_tune: False
+filepath_prefix: './autotune'
+autotune_per_step: 10
 
-# mindspore context init config
-context:
-  mode: 0 #0--Graph Mode; 1--Pynative Mode
-  device_target: "Ascend"
-  enable_graph_kernel: False
-  graph_kernel_flags: "--disable_expand_ops=Softmax,Dropout --enable_parallel_fusion=true --reduce_fuse_depth=8 --enable_auto_tensor_inplace=true"
-  max_call_depth: 10000
-  max_device_memory: "58GB"
-  save_graphs: False
-  save_graphs_path: "./graph"
-  device_id: 0
+# profile
+profile: False
+profile_start_step: 1
+profile_stop_step: 10
+init_start_profile: False
+profile_communication: False
+profile_memory: True
+
+# Trainer
+trainer:
+  type: ImageToTextGenerationTrainer
+  model_name: 'itt_blip2_stage2_vit_g_baichuan_7b'
 
-# model config
+# train dataset
+eval_dataset:  &eval_dataset
+  data_loader:
+    type: Flickr8kDataLoader
+    dataset_dir: "./checkpoint_download/Flickr8k"
+    stage: "train"
+    column_names: [ "image", "text"]
+    hypothesis_template: "{}"
+  transforms:
+    - type: ToPIL
+    - type: Resize
+      size: 224
+      interpolation: 'linear'
+    - type: CenterCrop
+      size: 224
+    - type: ToTensor
+    - type: Normalize
+      mean: [0.48145466, 0.4578275, 0.40821073]
+      std: [0.26862954, 0.26130258, 0.27577711]
+      is_hwc: False
+  num_parallel_workers: 8
+  python_multiprocessing: False
+  drop_remainder: False
+  batch_size: 32
+  repeat: 1
+  numa_enable: False
+  prefetch_size: 30
+  seed: 2022
+
+eval_dataset_task:
+  type: ZeroShotImageClassificationDataset
+  dataset_config: *eval_dataset
+# model
 model:
   model_config:
-    type: LlamaConfig
-    batch_size: 1 # add for increase predict
-    seq_length: 4096
-    hidden_size: 4096
-    num_layers: 32
-    num_heads: 32
-    vocab_size: 32000
-    multiple_of: 256
-    rms_norm_eps: 1.0e-5
-    bos_token_id: 1
-    eos_token_id: 2
-    pad_token_id: 0
-    ignore_token_id: -100
+    type: Blip2Config
+    batch_size: *batch_size
+    freeze_vision: True
+    freeze_text: True
+    max_txt_len: 32
+    checkpoint_name_or_path: ""
+    dtype: "float32"
     compute_dtype: "float16"
-    layernorm_compute_type: "float16"
-    softmax_compute_type: "float16"
-    rotary_dtype: "float16"
-    param_init_type: "float16"
-    use_past: True
-    scaling_factor: 1.0 # The scale factor of seq length
-    extend_method: "None" # support "None", "PI", "NTK"
-    use_flash_attention: True # FA can accelerate training or finetune
-    block_size: 16
-    num_blocks: 1024
-    is_dynamic: True
-    offset: 0
-    checkpoint_name_or_path: "llama2_7b"
-    repetition_penalty: 1
-    max_decode_length: 512
-    top_k: 3
-    top_p: 1
-    do_sample: False
+    layernorm_dtype: "float32"
+    softmax_dtype: "float32"
+    vision_config:
+      type: ViTConfig
+      image_size: 224
+      patch_size: 14
+      num_channels: 3
+      initializer_range: 0.001
+      hidden_size: 1408
+      num_hidden_layers: 39
+      num_attention_heads: 16
+      intermediate_size: 6144
+      qkv_bias: true
+      hidden_act: gelu
+      post_layernorm_residual: false
+      layer_norm_eps: 1.0e-6
+      attention_probs_dropout_prob: 0.0
+      hidden_dropout_prob: 0.0
+      drop_path_rate: 0.0
+      use_mean_pooling: false
+      encoder_stride: 16
+      checkpoint_name_or_path: "vit_g_p16"
+
+    text_config:
+      type: LlamaConfig
+      seq_length: 64  # sum of max_txt_len and num_query_token
+      hidden_size: 4096
+      num_layers: 32
+      num_heads: 32
+      vocab_size: 64001
+      multiple_of: 256
+      rms_norm_eps: 1.0e-6
+      bos_token_id: 1
+      eos_token_id: 2
+      pad_token_id: 0
+      ignore_token_id: -100
+      compute_dtype: "float16"
+      layernorm_compute_type: "float32"
+      softmax_compute_type: "float32"
+      rotary_dtype: "float16"
+      param_init_type: "float16"
+      use_past: True
+      offset: 0
+      repetition_penalty: 1
+      max_decode_length: 512
+      top_k: 1
+      top_p: 1
+      do_sample: False
+      checkpoint_name_or_path: ""
   arch:
-    type: LlamaForCausalLM
+    type: Blip2ImageToTextGeneration
 
+# processor
 processor:
-  return_tensors: ms
+  type: Blip2Processor
+  image_processor:
+    type: Blip2ImageProcessor
+    image_size: 224  # input image size
   tokenizer:
-    unk_token: '<unk>'
+    type: LlamaTokenizer
+    pad_token: '<pad>'
     bos_token: '<s>'
+    unk_token: '</s>'
     eos_token: '</s>'
-    pad_token: '<unk>'
-    type: LlamaTokenizer
-  type: LlamaProcessor
+    add_special_tokens: False
+    padding: 'max_length'
+    truncation: True
+    max_length: 32
+    vocab_file: ""
 
-# metric
-metric:
-  type: EmF1Metric
-
-# wrapper cell config
-runner_wrapper:
-  type: MFTrainOneStepCell
-  scale_sense:
-    type: DynamicLossScaleUpdateCell
-    loss_scale_value: 65536
-    scale_factor: 2
-    scale_window: 1000
-  use_clip_grad: True
 
+# callbacks
+callbacks:
+  - type: MFLossMonitor
+  - type: CheckpointMointor
+    prefix: "mindformers"
+    save_checkpoint_steps: 10000
+    integrated_save: True
+    async_save: False
+  - type: ObsMonitor
 eval_callbacks:
   - type: ObsMonitor
-
-auto_tune: False
-filepath_prefix: './autotune'
-autotune_per_step: 10
-
-profile: False
-profile_start_step: 1
-profile_stop_step: 10
-init_start_profile: False
-profile_communication: False
-profile_memory: True
-layer_scale: False
-layer_decay: 0.65
-lr_scale_factor: 256
-
-# aicc
-remote_save_url: "Please input obs url on AICC platform."
```

## Comparing `configs/llama2/run_llama2_13b_bf16_800T_A2_finetune.yaml` & `configs/blip2/run_blip2_stage2_vit_g_llama_7b_image_to_text_generation.yaml`

 * *Files 13% similar despite different names*

```diff
@@ -1,206 +1,198 @@
-seed: 0
+seed: 42
+run_mode: 'predict'
 output_dir: './output' # path to save checkpoint/strategy
-load_checkpoint: '{path}/llama2_13b.ckpt'
+load_checkpoint: ''
 src_strategy_path_or_dir: ''
 auto_trans_ckpt: False  # If true, auto transform load_checkpoint to load in distributed model
 only_save_strategy: False
 resume_training: False
-run_mode: 'finetune'
 
-# trainer config
-trainer:
-  type: CausalLanguageModelingTrainer
-  model_name: 'llama2_13b'
-# if True, do evaluate during the training process. if false, do nothing.
-# note that the task trainer should support _evaluate_in_training function.
-do_eval: False
+# context
+context:
+  mode: 0 #0--Graph Mode; 1--Pynative Mode
+  device_target: "Ascend"
+  enable_graph_kernel: False
+  graph_kernel_flags: str = "--disable_expand_ops=Softmax,Dropout " \
+                              "--enable_parallel_fusion=true --reduce_fuse_depth=8 --enable_auto_tensor_inplace=true"
+  max_call_depth: 10000
+  save_graphs: False
+  save_graphs_path: "./graph"
+  device_id: 0
+
+# aicc
+remote_save_url: "Please input obs url on AICC platform."
 
-# runner config
+# runner
 runner_config:
-  epochs: 2
-  batch_size: 4
-  sink_mode: True
+  epochs: 10
+  batch_size: &batch_size 1
   sink_size: 2
+  image_size: 224
 
-# optimizer
-optimizer:
-  type: FP32StateAdamWeightDecay
-  beta1: 0.9
-  beta2: 0.95
-  eps: 1.e-8 # 1e-8
-
-# lr sechdule
-lr_schedule:
-  type: CosineWithWarmUpLR
-  learning_rate: 1.e-5
-  lr_end: 0.0
-  warmup_ratio: 0.03
-  total_steps: -1 # -1 means it will load the total steps of the dataset
-
-# dataset
-train_dataset: &train_dataset
-  data_loader:
-    type: MindDataset
-    dataset_dir: ""
-    shuffle: True
-  input_columns: ["input_ids", "labels"]  # "input_ids", "labels" , labels are used in instruction finetune.
-  num_parallel_workers: 8
-  python_multiprocessing: False
-  drop_remainder: True
-  batch_size: 4
-  repeat: 1
-  numa_enable: False
-  prefetch_size: 1
-train_dataset_task:
-  type: CausalLanguageModelDataset
-  dataset_config: *train_dataset
-
-# eval dataset
-eval_dataset: &eval_dataset
-  data_loader:
-    type: MindDataset
-    dataset_dir: ""
-    shuffle: False
-  input_columns: ["input_ids", "labels"]
-  num_parallel_workers: 8
-  python_multiprocessing: False
-  drop_remainder: False
-  repeat: 1
-  numa_enable: False
-  prefetch_size: 1
-eval_dataset_task:
-  type: CausalLanguageModelDataset
-  dataset_config: *eval_dataset
-
-use_parallel: True
-# parallel context config
+# parallel
+use_parallel: False
 parallel:
-  parallel_mode: 1 # 0-data parallel, 1-semi-auto parallel, 2-auto parallel, 3-hybrid parallel
-  gradients_mean: False
-  enable_alltoall: False
-  full_batch: True
+  parallel_mode: 0 # 0-dataset, 1-semi, 2-auto, 3-hybrid
+  gradients_mean: True
   search_mode: "sharding_propagation"
-  enable_parallel_optimizer: True
-  strategy_ckpt_save_file: "./ckpt_strategy.ckpt"
-  parallel_optimizer_config:
-    gradient_accumulation_shard: False
-    parallel_optimizer_threshold: 64
-# default parallel of device num = 16 for Atlas 800T A2
+  enable_parallel_optimizer: False
+  full_batch: False
 parallel_config:
-  data_parallel: 8
+  data_parallel: 1
   model_parallel: 1
   pipeline_stage: 1
-  use_seq_parallel: False
   micro_batch_num: 1
   vocab_emb_dp: True
   gradient_aggregation_group: 4
-# when model parallel is greater than 1, we can set micro_batch_interleave_num=2, that may accelerate the train process.
 micro_batch_interleave_num: 1
 
-# recompute config
+# recompute
 recompute_config:
-  recompute: True
-  select_recompute: False
+  recompute: False
   parallel_optimizer_comm_recompute: False
   mp_comm_recompute: True
-  recompute_slice_activation: True
+  recompute_slice_activation: False
 
-# callbacks
-callbacks:
-  - type: MFLossMonitor
-  - type: CheckpointMointor
-    prefix: "llama2_13b"
-    save_checkpoint_steps: 100
-    integrated_save: False
-    async_save: False
-  - type: ObsMonitor
+# autotune
+auto_tune: False
+filepath_prefix: './autotune'
+autotune_per_step: 10
 
-# mindspore context init config
-context:
-  mode: 0 #0--Graph Mode; 1--Pynative Mode
-  device_target: "Ascend"
-  enable_graph_kernel: False
-  graph_kernel_flags: "--disable_expand_ops=Softmax,Dropout --enable_parallel_fusion=true --reduce_fuse_depth=8 --enable_auto_tensor_inplace=true"
-  max_call_depth: 10000
-  max_device_memory: "59GB"
-  save_graphs: False
-  save_graphs_path: "./graph"
-  device_id: 0
-  runtime_num_threads: 1
+# profile
+profile: False
+profile_start_step: 1
+profile_stop_step: 10
+init_start_profile: False
+profile_communication: False
+profile_memory: True
+
+# Trainer
+trainer:
+  type: ImageToTextGenerationTrainer
+  model_name: "itt_blip2_stage2_vit_g_llama_7b"
+
+# eval dataset
+eval_dataset:  &eval_dataset
+  data_loader:
+    type: Flickr8kDataLoader
+    dataset_dir: "./checkpoint_download/Flickr8k"
+    stage: "train"
+    column_names: [ "image", "text"]
+    hypothesis_template: "{}"
+  transforms:
+    - type: ToPIL
+    - type: Resize
+      size: 224
+      interpolation: "linear"
+    - type: CenterCrop
+      size: 224
+    - type: ToTensor
+    - type: Normalize
+      mean: [0.48145466, 0.4578275, 0.40821073]
+      std: [0.26862954, 0.26130258, 0.27577711]
+      is_hwc: False
+  num_parallel_workers: 8
+  python_multiprocessing: False
+  drop_remainder: False
+  batch_size: 32
+  repeat: 1
+  numa_enable: False
+  prefetch_size: 30
+  seed: 2022
 
-# model config
+eval_dataset_task:
+  type: ZeroShotImageClassificationDataset
+  dataset_config: *eval_dataset
+# model
 model:
   model_config:
-    type: LlamaConfig
-    batch_size: 1 # add for increase predict
-    seq_length: 4096
-    hidden_size: 5120
-    num_layers: 40
-    num_heads: 40
-    vocab_size: 32000
-    multiple_of: 256
-    rms_norm_eps: 1.0e-5
-    bos_token_id: 1
-    eos_token_id: 2
-    pad_token_id: 0
-    ignore_token_id: -100
-    compute_dtype: "bfloat16"
-    layernorm_compute_type: "float32"
-    softmax_compute_type: "float16"
-    rotary_dtype: "float32"
-    param_init_type: "float16"
-    embedding_init_type: "bfloat16"
-    use_past: False
-    scaling_factor: 1.0
-    extend_method: "None" # support "None", "PI", "NTK"
-    use_flash_attention: True # FA can accelerate training or finetune
-    offset: 0
-    checkpoint_name_or_path: "llama2_13b"
-    repetition_penalty: 1
-    max_decode_length: 512
-    top_k: 3
-    top_p: 1
-    do_sample: False
+    type: Blip2Config
+    batch_size: *batch_size
+    freeze_vision: True
+    freeze_text: True
+    max_txt_len: 32
+    checkpoint_name_or_path: "itt_blip2_stage2_vit_g_llama_7b"
+    dtype: "float32"
+    compute_dtype: "float16"
+    layernorm_dtype: "float32"
+    softmax_dtype: "float32"
+    vision_config:
+      type: ViTConfig
+      image_size: 224
+      patch_size: 14
+      num_channels: 3
+      initializer_range: 0.001
+      hidden_size: 1408
+      num_hidden_layers: 39
+      num_attention_heads: 16
+      intermediate_size: 6144
+      qkv_bias: True
+      hidden_act: gelu
+      post_layernorm_residual: false
+      layer_norm_eps: 1.0e-6
+      attention_probs_dropout_prob: 0.0
+      hidden_dropout_prob: 0.0
+      drop_path_rate: 0.0
+      use_mean_pooling: false
+      encoder_stride: 16
+      checkpoint_name_or_path: "vit_g_p16"
+
+    text_config:
+      type: LlamaConfig
+      seq_length: 64  # sum of max_txt_len and num_query_token
+      hidden_size: 4096
+      num_layers: 32
+      num_heads: 32
+      vocab_size: 32000
+      multiple_of: 256
+      rms_norm_eps: 1.0e-6
+      bos_token_id: 1
+      eos_token_id: 2
+      pad_token_id: 0
+      ignore_token_id: -100
+      compute_dtype: "float16"
+      layernorm_compute_type: "float32"
+      softmax_compute_type: "float16"
+      rotary_dtype: "float16"
+      param_init_type: "float16"
+      use_past: True
+      offset: 0
+      repetition_penalty: 1
+      max_decode_length: 512
+      top_k: 1
+      top_p: 1
+      do_sample: False
+      checkpoint_name_or_path: "llama_7b"
   arch:
-    type: LlamaForCausalLM
+    type: Blip2ImageToTextGeneration
 
+# processor
 processor:
-  return_tensors: ms
+  type: Blip2Processor
+  image_processor:
+    type: Blip2ImageProcessor
+    image_size: 224  # input image size
   tokenizer:
-    unk_token: '<unk>'
+    type: LlamaTokenizer
+    pad_token: '<unk>'
     bos_token: '<s>'
+    unk_token: '</s>'
     eos_token: '</s>'
-    pad_token: '<unk>'
-    type: LlamaTokenizer
-  type: LlamaProcessor
+    add_special_tokens: False
+    padding: 'max_length'
+    truncation: True
+    max_length: 32
+    checkpoint_name_or_path: "llama_7b"
 
-# metric
-metric:
-  type: PerplexityMetric
-
-# wrapper cell config
-runner_wrapper:
-  type: MFTrainOneStepCell
-  scale_sense: 1
-  use_clip_grad: True
-  enable_check_overflow: False
 
+# callbacks
+callbacks:
+  - type: MFLossMonitor
+  - type: CheckpointMointor
+    prefix: "mindformers"
+    save_checkpoint_steps: 10000
+    integrated_save: True
+    async_save: False
+  - type: ObsMonitor
 eval_callbacks:
   - type: ObsMonitor
-
-auto_tune: False
-filepath_prefix: './autotune'
-autotune_per_step: 10
-
-profile: False
-profile_start_step: 1
-profile_stop_step: 10
-init_start_profile: False
-profile_communication: False
-profile_memory: True
-layer_scale: False
-layer_decay: 0.65
-lr_scale_factor: 256
-
-# aicc
-remote_save_url: "Please input obs url on AICC platform."
```

## Comparing `configs/llama2/run_llama2_70b_bf16_800T_A2.yaml` & `configs/llama2/run_llama2_70b_bf16_910b.yaml`

 * *Files 16% similar despite different names*

```diff
@@ -1,210 +1,211 @@
-seed: 0
-output_dir: './output' # path to save checkpoint/strategy
-load_checkpoint: ''
-src_strategy_path_or_dir: ''
-auto_trans_ckpt: False  # If true, auto transform load_checkpoint to load in distributed model
-only_save_strategy: False
-resume_training: False
-run_mode: 'train'
-
-# trainer config
-trainer:
-  type: CausalLanguageModelingTrainer
-  model_name: 'llama2_70b'
-# if True, do evaluate during the training process. if false, do nothing.
-# note that the task trainer should support _evaluate_in_training function.
-do_eval: False
-eval_step_interval: -1        # num of step intervals between each eval, -1 means no step end eval.
-eval_epoch_interval: 50        # num of epoch intervals between each eval, 1 means eval on every epoch end.
-
-# runner config
-runner_config:
-  epochs: 3
-  batch_size: 1
-  sink_mode: True
-  sink_size: 2
-# optimizer
-optimizer:
-  type: FP32StateAdamWeightDecay
-  beta1: 0.9
-  beta2: 0.95
-  eps: 1.e-8 # 1e-8
-
-# lr sechdule
-lr_schedule:
-  type: CosineWithWarmUpLR
-  learning_rate: 1.e-5
-  lr_end: 0.0
-  warmup_ratio: 0.03
-  total_steps: -1 # -1 means it will load the total steps of the dataset
-
-# dataset
-train_dataset: &train_dataset
-  data_loader:
-    type: MindDataset
-    dataset_dir: ""
-    shuffle: True
-  input_columns: ["input_ids"]  # "input_ids", "labels" , labels are used in instruction finetune.
-  num_parallel_workers: 8
-  python_multiprocessing: False
-  drop_remainder: True
-  batch_size: 1
-  repeat: 1
-  numa_enable: False
-  prefetch_size: 1
-train_dataset_task:
-  type: CausalLanguageModelDataset
-  dataset_config: *train_dataset
-
-# eval dataset
-eval_dataset: &eval_dataset
-  data_loader:
-    type: MindDataset
-    dataset_dir: ""
-    shuffle: False
-  input_columns: ["input_ids"]
-  num_parallel_workers: 8
-  python_multiprocessing: False
-  drop_remainder: False
-  repeat: 1
-  numa_enable: False
-  prefetch_size: 1
-eval_dataset_task:
-  type: CausalLanguageModelDataset
-  dataset_config: *eval_dataset
-
-use_parallel: True
-# parallel context config
-parallel:
-  parallel_mode: 1 # 0-data parallel, 1-semi-auto parallel, 2-auto parallel, 3-hybrid parallel
-  gradients_mean: False
-  enable_alltoall: False
-  full_batch: True
-  search_mode: "sharding_propagation"
-  enable_parallel_optimizer: True
-  strategy_ckpt_save_file: "./ckpt_strategy.ckpt"
-  parallel_optimizer_config:
-    gradient_accumulation_shard: False
-    parallel_optimizer_threshold: 64
-# default parallel of device num = 32 for Atlas 800T A2
-parallel_config:
-  data_parallel: 2
-  model_parallel: 4
-  pipeline_stage: 8
-  use_seq_parallel: True
-  micro_batch_num: 256
-  vocab_emb_dp: False
-  gradient_aggregation_group: 4
-# when model parallel is greater than 1, we can set micro_batch_interleave_num=2, that may accelerate the train process.
-micro_batch_interleave_num: 1
-
-# recompute config
-recompute_config:
-  recompute: False
-  select_recompute: [10, 8, 6, 4, 2, 0, 0, 0]
-  parallel_optimizer_comm_recompute: False
-  mp_comm_recompute: True
-  recompute_slice_activation: True
-
-# callbacks
-callbacks:
-  - type: MFLossMonitor
-  - type: CheckpointMointor
-    prefix: "llama_70b"
-    save_checkpoint_steps: 1000
-    integrated_save: False
-    async_save: False
-  - type: ObsMonitor
-
-# mindspore context init config
-context:
-  mode: 0 #0--Graph Mode; 1--Pynative Mode
-  device_target: "Ascend"
-  enable_graph_kernel: False
-  graph_kernel_flags: "--disable_expand_ops=Softmax,Dropout --enable_parallel_fusion=true --reduce_fuse_depth=8 --enable_auto_tensor_inplace=true"
-  max_call_depth: 10000
-  max_device_memory: "52GB"
-  save_graphs: False
-  save_graphs_path: "./graph"
-  device_id: 0
-  runtime_num_threads: 1
-
-# model config
-model:
-  model_config:
-    type: LlamaConfig
-    batch_size: 1 # add for increase predict
-    seq_length: 4096
-    hidden_size: 8192
-    num_layers: 80
-    num_heads: 64
-    vocab_size: 32000
-    multiple_of: 256
-    n_kv_heads: 8
-    ffn_dim_multiplier: 1.3
-    rms_norm_eps: 1.0e-5
-    bos_token_id: 1
-    eos_token_id: 2
-    pad_token_id: 0
-    ignore_token_id: -100
-    compute_dtype: "bfloat16"
-    layernorm_compute_type: "float32"
-    softmax_compute_type: "float16"
-    rotary_dtype: "float32"
-    param_init_type: "bfloat16"
-    use_past: False
-    scaling_factor: 1.0
-    extend_method: "None" # support "None", "PI", "NTK"
-    use_flash_attention: True
-    fine_grain_interleave: 2
-    qkv_concat: False
-    offset: 0
-    checkpoint_name_or_path: ""
-    repetition_penalty: 1
-    max_decode_length: 512
-    top_k: 3
-    top_p: 1
-    do_sample: False
-  arch:
-    type: LlamaForCausalLM
-
-processor:
-  return_tensors: ms
-  tokenizer:
-    unk_token: '<unk>'
-    bos_token: '<s>'
-    eos_token: '</s>'
-    pad_token: '<unk>'
-    type: LlamaTokenizer
-  type: LlamaProcessor
-
-# metric
-metric:
-  type: PerplexityMetric
-
-# wrapper cell config
-runner_wrapper:
-  type: MFTrainOneStepCell
-  scale_sense: 1
-  use_clip_grad: True
-  enable_check_overflow: False
-
-eval_callbacks:
-  - type: ObsMonitor
-
-auto_tune: False
-filepath_prefix: './autotune'
-autotune_per_step: 10
-
-profile: False
-profile_start_step: 1
-profile_stop_step: 10
-init_start_profile: False
-profile_communication: False
-profile_memory: True
-layer_scale: False
-layer_decay: 0.65
-lr_scale_factor: 256
-
-# aicc
-remote_save_url: "Please input obs url on AICC platform."
+seed: 0
+output_dir: './output' # path to save checkpoint/strategy
+load_checkpoint: ''
+src_strategy_path_or_dir: ''
+auto_trans_ckpt: False  # If true, auto transform load_checkpoint to load in distributed model
+only_save_strategy: False
+resume_training: False
+run_mode: 'train'
+
+# trainer config
+trainer:
+  type: CausalLanguageModelingTrainer
+  model_name: 'llama2_70b'
+# if True, do evaluate during the training process. if false, do nothing.
+# note that the task trainer should support _evaluate_in_training function.
+do_eval: False
+eval_step_interval: -1        # num of step intervals between each eval, -1 means no step end eval.
+eval_epoch_interval: 50        # num of epoch intervals between each eval, 1 means eval on every epoch end.
+
+# runner config
+runner_config:
+  epochs: 3
+  batch_size: 1
+  sink_mode: True
+  sink_size: 2
+# optimizer
+optimizer:
+  type: FP32StateAdamWeightDecay
+  beta1: 0.9
+  beta2: 0.95
+  eps: 1.e-8 # 1e-8
+  learning_rate: 1.e-5
+
+# lr sechdule
+lr_schedule:
+  type: CosineWithWarmUpLR
+  learning_rate: 1.e-5
+  lr_end: 0
+  warmup_ratio: 0.03
+  total_steps: -1 # -1 means it will load the total steps of the dataset
+
+# dataset
+train_dataset: &train_dataset
+  data_loader:
+    type: MindDataset
+    dataset_dir: ""
+    shuffle: True
+  input_columns: ["input_ids"]  # "input_ids", "labels" , labels are used in instruction finetune.
+  num_parallel_workers: 8
+  python_multiprocessing: False
+  drop_remainder: True
+  batch_size: 1
+  repeat: 1
+  numa_enable: False
+  prefetch_size: 1
+train_dataset_task:
+  type: CausalLanguageModelDataset
+  dataset_config: *train_dataset
+
+# eval dataset
+eval_dataset: &eval_dataset
+  data_loader:
+    type: MindDataset
+    dataset_dir: ""
+    shuffle: False
+  input_columns: ["input_ids"]
+  num_parallel_workers: 8
+  python_multiprocessing: False
+  drop_remainder: False
+  repeat: 1
+  numa_enable: False
+  prefetch_size: 1
+eval_dataset_task:
+  type: CausalLanguageModelDataset
+  dataset_config: *eval_dataset
+
+use_parallel: True
+# parallel context config
+parallel:
+  parallel_mode: 1 # 0-data parallel, 1-semi-auto parallel, 2-auto parallel, 3-hybrid parallel
+  gradients_mean: False
+  enable_alltoall: False
+  full_batch: True
+  search_mode: "sharding_propagation"
+  enable_parallel_optimizer: True
+  strategy_ckpt_save_file: "./ckpt_strategy.ckpt"
+  parallel_optimizer_config:
+    gradient_accumulation_shard: False
+    parallel_optimizer_threshold: 64
+# default parallel of device num = 32 for Atlas 800T A2
+parallel_config:
+  data_parallel: 2
+  model_parallel: 4
+  pipeline_stage: 8
+  use_seq_parallel: True
+  micro_batch_num: 256
+  vocab_emb_dp: False
+  gradient_aggregation_group: 4
+# when model parallel is greater than 1, we can set micro_batch_interleave_num=2, that may accelerate the train process.
+micro_batch_interleave_num: 1
+
+# recompute config
+recompute_config:
+  recompute: False
+  select_recompute: False
+  parallel_optimizer_comm_recompute: False
+  mp_comm_recompute: True
+  recompute_slice_activation: True
+
+# callbacks
+callbacks:
+  - type: MFLossMonitor
+  - type: CheckpointMointor
+    prefix: "llama_70b"
+    save_checkpoint_steps: 1000
+    integrated_save: False
+    async_save: False
+  - type: ObsMonitor
+
+# mindspore context init config
+context:
+  mode: 0 #0--Graph Mode; 1--Pynative Mode
+  device_target: "Ascend"
+  enable_graph_kernel: False
+  graph_kernel_flags: "--disable_expand_ops=Softmax,Dropout --enable_parallel_fusion=true --reduce_fuse_depth=8 --enable_auto_tensor_inplace=true"
+  max_call_depth: 10000
+  max_device_memory: "53GB"
+  save_graphs: False
+  save_graphs_path: "./graph"
+  device_id: 0
+  runtime_num_threads: 1
+
+# model config
+model:
+  model_config:
+    type: LlamaConfig
+    batch_size: 1 # add for increase predict
+    seq_length: 4096
+    hidden_size: 8192
+    num_layers: 80
+    num_heads: 64
+    vocab_size: 32000
+    multiple_of: 256
+    n_kv_heads: 8
+    ffn_dim_multiplier: 1.3
+    rms_norm_eps: 1.0e-5
+    bos_token_id: 1
+    eos_token_id: 2
+    pad_token_id: 0
+    ignore_token_id: -100
+    compute_dtype: "bfloat16"
+    layernorm_compute_type: "float32"
+    softmax_compute_type: "float16"
+    rotary_dtype: "float32"
+    param_init_type: "bfloat16"
+    use_past: False
+    scaling_factor: 1.0
+    extend_method: "None" # support "None", "PI", "NTK"
+    use_flash_attention: True
+    fine_grain_interleave: 2
+    qkv_concat: False
+    offset: 0
+    checkpoint_name_or_path: ""
+    repetition_penalty: 1
+    max_decode_length: 512
+    top_k: 3
+    top_p: 1
+    do_sample: False
+  arch:
+    type: LlamaForCausalLM
+
+processor:
+  return_tensors: ms
+  tokenizer:
+    unk_token: '<unk>'
+    bos_token: '<s>'
+    eos_token: '</s>'
+    pad_token: '<unk>'
+    type: LlamaTokenizer
+  type: LlamaProcessor
+
+# metric
+metric:
+  type: PerplexityMetric
+
+# wrapper cell config
+runner_wrapper:
+  type: MFTrainOneStepCell
+  scale_sense: 1
+  use_clip_grad: True
+  enable_check_overflow: False
+
+eval_callbacks:
+  - type: ObsMonitor
+
+auto_tune: False
+filepath_prefix: './autotune'
+autotune_per_step: 10
+
+profile: False
+profile_start_step: 1
+profile_stop_step: 10
+init_start_profile: False
+profile_communication: False
+profile_memory: True
+layer_scale: False
+layer_decay: 0.65
+lr_scale_factor: 256
+
+# aicc
+remote_save_url: "Please input obs url on AICC platform."
```

## Comparing `configs/llama2/run_llama2_7b_bf16_800T_A2_finetune.yaml` & `configs/blip2/run_blip2_stage2_vit_g_baichuan_7b.yaml`

 * *Files 26% similar despite different names*

```diff
@@ -1,206 +1,260 @@
-seed: 0
+seed: 42
+run_mode: 'train'
 output_dir: './output' # path to save checkpoint/strategy
 load_checkpoint: ''
 src_strategy_path_or_dir: ''
 auto_trans_ckpt: False  # If true, auto transform load_checkpoint to load in distributed model
 only_save_strategy: False
 resume_training: False
-run_mode: 'finetune'
 
-# trainer config
-trainer:
-  type: CausalLanguageModelingTrainer
-  model_name: 'llama2_7b'
+# context
+context:
+  mode: 0 #0--Graph Mode; 1--Pynative Mode
+  device_target: "Ascend"
+  enable_graph_kernel: False
+  graph_kernel_flags: str = "--disable_expand_ops=Softmax,Dropout " \
+                              "--enable_parallel_fusion=true --reduce_fuse_depth=8 --enable_auto_tensor_inplace=true"
+  max_call_depth: 10000
+  max_device_memory: "30GB"
+  save_graphs: False
+  save_graphs_path: "./graph"
+  device_id: 0
+
+# aicc
+remote_save_url: "Please input obs url on AICC platform."
 
-# runner config
+# runner
 runner_config:
-  epochs: 2
-  batch_size: 1
-  sink_mode: True
+  epochs: 1
+  batch_size: &batch_size 16
   sink_size: 2
-  gradient_accumulation_steps: 8
-
-# optimizer
-optimizer:
-  type: FP32StateAdamWeightDecay
-  beta1: 0.9
-  beta2: 0.95
-  eps: 1.e-8
-
-# lr sechdule
-lr_schedule:
-  type: CosineWithWarmUpLR
-  learning_rate: 1.e-5
-  lr_end: 0
-  warmup_ratio: 0.03
-  total_steps: -1 # -1 means it will load the total steps of the dataset
-
-# dataset
-train_dataset: &train_dataset
-  data_loader:
-    type: MindDataset
-    dataset_dir: ""
-    shuffle: True
-  input_columns: ["input_ids", "labels"]  # "input_ids", "labels" , labels are used in instruction finetune.
-  num_parallel_workers: 8
-  python_multiprocessing: False
-  drop_remainder: True
-  batch_size: 6
-  repeat: 1
-  numa_enable: False
-  prefetch_size: 1
-train_dataset_task:
-  type: CausalLanguageModelDataset
-  dataset_config: *train_dataset
-# if True, do evaluate during the training process. if false, do nothing.
-# note that the task trainer should support _evaluate_in_training function.
-do_eval: False
-
-# eval dataset
-eval_dataset: &eval_dataset
-  data_loader:
-    type: MindDataset
-    dataset_dir: ""
-    shuffle: False
-  input_columns: ["input_ids"]
-  num_parallel_workers: 8
-  python_multiprocessing: False
-  drop_remainder: False
-  repeat: 1
-  numa_enable: False
-  prefetch_size: 1
-eval_dataset_task:
-  type: CausalLanguageModelDataset
-  dataset_config: *eval_dataset
+  image_size: 224
+  sink_mode: True
+  initial_epoch: 0
+  has_trained_epoches: 0
+  has_trained_steps: 0
+runner_wrapper:
+  type: TrainOneStepCell
+  sens: 1024
 
-use_parallel: True
-# parallel context config
+# parallel
+use_parallel: False
 parallel:
-  parallel_mode: 1 # 0-data parallel, 1-semi-auto parallel, 2-auto parallel, 3-hybrid parallel
-  gradients_mean: False
-  enable_alltoall: False
-  full_batch: True
+  parallel_mode: 0 # 0-dataset, 1-semi, 2-auto, 3-hybrid
+  gradients_mean: True
   search_mode: "sharding_propagation"
-  enable_parallel_optimizer: True
-  strategy_ckpt_save_file: "./ckpt_strategy.ckpt"
-  parallel_optimizer_config:
-    gradient_accumulation_shard: False
-    parallel_optimizer_threshold: 64
-# default parallel of device num = 8 for Atlas 800T A2
+  enable_parallel_optimizer: False
+  full_batch: False
 parallel_config:
-  data_parallel: 8
+  data_parallel: 1
   model_parallel: 1
   pipeline_stage: 1
-  use_seq_parallel: False
   micro_batch_num: 1
   vocab_emb_dp: True
   gradient_aggregation_group: 4
-# when model parallel is greater than 1, we can set micro_batch_interleave_num=2, that may accelerate the train process.
-micro_batch_interleave_num: 1
+micro_batch_interleave_num: &micro_batch_interleave_num 1
 
-# recompute config
+# recompute
 recompute_config:
   recompute: False
-  select_recompute: False
   parallel_optimizer_comm_recompute: False
   mp_comm_recompute: True
-  recompute_slice_activation: True
-
-# callbacks
-callbacks:
-  - type: MFLossMonitor
-  - type: CheckpointMointor
-    prefix: "llama2_7b"
-    save_checkpoint_steps: 100
-    integrated_save: False
-    async_save: False
-  - type: ObsMonitor
-
-# mindspore context init config
-context:
-  mode: 0 #0--Graph Mode; 1--Pynative Mode
-  device_target: "Ascend"
-  enable_graph_kernel: False
-  graph_kernel_flags: "--disable_expand_ops=Softmax,Dropout --enable_parallel_fusion=true --reduce_fuse_depth=8 --enable_auto_tensor_inplace=true"
-  max_call_depth: 10000
-  max_device_memory: "59GB"
-  save_graphs: False
-  save_graphs_path: "./graph"
-  device_id: 0
-  runtime_num_threads: 1
-
-# model config
-model:
-  model_config:
-    type: LlamaConfig
-    batch_size: 1 # add for increase predict
-    seq_length: 4096
-    hidden_size: 4096
-    num_layers: 32
-    num_heads: 32
-    vocab_size: 32000
-    multiple_of: 256
-    rms_norm_eps: 1.0e-5
-    bos_token_id: 1
-    eos_token_id: 2
-    pad_token_id: 0
-    ignore_token_id: -100
-    compute_dtype: "bfloat16"
-    layernorm_compute_type: "float32"
-    softmax_compute_type: "float16"
-    rotary_dtype: "float32"
-    param_init_type: "float16"
-    embedding_init_type: "bfloat16"
-    use_past: False
-    scaling_factor: 1.0
-    extend_method: "None" # support "None", "PI", "NTK"
-    use_flash_attention: True # FA can accelerate training or finetune
-    offset: 0
-    checkpoint_name_or_path: "llama2_7b"
-    repetition_penalty: 1
-    max_decode_length: 512
-    top_k: 3
-    top_p: 1
-    do_sample: False
-  arch:
-    type: LlamaForCausalLM
-
-processor:
-  return_tensors: ms
-  tokenizer:
-    unk_token: '<unk>'
-    bos_token: '<s>'
-    eos_token: '</s>'
-    pad_token: '<unk>'
-    type: LlamaTokenizer
-  type: LlamaProcessor
-
-# metric
-metric:
-  type: PerplexityMetric
-
-# wrapper cell config
-runner_wrapper:
-  type: MFTrainOneStepCell
-  scale_sense: 1.0
-  use_clip_grad: True
-
-eval_callbacks:
-  - type: ObsMonitor
+  recompute_slice_activation: False
 
+# autotune
 auto_tune: False
 filepath_prefix: './autotune'
 autotune_per_step: 10
 
+# profile
 profile: False
 profile_start_step: 1
 profile_stop_step: 10
 init_start_profile: False
 profile_communication: False
 profile_memory: True
+
+# Trainer
+trainer:
+  type: ContrastiveLanguageImagePretrainTrainer
+  model_name: 'blip2_stage2_vit_g_baichuan_7b'
+
+# train dataset
+train_dataset: &train_dataset
+  data_loader:
+    type: MultiImgCapDataLoader
+    dataset_dir: "/data"
+    annotation_files: [
+      "coco2014/coco/annotations/coco_karpathy_train.json"
+    ]
+    image_dirs: [
+      "coco2014/coco/images"
+    ]
+    stage: "train"
+    column_names: ["image", "text"]
+    shuffle: True
+  transforms:
+    - type: RandomResizedCrop
+      size: 224
+      scale: [0.5, 1.0]
+      interpolation: "bicubic"
+    - type: RandomHorizontalFlip
+    - type: ToTensor
+    - type: Normalize
+      mean: [0.48145466, 0.4578275, 0.40821073]
+      std: [0.26862954, 0.26130258, 0.27577711]
+      is_hwc: False
+  text_transforms:
+    type: CaptionTransform
+    prompt: ""
+    max_words: 50
+    max_length: 33  # it equals to Blip2Config.max_txt_len + 1 for constructing labels
+    padding: 'max_length'
+    random_seed: 2022
+    truncation: True
+    add_special_tokens: True
+  tokenizer:
+    type: LlamaTokenizer
+    unk_token: '<unk>'
+    bos_token: '<s>'
+    eos_token: '</s>'
+    pad_token: '<pad>'
+    add_special_tokens: False
+    padding: 'max_length'
+    truncation: True
+    max_length: 33  # it equals to Blip2Config.max_txt_len + 1 for constructing labels
+    add_bos_token: False
+    add_eos_token: True
+    vocab_file: ""
+
+  num_parallel_workers: 8
+  python_multiprocessing: False
+  drop_remainder: True
+  batch_size: 4
+  repeat: 1
+  numa_enable: False
+  prefetch_size: 30
+  seed: 2022
+train_dataset_task:
+  type: ContrastiveLanguageImagePretrainDataset
+  dataset_config: *train_dataset
+# model
+model:
+  model_config:
+    type: Blip2Config
+    batch_size: *batch_size
+    micro_batch_interleave_num: *micro_batch_interleave_num
+    freeze_vision: True
+    freeze_text: True
+    max_txt_len: 32
+    checkpoint_name_or_path: ""
+    dtype: "float32"
+    compute_dtype: "float16"
+    layernorm_dtype: "float32"
+    softmax_dtype: "float32"
+    prompt: False
+    prompt_length: 0
+    vision_config:
+      type: ViTConfig
+      image_size: 224
+      patch_size: 14
+      num_channels: 3
+      initializer_range: 0.001
+      hidden_size: 1408
+      num_hidden_layers: 39
+      num_attention_heads: 16
+      intermediate_size: 6144
+      qkv_bias: True
+      hidden_act: gelu
+      post_layernorm_residual: false
+      layer_norm_eps: 1.0e-6
+      attention_probs_dropout_prob: 0.0
+      hidden_dropout_prob: 0.0
+      drop_path_rate: 0.0
+      use_mean_pooling: false
+      encoder_stride: 16
+      checkpoint_name_or_path: "vit_g_p16"
+
+    qformer_config:
+      vocab_size: 44728
+
+    text_config:
+      type: LlamaConfig
+      seq_length: 64  # sum of max_txt_len and num_query_token
+      hidden_size: 4096
+      num_layers: 32
+      num_heads: 32
+      vocab_size: 64001
+      multiple_of: 256
+      rms_norm_eps: 1.0e-6
+      bos_token_id: 1
+      eos_token_id: 2
+      pad_token_id: 0
+      ignore_token_id: -100
+      compute_dtype: "float16"
+      layernorm_compute_type: "float32"
+      softmax_compute_type: "float32"
+      rotary_dtype: "float16"
+      param_init_type: "float16"
+      use_past: False
+      offset: 0
+      repetition_penalty: 1
+      max_decode_length: 512
+      top_k: 3
+      top_p: 1
+      do_sample: False
+      checkpoint_name_or_path: ""
+  arch:
+    type: Blip2Llm
+
+# lr schedule
+lr_schedule:
+  type: CosineWithWarmUpLR
+  learning_rate: 1.e-4
+  lr_end: 1.e-5
+  warmup_lr_init: 1.e-6
+  warmup_steps: 2000
+  total_steps: -1 # -1 means it will load the total steps of the dataset
 layer_scale: False
 layer_decay: 0.65
+lr_scale: False
 lr_scale_factor: 256
 
-# aicc
-remote_save_url: "Please input obs url on AICC platform."
+# optimizer
+optimizer:
+  type: FP32StateAdamWeightDecay
+  beta1: 0.9
+  beta2: 0.98
+  eps: 1.e-8 # 1e-8
+  weight_decay: 0.05
+  learning_rate: 1.e-4
+
+# callbacks
+callbacks:
+  - type: MFLossMonitor
+  - type: CheckpointMointor
+    prefix: "blip2_stage2_vig_g_baichuan_7b"
+    save_checkpoint_steps: 10000
+    integrated_save: True
+    async_save: False
+  - type: ObsMonitor
+eval_callbacks:
+  - type: ObsMonitor
+
+
+# processor
+processor:
+  type: Blip2Processor
+  image_processor:
+    type: Blip2ImageProcessor
+    image_size: 224  # input image size
+  tokenizer:
+    type: LlamaTokenizer
+    pad_token: '<pad>'
+    bos_token: '<s>'
+    unk_token: '</s>'
+    eos_token: '</s>'
+    add_special_tokens: False
+    padding: 'max_length'
+    truncation: True
+    max_length: 32
```

## Comparing `mindformers/experimental/__init__.py` & `mindformers/inference/__init__.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,25 +1,21 @@
-# Copyright 2024 Huawei Technologies Co., Ltd
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-# http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# ============================================================================
-
-"""mindformers init"""
-
-__version__ = "1.1"
-
-from .distri_cores import *
-from .distri_ckpt import *
-
-__all__ = []
-__all__.extend(distri_cores.__all__)
-__all__.extend(distri_ckpt.__all__)
+# Copyright 2023 Huawei Technologies Co., Ltd
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ============================================================================
+"""infer module"""
+
+from .infer_config import InferConfig
+from .infer_task import InferTask
+from .pipeline import get_mslite_pipeline
+
+__all__ = ['InferConfig', 'InferTask']
```

## Comparing `mindformers/experimental/distri_cores/__init__.py` & `mindformers/inference/infers/__init__.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,23 +1,17 @@
-# Copyright 2024 Huawei Technologies Co., Ltd
+# Copyright 2023 Huawei Technologies Co., Ltd
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 # http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ============================================================================
-
-"""mindformers init"""
-
-__version__ = "1.1"
-
-from .optimizer import *
+"""inference tasks."""
 
 __all__ = []
-__all__.extend(optimizer.__all__)
```

## Comparing `mindformers/modules/cache_engine.py` & `mindformers/inference/infers/cache_engine.py`

 * *Files identical despite different names*

## Comparing `mindformers-1.1.0.dist-info/LICENSE` & `mindformers-1.1.0rc1.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `mindformers-1.1.0.dist-info/METADATA` & `mindformers-1.1.0rc1.dist-info/METADATA`

 * *Files 9% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: mindformers
-Version: 1.1.0
+Version: 1.1.0rc1
 Summary: mindformers platform: linux, cpu: x86_64
 Home-page: https://www.mindspore.cn
 Download-URL: https://gitee.com/mindspore/mindformers/tags
 Author: The MindSpore Authors
 Author-email: contact@mindspore.cn
 License: Apache 2.0
 Project-URL: Sources, https://gitee.com/mindspore/mindformers
@@ -84,14 +84,15 @@
 |     [CodeLlama](docs/model_cards/codellama.md)     | codellama_34b                                                       |
 |     [CodeGeex2](docs/model_cards/codegeex2.md)     | codegeex2_6b                                                       |
 |         [LLama](docs/model_cards/llama.md)         | llama_7b, llama_13b, llama_7b_lora                                 |
 |           [GLM](docs/model_cards/glm.md)           | glm_6b, glm_6b_lora                                                |
 |         [Bloom](docs/model_cards/bloom.md)         | bloom_560m, bloom_7.1b                                             |
 |          [GPT2](docs/model_cards/gpt2.md)          | gpt2, gpt2_13b                                                     |
 |    [PanGuAlpha](docs/model_cards/pangualpha.md)    | pangualpha_2_6_b, pangualpha_13b                                   |
+|         [BLIP2](docs/model_cards/blip2.md)         | blip2_stage1_vit_g                                                 |
 |          [CLIP](docs/model_cards/clip.md)          | clip_vit_b_32, clip_vit_b_16, clip_vit_l_14, clip_vit_l_14@336     |
 |            [T5](docs/model_cards/t5.md)            | t5_small                                                           |
 |           [sam](docs/model_cards/sam.md)           | sam_vit_b, sam_vit_l, sam_vit_h                                    |
 |           [MAE](docs/model_cards/mae.md)           | mae_vit_base_p16                                                   |
 |           [VIT](docs/model_cards/vit.md)           | vit_base_p16                                                       |
 |          [Swin](docs/model_cards/swin.md)          | swin_base_p4w7                                                     |
 |       [skywork](research/skywork/skywork.md)       | skywork_13b                                                        |
@@ -111,122 +112,76 @@
 
 ```bash
 git clone -b dev https://gitee.com/mindspore/mindformers.git
 cd mindformers
 bash build.sh
 ```
 
+### 
+
+docker
+
+```shell
+docker pull swr.cn-central-221.ovaijisuan.com/mindformers/mindformers1.0_mindspore2.2.11:aarch_20240125
+```
+
+
+
+```shell
+# --deviceNPU
+# -v 
+# --name 
+
+docker run -it -u root \
+--ipc=host \
+--network host \
+--device=/dev/davinci0 \
+--device=/dev/davinci1 \
+--device=/dev/davinci2 \
+--device=/dev/davinci3 \
+--device=/dev/davinci4 \
+--device=/dev/davinci5 \
+--device=/dev/davinci6 \
+--device=/dev/davinci7 \
+--device=/dev/davinci_manager \
+--device=/dev/devmm_svm \
+--device=/dev/hisi_hdc \
+-v /etc/localtime:/etc/localtime \
+-v /usr/local/Ascend/driver:/usr/local/Ascend/driver \
+-v /var/log/npu/:/usr/slog \
+-v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \
+--name {} \
+swr.cn-central-221.ovaijisuan.com/mindformers/mindformers1.0_mindspore2.2.11:aarch_20240125 \
+/bin/bash
+```
+
+[](http://mirrors.cn-central-221.ovaijisuan.com/mirrors.html)
+
 ## 
 
 Atlas 800[Atlas 800T A2](https://www.hiascend.com/hardware/ai-server?tag=900A2)
 
 Python3.9
 
 | MindFormers | MindPet |                 MindSpore                  |                                                                                                                                               CANN                                                                                                                                               |                                                              |                                                              |                  |
 | :---------: | :-----: | :----------------------------------------: | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :------------------------------------------------------------------: | :------------------------------------------------------------------: | -------------------- |
-|     dev     |  1.0.4  | 2.3() |    |  |                                  /                                   | () |
+|     dev     |  1.0.3  | [2.2.11](https://www.mindspore.cn/install) |           7.0.0.beta1:<br> [aarch64](https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/CANN/CANN%207.0.0/Ascend-cann-toolkit_7.0.0_linux-aarch64.run)<br> [x86_64](https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/CANN/CANN%207.0.0/Ascend-cann-toolkit_7.0.0_linux-x86_64.run)           | [](https://www.hiascend.com/hardware/firmware-drivers/community) |                                  /                                   | () |
+|    r1.0     |  1.0.3  | [2.2.11](https://www.mindspore.cn/install) |           7.0.0.beta1:<br> [aarch64](https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/CANN/CANN%207.0.0/Ascend-cann-toolkit_7.0.0_linux-aarch64.run)<br> [x86_64](https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/CANN/CANN%207.0.0/Ascend-cann-toolkit_7.0.0_linux-x86_64.run)           | [](https://www.hiascend.com/hardware/firmware-drivers/community) | [](http://mirrors.cn-central-221.ovaijisuan.com/detail/118.html) |              |
+|    r0.8     |  1.0.2  | [2.2.1](https://www.mindspore.cn/install)  | 7.0.RC1.3.beta1:<br> [aarch64](https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/CANN/CANN%207.0.RC1.3/Ascend-cann-toolkit_7.0.RC1.3_linux-aarch64.run)<br> [x86_64](https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/CANN/CANN%207.0.RC1.3/Ascend-cann-toolkit_7.0.RC1.3_linux-x86_64.run) | [](https://www.hiascend.com/hardware/firmware-drivers/community) |                                  /                                   |                     |
 
 CANN
 
 ## 
 
 MindFormers
 
 ### 
 
 clone`configs`
 
-**[msrun](https://www.mindspore.cn/tutorials/experts/zh-CN/r2.3/parallel/msrun_launcher.html)MindSpore2.3**
-
-msrundevice_idmsrunrank_id
-
-- 
-
-  ```shell
-  # 8
-  bash scripts/msrun_launcher.sh "run_mindformer.py \
-   --config {CONFIG_PATH} \
-   --run_mode {train/finetune/eval/predict}"
-
-  # 
-  bash scripts/msrun_launcher.sh "run_mindformer.py \
-   --config {CONFIG_PATH} \
-   --run_mode {train/finetune/eval/predict}" WORKER_NUM
-
-  # 
-  bash scripts/msrun_launcher.sh "run_mindformer.py \
-   --config {CONFIG_PATH} \
-   --run_mode {train/finetune/eval/predict}" \
-   WORKER_NUM MASTER_PORT LOG_DIR JOIN CLUSTER_TIME_OUT
-  ```
-
-    - 
-
-      ```shell
-      # 8
-      bash scripts/msrun_launcher.sh "run_mindformer.py \
-       --config path/to/xxx.yaml \
-       --run_mode finetune"
-
-      # 
-      bash scripts/msrun_launcher.sh "run_mindformer.py \
-       --config path/to/xxx.yaml \
-       --run_mode finetune" 8
-
-      # 
-      bash scripts/msrun_launcher.sh "run_mindformer.py \
-       --config path/to/xxx.yaml \
-       --run_mode finetune" \
-       8 8118 output/msrun_log False 300
-      ```
-
-- 
-
-  MASTER_ADDRip
-  ipNODE_RANK
-
-  ```shell
-  # 
-  bash scripts/msrun_launcher.sh "run_mindformer.py \
-   --config {CONFIG_PATH} \
-   --run_mode {train/finetune/eval/predict}" \
-   WORKER_NUM LOCAL_WORKER MASTER_ADDR MASTER_PORT NODE_RANK LOG_DIR JOIN CLUSTER_TIME_OUT
-  ```
-
-    - 
-
-      ```shell
-      # 0ip192.168.1.184
-      bash scripts/msrun_launcher.sh "run_mindformer.py \
-       --config {CONFIG_PATH} \
-       --run_mode {train/finetune/eval/predict}" \
-       8 4 192.168.1.1 8118 0 output/msrun_log False 300
-
-      # 1ip192.168.1.201NODE_RANK
-      bash scripts/msrun_launcher.sh "run_mindformer.py \
-       --config {CONFIG_PATH} \
-       --run_mode {train/finetune/eval/predict}" \
-       8 4 192.168.1.1 8118 1 output/msrun_log False 300
-      ```
-
-- 
-
-  | ****           | ****  | **** |     ****      | ****           |
-  |------------------|:-----------:|:----------:|:----------------:|------------------|
-  | WORKER_NUM       |            |           |        8         |     |
-  | LOCAL_WORKER     |            |           |        8         |     |
-  | MASTER_ADDR      |            |           |    127.0.0.1     | ip    |
-  | MASTER_PORT      |            |           |       8118       |     |
-  | NODE_RANK        |            |           |        0         | rank id   |
-  | LOG_DIR          |            |           | output/msrun_log |  |
-  | JOIN             |            |           |      False       |     |
-  | CLUSTER_TIME_OUT |            |           |       600        |   |
-
-**rank table**
-
 - 
 
     - step1mindformers
 
       ```shell
       git clone -b dev https://gitee.com/mindspore/mindformers.git
       cd mindformers
```

## Comparing `mindformers-1.1.0.dist-info/RECORD` & `mindformers-1.1.0rc1.dist-info/RECORD`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,18 @@
-configs/README.md,sha256=bLbgMkJv86XUukzAoLGdt5M1Z8c3tVzG-K94l344GKA,15913
+configs/README.md,sha256=yMP3W6PqTgpSxavbFVnwhmZtwtYP6U1yRNq04thLfwI,15518
 configs/bert/run_bert_base_uncased.yaml,sha256=iWOw49tacAz3fmtLRlHq04Ioo0RZ0ozTLoolf_PwgnY,4241
 configs/bert/run_bert_tiny_uncased.yaml,sha256=A0ccR79xeoDmiVQcfTXJexO_w9QmcgUom101gvm8_Z0,4257
+configs/blip2/run_blip2_stage1_vit_g_qformer_pretrain.yaml,sha256=dmd4upjSosmlfmDYHpW6NvrKfcG-TyYOJWuIne-SRJA,6074
+configs/blip2/run_blip2_stage1_vit_g_retrieval_flickr30k.yaml,sha256=1Bdi0SPLXgSwkSozh3LEAq10nqaiOeNeVNKjKZTrVo4,6139
+configs/blip2/run_blip2_stage1_vit_g_zero_shot_image_classification_cifar100.yaml,sha256=YDNI1hyn6qpKGAa4crFpsdwzxC_ipomfBulS6dW7dM8,4951
+configs/blip2/run_blip2_stage2_vit_g_baichuan_7b.yaml,sha256=mpQ2UidIQmeismh7uEq7BitX0lO_tLOCrLcWDYw7wI8,6214
+configs/blip2/run_blip2_stage2_vit_g_baichuan_7b_image_to_text_generation.yaml,sha256=joW0qsvJ6oJjL8Ti2Ulrtli6kM6uLsZ014BeUxW4arc,4705
+configs/blip2/run_blip2_stage2_vit_g_llama_7b.yaml,sha256=_YSaeT_s2GfzZuGULbUSl0NIz4veVlTkqI-rKovXTuw,6272
+configs/blip2/run_blip2_stage2_vit_g_llama_7b_910b.yaml,sha256=hV_ngk7dliGjH6ZvYszSESUliDpaztEGg3XizjRJndw,6272
+configs/blip2/run_blip2_stage2_vit_g_llama_7b_image_to_text_generation.yaml,sha256=RWrL6677hL-BZdyoSmtk1PAU3EWLmwLL5GcfAtvR-N0,4761
 configs/bloom/run_bloom_560m.yaml,sha256=p7GbKV5RDe2IOwXDtuEBbWo0ActCzbpj__B4seXKEx8,4055
 configs/bloom/run_bloom_7.1b.yaml,sha256=S-SrarZEbCF3-lKmE63lXZ54_p9VzawwnftxBn1BfPg,4054
 configs/bloom/run_bloom_7.1b_910b.yaml,sha256=14mYyYzbQRk1oC1N4DoQC1sxUESALnwcSKGq2sPHzM8,4138
 configs/bloom/run_bloom_7.1b_910b_fa.yaml,sha256=K_O2hNin5sWTNtqy9LRJ54V_xPliD-Gnm-o_NMTuWIA,4156
 configs/clip/run_clip_vit_b_16_pretrain_flickr8k.yaml,sha256=7gJt5kt_wHn95W2nta79iGwmXR2u9mB_bZ-zVKB7H-I,4137
 configs/clip/run_clip_vit_b_16_zero_shot_image_classification_cifar100.yaml,sha256=pkXrachgpXAx13YBbIL9FTkudG4bovuE8F8x-PU2Y60,4285
 configs/clip/run_clip_vit_b_32_pretrain_flickr8k.yaml,sha256=KvSiCqSz6MaxdLDFCQTxF7PipKlXR3GdjXrbh0Ef8x4,4138
@@ -13,75 +21,73 @@
 configs/clip/run_clip_vit_l_14@336_zero_shot_image_classification_cifar100.yaml,sha256=yS8ik9FlngbsYsw30XWyMgoMJWCsm7z0sxUuYvIXITo,4295
 configs/clip/run_clip_vit_l_14_pretrain_flickr8k.yaml,sha256=uU7eMPTS6aeZPNCQmXeCrJ2bhkDyFI4c5aDx6AloHLU,4139
 configs/clip/run_clip_vit_l_14_zero_shot_image_classification_cifar100.yaml,sha256=LHHp3kZa6z_34yTYqANZX1hLML9O6Q-fvgaC_6Birbk,4286
 configs/codegeex2/run_codegeex2_6b.yaml,sha256=xluiExAoTNkKsDHZN6Nq0ijAxezJAc-WI16Elwaq248,6097
 configs/codegeex2/run_codegeex2_6b_eval.yaml,sha256=mz9coO4OY9EvU3qZ8P9mDwP8Ak2pIbPlGcLXCj4_LRE,5811
 configs/codegeex2/run_codegeex2_6b_finetune.yaml,sha256=PkmfFFH3srslwkDa8KbegRCnvp0hw8J0DyWESNeZ1Y0,5817
 configs/codegeex2/run_codegeex2_6b_finetune_2048.yaml,sha256=k8D4HKj0YEGn2p_XDghWGxTDt1fMSW1N5Rzn5ErUNEs,5821
-configs/codellama/finetune_codellama_34b_32p.yaml,sha256=MP-1DHlIFXqzMMcxEkEsHTOcFLdoemPa7SMm5FjNrsQ,5201
-configs/codellama/predict_codellama_34b_910b.yaml,sha256=asas2bPFIX3nyMFZgdO0c9Zbt-Xn0FqHk5V1_uBI3ME,3943
-configs/codellama/run_codellama_34b_910b.yaml,sha256=8F3IqLj-euTe_NFfGNXLG6MsShq0wf7EpgwQzL-T1Bs,5218
+configs/codellama/predict_codellama_34b_910b.yaml,sha256=ycNVz-2QcaBwfj4NnKx92u06LiVmKuiPIttmohtV61k,4008
+configs/codellama/run_codellama_34b_910b.yaml,sha256=w9kJcXUJcAse0KTfGKVOpomObojK10jjOAUIDVqPnNA,5330
+configs/codellama/run_codellama_34b_910b_32p.yaml,sha256=rBuopl-YORIlOBaRmNrJpbIW9GCiIKlr0TUEsdgl6lo,5303
 configs/convert_config/run_convert.yaml,sha256=RMwTSDPsJs-VmxqMFDgR8z3LPL82GjCeijFwUVXAkAg,843
 configs/convert_config/run_reversed_convert.yaml,sha256=zwHFpMtJrLXgRopQxjQy2XkheQNLf89YkkJGzwmo6zo,585
 configs/general/run_general_task.yaml,sha256=xzJcm3v3IUE_VqEDA90cMa8DR4oAipy9TcXXdepSFuI,2617
 configs/glm/run_glm_6b_finetune.yaml,sha256=jvFDWtYM47R0lHv1XnXSL6zmciY95k9cQPyH1AFlKsM,5756
 configs/glm/run_glm_6b_infer.yaml,sha256=LonnLMP4yQo7cvcHOE7yESGnhe0UzhLQrxoJO68GZ7Q,5677
 configs/glm/run_glm_6b_lora.yaml,sha256=0TlyQatKyxosS7PvX6v-Rl1Jxhl4OndMY4jre3OvhWA,5954
 configs/glm/run_glm_6b_lora_infer.yaml,sha256=8eq2ufiwvd1aG9CzlTXbYKOWs6VnvOnLQeL3zABq4FE,5861
-configs/glm2/predict_glm2_6b.yaml,sha256=8Z-CfkdIxUqTUn_7PuNlCFYRX-AbD68Lrgzt0NG4u28,6051
-configs/glm2/run_glm2_6b.yaml,sha256=ADcuTBm1qksxaIKZrwuXD8JC8-oQizOIXyGsVvkaQrc,6031
-configs/glm2/run_glm2_6b_finetune_2k_800T_A2_64G.yaml,sha256=9esijsRLw_IL7hvAzUi_cV_rrX4V8ZGq0ah7fnWUWlE,5924
+configs/glm2/export_glm2_6b.yaml,sha256=s_RLIOw30xaaauwXnKxlNbY6kfK-x4CN5nooQhm5zj4,1639
+configs/glm2/run_glm2_6b.yaml,sha256=CtaIEZOPHOBCyKGHSyOYuzHUuOURMC49lfhUAygNu0k,6029
+configs/glm2/run_glm2_6b_finetune_2k_800T_A2_64G.yaml,sha256=dMUIB5IQeyRxG6qiTJW6vs_jb7Z_D8HPKDYt2tb32kQ,5922
 configs/glm2/run_glm2_6b_finetune_2k_800_32G.yaml,sha256=pECOCOdnvvJje-uBrU1Z-z980VmD5Q3oEf825q3OdTI,5923
 configs/glm2/run_glm2_6b_finetune_800T_A2_64G.yaml,sha256=tiBBhtDmbbzra3aKGyVYAsqNq8DHjb9_8KZfSfc52Yc,5870
 configs/glm2/run_glm2_6b_finetune_800_32G.yaml,sha256=WJYujvRQa6u6up2_sbewGME_WB35UeDzhpIUOydxWHU,5916
 configs/glm2/run_glm2_6b_finetune_eval.yaml,sha256=Q7LPXlDTkNG_yfhcXKG-S3zzeJPaan8QdD3AQv2IKvc,5615
 configs/glm2/run_glm2_6b_lora_2k_800T_A2_64G.yaml,sha256=vANrMGCEMEMJwj2Y15MveeZyMeUPZAmth35itk-L89Y,6203
 configs/glm2/run_glm2_6b_lora_2k_800_32G.yaml,sha256=lsUx5gGD-I8wWnF5wNZrRP1Gy0ump6wxsG9qZr7wMSA,6205
-configs/glm2/run_glm2_6b_lora_800T_A2_64G.yaml,sha256=occU9ZZdZ3gzgVIzDi4ZRam1JGgHOc91IF_ZwhA3E8w,6193
+configs/glm2/run_glm2_6b_lora_800T_A2_64G.yaml,sha256=UolEs5UpBJFM-eIVWIUxmtsBfT_1GdLVYONyqdzNins,6191
 configs/glm2/run_glm2_6b_lora_800_32G.yaml,sha256=FFENxpfeYR3XdDQ4HiL8U9yY_RBuyjOQzAVN9yw3ubw,6197
 configs/glm2/run_glm2_6b_lora_eval.yaml,sha256=rWuDGG-UlHmL229UtL4S9URdQCYUs_Srt82dbi7QK94,5800
-configs/glm2/run_glm2_6b_ptuning2.yaml,sha256=1xWCnZgUup-3vajXmN_Ggk8q7MUMKWc9qPwmAFAgTtw,6152
-configs/glm3/predict_glm3_6b.yaml,sha256=ddRhWnrOB4vlSW3Zcovm7KvAZjSEpBe345XkpITIZik,6071
-configs/glm3/run_glm3_6b.yaml,sha256=X73CxsCllKIWIi0KQuRbg8SJrVEGe_5hY2slUMosCZE,6031
-configs/glm3/run_glm3_6b_finetune_2k_800T_A2_64G.yaml,sha256=Rq-eK2wkL3QTnOrfiZcjugX4WUa_1RxRan3a-TEPAz0,5909
+configs/glm2/run_glm2_6b_ptuning2.yaml,sha256=6noMvV4xKuMSUIiihqbsHphDsQkY_2TwhibS-f80Qm8,6150
+configs/glm3/export_glm3_6b.yaml,sha256=_dO0SLWEjiJIt-sZIO8oYA622nak7SA-8aDYMrmSIDM,1640
+configs/glm3/run_glm3_6b.yaml,sha256=IDKZZFKxHaUuepG-zSWG2Q0o2K4ECRl_sr_KpIExOes,6029
+configs/glm3/run_glm3_6b_finetune_2k_800T_A2_64G.yaml,sha256=XSH5IHoFpuUBh0Z3h-w7lcoHVgwgEAwW65OoPZVBV94,5907
 configs/glm3/run_glm3_6b_finetune_800T_A2_64G.yaml,sha256=27ypHFn8-Trd3vlII27iUKW92cKmOrhvBkioyImkWXQ,5903
 configs/glm3/run_glm3_6b_multiturn_finetune_800T_A2_64G.yaml,sha256=nSLypFpa1ulsHlsdYlN9spnZzWlTITBu5XqIli_mOW4,5234
-configs/gpt2/run_gpt2.yaml,sha256=LqwFHDRNBxlnRHVT_KPNPMW2YrywS-Xdkrqcwgls9q8,4339
-configs/gpt2/run_gpt2_13b.yaml,sha256=gFk0Qw4tT_2R9mtwISGOJv1gP-WdP62R89yNRji4X5c,4716
-configs/gpt2/run_gpt2_13b_910b.yaml,sha256=Kt9B23VfsuY_Z9O6aOp97JdRFeKnMSaOuP66qMNZniU,4860
-configs/gpt2/run_gpt2_52b.yaml,sha256=ppFGEhvjuAS9-Dke6-THVKZFJMskDes4tMlOU49MVo0,4666
-configs/gpt2/run_gpt2_lora.yaml,sha256=7l6b3C0WnjHrJqwB8Fyz82PLQlrPsLigqWqdvei3pkM,4414
-configs/gpt2/run_gpt2_txtcls.yaml,sha256=sHju2HfzEwys55ZY6JfB6jokLySpib3hCoD4seXHcZU,4299
-configs/gpt2/run_gpt2_xl.yaml,sha256=iufDN7aqe4-vtiFsa_kgBLThhQc9hErPxbwl7BzZhLM,4669
-configs/gpt2/run_gpt2_xl_lora.yaml,sha256=l_C9j5Oe2ToyrvMLmFACW0OFHFJfpawYLBssH8cTcFA,4907
-configs/llama/run_llama_13b.yaml,sha256=3mHYkplQC5q7oPoAqURd5aSZglcTTRQVUzm80RRSdUI,5024
-configs/llama/run_llama_13b_910b.yaml,sha256=PiG63uT1uXznbjxHqBIFKQiyZ8aBSYsQ6AxxwLxtqpA,5028
-configs/llama/run_llama_7b.yaml,sha256=KKZg1PMXKTWzj3qcjJDHreb8JmFr-ITbHQnCSvps2vo,5020
-configs/llama/run_llama_7b_910b.yaml,sha256=L1d-SqzxSt5gGIvoPfq9YW7CM6dPHEf5Rtv8d-hOHRc,5019
-configs/llama/run_llama_7b_lora.yaml,sha256=AVxIpZtbqLWtsftT19UfQa4O22Chykb2U8-gtiYVUcM,5473
-configs/llama2/predict_llama2_13b.yaml,sha256=wKw_67QogvCC3zV6tc0DQrSmKJa0MFnF7Ufq5gZvTP8,4023
-configs/llama2/predict_llama2_70b.yaml,sha256=XCavW9DoZAbhsBXomYXd9gAGWaE52K3_fUSj4s2vE8s,4246
-configs/llama2/predict_llama2_7b.yaml,sha256=4ehxts72IbPYm0yWStzy67_CyYoHzJyWAZS2FQmBUZw,5165
-configs/llama2/run_llama2_13b.yaml,sha256=7XHcU9CVSBe0p4hIbuUVUgugzg7ChY2oZVesW7iaIng,5275
-configs/llama2/run_llama2_13b_910b.yaml,sha256=5c22cqbvGfQlZ6G5-fxd8jli6NXnESLrbltY3hMbVNE,5086
-configs/llama2/run_llama2_13b_910b_auto_parallel.yaml,sha256=panH-lpdrAU7F3TX0rQRb-du0ryBMCfixU9jRr4VvMk,5507
-configs/llama2/run_llama2_13b_910b_finetune.yaml,sha256=EOAjVYuU6wCTS0rR1u-WwE2p_-ml8DSa_wPpkWOFbp4,5139
-configs/llama2/run_llama2_13b_bf16_800T_A2_finetune.yaml,sha256=DKlOyE6E1gHLgvEO8SgLKVjKK0lmLvLV8N2YG-ZtUok,5063
-configs/llama2/run_llama2_13b_lora_910b.yaml,sha256=KpkctHfi2_o8od6TBZdSCEiaVJILTqQ8U5R--8EcNSI,5272
-configs/llama2/run_llama2_70b.yaml,sha256=fzqhxr2VhB25yO3vqtGOZfOblCEfj7wyzMAh4rDLFg4,5324
-configs/llama2/run_llama2_70b_910b.yaml,sha256=JAUuVgHfqbbACQCFofIuVCfX_4vMBJAP1Du_gtpS-xE,5334
-configs/llama2/run_llama2_70b_910b_auto_parallel.yaml,sha256=x04fyFfwY-eYgCOZl-8p65vtfQ3pCcqr0tWu69mma-I,5751
-configs/llama2/run_llama2_70b_910b_finetune.yaml,sha256=qYPRnBRlqYE0qcyBw36PitGsAmyMHP95i_U6Ve2BYoQ,5354
-configs/llama2/run_llama2_70b_bf16_800T_A2.yaml,sha256=FkEn6lKkzvraQQ53TSa_P0nJsvlzueIZpM4wEeJMsqo,5259
-configs/llama2/run_llama2_7b.yaml,sha256=AeknBxCCyT0bf2-7szmpWBUXaCFHCbgpivWozn94sAI,5265
+configs/gpt2/run_gpt2.yaml,sha256=swcp1UFeVO91ZJvsP940Ntiylvq07LOvvEPf0GsVIBg,4376
+configs/gpt2/run_gpt2_13b.yaml,sha256=3tR91JQ5AA7nB5lCBJrdK0UUC_AR0nt9I5nelBaSAWg,4753
+configs/gpt2/run_gpt2_13b_910b.yaml,sha256=xs8ImBhIFZuhz6ukNYVjgwNR8ONLqfJogPWWz8W2VH0,4897
+configs/gpt2/run_gpt2_52b.yaml,sha256=4ZcYJXh3wSAhtyvbTX8Hk7gKBR5mD_RqhjVzaEYagSw,4703
+configs/gpt2/run_gpt2_lora.yaml,sha256=7tnYr2hk8PWHNVltoBqGhh1JHK8MJExZFaoRpHlpRW4,4451
+configs/gpt2/run_gpt2_txtcls.yaml,sha256=VSeOaq-wD9O7QfyVMELg64e016GQlPSpAa22Y5z6VJs,4336
+configs/gpt2/run_gpt2_xl.yaml,sha256=tkaKuntYyXRaYS1Qx0QpsCybflKPrDJMP6Zrc0az6wE,4706
+configs/gpt2/run_gpt2_xl_lora.yaml,sha256=sXC3ffpu8P7yLYrCfNHhXMnYqMXARtAp6b3XImnJrUA,4944
+configs/llama/run_llama_13b.yaml,sha256=FmiET1N2Dq94NGhLPgp7O2ylji7cfvfvxj-dAclHItM,5173
+configs/llama/run_llama_13b_910b.yaml,sha256=uFDLSPspQxIM8zRUoWulgisCC2PkaxcbDZZ8jWPj4Fs,5176
+configs/llama/run_llama_7b.yaml,sha256=3xCwffaPpiNS45fjWCz40sPYPEYlAf3XN6t7hcn9u6g,5169
+configs/llama/run_llama_7b_910b.yaml,sha256=CGbewna6u1Tq_pUdBBl8LzN-_VOKSnCU063RMamNH00,5167
+configs/llama/run_llama_7b_lora.yaml,sha256=CxhPIfVrqGnkI6sejAtLw4z145pnl8rpPMmAS0ckctM,5625
+configs/llama2/export_llama2_13b.yaml,sha256=etkMnV6erB8s3764A2etAs4lleio2LPXXobGhxALets,2890
+configs/llama2/export_llama2_7b.yaml,sha256=5J0gR3dMzGDvv8vThlCQjw6CV8vNoluNZtqMldygUJg,2721
+configs/llama2/predict_llama2_70b_910b.yaml,sha256=N8aa4MTGbNAlpR8bxou4E5vgFHpxYGec9kQe0WCboCs,4056
+configs/llama2/run_llama2_13b.yaml,sha256=hbMoLjEj1Su7st_2EXi7-FsvT-15BWUWDEf63JkbmZ0,5424
+configs/llama2/run_llama2_13b_910b.yaml,sha256=xkNqDwO85dch6hysr_ie650DOdfKznPnKDBHJB7na4E,5087
+configs/llama2/run_llama2_13b_910b_auto_parallel.yaml,sha256=sfT8kakOTCqYE6PwklYehDKUsEbLeBoZvTgVsWY-IAI,5655
+configs/llama2/run_llama2_13b_910b_finetune.yaml,sha256=vgSM_1k889p7ddfG6nJzbr9dRCorAj8MXlCMV_IQQTQ,5137
+configs/llama2/run_llama2_13b_lora_910b.yaml,sha256=OTnJ2ihN7AMk41BUqJNk3eB2tKziim6UTrJUmTNSNWc,5420
+configs/llama2/run_llama2_70b.yaml,sha256=5fpG-mkqzL0a8-XrGGpZXQVKKt8aa90urxIpZ1JVQfQ,5473
+configs/llama2/run_llama2_70b_910b.yaml,sha256=7hhwxdw907wP4h9zHCcFGnJW6kqwDBD0_vs8Ivd_26E,5332
+configs/llama2/run_llama2_70b_910b_auto_parallel.yaml,sha256=95dk7KiT5xH9fU0lNJ7IjmeqbzQq8LES2RIGUnDnJbE,5897
+configs/llama2/run_llama2_70b_910b_finetune.yaml,sha256=xLYRLIvgEWLa5llmI2nn9AFYkahrs9ltsewanGdojSU,5378
+configs/llama2/run_llama2_70b_bf16_910b.yaml,sha256=sWutOLAQlzNkjT2_oF1KmIe4F_iubQRpAnaicp213SQ,5469
+configs/llama2/run_llama2_7b.yaml,sha256=m_JOfChmRBmUbUF3MnkKQxJIjifGVQRoSbefvnSXn2Y,5414
 configs/llama2/run_llama2_7b_910b.yaml,sha256=7Cprq_WrcAMv1F7Hv0fq_MciBNjvt_JK0B3hDLOg3A0,5100
-configs/llama2/run_llama2_7b_910b_auto_parallel.yaml,sha256=W9XrNNW4Xz585_3S2vQyIZ834HnHdQeV4SMBeaYRSnc,5484
+configs/llama2/run_llama2_7b_910b_auto_parallel.yaml,sha256=4Pyypi1z7xKHMSvydkG00CJjOzA7N-ITMtXtzRdbHRM,5632
 configs/llama2/run_llama2_7b_910b_finetune.yaml,sha256=FYun0XLD1hf0e98fanmRm31rdU4bGkHTp_R81Ccd7dk,5113
-configs/llama2/run_llama2_7b_bf16_800T_A2_finetune.yaml,sha256=ot3FR6DWKp58D44PyaDgB_8_O8HhRCHT0EPg1c_tctE,5023
-configs/llama2/run_llama2_7b_lora_910b.yaml,sha256=mLPJl8x5gpfM1LcWrngQfHFWIRdvLZsLXl8LLgeVilc,5304
+configs/llama2/run_llama2_7b_lora_910b.yaml,sha256=qzasyo7o6YRPonT16bk9D48w7csWK0615irAliO_m-o,5452
 configs/mae/run_mae_vit_base_p16_224_800ep.yaml,sha256=k_1oK8Z6YjPuhlokUKyneRl-mDP4CV3lQ8_p59l8nUU,4785
 configs/pangualpha/run_pangualpha_13b.yaml,sha256=Njz1B-sB8fJP65eDf1xdVLifVFAnOUgH9g-Odc63hhk,4942
 configs/pangualpha/run_pangualpha_2_6b.yaml,sha256=bJOc2rnSDgNQowI5IcnK8U_yQwYjRmLWa-tkskiacIU,4791
 configs/pangualpha/run_pangualpha_2_6b_em_f1.yaml,sha256=w_H7ffl4sEykYtFfky83r-zOCCFTJVCuLklYLELmtHQ,4229
 configs/pangualpha/run_pangualpha_2_6b_prompt_txtcls.yaml,sha256=vaQlqe28Ne6Qgi6u_Z0a6kO_2FtbDJT5_WTKkMCTSmQ,4495
 configs/qa/run_qa_bert_base_uncased.yaml,sha256=wcKbfloeEEX5v27UYqqyeXJCARhiyUdElAsYaW0pFXc,5531
 configs/sam/run_sam_vit-b.yaml,sha256=4R16ueibxjfwfQNJXyYVgIQ__xEbCqMY3QzwHOzclbg,6823
@@ -91,46 +97,45 @@
 configs/t5/run_t5_small_on_wmt16.yaml,sha256=JSE8TBEnCYM6RAZd53vBpIU8HlyisrPmdBLDz0h2DgU,4494
 configs/t5/run_t5_tiny_on_wmt16.yaml,sha256=YcBNgrSl0YWkfXSP9YrQnXIMtY3EB1jqWkhe_rfqLz4,4455
 configs/tokcls/run_tokcls_bert_base_chinese.yaml,sha256=itd5Wk3zGYHunpUVI_oic1SKK1gwe3nwJIa4d8Y_Frs,5772
 configs/tokcls/run_tokcls_bert_base_chinese_cluener.yaml,sha256=GeRdztBCVDejR5t-lsw7rtqelOB1PHA2rVVygfxHhEE,5788
 configs/txtcls/run_txtcls_bert_base_uncased.yaml,sha256=gnymRAUiIEFLScDD_cwvhuKvBMe7AnYg0eFN4357VOg,4428
 configs/txtcls/run_txtcls_bert_base_uncased_mnli.yaml,sha256=7ePBhRKSAbhuiVJOSAAhRZWBKuiGcubkO3FKDiZ0lws,4438
 configs/vit/run_vit_base_p16_224_100ep.yaml,sha256=9veAu-i0_OSmP-91f5rB79XC_kvKM8cKg5SU12neXqs,6021
-mindformers/.commit_id,sha256=2Lf4dWz1c-la6AQuGyCxCp2_TUNrJrok_F9Os53icXc,243
-mindformers/__init__.py,sha256=ER7q5aYxJ8sfpG2q01QAkXy3Q5ObM2SmqbTQp9z7wAI,1577
-mindformers/auto_class.py,sha256=16285qFJpLicsNt7obmho47vgdtyuX0GUB6IwYtOCgI,38831
-mindformers/mindformer_book.py,sha256=_cOu9txSfeW3Js-R7Eu7RyLPUnOa3gStc5UpIR5G_4o,68882
-mindformers/model_runner.py,sha256=ykVHMu_OUc2MMnFhiF4t9s2sfT-xUGi0vBKxCRsGX-c,14332
-mindformers/version_control.py,sha256=nXwWZ-8HiBbvSJ9B8xWaKQmz6mmUc6cJZL7lxQgfY5o,11269
+mindformers/.commit_id,sha256=hR1bde8DhxhovVeUNpOxilLLvNE7fwj19-U6Hg9TjM4,215
+mindformers/__init__.py,sha256=2v1Z3a_-WX4D4eIuwU4h5srVxwKMOuRNGeBO5_D9lCM,1374
+mindformers/auto_class.py,sha256=Lo_QY8OnpnWXf3-istnRZ1FcxA_lA4eVgZRl7BihQL4,39713
+mindformers/mindformer_book.py,sha256=LE4EHlz4gLgQFqx5ylSSSvGhs5nhKpDQ7SCxrUA0LJY,68039
+mindformers/version_control.py,sha256=LO_QGD9zLyunjfvEJjo5kGDbusvBV_ooBP27wO1qP3Y,12378
 mindformers/core/__init__.py,sha256=B0UkFSF7dBbTq6RGvxc48dDWGStriydy0r5KveU3o_g,1345
 mindformers/core/clip_grad.py,sha256=3fcLDH4IvQGU-zIJh_a1x-WMzfEKD94WzS8PGjvxRg8,4002
 mindformers/core/parallel_config.py,sha256=19lX7QESeU3A31wY8qFm5zEdI8F8-JbyFHpYXM5EeTA,4315
 mindformers/core/callback/__init__.py,sha256=IldgJVA6wzO2z6K1Ee4ok0nchHEfdWpi1E7AASPt8iQ,809
 mindformers/core/callback/build_callback.py,sha256=yeP_njaaoNXyR1VOSyMbyIbo3f5Dd8_FHfnJ3f4tbFI,3309
 mindformers/core/callback/callback.py,sha256=cpilld679pB-wxbwrAow7jgU2O6bTnfgBilVwbLQQY4,46230
 mindformers/core/context/__init__.py,sha256=5EE_E-1ZrZbK2xMlfHgzh0-kOYkCh7QgmUCpflw-X7I,795
 mindformers/core/context/build_context.py,sha256=mK4eOV4T9khOlqo3dFPlC4173IU8PH7a7JGsN9csTvQ,7754
 mindformers/core/loss/__init__.py,sha256=L6BFg1ZN13rN4PZAlmC_rbAE32PzL4HQwemF7elOIIc,789
 mindformers/core/loss/build_loss.py,sha256=1ekYinU7IO8gunqD-aWqwZXvzLOZJpNWiGmO5-X3sxA,2866
 mindformers/core/loss/loss.py,sha256=znXqjPUE9SU9aDw31UBwmoaNvPNwykyyRmE5ioNnIgM,17965
 mindformers/core/lr/__init__.py,sha256=r-LXKf2VvEH_utdhKI_Qr69tsW3o6bQUCxJHqtZYNEo,802
 mindformers/core/lr/build_lr.py,sha256=RiyaS2rYh1_j-XXfkgQQG3t2OICcyvcQC7JXx92438c,4178
-mindformers/core/lr/lr_schedule.py,sha256=f4lnad2Gm9SlG6LTSxq0-ONEjLBnwFZfmmVcdAjkExA,24986
+mindformers/core/lr/lr_schedule.py,sha256=30mRQVb_CrE76hrCZZBeY7wDdjq4xm9t_fnVEuwarss,21488
 mindformers/core/metric/__init__.py,sha256=IfsAaFAeprlESACZUKNCafFL1GmSsifONbXFCYsMPRI,800
 mindformers/core/metric/build_metric.py,sha256=2bPJU76DSAxzl2eB_c2VqFfULEWvEbPME3eP1as-ago,2683
 mindformers/core/metric/metric.py,sha256=Vw9PAOevlorcUwcS8NEiocNWivy6lf2s1gLLeqE46eg,35009
 mindformers/core/metric/utils.py,sha256=YYI1Tu1xZ4tBEEmROFZ7rknwlDVeaV5sC2xCCXZfbzo,1768
 mindformers/core/optim/__init__.py,sha256=G-t1W5DIo2DM2c1AxptfAzTKPSU21yXry90Lo4r3X_M,847
 mindformers/core/optim/build_optim.py,sha256=XcyRCM2nYIGIvooYAyDrmNrwWnZOVzqTdf0VwzDRiK4,4623
-mindformers/core/optim/came.py,sha256=16oOmvz81kWIh63E3-rLum94U8yoCMIB3sKwlh_nMUY,21317
+mindformers/core/optim/came.py,sha256=4wJhrBLXPPfIjreRnEv3yV33e10GBcoaoyEgO1zwi3s,21063
 mindformers/core/optim/optim.py,sha256=ZDCzHDUnME-P6mz_8sF-fKJCly4E1SzGPo_wDMUcwSs,31399
 mindformers/dataset/__init__.py,sha256=8UjFf7SDaFYe52WwLPGaS-z5x_EBP6dYx7zg0ga2zr8,2608
 mindformers/dataset/base_dataset.py,sha256=ui18x6vOwt0arfljXSj94y4EYqEhaUrDeBb3pxcMvow,3379
 mindformers/dataset/build_dataset.py,sha256=KkfAMd382UP-EnDMnwhJ7nHaNh-JWmrNs8rXPDfgaGw,2560
-mindformers/dataset/causal_language_model_dataset.py,sha256=yic1Sq6nZ3OPvuXcxTh-MnMLUWFATIrB-1dU7-swlLI,12910
+mindformers/dataset/causal_language_model_dataset.py,sha256=3WnILxrDBsxf6gHc-63OTBKyBBARPimbjtpsKPbyMEM,12908
 mindformers/dataset/contrastive_language_image_pretrain_dataset.py,sha256=yADISmLe_pnzv4EF9I0aeYIgwFCDsnXPRp4FXWtC5FA,9758
 mindformers/dataset/general_dataset.py,sha256=j-8AJh3M1gXI4Fo5O7C6UFt5GypyPusRkOQJfKcUKuk,2998
 mindformers/dataset/img_cls_dataset.py,sha256=wGEtyFYiNzvrvd0uoRJ2AC7dPYCS4Gjn4tVecZ0HC8k,9994
 mindformers/dataset/keyword_gen_dataset.py,sha256=Kitmsz-3EbDPytO7qqw-nmJoHg0cx8NvAVqwQFs0ehY,21959
 mindformers/dataset/labels.py,sha256=euzaO-ZQeWlZEbU9gHxyWDLLzjva07Mo8qcBnL_Tr8U,16192
 mindformers/dataset/mask_language_model_dataset.py,sha256=Kd5QvLV2wmtgPEQbzkUnLoXAc8WX3DQKKBIP-Y5rluA,8478
 mindformers/dataset/mim_dataset.py,sha256=qr3SniQrHovfgJtGtqPU0gczmLeNyvzLVk6iR1VG3Tg,9100
@@ -151,168 +156,164 @@
 mindformers/dataset/dataloader/flickr8k_dataloader.py,sha256=Dc1UTYzBQzcVHNgYeEMuq49AMTAUpfUh99bVCtfVM_0,7099
 mindformers/dataset/dataloader/multi_image_cap_dataloader.py,sha256=yzLHkuGrOzfaOk_xQqrAtuYhrU048FFCc1Dx12KlMGA,6483
 mindformers/dataset/dataloader/multi_source_dataloader.py,sha256=7aIjZOaYpG9agcLo02TOTzXJ-s2S66VT2lc4LvjJpow,11335
 mindformers/dataset/dataloader/sft_dataloader.py,sha256=GaD32UlOUAOlTVwm0WGkqYMH2Hku7yWoat2IuNasFoA,12659
 mindformers/dataset/dataloader/sft_map_functions.py,sha256=KeNVCqBjoeD9j0P1ErqXdDNz_jOViw2e3541kg_KDbw,7423
 mindformers/dataset/dataloader/squad_dataloader.py,sha256=_U5AbqVZTrF38HkPiewAtnPQ0XIiojEqE5pCQjT6nV8,23889
 mindformers/dataset/dataloader/toolaplaca_dataloader.py,sha256=ERnB8A0wvm6oETpdD2IF0-1iSRnmuLyU3Chmxv4zMwo,7902
-mindformers/dataset/dataloader/training_dataloader.py,sha256=hiHmSrFCEmJ1wEggUb4xQbVIB9A3Hw8D5T181ubs2gQ,20483
+mindformers/dataset/dataloader/training_dataloader.py,sha256=F0NWbgszfmU7yS3QZ82JRuDgq90Jw8yUAcyyHNkrA30,20177
 mindformers/dataset/dataloader/wmt16_dataloader.py,sha256=lMSmKS6uGvSMKwah4GoUZfJgxkcU6EGrCoZN-rr7eJo,4360
 mindformers/dataset/mask/__init__.py,sha256=PcJwH3UeN9fdG8M_ddoyfSELEctfkSxVVQvsHyQfZGY,808
 mindformers/dataset/mask/build_mask.py,sha256=BjVwDbtmBJpRzQLl4rmJx2CAr8pPe_v_SqWvovsd4A0,2196
 mindformers/dataset/mask/vision_mask.py,sha256=jtdDItJuvHJKJGtmlgYbo3Apo4txukkjsuD3MtcIRiY,3760
 mindformers/dataset/sampler/__init__.py,sha256=Zt9oaUVGFlN0Pee544YlIQwMlYs_cD4VdKrrnImiF4c,754
 mindformers/dataset/sampler/build_sampler.py,sha256=UkpRrNzvjtV1FG_jfKlDYC7Bn1M4TST1AmTzN0c0FgQ,2719
 mindformers/dataset/transforms/__init__.py,sha256=N8-61FzOn1avMIg_TSJ4fxvq__zJI2Jmld-0uwbC8zs,1192
 mindformers/dataset/transforms/auto_augment.py,sha256=JAIGFf4aIvQaVg51bVqx4JsxtUoHuTmq6mJBuJimaY4,33409
 mindformers/dataset/transforms/build_transforms.py,sha256=b6cHR9WJwIQ6QObCnLLHRLGhbrtdJcDHxKI2V14gFOU,3364
 mindformers/dataset/transforms/mixup.py,sha256=7qrp-Ne0jC3TgVKexHb7R0Vu__ZO_d8vYpj3uVNMp8o,11513
 mindformers/dataset/transforms/random_erasing.py,sha256=i5NmEyCP2M1ULedOa7T_VIODN7A5XJzMh-6JEVIs1zQ,4846
 mindformers/dataset/transforms/text_transforms.py,sha256=ulIRtJRsYj5cAUieWQ6K2Mv0gT6ojLo5LtCch9BeYT8,6231
 mindformers/dataset/transforms/vision_transforms.py,sha256=xrQ5cxYnFjnA6GmGZ17bHKTXhC9cULJFLEk27R8PEqI,12281
-mindformers/experimental/__init__.py,sha256=TykogpkvMXYhf41LmrvBufiEnz4OcRVtbH6OjS-ac6s,855
-mindformers/experimental/distri_ckpt/__init__.py,sha256=3M9fGTBcFAlI_f_J5jCkUL-MwccqtovPffjsFJL6h6I,787
-mindformers/experimental/distri_ckpt/checkpointing.py,sha256=o6CC12Uakj2uaYSbycvkb4lRC2Sx9I1Fm81wIsiXFlk,7136
-mindformers/experimental/distri_cores/__init__.py,sha256=B1ZfFvnah7_x2sUoAkSqmwS2d8vwJ9uB6supJEqRYe0,786
-mindformers/experimental/distri_cores/activation_checkpointing/__init__.py,sha256=t-SF1qlWbWJrOkQLP8GiT9qOlBkMeqq2lAjmWdnySUU,712
-mindformers/experimental/distri_cores/optimizer/__init__.py,sha256=YsHBcs8lBEaxNHU5XWrDaKS8-qTPFsp4G-J62YXDU0c,776
-mindformers/experimental/distri_cores/optimizer/grads_accumulate/__init__.py,sha256=t-SF1qlWbWJrOkQLP8GiT9qOlBkMeqq2lAjmWdnySUU,712
-mindformers/experimental/distri_cores/optimizer/zero/__init__.py,sha256=jrFLs1cKUsSEbIer-E1Lnilr4dbiGuIgTVEJZ2AWMD8,789
-mindformers/experimental/distri_cores/optimizer/zero/adamw.py,sha256=0sGeG3HHBX5mzZ9NYuK4fOHix_BIebTOlS_Nvw4NZ3w,14836
-mindformers/experimental/distri_cores/pipeline_parallel/__init__.py,sha256=t-SF1qlWbWJrOkQLP8GiT9qOlBkMeqq2lAjmWdnySUU,712
-mindformers/experimental/distri_cores/sequence_parallel/__init__.py,sha256=t-SF1qlWbWJrOkQLP8GiT9qOlBkMeqq2lAjmWdnySUU,712
-mindformers/experimental/distri_cores/sequence_parallel/ring_attention.py,sha256=gCF9tvjcwKpX3vXVfTS9DfqdtEIW-u3hMwiZHiYBhyI,19713
-mindformers/experimental/distri_cores/sequence_parallel/utils.py,sha256=SkGCO2-_8cnVobSNAlIoQ7pN5yYqr-pfmf1V9t4addA,6853
-mindformers/experimental/distri_cores/tensor_parallel/__init__.py,sha256=t-SF1qlWbWJrOkQLP8GiT9qOlBkMeqq2lAjmWdnySUU,712
 mindformers/generation/__init__.py,sha256=eXETrOThoLytQzYnNTNmYQCdvxgTjQSDb2ces7LkFIY,984
 mindformers/generation/beam_search.py,sha256=wNAmKqF7qS7hxD0W5SK5x-O-uO7IcBv8lr6fSTuGcjU,18947
-mindformers/generation/generation_config.py,sha256=aBt923tclGPcl4z2UCP7K9GWi96W-3g-NaEr3zOrMRM,9241
+mindformers/generation/generation_config.py,sha256=S4Ljlf6woCBS13VCTN3jln4pyYm7fRFYpGnpbQJ4NjE,9205
 mindformers/generation/logits_process.py,sha256=LBeEUaqDgMF8PxpBzoOUHiWdSTm9lVFqEcCFshf2Lcc,14147
-mindformers/generation/streamers.py,sha256=N2qVEBfLBb7xN69uV4Y2GHju55gJZuDPV-ezQvOUuV8,11881
-mindformers/generation/text_generator.py,sha256=cV2XoR5mcTuMFhPxa9awG8YZKZsPhrZ4rCR7JPhX0WU,57851
+mindformers/generation/streamers.py,sha256=3xrhRDnVMp9FxyeI5zURjervHxyiLDtrK4NmPmoWk_w,11805
+mindformers/generation/text_generator.py,sha256=euif2WGCZzs0K6A7ZLKQUmC6o31FjJC8byRMnZ7Khzg,57463
 mindformers/generation/utils.py,sha256=pdc0-bwGdWMpTe6eHyvtljgJBmtSKbu5y65TXUjVh6Y,2956
-mindformers/models/__init__.py,sha256=EKVtGYjWrLDIKRQcttglk1CCwh-U8dvgZH7VkTZOCzw,2354
+mindformers/inference/__init__.py,sha256=jp5KKZOJp_XZt8Ac2y8LR_df5uK9QL-1Bjbh1QthEKw,862
+mindformers/inference/context.py,sha256=pWsZLvUY5XICQCZmE46F43c_LzQKmQAEgNofVJkVv0s,1406
+mindformers/inference/infer_config.py,sha256=IrNnXYLGUMpYD5xiaMH4LRU2pWg7OfQcFhiqtIWPN5Q,2532
+mindformers/inference/infer_task.py,sha256=z09VDeZ9RxbcxMaxiX4vgsvXgKa8SK9G9KX2DYMNbvw,1930
+mindformers/inference/pipeline.py,sha256=x2HVte87Lb9yTNED-TEIhx3-COO5CKRZz6voXS-kue0,10760
+mindformers/inference/postprocess_sampler.py,sha256=CmjGPt5ofAqjPIIGo3PxNIPJXTOj9KfCdh67Lu7uscA,2561
+mindformers/inference/infers/__init__.py,sha256=bZH2bUaomEs9XwJxjGs6VPC1B1aBADKInSKu90vGmuc,704
+mindformers/inference/infers/base_infer.py,sha256=ro6M-66VS03l3UaFbdiyfe-RY_89OmZqQOvYjBEo61o,8454
+mindformers/inference/infers/cache_engine.py,sha256=cDZroa8oifXDBYNigkqcXXJTPfQpZ1r5LGvHBsHwVIA,3234
+mindformers/inference/infers/text_generator_infer.py,sha256=BPpGJvmy71Yt0Patk_bEpzwf9kmAeWLkiOLprlLWagg,31740
+mindformers/models/__init__.py,sha256=gs82HXaUXd8cXoyu3XI1axQlZjnKM8g250s3zimhJ3I,2390
 mindformers/models/base_config.py,sha256=4XepQsvgGAduTKNclHhnv4fR0eoj26MKLCKMGamy96Q,10530
-mindformers/models/base_model.py,sha256=vHfg2Q4mE4AJYeGf7jx9urUZJqdfSgXUtEngr6Dq-aE,16983
+mindformers/models/base_model.py,sha256=Vwqg5KSyeMkw0VVicIr1g7fzypVUMf-EF_NjxS4NNkQ,17400
 mindformers/models/base_processor.py,sha256=Y9t6l-YXctQhm8PPT_t7mDcB9jAmucr5EKtTBS7aJmI,12083
 mindformers/models/build_config.py,sha256=Q401sHTaRZtWYYnUUZZb88CbWgPedHCqQ-kC1us0F2Y,2681
-mindformers/models/build_model.py,sha256=hlcZVBLURYpgc0sYHilBM7myQwstNhKFo8rA09_A21o,5053
+mindformers/models/build_model.py,sha256=Up6GQJD5aKFa7nMyBTAeC7eZpB9xOWV3OYOglSjdJkk,4427
 mindformers/models/build_processor.py,sha256=sPOz7Z50fk8egiG_jdDtHoAqX1tk78B64VVuIDJQqH8,2461
 mindformers/models/build_tokenizer.py,sha256=zxDKrhlUaING57WxfNHqMzODGRGg7DHEvaGDbz_BH1g,3570
 mindformers/models/configuration_utils.py,sha256=Vrjd31aLG4Oarcevc8ORfIWIeqj3wPcjWV_JLHHFvs8,35454
 mindformers/models/convert_slow_tokenizer.py,sha256=Vq-HucrhpcBizuOuPeYmw7RnyUqBRD-czUvs6p91Hy4,16629
 mindformers/models/image_processing_utils.py,sha256=f6T--e8_wcTDhMfueFwTsjlyA9kSHZAcRDIhOw_NHA8,32672
-mindformers/models/modeling_utils.py,sha256=eD9F-sNDEwBfHyjJqBZ-W7-TNp04eOcO19YSSQ2NyOI,70053
+mindformers/models/modeling_utils.py,sha256=vz-rkyq8gI06DgfBXcZhUECS2V3Wx_d-IqG9Ph9qK5k,69023
 mindformers/models/processing_utils.py,sha256=Oh6jD1lZ33q65tRZKk_VQJQlQbrBs16GN--PQNMYzt4,21177
 mindformers/models/sentencepiece_model_pb2.py,sha256=RS-rPV_XxiHPKYQtTPfYYzpNjpN9Dndx2-JHW_KCSpg,50997
 mindformers/models/sentencepiece_model_pb2_new.py,sha256=gAKWf8kz4v2rU4He10CJG86yjC3I5MwMPVRd10LRJjI,6645
-mindformers/models/tokenization_utils.py,sha256=DUSyLjgcZhXVrz1DKoKB_JbUzDqUDkFo1hGD4LJmLlc,47092
-mindformers/models/tokenization_utils_base.py,sha256=8mH3_H7Cs276OmKXyzhSqC2KgblzN3e5jf74WMRzkWA,210149
-mindformers/models/tokenization_utils_fast.py,sha256=Di-I_t5g77haBo9BjiQmj0P1XMvZUKLhSrwNad-mvcM,38916
-mindformers/models/utils.py,sha256=splczDAOJYm8U-maMgLe1IyV_9jpMec2uwEnOuDYnQQ,4969
+mindformers/models/tokenization_utils.py,sha256=bG9oDaKxKZBwpKW_IEatUUidCHmecH5Q-K7ieRq3mlQ,47042
+mindformers/models/tokenization_utils_base.py,sha256=Z2B2g_yIOvjGJTbdzI9RRPevNRxNzdraH3b_T48mx10,209768
+mindformers/models/tokenization_utils_fast.py,sha256=SnwQjo5bE3iUjYeYVbeU-8Q3GeKVuCaKrWHpKE0qEWY,38870
+mindformers/models/utils.py,sha256=iR5_JwviiO10_cgXpJP2E0_v_uN0fmy9ApfySw3CkOI,2021
 mindformers/models/auto/__init__.py,sha256=k35QK11fqOFBXJWzezgHSQ7Jg76PajNADgBPNU-FjXU,2334
-mindformers/models/auto/auto_factory.py,sha256=g322y6gZT04R4YThlvvMSN7joPeslRosUIX5kdjBsww,42029
-mindformers/models/auto/configuration_auto.py,sha256=-ziGrJqS7Dz8Nb4FuSb4x_ASF8FM9R5lctR-E4SUNc0,19644
-mindformers/models/auto/image_processing_auto.py,sha256=TkmZ_14ZmIM2_RwbUXu8sRMkXlc3tuUYIamfw0SDTKQ,17407
-mindformers/models/auto/modeling_auto.py,sha256=nFyR2lOh9xyO-si9y9LQTlH-5tIJ69V0vGA4QRXZQ4o,11169
-mindformers/models/auto/processing_auto.py,sha256=vA9uJ9ODZkurlF2dItjT51G6uYMIrutBsHhjyRJVgD8,18227
-mindformers/models/auto/tokenization_auto.py,sha256=PXuzrEbt4Sp2I4ube0lvd-5MgzZGgpBrhzb6levUBWw,32868
+mindformers/models/auto/auto_factory.py,sha256=aWF13UgdW03uFldcJS2OmNx_oQL7DjgYeZILjaaZlaA,42560
+mindformers/models/auto/configuration_auto.py,sha256=A9M8NFroIaUeXdMaIIQML9tVe4pkqmInhnucdlWXtS8,19463
+mindformers/models/auto/image_processing_auto.py,sha256=nvtXKgjiLjiFPNW2VuVEYscW_ST6NFbyz0PjsMIyAJg,17195
+mindformers/models/auto/modeling_auto.py,sha256=Iw_KOr439RqaE7La5LC-V_HH4zUNu2CmyeE-3nPewug,11172
+mindformers/models/auto/processing_auto.py,sha256=9fJlg7SxEZoUNAVkSY6LLuFuUU_7qFRivLGXebEHqBo,18016
+mindformers/models/auto/tokenization_auto.py,sha256=mxoqCq6T5pSpzP-DSbN76Xu0lbM6pa6p0SRuFMXJ64o,32587
 mindformers/models/bert/__init__.py,sha256=8bnX4f6O6gdw90pxS-OXdt7GFglXOZ4Nxf85K4RtDX0,1191
 mindformers/models/bert/bert.py,sha256=PUS1piY0yUOrHrTRw07II0P8v9f-Ec6_q6Ib3ofJfPA,29699
 mindformers/models/bert/bert_config.py,sha256=09LIoeOVzZeTi8P4_2djKtildBG6vk0DoebxzMxSqrE,8583
 mindformers/models/bert/bert_processor.py,sha256=_g4XUPJ3JqhYu2FjsLgjVL9oofKZZZgXNwn1xu8Dwvo,3764
 mindformers/models/bert/bert_tokenizer.py,sha256=zPDimVIkrlnzh86UbkbwbKqfzEMQQdOa1lHhM3wvg_0,25732
 mindformers/models/bert/bert_tokenizer_fast.py,sha256=wnXMH4aPID-92S46x1DcHMO75jtXhiq2ZZg0xSUHeMg,8228
 mindformers/models/bert/convert_weight.py,sha256=1NLQvxu15R0WUXZIxfrvRTBlAZMk1L4rccvphNY247Q,8638
 mindformers/models/blip2/__init__.py,sha256=ZztycDWajP5Zr4TH9vyiyDx4w2GhaEf4ga1vJoZf4jA,1189
 mindformers/models/blip2/blip2.py,sha256=PZlnvWQV6y2KXoW13GGkrVXoZiD8kIIJPHyhNtEH2iU,4045
-mindformers/models/blip2/blip2_config.py,sha256=8HAGBWrgr8_fUT2HrRH9NAsAlBnP1UqrNoFLGweedlA,7102
-mindformers/models/blip2/blip2_itm_evaluator.py,sha256=A1fTJu-m5VLZ6tEs6XcRVu-NjK-phOpZBSKf_sMYCCA,9582
-mindformers/models/blip2/blip2_llama.py,sha256=h9e90N6RlW5dMphKOp8SaahQttv8kepFsjgBVgaaE9E,7861
+mindformers/models/blip2/blip2_config.py,sha256=td6JHyTqA5KjkCre5e2-JKm2xhxlaoGbtmjOBci1BIk,6941
+mindformers/models/blip2/blip2_itm_evaluator.py,sha256=g3SjoQOhZ9zyxXAs5Ep_ujouVKY2d_noEfZJwYj3--s,9584
+mindformers/models/blip2/blip2_llama.py,sha256=Zc6nP9bXoLttFqbEfDpQWVJbEHwsbs9ktGBup8Sgrts,8142
 mindformers/models/blip2/blip2_llm.py,sha256=NFAC0DeccdjVv_bxuY2hRERa0UEhSZ7vTr2sP15B1Xg,11459
 mindformers/models/blip2/blip2_processor.py,sha256=OWNHEOGYT2TSG1G0WNe--GW-RyqVmqtpGi-gyX33X3M,8689
 mindformers/models/blip2/blip2_qformer.py,sha256=VQ-mX0RqAB_xDaVsjfhKYSnNfsPlPgrlwENwUKRPPV0,23783
 mindformers/models/blip2/blip2_vit.py,sha256=yzUIKc3YV0a_FSqsIsFW1ml8WCbIHHq_9Yya9h6lNwY,1501
 mindformers/models/blip2/convert_reversed.py,sha256=cXoqGNKZRT2v1yLIE2crEHLN_9NovM6CtZHbklAPmsw,4365
 mindformers/models/blip2/convert_weight.py,sha256=CDNI40BqqLRxXUdnjjIgcs8cBVIlAEcKxQguGw72I34,5482
 mindformers/models/blip2/layers.py,sha256=boTZuGxixWFqe-RLOQP4TIu5FmHyA0EwkDYUU-DCdxE,4115
 mindformers/models/blip2/qformer.py,sha256=Ldo7qKJCW2XQQD0lk3E1hQgNhpHIiJMK9_yxqxolmHU,70439
 mindformers/models/blip2/qformer_config.py,sha256=2jORGF0J88221m_sWpeJd8QeRPGD6EoN-kVjuf3523M,5191
 mindformers/models/bloom/__init__.py,sha256=h30slKezFmVpvmG8eDR5yr0a-pDGmFOyV_inzSt_3c4,1137
-mindformers/models/bloom/bloom.py,sha256=ox3ps0VUV4PKRjLlUUFMS9ZTt90CTenoyWs8gx3rknE,17832
+mindformers/models/bloom/bloom.py,sha256=QCj8MEWV_IXnW2RyHG9XSu4aLEVHDZ7zS9i-B6h6jtA,17489
 mindformers/models/bloom/bloom_config.py,sha256=9qUS3juHr_k3SgVUP0w2v2xFr3kUmKS9chirS0iWLYA,9617
 mindformers/models/bloom/bloom_processor.py,sha256=t5oyDVTqeEXsHOw9SFgaSV0wZRJpxYiz5cghBBMB2WM,3632
 mindformers/models/bloom/bloom_reward.py,sha256=2x28qUKKhlnl0-1o_6MkOBh--tnIrsIDCfBzO3b2FPA,4576
 mindformers/models/bloom/bloom_tokenizer.py,sha256=GEJgPQxOM_IG4RsTW0v7qe_SWSPxvkeKkDhtWqoRd_E,10525
 mindformers/models/bloom/bloom_tokenizer_fast.py,sha256=hTT7LyMm1VqAIognBKk9OmLm90j0imxvVjUfbXMt1hc,6814
 mindformers/models/bloom/convert_reversed.py,sha256=k3MYdWk94xaqWMC5NytS1vkfsK6KoFja5NQ3jWxu-zg,5263
 mindformers/models/bloom/convert_weight.py,sha256=jDQMmT2Wul14hUe45oG87OXopJjPKGwDFiUN03Qz9DU,5668
-mindformers/models/bloom/layers.py,sha256=Pe5PlGUSSpyKO-oVXundkUk0JZkq0kZlPlL88NiEoD0,32828
+mindformers/models/bloom/layers.py,sha256=sqnpRNjDTTcL_hymjbTPtwfR3_ZEcnhdnQdZhh61p5k,32708
 mindformers/models/clip/__init__.py,sha256=GOAcQFfxjsPryUH79AB_hsv8aMQwIjVC8Abm1UXckfg,1030
 mindformers/models/clip/clip.py,sha256=dhg7wevFoFK4XKnrgQb2slOyQ3vekYufFICPl0E45ow,10413
 mindformers/models/clip/clip_config.py,sha256=uDxdzE1MyG1Fpp39RcVVCEmNrt0VpaepbSmpJO-lDG0,9192
 mindformers/models/clip/clip_modules.py,sha256=cOl-_wS31h-jq100seS7TOfx5QXTSr9MDS1l94Bay_g,10123
 mindformers/models/clip/clip_processor.py,sha256=M6wetwZMmdW2yP_xfawLZ2cHjfNuErmq_UNzuHspPFk,6379
 mindformers/models/clip/clip_tokenizer.py,sha256=Y8h_fcKx3qyJDJmhesJ5hOa1lgkU0eHxofj3t1Ayqps,11888
 mindformers/models/clip/convert_weight.py,sha256=eZKWQfYnZuHVnizddcnOBh8IdJDUo0FBx-8dooCufNw,2989
 mindformers/models/glm/__init__.py,sha256=tsVKUZjMz-fja3G0w-ShoGg-NH8vP_M7tUGNyXV7caQ,1036
 mindformers/models/glm/attention.py,sha256=qZX35qd5iuJTVqzVck7gJu9SkQU7GCXlxer9kMni6U4,20335
 mindformers/models/glm/chatglm_6b_tokenizer.py,sha256=_Xq7-E3tPRz-cOOYDakrScVmYN-OP-ItgoA-Yrf4Unk,16157
 mindformers/models/glm/convert_reversed.py,sha256=tEMPg-PfRmxU67428taPjhn-n8MXO3kdkgegtbIiIjg,2308
 mindformers/models/glm/convert_weight.py,sha256=D47_ZAmMv8KslKix__Ltfp9jywsB-m4FZ3KQoud1iOk,2458
-mindformers/models/glm/glm.py,sha256=fj3GxLoVOKdoLWp7XL-c95nN6viOu23BCUVXVjsu0d0,23408
+mindformers/models/glm/glm.py,sha256=1pTksQ1VpKFDCWA_snQKWjV9h-siR9x5rUFb-D4BbLM,22573
 mindformers/models/glm/glm_config.py,sha256=XN3h5NHHSi-uvmtk2CLf1VwPIJlPis-0_o-K1BYQyoc,12201
 mindformers/models/glm/glm_processor.py,sha256=sCUI2iGodbMnH4eyawyVp4nZv9BH6LlAMuQMXiHRdno,4446
 mindformers/models/glm/layers.py,sha256=Pvm-EXqdEaTF0iLKMA_YVpikBaded9BLBQNz0Lx4d9k,13117
 mindformers/models/glm2/__init__.py,sha256=NHnhl0EVXBoTJnEqmi5XyXTPLGRn2keFFzpeztu5uOE,901
 mindformers/models/glm2/convert_reversed.py,sha256=xrZ4BpfBeGvxr5jdoXe1QUDYw9jWLfY3MI0Mtl75Sxw,2069
 mindformers/models/glm2/convert_weight.py,sha256=_PQ3bFDnm9isb4AznugDC58L9u0WP60xVftWt2eqxrk,2172
-mindformers/models/glm2/glm2.py,sha256=PhS11km3Ds1q-xm5Sbamyn4-o9YRsRaSAFRtBEIRZXI,15785
-mindformers/models/glm2/glm2_config.py,sha256=BiGcUYrQcYRLcpYLak2AlPtEu2qdo6Ze59Dv1I76CTE,6147
-mindformers/models/glm2/glm2_modules.py,sha256=Z-kum6VucqYKjPR6ZyhA1fC5tqV-o3lE-5FTQ7jfaUU,7779
+mindformers/models/glm2/glm2.py,sha256=2XIg6W70vI5eFY9_Kdd8EUihjWhAQgTvB0d_fz97HDg,13392
+mindformers/models/glm2/glm2_config.py,sha256=PGxZ8RRFbaseVciuLAfUlFoLxhhxqbpzIwvtHYVU63o,5627
+mindformers/models/glm2/glm2_modules.py,sha256=KovBjp-GJpaKNUmpV5JHgE_MKldHz8YtwpRyX-jh2Pg,6386
 mindformers/models/glm2/glm2_tokenizer.py,sha256=q3GnNAfIf8TrJVttOCheyucOdzLt4t2BC87C9STxPAM,10952
-mindformers/models/glm2/glm2_transformer.py,sha256=2L5yqMnz9Xwvn8H5K07L5Fy8Tov4n5lSv2HQPcI8eEs,25284
+mindformers/models/glm2/glm2_transformer.py,sha256=Pv9fhbmeDbPXS6ZG22QTAqOJ460IPWttmmGWh0mzIxc,35424
 mindformers/models/glm3/__init__.py,sha256=1aRqJzTEWduImVv9MJd0K17tfUPH9gYvovTRfidr9l4,789
-mindformers/models/glm3/glm3_tokenizer.py,sha256=tUcx-W6JAugyOqXsvYazqlm2of2oPhTKQHhdLzTL0SM,17380
+mindformers/models/glm3/glm3_tokenizer.py,sha256=0k9ASdElraEVpY2AgOUaYSp-PoSY3YyR95mKWroVYeA,16878
 mindformers/models/gpt2/__init__.py,sha256=J4YrFIEGc2fuFgahxprEQLBy4EuarxG-ikjswTCSCx8,1106
 mindformers/models/gpt2/convert_reversed.py,sha256=41PSpIcz-0lbqEoA4x9DNtQ7kvCcnq8dfuJUziEaYcg,5661
 mindformers/models/gpt2/convert_weight.py,sha256=RU-DnxclgFnf8gSTA1iZjWPDuwUTy7bS-J1hj3W5JoQ,7211
-mindformers/models/gpt2/gpt2.py,sha256=M-wygdwF1kw-5sbKU5mAkNIpyo0nI4tHSmtxEeN6gkM,28509
-mindformers/models/gpt2/gpt2_config.py,sha256=8HdqQGtjlBeRw4f9Q5BmRyLwiZNNsZ5UrAS9snW306s,9790
+mindformers/models/gpt2/gpt2.py,sha256=CYCdzKQ0ZF0U7chht-z4wFLYtuQXSGRu8J_m4G8zwiQ,28921
+mindformers/models/gpt2/gpt2_config.py,sha256=viXJdWNek1fKwB_FhqC0p4P8UFzQWOcMfaZ_KV1TgJ8,9701
 mindformers/models/gpt2/gpt2_processor.py,sha256=MBZD92roAq2-CrMA8hWvt2Qs_untAZCiDlDelo_IYa4,3629
 mindformers/models/gpt2/gpt2_tokenizer.py,sha256=T1v4QJYAXvCYJ3CkNP3CMUnrew47cqBKVqlyDF81TEs,14404
 mindformers/models/gpt2/gpt2_tokenizer_fast.py,sha256=xXtX6A98o7KPz1Sm6vaPuZvEHrYJ1bxGFQ1EzMjYFlM,9082
-mindformers/models/gpt2/gpt_modules.py,sha256=PFBgrDwKzhj0qQoa7hCbLo_xWrrkFrno02VR2GRE-Sw,11209
+mindformers/models/gpt2/gpt_modules.py,sha256=6satoE4EOGxXCY1tmGfv9HiarHsvxlTQga6MyEJhHSA,11324
 mindformers/models/llama/__init__.py,sha256=5IFM9yzk7bwbZqL8jAeI5JjFV4cDcabf920vNJfqPDs,1061
 mindformers/models/llama/convert_reversed.py,sha256=MuR35E9FEz8LrRNu89i5EWCUPZrJaxTrEIpOVAhaFcM,2833
 mindformers/models/llama/convert_weight.py,sha256=Hw_wRE7eL0hCbOmDxEyKVNqMIKh7MEhh6Ig4e8vPCiI,7035
-mindformers/models/llama/llama.py,sha256=ODr4fNUAWNGU7bHE8jb_lPF7CMQURg4iSg99WtOn-A0,21015
-mindformers/models/llama/llama_config.py,sha256=5f15BF1ILbE-_8u3OiL_kpf37BJhB-KeyKTPf7bLx6o,11794
-mindformers/models/llama/llama_interleave.py,sha256=WihjW3ZJ6uog2_9-wLh7pFg4sbfyIuIP8HLoEWyWeuc,34134
-mindformers/models/llama/llama_layer.py,sha256=h6sIbhRzWaw7UCX6MM39JU5VcpZsAgkN-QOyLUneQFM,20921
+mindformers/models/llama/llama.py,sha256=uKLytJLLkFtfjUfMxmGpwUBjqIp7fEoGDsD9_ILoNYk,26734
+mindformers/models/llama/llama_config.py,sha256=5IRd94Q9ApXCoSlvSlDOCf_njLm0Yz5S8RDzTlTLKM8,11843
+mindformers/models/llama/llama_interleave.py,sha256=xTJ5pFCMEvu6pZv2s71PM8VTA7OYI9BHkAU7PR4SLcU,34597
+mindformers/models/llama/llama_layer.py,sha256=JYwe3591U0ZypDHaZ1SzNks-idi05dFW8aWI2dYO39U,27060
 mindformers/models/llama/llama_moe.py,sha256=PltFzeykeZjCNCxggpzwxi23raLd_JyoT9jyCIXNaCc,13664
 mindformers/models/llama/llama_processor.py,sha256=8Q35xhpgg5sfuwPPvk3OIoZVf5FjpJmNEWFChPpEOlg,3641
 mindformers/models/llama/llama_tokenizer.py,sha256=ivIRMYQqQ__a4S41GmvfnCtIG7ICAdAeF-8mURV8yaI,17126
-mindformers/models/llama/llama_tokenizer_fast.py,sha256=4b7_iPxjdQSR0UEY2Sn3FXabjkqowgtj7fM-3JRFjG8,9130
-mindformers/models/llama/llama_transformer.py,sha256=YHlkqYRB2zyDUh7qBSp0cRsy5JQvw_bzC6EiRZuJi-4,29001
+mindformers/models/llama/llama_tokenizer_fast.py,sha256=mhYZ90ugcYnufpwEen5VsGnZE1iL1C863cMLcC_HPY4,9038
+mindformers/models/llama/llama_transformer.py,sha256=HGc3Kk4UyMkcofLX0-FNTZL10e1iORksT0jQ7KoMQEM,34843
 mindformers/models/mae/__init__.py,sha256=IB4gzMH574udRoFXuX0SZ8G_x9nv-cwE7yE4vaEMYnI,878
 mindformers/models/mae/convert_weight.py,sha256=b4wrOj3NUomuH3pn0EImZkNq4IPZuYA5P208ywL37QE,3416
 mindformers/models/mae/mae.py,sha256=FBM7Hkp2q0t-xxTRoodly1jO2bt0HA2JjZc5els6qWs,18363
 mindformers/models/mae/mae_config.py,sha256=DTiQ7X9yV0L0RnflnjTZxc5BOSn90VL9erAh8ogAsEQ,7851
 mindformers/models/mae/mae_modules.py,sha256=DI1ZoWxa2l4-3aVRRsWhRFyBYj1hXmLYNRGDPH6aI-0,36274
 mindformers/models/mae/mae_processor.py,sha256=My-QbcC9FJIkXYKtPQcRPTV6xlT-_mW-ABFJqoT2Aww,8056
 mindformers/models/pangualpha/__init__.py,sha256=PqPG5THtOAbcpdo-81VvIlVLyI-4Y4CHnbSaJ83S47o,1022
 mindformers/models/pangualpha/convert_weight.py,sha256=UzBhlzvcV3MaSTkPYa-CRQeyz7y4xpwP9xs0-alAHbU,5469
 mindformers/models/pangualpha/pangualpha.py,sha256=ngA-fRT_6-vvCIrVsNWdjiLyUsG2T6_qmzvxHLqjxLA,28704
 mindformers/models/pangualpha/pangualpha_config.py,sha256=dFX8i5H_VTdMrq2Nv0NI-PQqvcbTqXklhiH8WPmwK2g,4905
 mindformers/models/pangualpha/pangualpha_processor.py,sha256=1YzgkRT67GIiKX8Qm6gurRIQVBXbFKTacAT5l8vaW7Q,2523
 mindformers/models/pangualpha/pangualpha_tokenizer.py,sha256=qosrYJTyfJXpD-HzKgKToxuPk4cIqPLqKqbmSQ2XzQ0,8201
 mindformers/models/sam/__init__.py,sha256=JkMuMrYLTggkVpdbqQ7dREEypEaam3o8AGck15Ja4mU,1190
 mindformers/models/sam/conver_weight.py,sha256=Y763RDeZkkiSiPAK4NZwZq23ooEIN0M9kUHnDwhg14o,3489
-mindformers/models/sam/sam.py,sha256=ftPwGiZ-14m9kek5qtMK5lMNrpM0zNH1kvC5LRD-Y-I,6289
+mindformers/models/sam/sam.py,sha256=j_7DerN_4JpnSHMSqZKWjfX2LuYgX-objM5py2XozAs,6281
 mindformers/models/sam/sam_config.py,sha256=iBNbVlDZGkLqhRY0UEkvbbZLI5MP0vz9IQW6shuuxsU,8641
 mindformers/models/sam/sam_image_encoder.py,sha256=JSp_uJMpBeXNV01zkXCYS7MdKbNH45kP8AP1_-eW6pQ,17601
 mindformers/models/sam/sam_layers.py,sha256=fDJ0tVEe5lJSpiejvSyOLzu5jkaiq_eOztUiSXfwQP0,2944
 mindformers/models/sam/sam_mask_decoder.py,sha256=NCkWSYd1Tqgzkhk6t9K0wltCAO_dfP_vRE0YNIa81Nk,22840
 mindformers/models/sam/sam_processor.py,sha256=1OLqaRHoaOKTXGuAr6fcQW9qbl26r77hgb8tys_lUCU,10277
 mindformers/models/sam/sam_prompt_encoder.py,sha256=aHaw1N-8U5QkAw3m_OKAYJyARGuKmGcOBkeO6x4SPGg,10597
 mindformers/models/sam/sam_utils.py,sha256=ZN8ON-LPGmvDR0MyLafcgMrbwoxP814bxavDAmNnhsY,22757
@@ -334,122 +335,119 @@
 mindformers/models/vit/convert_weight.py,sha256=zhmsxx8Unr3iCYo6VjTcKYMKpeteyt_MROnICVm1xfg,3364
 mindformers/models/vit/vit.py,sha256=vfE_V67-oLn5zqTR2toTWvyQ2eH0rCffDBAjmDpbO1M,13786
 mindformers/models/vit/vit_config.py,sha256=8rpAKaHNlNnCue6NACfrcrTpfDFzL5Ym9IW2roOgJWQ,8809
 mindformers/models/vit/vit_modules.py,sha256=3INeOVqVZH6JcWzJg4ZhRE-isPFoSJbpi1UPP-05aJQ,37548
 mindformers/models/vit/vit_processor.py,sha256=MevKV-POlpBO_lAV4RYMeN02zuc7xOzi3GSOWewDyNQ,6797
 mindformers/modules/__init__.py,sha256=AFMSj9ZmfVDqslkGhxADVjJk4qequ-s9cxYQFt7f7Zw,992
 mindformers/modules/activation.py,sha256=KWbuLz7-vJTPisX8tstP9b7ibloEqlcysgX6tki__n0,50360
-mindformers/modules/block_tables.py,sha256=RGPA1zs75hvyoDIKIPzix4QApxZGktWjGgb4c0pwy1s,6108
-mindformers/modules/cache_engine.py,sha256=cDZroa8oifXDBYNigkqcXXJTPfQpZ1r5LGvHBsHwVIA,3234
 mindformers/modules/flash_attention.py,sha256=U-bBM1cifyNDfqFHWunWAL4m12B28u3GJVSYXEGGZXE,12130
-mindformers/modules/infer_attention.py,sha256=vZwEam2gUnXAmH_BZGeaCaLTbuKWZ8kkMssVaGJdFaU,16303
-mindformers/modules/kvcache_mgr.py,sha256=GnqW09tYeutzJLDtXjxPL04va_df9VhNlrcXJ_isYiQ,13108
-mindformers/modules/layers.py,sha256=0QmtupgravZO1iFQdKYhOOOC3kOBV2xhN3zN4ANyL3g,47993
+mindformers/modules/kvcache_mgr.py,sha256=ZkUbs6iq4FK88Gt1PCdUO2GG-wqO2FTSABjPQklSYtw,13465
+mindformers/modules/layers.py,sha256=NCwVFqSmPnayCQ_sfsdoF89AR7TWTX0YWlktBnNec2k,48081
 mindformers/modules/local_block_sparse_attention.py,sha256=Zc_Q0DMOVjBTKpRbcokuoWwiBG7lrgvXiroIJcUV2ZM,14414
-mindformers/modules/paged_attention_mgr.py,sha256=DAgglzXNw_aJo7CD3pFnCdXfpWknrXBMZD76cbL-mTE,3810
+mindformers/modules/paged_attention_mgr.py,sha256=QRAZQml7eak7Dh1u3UWzGa4RKD8Jigi47rx3BTSXYT4,5310
 mindformers/modules/transformer/__init__.py,sha256=m_AJH_u4x0yS5K9WLAQs739JFzCszpkTdZDE2D-0EF8,1335
-mindformers/modules/transformer/moe.py,sha256=QkERhG5dt0X4PGB3Tn91b0--ja7o4BT0_yA0_ZR5z90,72609
-mindformers/modules/transformer/op_parallel_config.py,sha256=GaWeAq2SmrDBRdii0RJ8uTQ_LiL0ljRgLzxw_EHZCeo,9066
-mindformers/modules/transformer/transformer.py,sha256=74URde16f44k1YF_nopi5u18UaHUhdEPHrg164D4acY,213326
+mindformers/modules/transformer/moe.py,sha256=TFD2AWcjMMrwYckE9029078BW4j0QKXkJ3Tvgkb5zIM,63615
+mindformers/modules/transformer/op_parallel_config.py,sha256=NtScpEa5R_muxZCkFBYk3KYSEAWOOzcrbZd3mc5umuo,9135
+mindformers/modules/transformer/transformer.py,sha256=J5-1-oY6xogAbT7_4FrKE-haM-8DViSXANRNlRyobgs,214520
 mindformers/pet/__init__.py,sha256=mRgbD5cOL1zTOdrivVS6xXMjZIOwScMM8PNciYTPcnE,897
 mindformers/pet/constants.py,sha256=XMxgWJ9Ma0aG1woOs_FjL9lnGyscb8iH3kPAO6kekMg,1176
 mindformers/pet/pet_config.py,sha256=G767seh4pC-4jGUxhq9B8rQT3Qbp_Gjs7SBxzXVUJKo,4907
-mindformers/pet/pet_model.py,sha256=PN5ifInCNct_vTbJDG7EFlvGUqq2OuIa589V3yH9AcA,4376
+mindformers/pet/pet_model.py,sha256=gJvMNk7hf_hmu193Y26_Dxlsto2h80ytqDI1FGIByyU,4196
 mindformers/pet/utils.py,sha256=j7XZR7qyzEinZj5qbcogvXF9usX6G4WqCeeMj2XCxWQ,1096
 mindformers/pet/models/__init__.py,sha256=p3ZkxxsIlz8MV9Mxgwc1wBP7DpTXbi-Ico33EUTZVeI,696
-mindformers/pet/models/lora.py,sha256=l31JyE4vXQo8pwMg7ICSxnSRvPxeJBFDos9YO4NRKG4,3988
+mindformers/pet/models/lora.py,sha256=OakqWxPOMTAXH3TrTLUQXvgL9cPR7VJVf_qsCViOt5Y,3640
 mindformers/pet/tuners/__init__.py,sha256=K11Oe106FM-CgAISBDCDoSsxZ9X__XYrW1B6GqgvQU0,955
 mindformers/pet/tuners/ada_adapter.py,sha256=bkqApRE5ZQSdr8LTca6iWV57NRKvBDVvijLaxF8A17E,1176
 mindformers/pet/tuners/adalora_adapter.py,sha256=fg2JgoxmQJ8RDZOK65DX8xOKuxtt-GRak_uQU50Uw2Y,1194
 mindformers/pet/tuners/lora_adapter.py,sha256=ZOpKeI1nC6PnK-RfqpSJqUQFqp54tTTBapLyMw92HkY,5497
 mindformers/pet/tuners/pet_adapter.py,sha256=3UU15P_QSmmGOOqtO505uH5JtHVNeYV2DTEohgDDjzY,1649
 mindformers/pet/tuners/prefix_tuning_adapter.py,sha256=T3jg-Da1L6DvRw54ddcr5DWkUA3XiulmAEGe5wU8m7Y,1165
-mindformers/pet/tuners/ptuning2_adapter.py,sha256=CwAhd23KMN6XVOgxOFP8nIfLsbEYWuJLTo9jj0G2778,1470
+mindformers/pet/tuners/ptuning2_adapter.py,sha256=aLL5HTnCFXh0vLa3WqNUrrUYBjZMrYDlE8XPVnncWec,1350
 mindformers/pipeline/__init__.py,sha256=ujHmIWBrFrjWL18Fte99is3JbMLCiPtQ1ngQ61sdRyY,2142
 mindformers/pipeline/base_pipeline.py,sha256=5pLse0dyuDss9Qy9N13o53cTRNfVvEa62hs91lD6jGU,13012
 mindformers/pipeline/build_pipeline.py,sha256=vUNcEKnPIbI2RTOCLKTZJBpox9erZAAUqJCV2BaWWH8,2393
 mindformers/pipeline/fill_mask_pipeline.py,sha256=0D-WjItaehnj7y9rDBHi2MjBe7Zz-wxLz2pTJSyOJP0,5452
 mindformers/pipeline/image_classification_pipeline.py,sha256=SzQNalvVjHc34O0gkIMxdDpnQCPR8JOHMrnc5oPuxbc,6648
 mindformers/pipeline/image_to_text_generation_pipeline.py,sha256=OWSJmr3TGvHwDICPc_eZ21PjeOrSIQrAIYSqyE9dLTQ,6249
 mindformers/pipeline/masked_image_modeling_pipeline.py,sha256=-_Okf0bpe-06_HcZ9ATwtui7F4AvlRsOA__Y3iz1ZXg,5695
-mindformers/pipeline/pipeline.py,sha256=LKpmouW7CKneNxgYGC8epRa6ghws4a2eTmrVy9xeJww,27730
+mindformers/pipeline/pipeline.py,sha256=1MxwSuaI0o6FJ80gjdFaJCYlpzrXKTz1F6XfPtVX5F0,28220
 mindformers/pipeline/pipeline_registry.py,sha256=zGsknHnLesXI8DPaSGu_I1xQQE7mLBIehvgvlrz93xs,3369
 mindformers/pipeline/question_answering_pipeline.py,sha256=fvQob6lVwewIe0yLxLVBSBZGfp6HEJ6vFkHFQYlAfQE,17599
 mindformers/pipeline/registry_constant.py,sha256=q3S-GfpmYdxk_4husXLfj6FOrkKa2IDmNEe6u39cG1I,5728
 mindformers/pipeline/segment_anything_pipeline.py,sha256=CCEhNHeroyQmTTLEWmntvPO6Y-OANmVNhAyZpO-eJz0,25379
 mindformers/pipeline/text_classification_pipeline.py,sha256=NY_Z8j5MgMbjjCLHdHbCwGIwVfABaEb-hfD1L9Ak2OA,9668
-mindformers/pipeline/text_generation_pipeline.py,sha256=3BKJLtWKSGwu697T6mY7k1KPkNnW1NqpGF7ReY1iVBo,9744
+mindformers/pipeline/text_generation_pipeline.py,sha256=L1xI8Fta0OgnTu28ag7zOaSsAsroZ1qNVkkLF6fxNwk,9883
 mindformers/pipeline/token_classification_pipeline.py,sha256=gpz3n-ocEfxTleAKGRsvxlu0hrUpgrws-sYlr88NKO0,9160
 mindformers/pipeline/translation_pipeline.py,sha256=OGOcIzxOsf22t3T6RTSeeuU7_2KQbWhD1uwyHf9hOsU,7484
 mindformers/pipeline/zero_shot_image_classification_pipeline.py,sha256=9bD5I_GnUc8G17nB_LWDl398VmXEroLXacYJQ8fUv-c,8281
 mindformers/tools/__init__.py,sha256=xJvunVd4KFsODhAfu5dbFbxTSBGTQ3H1h3F5SPWKwCY,1194
-mindformers/tools/check_rules.py,sha256=cgqvAxSFIZDTyTPwzImA7EoO9bogOqRJGa9gC86OpIo,10734
+mindformers/tools/check_rules.py,sha256=vZyqnZH5tYsRMT1eBuSgQVgBKNq9zgJR_bijl47jljo,10542
 mindformers/tools/download_tools.py,sha256=P26bZQX101wIOUqm_2k0v_KUa81wfWu0Z7ttSxQS02U,4399
 mindformers/tools/download_tools_multithread.py,sha256=jQWWx4Qgikw5AGWDjSw5bPxvvPtGVGNU5BBpbHlrfic,6243
-mindformers/tools/generic.py,sha256=IW_lDV_pMl1Y9qDiP2ZYvEcmmV-c6sCYdDKFKjwyxx8,3029
+mindformers/tools/export.py,sha256=RJWeG7C8oKm1d0ls23VI8vaU1U1-tchHxHeTGqKiM1I,12033
+mindformers/tools/generic.py,sha256=ZUViqYiCzsihM3dX2lrEECj7ueJsaQPrLjJiZa5lxbA,2999
 mindformers/tools/hccl_tools.py,sha256=JyVnATd8YAyuIP_jYcK06aEamNLU5iq7YydY9xDY4pI,6862
 mindformers/tools/image_tools.py,sha256=FAgyl164Emo1V-tAxE0jUXU1OGl9lLwi9-FGbtYAeLY,1925
-mindformers/tools/logger.py,sha256=y_xevCnbowne5PWHyVzsV9Z8jh6RvbDi3v8QBWh5rmc,23336
+mindformers/tools/logger.py,sha256=hQBTXKEkP5syL4f4_LV8g4R8RtZ-FEeIMdbYnhefkLc,22670
 mindformers/tools/merge_hccl.py,sha256=LygpRvgIJz2_WkNVMES5j5JpHGmYZmqZcP1IsA_GbDY,2316
 mindformers/tools/moe_token_distribution_tools.py,sha256=pKA8a6km-WIhW6SadBlR9TzzPEe2MIOh8EmyTCfv48M,5653
 mindformers/tools/transform_ckpt.py,sha256=UzPq2hyzu4WWHTi62GBBdFRTA8wFTyFHfQUcYL-kMNM,3164
-mindformers/tools/transform_ckpt_lora.py,sha256=ALGaMDAaZYMckh1yzCuuXUCblHbkJK3RHbgSkWxvurI,5383
 mindformers/tools/utils.py,sha256=9hpU3gOyWbWCp0HQJ3s2y1vk0R3tIHA0hYdLU-xrLKs,13940
 mindformers/tools/cloud_adapter/__init__.py,sha256=Sj46iusOGBVYh5mx1SE8j1JVNYIxj96toskg4QezNi8,842
 mindformers/tools/cloud_adapter/cloud_adapter.py,sha256=URV7BYDEtL0FsCv5UxsLON6CIrVUVdh3ILUfXdwafls,8522
 mindformers/tools/cloud_adapter/cloud_monitor.py,sha256=Bwx9lvzDXMkcHOmizg4AuNrykic7RPUMUJ-g8e4ILMo,3464
 mindformers/tools/hub/__init__.py,sha256=ejNmZquwx_jJHq-qbL6QboJ4ras4yptH68TmeNunROc,152
-mindformers/tools/hub/dynamic_module_utils.py,sha256=xQHVzc9TsKDSGEZbsU0KGGsXNg5cfc6MGIa0us1aJvw,27730
-mindformers/tools/hub/hub.py,sha256=Zkp4QSB2rHDkFG7AS8bd9XSpEpM9KH9twdNfZRAl-Tg,32184
+mindformers/tools/hub/dynamic_module_utils.py,sha256=a-BHEVE7WMyxMEssgQPcjreGvNmJrADF9tOQLgSySuE,27727
+mindformers/tools/hub/hub.py,sha256=GVIJDwSd-V0sQS7wWu5OuSm5fv0rH0lKlxqa9jJz880,32184
 mindformers/tools/register/__init__.py,sha256=rVNqDn_oozum4nio6hJuxqOIUKdfCkIlka0NYyB46zA,942
 mindformers/tools/register/config.py,sha256=ywVlNvKEfqWBTvf0aQgtQwRJJID7NWbmi4NLoEmESp4,11052
 mindformers/tools/register/register.py,sha256=QJGTgZaoPB7EMAOYb5d0UIa6mFOYrvEaCGi1ixh62nE,7056
 mindformers/trainer/__init__.py,sha256=uzTXgoCNl08_kxVpUAtZ6xCsLkv4pfBXh17XPOgm3U0,1982
-mindformers/trainer/base_trainer.py,sha256=7T8i0sC8yxrMz8Xii7dllhU7vME1axqhV8U7wtlvngc,50692
+mindformers/trainer/base_trainer.py,sha256=6kP9_TK0DeiO-2Knr1zxl7PWG16mqzvSlHZSEjIsxCw,53834
 mindformers/trainer/build_trainer.py,sha256=RvcstU-InuwHWkO0c_rV4B08kxH0xlU6frIGxSOC83I,4428
 mindformers/trainer/config_args.py,sha256=DL9aGSp3UCPnQnDyDvLtsz0M29S69r8z38X909hfArg,55851
 mindformers/trainer/optimizer_grouped_parameters.py,sha256=COXaPUn1leMHKoLDkUy-hpPuudx2i_nCXrEfXbbjuSg,5949
-mindformers/trainer/trainer.py,sha256=5_aYyZi-svUwdM6TDwrtq-RggoQZyl_4xR9JgmVFTXQ,69793
-mindformers/trainer/training_args.py,sha256=QDLSAzf5lkwu7-1FOVehR4AVCHaswdZyFLtsNW8itbU,82738
-mindformers/trainer/utils.py,sha256=uNVxdfU6E801dq5pC4iQksagLFWEJnj79tepBn4TsBo,35540
+mindformers/trainer/trainer.py,sha256=8H401JTIBLZKwM2L3VLCZ_ANaOCGP_8PLOChJ40j9Yo,71479
+mindformers/trainer/training_args.py,sha256=xlfkppaLjJPirBoyZ8wToLKUqrcIoOgEK1ddJXXDFdM,82482
+mindformers/trainer/utils.py,sha256=-kovnwFlrgyCzmbyxylXYJeBslN7AfaldWOKbn3vpPE,35774
 mindformers/trainer/causal_language_modeling/__init__.py,sha256=ajvw2Xr-stOAdwVS56K7n1Ctx6iVIv91kktl_AylBts,821
-mindformers/trainer/causal_language_modeling/causal_language_modeling.py,sha256=n7ZHZLrh5WVTncUqNyb859Iz-sUP8cgzzAgoJiqx5d0,21478
+mindformers/trainer/causal_language_modeling/causal_language_modeling.py,sha256=KEqV2KdqHqYv3vSXXMGn4_hlzCFJuG1EjiYnEziHdZI,21619
 mindformers/trainer/contrastive_language_image_pretrain/__init__.py,sha256=_2QLmjdq0aYkYYeGQQGdrv2UuRNokZaGgPTlkicFXKM,866
-mindformers/trainer/contrastive_language_image_pretrain/contrastive_language_image_pretrain.py,sha256=M6bxbTnTvGG1G6ZxKYGBUrY8drVdf-lxjvYWOJAbESU,4480
+mindformers/trainer/contrastive_language_image_pretrain/contrastive_language_image_pretrain.py,sha256=681YRJ2gamSn9e0VDp320Bjgi9tv5JvvZO3fj3dHrJM,4633
 mindformers/trainer/general_task_trainer/__init__.py,sha256=KQhPQnJv5tkVZ7hsfN5JVG7Vdjd3rDrn0xH9y2htnNQ,795
-mindformers/trainer/general_task_trainer/general_task_trainer.py,sha256=2c2KRynzv47Kel2Howo0wBj1eW2rbRhVQUtJQUAj5FA,8976
+mindformers/trainer/general_task_trainer/general_task_trainer.py,sha256=4j-ooWwaQHqUDIT6IBHlqo6-hoBdNF5pE7Zj9ZjUXRQ,9138
 mindformers/trainer/image_classification/__init__.py,sha256=6mp76hxxRWCQ9H2YdoVB8FJ-RTbrfFnHb79rd8h5Qr4,928
 mindformers/trainer/image_classification/group_ic_params.py,sha256=rPCwALHegc-JfemxhrIzfaFyP0rabb_dHkBZdxXQpeg,4622
-mindformers/trainer/image_classification/image_classification.py,sha256=lrR1Wha0GwtS8YC-MoawiF8bEKAbyrMGysNL55hlku8,9361
-mindformers/trainer/image_classification/zero_shot_image_classification.py,sha256=4u7o_XqIGbfFQ5PPWQq3QFe3ErbpURB7sqN_pa2kim4,8106
+mindformers/trainer/image_classification/image_classification.py,sha256=9840tVPqy2L38HCF0j2hjXpYX1ETtwj54ZiglKoQEDM,9536
+mindformers/trainer/image_classification/zero_shot_image_classification.py,sha256=A0720lNq-LZmAzZL8EN92DOS4Tq-WCtjRz8P5VEaDms,8281
 mindformers/trainer/image_to_text_generation/__init__.py,sha256=e0eEoUBNaGDEGhQarJfBikYMOEltIPOJbADN9u7n_8U,823
-mindformers/trainer/image_to_text_generation/image_to_text_generation.py,sha256=k6rJYVBPLmgAAD8eR_SkFZ3f-liXBIqujParo-pEXoI,5982
+mindformers/trainer/image_to_text_generation/image_to_text_generation.py,sha256=s7SjhIFUzwIwjg7oxgD0h_wPsJAXEwmzlFHwWcSoAow,6161
 mindformers/trainer/image_to_text_retrieval/__init__.py,sha256=D3rDHqHHlrKlzUa-B9K7FRu1wUqBOhATw1ug2YBsf70,818
 mindformers/trainer/image_to_text_retrieval/eval_utils.py,sha256=Sj8_7p_54MbjdnMedx2bvJ7J2P3KCc9Gxh-OYAMl3Xc,12082
-mindformers/trainer/image_to_text_retrieval/image_to_text_retrieval.py,sha256=f0BEljoFi58tYyppQWLkDXSV_FMPBqVIeO1SP86vYC4,7317
+mindformers/trainer/image_to_text_retrieval/image_to_text_retrieval.py,sha256=WPuI4PUWEMwD93kyvFJ3hRD2mCKbrSJTN3-N3TkOrTo,7801
 mindformers/trainer/masked_image_modeling/__init__.py,sha256=Li0FkNJKt4rzc5ty-RvYp64xU5X_LvD6N-Q7Vm8WYgk,818
 mindformers/trainer/masked_image_modeling/group_mim_parameters.py,sha256=kjFjmhRBshJ6zDRqqL1vpUoN4d6xviwfmfl5YjAApro,2197
 mindformers/trainer/masked_image_modeling/masked_image_modeling_pretrain.py,sha256=v5KgQp6OfgORaMUhzCHCsDPGHBwCaThjt1skZG244SM,7482
 mindformers/trainer/masked_language_modeling/__init__.py,sha256=7hmyp9r5IC96bUbvcm2Q3oBaSA7xOlD3VjakyvBl-wA,830
 mindformers/trainer/masked_language_modeling/masked_language_modeling_pretrain.py,sha256=mIMPjpoHtslmj05UjjkFa_cv5kyBgJTymhKrGhAZ7x8,6931
 mindformers/trainer/question_answering/__init__.py,sha256=Dmu0SpmAv-LCLALGgPsyv9c1FXlyTuZBo6bbZCniClw,799
 mindformers/trainer/question_answering/question_answering.py,sha256=rrfqYd3oDWxSIzPicUNoiHivW79cARnsNg-WSYa9T5M,9308
 mindformers/trainer/text_classfication/__init__.py,sha256=QZijWXRX-_iq8tb8kps_IuntZZhOfMthdo6nE0IaVEA,803
 mindformers/trainer/text_classfication/text_classification.py,sha256=H8CItYadX3HL7T-UfuJCBLJvio7dCG0u8AojOXjBRe0,9476
 mindformers/trainer/token_classification/__init__.py,sha256=a_5Y4Kk-pNCooniku6ewvhhc8zzzirM3xZJ33-q4lxw,807
 mindformers/trainer/token_classification/token_classification.py,sha256=c2t8KDj3QuNoqmtGzL3jP_xwAehhKrXdeRB9LlDo3x8,9590
 mindformers/trainer/translation/__init__.py,sha256=ghZF7nb3MoZBOhBg5WJxhzT5-Xzdp8TTZU4e5cjWww8,795
 mindformers/trainer/translation/translation_finetune.py,sha256=TGRdEJfV54hPvPQndDwAyE5DyB4mRAqIuuapzqS3dnE,6810
 mindformers/utils/__init__.py,sha256=h0H5PhuC2vLnXMesOUQd0VIR7IMR7eHmFy6PdVUXEbg,122
-mindformers/utils/convert_utils.py,sha256=c4qrHPOqfE-BeGOL8G0KYIBFrozSNovA6WJVJvH1B_g,1734
+mindformers/utils/convert_utils.py,sha256=lRP8vPgIRWmrmJd4Wtl1aIp4ae9NVQoSyQJemfYcgCI,1626
 mindformers/utils/image_transforms.py,sha256=pPxx946L0r88jEEGSPvPIBZM9uc269As-KQP0ZpPFlo,14764
 mindformers/utils/image_utils.py,sha256=mS6XZ1izz5uAoBa46jKZPjgaXGphXwaVLpR4_wUovPw,3153
 mindformers/utils/import_utils.py,sha256=lbq6Y_ysrwixX-_yBfAdNN6JtEulZ1_0abr281b7_8s,1739
 mindformers/wrapper/__init__.py,sha256=XjjgWv6F36VtMMoD6FQDNPCI1AlGFoZqitVPvDVLh0s,913
 mindformers/wrapper/adaptive_loss_scale.py,sha256=VCQdTjFb5GtkdfUwSMdk6h9dmYVLVQi55FoZowO4qEU,14329
 mindformers/wrapper/build_wrapper.py,sha256=MoEaObzg8VjfaYcZAZp3BwLX9eHLa7COwCrSSpsUJ2U,4117
 mindformers/wrapper/wrapper.py,sha256=YGdPlck41X-MaPx6SgQmnUeOGQ0fyOysD_m_G9civP0,12419
-mindformers-1.1.0.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
-mindformers-1.1.0.dist-info/METADATA,sha256=U-9jwOfUnqxfvr6zEw4G1FoFdc3S2QrsAFzHCfrOqKk,22642
-mindformers-1.1.0.dist-info/WHEEL,sha256=g4nMs7d-Xl9-xC9XovUrsDHGXt-FT0E17Yqo92DEfvY,92
-mindformers-1.1.0.dist-info/top_level.txt,sha256=z7ktcLb3g0gVTBtVuNbJpidDh-9SFrXB4k39eSaUa18,12
-mindformers-1.1.0.dist-info/RECORD,,
+mindformers-1.1.0rc1.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
+mindformers-1.1.0rc1.dist-info/METADATA,sha256=IzwKcKXF9rDoE5J8KQ_pav-ocu89ObSj-LXyLkMd-c4,21404
+mindformers-1.1.0rc1.dist-info/WHEEL,sha256=g4nMs7d-Xl9-xC9XovUrsDHGXt-FT0E17Yqo92DEfvY,92
+mindformers-1.1.0rc1.dist-info/top_level.txt,sha256=z7ktcLb3g0gVTBtVuNbJpidDh-9SFrXB4k39eSaUa18,12
+mindformers-1.1.0rc1.dist-info/RECORD,,
```

