# Comparing `tmp/onnxruntime-1.8.1-cp39-cp39-win_amd64.whl.zip` & `tmp/onnxruntime-1.9.0-cp39-cp39-win_amd64.whl.zip`

## zipinfo {}

```diff
@@ -1,142 +1,150 @@
-Zip file size: 4691454 bytes, number of entries: 140
--rw-rw-rw-  2.0 fat     1094 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/LICENSE
--rw-rw-rw-  2.0 fat     2490 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/Privacy.md
--rw-rw-rw-  2.0 fat   243881 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/ThirdPartyNotices.txt
--rw-rw-rw-  2.0 fat     2400 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/__init__.py
--rw-rw-rw-  2.0 fat      320 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/backend/__init__.py
--rw-rw-rw-  2.0 fat     6957 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/backend/backend.py
--rw-rw-rw-  2.0 fat     1812 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/backend/backend_rep.py
--rw-rw-rw-  2.0 fat      251 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/capi/__init__.py
--rw-rw-rw-  2.0 fat      487 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/capi/_ld_preload.py
--rw-rw-rw-  2.0 fat      823 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/capi/_pybind_state.py
--rw-rw-rw-  2.0 fat     3795 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/capi/onnxruntime_collect_build_info.py
--rw-rw-rw-  2.0 fat    23843 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/capi/onnxruntime_inference_collection.py
--rw-rw-rw-  2.0 fat    19848 b- defN 21-Jul-01 21:43 onnxruntime-1.8.1.data/purelib/onnxruntime/capi/onnxruntime_providers_shared.dll
--rw-rw-rw-  2.0 fat 13496696 b- defN 21-Jul-01 21:43 onnxruntime-1.8.1.data/purelib/onnxruntime/capi/onnxruntime_pybind11_state.pyd
--rw-rw-rw-  2.0 fat     5975 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/capi/onnxruntime_validation.py
--rw-rw-rw-  2.0 fat       33 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/capi/version_info.py
--rw-rw-rw-  2.0 fat      326 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/capi/training/__init__.py
--rw-rw-rw-  2.0 fat      480 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/datasets/__init__.py
--rw-rw-rw-  2.0 fat      670 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/datasets/logreg_iris.onnx
--rw-rw-rw-  2.0 fat      130 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/datasets/mul_1.onnx
--rw-rw-rw-  2.0 fat      103 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/datasets/sigmoid.onnx
--rw-rw-rw-  2.0 fat      313 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/__init__.py
--rw-rw-rw-  2.0 fat    22358 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/calibrate.py
--rw-rw-rw-  2.0 fat    12554 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/onnx_model.py
--rw-rw-rw-  2.0 fat    43846 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/onnx_quantizer.py
--rw-rw-rw-  2.0 fat     9295 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/qdq_quantizer.py
--rw-rw-rw-  2.0 fat    13351 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/quant_utils.py
--rw-rw-rw-  2.0 fat    15181 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/quantize.py
--rw-rw-rw-  2.0 fat     2965 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/registry.py
--rw-rw-rw-  2.0 fat     1413 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/CalTableFlatBuffers/KeyValue.py
--rw-rw-rw-  2.0 fat     1765 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/CalTableFlatBuffers/TrtTable.py
--rw-rw-rw-  2.0 fat        0 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/CalTableFlatBuffers/__init__.py
--rw-rw-rw-  2.0 fat       83 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/__init__.py
--rw-rw-rw-  2.0 fat     3674 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/activation.py
--rw-rw-rw-  2.0 fat     1647 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/attention.py
--rw-rw-rw-  2.0 fat      980 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/base_operator.py
--rw-rw-rw-  2.0 fat     2449 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/binary_op.py
--rw-rw-rw-  2.0 fat     2253 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/concat.py
--rw-rw-rw-  2.0 fat     6932 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/conv.py
--rw-rw-rw-  2.0 fat     1634 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/direct_q8.py
--rw-rw-rw-  2.0 fat      632 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/embed_layernorm.py
--rw-rw-rw-  2.0 fat     1266 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/gather.py
--rw-rw-rw-  2.0 fat     2178 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/gavgpool.py
--rw-rw-rw-  2.0 fat     4836 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/lstm.py
--rw-rw-rw-  2.0 fat     4698 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/matmul.py
--rw-rw-rw-  2.0 fat      965 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/maxpool.py
--rw-rw-rw-  2.0 fat     4055 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/pad.py
--rw-rw-rw-  2.0 fat     1907 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/pooling.py
--rw-rw-rw-  2.0 fat      481 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/qdq_base_operator.py
--rw-rw-rw-  2.0 fat      966 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/resize.py
--rw-rw-rw-  2.0 fat     1632 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/split.py
--rw-rw-rw-  2.0 fat      251 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/tools/__init__.py
--rw-rw-rw-  2.0 fat    10517 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/tools/convert_onnx_models_to_ort.py
--rw-rw-rw-  2.0 fat      305 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/tools/logger.py
--rw-rw-rw-  2.0 fat     5576 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/tools/onnxruntime_test.py
--rw-rw-rw-  2.0 fat    80422 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/tools/symbolic_shape_infer.py
--rw-rw-rw-  2.0 fat     1232 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/__init__.py
--rw-rw-rw-  2.0 fat    26813 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/operator_type_usage_processors.py
--rw-rw-rw-  2.0 fat     4483 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_model_processor.py
--rw-rw-rw-  2.0 fat     4184 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/types.py
--rw-rw-rw-  2.0 fat     3721 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/utils.py
--rw-rw-rw-  2.0 fat        0 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/__init__.py
--rw-rw-rw-  2.0 fat        0 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/__init__.py
--rw-rw-rw-  2.0 fat     9362 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Attribute.py
--rw-rw-rw-  2.0 fat      348 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/AttributeType.py
--rw-rw-rw-  2.0 fat     1805 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Dimension.py
--rw-rw-rw-  2.0 fat     1988 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/DimensionValue.py
--rw-rw-rw-  2.0 fat      176 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/DimensionValueType.py
--rw-rw-rw-  2.0 fat     1076 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/EdgeEnd.py
--rw-rw-rw-  2.0 fat     8459 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Graph.py
--rw-rw-rw-  2.0 fat     2439 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/InferenceSession.py
--rw-rw-rw-  2.0 fat     3515 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/KernelCreateInfos.py
--rw-rw-rw-  2.0 fat     1741 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/MapType.py
--rw-rw-rw-  2.0 fat     5037 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Model.py
--rw-rw-rw-  2.0 fat     8648 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Node.py
--rw-rw-rw-  2.0 fat     3365 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/NodeEdge.py
--rw-rw-rw-  2.0 fat      153 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/NodeType.py
--rw-rw-rw-  2.0 fat     1621 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/OperatorSetId.py
--rw-rw-rw-  2.0 fat     1450 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/SequenceType.py
--rw-rw-rw-  2.0 fat     2716 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/SessionState.py
--rw-rw-rw-  2.0 fat     1902 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Shape.py
--rw-rw-rw-  2.0 fat     3159 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/SparseTensor.py
--rw-rw-rw-  2.0 fat     1936 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/SubGraphSessionState.py
--rw-rw-rw-  2.0 fat     5144 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Tensor.py
--rw-rw-rw-  2.0 fat      408 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/TensorDataType.py
--rw-rw-rw-  2.0 fat     1841 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/TensorTypeAndShape.py
--rw-rw-rw-  2.0 fat     2039 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/TypeInfo.py
--rw-rw-rw-  2.0 fat      200 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/TypeInfoValue.py
--rw-rw-rw-  2.0 fat     2131 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/ValueInfo.py
--rw-rw-rw-  2.0 fat      259 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/__init__.py
--rw-rw-rw-  2.0 fat       69 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/__init__.py
--rw-rw-rw-  2.0 fat    26267 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/benchmark.py
--rw-rw-rw-  2.0 fat    20886 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/benchmark_gpt2.py
--rw-rw-rw-  2.0 fat    14205 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/benchmark_helper.py
--rw-rw-rw-  2.0 fat    13638 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/bert_perf_test.py
--rw-rw-rw-  2.0 fat    18567 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/bert_test_data.py
--rw-rw-rw-  2.0 fat     8014 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/compare_bert_results.py
--rw-rw-rw-  2.0 fat     6317 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/convert_tf_models_to_pytorch.py
--rw-rw-rw-  2.0 fat    15866 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/convert_to_onnx.py
--rw-rw-rw-  2.0 fat    19141 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_attention.py
--rw-rw-rw-  2.0 fat     2366 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_base.py
--rw-rw-rw-  2.0 fat     2374 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_biasgelu.py
--rw-rw-rw-  2.0 fat    15782 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_embedlayer.py
--rw-rw-rw-  2.0 fat    13069 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_fastgelu.py
--rw-rw-rw-  2.0 fat    10211 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_gelu.py
--rw-rw-rw-  2.0 fat     1105 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_gelu_approximation.py
--rw-rw-rw-  2.0 fat    12554 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_gpt_attention.py
--rw-rw-rw-  2.0 fat     8325 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_gpt_attention_no_past.py
--rw-rw-rw-  2.0 fat    11529 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_layernorm.py
--rw-rw-rw-  2.0 fat     6210 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_reshape.py
--rw-rw-rw-  2.0 fat     6494 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_skiplayernorm.py
--rw-rw-rw-  2.0 fat     3060 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_utils.py
--rw-rw-rw-  2.0 fat    45031 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/gpt2_beamsearch_helper.py
--rw-rw-rw-  2.0 fat    17517 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/gpt2_beamsearch_tester.py
--rw-rw-rw-  2.0 fat    30075 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/gpt2_helper.py
--rw-rw-rw-  2.0 fat    19875 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/gpt2_tester.py
--rw-rw-rw-  2.0 fat     8458 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/huggingface_models.py
--rw-rw-rw-  2.0 fat     6758 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/machine_info.py
--rw-rw-rw-  2.0 fat    23352 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/onnx_exporter.py
--rw-rw-rw-  2.0 fat    36626 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/onnx_model.py
--rw-rw-rw-  2.0 fat    15669 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/onnx_model_bert.py
--rw-rw-rw-  2.0 fat    18301 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/onnx_model_bert_keras.py
--rw-rw-rw-  2.0 fat    23486 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/onnx_model_bert_tf.py
--rw-rw-rw-  2.0 fat     3658 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/onnx_model_gpt2.py
--rw-rw-rw-  2.0 fat    15789 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/optimizer.py
--rw-rw-rw-  2.0 fat     6102 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/parity_check_helper.py
--rw-rw-rw-  2.0 fat    19306 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/profiler.py
--rw-rw-rw-  2.0 fat     3409 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/quantize_helper.py
--rw-rw-rw-  2.0 fat     3649 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/shape_infer_helper.py
--rw-rw-rw-  2.0 fat    15456 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/shape_optimizer.py
--rw-rw-rw-  2.0 fat      388 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/longformer/__init__.py
--rw-rw-rw-  2.0 fat    21397 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/longformer/benchmark_longformer.py
--rw-rw-rw-  2.0 fat    14159 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/longformer/convert_longformer_to_onnx.py
--rw-rw-rw-  2.0 fat     9294 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/longformer/generate_test_data.py
--rw-rw-rw-  2.0 fat     3359 b- defN 21-Jul-01 21:15 onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/longformer/longformer_helper.py
--rw-rw-rw-  2.0 fat     3066 b- defN 21-Jul-01 21:44 onnxruntime-1.8.1.dist-info/METADATA
--rw-rw-rw-  2.0 fat      100 b- defN 21-Jul-01 21:44 onnxruntime-1.8.1.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       78 b- defN 21-Jul-01 21:44 onnxruntime-1.8.1.dist-info/entry_points.txt
--rw-rw-rw-  2.0 fat       12 b- defN 21-Jul-01 21:44 onnxruntime-1.8.1.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat    19273 b- defN 21-Jul-01 21:44 onnxruntime-1.8.1.dist-info/RECORD
-140 files, 14808837 bytes uncompressed, 4658094 bytes compressed:  68.5%
+Zip file size: 5101318 bytes, number of entries: 148
+-rw-rw-rw-  2.0 fat     1094 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/LICENSE
+-rw-rw-rw-  2.0 fat     2490 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/Privacy.md
+-rw-rw-rw-  2.0 fat   246970 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/ThirdPartyNotices.txt
+-rw-rw-rw-  2.0 fat     2439 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/__init__.py
+-rw-rw-rw-  2.0 fat      320 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/backend/__init__.py
+-rw-rw-rw-  2.0 fat     7015 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/backend/backend.py
+-rw-rw-rw-  2.0 fat     1812 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/backend/backend_rep.py
+-rw-rw-rw-  2.0 fat      251 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/capi/__init__.py
+-rw-rw-rw-  2.0 fat      413 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/capi/_ld_preload.py
+-rw-rw-rw-  2.0 fat      837 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/capi/_pybind_state.py
+-rw-rw-rw-  2.0 fat     3795 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/capi/onnxruntime_collect_build_info.py
+-rw-rw-rw-  2.0 fat    35718 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/capi/onnxruntime_inference_collection.py
+-rw-rw-rw-  2.0 fat    19848 b- defN 21-Sep-22 01:30 onnxruntime-1.9.0.data/purelib/onnxruntime/capi/onnxruntime_providers_shared.dll
+-rw-rw-rw-  2.0 fat 14809480 b- defN 21-Sep-22 01:30 onnxruntime-1.9.0.data/purelib/onnxruntime/capi/onnxruntime_pybind11_state.pyd
+-rw-rw-rw-  2.0 fat     6299 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/capi/onnxruntime_validation.py
+-rw-rw-rw-  2.0 fat       33 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/capi/version_info.py
+-rw-rw-rw-  2.0 fat      326 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/capi/training/__init__.py
+-rw-rw-rw-  2.0 fat      480 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/datasets/__init__.py
+-rw-rw-rw-  2.0 fat      670 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/datasets/logreg_iris.onnx
+-rw-rw-rw-  2.0 fat      130 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/datasets/mul_1.onnx
+-rw-rw-rw-  2.0 fat      103 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/datasets/sigmoid.onnx
+-rw-rw-rw-  2.0 fat      313 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/__init__.py
+-rw-rw-rw-  2.0 fat    22358 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/calibrate.py
+-rw-rw-rw-  2.0 fat    14885 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/onnx_model.py
+-rw-rw-rw-  2.0 fat    45468 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/onnx_quantizer.py
+-rw-rw-rw-  2.0 fat     9295 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/qdq_quantizer.py
+-rw-rw-rw-  2.0 fat    14265 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/quant_utils.py
+-rw-rw-rw-  2.0 fat    15686 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/quantize.py
+-rw-rw-rw-  2.0 fat     2965 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/registry.py
+-rw-rw-rw-  2.0 fat     1413 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/CalTableFlatBuffers/KeyValue.py
+-rw-rw-rw-  2.0 fat     1790 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/CalTableFlatBuffers/TrtTable.py
+-rw-rw-rw-  2.0 fat        0 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/CalTableFlatBuffers/__init__.py
+-rw-rw-rw-  2.0 fat       83 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/__init__.py
+-rw-rw-rw-  2.0 fat     3692 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/activation.py
+-rw-rw-rw-  2.0 fat     1729 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/attention.py
+-rw-rw-rw-  2.0 fat      947 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/base_operator.py
+-rw-rw-rw-  2.0 fat     2412 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/binary_op.py
+-rw-rw-rw-  2.0 fat     2185 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/concat.py
+-rw-rw-rw-  2.0 fat     8196 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/conv.py
+-rw-rw-rw-  2.0 fat     1634 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/direct_q8.py
+-rw-rw-rw-  2.0 fat     3682 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/embed_layernorm.py
+-rw-rw-rw-  2.0 fat     1348 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/gather.py
+-rw-rw-rw-  2.0 fat     2180 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/gavgpool.py
+-rw-rw-rw-  2.0 fat     4836 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/lstm.py
+-rw-rw-rw-  2.0 fat     4610 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/matmul.py
+-rw-rw-rw-  2.0 fat      965 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/maxpool.py
+-rw-rw-rw-  2.0 fat     4135 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/pad.py
+-rw-rw-rw-  2.0 fat     1940 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/pooling.py
+-rw-rw-rw-  2.0 fat      481 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/qdq_base_operator.py
+-rw-rw-rw-  2.0 fat      966 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/resize.py
+-rw-rw-rw-  2.0 fat     1716 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/split.py
+-rw-rw-rw-  2.0 fat      251 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/tools/__init__.py
+-rw-rw-rw-  2.0 fat    12751 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/tools/convert_onnx_models_to_ort.py
+-rw-rw-rw-  2.0 fat      305 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/tools/logger.py
+-rw-rw-rw-  2.0 fat     5576 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/tools/onnxruntime_test.py
+-rw-rw-rw-  2.0 fat     3415 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/tools/pytorch_export_contrib_ops.py
+-rw-rw-rw-  2.0 fat    99985 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/tools/symbolic_shape_infer.py
+-rw-rw-rw-  2.0 fat     1232 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/__init__.py
+-rw-rw-rw-  2.0 fat    27619 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/operator_type_usage_processors.py
+-rw-rw-rw-  2.0 fat     4483 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_model_processor.py
+-rw-rw-rw-  2.0 fat     4184 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/types.py
+-rw-rw-rw-  2.0 fat     3721 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/utils.py
+-rw-rw-rw-  2.0 fat        0 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/__init__.py
+-rw-rw-rw-  2.0 fat        0 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/__init__.py
+-rw-rw-rw-  2.0 fat     9362 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Attribute.py
+-rw-rw-rw-  2.0 fat      348 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/AttributeType.py
+-rw-rw-rw-  2.0 fat     1805 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Dimension.py
+-rw-rw-rw-  2.0 fat     1988 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/DimensionValue.py
+-rw-rw-rw-  2.0 fat      176 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/DimensionValueType.py
+-rw-rw-rw-  2.0 fat     1076 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/EdgeEnd.py
+-rw-rw-rw-  2.0 fat     8459 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Graph.py
+-rw-rw-rw-  2.0 fat     2439 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/InferenceSession.py
+-rw-rw-rw-  2.0 fat     3515 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/KernelCreateInfos.py
+-rw-rw-rw-  2.0 fat     1741 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/MapType.py
+-rw-rw-rw-  2.0 fat     6184 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Model.py
+-rw-rw-rw-  2.0 fat     8648 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Node.py
+-rw-rw-rw-  2.0 fat     3365 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/NodeEdge.py
+-rw-rw-rw-  2.0 fat      153 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/NodeType.py
+-rw-rw-rw-  2.0 fat     1621 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/OperatorSetId.py
+-rw-rw-rw-  2.0 fat     1450 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/SequenceType.py
+-rw-rw-rw-  2.0 fat     2716 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/SessionState.py
+-rw-rw-rw-  2.0 fat     1902 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Shape.py
+-rw-rw-rw-  2.0 fat     3159 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/SparseTensor.py
+-rw-rw-rw-  2.0 fat     1673 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/StringStringEntry.py
+-rw-rw-rw-  2.0 fat     1936 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/SubGraphSessionState.py
+-rw-rw-rw-  2.0 fat     5144 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Tensor.py
+-rw-rw-rw-  2.0 fat      408 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/TensorDataType.py
+-rw-rw-rw-  2.0 fat     1841 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/TensorTypeAndShape.py
+-rw-rw-rw-  2.0 fat     2039 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/TypeInfo.py
+-rw-rw-rw-  2.0 fat      200 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/TypeInfoValue.py
+-rw-rw-rw-  2.0 fat     2131 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/ValueInfo.py
+-rw-rw-rw-  2.0 fat      259 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/__init__.py
+-rw-rw-rw-  2.0 fat       69 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/__init__.py
+-rw-rw-rw-  2.0 fat     1361 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/affinity_helper.py
+-rw-rw-rw-  2.0 fat    26247 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/benchmark.py
+-rw-rw-rw-  2.0 fat    20898 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/benchmark_gpt2.py
+-rw-rw-rw-  2.0 fat    14224 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/benchmark_helper.py
+-rw-rw-rw-  2.0 fat    13722 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/bert_perf_test.py
+-rw-rw-rw-  2.0 fat    18567 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/bert_test_data.py
+-rw-rw-rw-  2.0 fat     8014 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/compare_bert_results.py
+-rw-rw-rw-  2.0 fat     6313 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/convert_tf_models_to_pytorch.py
+-rw-rw-rw-  2.0 fat    22365 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/convert_to_onnx.py
+-rw-rw-rw-  2.0 fat    16730 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/float16.py
+-rw-rw-rw-  2.0 fat    22424 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_attention.py
+-rw-rw-rw-  2.0 fat     2538 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_base.py
+-rw-rw-rw-  2.0 fat     2374 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_biasgelu.py
+-rw-rw-rw-  2.0 fat    26736 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_embedlayer.py
+-rw-rw-rw-  2.0 fat    13363 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_fastgelu.py
+-rw-rw-rw-  2.0 fat    10211 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_gelu.py
+-rw-rw-rw-  2.0 fat     1105 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_gelu_approximation.py
+-rw-rw-rw-  2.0 fat    18383 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_gpt_attention.py
+-rw-rw-rw-  2.0 fat     8325 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_gpt_attention_no_past.py
+-rw-rw-rw-  2.0 fat    11381 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_layernorm.py
+-rw-rw-rw-  2.0 fat     5128 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_options.py
+-rw-rw-rw-  2.0 fat     6352 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_reshape.py
+-rw-rw-rw-  2.0 fat     3788 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_shape.py
+-rw-rw-rw-  2.0 fat     6776 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_skiplayernorm.py
+-rw-rw-rw-  2.0 fat     6407 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_utils.py
+-rw-rw-rw-  2.0 fat    45738 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/gpt2_beamsearch_helper.py
+-rw-rw-rw-  2.0 fat    17517 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/gpt2_beamsearch_tester.py
+-rw-rw-rw-  2.0 fat    37958 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/gpt2_helper.py
+-rw-rw-rw-  2.0 fat     6539 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/gpt2_parity.py
+-rw-rw-rw-  2.0 fat    19875 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/gpt2_tester.py
+-rw-rw-rw-  2.0 fat     8502 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/huggingface_models.py
+-rw-rw-rw-  2.0 fat     6960 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/machine_info.py
+-rw-rw-rw-  2.0 fat    23647 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/onnx_exporter.py
+-rw-rw-rw-  2.0 fat    38053 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/onnx_model.py
+-rw-rw-rw-  2.0 fat    11567 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/onnx_model_bart.py
+-rw-rw-rw-  2.0 fat    17628 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/onnx_model_bert.py
+-rw-rw-rw-  2.0 fat    18311 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/onnx_model_bert_keras.py
+-rw-rw-rw-  2.0 fat    23496 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/onnx_model_bert_tf.py
+-rw-rw-rw-  2.0 fat     3524 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/onnx_model_gpt2.py
+-rw-rw-rw-  2.0 fat    15290 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/optimizer.py
+-rw-rw-rw-  2.0 fat     6102 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/parity_check_helper.py
+-rw-rw-rw-  2.0 fat    19306 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/profiler.py
+-rw-rw-rw-  2.0 fat     3409 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/quantize_helper.py
+-rw-rw-rw-  2.0 fat     3814 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/shape_infer_helper.py
+-rw-rw-rw-  2.0 fat    15456 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/shape_optimizer.py
+-rw-rw-rw-  2.0 fat      388 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/longformer/__init__.py
+-rw-rw-rw-  2.0 fat    21397 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/longformer/benchmark_longformer.py
+-rw-rw-rw-  2.0 fat    13913 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/longformer/convert_longformer_to_onnx.py
+-rw-rw-rw-  2.0 fat     9294 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/longformer/generate_test_data.py
+-rw-rw-rw-  2.0 fat     3359 b- defN 21-Sep-22 01:25 onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/longformer/longformer_helper.py
+-rw-rw-rw-  2.0 fat     3248 b- defN 21-Sep-22 01:30 onnxruntime-1.9.0.dist-info/METADATA
+-rw-rw-rw-  2.0 fat      100 b- defN 21-Sep-22 01:30 onnxruntime-1.9.0.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       78 b- defN 21-Sep-22 01:30 onnxruntime-1.9.0.dist-info/entry_points.txt
+-rw-rw-rw-  2.0 fat       12 b- defN 21-Sep-22 01:30 onnxruntime-1.9.0.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat    20360 b- defN 21-Sep-22 01:30 onnxruntime-1.9.0.dist-info/RECORD
+148 files, 16264600 bytes uncompressed, 5066094 bytes compressed:  68.9%
```

## zipnote {}

```diff
@@ -1,421 +1,445 @@
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/LICENSE
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/LICENSE
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/Privacy.md
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/Privacy.md
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/ThirdPartyNotices.txt
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/ThirdPartyNotices.txt
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/__init__.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/__init__.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/backend/__init__.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/backend/__init__.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/backend/backend.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/backend/backend.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/backend/backend_rep.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/backend/backend_rep.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/capi/__init__.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/capi/__init__.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/capi/_ld_preload.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/capi/_ld_preload.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/capi/_pybind_state.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/capi/_pybind_state.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/capi/onnxruntime_collect_build_info.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/capi/onnxruntime_collect_build_info.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/capi/onnxruntime_inference_collection.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/capi/onnxruntime_inference_collection.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/capi/onnxruntime_providers_shared.dll
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/capi/onnxruntime_providers_shared.dll
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/capi/onnxruntime_pybind11_state.pyd
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/capi/onnxruntime_pybind11_state.pyd
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/capi/onnxruntime_validation.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/capi/onnxruntime_validation.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/capi/version_info.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/capi/version_info.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/capi/training/__init__.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/capi/training/__init__.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/datasets/__init__.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/datasets/__init__.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/datasets/logreg_iris.onnx
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/datasets/logreg_iris.onnx
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/datasets/mul_1.onnx
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/datasets/mul_1.onnx
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/datasets/sigmoid.onnx
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/datasets/sigmoid.onnx
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/__init__.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/__init__.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/calibrate.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/calibrate.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/onnx_model.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/onnx_model.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/onnx_quantizer.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/onnx_quantizer.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/qdq_quantizer.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/qdq_quantizer.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/quant_utils.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/quant_utils.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/quantize.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/quantize.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/registry.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/registry.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/CalTableFlatBuffers/KeyValue.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/CalTableFlatBuffers/KeyValue.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/CalTableFlatBuffers/TrtTable.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/CalTableFlatBuffers/TrtTable.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/CalTableFlatBuffers/__init__.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/CalTableFlatBuffers/__init__.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/__init__.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/__init__.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/activation.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/activation.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/attention.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/attention.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/base_operator.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/base_operator.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/binary_op.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/binary_op.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/concat.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/concat.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/conv.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/conv.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/direct_q8.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/direct_q8.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/embed_layernorm.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/embed_layernorm.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/gather.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/gather.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/gavgpool.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/gavgpool.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/lstm.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/lstm.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/matmul.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/matmul.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/maxpool.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/maxpool.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/pad.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/pad.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/pooling.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/pooling.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/qdq_base_operator.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/qdq_base_operator.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/resize.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/resize.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/split.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/split.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/tools/__init__.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/tools/__init__.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/tools/convert_onnx_models_to_ort.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/tools/convert_onnx_models_to_ort.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/tools/logger.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/tools/logger.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/tools/onnxruntime_test.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/tools/onnxruntime_test.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/tools/symbolic_shape_infer.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/tools/pytorch_export_contrib_ops.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/__init__.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/tools/symbolic_shape_infer.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/operator_type_usage_processors.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/__init__.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_model_processor.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/operator_type_usage_processors.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/types.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_model_processor.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/utils.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/types.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/__init__.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/utils.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/__init__.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/__init__.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Attribute.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/__init__.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/AttributeType.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Attribute.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Dimension.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/AttributeType.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/DimensionValue.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Dimension.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/DimensionValueType.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/DimensionValue.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/EdgeEnd.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/DimensionValueType.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Graph.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/EdgeEnd.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/InferenceSession.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Graph.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/KernelCreateInfos.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/InferenceSession.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/MapType.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/KernelCreateInfos.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Model.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/MapType.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Node.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Model.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/NodeEdge.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Node.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/NodeType.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/NodeEdge.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/OperatorSetId.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/NodeType.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/SequenceType.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/OperatorSetId.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/SessionState.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/SequenceType.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Shape.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/SessionState.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/SparseTensor.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Shape.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/SubGraphSessionState.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/SparseTensor.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Tensor.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/StringStringEntry.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/TensorDataType.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/SubGraphSessionState.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/TensorTypeAndShape.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Tensor.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/TypeInfo.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/TensorDataType.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/TypeInfoValue.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/TensorTypeAndShape.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/ValueInfo.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/TypeInfo.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/__init__.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/TypeInfoValue.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/__init__.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/ValueInfo.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/benchmark.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/__init__.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/benchmark_gpt2.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/__init__.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/benchmark_helper.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/affinity_helper.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/bert_perf_test.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/benchmark.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/bert_test_data.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/benchmark_gpt2.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/compare_bert_results.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/benchmark_helper.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/convert_tf_models_to_pytorch.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/bert_perf_test.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/convert_to_onnx.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/bert_test_data.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_attention.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/compare_bert_results.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_base.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/convert_tf_models_to_pytorch.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_biasgelu.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/convert_to_onnx.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_embedlayer.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/float16.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_fastgelu.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_attention.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_gelu.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_base.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_gelu_approximation.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_biasgelu.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_gpt_attention.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_embedlayer.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_gpt_attention_no_past.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_fastgelu.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_layernorm.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_gelu.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_reshape.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_gelu_approximation.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_skiplayernorm.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_gpt_attention.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_utils.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_gpt_attention_no_past.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/gpt2_beamsearch_helper.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_layernorm.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/gpt2_beamsearch_tester.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_options.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/gpt2_helper.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_reshape.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/gpt2_tester.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_shape.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/huggingface_models.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_skiplayernorm.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/machine_info.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_utils.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/onnx_exporter.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/gpt2_beamsearch_helper.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/onnx_model.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/gpt2_beamsearch_tester.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/onnx_model_bert.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/gpt2_helper.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/onnx_model_bert_keras.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/gpt2_parity.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/onnx_model_bert_tf.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/gpt2_tester.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/onnx_model_gpt2.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/huggingface_models.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/optimizer.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/machine_info.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/parity_check_helper.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/onnx_exporter.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/profiler.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/onnx_model.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/quantize_helper.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/onnx_model_bart.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/shape_infer_helper.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/onnx_model_bert.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/shape_optimizer.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/onnx_model_bert_keras.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/longformer/__init__.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/onnx_model_bert_tf.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/longformer/benchmark_longformer.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/onnx_model_gpt2.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/longformer/convert_longformer_to_onnx.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/optimizer.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/longformer/generate_test_data.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/parity_check_helper.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/longformer/longformer_helper.py
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/profiler.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.dist-info/METADATA
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/quantize_helper.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.dist-info/WHEEL
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/shape_infer_helper.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.dist-info/entry_points.txt
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/shape_optimizer.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.dist-info/top_level.txt
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/longformer/__init__.py
 Comment: 
 
-Filename: onnxruntime-1.8.1.dist-info/RECORD
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/longformer/benchmark_longformer.py
+Comment: 
+
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/longformer/convert_longformer_to_onnx.py
+Comment: 
+
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/longformer/generate_test_data.py
+Comment: 
+
+Filename: onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/longformer/longformer_helper.py
+Comment: 
+
+Filename: onnxruntime-1.9.0.dist-info/METADATA
+Comment: 
+
+Filename: onnxruntime-1.9.0.dist-info/WHEEL
+Comment: 
+
+Filename: onnxruntime-1.9.0.dist-info/entry_points.txt
+Comment: 
+
+Filename: onnxruntime-1.9.0.dist-info/top_level.txt
+Comment: 
+
+Filename: onnxruntime-1.9.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/LICENSE` & `onnxruntime-1.9.0.data/purelib/onnxruntime/LICENSE`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/Privacy.md` & `onnxruntime-1.9.0.data/purelib/onnxruntime/Privacy.md`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/ThirdPartyNotices.txt` & `onnxruntime-1.9.0.data/purelib/onnxruntime/ThirdPartyNotices.txt`

 * *Files 0% similar despite different names*

```diff
@@ -4709,7 +4709,69 @@
 FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 SOFTWARE.
 
 _____
+
+pytorch/cpuinfo
+
+BSD 2-Clause "Simplified" License
+
+https://github.com/pytorch/cpuinfo
+
+Copyright (c) 2019 Google LLC
+Copyright (c) 2017-2018 Facebook Inc.
+Copyright (C) 2012-2017 Georgia Institute of Technology
+Copyright (C) 2010-2012 Marat Dukhan
+
+All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are met:
+
+* Redistributions of source code must retain the above copyright notice, this
+  list of conditions and the following disclaimer.
+
+* Redistributions in binary form must reproduce the above copyright notice,
+  this list of conditions and the following disclaimer in the documentation
+  and/or other materials provided with the distribution.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+_____
+
+SQLite Is Public Domain
+
+All of the code and documentation in SQLite has been dedicated to the public
+domain by the authors. All code authors, and representatives of the companies
+they work for, have signed affidavits dedicating their contributions to the
+public domain and originals of those signed affidavits are stored in a firesafe
+at the main offices of Hwaci. Anyone is free to copy, modify, publish, use,
+compile, sell, or distribute the original SQLite code, either in source code
+form or as a compiled binary, for any purpose, commercial or non-commercial,
+and by any means.
+
+The previous paragraph applies to the deliverable code and documentation in
+SQLite - those parts of the SQLite library that you actually bundle and ship
+with a larger application. Some scripts used as part of the build process (for
+example the "configure" scripts generated by autoconf) might fall under other
+open-source licenses. Nothing from these build scripts ever reaches the final
+deliverable SQLite library, however, and so the licenses associated with those
+scripts should not be a factor in assessing your rights to copy and use the
+SQLite library.
+
+All of the deliverable code in SQLite has been written from scratch. No code
+has been taken from other projects or from the open internet. Every line of
+code can be traced back to its original author, and all of those authors have
+public domain dedications on file. So the SQLite code base is clean and is
+uncontaminated with licensed code from other projects.
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/__init__.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/__init__.py`

 * *Files 4% similar despite different names*

```diff
@@ -3,41 +3,42 @@
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 """
 ONNX Runtime is a performance-focused scoring engine for Open Neural Network Exchange (ONNX) models.
 For more information on ONNX Runtime, please see `aka.ms/onnxruntime <https://aka.ms/onnxruntime/>`_
 or the `Github project <https://github.com/microsoft/onnxruntime/>`_.
 """
-__version__ = "1.8.1"
+__version__ = "1.9.0"
 __author__ = "Microsoft"
 
 # we need to do device version validation (for example to check Cuda version for an onnxruntime-training package).
 # in order to know whether the onnxruntime package is for training it needs
 # to do import onnxruntime.training.ortmodule first.
 # onnxruntime.capi._pybind_state is required before import onnxruntime.training.ortmodule.
 # however, import onnxruntime.capi._pybind_state will already raise an exception if a required Cuda version
 # is not found.
 # here we need to save the exception and continue with Cuda version validation in order to post
 # meaningful messages to the user.
 # the saved exception is raised after device version validation.
 try:
     from onnxruntime.capi._pybind_state import get_all_providers, get_available_providers, get_device, set_seed, \
         RunOptions, SessionOptions, set_default_logger_severity, enable_telemetry_events, disable_telemetry_events, \
-        NodeArg, ModelMetadata, GraphOptimizationLevel, ExecutionMode, ExecutionOrder, OrtDevice, SessionIOBinding, \
-        OrtAllocatorType, OrtMemType, OrtArenaCfg, OrtMemoryInfo, create_and_register_allocator
+        NodeArg, ModelMetadata, GraphOptimizationLevel, ExecutionMode, ExecutionOrder, SessionIOBinding, \
+        OrtAllocatorType, OrtMemType, OrtArenaCfg, OrtMemoryInfo, create_and_register_allocator,  OrtSparseFormat
     import_capi_exception = None
 except Exception as e:
     import_capi_exception = e
 
 from onnxruntime.capi import onnxruntime_validation
 
 if import_capi_exception:
     raise import_capi_exception
 
-from onnxruntime.capi.onnxruntime_inference_collection import InferenceSession, IOBinding, OrtValue
+from onnxruntime.capi.onnxruntime_inference_collection import InferenceSession, IOBinding, OrtValue, SparseTensor, \
+    OrtDevice
 
 from onnxruntime.capi.training import *  # noqa: F403
 
 # TODO: thiagofc: Temporary experimental namespace for new PyTorch front-end
 try:
     from . import experimental
 except ImportError:
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/backend/backend.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/backend/backend.py`

 * *Files 2% similar despite different names*

```diff
@@ -75,14 +75,16 @@
 
     @classmethod
     def supports_device(cls, device):
         """
         Check whether the backend is compiled with particular device support.
         In particular it's used in the testing suite.
         """
+        if device == 'CUDA':
+            device = 'GPU'
         return device in get_device()
 
     @classmethod
     def prepare(cls, model, device=None, **kwargs):
         """
         Load the model and creates a :class:`onnxruntime.InferenceSession`
         ready to be used as a backend.
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/backend/backend_rep.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/backend/backend_rep.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/capi/_pybind_state.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/capi/_pybind_state.py`

 * *Files 9% similar despite different names*

```diff
@@ -3,19 +3,21 @@
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 """
 Ensure that dependencies are available and then load the extension module.
 """
 import os
 import platform
+import sys
 
 from . import _ld_preload  # noqa: F401
 
 if platform.system() == "Windows":
     from . import version_info
 
     if version_info.vs2019 and platform.architecture()[0] == "64bit":
         if not os.path.isfile("C:\\Windows\\System32\\vcruntime140_1.dll"):
             raise ImportError(
                 "Microsoft Visual C++ Redistributable for Visual Studio 2019 not installed on the machine.")
 
 from .onnxruntime_pybind11_state import *  # noqa
+
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/capi/onnxruntime_collect_build_info.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/capi/onnxruntime_collect_build_info.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/capi/onnxruntime_providers_shared.dll` & `onnxruntime-1.9.0.data/purelib/onnxruntime/capi/onnxruntime_providers_shared.dll`

 * *Files 7% similar despite different names*

### objdump

```diff
@@ -4,15 +4,15 @@
 start address 0x0000000180001370
 
 Characteristics 0x2022
 	executable
 	large address aware
 	DLL
 
-Time/Date		Thu Jul  1 21:10:52 2021
+Time/Date		Wed Sep 22 01:18:58 2021
 Magic			020b	(PE32+)
 MajorLinkerVersion	14
 MinorLinkerVersion	29
 SizeOfCode		0000000000001000
 SizeOfInitializedData	0000000000001c00
 SizeOfUninitializedData	0000000000000000
 AddressOfEntryPoint	0000000000001370
@@ -25,15 +25,15 @@
 MajorImageVersion	0
 MinorImageVersion	0
 MajorSubsystemVersion	6
 MinorSubsystemVersion	0
 Win32Version		00000000
 SizeOfImage		00007000
 SizeOfHeaders		00000400
-CheckSum		00013b41
+CheckSum		00006a01
 Subsystem		00000003	(Windows CUI)
 DllCharacteristics	00004160
 					HIGH_ENTROPY_VA
 					DYNAMIC_BASE
 					NX_COMPAT
 					GUARD_CF
 SizeOfStackReserve	0000000000100000
@@ -429,15 +429,15 @@
 	reloc   14 offset  328 [2328] DIR64
 	reloc   15 offset  330 [2330] DIR64
 
 There is a debug directory in .rdata at 0x1800021a0
 
 Type                Size     Rva      Offset
   2        CodeView 00000066 000023e4 000017e4
-(format RSDS signature 06d6127b3f5045ee92c36b4541437f8f age 1 pdb D:\a\_work\1\b\RelWithDebInfo\RelWithDebInfo\onnxruntime_providers_shared.pdb)
+(format RSDS signature bbe2b4717e024aac93b5f8cfbb77873a age 1 pdb D:\a\_work\1\b\RelWithDebInfo\RelWithDebInfo\onnxruntime_providers_shared.pdb)
  12         Feature 00000014 0000244c 0000184c
  13         CoffGrp 00000268 00002460 00001860
 
 The .rsrc Resource Directory section:
 000  Type Table: Char: 0, Time: 00000000, Ver: 0/0, Num Names: 0, IDs: 1
 010   Entry: ID: 0x000018, Value: 0x80000018
 018    Name Table: Char: 0, Time: 00000000, Ver: 0/0, Num Names: 0, IDs: 1
@@ -1664,45 +1664,41 @@
    18000219a:	(bad)
    18000219b:	(bad)
    18000219c:	(bad)
    18000219d:	(bad)
    18000219e:	(bad)
    18000219f:	incl   (%rax)
    1800021a1:	add    %al,(%rax)
-   1800021a3:	add    %bl,-0x22(%rdi,%rbp,1)
-   1800021a7:	(bad)
-   1800021a8:	add    %al,(%rax)
-   1800021aa:	add    %al,(%rax)
-   1800021ac:	add    (%rax),%al
-   1800021ae:	add    %al,(%rax)
-   1800021b0:	data16 add %al,(%rax)
-   1800021b3:	add    %ah,%ah
-   1800021b5:	and    (%rax),%eax
-   1800021b7:	add    %ah,%ah
-   1800021b9:	(bad)
+   1800021a3:	add    %al,0x614a84(%rdx)
+   1800021a9:	add    %al,(%rax)
+   1800021ab:	add    %al,(%rdx)
+   1800021ad:	add    %al,(%rax)
+   1800021af:	add    %ah,0x0(%rsi)
+   1800021b2:	add    %al,(%rax)
+   1800021b4:	in     $0x23,%al
+   1800021b6:	add    %al,(%rax)
+   1800021b8:	in     $0x17,%al
    1800021ba:	add    %al,(%rax)
    1800021bc:	add    %al,(%rax)
    1800021be:	add    %al,(%rax)
-   1800021c0:	pop    %rsp
-   1800021c1:	(bad)
-   1800021c2:	fisubs 0x0(%rax)
-   1800021c5:	add    %al,(%rax)
-   1800021c7:	add    %cl,(%rax,%rax,1)
+   1800021c0:	(bad)
+   1800021c1:	test   %cl,0x61(%rdx)
+   1800021c4:	add    %al,(%rax)
+   1800021c6:	add    %al,(%rax)
+   1800021c8:	or     $0x0,%al
    1800021ca:	add    %al,(%rax)
    1800021cc:	adc    $0x0,%al
    1800021ce:	add    %al,(%rax)
    1800021d0:	rex.WR and $0x0,%al
    1800021d3:	add    %cl,0x0(%rax,%rbx,1)
    1800021d7:	add    %al,(%rax)
    1800021d9:	add    %al,(%rax)
-   1800021db:	add    %bl,-0x22(%rdi,%rbp,1)
-   1800021df:	(bad)
-   1800021e0:	add    %al,(%rax)
-   1800021e2:	add    %al,(%rax)
-   1800021e4:	or     $0x68000000,%eax
+   1800021db:	add    %al,0x614a84(%rdx)
+   1800021e1:	add    %al,(%rax)
+   1800021e3:	add    %cl,0x68000000(%rip)        # 0x1e80021e9
    1800021e9:	add    (%rax),%al
    1800021eb:	add    %ah,0x24(%rax)
    1800021ee:	add    %al,(%rax)
    1800021f0:	(bad)
    1800021f1:	sbb    %al,(%rax)
 	...
    1800021ff:	add    %bh,(%rax)
@@ -1781,27 +1777,25 @@
    1800023da:	add    %al,(%rax)
    1800023dc:	xchg   %eax,%esi
    1800023dd:	sbb    $0x920000,%eax
    1800023e2:	add    %al,(%rax)
    1800023e4:	push   %rdx
    1800023e5:	push   %rbx
    1800023e6:	rex.R push %rbx
-   1800023e8:	jnp    0x1800023fc
-   1800023ea:	(bad)
-   1800023eb:	(bad)
-   1800023ec:	push   %rax
-   1800023ed:	(bad)
-   1800023ee:	out    %al,(%dx)
-   1800023ef:	rex.RB xchg %eax,%r10d
-   1800023f1:	ret
-   1800023f2:	imul   $0x43,0x41(%rbp),%eax
-   1800023f6:	jg     0x180002387
-   1800023f8:	add    %eax,(%rax)
-   1800023fa:	add    %al,(%rax)
-   1800023fc:	cmp    0x5c(%rcx,%riz,2),%r11b
+   1800023e8:	jno    0x18000239e
+   1800023ea:	loop   0x1800023a7
+   1800023ec:	add    -0x54(%rsi),%bh
+   1800023ef:	rex.WX xchg %rax,%rbx
+   1800023f1:	mov    $0xf8,%ch
+   1800023f3:	iret
+   1800023f4:	mov    $0x13a8777,%ebx
+   1800023f9:	add    %al,(%rax)
+   1800023fb:	add    %al,0x5c(%rdx,%rdi,1)
+   1800023ff:	(bad)
+   180002400:	pop    %rsp
    180002401:	pop    %rdi
    180002402:	ja     0x180002473
    180002404:	jb     0x180002471
    180002406:	pop    %rsp
    180002407:	xor    %ebx,0x5c(%rdx,%riz,2)
    18000240b:	push   %rdx
    18000240c:	gs insb (%dx),%es:(%rdi)
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/capi/onnxruntime_validation.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/capi/onnxruntime_validation.py`

 * *Files 6% similar despite different names*

```diff
@@ -56,29 +56,37 @@
     else:
         warnings.warn('Unsupported platform (%s). ONNX Runtime supports Linux, macOS and Windows platforms, only.' %
                       __my_system__)
 
 
 def validate_build_package_info():
     import_ortmodule_exception = None
+
+    has_ortmodule = False
     try:
         from onnxruntime.training.ortmodule import ORTModule # noqa
         has_ortmodule = True
     except ImportError:
+        # ORTModule not present
         has_ortmodule = False
-    except EnvironmentError:
-        # ORTModule is present but not ready to run yet
-        has_ortmodule = True
-        pass
     except Exception as e:
         # this may happen if Cuda is not installed, we want to raise it after
         # for any exception other than not having ortmodule, we want to continue
         # device version validation and raise the exception after.
-        import_ortmodule_exception = e
-        has_ortmodule = True
+        try:
+            from onnxruntime.training.ortmodule._fallback import ORTModuleInitException
+            if isinstance(e, ORTModuleInitException):
+                # ORTModule is present but not ready to run yet
+                has_ortmodule = True
+        except Exception:
+            # ORTModule not present
+            has_ortmodule = False
+
+        if not has_ortmodule:
+            import_ortmodule_exception = e
 
     package_name = ''
     version = ''
     cuda_version = ''
 
     if has_ortmodule:
         try:
@@ -105,15 +113,15 @@
                     warnings.warn('onnxruntime training package info: __version__: %s' % version)
                     warnings.warn('onnxruntime training package info: cuda_version: %s' % cuda_version)
                     warnings.warn('onnxruntime build info: cudart_version: %s' % cudart_version)
 
                 # collection cuda library info from current environment.
                 from onnxruntime.capi.onnxruntime_collect_build_info import find_cudart_versions
                 local_cudart_versions = find_cudart_versions(build_env=False, build_cuda_version=cuda_version)
-                if cudart_version and cudart_version not in local_cudart_versions:
+                if cudart_version and local_cudart_versions and cudart_version not in local_cudart_versions:
                     print_build_package_info()
                     warnings.warn('WARNING: failed to find cudart version that matches onnxruntime build info')
                     warnings.warn('WARNING: found cudart versions: %s' % local_cudart_versions)
             else:
                 # TODO: rcom
                 pass
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/datasets/logreg_iris.onnx` & `onnxruntime-1.9.0.data/purelib/onnxruntime/datasets/logreg_iris.onnx`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/calibrate.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/calibrate.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/onnx_model.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/onnx_model.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,13 +1,12 @@
 import onnx
 import itertools
-from .quant_utils import find_by_name
+from .quant_utils import find_by_name, attribute_to_kwarg
 from pathlib import Path
 
-
 class ONNXModel:
     def __init__(self, model):
         self.model = model
 
     def nodes(self):
         return self.model.graph.node
 
@@ -43,26 +42,37 @@
 
     def get_initializer(self, name):
         for tensor in self.model.graph.initializer:
             if tensor.name == name:
                 return tensor
         return None
 
+    def get_initializer_name_set(self):
+        return set(initializer.name for initializer in self.model.graph.initializer)
+
     def remove_initializer(self, tensor):
         if tensor in self.model.graph.initializer:
             self.model.graph.initializer.remove(tensor)
             for input in self.model.graph.input:
                 if input.name == tensor.name:
                     self.model.graph.input.remove(input)
                     break
 
     def remove_initializers(self, init_to_remove):
         for initializer in init_to_remove:
             self.remove_initializer(initializer)
 
+    def get_non_initializer_inputs(self):
+        initializer_names = self.get_initializer_name_set()
+        non_initializer_inputs = set()
+        for input in self.model.graph.input:
+            if input.name not in initializer_names:
+                non_initializer_inputs.add(input.name)
+        return non_initializer_inputs
+
     def input_name_to_nodes(self):
         input_name_to_nodes = {}
         for node in self.model.graph.node:
             for input_name in node.input:
                 if input_name not in input_name_to_nodes:
                     input_name_to_nodes[input_name] = [node]
                 else:
@@ -127,18 +137,47 @@
         nodes = []
         for node in graph.node:
             for node_input in node.input:
                 if node_input == initializer.name:
                     nodes.append(node)
         return nodes
 
-    def replace_gemm_with_matmul(self):
+    @staticmethod
+    def __get_initializer(name, graph_path):
+        for gid in range(len(graph_path) - 1, -1, -1):
+            graph = graph_path[gid]
+            for tensor in graph.initializer:
+                if tensor.name == name:
+                    return tensor, graph
+        return None, None
+
+    @staticmethod
+    def __replace_gemm_with_matmul(graph_path):
         new_nodes = []
+        graph = graph_path[-1]
+        for node in graph.node:
+            graph_attrs = [attr for attr in node.attribute if attr.type == 5 or attr.type == 10]
+            if len(graph_attrs):
+                node_name = node.name
+                kwargs = {}
+                for attr in node.attribute:
+                    if attr.type == 5:
+                        graph_path.append(attr.g)
+                        kv = {attr.name: ONNXModel.__replace_gemm_with_matmul(graph_path)}
+                    elif attr.type == 10:
+                        value = []
+                        for subgraph in attr.graphs:
+                            graph_path.append(subgraph)
+                            value.extend([ONNXModel.__replace_gemm_with_matmul(graph_path)])
+                        kv = {attr.name: value}
+                    else:
+                        kv = attribute_to_kwarg(attr)
+                    kwargs.update(kv)
+                node = onnx.helper.make_node(node.op_type, node.input, node.output, name=node.name, **kwargs)
 
-        for node in self.nodes():
             if node.op_type == 'Gemm':
                 alpha = 1.0
                 beta = 1.0
                 transA = 0
                 transB = 0
                 for attr in node.attribute:
                     if attr.name == 'alpha':
@@ -148,54 +187,64 @@
                     elif attr.name == 'transA':
                         transA = onnx.helper.get_attribute_value(attr)
                     elif attr.name == 'transB':
                         transB = onnx.helper.get_attribute_value(attr)
                 if alpha == 1.0 and beta == 1.0 and transA == 0:
                     inputB = node.input[1]
                     if transB == 1:
-                        B = self.get_initializer(node.input[1])
+                        B, Bs_graph = ONNXModel.__get_initializer(node.input[1], graph_path)
                         if B:
                             # assume B is not used by any other node
                             B_array = onnx.numpy_helper.to_array(B)
                             B_trans = onnx.numpy_helper.from_array(B_array.T)
                             B_trans.name = B.name
-                            self.remove_initializer(B)
-                            self.add_initializer(B_trans)
+                            Bs_graph.initializer.remove(B)
+                            for input in Bs_graph.input:
+                                if input.name == inputB:
+                                    Bs_graph.input.remove(input)
+                                    break
+                            Bs_graph.initializer.extend([B_trans])
                         else:
                             inputB += '_Transposed'
                             transpose_node = onnx.helper.make_node('Transpose',
                                                                    inputs=[node.input[1]],
                                                                    outputs=[inputB],
-                                                                   name=node.name + '_Transpose')
+                                                                   name=node.name + '_Transpose' if node.name != "" else "")
                             new_nodes.append(transpose_node)
 
                     matmul_node = onnx.helper.make_node(
                         'MatMul',
                         inputs=[node.input[0], inputB],
                         outputs=[node.output[0] + ('_MatMul' if len(node.input) > 2 else '')],
-                        name=node.name + '_MatMul' if node.name else "")
+                        name=node.name + '_MatMul' if node.name != "" else "")
                     new_nodes.append(matmul_node)
 
                     if len(node.input) > 2:
                         add_node = onnx.helper.make_node('Add',
                                                          inputs=[node.output[0] + '_MatMul', node.input[2]],
                                                          outputs=node.output,
-                                                         name=node.name + '_Add' if node.name else "")
+                                                         name=node.name + '_Add' if node.name != "" else "")
                         new_nodes.append(add_node)
 
                 # unsupported
                 else:
                     new_nodes.append(node)
 
             # not GEMM
             else:
                 new_nodes.append(node)
 
-        self.graph().ClearField('node')
-        self.graph().node.extend(new_nodes)
+        graph.ClearField('node')
+        graph.node.extend(new_nodes)
+        graph_path.pop()
+        return graph
+
+    def replace_gemm_with_matmul(self):
+        graph_path = [self.graph()]
+        ONNXModel.__replace_gemm_with_matmul(graph_path)
 
     def save_model_to_file(self, output_path, use_external_data_format=False):
         '''
         Save model to external data, which is needed for model size > 2GB
         '''
         self.topological_sort()
         if use_external_data_format:
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/onnx_quantizer.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/onnx_quantizer.py`

 * *Files 10% similar despite different names*

```diff
@@ -12,41 +12,43 @@
 import onnx
 import onnx.numpy_helper
 from onnx import onnx_pb as onnx_proto
 from onnxruntime import SessionOptions, InferenceSession, GraphOptimizationLevel
 
 from .quant_utils import QuantizationMode, QuantizedValueType, QuantizedInitializer, QuantizedValue
 from .quant_utils import find_by_name, get_elem_index, get_mul_node, generate_identified_filename, attribute_to_kwarg, type_to_name
-from .quant_utils import quantize_nparray, quantize_data, compute_scale_zp, get_qrange_for_qType
+from .quant_utils import quantize_nparray, quantize_data, compute_scale_zp, get_qrange_for_qType, get_qmin_qmax_for_qType
 from .quant_utils import QuantType, onnx_domain, __producer__, __version__
 
 from .registry import CreateOpQuantizer, CreateDefaultOpQuantizer
 
 from .onnx_model import ONNXModel
 
-
 class ONNXQuantizer:
     def __init__(self, model, per_channel, reduce_range, mode, static, weight_qType, input_qType, tensors_range,
                  nodes_to_quantize, nodes_to_exclude, op_types_to_quantize, extra_options={}):
 
-        # run shape inference on the model
-        model = onnx.shape_inference.infer_shapes(model)
+        # run shape inference on the model (enabled by default)
+        self.extra_options = extra_options if extra_options is not None else {}
+        if not ('DisableShapeInference' in self.extra_options and self.extra_options['DisableShapeInference']):
+            model = onnx.shape_inference.infer_shapes(model)
         self.value_infos = {vi.name: vi for vi in model.graph.value_info}
         self.value_infos.update({ot.name: ot for ot in model.graph.output})
         self.value_infos.update({it.name: it for it in model.graph.input})
 
         self.model = ONNXModel(model)
         self.per_channel = per_channel  # weight-pack per channel
         self.reduce_range = reduce_range
         self.mode = mode  # QuantizationMode.Value
         self.static = static  # use static quantization for inputs.
         self.fuse_dynamic_quant = False
-        self.extra_options = extra_options if extra_options is not None else {}
+        self.enable_subgraph_quantization = 'EnableSubgraph' in self.extra_options and self.extra_options['EnableSubgraph']
         self.q_matmul_const_b_only = 'MatMulConstBOnly' in self.extra_options and self.extra_options['MatMulConstBOnly']
-        self.is_weight_symmetric = 'WeightSymmetric' not in self.extra_options or self.extra_options['WeightSymmetric']
+        self.is_weight_symmetric = True if 'WeightSymmetric' not in self.extra_options else self.extra_options['WeightSymmetric']
+        self.is_activation_symmetric = False if 'ActivationSymmetric' not in self.extra_options else self.extra_options['ActivationSymmetric']
 
         self.input_qType = onnx_proto.TensorProto.INT8 if input_qType == QuantType.QInt8 else onnx_proto.TensorProto.UINT8
         self.weight_qType = onnx_proto.TensorProto.INT8 if weight_qType == QuantType.QInt8 else onnx_proto.TensorProto.UINT8
         '''
             Dictionary specifying the min and max values for tensors. It has following format:
                 {
                     "param_name": [min, max]
@@ -58,14 +60,21 @@
                 }
         '''
         self.tensors_range = tensors_range
         self.nodes_to_quantize = nodes_to_quantize  # specific nodes to quantize
         self.nodes_to_exclude = nodes_to_exclude  # specific nodes to exclude
         self.op_types_to_quantize = op_types_to_quantize
         self.new_nodes = []
+        self.parent = None
+        self.graph_scope = "/" # for human readable debug information
+        self.tensor_names = { } # in case the shape inference not totally working
+        self.tensor_names.update({ot.name: 1 for ot in model.graph.output})
+        self.tensor_names.update({it.name: 1 for it in model.graph.input})
+        for node in self.model.model.graph.node:
+            self.tensor_names.update({output_name: 1 for output_name in node.output})
 
         self.opset_version = self.check_opset_version()
 
         if not self.mode in QuantizationMode:
             raise ValueError('unsupported quantization mode {}'.format(self.mode))
 
         self.quantization_params = self.calculate_quantization_params()
@@ -79,15 +88,64 @@
         # For int8 data-type, zero point is always zero (respresented by fixed_zero_point_name tensor)
         self.fixed_zero_zp_name = "fixed_zero_zp"
 
         # Map of all original value names to quantized value names
         self.quantized_value_map = {}
         # some output from nodes will be quantized, yet itself should be treat as existing so
         # no dequantized will be applied when needed later
-        self.generated_value_names = {}
+        self.generated_value_names = self.model.get_non_initializer_inputs()
+
+    # routines for subgraph support
+    def quantize_subgraph(self, subgraph, graph_key):
+        '''
+            generate submodel for the subgraph, so that we re-utilize current quantization implementation.
+            quantize the submodel
+            update subgraph and set it back to node
+        '''
+        warped_model = onnx.helper.make_model(subgraph, producer_name='onnx-quantizer',
+                                              opset_imports=self.model.model.opset_import)
+        sub_quanitzer = ONNXQuantizer(warped_model,
+                                      self.per_channel,
+                                      self.reduce_range,
+                                      self.mode,
+                                      self.static,
+                                      self.weight_qType,
+                                      self.input_qType,
+                                      self.tensors_range,
+                                      self.nodes_to_quantize,
+                                      self.nodes_to_exclude,
+                                      self.op_types_to_quantize,
+                                      self.extra_options)
+        sub_quanitzer.parent = self
+        sub_quanitzer.graph_scope = "{}{}/".format(self.graph_scope, graph_key)
+        sub_quanitzer.quantize_model()
+        return sub_quanitzer.model.model.graph
+
+    def quantize_node_with_sub_graph(self, node):
+        '''
+        Check subgraph, if any, quantize it and replace it.
+        return new_nodes added for quantizing subgraph
+        '''
+        graph_attrs = [attr for attr in node.attribute if attr.type == onnx.AttributeProto.GRAPH or attr.type == onnx.AttributeProto.GRAPHS]
+        if len(graph_attrs) == 0:
+            return node
+        node_name = node.name if node.name != "" else "{}_node_count_{}".format(node.op_type, len(self.new_nodes))
+        kwargs = {}
+        for attr in node.attribute:
+            if attr.type == onnx.AttributeProto.GRAPH:
+                kv = {attr.name: self.quantize_subgraph(attr.g, "{}:{}".format(node_name, attr.name))}
+            elif attr.type == onnx.AttributeProto.GRAPHS:
+                value = []
+                for subgraph in attr.graphs:
+                    value.extend([self.quantize_subgraph(subgraph, "{}:{}:{}".format(node_name, attr.name, len(value)))])
+                kv = {attr.name: value}
+            else:
+                kv = attribute_to_kwarg(attr)
+            kwargs.update(kv)
+        return onnx.helper.make_node(node.op_type, node.input, node.output, name=node.name, **kwargs)
 
     def check_opset_version(self):
         ai_onnx_domain = [
             opset for opset in self.model.model.opset_import if not opset.domain or opset.domain == "ai.onnx"
         ]
         if 1 != len(ai_onnx_domain):
             raise ValueError('Failed to find proper ai.onnx domain')
@@ -173,46 +231,64 @@
                 initializers_to_remove.extend([initializer_zp])
 
         self.model.remove_nodes(nodes_to_remove)
         self.model.remove_initializers(initializers_to_remove)
 
         return self.model.model
 
+    def find_initializer_in_path(self, initializer_name):
+        if find_by_name(initializer_name, self.model.initializer()) is not None:
+            return True
+        if self.parent is not None:
+            return self.parent.find_initializer_in_path(initializer_name)
+        return False
+
     def should_quantize(self, node):
         if self.nodes_to_quantize is not None and len(
                 self.nodes_to_quantize) != 0 and node.name not in self.nodes_to_quantize:
             return False
 
         if (node.op_type not in self.op_types_to_quantize):
             return False
 
         if self.nodes_to_exclude is not None and node.name in self.nodes_to_exclude:
             return False
 
         # do not quantize non-constant B matrices for matmul
         if self.q_matmul_const_b_only:
-            if node.op_type == "MatMul" and find_by_name(node.input[1], self.model.initializer()) is None:
+            if node.op_type == "MatMul" and (not self.find_initializer_in_path(node.input[1])):
+                print("Ignore MatMul due to non constant B: {}[{}]".format(self.graph_scope, node.name))
                 return False
 
         return True
 
+    def add_new_nodes(self, nodes):
+        self.new_nodes.extend(nodes)
+        for node in nodes:
+            for output_name in node.output:
+                self.generated_value_names.add(output_name)
+
     def quantize_model(self):
         self.remove_fake_quantized_nodes()
 
         for node in self.model.nodes():
+            # quantize subgraphes if have
+            if self.enable_subgraph_quantization:
+                node = self.quantize_node_with_sub_graph(node)
+
             number_of_existing_new_nodes = len(self.new_nodes)
             if self.should_quantize(node):
                 op_quantizer = CreateOpQuantizer(self, node)
             else:
                 op_quantizer = CreateDefaultOpQuantizer(self, node)
 
             op_quantizer.quantize()
             for i in range(number_of_existing_new_nodes, len(self.new_nodes)):
                 for output_name in self.new_nodes[i].output:
-                    self.generated_value_names.update({output_name : 1})
+                    self.generated_value_names.add(output_name)
 
         self._dequantize_outputs()
 
         # extend is used to append to the list for a protobuf fields
         # https://developers.google.com/protocol-buffers/docs/reference/python-generated?csw=1#fields
         self.model.graph().ClearField('node')
         self.model.graph().node.extend(self.new_nodes)
@@ -383,14 +459,15 @@
         Create initializers and inputs in the graph for zero point and scale of output.
         Zero point and scale values are obtained from self.quantization_params if specified.
             parameter param_name: Name of the quantization parameter.
             return: result, scale_name, zero_point_name, scale_shape, zero_point_shape.
         '''
         if use_scale is None or use_zeropoint is None:
             if self.quantization_params is None or param_name not in self.quantization_params:
+                logging.info("Quantization parameters for tensor:\"{}\" not specified".format(param_name))
                 return False, "", "", "", ""
 
             params = self.quantization_params[param_name]
             if params is None or len(params) != 2:
                 raise ValueError("Quantization parameters should contain zero point and scale. "
                                  "Specified values for output {}: {}".format(param_name, params))
 
@@ -437,18 +514,15 @@
 
         nodes = []
         if data_found == True:
             qlinear_node = onnx.helper.make_node("QuantizeLinear", [input_name, scale_name, zp_name],
                                                  [output_name], ql_node_name)
         else:
             if self.static:
-                raise ValueError(
-                    "Quantization parameters are not specified for param {}."
-                    "In static mode quantization params for inputs and outputs of nodes to be quantized are required.".
-                    format(input_name))
+                return None
             # dynamic mode
             # Scale and Zero Points not available for this input. Add nodes to dynamically compute it
             if self.fuse_dynamic_quant and qType == onnx_proto.TensorProto.UINT8:
                 scale_name = input_name + "_scale"
                 zp_name = input_name + "_zero_point"
                 qlinear_node = onnx.helper.make_node("DynamicQuantizeLinear", [input_name],
                                                      [output_name, scale_name, zp_name], ql_node_name)
@@ -457,89 +531,14 @@
                     self._get_dynamic_input_quantization_params(input_name, nodes, qType)
                 qlinear_node = onnx.helper.make_node("QuantizeLinear", [input_name, scale_name, zp_name],
                                                      [output_name], ql_node_name)
 
         self.quantized_value_map[input_name] = QuantizedValue(input_name, output_name, scale_name, zp_name, qType)
         return nodes + [qlinear_node]
 
-    def get_bias_add_nodes(self, nodes, node, last_output, quantized_bias_name):
-        '''
-        Given a node, this function handles bias add by adding a "reshape" node on bias and an "add" node
-            parameter nodes: new nodes would be appended into nodes
-            parameter node: current node (Conv)
-            parameter last_output: output of previous node (input to bias add)
-            return: the name of output
-        '''
-        # Add tensors for the shape to be reshaped to
-        weight = find_by_name(node.input[1], self.model.initializer())
-        if weight is None:
-            raise ValueError("Expected {} to be an initializer".format(node.input[1]))
-
-        # Add reshape for correct broadcase
-        reshape_input_data = quantized_bias_name
-        reshape_input_shape = quantized_bias_name + "_reshape_shape"
-        reshape_input = [reshape_input_data, reshape_input_shape]
-
-        reshape_shape = np.ones((len(weight.dims)), dtype=np.int64)
-        reshape_shape[1] = -1
-        init_shape = onnx.helper.make_tensor(reshape_input_shape, onnx_proto.TensorProto.INT64, [len(weight.dims)],
-                                             reshape_shape)
-        self.model.add_initializer(init_shape)
-
-        reshape_op_output = node.output[0] + "_reshape"
-        reshape_node = onnx.helper.make_node("Reshape", reshape_input, [reshape_op_output],
-                                             quantized_bias_name + "reshape")
-        nodes.append(reshape_node)
-
-        # Add an Add operation for bias
-        bias_add_input = [last_output]
-        bias_add_input.append(reshape_op_output)
-        add_node_output = node.output[0] + "_bias_add"
-        add_node = onnx.helper.make_node("Add", bias_add_input, [add_node_output], quantized_bias_name + "bias_add")
-        nodes.append(add_node)
-        return add_node_output
-
-    def quantize_bias_dynamic(self, bias_name, input_name, weight_name, new_node_list):
-        '''
-        Quantized the bias. Zero Point == 0 and Scale == Input_Scale * Weight_Scale
-        '''
-
-        # get scale for weight
-        weight_scale_name = self.quantized_value_map[weight_name].scale_name
-        weight_initializer = find_by_name(weight_scale_name, self.model.initializer())
-        weight_scale = self.tensor_proto_to_array(weight_initializer)
-
-        # get bias
-        bias_initializer = find_by_name(bias_name, self.model.initializer())
-        bias_data = self.tensor_proto_to_array(bias_initializer)
-        quantized_bias_name = bias_name + "_quantized"
-
-        qType = onnx_proto.TensorProto.INT32
-
-        input_scale_name = input_name + "_scale"
-        bias_scale_node = onnx.helper.make_node("Mul", [input_scale_name, weight_scale_name], [bias_name + "_scale"],
-                                                bias_name + "_scale_node")
-        new_node_list.append(bias_scale_node)
-
-        quantize_bias_node = onnx.helper.make_node("Div", [bias_name, bias_scale_node.output[0]],
-                                                   [bias_name + "_tmp_quant:0"], bias_name + "_tmp_qaunt")
-        new_node_list.append(quantize_bias_node)
-
-        bias_rounded_node = onnx.helper.make_node("Floor", quantize_bias_node.output, [bias_name + "_quant_rounded:0"],
-                                                  bias_name + "_quant_rounded")
-        new_node_list.append(bias_rounded_node)
-
-        bias_cast_node = onnx.helper.make_node("Cast",
-                                               bias_rounded_node.output, [quantized_bias_name],
-                                               quantized_bias_name + "_node",
-                                               to=qType)
-        new_node_list.append(bias_cast_node)
-
-        return quantized_bias_name
-
     def quantize_bias_static(self, bias_name, input_name, weight_name):
         '''
         Quantized the bias. Zero Point == 0 and Scale == Input_Scale * Weight_Scale
         '''
 
         # Handle case where bias already in quantizatio map
         if bias_name in self.quantized_value_map:
@@ -593,15 +592,21 @@
         quantized_value = QuantizedValue(bias_name, quantized_bias_name, quantized_bias_scale_name,
                                          quantized_bias_zp_name, QuantizedValueType.Initializer,
                                          0 if bias_scale_data.size > 1 else None)
         self.quantized_value_map[bias_name] = quantized_value
 
         return quantized_bias_name
 
-    def quantize_inputs(self, node, indices, initializer_use_weight_qType=True, reduce_range=False, op_level_per_channel=False, axis=-1):
+    def contains_tensor(self, tensor_name):
+        '''
+        only check for value info and newly generated tensor names, initializers are checked seperately
+        '''
+        return (tensor_name in self.value_infos) or (tensor_name in self.tensor_names) or (tensor_name in self.generated_value_names)
+
+    def quantize_inputs(self, node, indices, initializer_use_weight_qType=True, reduce_range=False, op_level_per_channel=False, axis=-1, from_subgraph=False):
         '''
         Given a node, this function quantizes the inputs as follows:
             - If input is an initializer, quantize the initializer data, replace old initializer
               with new initializer
             - Else, add QuantizeLinear nodes to perform quantization
             parameter node: node being quantized in NodeProto format.
             parameter indices: input indices to quantize.
@@ -638,31 +643,51 @@
                     q_weight_name, zp_name, scale_name = self.quantize_weight(
                         initializer, self.weight_qType if initializer_use_weight_qType else self.input_qType,
                         reduce_range)
 
                 quantized_input_names.append(q_weight_name)
                 zero_point_names.append(zp_name)
                 scale_names.append(scale_name)
-            else:
+            elif self.contains_tensor(node_input):
                 # Add QuantizeLinear node.
                 qlinear_node = self.model.find_node_by_name(node_input + "_QuantizeLinear", self.new_nodes,
                                                             self.model.graph())
                 if qlinear_node is None:
                     quantize_input_nodes = self._get_quantize_input_nodes(node, input_index, self.input_qType)
-                    nodes.extend(quantize_input_nodes)
+                    if quantize_input_nodes is None:
+                        return (None, None, None, None)
+                    if from_subgraph:
+                        self.add_new_nodes(quantize_input_nodes)
+                    else:
+                        nodes.extend(quantize_input_nodes)
                     qlinear_node = quantize_input_nodes[-1]
 
                 if qlinear_node.op_type == "QuantizeLinear":
                     quantized_input_names.extend(qlinear_node.output)
                     scale_names.append(qlinear_node.input[1])
                     zero_point_names.append(qlinear_node.input[2])
                 else:
                     quantized_input_names.append(qlinear_node.output[0])
                     scale_names.append(qlinear_node.output[1])
                     zero_point_names.append(qlinear_node.output[2])
+            elif self.parent is not None:
+                (parent_quantized_input_names, parent_zero_point_names, parent_scale_names, _) = self.parent.quantize_inputs(
+                    node,
+                    [input_index],
+                    initializer_use_weight_qType=initializer_use_weight_qType,
+                    reduce_range=reduce_range,
+                    op_level_per_channel=op_level_per_channel,
+                    axis=axis,
+                    from_subgraph=True)
+                quantized_input_names.append(parent_quantized_input_names[0])
+                scale_names.append(parent_scale_names[0])
+                zero_point_names.append(parent_zero_point_names[0])
+                # node should not be add this child level here
+            else:
+                raise ValueError('Invalid tensor name to quantize: {} @graph scope{}'.format(node_input, self.graph_scope))
 
         return (quantized_input_names, zero_point_names, scale_names, nodes)
 
     def quantize_weight(self, weight, qType, reduce_range=False):
         '''
             :param weight: TensorProto initializer
             :param qType: type to quantize to
@@ -676,16 +701,16 @@
         q_weight_name = weight.name + "_quantized"
         zp_name = weight.name + "_zero_point"
         scale_name = weight.name + "_scale"
 
         # Update packed weight, zero point, and scale initializers
         weight_data = self.tensor_proto_to_array(weight)
         _, _, zero_point, scale, q_weight_data = quantize_data(weight_data.flatten().tolist(),
-                                                               get_qrange_for_qType(qType, self.reduce_range and reduce_range),
-                                                               qType, self.is_weight_symmetric)
+                                                               qType, self.is_weight_symmetric,
+                                                               self.reduce_range and reduce_range)
         q_weight_data = np.asarray(q_weight_data, dtype=onnx.mapping.TENSOR_TYPE_TO_NP_TYPE[qType]).reshape(weight.dims)
         q_weight_initializer = onnx.numpy_helper.from_array(q_weight_data, q_weight_name)
 
         scale_initializer = onnx.helper.make_tensor(scale_name, onnx_proto.TensorProto.FLOAT, [], [scale])
         zero_initializer = onnx.helper.make_tensor(zp_name, qType, [], [zero_point])
         self.model.initializer().extend([q_weight_initializer, scale_initializer, zero_initializer])
 
@@ -712,17 +737,16 @@
         rmax_list = []
         zero_point_list = []
         scale_list = []
         quantized_per_channel_data_list = []
         for i in range(channel_count):
             per_channel_data = weights.take(i, channel_axis)
             rmin, rmax, zero_point, scale, quantized_per_channel_data = quantize_data(
-                per_channel_data.flatten().tolist(),
-                get_qrange_for_qType(weight_qType, self.reduce_range and reduce_range),
-                weight_qType, self.is_weight_symmetric)
+                per_channel_data.flatten().tolist(), weight_qType,
+                self.is_weight_symmetric, self.reduce_range and reduce_range)
             rmin_list.append(rmin)
             rmax_list.append(rmax)
             zero_point_list.append(zero_point)
             scale_list.append(scale)
             quantized_per_channel_data_list.append(quantized_per_channel_data)
 
         # combine per_channel_data into one
@@ -806,23 +830,19 @@
             if node.input[0] not in self.tensors_range.keys() or node.output[0] not in self.tensors_range.keys():
                 continue
             self.tensors_range[node.input[0]] = self.tensors_range[node.output[0]]
 
         quantization_params = {}
         for tensor_name in self.tensors_range.keys():
             rmin, rmax = self.tensors_range[tensor_name]
+            qmin, qmax = get_qmin_qmax_for_qType(self.input_qType)
 
-            # adjust rmin and rmax such that 0 is included in the range. This is required
-            # to make sure zero can be uniquely represented.
-            rmin = min(rmin, 0)
-            rmax = max(rmax, 0)
-
-            quantization_params[tensor_name] = compute_scale_zp(rmin, rmax, self.input_qType,
-                                                                get_qrange_for_qType(self.input_qType),
-                                                                self.is_weight_symmetric)
+            quantization_params[tensor_name] = compute_scale_zp(rmin, rmax,
+                                                                qmin, qmax,
+                                                                self.is_activation_symmetric)
 
         return quantization_params
 
     def remove_quantized_weights(self):
         ''' Remove the weights which are already quantized from graph initializer list.
             This function assumes that after quantization, all nodes that previously use a weight:
                 - use output from DequantizeLinear as input if they do not support quantization.
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/qdq_quantizer.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/qdq_quantizer.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/quant_utils.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/quant_utils.py`

 * *Files 6% similar despite different names*

```diff
@@ -105,73 +105,99 @@
     cliplow = max(0 if dtype == numpy.uint8 else -127, -127 if low is None else low)
     cliphigh = min(255 if dtype == numpy.uint8 else 127, 255 if high is None else high)
     arr_fp32 = numpy.asarray((arr.astype(numpy.float32) / scale).round() + zero_point)
     numpy.clip(arr_fp32, cliplow, cliphigh, out=arr_fp32)
     return arr_fp32.astype(dtype)
 
 
-def compute_scale_zp(rmin, rmax, qType, quantize_range, symmetric):
-    if qType == onnx_proto.TensorProto.INT8:
-        if symmetric:
-            max_range = max(abs(rmin), abs(rmax))
-            scale = (float(max_range) * 2) / quantize_range if max_range > 0 else 1.0
-            zero_point = 0
-        else:
-            max_range = float(rmax) - float(rmin)
-            scale = float(max_range) / quantize_range if max_range > 0 else 1.0
-            zero_point = round((quantize_range / 2) - rmax / scale)
-    elif qType == onnx_proto.TensorProto.UINT8:
-        scale = (float(rmax) - rmin) / quantize_range if rmin != rmax else 1
-        zero_point = round((0 - rmin) / scale)  # round to nearest integer
-    else:
-        raise ValueError("Unexpected data type {} requested. Only INT8 and UINT8 are supported.".format(qType))
+def compute_scale_zp(rmin, rmax, qmin, qmax, symmetric=False):
+    '''
+    Calculate the scale s and zero point z for the quantization relation 
+    r = s(q-z), where r are the original values and q are the corresponding
+    quantized values. 
+
+    r and z are calculated such that every value within [rmin,rmax] has an
+    approximate representation within [qmin,qmax]. In addition, qmin <= z <=
+    qmax is enforced. If the symmetric flag is set to True, the interval
+    [rmin,rmax] is symmetrized to [-absmax, +absmax], where
+    absmax = max(abs(rmin), abs(rmax)).
+
+    :parameter rmin: minimum value of r
+    :parameter rmax: maximum value of r
+    :parameter qmin: minimum value representable by the target quantization data type
+    :parameter qmax: maximum value representable by the target quantization data type
+    :return: zero and scale [z, s]
+
+    '''
+    
+    # Adjust rmin and rmax such that 0 is included in the range. This is
+    # required to make sure zero can be represented by the quantization data
+    # type (i.e. to make sure qmin <= zero_point <= qmax)
+    rmin = min(rmin, 0)
+    rmax = max(rmax, 0)
+
+    if symmetric:
+        absmax = max(abs(rmin), abs(rmax))
+        rmin = -absmax
+        rmax = +absmax
+
+    scale = (rmax - rmin) / float(qmax-qmin) if rmax!=rmin else 1.0
+    zero_point = round(qmin - rmin/scale)
 
     return [zero_point, scale]
 
 
-def quantize_data(data, quantize_range, qType, symmetric=True):
+def quantize_data(data, qType, symmetric, reduce_range=False):
     '''
         :parameter data: data to quantize
-        :parameter quantize_range: list of data to weight pack.
         :parameter qType: data type to quantize to. Supported types UINT8 and INT8
         :parameter symmetric: whether symmetric quantization is used or not. This is applied to INT8.
         :return: minimum, maximum, zero point, scale, and quantized weights
         To pack weights, we compute a linear transformation
             - when data type == uint8 mode, from [rmin, rmax] -> [0, 2^{b-1}] and
             - when data type == int8, from [-m , m] -> [-(2^{b-1}-1), 2^{b-1}-1] where
                 m = max(abs(rmin), abs(rmax))
         and add necessary intermediate nodes to trasnform quantized weight to full weight using the equation
         r = S(q-z), where
             r: real original value
             q: quantized value
             S: scale
             z: zero point
     '''
-    rmin = min(min(data), 0.)
-    rmax = max(max(data), 0.)
+    rmin = min(data)
+    rmax = max(data)
+    qmin, qmax = get_qmin_qmax_for_qType(qType, reduce_range)
 
-    zero_point, scale = compute_scale_zp(rmin, rmax, qType, quantize_range, symmetric)
+    zero_point, scale = compute_scale_zp(rmin, rmax, qmin, qmax, symmetric)
     quantized_data = quantize_nparray(qType, numpy.asarray(data), scale, zero_point)
 
     return rmin, rmax, zero_point, scale, quantized_data
 
+def get_qmin_qmax_for_qType(qType, reduce_range=False):
+    '''
+    Return qmin and qmax, the minimum and maximum value representable by the given qType
+    :parameter qType: onnx.onnx_pb.TensorProto.UINT8 or onnx.onnx_pb.TensorProto.UINT8
+    :return: qmin, qmax
+    '''
+    if qType == onnx_proto.TensorProto.UINT8:
+        (qmin, qmax) = (0,127) if reduce_range else (0,255)
+    elif qType == onnx_proto.TensorProto.INT8:
+        (qmin, qmax) = (-64,64) if reduce_range else (-127,127)
+    else:
+        raise ValueError("Unexpected data type {} requested. Only INT8 and UINT8 are supported.".format(qType))
+    return qmin, qmax
 
 def get_qrange_for_qType(qType, reduce_range=False):
     '''
     Helper function to get the quantization range for a type.
         parameter qType: quantization type.
         return: quantization range.
     '''
-    if qType == onnx_proto.TensorProto.UINT8:
-        return 127 if reduce_range else 255
-    elif qType == onnx_proto.TensorProto.INT8:
-        return 128 if reduce_range else 254  # [-64, 64] for reduce_range, and [-127, 127] full_range.
-    else:
-        raise ValueError('unsupported quantization data type')
-
+    qmin, qmax = get_qmin_qmax_for_qType(qType, reduce_range)
+    return  qmax - qmin
 
 class QuantizedInitializer:
     '''
         Represents a linearly quantized weight input from ONNX operators
     '''
     def __init__(self,
                  name,
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/quantize.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/quantize.py`

 * *Files 2% similar despite different names*

```diff
@@ -171,25 +171,29 @@
             'Conv__224',
             'Conv__252'
         ]
     :param nodes_to_exclude:
         List of nodes names to exclude. The nodes in this list will be excluded from quantization
         when it is not None.
     :param optimize_model: optimize model before quantization.
-    :parma use_external_data_format: option used for large size (>2GB) model. Set to False by default. 
+    :param use_external_data_format: option used for large size (>2GB) model. Set to False by default. 
     :param calibrate_method: 
         Current calibration methods supported are MinMax and Entropy. 
         Please use CalibrationMethod.MinMax or CalibrationMethod.Entropy as options.
     :param extra_options:
         key value pair dictionary for various options in different case. Current used:
-            extra.Sigmoid.nnapi = True  (Default is False)
-    '''
+            extra.Sigmoid.nnapi = True/False  (Default is False)
+            ActivationSymmetric = True/False: symmetrize calibration data for activations (default is False).
+            WeightSymmetric = True/False: symmetrize calibration data for weights (default is True).
+            EnableSubgraph = True/False : Default is False. If enabled, subgraph will be quantized.
+                                          Dyanmic mode currently is supported. Will support more in future.
+            DisableShapeInference = True/False : in dynamic quantize mode, shape inference is not must have
+                                                 and if it cause some issue, you could disable it.
 
-    if activation_type != QuantType.QUInt8:
-        raise ValueError("Static quantization only support uint8 for activation now.")
+    '''
 
     mode = QuantizationMode.QLinearOps
 
     if not op_types_to_quantize or len(op_types_to_quantize) == 0:
         op_types_to_quantize = list(QLinearOpsRegistry.keys())
 
     model = load_model(Path(model_input), optimize_model)
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/registry.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/registry.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/CalTableFlatBuffers/KeyValue.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/CalTableFlatBuffers/KeyValue.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/CalTableFlatBuffers/TrtTable.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/CalTableFlatBuffers/TrtTable.py`

 * *Files 7% similar despite different names*

```diff
@@ -24,15 +24,15 @@
     # TrtTable
     def Dict(self, j):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(4))
         if o != 0:
             x = self._tab.Vector(o)
             x += flatbuffers.number_types.UOffsetTFlags.py_type(j) * 4
             x = self._tab.Indirect(x)
-            from CalTableFlatBuffers.KeyValue import KeyValue
+            from onnxruntime.quantization.CalTableFlatBuffers.KeyValue import KeyValue
             obj = KeyValue()
             obj.Init(self._tab.Bytes, x)
             return obj
         return None
 
     # TrtTable
     def DictLength(self):
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/activation.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/activation.py`

 * *Files 4% similar despite different names*

```diff
@@ -36,19 +36,17 @@
         use_scale = 1 / 256.0 if sigmoid_nnapi_mode else None
         use_zeropoint = 0 if sigmoid_nnapi_mode else None
 
         # No assert on op_type as it is controlled by registry
         # only try to quantize when given quantization parameters for it
         data_found, output_scale_name, output_zp_name, _, _ = \
             self.quantizer._get_quantization_params(node.output[0], use_scale, use_zeropoint)
-        if not data_found:
-            super().quantize()
-            return
-
         quantized_input_names, zero_point_names, scale_names, nodes = self.quantizer.quantize_inputs(node, [0])
+        if not data_found or quantized_input_names is None:
+            return super().quantize()
 
         qlinear_activation_output = node.output[0] + "_quantized"
         qlinear_activation_name = ""
         if node.name != "":
             qlinear_activation_name = node.name + "_quant"
         kwargs = {}
         for attribute in node.attribute:
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/attention.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/attention.py`

 * *Files 10% similar despite different names*

```diff
@@ -18,14 +18,16 @@
             return: a list of nodes in topological order that represents quantized Attention node.
         '''
         node = self.node
         assert (node.op_type == "Attention")
 
         (quantized_input_names, zero_point_names, scale_names, nodes) = \
             self.quantizer.quantize_inputs(node, [0, 1], reduce_range=True, op_level_per_channel=True)
+        if quantized_input_names is None:
+            return super().quantize()
 
         qattention_name = "" if node.name == "" else node.name + "_quant"
 
         inputs = []
         inputs.extend(quantized_input_names)
         inputs.extend([node.input[2]])
         inputs.extend(scale_names)
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/base_operator.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/base_operator.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,22 +1,21 @@
 class QuantOperatorBase:
     def __init__(self, onnx_quantizer, onnx_node):
         self.quantizer = onnx_quantizer
         self.node = onnx_node
 
     def quantize(self):
         '''
-        Given a node which does not support quantization(Conv, Matmul, Gather), this method
-        checks whether the input to this node is quantized and adds a DequantizeLinear node
-        to dequantize this input back to FP32
+        Given a node which does not support quantization, this method checks whether the input to
+        this node is quantized and adds a DequantizeLinear node to dequantize this input back to FP32
             parameter node: Current node
             parameter new_nodes_list: List of new nodes created before processing current node
             return: List of new nodes created
         '''
         nodes = []
         for index, node_input in enumerate(self.node.input):
             dequantize_node = self.quantizer._dequantize_value(node_input)
             if dequantize_node is not None:
                 self.quantizer.new_nodes.append(dequantize_node)
 
         # Append the original node
-        self.quantizer.new_nodes.append(self.node)
+        self.quantizer.new_nodes.append(self.node)
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/binary_op.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/binary_op.py`

 * *Files 2% similar despite different names*

```diff
@@ -9,19 +9,18 @@
         super().__init__(onnx_quantizer, onnx_node)
 
     def quantize(self):
         node = self.node
 
         data_found, output_scale_name, output_zp_name, _, _ = \
             self.quantizer._get_quantization_params(node.output[0])
-        if (not data_found):  # only try to quantize when given quantization parameters for it
-            return super().quantize()
-
         (quantized_input_names, zero_point_names, scale_names, nodes) = \
             self.quantizer.quantize_inputs(node, [0, 1], initializer_use_weight_qType=False)
+        if not data_found or quantized_input_names is None:
+            return super().quantize()
 
         qlinear_binary_math_output = node.output[0] + "_quantized"
         qlinear_binary_math_name = node.name + "_quant" if node.name != "" else ""
 
         kwargs = {}
         for attribute in node.attribute:
             kwargs.update(attribute_to_kwarg(attribute))
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/concat.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/concat.py`

 * *Files 8% similar despite different names*

```diff
@@ -8,17 +8,17 @@
         super().__init__(onnx_quantizer, onnx_node)
 
     def quantize(self):
         node = self.node
 
         data_found, output_scale_name, output_zp_name, _, _ = \
             self.quantizer._get_quantization_params(node.output[0])
-        if not data_found:
-            raise ValueError("Quantization parameters for :\"{}\" of node:\"{}\" not specified".format(node.output[0], node.name))
         (q_input_names, zero_point_names, scale_names, nodes) = self.quantizer.quantize_inputs(node, [*range(0, len(node.input))])
+        if not data_found or q_input_names is None:
+            return super().quantize()
 
         # Create an entry for output quantized value
         quantized_input_value = self.quantizer.quantized_value_map[node.input[0]]
         quantized_output_value = QuantizedValue(node.output[0], node.output[0] + "_quantized",
                                                 output_scale_name, output_zp_name,
                                                 quantized_input_value.value_type)
         self.quantizer.quantized_value_map[node.output[0]] = quantized_output_value
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/conv.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/conv.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,48 +1,72 @@
 import onnx
+import numpy as np
 from .base_operator import QuantOperatorBase
 from .qdq_base_operator import QDQOperatorBase
 from ..quant_utils import find_by_name, get_mul_node, QuantizedValue, QuantizedValueType, attribute_to_kwarg, BiasToQuantize
 from onnx import onnx_pb as onnx_proto
 
 
 class ConvInteger(QuantOperatorBase):
     def __init__(self, onnx_quantizer, onnx_node):
         super().__init__(onnx_quantizer, onnx_node)
 
+    def add_bias(self, nodes, scaled_output):
+        '''
+        Given a node, this function handles bias add by adding a "reshape" node on bias and an "add" node
+            parameter nodes: new nodes would be appended into nodes
+            parameter node: current node (Conv)
+            parameter scaled_output: output of quant conv without bias
+            parameter output: output of Conv
+            parameter bias_name: bias of Conv
+            return: the name of output
+        '''
+        node = self.node
+        model = self.quantizer.model
+        # Add tensors for the shape to be reshaped to
+        weight = find_by_name(node.input[1], model.initializer())
+        if weight is None:
+            raise ValueError("Expected {} to be an initializer".format(node.input[1]))
+
+        # Add reshape for correct broadcase
+        output = node.output[0]
+        reshape_input_data = node.input[2] # bias of Conv
+        reshape_input_shape = output + "_bias_reshape_shape"
+        reshape_output = output + "_bias_reshape_output"
+
+        shape = np.ones((len(weight.dims)), dtype=np.int64)
+        shape[1] = -1
+        init_shape = onnx.helper.make_tensor(reshape_input_shape, onnx_proto.TensorProto.INT64, [len(weight.dims)],
+                                             shape)
+        model.add_initializer(init_shape)
+
+        reshape_node = onnx.helper.make_node("Reshape", [reshape_input_data, reshape_input_shape], [reshape_output])
+        nodes.append(reshape_node)
+
+        # Add an Add operation for bias
+        add_node = onnx.helper.make_node("Add", [scaled_output, reshape_output], [output], output + "_bias_add")
+        nodes.append(add_node)
+
     def quantize(self):
         node = self.node
         assert (node.op_type == "Conv")
 
         (quantized_input_names, zero_point_names, scale_names, nodes) = \
             self.quantizer.quantize_inputs(node, [0, 1])
 
-        # quantize bias if exist
-        quantized_bias_name = ""
-        bias_present = False
-        if len(node.input) == 3:
-            quantized_bias_name = self.quantizer.quantize_bias_dynamic(node.input[2], node.input[0], node.input[1],
-                                                                       nodes)
-            bias_present = True
-
         conv_integer_output = node.output[0] + "_output_quantized"
         conv_integer_name = node.name + "_quant" if node.name != "" else ""
 
         kwargs = {}
         for attribute in node.attribute:
             kwargs.update(attribute_to_kwarg(attribute))
         conv_integer_node = onnx.helper.make_node("ConvInteger", quantized_input_names + zero_point_names,
                                                   [conv_integer_output], conv_integer_name, **kwargs)
         nodes.append(conv_integer_node)
 
-        # Add bias add nodes
-        if bias_present:
-            conv_integer_output = self.quantizer.get_bias_add_nodes(nodes, node, conv_integer_output,
-                                                                    quantized_bias_name)
-
         # Add cast operation to cast convInteger output to float.
         cast_op_output = conv_integer_output + "_cast_output"
         cast_node = onnx.helper.make_node("Cast", [conv_integer_output], [cast_op_output],
                                           conv_integer_output + "_cast",
                                           to=onnx_proto.TensorProto.FLOAT)
         nodes.append(cast_node)
 
@@ -56,53 +80,59 @@
         scales_mul_node = find_by_name(scales_mul_op, self.quantizer.new_nodes)
         if scales_mul_node is None:
             scales_mul_node = get_mul_node(scale_names, scales_mul_op + ":0", scales_mul_op)
             nodes.append(scales_mul_node)
 
         scales_mul_op_output = scales_mul_node.output[0]
 
+        has_bias = len(node.input) == 3
+        scaled_output_name = node.output[0] if not has_bias else node.output[0] + "quant_scaled_output"
+
         # Add mul operation to multiply mul_scales_op result with output of ConvInteger
         # and make the output of this node the same as output of original conv node.
         output_scale_mul_op = conv_integer_name + "_output_scale_mul" if conv_integer_name != "" else ""
-        nodes.append(get_mul_node([cast_op_output, scales_mul_op_output], node.output[0], output_scale_mul_op))
+        nodes.append(get_mul_node([cast_op_output, scales_mul_op_output], scaled_output_name, output_scale_mul_op))
+
+        if has_bias:
+            self.add_bias(nodes, scaled_output_name)
 
         self.quantizer.new_nodes += nodes
 
 
 class QLinearConv(QuantOperatorBase):
     def __init__(self, onnx_quantizer, onnx_node):
         super().__init__(onnx_quantizer, onnx_node)
 
     def quantize(self):
         node = self.node
         assert (node.op_type == "Conv")
 
+        data_found, output_scale_name, output_zp_name, _, _ = \
+            self.quantizer._get_quantization_params(node.output[0])
+
         if self.quantizer.is_input_a_weight(node.input[1]) and self.quantizer.is_per_channel():
             (quantized_input_names, zero_point_names, scale_names, nodes) = \
                 self.quantizer.quantize_inputs(node, [0])
             quant_weight_tuple = self.quantizer.quantize_weight_per_channel(node.input[1], onnx_proto.TensorProto.INT8,
                                                                             0)
             quantized_input_names.append(quant_weight_tuple[0])
             zero_point_names.append(quant_weight_tuple[1])
             scale_names.append(quant_weight_tuple[2])
         else:
             (quantized_input_names, zero_point_names, scale_names, nodes) = \
                 self.quantizer.quantize_inputs(node, [0, 1])
 
+        if not data_found or quantized_input_names is None:
+            return super().quantize()
+
         quantized_bias_name = ""
         bias_present = False
         if len(node.input) == 3:
             quantized_bias_name = self.quantizer.quantize_bias_static(node.input[2], node.input[0], node.input[1])
             bias_present = True
-        data_found, output_scale_name, output_zp_name, _, _ = \
-            self.quantizer._get_quantization_params(node.output[0])
-
-        if not data_found:
-            raise ValueError("Quantization parameters for output:\"{}\" of node:\"{}\" not specified".format(
-                node.output[0], node.name))
 
         qlinear_conv_output = node.output[0] + "_quantized"
         qlinear_conv_name = qlinear_conv_name = node.name + "_quant" if node.name != "" else ""
 
         kwargs = {}
         for attribute in node.attribute:
             kwargs.update(attribute_to_kwarg(attribute))
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/direct_q8.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/direct_q8.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/gather.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/gather.py`

 * *Files 4% similar despite different names*

```diff
@@ -16,14 +16,16 @@
         assert (node.op_type == "Gather")
         if (not self.quantizer.is_valid_quantize_weight(node.input[0])):
             super().quantize()
             return
 
         (quantized_input_names, zero_point_names, scale_names, nodes) = \
             self.quantizer.quantize_inputs(node, [0])
+        if quantized_input_names is None:
+            return super().quantize()
 
         gather_new_output = node.output[0] + "_quantized"
 
         # Create an entry for this quantized value
         q_output = QuantizedValue(node.output[0], gather_new_output, scale_names[0], zero_point_names[0],
                                   QuantizedValueType.Input)
         self.quantizer.quantized_value_map[node.output[0]] = q_output
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/gavgpool.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/gavgpool.py`

 * *Files 1% similar despite different names*

```diff
@@ -10,14 +10,15 @@
     def quantize(self):
         node = self.node
         assert (node.op_type == "GlobalAveragePool")
 
         # If input to this node is not quantized then keep this node.
         if node.input[0] not in self.quantizer.quantized_value_map:
             return super().quantize()
+
         quantized_input_value = self.quantizer.quantized_value_map[node.input[0]]
 
         # Create an entry for output quantized value.
         quantized_input_value = self.quantizer.quantized_value_map[node.input[0]]
         data_found, output_scale_name_from_parameter, output_zp_name_from_parameter, _, _ = \
             self.quantizer._get_quantization_params(node.output[0])
         # Just use input scale and zp if parameters for output is not specified.
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/lstm.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/lstm.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/matmul.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/matmul.py`

 * *Files 16% similar despite different names*

```diff
@@ -63,21 +63,18 @@
 
     def quantize(self):
         node = self.node
         assert (node.op_type == "MatMul")
 
         (quantized_input_names, zero_point_names, scale_names, nodes) = \
             self.quantizer.quantize_inputs(node, [0, 1], reduce_range=True, op_level_per_channel=True)
-
         data_found, output_scale_name, output_zp_name, _, _ = \
             self.quantizer._get_quantization_params(node.output[0])
-
-        if not data_found:
-            raise ValueError("Quantization parameters for output:\"{}\" of node:\"{}\" not specified".format(
-                node.output[0], node.name))
+        if not data_found or quantized_input_names is None:
+            return super().quantize()
 
         qlinear_matmul_output = node.output[0] + "_quantized"
         qlinear_matmul_name = node.name + "_quant" if node.name != "" else ""
 
         qlinear_matmul_inputs = []
         # Input 0
         qlinear_matmul_inputs.append(quantized_input_names[0])
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/maxpool.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/maxpool.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/pad.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/pad.py`

 * *Files 0% similar despite different names*

```diff
@@ -45,14 +45,15 @@
                     quantized_padding_constant_initializer = onnx.numpy_helper.from_array(
                         quantized_padding_constant_array, quantized_padding_constant_name)
                     # Suppose this padding constant initializer only used by the node
                     self.quantizer.model.remove_initializer(padding_constant_initializer)
                     self.quantizer.model.add_initializer(quantized_padding_constant_initializer)
                     node.input[2] = quantized_padding_constant_name
                 else:
+                    # TODO: check quantize_inputs after sub graph is supported
                     pad_value_qnodes = self.quantizer._get_quantize_input_nodes(node, 2, self.quantizer.input_qType,
                                                                                 quantized_input_value.scale_name,
                                                                                 quantized_input_value.zp_name)
                     self.quantizer.new_nodes += [pad_value_qnodes]
                     node.input[2] = pad_value_qnodes.output[0]
             else:
                 node.input.extend([quantized_input_value.zp_name])  # pad zero_point for original zero
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/pooling.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/pooling.py`

 * *Files 2% similar despite different names*

```diff
@@ -8,20 +8,21 @@
 
     def quantize(self):
         node = self.node
 
         # only try to quantize when given quantization parameters for it
         data_found, output_scale_name, output_zp_name, _, _ = \
             self.quantizer._get_quantization_params(node.output[0])
-        if (not data_found):
-            return super().quantize()
 
         # get quantized input tensor names, quantize input if needed
         quantized_input_names, input_zero_point_names, input_scale_names, nodes = self.quantizer.quantize_inputs(node, [0])
 
+        if not data_found or quantized_input_names is None:
+            return super().quantize()
+
         # Create an entry for output quantized value.
         qlinear_output_name = node.output[0] + "_quantized"
         quantized_output_value = QuantizedValue(
             node.output[0], qlinear_output_name, output_scale_name, output_zp_name, QuantizedValueType.Input)
         self.quantizer.quantized_value_map[node.output[0]] = quantized_output_value
 
         # Create qlinear pool node for given type (AveragePool, etc)
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/resize.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/resize.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/split.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/split.py`

 * *Files 12% similar despite different names*

```diff
@@ -7,14 +7,17 @@
 class QSplit(QuantOperatorBase):
     def __init__(self, onnx_quantizer, onnx_node):
         super().__init__(onnx_quantizer, onnx_node)
 
     def quantize(self):
         node = self.node
         quantized_input_names, zero_point_names, scale_names, nodes = self.quantizer.quantize_inputs(node, [0])
+        if quantized_input_names is None:
+            return super().quantize()
+
         quantized_node_name = ""
         if node.name != "":
             quantized_node_name = node.name + "_quant"
         kwargs = {}
         for attribute in node.attribute:
             kwargs.update(attribute_to_kwarg(attribute))
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/tools/convert_onnx_models_to_ort.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/tools/convert_onnx_models_to_ort.py`

 * *Files 18% similar despite different names*

```diff
@@ -17,46 +17,52 @@
 
 
 def _onnx_model_path_to_ort_model_path(onnx_model_path: pathlib.Path, optimization_level_str: str):
     assert onnx_model_path.is_file() and _path_match_suffix_ignore_case(onnx_model_path, ".onnx")
     return onnx_model_path.with_suffix(".{}.ort".format(optimization_level_str))
 
 
-def _create_config_file_from_ort_models(onnx_model_path_or_dir: pathlib.Path, enable_type_reduction: bool):
+def _create_config_file_from_ort_models(onnx_model_path_or_dir: pathlib.Path, optimization_level_str: str,
+                                        enable_type_reduction: bool):
     if onnx_model_path_or_dir.is_dir():
         # model directory
         model_path_or_dir = onnx_model_path_or_dir
         config_path = None  # default path in model directory
     else:
         # single model
-        model_path_or_dir = _onnx_model_path_to_ort_model_path(onnx_model_path_or_dir)
+        model_path_or_dir = _onnx_model_path_to_ort_model_path(onnx_model_path_or_dir, optimization_level_str)
         config_suffix = ".{}".format(
             'required_operators_and_types.config' if enable_type_reduction else 'required_operators.config')
         config_path = model_path_or_dir.with_suffix(config_suffix)
 
     create_config_from_models(model_path_or_dir=str(model_path_or_dir),
                               output_file=str(config_path) if config_path is not None else None,
                               enable_type_reduction=enable_type_reduction)
 
 
 def _create_session_options(optimization_level: ort.GraphOptimizationLevel,
                             output_model_path: pathlib.Path,
-                            custom_op_library: pathlib.Path):
+                            custom_op_library: pathlib.Path,
+                            session_options_config_entries: typing.Dict[str, str]):
     so = ort.SessionOptions()
     so.optimized_model_filepath = str(output_model_path)
     so.graph_optimization_level = optimization_level
 
     if custom_op_library:
         so.register_custom_ops_library(str(custom_op_library))
 
+    for key, value in session_options_config_entries.items():
+        so.add_session_config_entry(key, value)
+
     return so
 
 
-def _convert(model_path_or_dir: pathlib.Path, optimization_level_str: str, use_nnapi: bool,
-             custom_op_library: pathlib.Path, create_optimized_onnx_model: bool):
+def _convert(model_path_or_dir: pathlib.Path, optimization_level_str: str, use_nnapi: bool, use_coreml: bool,
+             custom_op_library: pathlib.Path, create_optimized_onnx_model: bool, allow_conversion_failures: bool,
+             session_options_config_entries: typing.Dict[str, str]):
 
     optimization_level = _get_optimization_level(optimization_level_str)
 
     models = []
     if model_path_or_dir.is_file() and _path_match_suffix_ignore_case(model_path_or_dir, ".onnx"):
         models.append(model_path_or_dir)
     elif model_path_or_dir.is_dir():
@@ -68,14 +74,17 @@
     if len(models) == 0:
         raise ValueError("No .onnx files were found in '{}'".format(model_path_or_dir))
 
     providers = ['CPUExecutionProvider']
     if use_nnapi:
         # providers are priority based, so register NNAPI first
         providers.insert(0, 'NnapiExecutionProvider')
+    if use_coreml:
+        # providers are priority based, so register CoreML first
+        providers.insert(0, 'CoreMLExecutionProvider')
 
     # if the optimization level is 'all' we manually exclude the NCHWc transformer. It's not applicable to ARM
     # devices, and creates a device specific model which won't run on all hardware.
     # If someone really really really wants to run it they could manually create an optimized onnx model first,
     # or they could comment out this code.
     optimizer_filter = None
     if optimization_level == ort.GraphOptimizationLevel.ORT_ENABLE_ALL:
@@ -95,34 +104,38 @@
             # create .ort file in same dir as original onnx model
             ort_target_path = _onnx_model_path_to_ort_model_path(model, optimization_level_str)
 
             if create_optimized_onnx_model:
                 # Create an ONNX file with the same optimizations that will be used for the ORT format file.
                 # This allows the ONNX equivalent of the ORT format model to be easily viewed in Netron.
                 optimized_target_path = model.with_suffix(".{}.optimized.onnx".format(optimization_level_str))
-                so = _create_session_options(optimization_level, optimized_target_path, custom_op_library)
+                so = _create_session_options(optimization_level, optimized_target_path, custom_op_library,
+                                             session_options_config_entries)
 
                 print("Saving optimized ONNX model {} to {}".format(model, optimized_target_path))
                 _ = ort.InferenceSession(str(model), sess_options=so, providers=providers,
                                          disabled_optimizers=optimizer_filter)
 
             # Load ONNX model, optimize, and save to ORT format
-            so = _create_session_options(optimization_level, ort_target_path, custom_op_library)
+            so = _create_session_options(optimization_level, ort_target_path, custom_op_library,
+                                         session_options_config_entries)
             so.add_session_config_entry('session.save_model_format', 'ORT')
 
             print("Converting optimized ONNX model {} to ORT format model {}".format(model, ort_target_path))
             _ = ort.InferenceSession(str(model), sess_options=so, providers=providers,
                                      disabled_optimizers=optimizer_filter)
 
             # orig_size = os.path.getsize(onnx_target_path)
             # new_size = os.path.getsize(ort_target_path)
             # print("Serialized {} to {}. Sizes: orig={} new={} diff={} new:old={:.4f}:1.0".format(
             #     onnx_target_path, ort_target_path, orig_size, new_size, new_size - orig_size, new_size / orig_size))
         except Exception as e:
             print("Error converting {}: {}".format(model, e))
+            if not allow_conversion_failures:
+                raise
             num_failures += 1
 
     print("Converted {} models. {} failures.".format(len(models), num_failures))
 
 
 def _get_optimization_level(level):
     if level == 'disable':
@@ -153,14 +166,20 @@
 
     parser.add_argument('--use_nnapi', action='store_true',
                         help='Enable the NNAPI Execution Provider when creating models and determining required '
                              'operators. Note that this will limit the optimizations possible on nodes that the '
                              'NNAPI execution provider takes, in order to preserve those nodes in the ORT format '
                              'model.')
 
+    parser.add_argument('--use_coreml', action='store_true',
+                        help='Enable the CoreML Execution Provider when creating models and determining required '
+                             'operators. Note that this will limit the optimizations possible on nodes that the '
+                             'CoreML execution provider takes, in order to preserve those nodes in the ORT format '
+                             'model.')
+
     parser.add_argument('--optimization_level', default='all',
                         choices=['disable', 'basic', 'extended', 'all'],
                         help="Level to optimize ONNX model with, prior to converting to ORT format model. "
                              "These map to the onnxruntime.GraphOptimizationLevel values. "
                              "If the level is 'all' the NCHWc transformer is manually disabled as it contains device "
                              "specific logic, so the ORT format model must be generated on the device it will run on. "
                              "Additionally, the NCHWc optimizations are not applicable to ARM devices."
@@ -173,14 +192,22 @@
     parser.add_argument('--custom_op_library', type=pathlib.Path, default=None,
                         help='Provide path to shared library containing custom operator kernels to register.')
 
     parser.add_argument('--save_optimized_onnx_model', action='store_true',
                         help='Save the optimized version of each ONNX model. '
                              'This will have the same optimizations applied as the ORT format model.')
 
+    parser.add_argument('--allow_conversion_failures', action='store_true',
+                        help='Whether to proceed after encountering model conversion failures.')
+
+    parser.add_argument('--nnapi_partitioning_stop_ops',
+                        help='Specify the list of NNAPI EP partitioning stop ops. '
+                             'In particular, specify the value of the "ep.nnapi.partitioning_stop_ops" session '
+                             'options config entry.')
+
     parser.add_argument('model_path_or_dir', type=pathlib.Path,
                         help='Provide path to ONNX model or directory containing ONNX model/s to convert. '
                              'All files with a .onnx extension, including in subdirectories, will be processed.')
 
     return parser.parse_args()
 
 
@@ -195,15 +222,23 @@
 
     if custom_op_library and not custom_op_library.is_file():
         raise FileNotFoundError("Unable to find custom operator library '{}'".format(custom_op_library))
 
     if args.use_nnapi and 'NnapiExecutionProvider' not in ort.get_available_providers():
         raise ValueError('The NNAPI Execution Provider was not included in this build of ONNX Runtime.')
 
-    _convert(model_path_or_dir, args.optimization_level, args.use_nnapi, custom_op_library,
-             args.save_optimized_onnx_model)
+    if args.use_coreml and 'CoreMLExecutionProvider' not in ort.get_available_providers():
+        raise ValueError('The CoreML Execution Provider was not included in this build of ONNX Runtime.')
+
+    session_options_config_entries = {}
+
+    if args.nnapi_partitioning_stop_ops is not None:
+        session_options_config_entries["ep.nnapi.partitioning_stop_ops"] = args.nnapi_partitioning_stop_ops
+
+    _convert(model_path_or_dir, args.optimization_level, args.use_nnapi, args.use_coreml, custom_op_library,
+             args.save_optimized_onnx_model, args.allow_conversion_failures, session_options_config_entries)
 
-    _create_config_file_from_ort_models(model_path_or_dir, args.enable_type_reduction)
+    _create_config_file_from_ort_models(model_path_or_dir, args.optimization_level, args.enable_type_reduction)
 
 
 if __name__ == '__main__':
     convert_onnx_models_to_ort()
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/tools/onnxruntime_test.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/tools/onnxruntime_test.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/tools/symbolic_shape_infer.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/tools/symbolic_shape_infer.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,36 +1,63 @@
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 
 # -*- coding: UTF-8 -*-
 import argparse
 import numpy as np
 import onnx
-import sys
 from onnx import helper, numpy_helper, shape_inference
 import sympy
-import json
 
 from packaging import version
-assert version.parse(onnx.__version__) >= version.parse("1.5.0")
+assert version.parse(onnx.__version__) >= version.parse("1.8.0")
 
 
 def get_attribute(node, attr_name, default_value=None):
     found = [attr for attr in node.attribute if attr.name == attr_name]
     if found:
         return helper.get_attribute_value(found[0])
     return default_value
 
 
-def get_dim_from_type_proto(dim):
+def get_dim_from_proto(dim):
     return getattr(dim, dim.WhichOneof('value')) if type(dim.WhichOneof('value')) == str else None
 
 
+def is_sequence(type_proto):
+    cls_type = type_proto.WhichOneof('value')
+    assert cls_type in ['tensor_type', 'sequence_type']
+    return cls_type == 'sequence_type'
+
+
 def get_shape_from_type_proto(type_proto):
-    return [get_dim_from_type_proto(d) for d in type_proto.tensor_type.shape.dim]
+    assert not is_sequence(type_proto)
+    if type_proto.tensor_type.HasField('shape'):
+        return [get_dim_from_proto(d) for d in type_proto.tensor_type.shape.dim]
+    else:
+        return None  # note no shape is different from shape without dim (scalar)
+
+
+def get_shape_from_value_info(vi):
+    cls_type = vi.type.WhichOneof('value')
+    if cls_type is None:
+        return None
+    if is_sequence(vi.type):
+        if 'tensor_type' == vi.type.sequence_type.elem_type.WhichOneof('value'):
+            return get_shape_from_type_proto(vi.type.sequence_type.elem_type)
+        else:
+            return None
+    else:
+        return get_shape_from_type_proto(vi.type)
+
+
+def make_named_value_info(name):
+    vi = onnx.ValueInfoProto()
+    vi.name = name
+    return vi
 
 
 def get_shape_from_sympy_shape(sympy_shape):
     return [None if i is None else (int(i) if is_literal(i) else str(i)) for i in sympy_shape]
 
 
 def is_literal(dim):
@@ -81,29 +108,31 @@
             value = value * v
     else:
         value = x
     return value
 
 
 class SymbolicShapeInference:
-    def __init__(self, int_max, auto_merge, guess_output_rank, verbose):
+    def __init__(self, int_max, auto_merge, guess_output_rank, verbose, prefix=''):
         self.dispatcher_ = {
             'Add': self._infer_symbolic_compute_ops,
             'ArrayFeatureExtractor': self._infer_ArrayFeatureExtractor,
             'AveragePool': self._infer_Pool,
             'BatchNormalization': self._infer_BatchNormalization,
             'Cast': self._infer_Cast,
             'CategoryMapper': self._infer_CategoryMapper,
             'Compress': self._infer_Compress,
             'Concat': self._infer_Concat,
+            'ConcatFromSequence': self._infer_ConcatFromSequence,
             'Constant': self._infer_Constant,
             'ConstantOfShape': self._infer_ConstantOfShape,
             'Conv': self._infer_Conv,
             'CumSum': self._pass_on_shape_and_type,
             'Div': self._infer_symbolic_compute_ops,
+            'Einsum': self._infer_Einsum,
             'Expand': self._infer_Expand,
             'Equal': self._infer_symbolic_compute_ops,
             'Floor': self._infer_symbolic_compute_ops,
             'Gather': self._infer_Gather,
             'GatherElements': self._infer_GatherElements,
             'GatherND': self._infer_GatherND,
             'Gelu': self._pass_on_shape_and_type,
@@ -116,20 +145,24 @@
             'Min': self._infer_symbolic_compute_ops,
             'Mul': self._infer_symbolic_compute_ops,
             'NonMaxSuppression': self._infer_NonMaxSuppression,
             'NonZero': self._infer_NonZero,
             'OneHot': self._infer_OneHot,
             'Pad': self._infer_Pad,
             'Range': self._infer_Range,
+            'Reciprocal': self._pass_on_shape_and_type,
+            'ReduceSum': self._infer_ReduceSum,
             'ReduceProd': self._infer_ReduceProd,
             'Reshape': self._infer_Reshape,
             'Resize': self._infer_Resize,
             'Round': self._pass_on_shape_and_type,
             'Scan': self._infer_Scan,
             'ScatterElements': self._infer_ScatterElements,
+            'SequenceAt': self._infer_SequenceAt,
+            'SequenceInsert': self._infer_SequenceInsert,
             'Shape': self._infer_Shape,
             'Size': self._infer_Size,
             'Slice': self._infer_Slice,
             'SoftmaxCrossEntropyLoss': self._infer_SoftmaxCrossEntropyLoss,
             'SoftmaxCrossEntropyLossInternal': self._infer_SoftmaxCrossEntropyLoss,
             'NegativeLogLikelihoodLossInternal': self._infer_SoftmaxCrossEntropyLoss,
             'Split': self._infer_Split,
@@ -147,29 +180,36 @@
             'Attention': self._infer_Attention,
             'BiasGelu': self._infer_BiasGelu,
             'EmbedLayerNormalization': self._infer_EmbedLayerNormalization,
             'FastGelu': self._infer_FastGelu,
             'Gelu': self._infer_Gelu,
             'LayerNormalization': self._infer_LayerNormalization,
             'LongformerAttention': self._infer_LongformerAttention,
+            'PythonOp': self._infer_PythonOp,
             'SkipLayerNormalization': self._infer_SkipLayerNormalization
         }
         self.aten_op_dispatcher_ = {
             'aten::embedding': self._infer_Gather,
-            'aten::max_pool2d_with_indices': self._infer_aten_max_pool2d,
+            'aten::diagonal': self._infer_aten_diagonal,
+            'aten::max_pool2d_with_indices': self._infer_aten_pool2d,
             'aten::unfold': self._infer_aten_unfold,
+            'aten::argmax': self._infer_aten_argmax,
+            'aten::avg_pool2d': self._infer_aten_pool2d,
+            'aten::_adaptive_avg_pool2d': self._infer_aten_pool2d,
         }
         self.run_ = True
         self.suggested_merge_ = {}
         self.symbolic_dims_ = {}
         self.input_symbols_ = {}
         self.auto_merge_ = auto_merge
         self.guess_output_rank_ = guess_output_rank
         self.verbose_ = verbose
         self.int_max_ = int_max
+        self.subgraph_id_ = 0
+        self.prefix_ = prefix
 
     def _add_suggested_merge(self, symbols, apply=False):
         assert all([(type(s) == str and s in self.symbolic_dims_) or is_literal(s) for s in symbols])
         symbols = set(symbols)
         for k, v in self.suggested_merge_.items():
             if k in symbols:
                 symbols.remove(k)
@@ -287,15 +327,16 @@
                         print('unsupported broadcast between ' + str(dim1) + ' ' + str(dim2))
             new_shape = [new_dim] + new_shape
         return new_shape
 
     def _get_shape(self, node, idx):
         name = node.input[idx]
         if name in self.known_vi_:
-            return get_shape_from_type_proto(self.known_vi_[name].type)
+            vi = self.known_vi_[name]
+            return get_shape_from_value_info(vi)
         else:
             assert name in self.initializers_
             return list(self.initializers_[name].dims)
 
     def _get_shape_rank(self, node, idx):
         return len(self._get_shape(node, idx))
 
@@ -336,15 +377,23 @@
                     if not str(new_dim) in self.symbolic_dims_:
                         self.symbolic_dims_[str(new_dim)] = new_dim
 
     def _onnx_infer_single_node(self, node):
         # skip onnx shape inference for some ops, as they are handled in _infer_*
         skip_infer = node.op_type in [
             'If', 'Loop', 'Scan', 'SplitToSequence', 'ZipMap', \
-            'Attention', 'BiasGelu', 'EmbedLayerNormalization', 'FastGelu', 'Gelu', 'LayerNormalization', 'LongformerAttention', 'SkipLayerNormalization' # contrib ops
+            # contrib ops
+
+
+            'Attention', 'BiasGelu', \
+            'EmbedLayerNormalization', \
+            'FastGelu', 'Gelu', 'LayerNormalization', \
+            'LongformerAttention', \
+            'SkipLayerNormalization', \
+            'PythonOp'
         ]
 
         if not skip_infer:
             # Only pass initializers that satisfy the following condition:
             # (1) Operator need value of some input for shape inference.
             #     For example, Unsqueeze in opset 13 uses the axes input to calculate shape of output.
             # (2) opset version >= 9. In older version, initializer is required in graph input by onnx spec.
@@ -353,50 +402,54 @@
             if (get_opset(self.out_mp_) >= 9) and node.op_type in ['Unsqueeze']:
                 initializers = [
                     self.initializers_[name] for name in node.input
                     if (name in self.initializers_ and name not in self.graph_inputs_)
                 ]
 
             # run single node inference with self.known_vi_ shapes
-            tmp_graph = helper.make_graph(
-                [node], 'tmp', [self.known_vi_[i] for i in node.input if i],
-                [helper.make_tensor_value_info(i, onnx.TensorProto.UNDEFINED, None) for i in node.output], initializers)
+            tmp_graph = helper.make_graph([node], 'tmp', [self.known_vi_[i] for i in node.input if i],
+                                          [make_named_value_info(i) for i in node.output], initializers)
 
             self.tmp_mp_.graph.CopyFrom(tmp_graph)
             self.tmp_mp_ = shape_inference.infer_shapes(self.tmp_mp_)
 
         for i_o in range(len(node.output)):
             o = node.output[i_o]
             vi = self.out_mp_.graph.value_info.add()
             if not skip_infer:
                 vi.CopyFrom(self.tmp_mp_.graph.output[i_o])
             else:
                 vi.name = o
             self.known_vi_[o] = vi
 
-    def _onnx_infer_subgraph(self, node, subgraph, use_node_input=True):
+    def _onnx_infer_subgraph(self, node, subgraph, use_node_input=True, inc_subgraph_id=True):
         if self.verbose_ > 2:
             print('Inferencing subgraph of node {} with output({}...): {}'.format(node.name, node.output[0],
                                                                                   node.op_type))
         # node inputs are not passed directly to the subgraph
         # it's up to the node dispatcher to prepare subgraph input
         # for example, with Scan/Loop, subgraph input shape would be trimmed from node input shape
         # besides, inputs in subgraph could shadow implicit inputs
         subgraph_inputs = set([i.name for i in list(subgraph.initializer) + list(subgraph.input)])
         subgraph_implicit_input = set([name for name in self.known_vi_.keys() if not name in subgraph_inputs])
-        tmp_graph = helper.make_graph(
-            list(subgraph.node), 'tmp',
-            list(subgraph.input) + [self.known_vi_[i] for i in subgraph_implicit_input],
-            [helper.make_tensor_value_info(i.name, onnx.TensorProto.UNDEFINED, None) for i in subgraph.output])
+        tmp_graph = helper.make_graph(list(subgraph.node), 'tmp',
+                                      list(subgraph.input) + [self.known_vi_[i] for i in subgraph_implicit_input],
+                                      [make_named_value_info(i.name) for i in subgraph.output])
         tmp_graph.initializer.extend([i for i in self.out_mp_.graph.initializer if i.name in subgraph_implicit_input])
         tmp_graph.initializer.extend(subgraph.initializer)
         self.tmp_mp_.graph.CopyFrom(tmp_graph)
 
-        symbolic_shape_inference = SymbolicShapeInference(self.int_max_, self.auto_merge_, self.guess_output_rank_,
-                                                          self.verbose_)
+        symbolic_shape_inference = SymbolicShapeInference(self.int_max_,
+                                                          self.auto_merge_,
+                                                          self.guess_output_rank_,
+                                                          self.verbose_,
+                                                          prefix=self.prefix_ + '_' + str(self.subgraph_id_))
+        if inc_subgraph_id:
+            self.subgraph_id_ += 1
+
         all_shapes_inferred = False
         symbolic_shape_inference._preprocess(self.tmp_mp_)
         symbolic_shape_inference.suggested_merge_ = self.suggested_merge_.copy()
         while symbolic_shape_inference.run_:
             all_shapes_inferred = symbolic_shape_inference._infer_impl(self.sympy_data_.copy())
         symbolic_shape_inference._update_output_from_vi()
         if use_node_input:
@@ -406,15 +459,15 @@
         subgraph.ClearField('output')
         subgraph.output.extend(symbolic_shape_inference.out_mp_.graph.output)
         subgraph.ClearField('value_info')
         subgraph.value_info.extend(symbolic_shape_inference.out_mp_.graph.value_info)
         subgraph.ClearField('node')
         subgraph.node.extend(symbolic_shape_inference.out_mp_.graph.node)
         # for new symbolic dims from subgraph output, add to main graph symbolic dims
-        subgraph_shapes = [get_shape_from_type_proto(o.type) for o in symbolic_shape_inference.out_mp_.graph.output]
+        subgraph_shapes = [get_shape_from_value_info(o) for o in symbolic_shape_inference.out_mp_.graph.output]
         subgraph_new_symbolic_dims = set(
             [d for s in subgraph_shapes if s for d in s if type(d) == str and not d in self.symbolic_dims_])
         new_dims = {}
         for d in subgraph_new_symbolic_dims:
             assert d in symbolic_shape_inference.symbolic_dims_
             new_dims[d] = symbolic_shape_inference.symbolic_dims_[d]
         self.symbolic_dims_.update(new_dims)
@@ -472,23 +525,24 @@
             helper.make_tensor_value_info(node.output[0], self.known_vi_[node.input[0]].type.tensor_type.elem_type,
                                           self._get_shape(node, 0)))
 
     def _new_symbolic_dim(self, prefix, dim):
         new_dim = '{}_d{}'.format(prefix, dim)
         if new_dim in self.suggested_merge_:
             v = self.suggested_merge_[new_dim]
-            new_dim = sympy.Integer(int(v)) if is_literal(v) else v
+            new_symbolic_dim = sympy.Integer(int(v)) if is_literal(v) else v
         else:
-            self.symbolic_dims_[new_dim] = sympy.Symbol(new_dim, integer=True, nonnegative=True)
-        return new_dim
+            new_symbolic_dim = sympy.Symbol(new_dim, integer=True, nonnegative=True)
+            self.symbolic_dims_[new_dim] = new_symbolic_dim
+        return new_symbolic_dim
 
     def _new_symbolic_dim_from_output(self, node, out_idx=0, dim=0):
         return self._new_symbolic_dim(
-            '{}{}_o{}_'.format(node.op_type,
-                               list(self.out_mp_.graph.node).index(node), out_idx), dim)
+            '{}{}_{}_o{}_'.format(node.op_type, self.prefix_,
+                                  list(self.out_mp_.graph.node).index(node), out_idx), dim)
 
     def _new_symbolic_shape(self, rank, node, out_idx=0):
         return [self._new_symbolic_dim_from_output(node, out_idx, i) for i in range(rank)]
 
     def _compute_conv_pool_shape(self, node):
         sympy_shape = self._get_sympy_shape(node, 0)
         if len(node.input) > 1:
@@ -503,15 +557,15 @@
 
         assert len(sympy_shape) == rank + 2
 
         # only need to symbolic shape inference if input has symbolic dims in spatial axes
         is_symbolic_dims = [not is_literal(i) for i in sympy_shape[-rank:]]
 
         if not any(is_symbolic_dims):
-            shape = get_shape_from_type_proto(self.known_vi_[node.output[0]].type)
+            shape = get_shape_from_value_info(self.known_vi_[node.output[0]])
             if len(shape) > 0:
                 assert len(sympy_shape) == len(shape)
                 sympy_shape[-rank:] = [sympy.Integer(d) for d in shape[-rank:]]
                 return sympy_shape
 
         dilations = get_attribute(node, 'dilations', [1] * rank)
         strides = get_attribute(node, 'strides', [1] * rank)
@@ -581,14 +635,35 @@
         self._check_merged_dims([lhs_shape[lhs_reduce_dim], rhs_shape[rhs_reduce_dim]], allow_broadcast=False)
         if output_dtype is None:
             # infer output_dtype from input type when not specified
             output_dtype = self.known_vi_[node.input[0]].type.tensor_type.elem_type
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(helper.make_tensor_value_info(node.output[0], output_dtype, new_shape))
 
+    def _fuse_tensor_type(self, node, out_idx, dst_type, src_type):
+        ''' 
+        update dst_tensor_type to be compatible with src_tensor_type when dimension mismatches
+        '''
+        dst_tensor_type = dst_type.sequence_type.elem_type.tensor_type if is_sequence(
+            dst_type) else dst_type.tensor_type
+        src_tensor_type = src_type.sequence_type.elem_type.tensor_type if is_sequence(
+            src_type) else src_type.tensor_type
+        assert dst_tensor_type.elem_type == src_tensor_type.elem_type
+        if dst_tensor_type.HasField('shape'):
+            for di, ds in enumerate(zip(dst_tensor_type.shape.dim, src_tensor_type.shape.dim)):
+                if ds[0] != ds[1]:
+                    # create a new symbolic dimension for node/out_idx/mismatch dim id in dst_tensor_type for tensor_type
+                    # for sequence_type, clear the dimension
+                    new_dim = onnx.TensorShapeProto.Dimension()
+                    if not is_sequence(dst_type):
+                        new_dim.dim_param = str(self._new_symbolic_dim_from_output(node, out_idx, di))
+                    dst_tensor_type.shape.dim[di].CopyFrom(new_dim)
+        else:
+            dst_tensor_type.CopyFrom(src_tensor_type)
+
     def _infer_ArrayFeatureExtractor(self, node):
         data_shape = self._get_shape(node, 0)
         indices_shape = self._get_shape(node, 1)
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(
             helper.make_tensor_value_info(node.output[0], self.known_vi_[node.input[0]].type.tensor_type.elem_type,
                                           data_shape[:-1] + indices_shape))
@@ -632,15 +707,15 @@
             output_type = onnx.TensorProto.STRING
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(helper.make_tensor_value_info(node.output[0], output_type, self._get_shape(node, 0)))
 
     def _infer_Compress(self, node):
         input_shape = self._get_shape(node, 0)
         # create a new symbolic dimension for Compress output
-        compress_len = self._new_symbolic_dim_from_output(node)
+        compress_len = str(self._new_symbolic_dim_from_output(node))
         axis = get_attribute(node, 'axis')
         if axis == None:
             # when axis is not specified, input is flattened before compress so output is 1D
             output_shape = [compress_len]
         else:
             output_shape = input_shape
             output_shape[handle_negative_axis(axis, len(input_shape))] = compress_len
@@ -682,14 +757,30 @@
             else:
                 sympy_shape[d] = merged
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(
             helper.make_tensor_value_info(node.output[0], self.known_vi_[node.input[0]].type.tensor_type.elem_type,
                                           get_shape_from_sympy_shape(sympy_shape)))
 
+    def _infer_ConcatFromSequence(self, node):
+        seq_shape = self._get_shape(node, 0)
+        new_axis = 1 if get_attribute(node, 'new_axis') else 0
+        axis = handle_negative_axis(get_attribute(node, 'axis'), len(seq_shape) + new_axis)
+        concat_dim = str(self._new_symbolic_dim_from_output(node, 0, axis))
+        new_shape = seq_shape
+        if new_axis:
+            new_shape = seq_shape[:axis] + [concat_dim] + seq_shape[axis:]
+        else:
+            new_shape[axis] = concat_dim
+        vi = self.known_vi_[node.output[0]]
+        vi.CopyFrom(
+            helper.make_tensor_value_info(
+                node.output[0], self.known_vi_[node.input[0]].type.sequence_type.elem_type.tensor_type.elem_type,
+                new_shape))
+
     def _infer_Constant(self, node):
         t = get_attribute(node, 'value')
         self.sympy_data_[node.output[0]] = numpy_helper.to_array(t)
 
     def _infer_ConstantOfShape(self, node):
         sympy_shape = self._get_int_values(node)[0]
         vi = self.known_vi_[node.output[0]]
@@ -715,14 +806,75 @@
         sympy_shape = self._compute_conv_pool_shape(node)
         self._update_computed_dims(sympy_shape)
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(
             helper.make_tensor_value_info(node.output[0], vi.type.tensor_type.elem_type,
                                           get_shape_from_sympy_shape(sympy_shape)))
 
+    def _infer_Einsum(self, node):
+        # ref:https://github.com/onnx/onnx/blob/623dfaa0151b2e4ce49779c3ec31cbd78c592b80/onnx/defs/math/defs.cc#L3275
+        equation = get_attribute(node, 'equation')
+        equation = equation.replace(b' ', b'')
+        mid_index = equation.find(b'->')
+        left_equation = equation[:mid_index] if mid_index != -1 else equation
+
+        num_operands = 0
+        num_ellipsis = 0
+        num_ellipsis_indices = 0
+
+        letter_to_dim = {}
+
+        terms = left_equation.split(b',')
+        for term in terms:
+            ellipsis_index = term.find(b'...')
+            shape = self._get_shape(node, num_operands)
+            rank = len(shape)
+            if ellipsis_index != -1:
+                if num_ellipsis == 0:
+                    num_ellipsis_indices = rank - len(term) + 3
+                num_ellipsis = num_ellipsis + 1
+            for i in range(1, rank + 1):
+                letter = term[-i]
+                if letter != 46: # letter != b'.'
+                    dim = shape[-i]
+                    if letter not in letter_to_dim.keys():
+                        letter_to_dim[letter] = dim
+                    elif type(dim) != sympy.Symbol:
+                        letter_to_dim[letter] = dim
+            num_operands = num_operands + 1
+
+        new_sympy_shape = []
+        from collections import OrderedDict
+        num_letter_occurrences = OrderedDict()
+        if mid_index != -1:
+            right_equation = equation[mid_index + 2:]
+            right_ellipsis_index = right_equation.find(b'...')
+            if right_ellipsis_index != -1:
+                for i in range(num_ellipsis_indices):
+                    new_sympy_shape.append(shape[i])
+            for c in right_equation:
+                if c != 46: # c != b'.'
+                    new_sympy_shape.append(letter_to_dim[c])
+        else:
+            for i in range(num_ellipsis_indices):
+                new_sympy_shape.append(shape[i])
+            for c in left_equation:
+                if c != 44 and c != 46: # c != b',' and c != b'.':
+                    if c in num_letter_occurrences:
+                        num_letter_occurrences[c] = num_letter_occurrences[c] + 1
+                    else:
+                        num_letter_occurrences[c] = 1
+            for key, value in num_letter_occurrences.items():
+                if value == 1:
+                    new_sympy_shape.append(letter_to_dim[key])
+
+        output_dtype = self.known_vi_[node.input[0]].type.tensor_type.elem_type
+        vi = self.known_vi_[node.output[0]]
+        vi.CopyFrom(helper.make_tensor_value_info(node.output[0], output_dtype, new_sympy_shape))
+
     def _infer_Expand(self, node):
         expand_to_shape = as_list(self._try_get_value(node, 1), keep_none=True)
         if expand_to_shape is not None:
             # new_shape's dim can come from shape value
             self._update_computed_dims(expand_to_shape)
             shape = self._get_shape(node, 0)
             new_shape = self._broadcast_shapes(shape, get_shape_from_sympy_shape(expand_to_shape))
@@ -786,60 +938,94 @@
             subgraph_infer = self._onnx_infer_subgraph(node, subgraph, use_node_input=False)
             for i_out in range(len(node.output)):
                 vi = self.known_vi_[node.output[i_out]]
                 if i_sub == 0:
                     vi.CopyFrom(subgraph.output[i_out])
                     vi.name = node.output[i_out]
                 else:
-                    assert all([
-                        d1 == d2 for d1, d2 in zip(vi.type.tensor_type.shape.dim,
-                                                   subgraph.output[i_out].type.tensor_type.shape.dim)
-                    ])
+                    self._fuse_tensor_type(node, i_out, vi.type, subgraph.output[i_out].type)
+
                 # pass on sympy data from subgraph, if cond is constant
-                if cond is not None and i_sub == (0 if cond > 0 else 1):
+                if cond is not None and i_sub == (0 if as_scalar(cond) > 0 else 1):
                     if subgraph.output[i_out].name in subgraph_infer.sympy_data_:
                         self.sympy_data_[vi.name] = subgraph_infer.sympy_data_[subgraph.output[i_out].name]
 
     def _infer_Loop(self, node):
         subgraph = get_attribute(node, 'body')
         assert len(subgraph.input) == len(node.input)
+        num_loop_carried = len(node.input) - 2  # minus the length and initial loop condition
+        # when sequence_type is used as loop carried input
+        # needs to run subgraph infer twice if the tensor shape in sequence contains None
         for i, si in enumerate(subgraph.input):
-            subgraph_name = si.name
+            si_name = si.name
             si.CopyFrom(self.known_vi_[node.input[i]])
-            si.name = subgraph_name
+            si.name = si_name
+
         self._onnx_infer_subgraph(node, subgraph)
+
+        # check subgraph input/output for shape changes in loop carried variables
+        # for tensor_type, create new symbolic dim when changing, i.e., output = Concat(input, a)
+        # for sequence_type, propagate from output to input
+        need_second_infer = False
+        for i_out in range(1, num_loop_carried + 1):
+            so = subgraph.output[i_out]
+            so_shape = get_shape_from_value_info(so)
+            if is_sequence(so.type):
+                if so_shape and None in so_shape:
+                    # copy shape from output to input
+                    # note that loop input is [loop_len, cond, input_0, input_1, ...]
+                    # while loop output is [cond, output_0, output_1, ...]
+                    subgraph.input[i_out + 1].type.sequence_type.elem_type.CopyFrom(so.type.sequence_type.elem_type)
+                    need_second_infer = True
+            else:
+                si = subgraph.input[i_out + 1]
+                si_shape = get_shape_from_value_info(si)
+                for di, dims in enumerate(zip(si_shape, so_shape)):
+                    if dims[0] != dims[1]:
+                        new_dim = onnx.TensorShapeProto.Dimension()
+                        new_dim.dim_param = str(self._new_symbolic_dim_from_output(node, i_out, di))
+                        si.type.tensor_type.shape.dim[di].CopyFrom(new_dim)
+                        so.type.tensor_type.shape.dim[di].CopyFrom(new_dim)
+                        need_second_infer = True
+
+        if need_second_infer:
+            if self.verbose_ > 2:
+                print("Rerun Loop: {}({}...), because of sequence in loop carried variables".format(
+                    node.name, node.output[0]))
+            self._onnx_infer_subgraph(node, subgraph, inc_subgraph_id=False)
+
         # create a new symbolic dimension for iteration dependent dimension
-        loop_iter_dim = self._new_symbolic_dim_from_output(node)
-        num_loop_carried = len(node.input) - 2
+        loop_iter_dim = str(self._new_symbolic_dim_from_output(node))
         for i in range(len(node.output)):
             vi = self.known_vi_[node.output[i]]
             vi.CopyFrom(subgraph.output[i + 1])  # first subgraph output is condition, not in node output
             if i >= num_loop_carried:
+                assert not is_sequence(vi.type)  # TODO: handle loop accumulation in sequence_type
                 subgraph_vi_dim = subgraph.output[i + 1].type.tensor_type.shape.dim
                 vi.type.tensor_type.shape.ClearField('dim')
                 vi_dim = vi.type.tensor_type.shape.dim
                 vi_dim.add().dim_param = loop_iter_dim
                 vi_dim.extend(list(subgraph_vi_dim))
             vi.name = node.output[i]
 
     def _infer_MatMul(self, node):
         self._compute_matmul_shape(node)
 
     def _infer_MatMulInteger(self, node):
         self._compute_matmul_shape(node, onnx.TensorProto.INT32)
 
     def _infer_NonMaxSuppression(self, node):
-        selected = self._new_symbolic_dim_from_output(node)
+        selected = str(self._new_symbolic_dim_from_output(node))
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(helper.make_tensor_value_info(node.output[0], onnx.TensorProto.INT64, [selected, 3]))
 
     def _infer_NonZero(self, node):
         input_rank = self._get_shape_rank(node, 0)
         # create a new symbolic dimension for NonZero output
-        nz_len = self._new_symbolic_dim_from_output(node, 0, 1)
+        nz_len = str(self._new_symbolic_dim_from_output(node, 0, 1))
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(helper.make_tensor_value_info(node.output[0], vi.type.tensor_type.elem_type, [input_rank, nz_len]))
 
     def _infer_OneHot(self, node):
         sympy_shape = self._get_sympy_shape(node, 0)
         depth = self._try_get_value(node, 1)
         axis = get_attribute(node, 'axis', -1)
@@ -883,44 +1069,100 @@
             if not o:
                 continue
             vi = self.known_vi_[o]
             vi.CopyFrom(
                 helper.make_tensor_value_info(o, vi.type.tensor_type.elem_type,
                                               get_shape_from_sympy_shape(sympy_shape)))
 
-    def _infer_aten_max_pool2d(self, node):
+    def _infer_aten_diagonal(self, node):
+        sympy_shape = self._get_sympy_shape(node, 0)
+        rank = len(sympy_shape)
+        offset = self._try_get_value(node, 1)
+        dim1 = self._try_get_value(node, 2)
+        dim2 = self._try_get_value(node, 3)
+
+        assert offset is not None and dim1 is not None and dim2 is not None
+        dim1 = handle_negative_axis(dim1, rank)
+        dim2 = handle_negative_axis(dim2, rank)
+
+        new_shape = []
+        for dim, val in enumerate(sympy_shape):
+            if dim not in [dim1, dim2]:
+                new_shape.append(val)
+
+        shape1 = sympy_shape[dim1]
+        shape2 = sympy_shape[dim2]
+        if offset >= 0:
+            diag_shape = sympy.Max(0, sympy.Min(shape1, shape2 - offset))
+        else:
+            diag_shape = sympy.Max(0, sympy.Min(shape1 + offset, shape2))
+        new_shape.append(diag_shape)
+
+        if node.output[0]:
+            vi = self.known_vi_[node.output[0]]
+            vi.CopyFrom(
+                helper.make_tensor_value_info(node.output[0], self.known_vi_[node.input[0]].type.tensor_type.elem_type,
+                                              get_shape_from_sympy_shape(new_shape)))
+
+    def _infer_aten_pool2d(self, node):
         sympy_shape = self._get_sympy_shape(node, 0)
         assert len(sympy_shape) == 4
         sympy_shape[-2:] = [self._new_symbolic_dim_from_output(node, 0, i) for i in [2, 3]]
         self._update_computed_dims(sympy_shape)
         for i, o in enumerate(node.output):
             if not o:
                 continue
             vi = self.known_vi_[o]
             elem_type = onnx.TensorProto.INT64 if i == 1 else self.known_vi_[node.input[0]].type.tensor_type.elem_type
-            vi.CopyFrom(
-                helper.make_tensor_value_info(o, elem_type,
-                                              get_shape_from_sympy_shape(sympy_shape)))
+            vi.CopyFrom(helper.make_tensor_value_info(o, elem_type, get_shape_from_sympy_shape(sympy_shape)))
 
     def _infer_aten_unfold(self, node):
-        attr_values = json.loads(get_attribute(node, 'custom_attributes_json'))
-        dimension = attr_values['dimension']
-        size = attr_values['size']
-        step = attr_values['step']
         sympy_shape = self._get_sympy_shape(node, 0)
-        assert dimension < len(sympy_shape)
-        sympy_shape[dimension] = (sympy_shape[dimension] - size) // step + 1
-        sympy_shape.append(size)
+        dimension = self._try_get_value(node, 1)
+        size = self._try_get_value(node, 2)
+        step = self._try_get_value(node, 3)
+        if dimension is not None and size is not None and step is not None:
+            assert dimension < len(sympy_shape)
+            sympy_shape[dimension] = (sympy_shape[dimension] - size) // step + 1
+            sympy_shape.append(size)
+        else:
+            rank = len(sympy_shape)
+            sympy_shape = self._new_symbolic_shape(rank + 1, node)
         self._update_computed_dims(sympy_shape)
         if node.output[0]:
             vi = self.known_vi_[node.output[0]]
             vi.CopyFrom(
                 helper.make_tensor_value_info(node.output[0], self.known_vi_[node.input[0]].type.tensor_type.elem_type,
                                               get_shape_from_sympy_shape(sympy_shape)))
 
+    def _infer_aten_argmax(self, node):
+        new_shape = None
+        if node.input[1] == '':
+            # The argmax of the flattened input is returned.
+            new_shape = []
+        else:
+            dim = self._try_get_value(node, 1)
+            keepdim = self._try_get_value(node, 2)
+            if keepdim is not None:
+                sympy_shape = self._get_sympy_shape(node, 0)
+                if dim is not None:
+                    dim = handle_negative_axis(dim, len(sympy_shape))
+                    if keepdim:
+                        sympy_shape[dim] = 1
+                    else:
+                        del sympy_shape[dim]
+                else:
+                    rank = len(sympy_shape)
+                    sympy_shape = self._new_symbolic_shape(rank if keepdim else rank - 1, node)
+                self._update_computed_dims(sympy_shape)
+                new_shape = get_shape_from_sympy_shape(sympy_shape)
+        if node.output[0] and new_shape is not None:
+            vi = self.known_vi_[node.output[0]]
+            vi.CopyFrom(helper.make_tensor_value_info(node.output[0], onnx.TensorProto.INT64, new_shape))
+
     def _infer_BatchNormalization(self, node):
         self._propagate_shape_and_type(node)
 
         # this works for opsets < 14 and 14 since we check i < len(node.output) in the loop
         for i in [1, 2, 3, 4]:
             if i < len(node.output) and node.output[i] != "":
                 # all of these parameters have the same shape as the 1st input
@@ -931,24 +1173,50 @@
         input_data = self._get_int_values(node)
         if all([i is not None for i in input_data]):
             start = as_scalar(input_data[0])
             limit = as_scalar(input_data[1])
             delta = as_scalar(input_data[2])
             new_sympy_shape = [sympy.Max(sympy.ceiling((limit - start) / delta), 0)]
         else:
-            new_dim = self._new_symbolic_dim_from_output(node)
-            new_sympy_shape = [self.symbolic_dims_[new_dim]]
+            new_sympy_shape = [self._new_symbolic_dim_from_output(node)]
         self._update_computed_dims(new_sympy_shape)
         vi.CopyFrom(
             helper.make_tensor_value_info(node.output[0], self.known_vi_[node.input[0]].type.tensor_type.elem_type,
                                           get_shape_from_sympy_shape(new_sympy_shape)))
 
+    def _infer_ReduceSum(self, node):
+        keep_dims = get_attribute(node, 'keepdims', 1)
+        if get_opset(self.out_mp_) >= 13 and len(node.input) > 1:
+            # ReduceSum changes axes to input[1] in opset 13
+            axes = self._try_get_value(node, 1)
+            vi = self.known_vi_[node.output[0]]
+            if axes is None:
+                assert keep_dims  # can only handle keep_dims==True when axes is unknown, by generating new ranks
+                vi.CopyFrom(
+                    helper.make_tensor_value_info(
+                        node.output[0], self.known_vi_[node.input[0]].type.tensor_type.elem_type,
+                        get_shape_from_sympy_shape(self._new_symbolic_shape(self._get_shape_rank(node, 0), node))))
+            else:
+                shape = self._get_shape(node, 0)
+                output_shape = []
+                axes = [handle_negative_axis(a, len(shape)) for a in axes]
+                for i, d in enumerate(shape):
+                    if i in axes:
+                        if keep_dims:
+                            output_shape.append(1)
+                    else:
+                        output_shape.append(d)
+                vi.CopyFrom(
+                    helper.make_tensor_value_info(node.output[0],
+                                                  self.known_vi_[node.input[0]].type.tensor_type.elem_type,
+                                                  output_shape))
+
     def _infer_ReduceProd(self, node):
         axes = get_attribute(node, 'axes')
-        keep_dims = get_attribute(node, 'keepdims')
+        keep_dims = get_attribute(node, 'keepdims', 1)
         if keep_dims == 0 and axes == [0]:
             data = self._get_int_values(node)[0]
             if data is not None:
                 self.sympy_data_[node.output[0]] = sympy_reduce_product(data)
 
     def _infer_Reshape(self, node):
         shape_value = self._try_get_value(node, 1)
@@ -1073,14 +1341,35 @@
     def _infer_ScatterElements(self, node):
         data_shape = self._get_shape(node, 0)
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(
             helper.make_tensor_value_info(node.output[0], self.known_vi_[node.input[0]].type.tensor_type.elem_type,
                                           data_shape))
 
+    def _infer_SequenceAt(self, node):
+        # need to create new symbolic dimension if sequence shape has None:
+        seq_shape = self._get_shape(node, 0)
+        vi = self.known_vi_[node.output[0]]
+        if seq_shape is not None:
+            for di, d in enumerate(seq_shape):
+                if d is not None:
+                    continue
+                new_dim = onnx.TensorShapeProto.Dimension()
+                new_dim.dim_param = str(self._new_symbolic_dim_from_output(node, 0, di))
+                vi.type.tensor_type.shape.dim[di].CopyFrom(new_dim)
+
+    def _infer_SequenceInsert(self, node):
+        # workaround bug in onnx's shape inference
+        vi_seq = self.known_vi_[node.input[0]]
+        vi_tensor = self.known_vi_[node.input[1]]
+        vi_out_seq = self.known_vi_[node.output[0]]
+        vi_out_seq.CopyFrom(vi_seq)
+        vi_out_seq.name = node.output[0]
+        self._fuse_tensor_type(node, 0, vi_out_seq.type, vi_tensor.type)
+
     def _infer_Shape(self, node):
         self.sympy_data_[node.output[0]] = self._get_sympy_shape(node, 0)
 
     def _infer_Size(self, node):
         sympy_shape = self._get_sympy_shape(node, 0)
         self.sympy_data_[node.output[0]] = sympy_reduce_product(sympy_shape)
         self.known_vi_[node.output[0]].CopyFrom(
@@ -1194,14 +1483,15 @@
                                                   and len(input_sympy_data.shape) == 1):
                 self.sympy_data_[node.output[0]] = input_sympy_data[starts[0]:ends[0]:steps[0]]
 
     def _infer_SoftmaxCrossEntropyLoss(self, node):
         vi = self.known_vi_[node.output[0]]
         elem_type = self.known_vi_[node.input[0]].type.tensor_type.elem_type
         vi.type.tensor_type.elem_type = elem_type
+        vi.type.tensor_type.shape.CopyFrom(onnx.TensorShapeProto())
 
         if len(node.output) > 1:
             data_shape = self._get_shape(node, 0)
             vi = self.known_vi_[node.output[1]]
             vi.CopyFrom(helper.make_tensor_value_info(vi.name, elem_type, data_shape))
 
     def _infer_Split_Common(self, node, make_value_info_func):
@@ -1266,21 +1556,24 @@
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(
             helper.make_tensor_value_info(node.output[0], self.known_vi_[node.input[0]].type.tensor_type.elem_type,
                                           output_shape))
         self._pass_on_sympy_data(node)
 
     def _infer_Tile(self, node):
-        repeats_value = self._get_value(node, 1)
-        input_sympy_shape = self._get_sympy_shape(node, 0)
+        repeats_value = self._try_get_value(node, 1)
         new_sympy_shape = []
-        for i, d in enumerate(input_sympy_shape):
-            new_dim = d * repeats_value[i]
-            new_sympy_shape.append(new_dim)
-        self._update_computed_dims(new_sympy_shape)
+        if repeats_value is not None:
+            input_sympy_shape = self._get_sympy_shape(node, 0)
+            for i, d in enumerate(input_sympy_shape):
+                new_dim = d * repeats_value[i]
+                new_sympy_shape.append(new_dim)
+            self._update_computed_dims(new_sympy_shape)
+        else:
+            new_sympy_shape = self._new_symbolic_shape(self._get_shape_rank(node, 0), node)
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(
             helper.make_tensor_value_info(node.output[0], vi.type.tensor_type.elem_type,
                                           get_shape_from_sympy_shape(new_sympy_shape)))
 
     def _infer_TopK(self, node):
         rank = self._get_shape_rank(node, 0)
@@ -1316,14 +1609,42 @@
             data_shape = self._get_shape(node, 0)
             perm = get_attribute(node, 'perm', reversed(list(range(len(data_shape)))))
             input_data = self.sympy_data_[node.input[0]]
             self.sympy_data_[node.output[0]] = np.transpose(np.array(input_data).reshape(*data_shape),
                                                             axes=tuple(perm)).flatten().tolist()
 
     def _infer_Unsqueeze(self, node):
+        input_shape = self._get_shape(node, 0)
+        op_set = get_opset(self.out_mp_)
+
+        # Depending on op-version 'axes' are provided as attribute or via 2nd input
+        if op_set < 13:
+            axes = get_attribute(node, 'axes')
+            assert self._try_get_value(node, 1) is None
+        else:
+            axes = self._try_get_value(node, 1)
+            assert get_attribute(node, 'axes') is None
+
+        output_rank = len(input_shape) + len(axes)
+        axes = [handle_negative_axis(a, output_rank) for a in axes]
+
+        input_axis = 0
+        output_shape = []
+        for i in range(output_rank):
+            if i in axes:
+                output_shape.append(1)
+            else:
+                output_shape.append(input_shape[input_axis])
+                input_axis += 1
+
+        vi = self.known_vi_[node.output[0]]
+        vi.CopyFrom(
+            helper.make_tensor_value_info(node.output[0], self.known_vi_[node.input[0]].type.tensor_type.elem_type,
+                                          output_shape))
+
         self._pass_on_sympy_data(node)
 
     def _infer_ZipMap(self, node):
         map_key_type = None
         if get_attribute(node, 'classlabels_int64s') is not None:
             map_key_type = onnx.TensorProto.INT64
         elif get_attribute(node, 'classlabels_strings') is not None:
@@ -1334,23 +1655,38 @@
         new_vi.name = node.output[0]
         new_vi.type.sequence_type.elem_type.map_type.value_type.tensor_type.elem_type = onnx.TensorProto.FLOAT
         new_vi.type.sequence_type.elem_type.map_type.key_type = map_key_type
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(new_vi)
 
     def _infer_Attention(self, node):
-        #TODO: shape inference for the other output (present).
         shape = self._get_shape(node, 0)
         shape_bias = self._get_shape(node, 2)
         assert len(shape) == 3 and len(shape_bias) == 1
-        shape[2] = int(shape_bias[0] / 3)
+        qkv_hidden_sizes_attr = get_attribute(node, 'qkv_hidden_sizes')
+        if qkv_hidden_sizes_attr is not None:
+            assert len(qkv_hidden_sizes_attr) == 3
+            shape[2] = int(qkv_hidden_sizes_attr[2])
+        else:
+            shape[2] = int(shape_bias[0] / 3)
         output_dtype = self.known_vi_[node.input[0]].type.tensor_type.elem_type
         vi = self.known_vi_[node.output[0]]
         vi.CopyFrom(helper.make_tensor_value_info(node.output[0], output_dtype, shape))
 
+        if len(node.output) > 1:
+            # past shape: (2, batch_size, num_heads, past_sequence_length, head_size)
+            # mask shape: (batch_size, total_sequence_length) or (batch_size, sequence_length, total_sequence_length)
+            # present shape: (2, batch_size, num_heads, total_sequence_length, head_size)
+            past_shape = self._get_shape(node, 4)
+            mask_shape = self._get_shape(node, 3)
+            if len(past_shape) == 5 and len(mask_shape) in [2, 3]:
+                past_shape[3] = mask_shape[-1]
+                vi = self.known_vi_[node.output[1]]
+                vi.CopyFrom(helper.make_tensor_value_info(vi.name, output_dtype, past_shape))
+
     def _infer_BiasGelu(self, node):
         self._propagate_shape_and_type(node)
 
     def _infer_FastGelu(self, node):
         self._propagate_shape_and_type(node)
 
     def _infer_Gelu(self, node):
@@ -1364,43 +1700,73 @@
 
     def _infer_EmbedLayerNormalization(self, node):
         input_ids_shape = self._get_shape(node, 0)
         word_embedding_shape = self._get_shape(node, 2)
         assert len(input_ids_shape) == 2 and len(word_embedding_shape) == 2
         output_shape = input_ids_shape + [word_embedding_shape[1]]
 
-        input_dtype = self.known_vi_[node.input[0]].type.tensor_type.elem_type
+        word_embedding_dtype = self.known_vi_[node.input[2]].type.tensor_type.elem_type
         vi = self.known_vi_[node.output[0]]
-        vi.CopyFrom(helper.make_tensor_value_info(node.output[0], input_dtype, output_shape))
+        vi.CopyFrom(helper.make_tensor_value_info(node.output[0], word_embedding_dtype, output_shape))
 
         mask_index_shape = [input_ids_shape[0]]
         vi = self.known_vi_[node.output[1]]
         vi.CopyFrom(helper.make_tensor_value_info(node.output[1], onnx.TensorProto.INT32, mask_index_shape))
 
     def _infer_SkipLayerNormalization(self, node):
         self._propagate_shape_and_type(node)
 
+    def _infer_PythonOp(self, node):
+        output_tensor_types = get_attribute(node, 'output_tensor_types')
+        assert output_tensor_types
+        output_tensor_ranks = get_attribute(node, 'output_tensor_ranks')
+        assert output_tensor_ranks
+
+        # set the context output seperately.
+        # The first output is autograd's context.
+        vi = self.known_vi_[node.output[0]]
+        vi.CopyFrom(helper.make_tensor_value_info(node.output[0], onnx.TensorProto.INT64, []))
+
+        # Outputs after autograd's context are tensors.
+        # We assume their ranks are fixed for different model inputs.
+        for i in range(len(node.output) - 1):
+            # Process the i-th tensor outputs.
+            vi = self.known_vi_[node.output[i + 1]]
+            sympy_shape = self._new_symbolic_shape(output_tensor_ranks[i], node)
+            shape = get_shape_from_sympy_shape(sympy_shape)
+            value_info = helper.make_tensor_value_info(node.output[i + 1], output_tensor_types[i], shape)
+            vi.CopyFrom(value_info)
+
     def _propagate_shape_and_type(self, node, input_index=0, output_index=0):
         shape = self._get_shape(node, input_index)
         output_dtype = self.known_vi_[node.input[input_index]].type.tensor_type.elem_type
         vi = self.known_vi_[node.output[output_index]]
         vi.CopyFrom(helper.make_tensor_value_info(node.output[output_index], output_dtype, shape))
 
     def _infer_impl(self, start_sympy_data=None):
         self.sympy_data_ = start_sympy_data or {}
         self.out_mp_.graph.ClearField('value_info')
         self._apply_suggested_merge(graph_input_only=True)
         self.input_symbols_ = set()
         for i in self.out_mp_.graph.input:
-            input_dims = i.type.tensor_type.shape.dim
-            for i_dim in range(len(input_dims)):
-                if get_dim_from_type_proto(input_dims[i_dim]) is None:
+            input_shape = get_shape_from_value_info(i)
+            if input_shape is None:
+                continue
+
+            if is_sequence(i.type):
+                input_dims = i.type.sequence_type.elem_type.tensor_type.shape.dim
+            else:
+                input_dims = i.type.tensor_type.shape.dim
+
+            for i_dim, dim in enumerate(input_shape):
+                if dim is None:
                     # some models use None for symbolic dim in input, replace it with a string
-                    input_dims[i_dim].dim_param = self._new_symbolic_dim(i.name, i_dim)
-            self.input_symbols_.update([d for d in get_shape_from_type_proto(i.type) if type(d) == str])
+                    input_dims[i_dim].dim_param = str(self._new_symbolic_dim(i.name, i_dim))
+
+            self.input_symbols_.update([d for d in input_shape if type(d) == str])
 
         for s in self.input_symbols_:
             if s in self.suggested_merge_:
                 s_merge = self.suggested_merge_[s]
                 assert s_merge in self.symbolic_dims_
                 self.symbolic_dims_[s] = self.symbolic_dims_[s_merge]
             else:
@@ -1409,25 +1775,54 @@
         # create a temporary ModelProto for single node inference
         # note that we remove initializer to have faster inference
         # for tensor ops like Reshape/Tile/Expand that read initializer, we need to do sympy computation based inference anyways
         self.tmp_mp_ = onnx.ModelProto()
         self.tmp_mp_.CopyFrom(self.out_mp_)
         self.tmp_mp_.graph.ClearField('initializer')
 
+        # compute prerequesite for node for topological sort
+        # node with subgraphs may have dependency on implicit inputs, which will affect topological sort
+        prereq_for_node = {}  # map from node to all its inputs, including implicit ones in subgraph
+
+        def get_prereq(node):
+            names = set(i for i in node.input if i)
+            subgraphs = []
+            if 'If' == node.op_type:
+                subgraphs = [get_attribute(node, 'then_branch'), get_attribute(node, 'else_branch')]
+            elif node.op_type in ['Loop', 'Scan']:
+                subgraphs = [get_attribute(node, 'body')]
+            for g in subgraphs:
+                g_outputs_and_initializers = {i.name for i in g.initializer}
+                g_prereq = set()
+                for n in g.node:
+                    g_outputs_and_initializers.update(n.output)
+                for n in g.node:
+                    g_prereq.update([i for i in get_prereq(n) if i not in g_outputs_and_initializers])
+                names.update(g_prereq)
+                # remove subgraph inputs from g_prereq since those are local-only
+                for i in g.input:
+                    if i.name in names:
+                        names.remove(i.name)
+            return names
+
+        for n in self.tmp_mp_.graph.node:
+            prereq_for_node[n.output[0]] = get_prereq(n)
+
         # topological sort nodes, note there might be dead nodes so we check if all graph outputs are reached to terminate
         sorted_nodes = []
         sorted_known_vi = set([i.name for i in list(self.out_mp_.graph.input) + list(self.out_mp_.graph.initializer)])
-        if all([o.name in sorted_known_vi for o in self.out_mp_.graph.output]):
-            # Loop/Scan will have all graph output in graph inputs, so don't do topological sort
+        if any([o.name in sorted_known_vi for o in self.out_mp_.graph.output]):
+            # Loop/Scan will have some graph output in graph inputs, so don't do topological sort
             sorted_nodes = self.out_mp_.graph.node
         else:
             while not all([o.name in sorted_known_vi for o in self.out_mp_.graph.output]):
                 old_sorted_nodes_len = len(sorted_nodes)
                 for node in self.out_mp_.graph.node:
-                    if (node.output[0] not in sorted_known_vi) and all([i in sorted_known_vi for i in node.input if i]):
+                    if (node.output[0] not in sorted_known_vi) and all(
+                        [i in sorted_known_vi for i in prereq_for_node[node.output[0]] if i]):
                         sorted_known_vi.update(node.output)
                         sorted_nodes.append(node)
                 if old_sorted_nodes_len == len(sorted_nodes) and not all(
                     [o.name in sorted_known_vi for o in self.out_mp_.graph.output]):
                     raise Exception('Invalid model with cyclic graph')
 
         for node in sorted_nodes:
@@ -1470,29 +1865,44 @@
                     if len(in_dims) > 1:
                         self._check_merged_dims(in_dims, allow_broadcast=True)
 
             for i_o in range(len(node.output)):
                 vi = self.known_vi_[node.output[i_o]]
                 out_type = vi.type
                 out_type_kind = out_type.WhichOneof('value')
-                # only TensorProto and SparseTensorProto have shape
-                if out_type_kind != 'tensor_type' and out_type_kind != 'sparse_tensor_type':
+
+                # do not process shape for non-tensors
+                if out_type_kind not in ['tensor_type', 'sparse_tensor_type', None]:
+                    if self.verbose_ > 2:
+                        if out_type_kind == 'sequence_type':
+                            seq_cls_type = out_type.sequence_type.elem_type.WhichOneof('value')
+                            if 'tensor_type' == seq_cls_type:
+                                print('  {}: sequence of {} {}'.format(
+                                    node.output[i_o], str(get_shape_from_value_info(vi)),
+                                    onnx.TensorProto.DataType.Name(
+                                        vi.type.sequence_type.elem_type.tensor_type.elem_type)))
+                            else:
+                                print('  {}: sequence of {}'.format(node.output[i_o], seq_cls_type))
+                        else:
+                            print('  {}: {}'.format(node.output[i_o], out_type_kind))
                     continue
-                out_shape = get_shape_from_type_proto(vi.type)
+
+                out_shape = get_shape_from_value_info(vi)
                 out_type_undefined = out_type.tensor_type.elem_type == onnx.TensorProto.UNDEFINED
                 if self.verbose_ > 2:
-                    print('  {}: {} {}'.format(node.output[i_o], str(out_shape), vi.type.tensor_type.elem_type))
+                    print('  {}: {} {}'.format(node.output[i_o], str(out_shape),
+                                               onnx.TensorProto.DataType.Name(vi.type.tensor_type.elem_type)))
                     if node.output[i_o] in self.sympy_data_:
                         print('  Sympy Data: ' + str(self.sympy_data_[node.output[i_o]]))
 
-                if None in out_shape or out_type_undefined:
+                if (out_shape is not None and None in out_shape) or out_type_undefined:
                     if self.auto_merge_:
                         if node.op_type in [
                                 'Add', 'Sub', 'Mul', 'Div', 'MatMul', 'MatMulInteger', 'MatMulInteger16', 'Concat',
-                                'Where', 'Sum'
+                                'Where', 'Sum', 'Equal', 'Less', 'Greater', 'LessOrEqual', 'GreaterOrEqual'
                         ]:
                             shapes = [self._get_shape(node, i) for i in range(len(node.input))]
                             if node.op_type in ['MatMul', 'MatMulInteger', 'MatMulInteger16']:
                                 if None in out_shape:
                                     idx = out_shape.index(None)
                                     dim_idx = [len(s) - len(out_shape) + idx for s in shapes]
                                     # only support auto merge for MatMul for dim < rank-2 when rank > 2
@@ -1520,15 +1930,15 @@
                         else:
                             self.run_ = False
                     else:
                         self.run_ = False
 
                     # create new dynamic dims for ops not handled by symbolic shape inference
                     if self.run_ == False and not node.op_type in self.dispatcher_ and not known_aten_op:
-                        is_unknown_op = (out_type_undefined and len(out_shape) == 0)
+                        is_unknown_op = out_type_undefined and (out_shape is None or len(out_shape) == 0)
                         if is_unknown_op:
                             # unknown op to ONNX, maybe from higher opset or other domain
                             # only guess the output rank from input 0 when using guess_output_rank option
                             out_rank = self._get_shape_rank(node, 0) if self.guess_output_rank_ else -1
                         else:
                             # valid ONNX op, but not handled by symbolic shape inference, just assign dynamic shape
                             out_rank = len(out_shape)
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/__init__.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/__init__.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/operator_type_usage_processors.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/operator_type_usage_processors.py`

 * *Files 2% similar despite different names*

```diff
@@ -236,14 +236,27 @@
                 self._input_types[int(i_str)] = set(values)
 
         if 'outputs' in aggregate_info:
             for o_str, values in aggregate_info['outputs'].items():
                 self._output_types[int(o_str)] = set(values)
 
 
+class Input1TypedRegistrationProcessor(DefaultTypeUsageProcessor):
+    '''
+    Processor for operators where the second input type is used in a typed kernel registration.
+    '''
+    def __init__(self, domain: str, optype: str):
+        # init with tracking of input 1 only.
+        super().__init__(domain, optype, inputs=[1], outputs=[])
+
+    def is_typed_registration_needed(self, type_in_registration: str,
+                                     globally_allowed_types: typing.Optional[typing.Set[str]]):
+        return self.is_input_type_enabled(type_in_registration, 1, globally_allowed_types)
+
+
 class Output0TypedRegistrationProcessor(DefaultTypeUsageProcessor):
     '''
     Processor for operators where the first output type is used in a typed kernel registration.
     '''
     def __init__(self, domain: str, optype: str):
         # init with tracking of output 0 only.
         super().__init__(domain, optype, inputs=[], outputs=[0])
@@ -335,19 +348,19 @@
                                   'Pad',
                                   'Range', 'Reciprocal', 'ReduceL1', 'ReduceL2', 'ReduceLogSum', 'ReduceLogSumExp',
                                   'ReduceMax', 'ReduceMean', 'ReduceMin', 'ReduceProd', 'ReduceSum', 'ReduceSumSquare',
                                   'Relu', 'Resize', 'ReverseSequence', 'RoiAlign', 'Round',
                                   'Scatter', 'ScatterElements', 'ScatterND', 'Shrink', 'Sigmoid', 'Sign', 'Sin',
                                   'Softmax', 'Split', 'SplitToSequence', 'Sqrt', 'Sum',
                                   'Tanh', 'TopK', 'Transpose',
-                                  'Unique',
-                                  'Where']
+                                  'Unique']
 
     # ops that are used to manipulate shapes or indices so require int32_t and int64_t to be available
     default_processor_onnx_ops_requiring_ints_for_input_0 = ['Add',
+                                                             'Concat',
                                                              'Div',
                                                              'Equal',
                                                              'Greater',
                                                              'Less',
                                                              'Mul',
                                                              'Neg',  # used in tflite TransposeConv conversion
                                                              'Sub']
@@ -387,14 +400,17 @@
     # Operators that switch on output type
     add(DefaultTypeUsageProcessor('ai.onnx', 'ConstantOfShape', inputs=[], outputs=[0]))
 
     # Random generator ops produce new data so we track the output type
     onnx_random_ops = ['RandomNormal', 'RandomNormalLike', 'RandomUniform', 'RandomUniformLike', 'Multinomial']
     [add(DefaultTypeUsageProcessor('ai.onnx', op, inputs=[], outputs=[0])) for op in onnx_random_ops]
 
+    # Where always has a boolean first input so track the second input type for typed registration
+    add(Input1TypedRegistrationProcessor('ai.onnx', 'Where'))
+
     # we only support 'float' as input for [Dynamic]QuantizeLinear so just track the output type
     # as that's what is used in the typed registration
     add(Output0TypedRegistrationProcessor('ai.onnx', 'QuantizeLinear'))
     add(Output0TypedRegistrationProcessor('ai.onnx', 'DynamicQuantizeLinear'))
 
     # make sure all the dequantize types are enabled. we use int32_t for parts of GEMM and Conv so just
     # enabling int8 and uint8 is not enough.
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_model_processor.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_model_processor.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/types.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/types.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/utils.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/utils.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Attribute.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Attribute.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Dimension.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Dimension.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/DimensionValue.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/DimensionValue.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/EdgeEnd.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/EdgeEnd.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Graph.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Graph.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/InferenceSession.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/InferenceSession.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/KernelCreateInfos.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/KernelCreateInfos.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/MapType.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/MapType.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Model.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Model.py`

 * *Files 14% similar despite different names*

```diff
@@ -105,19 +105,46 @@
     # Model
     def GraphDocString(self):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(20))
         if o != 0:
             return self._tab.String(o + self._tab.Pos)
         return None
 
-def ModelStart(builder): builder.StartObject(9)
+    # Model
+    def MetadataProps(self, j):
+        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(22))
+        if o != 0:
+            x = self._tab.Vector(o)
+            x += flatbuffers.number_types.UOffsetTFlags.py_type(j) * 4
+            x = self._tab.Indirect(x)
+            from ort_flatbuffers_py.experimental.fbs.StringStringEntry import StringStringEntry
+            obj = StringStringEntry()
+            obj.Init(self._tab.Bytes, x)
+            return obj
+        return None
+
+    # Model
+    def MetadataPropsLength(self):
+        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(22))
+        if o != 0:
+            return self._tab.VectorLen(o)
+        return 0
+
+    # Model
+    def MetadataPropsIsNone(self):
+        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(22))
+        return o == 0
+
+def ModelStart(builder): builder.StartObject(10)
 def ModelAddIrVersion(builder, irVersion): builder.PrependInt64Slot(0, irVersion, 0)
 def ModelAddOpsetImport(builder, opsetImport): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(opsetImport), 0)
 def ModelStartOpsetImportVector(builder, numElems): return builder.StartVector(4, numElems, 4)
 def ModelAddProducerName(builder, producerName): builder.PrependUOffsetTRelativeSlot(2, flatbuffers.number_types.UOffsetTFlags.py_type(producerName), 0)
 def ModelAddProducerVersion(builder, producerVersion): builder.PrependUOffsetTRelativeSlot(3, flatbuffers.number_types.UOffsetTFlags.py_type(producerVersion), 0)
 def ModelAddDomain(builder, domain): builder.PrependUOffsetTRelativeSlot(4, flatbuffers.number_types.UOffsetTFlags.py_type(domain), 0)
 def ModelAddModelVersion(builder, modelVersion): builder.PrependInt64Slot(5, modelVersion, 0)
 def ModelAddDocString(builder, docString): builder.PrependUOffsetTRelativeSlot(6, flatbuffers.number_types.UOffsetTFlags.py_type(docString), 0)
 def ModelAddGraph(builder, graph): builder.PrependUOffsetTRelativeSlot(7, flatbuffers.number_types.UOffsetTFlags.py_type(graph), 0)
 def ModelAddGraphDocString(builder, graphDocString): builder.PrependUOffsetTRelativeSlot(8, flatbuffers.number_types.UOffsetTFlags.py_type(graphDocString), 0)
+def ModelAddMetadataProps(builder, metadataProps): builder.PrependUOffsetTRelativeSlot(9, flatbuffers.number_types.UOffsetTFlags.py_type(metadataProps), 0)
+def ModelStartMetadataPropsVector(builder, numElems): return builder.StartVector(4, numElems, 4)
 def ModelEnd(builder): return builder.EndObject()
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Node.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Node.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/NodeEdge.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/NodeEdge.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/OperatorSetId.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/OperatorSetId.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/SequenceType.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/SequenceType.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/SessionState.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/SessionState.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Shape.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Shape.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/SparseTensor.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/SparseTensor.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/SubGraphSessionState.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/SubGraphSessionState.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Tensor.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Tensor.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/TensorTypeAndShape.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/TensorTypeAndShape.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/TypeInfo.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/TypeInfo.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/ValueInfo.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/ValueInfo.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/benchmark.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/benchmark.py`

 * *Files 0% similar despite different names*

```diff
@@ -123,16 +123,16 @@
                 if batch_size <= 0:
                     continue
                 for sequence_length in sequence_lengths:
                     if max_sequence_length is not None and sequence_length > max_sequence_length:
                         continue
 
                     input_value_type = numpy.int64 if 'pt' in model_source else numpy.int32
-                    ort_inputs = create_onnxruntime_input(vocab_size, batch_size, sequence_length, input_names,
-                                                          config, input_value_type)
+                    ort_inputs = create_onnxruntime_input(vocab_size, batch_size, sequence_length, input_names, config,
+                                                          input_value_type)
                     result_template = {
                         "engine": "onnxruntime",
                         "version": onnxruntime.__version__,
                         "device": device,
                         "optimizer": optimize_onnx,
                         "precision": precision,
                         "io_binding": not disable_ort_io_binding,
@@ -329,15 +329,15 @@
                     @run_with_tf_optimizations(do_eager_mode=False, use_xla=False)
                     def encoder_forward():
                         return model(input_ids, training=False)
 
                     @run_with_tf_optimizations(do_eager_mode=False, use_xla=False)
                     def encoder_decoder_forward():
                         return model(input_ids, decoder_input_ids=input_ids, training=False)
-                    
+
                     @run_with_tf_optimizations(do_eager_mode=False, use_xla=False)
                     def lxmert_forward():
                         feats = tf.random.normal([1, 1, config.visual_feat_dim])
                         pos = tf.random.normal([1, 1, config.visual_pos_dim])
                         return model(input_ids, visual_feats=feats, visual_pos=pos, training=False)
 
                     inference = encoder_forward
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/benchmark_gpt2.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/benchmark_gpt2.py`

 * *Files 0% similar despite different names*

```diff
@@ -14,15 +14,15 @@
 import psutil
 import argparse
 import logging
 import torch
 import onnx
 from packaging import version
 from transformers import AutoConfig
-from gpt2_helper import DEFAULT_TOLERANCE, PRETRAINED_GPT2_MODELS
+from gpt2_helper import Gpt2Helper, DEFAULT_TOLERANCE, PRETRAINED_GPT2_MODELS
 from gpt2_beamsearch_helper import Gpt2HelperFactory, MODEL_CLASSES
 from quantize_helper import QuantizeHelper
 from benchmark_helper import create_onnxruntime_session, setup_logger, prepare_environment, Precision
 
 logger = logging.getLogger('')
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/benchmark_helper.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/benchmark_helper.py`

 * *Files 1% similar despite different names*

```diff
@@ -26,18 +26,21 @@
     FLOAT32 = 'fp32'
     FLOAT16 = 'fp16'
     INT8 = 'int8'
 
     def __str__(self):
         return self.value
 
+
 IO_BINDING_DATA_TYPE_MAP = {
     "float32": numpy.float32,
-    # TODO: Add more. 
+    # TODO: Add more.
 }
+
+
 def create_onnxruntime_session(onnx_model_path,
                                use_gpu,
                                enable_all_optimization=True,
                                num_threads=-1,
                                enable_profiling=False,
                                verbose=False):
     session = None
@@ -213,15 +216,16 @@
     result = {}
 
     # Bind inputs and outputs to onnxruntime session
     io_binding = ort_session.io_binding()
     # Bind inputs to device
     for name in ort_inputs.keys():
         np_input = torch.from_numpy(ort_inputs[name]).to(device)
-        input_type = IO_BINDING_DATA_TYPE_MAP[str(ort_inputs[name].dtype)] if str(ort_inputs[name].dtype) in IO_BINDING_DATA_TYPE_MAP else data_type
+        input_type = IO_BINDING_DATA_TYPE_MAP[str(ort_inputs[name].dtype)] if str(
+            ort_inputs[name].dtype) in IO_BINDING_DATA_TYPE_MAP else data_type
         io_binding.bind_input(name, np_input.device.type, 0, input_type, np_input.shape, np_input.data_ptr())
     # Bind outputs buffers with the sizes needed if not allocated already
     if len(output_buffers) == 0:
         allocateOutputBuffers(output_buffers, output_buffer_max_sizes, device)
 
     for i in range(len(ort_output_names)):
         io_binding.bind_output(ort_output_names[i], output_buffers[i].device.type, 0, numpy.float32,
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/bert_perf_test.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/bert_perf_test.py`

 * *Files 1% similar despite different names*

```diff
@@ -36,14 +36,15 @@
     test_cases: int
     test_times: int
     use_gpu: bool
     intra_op_num_threads: int
     seed: int
     verbose: bool
 
+
 @dataclass
 class ModelSetting:
     model_path: str
     input_ids_name: str
     segment_ids_name: str
     input_mask_name: str
     opt_level: int
@@ -56,15 +57,16 @@
         print(
             "Warning: Please install onnxruntime-gpu package instead of onnxruntime, and use a machine with GPU for testing gpu performance."
         )
 
     if intra_op_num_threads is None and graph_optimization_level is None:
         session = onnxruntime.InferenceSession(model_path)
     else:
-        execution_providers = ['CPUExecutionProvider'] if not use_gpu else ['CUDAExecutionProvider', 'CPUExecutionProvider']
+        execution_providers = ['CPUExecutionProvider'
+                               ] if not use_gpu else ['CUDAExecutionProvider', 'CPUExecutionProvider']
 
         sess_options = onnxruntime.SessionOptions()
         sess_options.execution_mode = onnxruntime.ExecutionMode.ORT_SEQUENTIAL
 
         if graph_optimization_level is None:
             sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL
         elif graph_optimization_level == 0:
@@ -99,23 +101,25 @@
         start_time = timeit.default_timer()
         result = session.run(output_names, inputs)
         latency = timeit.default_timer() - start_time
         results.append(result)
         latency_list.append(latency)
     return results, latency_list
 
+
 def to_string(model_path, session, test_setting):
     sess_options = session.get_session_options()
     option = "model={},".format(os.path.basename(model_path))
     option += "graph_optimization_level={},intra_op_num_threads={},".format(sess_options.graph_optimization_level,
                                                                             sess_options.intra_op_num_threads).replace(
                                                                                 'GraphOptimizationLevel.ORT_', '')
     option += f"batch_size={test_setting.batch_size},sequence_length={test_setting.sequence_length},test_cases={test_setting.test_cases},test_times={test_setting.test_times},use_gpu={test_setting.use_gpu}"
     return option
 
+
 def run_one_test(model_setting, test_setting, perf_results, all_inputs, intra_op_num_threads):
     session = create_session(model_setting.model_path, test_setting.use_gpu, intra_op_num_threads,
                              model_setting.opt_level)
     output_names = [output.name for output in session.get_outputs()]
 
     key = to_string(model_setting.model_path, session, test_setting)
     if key in perf_results:
@@ -144,15 +148,16 @@
 
     print("Average latency = {} ms, Throughput = {} QPS".format(format(average_latency, '.2f'),
                                                                 format(throughput, '.2f')))
 
 
 def launch_test(model_setting, test_setting, perf_results, all_inputs, intra_op_num_threads):
     process = multiprocessing.Process(target=run_one_test,
-                                      args=(model_setting, test_setting, perf_results, all_inputs, intra_op_num_threads))
+                                      args=(model_setting, test_setting, perf_results, all_inputs,
+                                            intra_op_num_threads))
     process.start()
     process.join()
 
 
 def run_perf_tests(model_setting, test_setting, perf_results, all_inputs):
     if (test_setting.intra_op_num_threads is not None):
         launch_test(model_setting, test_setting, perf_results, all_inputs, test_setting.intra_op_num_threads)
@@ -165,15 +170,16 @@
     for i in range(1, min(16, logical_cores)):
         if i not in candidate_threads:
             candidate_threads.append(i)
     candidate_threads.sort(reverse=True)
 
     for intra_op_num_threads in candidate_threads:
         launch_test(model_setting, test_setting, perf_results, all_inputs, intra_op_num_threads)
-        
+
+
 def run_performance(model_setting, test_setting, perf_results):
     input_ids, segment_ids, input_mask = get_bert_inputs(model_setting.model_path, model_setting.input_ids_name,
                                                          model_setting.segment_ids_name, model_setting.input_mask_name)
 
     # Do not generate random mask for performance test.
     print(
         f"Generating {test_setting.test_cases} samples for batch_size={test_setting.batch_size} sequence_length={test_setting.sequence_length}"
@@ -191,25 +197,27 @@
     run_perf_tests(model_setting, test_setting, perf_results, all_inputs)
 
 
 def parse_arguments():
     parser = argparse.ArgumentParser()
     parser.add_argument('--model', required=True, type=str, help="bert onnx model path")
 
-    parser.add_argument('-b', '--batch_size',
+    parser.add_argument('-b',
+                        '--batch_size',
                         required=True,
                         type=int,
                         nargs="+",
                         help="batch size of input. Allow one or multiple values in the range of [1, 128].")
 
     parser.add_argument('-s', '--sequence_length', required=True, type=int, help="maximum sequence length of input")
 
     parser.add_argument('--samples', required=False, type=int, default=10, help="number of samples to be generated")
 
-    parser.add_argument('-t', '--test_times',
+    parser.add_argument('-t',
+                        '--test_times',
                         required=False,
                         type=int,
                         default=0,
                         help="number of times to run per sample. By default, the value is 1000 / samples")
 
     parser.add_argument(
         '--opt_level',
@@ -227,15 +235,16 @@
 
     parser.add_argument('--verbose', required=False, action='store_true', help="print verbose information")
     parser.set_defaults(verbose=False)
 
     parser.add_argument('--use_gpu', required=False, action='store_true', help="use GPU")
     parser.set_defaults(use_gpu=False)
 
-    parser.add_argument('-n', '--intra_op_num_threads',
+    parser.add_argument('-n',
+                        '--intra_op_num_threads',
                         required=False,
                         type=int,
                         default=None,
                         help=">=0, set intra_op_num_threads")
 
     parser.add_argument('--input_ids_name', required=False, type=str, default=None, help="input name for input ids")
     parser.add_argument('--segment_ids_name', required=False, type=str, default=None, help="input name for segment ids")
@@ -262,23 +271,16 @@
     if not min(batch_size_set) >= 1 and max(batch_size_set) <= 128:
         raise Exception("batch_size not in range [1, 128]")
 
     model_setting = ModelSetting(args.model, args.input_ids_name, args.segment_ids_name, args.input_mask_name,
                                  args.opt_level)
 
     for batch_size in batch_size_set:
-        test_setting = TestSetting(
-            batch_size,
-            args.sequence_length,
-            args.samples,
-            args.test_times,
-            args.use_gpu,
-            args.intra_op_num_threads,
-            args.seed,
-            args.verbose)
+        test_setting = TestSetting(batch_size, args.sequence_length, args.samples, args.test_times, args.use_gpu,
+                                   args.intra_op_num_threads, args.seed, args.verbose)
 
         print("test setting", test_setting)
         run_performance(model_setting, test_setting, perf_results)
 
     # Sort the results so that the first one has smallest latency.
     sorted_results = sorted(perf_results.items(), reverse=False, key=lambda x: x[1])
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/bert_test_data.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/bert_test_data.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/compare_bert_results.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/compare_bert_results.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/convert_tf_models_to_pytorch.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/convert_tf_models_to_pytorch.py`

 * *Files 1% similar despite different names*

```diff
@@ -90,17 +90,17 @@
 def init_pytorch_model(model_name, tf_checkpoint_path):
     config_name = TFMODELS[model_name][1]
     config_module = __import__("transformers", fromlist=[config_name])
     model_config = getattr(config_module, config_name)
 
     parent_path = tf_checkpoint_path.rpartition('/')[0]
     config_path = glob.glob(parent_path + "/*config.json")
-    config = model_config() if len(config_path) is 0 else model_config.from_json_file(str(config_path[0]))
+    config = model_config() if len(config_path) == 0 else model_config.from_json_file(str(config_path[0]))
 
-    if TFMODELS[model_name][2] is "":
+    if TFMODELS[model_name][2] == "":
         from transformers import AutoModelForPreTraining
         init_model = AutoModelForPreTraining.from_config(config)
     else:
         model_categroy_name = TFMODELS[model_name][2]
         module = __import__("transformers", fromlist=[model_categroy_name])
         model_categroy = getattr(module, model_categroy_name)
         init_model = model_categroy(config)
@@ -111,15 +111,15 @@
     load_tf_weight_func_name = "load_tf_weights_in_" + TFMODELS[model_name][0]
 
     module = __import__("transformers", fromlist=[load_tf_weight_func_name])
 
     if is_tf2 is False:
         load_tf_weight_func = getattr(module, load_tf_weight_func_name)
     else:
-        if TFMODELS[model_name][0] is not "bert":
+        if TFMODELS[model_name][0] != "bert":
             raise NotImplementedError("Only support tf2 ckeckpoint for Bert model")
         from transformers import convert_bert_original_tf2_checkpoint_to_pytorch
         load_tf_weight_func = convert_bert_original_tf2_checkpoint_to_pytorch.load_tf2_weights_in_bert
 
     # Expect transformers team will unify the order of signature in the future
     model = load_tf_weight_func(init_model, config, tf_checkpoint_path) if is_tf2 is False else load_tf_weight_func(
         init_model, tf_checkpoint_path, config)
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/convert_to_onnx.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/convert_to_onnx.py`

 * *Files 22% similar despite different names*

```diff
@@ -30,15 +30,15 @@
 from gpt2_beamsearch_tester import Gpt2TesterFactory
 from quantize_helper import QuantizeHelper
 from benchmark_helper import create_onnxruntime_session, setup_logger, prepare_environment, Precision
 
 logger = logging.getLogger('')
 
 
-def parse_arguments():
+def parse_arguments(argv=None):
     parser = argparse.ArgumentParser()
 
     parser.add_argument('-m',
                         '--model_name_or_path',
                         required=True,
                         type=str,
                         help='Model path, or pretrained model name in the list: ' + ', '.join(PRETRAINED_GPT2_MODELS))
@@ -90,14 +90,21 @@
         "--precision",
         required=False,
         type=Precision,
         default=Precision.FLOAT32,
         choices=list(Precision),
         help="Precision of model to run. fp32 for full precision, fp16 for half precision, and int8 for quantization")
 
+    parser.add_argument("-t",
+                        "--test_cases",
+                        required=False,
+                        type=int,
+                        default=1000,
+                        help="Number of test cases for parity")
+
     parser.add_argument('--verbose', required=False, action='store_true')
     parser.set_defaults(verbose=False)
 
     parser.add_argument('-e', '--use_external_data_format', required=False, action='store_true')
     parser.set_defaults(use_external_data_format=False)
     parser.add_argument('--beam_size', type=int, default=4, help='Beam size if greedy/top-p/top-k sampling is needed')
 
@@ -131,28 +138,66 @@
                                        help='If to do sampling instead of beam search or greedy.')
     sampling_option_group.add_argument('--do_sample_top_p',
                                        type=float,
                                        default=0.95,
                                        help='Nuclear/top-p sampling accumulation probability.')
     sampling_option_group.add_argument('--do_sample_top_k', type=int, default=0, help='Use top-k if non-zero.')
 
-    args = parser.parse_args()
+    fp16_option_group = parser.add_argument_group(
+        "float to float16 conversion parameters that works when \"--precision fp16\" is specified")
+
+    fp16_option_group.add_argument('--keep_io_types',
+                                   required=False,
+                                   action='store_true',
+                                   help='Use float32 for past inputs, present and logits outputs.')
+    fp16_option_group.set_defaults(keep_io_types=False)
+
+    fp16_option_group.add_argument('--io_block_list',
+                                   nargs='+',
+                                   default=[],
+                                   help='List of inputs or outputs in float32 instead of float16')
+
+    fp16_option_group.add_argument(
+        '--op_block_list',
+        nargs='+',
+        default=[],
+        help=
+        'List of operators (like Attention Gather Add LayerNormalization FastGelu MatMul) to compute in float32 instead of float16.'
+    )
+
+    fp16_option_group.add_argument('--node_block_list',
+                                   nargs='+',
+                                   default=[],
+                                   help='List of node names to compute in float32 instead of float16.')
+
+    fp16_option_group.add_argument('--force_fp16_initializers',
+                                   required=False,
+                                   action='store_true',
+                                   help='Convert all float initializers to float16.')
+    fp16_option_group.set_defaults(force_fp16_initializers=False)
+
+    args = parser.parse_args(argv)
 
     return args
 
 
-def main():
+def main(argv=None, experiment_name="", run_id=0, csv_filename="gpt2_parity_results.csv"):
+    result = {}
     from transformers import __version__ as transformers_version
     if version.parse(transformers_version) < version.parse(
             "3.1.0"):  # past_key_values name does not exist in 3.0.2 or older
         raise RuntimeError("This tool requires transformers 3.1.0 or later.")
 
-    args = parse_arguments()
+    args = parse_arguments(argv)
     setup_logger(args.verbose)
 
+    if not experiment_name:
+        import sys
+        experiment_name = " ".join(argv if argv else sys.argv[1:])
+
     if args.tolerance == 0:
         args.tolerance = DEFAULT_TOLERANCE[args.precision]
 
     logger.info(f"Arguments:{args}")
 
     cache_dir = args.cache_dir
     output_dir = args.output if not args.output.endswith(".onnx") else os.path.dirname(args.output)
@@ -215,29 +260,42 @@
                                                  args.model_class,
                                                  new_folder=args.use_external_data_format)
 
     raw_onnx_model = onnx_model_paths["raw"]
 
     logger.info(f"Exporting ONNX model to {raw_onnx_model}")
     use_padding = MODEL_CLASSES[args.model_class][2]
+
     gpt2helper.export_onnx(model,
                            device,
                            raw_onnx_model,
                            args.verbose,
                            args.use_external_data_format,
                            has_position_ids=use_padding,
                            has_attention_mask=use_padding)
 
+    fp16_params = {"keep_io_types": args.keep_io_types}
+    if args.io_block_list:
+        fp16_params["keep_io_types"] = args.io_block_list
+    if args.node_block_list:
+        fp16_params["node_block_list"] = args.node_block_list
+    if args.op_block_list:
+        fp16_params["op_block_list"] = args.op_block_list
+    if args.force_fp16_initializers:
+        fp16_params["force_fp16_initializers"] = args.force_fp16_initializers
+
+    is_io_float16 = (args.precision == Precision.FLOAT16 and not args.keep_io_types)
+
     if args.optimize_onnx or args.precision != Precision.FLOAT32:
         output_path = onnx_model_paths[str(args.precision) if args.precision != Precision.INT8 else 'fp32']
 
         logger.info(f"Optimizing model to {output_path}")
         gpt2helper.optimize_onnx(raw_onnx_model, output_path, args.precision == Precision.FLOAT16,
                                  model.config.num_attention_heads, model.config.hidden_size,
-                                 args.use_external_data_format)
+                                 args.use_external_data_format, **fp16_params)
     else:
         output_path = raw_onnx_model
 
     if args.precision == Precision.INT8:
         logger.info("quantizing model...")
         QuantizeHelper.quantize_onnx_model(output_path, onnx_model_paths['int8'], args.use_external_data_format)
         model = QuantizeHelper.quantize_torch_model(model)
@@ -248,45 +306,108 @@
         import shutil
         shutil.move(output_path, args.output)
         output_path = args.output
 
     logger.info(f"Output path: {output_path}")
 
     session = create_onnxruntime_session(output_path, args.use_gpu, enable_all_optimization=True, verbose=args.verbose)
-    if session is not None:
-        gpt2helper.test_parity(session,
-                               model,
-                               device,
-                               args.precision == Precision.FLOAT16,
-                               rtol=args.tolerance,
-                               atol=args.tolerance,
-                               model_class=args.model_class,
-                               has_position_ids=use_padding,
-                               has_attention_mask=use_padding)
+    if args.model_class == "GPT2LMHeadModel" and session is not None:
+        parity_result = gpt2helper.test_parity(session,
+                                               model,
+                                               device,
+                                               is_io_float16,
+                                               rtol=args.tolerance,
+                                               atol=args.tolerance,
+                                               model_class=args.model_class,
+                                               has_position_ids=use_padding,
+                                               has_attention_mask=use_padding,
+                                               total_test_cases=args.test_cases,
+                                               verbose=args.verbose)
+
+        latency = gpt2helper.test_performance(session,
+                                              model,
+                                              device,
+                                              is_io_float16,
+                                              total_runs=100,
+                                              use_io_binding=True,
+                                              model_class=args.model_class,
+                                              has_position_ids=use_padding,
+                                              has_attention_mask=use_padding,
+                                              batch_size=8,
+                                              sequence_length=1,
+                                              past_sequence_length=32)
+
+        if args.precision == Precision.FLOAT16:
+            logger.info(f"fp16 conversion parameters:{fp16_params}")
+
+        # Write results to file
+        import csv
+        from onnxruntime import __version__ as ort_version
+        latency_name = "average_latency(batch_size=8,sequence_length=1,past_sequence_length=32)"
+        csv_file_existed = os.path.exists(csv_filename)
+        with open(csv_filename, mode="a", newline='') as csv_file:
+            column_names = [
+                "experiment", "run_id", "model_name", "model_class", "gpu", "precision", "optimizer", "test_cases",
+                "keep_io_types", "io_block_list", "op_block_list", "node_block_list", "force_fp16_initializers",
+                "ORT_TRANSFORMER_OPTIONS", "ORT_CUDA_GEMM_OPTIONS", "onnxruntime", latency_name, "diff_50_percentile",
+                "diff_90_percentile", "diff_95_percentile", "diff_99_percentile", "diff_pass_rate", "nan_rate",
+                "top1_match_rate", "onnx_size_in_MB"
+            ]
+            csv_writer = csv.DictWriter(csv_file, fieldnames=column_names)
+            if not csv_file_existed:
+                csv_writer.writeheader()
+            row = {
+                "experiment": experiment_name,
+                "run_id": run_id,
+                "model_name": args.model_name_or_path,
+                "model_class": args.model_class,
+                "gpu": args.use_gpu,
+                "precision": args.precision,
+                "optimizer": args.optimize_onnx,
+                "test_cases": args.test_cases,
+                "keep_io_types": args.keep_io_types,
+                "io_block_list": args.io_block_list,
+                "op_block_list": args.op_block_list,
+                "node_block_list": args.node_block_list,
+                "force_fp16_initializers": args.force_fp16_initializers,
+                "ORT_TRANSFORMER_OPTIONS": os.getenv('ORT_TRANSFORMER_OPTIONS'),
+                "ORT_CUDA_GEMM_OPTIONS": os.getenv('ORT_CUDA_GEMM_OPTIONS'),
+                "onnxruntime": ort_version,
+                latency_name: f"{latency:.2f}",
+                "diff_50_percentile": parity_result["max_diff_percentile_50"],
+                "diff_90_percentile": parity_result["max_diff_percentile_90"],
+                "diff_95_percentile": parity_result["max_diff_percentile_95"],
+                "diff_99_percentile": parity_result["max_diff_percentile_99"],
+                "diff_pass_rate": parity_result["diff_pass_rate"],
+                "nan_rate": parity_result["nan_rate"],
+                "top1_match_rate": parity_result["top1_match_rate"],
+                "onnx_size_in_MB": "{}".format(int(os.path.getsize(output_path) / 1024 / 1024))
+            }
+            logger.info(f"result: {row}")
+            result.update(row)
+            csv_writer.writerow(row)
 
     if args.input_test_file:
         test_inputs = []
         # Each line of test file is a JSON string like:
         # {"input_ids": [[14698, 257, 1310, 13688, 319, 326]]}
         with open(args.input_test_file) as read_f:
             for _, line in enumerate(read_f):
                 line = line.rstrip()
                 data = json.loads(line)
                 input_ids = torch.from_numpy(numpy.asarray(data["input_ids"], dtype=numpy.int64)).to(device)
 
                 if use_padding:
                     if "attention_mask" in data:
-                        numpy_float = numpy.float16 if args.precision == Precision.FLOAT16 else numpy.float32
+                        numpy_float = numpy.float16 if is_io_float16 else numpy.float32
                         attention_mask = torch.from_numpy(numpy.asarray(data["attention_mask"],
                                                                         dtype=numpy_float)).to(device)
                     else:
                         padding = -1
-                        attention_mask = (
-                            input_ids !=
-                            padding).type(torch.float16 if args.precision == Precision.FLOAT16 else torch.float32)
+                        attention_mask = (input_ids != padding).type(torch.float16 if is_io_float16 else torch.float32)
                         input_ids.masked_fill_(input_ids == padding, 0)
 
                     if "position_ids" in data:
                         position_ids = torch.from_numpy(numpy.asarray(data["position_ids"],
                                                                       dtype=numpy.int64)).to(device)
                     else:
                         position_ids = (attention_mask.long().cumsum(-1) - 1)
@@ -320,11 +441,12 @@
                                    max_steps=24,
                                    max_inputs=0,
                                    verbose=args.verbose,
                                    save_test_data=3,
                                    save_test_data_dir=Path(output_path).parent)
 
     logger.info(f"Done. Output model: {output_path}")
+    return result
 
 
 if __name__ == '__main__':
     main()
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_attention.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_attention.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,30 +1,27 @@
 #-------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
 #--------------------------------------------------------------------------
+from os import name
+from sys import path
 import numpy as np
 from logging import getLogger
 from enum import Enum
 from typing import Tuple, Union
 from onnx import helper, numpy_helper, TensorProto, NodeProto
 from onnx_model import OnnxModel
 from fusion_base import Fusion
 from fusion_utils import FusionUtils, NumpyHelper
+from fusion_options import AttentionMaskFormat
+from shape_infer_helper import SymbolicShapeInferenceHelper, get_shape_from_type_proto
 
 logger = getLogger(__name__)
 
 
-class AttentionMaskFormat:
-    MaskIndexEnd = 0
-    MaskIndexEndAndStart = 1
-    AttentionMask = 2
-    NoMask = 3
-
-
 class AttentionMask():
     """
     Fuse Attention subgraph into one Attention node.
     """
     def __init__(self, model: OnnxModel):
         self.model = model
         # A lookup table with mask input as key, and mask index output as value
@@ -113,24 +110,42 @@
             return self.num_heads, self.hidden_size  # Fall back to user specified value
 
         num_heads = q_shape_value[2]
         head_size = q_shape_value[3]
         hidden_size = num_heads * head_size
 
         if self.num_heads > 0 and num_heads != self.num_heads:
-            logger.warn("--num_heads is {self.num_heads}. Detected value is {num_heads}. Using detected value.")
+            logger.warn(f"--num_heads is {self.num_heads}. Detected value is {num_heads}. Using detected value.")
 
         if self.hidden_size > 0 and hidden_size != self.hidden_size:
-            logger.warn("--hidden_size is {self.hidden_size}. Detected value is {hidden_size}. Using detected value.")
+            logger.warn(f"--hidden_size is {self.hidden_size}. Detected value is {hidden_size}. Using detected value.")
 
         return num_heads, hidden_size
 
+    def get_add_qk_str(self, add_qk: NodeProto):
+        shape_infer = self.model.infer_runtime_shape(update=True)
+        if shape_infer is None:
+            return
+
+        input_0_shape = shape_infer.get_edge_shape(add_qk.input[0])
+        input_1_shape = shape_infer.get_edge_shape(add_qk.input[1])
+
+        if input_0_shape is None or input_1_shape is None:
+            logger.debug(f"one of the inputs of {add_qk} is None")
+            return None
+
+        if input_0_shape != input_1_shape:
+            logger.debug(f"the shape of two inputs of {add_qk} is not same")
+            return None
+
+        return add_qk.input[1]
+
     def create_attention_node(self, mask_index: str, q_matmul: NodeProto, k_matmul: NodeProto, v_matmul: NodeProto,
                               q_add: NodeProto, k_add: NodeProto, v_add: NodeProto, num_heads: int, hidden_size: int,
-                              input: str, output: str) -> Union[NodeProto, None]:
+                              input: str, output: str, add_qk_str: str) -> Union[NodeProto, None]:
         """ Create an Attention node.
 
         Args:
             mask_index (str): mask input
             q_matmul (NodeProto): MatMul node in fully connection for Q
             k_matmul (NodeProto): MatMul node in fully connection for  K
             v_matmul (NodeProto): MatMul node in fully connection for  V
@@ -141,87 +156,129 @@
             hidden_size (int): hidden dimension. If a model is pruned, it is the hidden dimension after pruning.
             input (str): input name
             output (str): output name
 
         Returns:
             Union[NodeProto, None]: the node created or None if failed.
         """
-        assert num_heads > 0 and hidden_size > 0 and (hidden_size % num_heads) == 0
+        assert num_heads > 0
+
+        if hidden_size > 0 and (hidden_size % num_heads) != 0:
+            logger.debug(f"input hidden size {hidden_size} is not a multiple of num of heads {num_heads}")
+            return None
 
         q_weight = self.model.get_initializer(q_matmul.input[1])
         k_weight = self.model.get_initializer(k_matmul.input[1])
         v_weight = self.model.get_initializer(v_matmul.input[1])
         q_bias = self.model.get_initializer(q_add.input[1]) or self.model.get_initializer(q_add.input[0])
         k_bias = self.model.get_initializer(k_add.input[1]) or self.model.get_initializer(k_add.input[0])
         v_bias = self.model.get_initializer(v_add.input[1]) or self.model.get_initializer(v_add.input[0])
 
         if q_weight is None:
             print(f"{q_matmul.input[1]} is not initializer. Please set do_constant_folding=True in torch.onnx.export")
             return None
         if not (k_weight and v_weight and q_bias and k_bias):
             return None
+
         qw = NumpyHelper.to_array(q_weight)
         kw = NumpyHelper.to_array(k_weight)
         vw = NumpyHelper.to_array(v_weight)
 
-        # Check if all matrices have the same shape
-        assert qw.shape == kw.shape == vw.shape
+        # assert q and k have same shape as expected
+        assert qw.shape == kw.shape
 
-        # All the matrices have the same shape. For 2d weights, the shapes would be [in_size, out_size].
-        # For 3d weights, shape would be [in_size, a, b] where a*b = out_size
-        in_size = qw.shape[0]
-        out_size = np.prod(qw.shape[1:])
+        qw_in_size = qw.shape[0]
+        kw_in_size = kw.shape[0]
+        vw_in_size = vw.shape[0]
 
-        qkv_weight = np.stack((qw, kw, vw), axis=1)
+        assert qw_in_size == kw_in_size == vw_in_size
+
+        if hidden_size > 0 and hidden_size != qw_in_size:
+            logger.debug(
+                f"Input hidden size {hidden_size} is not same as weight matrix dimension of q,k,v paths {qw_in_size}, provide correct input hidden size or pass 0"
+            )
+            return None
+
+        is_qkv_diff_dims = False
+        if qw.shape != vw.shape:
+            is_qkv_diff_dims = True
+
+        # All the matrices can have the same shape or q, k matrics can have the same shape with v being different
+        # For 2d weights, the shapes would be [in_size, out_size].
+        # For 3d weights, shape would be [in_size, a, b] where a*b = out_size
+        qw_out_size = np.prod(qw.shape[1:])
+        kw_out_size = np.prod(qw.shape[1:])
+        vw_out_size = np.prod(vw.shape[1:])
+
+        qkv_weight_dim = 0
+        if is_qkv_diff_dims:
+            qkv_weight = np.concatenate((qw, kw, vw), axis=1)
+            qkv_weight_dim = qw_out_size + kw_out_size + vw_out_size
+        else:
+            qkv_weight = np.stack((qw, kw, vw), axis=1)
+            qkv_weight_dim = 3 * qw_out_size
 
         qb = NumpyHelper.to_array(q_bias)
         kb = NumpyHelper.to_array(k_bias)
         vb = NumpyHelper.to_array(v_bias)
 
-        # 1d bias shape: [outsize,]. 2d bias shape: [a, b] where a*b = out_size
-        assert qb.shape == kb.shape == vb.shape
-        assert np.prod(qb.shape) == out_size
-
-        if out_size != hidden_size:
-            logger.debug(
-                f"Shape for weights of Q is {in_size, out_size}, which does not match hidden_size={hidden_size}")
-            return None
+        q_bias_shape = np.prod(qb.shape)
+        k_bias_shape = np.prod(kb.shape)
+        v_bias_shape = np.prod(vb.shape)
+
+        assert q_bias_shape == k_bias_shape == qw_out_size
+        assert v_bias_shape == vw_out_size
+
+        qkv_bias_dim = 0
+        if is_qkv_diff_dims:
+            qkv_bias = np.concatenate((qb, kb, vb), axis=0)
+            qkv_bias_dim = q_bias_shape + k_bias_shape + v_bias_shape
+        else:
+            qkv_bias = np.stack((qb, kb, vb), axis=0)
+            qkv_bias_dim = 3 * q_bias_shape
 
-        qkv_bias = np.stack((qb, kb, vb), axis=0)
         attention_node_name = self.model.create_node_name('Attention')
 
         weight = helper.make_tensor(name=attention_node_name + '_qkv_weight',
                                     data_type=TensorProto.FLOAT,
-                                    dims=[in_size, 3 * out_size],
+                                    dims=[qw_in_size, qkv_weight_dim],
                                     vals=qkv_weight.flatten().tolist())
 
         # Sometimes weights and bias are stored in fp16
         if q_weight.data_type == 10:
             weight.CopyFrom(numpy_helper.from_array(NumpyHelper.to_array(weight).astype(np.float16), weight.name))
         self.model.add_initializer(weight, self.this_graph_name)
 
         bias = helper.make_tensor(name=attention_node_name + '_qkv_bias',
                                   data_type=TensorProto.FLOAT,
-                                  dims=[3 * out_size],
+                                  dims=[qkv_bias_dim],
                                   vals=qkv_bias.flatten().tolist())
         if q_bias.data_type == 10:
             bias.CopyFrom(numpy_helper.from_array(NumpyHelper.to_array(bias).astype(np.float16), bias.name))
         self.model.add_initializer(bias, self.this_graph_name)
 
         attention_inputs = [input, attention_node_name + '_qkv_weight', attention_node_name + '_qkv_bias']
         if mask_index is not None:
             attention_inputs.append(mask_index)
 
+        if add_qk_str is not None:
+            attention_inputs.append("")
+            attention_inputs.append(add_qk_str)
+
         attention_node = helper.make_node('Attention',
                                           inputs=attention_inputs,
                                           outputs=[output],
                                           name=attention_node_name)
         attention_node.domain = "com.microsoft"
         attention_node.attribute.extend([helper.make_attribute("num_heads", num_heads)])
 
+        if is_qkv_diff_dims:
+            attention_node.attribute.extend(
+                [helper.make_attribute("qkv_hidden_sizes", [qw_out_size, kw_out_size, vw_out_size])])
+
         return attention_node
 
     def fuse(self, normalize_node, input_name_to_nodes, output_name_to_node):
         # Sometimes we can not fuse skiplayernormalization since the add before layernorm has an output that used by nodes outside skiplayernorm
         # Conceptually we treat add before layernorm as skiplayernorm node since they share the same pattern
         start_node = normalize_node
         if normalize_node.op_type == 'LayerNormalization':
@@ -293,29 +350,44 @@
         v_nodes = self.model.match_parent_path(matmul_qkv, ['Transpose', 'Reshape', 'Add', 'MatMul'], [1, 0, 0, None])
         if v_nodes is None:
             logger.debug("fuse_attention: failed to match v path")
             return
         (_, _, add_v, matmul_v) = v_nodes
 
         is_distill = False
-        qk_nodes = self.model.match_parent_path(matmul_qkv, ['Softmax', 'Add', 'Div', 'MatMul'], [0, 0, None, 0])
-        if qk_nodes is None:
-            qk_nodes = self.model.match_parent_path(matmul_qkv, ['Softmax', 'Add', 'Mul', 'MatMul'], [0, 0, None, 0])
+        is_distill_add = False
+        qk_paths = {
+            "path1": (['Softmax', 'Add', 'Div', 'MatMul'], [0, 0, None, 0]),
+            "path2": (['Softmax', 'Add', 'Mul', 'MatMul'], [0, 0, None, 0]),
+            "path3": (['Softmax', 'Where', 'MatMul', 'Div'], [0, 0, 2, 0]),
+            "path4": (['Softmax', 'Add', 'Where', 'MatMul'], [0, 0, 0, 2])
+        }
+
+        qk_nodes = None
+        for k, v in qk_paths.items():
+            qk_nodes = self.model.match_parent_path(matmul_qkv, v[0], v[1])
             if qk_nodes is None:
-                qk_nodes = self.model.match_parent_path(matmul_qkv, ['Softmax', 'Where', 'MatMul', 'Div'], [0, 0, 2, 0])
+                continue
+            if k == "path3":
                 is_distill = True
-                if qk_nodes is None:
-                    logger.debug("fuse_attention: failed to match qk path")
-                    return
+            if k == "path4":
+                is_distill_add = True
+            break
+
+        if qk_nodes is None:
+            logger.debug("fuse_attention: failed to match qk path")
+            return
 
         add_qk = None
         matmul_qk = None
         where_qk = None
         if is_distill:
             (_, where_qk, matmul_qk, _) = qk_nodes
+        elif is_distill_add:
+            (_, add_qk, where_qk, matmul_qk) = qk_nodes
         else:
             (_, add_qk, _, matmul_qk) = qk_nodes
 
         q_nodes = self.model.match_parent_path(matmul_qk, ['Transpose', 'Reshape', 'Add', 'MatMul'], [0, 0, 0, None])
         if q_nodes is None:
             q_nodes = self.model.match_parent_path(matmul_qk, ['Div', 'Transpose', 'Reshape', 'Add', 'MatMul'],
                                                    [0, 0, 0, 0, None])
@@ -334,53 +406,62 @@
                 logger.debug("fuse_attention: failed to match k path")
                 return
         add_k = k_nodes[-2]
         matmul_k = k_nodes[-1]
 
         # Note that Cast might be removed by OnnxRuntime so we match two patterns here.
         mask_nodes = None
+        add_qk_str = None
         if is_distill:
             _, mask_nodes, _ = self.model.match_parent_paths(where_qk,
                                                              [(['Expand', 'Reshape', 'Equal'], [0, 0, 0]),
                                                               (['Cast', 'Expand', 'Reshape', 'Equal'], [0, 0, 0, 0])],
                                                              output_name_to_node)
+        elif is_distill_add:
+            _, mask_nodes, _ = self.model.match_parent_paths(
+                where_qk, [(['Cast', 'Equal', 'Unsqueeze', 'Unsqueeze'], [0, 0, 0, 0]),
+                           (['Equal', 'Unsqueeze', 'Unsqueeze'], [0, 0, 0])], output_name_to_node)
+            if add_qk is not None:
+                add_qk_str = self.get_add_qk_str(add_qk)
+                if add_qk_str is None:
+                    logger.debug(f"fuse_attention: failed to verify shape inference of {add_qk}")
+                    return
         else:
             _, mask_nodes, _ = self.model.match_parent_paths(
                 add_qk, [(['Mul', 'Sub', 'Cast', 'Unsqueeze', 'Unsqueeze'], [None, 0, 1, 0, 0]),
                          (['Mul', 'Sub', 'Unsqueeze', 'Unsqueeze'], [None, 0, 1, 0])], output_name_to_node)
         if mask_nodes is None:
             logger.debug("fuse_attention: failed to match mask path")
             return
 
-        if matmul_v.input[0] == root_input and matmul_q.input[0] == root_input and matmul_v.input[0] == root_input:
+        if matmul_v.input[0] == root_input and matmul_q.input[0] == root_input and matmul_k.input[0] == root_input:
             mask_index = self.attention_mask.process_mask(mask_nodes[-1].input[0])
 
             attention_last_node = reshape_qkv if einsum_node is None else transpose_qkv
 
-            num_heads, hidden_size = self.get_num_heads_and_hidden_size(reshape_q)
-            if num_heads <= 0 or hidden_size <= 0 or (hidden_size % num_heads) != 0:
-                logger.debug("fuse_attention: failed to detect num_heads or hidden_size")
-                return
-
+            q_num_heads, q_hidden_size = self.get_num_heads_and_hidden_size(reshape_q)
+            # number of heads are same for all the paths, hence to create attention node, we pass the q_num_heads
+            # the input_hidden_size represents the input hidden size, this is used as needed but hidden sizes for Q, K are extracted appropriately
             new_node = self.create_attention_node(mask_index, matmul_q, matmul_k, matmul_v, add_q, add_k, add_v,
-                                                  num_heads, hidden_size, root_input, attention_last_node.output[0])
+                                                  q_num_heads, self.hidden_size, root_input,
+                                                  attention_last_node.output[0], add_qk_str)
             if new_node is None:
                 return
 
             self.nodes_to_add.append(new_node)
             self.node_name_to_graph_name[new_node.name] = self.this_graph_name
 
             if einsum_node is not None:
                 unique_index = einsum_node.input[0]
                 new_edge = "edge_modified_" + unique_index
                 shape_tensor = helper.make_tensor(name="shape_modified_tensor" + unique_index,
                                                   data_type=TensorProto.INT64,
                                                   dims=[4],
-                                                  vals=np.int64([0, 0, num_heads,
-                                                                 int(hidden_size / num_heads)]).tobytes(),
+                                                  vals=np.int64([0, 0, q_num_heads,
+                                                                 int(q_hidden_size / q_num_heads)]).tobytes(),
                                                   raw=True)
                 self.model.add_initializer(shape_tensor, self.this_graph_name)
                 self.model.add_node(
                     helper.make_node("Reshape", [attention_last_node.output[0], shape_tensor.name], [new_edge],
                                      "reshape_modified_" + unique_index), self.this_graph_name)
                 einsum_node.input[0] = new_edge
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_base.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_base.py`

 * *Files 9% similar despite different names*

```diff
@@ -21,14 +21,16 @@
         self.description: str = f"{fused_op_type}({description})" if description else fused_op_type
         self.model: OnnxModel = model
         self.nodes_to_remove: List = []
         self.nodes_to_add: List = []
         self.prune_graph: bool = False
         self.node_name_to_graph_name: dict = {}
         self.this_graph_name: str = None
+        # It is optional that subclass updates fused_count since we will also check nodes_to_add to get counter.
+        self.fused_count: int = 0
 
     def apply(self):
         logger.debug(f"start {self.description} fusion...")
         input_name_to_nodes = self.model.input_name_to_nodes()
         output_name_to_node = self.model.output_name_to_node()
 
         # This assumes that two search ops will not be fused at same time!
@@ -37,15 +39,15 @@
                 graph = self.model.get_graph_by_node(node)
                 if graph is None:
                     raise Exception("Can not find node in any graphs")
                 self.this_graph_name = graph.name
                 self.fuse(node, input_name_to_nodes, output_name_to_node)
 
         op_list = [node.op_type for node in self.nodes_to_add]
-        count = op_list.count(self.fused_op_type)
+        count = max(self.fused_count, op_list.count(self.fused_op_type))
         if count > 0:
             logger.info(f"Fused {self.description} count: {count}")
 
         self.model.remove_nodes(self.nodes_to_remove)
         self.model.add_nodes(self.nodes_to_add, self.node_name_to_graph_name)
 
         if self.prune_graph:
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_biasgelu.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_biasgelu.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_fastgelu.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_fastgelu.py`

 * *Files 5% similar despite different names*

```diff
@@ -58,58 +58,63 @@
         if mul_half is None:
             return
 
         i = self.model.find_constant_input(mul_half, 0.5)
         if i < 0:
             return
 
+        root_input = mul_half.input[0 if i == 1 else 1]
+
+        #root_node could be None when root_input is graph input
         root_node = self.model.get_parent(mul_half, 0 if i == 1 else 1, output_name_to_node)
-        if root_node is None:
-            return
 
         mul_before_tanh = self.model.match_parent(tanh_node, 'Mul', 0, output_name_to_node)
         if mul_before_tanh is None:
             return
 
         i = self.model.find_constant_input(mul_before_tanh, 0.7978, delta=0.0001)
         if i < 0:
             return
 
         add_before_tanh = self.model.match_parent(mul_before_tanh, 'Add', 0 if i == 1 else 1, output_name_to_node)
         if add_before_tanh is None:
             return
 
-        mul_after_pow = self.model.match_parent(add_before_tanh, 'Mul', None, output_name_to_node, exclude=[root_node])
+        mul_after_pow = self.model.match_parent(add_before_tanh,
+                                                'Mul',
+                                                None,
+                                                output_name_to_node,
+                                                exclude=[root_node] if root_node else [])
         if mul_after_pow is None:
             return
 
         i = self.model.find_constant_input(mul_after_pow, 0.0447, delta=0.0001)
         if i < 0:
             return
 
         pow = self.model.match_parent(mul_after_pow, 'Pow', 0 if i == 1 else 1, output_name_to_node)
         if pow is None:
             return
 
         if not self.model.has_constant_input(pow, 3.0):
             return
 
-        if pow.input[0] != root_node.output[0]:
+        if pow.input[0] != root_input:
             return
 
         subgraph_nodes = [
             mul_after_tanh, mul_half, add_after_tanh, tanh_node, mul_before_tanh, add_before_tanh, mul_after_pow, pow
         ]
         if not self.model.is_safe_to_fuse_nodes(subgraph_nodes, [mul_after_tanh.output[0]], input_name_to_nodes,
                                                 output_name_to_node):
             return
 
         self.nodes_to_remove.extend(subgraph_nodes)
         fused_node = helper.make_node('FastGelu',
-                                      inputs=[root_node.output[0]],
+                                      inputs=[root_input],
                                       outputs=mul_after_tanh.output,
                                       name=self.model.create_node_name('FastGelu'))
         fused_node.domain = "com.microsoft"
         self.nodes_to_add.append(fused_node)
         self.node_name_to_graph_name[fused_node.name] = self.this_graph_name
         return True
 
@@ -262,15 +267,15 @@
         j = self.model.find_constant_input(add_1, 1.0)
         if j < 0:
             return
 
         mul_7978 = self.model.match_parent(mul_before_tanh, 'Mul', None, output_name_to_node)
         if mul_7978 is None:
             return
-        k = self.model.find_constant_input(mul_7978, 0.79788456)
+        k = self.model.find_constant_input(mul_7978, 0.7978, delta=0.0001)
         if k < 0:
             return
         if mul_7978.input[0 if k == 1 else 1] != root_input:
             return
 
         mul_before_add_1 = self.model.match_parent(add_1, 'Mul', 0 if j == 1 else 1, output_name_to_node)
         if mul_before_add_1 is None:
@@ -282,15 +287,15 @@
             another = 0
         else:
             return
 
         mul_0447 = self.model.match_parent(mul_before_add_1, 'Mul', another, output_name_to_node)
         if mul_0447 is None:
             return
-        m = self.model.find_constant_input(mul_0447, 0.044715)
+        m = self.model.find_constant_input(mul_0447, 0.0447, delta=0.0001)
         if m < 0:
             return
 
         if mul_0447.input[0 if m == 1 else 1] != root_input:
             return
 
         subgraph_nodes = [
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_gelu.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_gelu.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_gelu_approximation.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_gelu_approximation.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_gpt_attention.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_gpt_attention.py`

 * *Files 26% similar despite different names*

```diff
@@ -20,18 +20,19 @@
     def __init__(self, model: OnnxModel, num_heads: int):
         super().__init__(model, "Attention", "LayerNormalization", "with past")
         # TODO: detect num_heads from graph like FusionAttention
         self.num_heads = num_heads
         self.utils = FusionUtils(model)
         self.casted_attention_mask = {}  # map from name of attention mask to the name that casted to int32
 
-    def create_attention_node(self, gemm, gemm_qkv, past, present, input, output, mask, is_unidirectional):
+    def create_attention_node(self, fc_weight, fc_bias, gemm_qkv, past, present, input, output, mask,
+                              is_unidirectional):
         attention_node_name = self.model.create_node_name('GptAttention')
         attention_node = helper.make_node('Attention',
-                                          inputs=[input, gemm.input[1], gemm.input[2], mask, past],
+                                          inputs=[input, fc_weight, fc_bias, mask, past],
                                           outputs=[attention_node_name + "_output", present],
                                           name=attention_node_name)
         attention_node.domain = "com.microsoft"
         attention_node.attribute.extend([
             helper.make_attribute("num_heads", self.num_heads),
             helper.make_attribute("unidirectional", 1 if is_unidirectional else 0)
         ])
@@ -46,14 +47,145 @@
                                     outputs=[output],
                                     name=attention_node_name + "_add")
         self.nodes_to_add.extend([attention_node, matmul_node, add_node])
         self.node_name_to_graph_name[attention_node.name] = self.this_graph_name
         self.node_name_to_graph_name[matmul_node.name] = self.this_graph_name
         self.node_name_to_graph_name[add_node.name] = self.this_graph_name
 
+    def match_past_pattern_1(self, concat_k, concat_v, output_name_to_node):
+        # Pattern 1:
+        #                      {past}
+        #                    /        \
+        #                   /          \
+        #    Gather(axes=0, indices=0)  Gather(indices=1)
+        #      |                          |
+        #    Transpose (perm=0,1,3,2)     |
+        #      |                          |
+        #  Concat_k                     Concat_v
+        #      |                        /
+        #  Transpose (perm=0,1,3,2)    /
+        #      |                      /
+        #  Unsqueeze        Unsqueeze
+        #        \        /
+        #         \      /
+        #           Concat
+        #             |
+        #         {present}
+        gather = self.model.get_parent(concat_v, 0, output_name_to_node)
+        if gather.op_type != 'Gather':
+            logger.debug("match_past_pattern_1: expect Gather for past")
+            return None
+
+        if not self.model.find_constant_input(gather, 1) == 1:
+            logger.debug("match_past_pattern_1: expect indices=1 for Gather of past")
+            return None
+        past = gather.input[0]
+
+        past_k_nodes = self.model.match_parent_path(concat_k, ['Transpose', 'Gather'], [0, 0])
+        if past_k_nodes is None:
+            logger.debug("match_past_pattern_1: failed match Transpose and Gather")
+            return None
+
+        gather_past_k = past_k_nodes[-1]
+        if not self.model.find_constant_input(gather_past_k, 0) == 1:
+            logger.debug("match_past_pattern_1: expect indices=0 for Gather k of past")
+            return None
+        past_k = gather_past_k.input[0]
+        if past != past_k:
+            logger.debug("match_past_pattern_1: expect past to be same")
+            return None
+
+        return past
+
+    def match_past_pattern_2(self, concat_k, concat_v, output_name_to_node):
+        # Pattern 2:
+        #      Split (QKV)
+        #      / |   |
+        #     /  |   +----------------------+
+        #        |                          |
+        #        |         {past}           |
+        #        |           |              |
+        #      Reshape     Split         Reshape
+        #        |         /    \           |
+        # Transpose_k  Squeeze  Squeeze  Transpose_v
+        #        |      |        \        /
+        #        +------|---+     \      /
+        #               |   |      \    /
+        #              Concat_k   Concat_v
+        #               |            |
+        #          Unsqueeze    Unsqueeze
+        #                \       /
+        #                 Concat
+        #                   |
+        #               {present}
+        #
+        squeeze = self.model.get_parent(concat_v, 0, output_name_to_node)
+        if squeeze.op_type != 'Squeeze':
+            logger.debug("match_past_pattern_2: expect Squeeze as parent of concat_v")
+            return None
+
+        split = self.model.get_parent(squeeze, 0, output_name_to_node)
+        if split.op_type != "Split":
+            logger.debug("match_past_pattern_2: expect Split for past path")
+            return None
+
+        opset_version = self.model.get_opset_version()
+        if opset_version < 13:
+            if not FusionUtils.check_node_attribute(squeeze, 'axes', [0]):
+                logger.debug("match_past_pattern_2: axes != [0] for Squeeze in past path")
+                return None
+
+            if not FusionUtils.check_node_attribute(split, 'split', [1, 1]):
+                logger.debug("match_past_pattern_2: split != [1, 1] for Split in past path")
+                return None
+        else:
+            if not self.utils.check_node_input_value(squeeze, 1, [0]):
+                logger.debug("match_past_pattern_2: axes != [0] for Squeeze in past path")
+                return None
+
+            if not self.utils.check_node_input_value(split, 1, [1, 1]):
+                logger.debug("match_past_pattern_2: split != [1, 1] for Split in past path")
+                return None
+
+        if not FusionUtils.check_node_attribute(split, 'axis', 0, default_value=0):
+            logger.debug("match_past_pattern_2: attribute axis of Split are not expected in past path")
+            return None
+        past = split.input[0]
+
+        past_k_nodes = self.model.match_parent_path(concat_k, ['Squeeze', 'Split'], [0, 0])
+        if past_k_nodes is None:
+            logger.debug("match_past_pattern_2: failed to match past_k_nodes path")
+            return None
+        past_k = past_k_nodes[-1].input[0]
+
+        if past != past_k:
+            logger.info("match_past_pattern_2: expect past to be same")
+            return None
+
+        return past
+
+    def match_present(self, concat_v, input_name_to_nodes):
+        unsqueeze_present_v = self.model.find_first_child_by_type(concat_v,
+                                                                  'Unsqueeze',
+                                                                  input_name_to_nodes,
+                                                                  recursive=False)
+        if not unsqueeze_present_v:
+            logger.info("expect unsqueeze for present")
+            return None
+        concat_present = self.model.find_first_child_by_type(unsqueeze_present_v,
+                                                             'Concat',
+                                                             input_name_to_nodes,
+                                                             recursive=False)
+        if not concat_present:
+            logger.info("expect concat for present")
+            return None
+
+        present = concat_present.output[0]
+        return present
+
     def fuse(self, normalize_node, input_name_to_nodes, output_name_to_node):
         past = None
         present = None
         return_indice = []
         qkv_nodes = self.model.match_parent_path(
             normalize_node,
             ['Add', 'Reshape', 'Gemm', 'Reshape', 'Reshape', 'Transpose', 'MatMul'],
@@ -63,70 +195,45 @@
             ) # yapf: disable
         if qkv_nodes is None:
             return
         (add_qkv, reshape_qkv, gemm_qkv, reshape_1, reshape_2, transpose_qkv, matmul_qkv) = qkv_nodes
 
         another_input = add_qkv.input[1 - return_indice[0]]
 
-        v_nodes = self.model.match_parent_path(
-            matmul_qkv,
-            ['Concat', 'Transpose', 'Reshape', 'Split', 'Reshape', 'Gemm', 'Reshape'],
-            [1,        1,            0,         0,       0,         0,      0]) # yapf: disable
+        v_nodes = self.model.match_parent_path(matmul_qkv, ['Concat', 'Transpose', 'Reshape', 'Split'], [1, 1, 0, 0])
         if v_nodes is None:
             logger.debug("fuse_attention: failed to match v path")
             return
-        (concat_v, transpose_v, reshape_v, split_v, reshape_after_gemm, gemm, reshape_before_gemm) = v_nodes
+        (concat_v, transpose_v, reshape_v, split_fc) = v_nodes
 
-        #      concat <-- Gather(indices=1) <-- past
-        #        |
-        #      unsqueeze
-        #        |
-        #    concat  -->  present
-        gather_v = self.model.get_parent(concat_v, 0, output_name_to_node)
-        if gather_v.op_type != 'Gather':
-            logger.info("expect Gather for past")
-            return
-        if not self.model.find_constant_input(gather_v, 1) == 1:
-            logger.info("expect indices=1 for Gather of past")
-            return
-        past = gather_v.input[0]
-        if not self.model.find_graph_input(past):
-            logger.info("expect past to be graph input")
-            return
-        unsqueeze_present_v = self.model.find_first_child_by_type(concat_v,
-                                                                  'Unsqueeze',
-                                                                  input_name_to_nodes,
-                                                                  recursive=False)
-        if not unsqueeze_present_v:
-            logger.info("expect unsqueeze for present")
-            return
-        concat_present = self.model.find_first_child_by_type(unsqueeze_present_v,
-                                                             'Concat',
-                                                             input_name_to_nodes,
-                                                             recursive=False)
-        if not concat_present:
-            logger.info("expect concat for present")
-            return
-        present = concat_present.output[0]
-        if not self.model.find_graph_output(present):
-            logger.info("expect present to be graph input")
-            return
+        fc_nodes = self.model.match_parent_path(split_fc, ['Reshape', 'Gemm', 'Reshape', 'LayerNormalization'],
+                                                [0, 0, 0, 0], output_name_to_node)
+        if fc_nodes is None:
+            fc_nodes = self.model.match_parent_path(split_fc, ['Add', 'MatMul', 'LayerNormalization'], [0, None, 0],
+                                                    output_name_to_node)
+            if fc_nodes is None:
+                logger.debug("fuse_attention: failed to match fc path")
+                return
+            fc_weight = fc_nodes[1].input[1]
+            i, _ = self.model.get_constant_input(fc_nodes[0])
+            fc_bias = fc_nodes[0].input[i]
+        else:
+            fc_weight = fc_nodes[1].input[1]
+            fc_bias = fc_nodes[1].input[2]
 
-        layernorm_before_attention = self.model.get_parent(reshape_before_gemm, 0, output_name_to_node)
-        if layernorm_before_attention is None or layernorm_before_attention.op_type != 'LayerNormalization':
-            logger.debug(f"failed to get layernorm before gemm. Got {layernorm_before_attention.op_type}")
-            return
+        layernorm_before_attention = fc_nodes[-1]
 
         if not another_input in layernorm_before_attention.input:
             logger.debug("Add and LayerNormalization shall have one same input")
             return
 
         is_unidirectional = True
         slice_mask = None
         input_mask_nodes = None
+        concat_k_to_match = None
         qk_nodes = self.model.match_parent_path(matmul_qkv, ['Softmax', 'Sub', 'Mul', 'Div', 'MatMul'], [0, 0, 0, 0, 0])
         if qk_nodes is not None:
             (softmax_qk, sub_qk, mul_qk, div_qk, matmul_qk) = qk_nodes
             mask_nodes = self.model.match_parent_path(
                 sub_qk,
                 ['Mul', 'Sub', 'Slice', 'Slice', 'Unsqueeze', 'Sub', 'Squeeze', 'Slice', 'Shape', 'Div'],
                 [1,      0,     1,       0,       1,           0,     0,         0,       0,       0])  # yapf: disable
@@ -139,46 +246,59 @@
             if div_qk != div_mask:
                 logger.debug("fuse_attention: skip since div_qk != div_mask")
                 return
         else:
             # New pattern for gpt2 from PyTorch 1.5.0 and Transformers 2.9.0.
             i, qk_nodes, _ = self.model.match_parent_paths(
                 matmul_qkv, [(['Softmax', 'Where', 'Div', 'MatMul'], [0, 0, 1, 0]),
-                             (['Softmax', 'Add', 'Where', 'Div', 'MatMul'], [0, 0, 0, 1, 0])], output_name_to_node)
+                             (['Softmax', 'Add', 'Where', 'Div', 'MatMul'], [0, 0, None, 1, 0])], output_name_to_node)
             if qk_nodes is None:
                 logger.debug("fuse_attention: failed to match qk nodes")
                 return
 
             where_qk = qk_nodes[-3]
             div_qk = qk_nodes[-2]
             matmul_qk = qk_nodes[-1]
 
             if i == 1:
                 add_qk = qk_nodes[1]
                 _, input_mask_nodes, _ = self.model.match_parent_paths(
-                    add_qk, [(['Mul', 'Sub', 'Cast', 'Unsqueeze', 'Unsqueeze', 'Reshape'], [1, 0, 1, 0, 0, 0]),
-                             (['Mul', 'Sub', 'Unsqueeze', 'Unsqueeze', 'Reshape'], [1, 0, 1, 0, 0])],
-                    output_name_to_node)
+                    add_qk,
+                    [
+                        (['Mul', 'Sub', 'Cast', 'Unsqueeze', 'Unsqueeze', 'Reshape'], [None, 0, 1, 0, 0, 0]),
+                        (['Mul', 'Sub', 'Unsqueeze', 'Unsqueeze', 'Reshape'], [None, 0, 1, 0, 0]),
+                        (['Mul', 'Sub', 'Unsqueeze', 'Unsqueeze'], [None, 0, 1, 0]),  # useless cast and reshape are removed.
+                    ],
+                    output_name_to_node)  # yapf: disable
                 if input_mask_nodes is None:
                     logger.debug("fuse_attention: failed to match input attention mask path")
                     return
 
             mask_nodes = self.model.match_parent_path(
                 where_qk,
-                ['Cast', 'Slice', 'Slice', 'Unsqueeze', 'Sub', 'Squeeze', 'Slice', 'Shape', 'Div'],
-                [ 0,     0,       0,       1,           0,     0,         0,       0,       0])  # yapf: disable
+                ['Cast', 'Slice', 'Slice', 'Unsqueeze', 'Sub', 'Squeeze', 'Slice', 'Shape'],
+                [ 0,     0,       0,       1,           0,     0,         0,       0],
+                output_name_to_node)  # yapf: disable
             if mask_nodes is None:
+                # TODO: match mask path for GPT2LMHeadModel_BeamSearchStep.
                 logger.debug("fuse_attention: failed to match mask path")
                 return
-            div_mask = mask_nodes[-1]
+
             slice_mask = mask_nodes[2]
 
-            if div_qk != div_mask:
-                logger.debug("fuse_attention: skip since div_qk != div_mask")
-                return
+            div_or_concat = self.model.get_parent(mask_nodes[-1], 0, output_name_to_node)
+            if div_or_concat.op_type == "Div":
+                div_mask = div_or_concat
+                if div_qk != div_mask:
+                    logger.debug("fuse_attention: skip since div_qk != div_mask")
+                    return
+            elif div_or_concat.op_type == "Concat":
+                concat_k_to_match = div_or_concat
+            else:
+                logger.debug("fuse_attention: failed to match mask path")
 
         # Validate that the mask data is either lower triangular (unidirectional) or all ones
         mask_data = numpy_helper.to_array(self.model.get_initializer(slice_mask.input[0]))
         if not (len(mask_data.shape) == 4 and mask_data.shape[:2] == (1, 1)
                 and mask_data.shape[2] == mask_data.shape[3]):
             logger.debug("fuse_attention: skip since mask shape is not 1x1xWxW")
             return
@@ -189,59 +309,67 @@
             return
 
         q_nodes = self.model.match_parent_path(matmul_qk, ['Transpose', 'Reshape', 'Split'], [0, 0, 0])
         if q_nodes is None:
             logger.debug("fuse_attention: failed to match q path")
             return
         (transpose_q, reshape_q, split_q) = q_nodes
-        if split_v != split_q:
-            logger.debug("fuse_attention: skip since split_v != split_q")
+        if split_fc != split_q:
+            logger.debug("fuse_attention: skip since split_fc != split_q")
             return
 
         k_nodes = self.model.match_parent_path(matmul_qk, ['Concat', 'Transpose', 'Reshape', 'Split'], [1, 1, 0, 0])
         if k_nodes is None:
-            logger.debug("fuse_attention: failed to match k path")
-            return
-        (concat_k, transpose_k, reshape_k, split_k) = k_nodes
-        if split_v != split_k:
-            logger.debug("fuse_attention: skip since split_v != split_k")
+            # This pattern is from pytorch 1.7.1 and transformers 4.6.1
+            k_nodes = self.model.match_parent_path(matmul_qk, ['Transpose', 'Concat', 'Transpose', 'Reshape', 'Split'],
+                                                   [1, 0, 1, 0, 0])
+            if k_nodes is None:
+                logger.debug("fuse_attention: failed to match k path")
+                return
+            else:
+                (_, concat_k, transpose_k, reshape_k, split_k) = k_nodes
+        else:
+            (concat_k, transpose_k, reshape_k, split_k) = k_nodes
+        if split_fc != split_k:
+            logger.debug("fuse_attention: skip since split_fc != split_k")
             return
 
-        #     concat_k <-- Transpose (perm=0,1,3,2) <-- Gather(axes=0, indices=0) <-- past
-        #        |
-        #     Transpose (perm=0,1,3,2)
-        #        |
-        #      unsqueeze
-        #        |
-        #    concat  -->  present
-        past_k_nodes = self.model.match_parent_path(concat_k, ['Transpose', 'Gather'], [0, 0])
-        if past_k_nodes is None:
-            logger.debug("fuse_attention: failed to match past_k_nodes path")
-            return
-
-        gather_past_k = past_k_nodes[-1]
-        if not self.model.find_constant_input(gather_past_k, 0) == 1:
-            logger.info("expect indices=0 for Gather k of past")
-            return
-        past_k = gather_past_k.input[0]
-        if past != past_k:
-            logger.info("expect past to be same")
+        if concat_k_to_match and concat_k != concat_k_to_match:
+            logger.debug("fuse_attention: skip since concat_k != concat_k_to_match")
             return
 
         attention_mask_input_name = ''
         if input_mask_nodes is not None:
             input_name = input_mask_nodes[-1].input[0]
             if input_name in self.casted_attention_mask:
                 attention_mask_input_name = self.casted_attention_mask[input_name]
             elif self.model.find_graph_input(input_name):
                 casted, attention_mask_input_name = self.utils.cast_graph_input_to_int32(input_name)
                 self.casted_attention_mask[input_name] = attention_mask_input_name
             else:
                 attention_mask_input_name, cast_node = self.utils.cast_input_to_int32(input_name)
                 self.casted_attention_mask[input_name] = attention_mask_input_name
 
-        self.create_attention_node(gemm, gemm_qkv, past, present, layernorm_before_attention.output[0],
+        # Match past and present paths
+        past = self.match_past_pattern_1(concat_k, concat_v, output_name_to_node) or \
+               self.match_past_pattern_2(concat_k, concat_v, output_name_to_node)
+        if past is None:
+            logger.info("fuse_attention: failed to match past path")
+            return
+        if not self.model.find_graph_input(past):
+            logger.debug("past is not graph input.")
+            # For GPT2LMHeadModel_BeamSearchStep, there is an extra Gather node to select beam index so it is not graph input.
+
+        present = self.match_present(concat_v, input_name_to_nodes)
+        if present is None:
+            logger.info("fuse_attention: failed to match present path")
+            return
+        if not self.model.find_graph_output(present):
+            logger.info("expect present to be graph output")
+            return
+
+        self.create_attention_node(fc_weight, fc_bias, gemm_qkv, past, present, layernorm_before_attention.output[0],
                                    reshape_qkv.output[0], attention_mask_input_name, is_unidirectional)
 
         # we rely on prune_graph() to clean old subgraph nodes:
         # qk_nodes + q_nodes + k_nodes + v_nodes + mask_nodes + [reshape_qkv, transpose_qkv, matmul_qkv]
         self.prune_graph = True
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_gpt_attention_no_past.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_gpt_attention_no_past.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_layernorm.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_layernorm.py`

 * *Files 3% similar despite different names*

```diff
@@ -37,23 +37,21 @@
               |                      |
               +----------------------+
         """
         children = self.model.get_children(node, input_name_to_nodes)
         if len(children) == 0 or len(children) > 2:
             return
 
-        parent = self.model.get_parent(node, 0, output_name_to_node)
-        if parent is None:
-            return
+        root_input = node.input[0]
 
-        if children[0].op_type != 'Sub' or self.model.get_parent(children[0], 0, output_name_to_node) != parent:
+        if children[0].op_type != 'Sub' or children[0].input[0] != root_input:
             return
 
         if len(children) == 2:
-            if children[1].op_type != 'Sub' or self.model.get_parent(children[1], 0, output_name_to_node) != parent:
+            if children[1].op_type != 'Sub' or children[1].input[0] != root_input:
                 return
 
         div_node = None
         for child in children:
             div_node = self.model.find_first_child_by_type(child, 'Div', input_name_to_nodes, recursive=False)
             if div_node is not None:
                 break
@@ -216,11 +214,11 @@
         weight_input = mul_node_1.input[1]
         bias_input = sub_node_0.input[0]
 
         #TODO: add epsilon attribute
         fused_node = helper.make_node('LayerNormalization',
                                       inputs=[mul_node_3.input[0], weight_input, bias_input],
                                       outputs=[node.output[0]],
-                                      name=self.model.create_node_name("LayerNormalization",
-                                                                       name_prefix="SkipLayerNorm"))
+                                      name=self.model.create_node_name("LayerNormalization", name_prefix="LayerNorm"))
         fused_node.attribute.extend([helper.make_attribute("epsilon", float(epsilon))])
         self.nodes_to_add.append(fused_node)
+        self.node_name_to_graph_name[fused_node.name] = self.this_graph_name
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_reshape.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_reshape.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,25 +1,42 @@
 #-------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
 #--------------------------------------------------------------------------
 
+from fusion_base import Fusion
 from logging import getLogger
+import numpy as np
 from onnx import helper, numpy_helper, TensorProto
 from onnx_model import OnnxModel
-from fusion_base import Fusion
-import numpy as np
 
 logger = getLogger(__name__)
 
 
 class FusionReshape(Fusion):
     def __init__(self, model: OnnxModel):
         super().__init__(model, "Reshape", "Reshape")
 
+    def replace_reshape_node(self, shape, reshape_node, concat_node):
+        shape_value = np.asarray(shape, dtype=np.int64)
+        constant_shape_name = self.model.create_node_name('Constant', 'constant_shape')
+        new_node = helper.make_node('Constant',
+                                    inputs=[],
+                                    outputs=[constant_shape_name],
+                                    value=helper.make_tensor(name='const_tensor',
+                                                             data_type=TensorProto.INT64,
+                                                             dims=shape_value.shape,
+                                                             vals=bytes(shape_value),
+                                                             raw=True))
+        reshape_node.input[1] = constant_shape_name
+        reshape_node.name = self.model.create_node_name('Reshape', 'Reshape_Fuse')
+        self.nodes_to_remove.extend([concat_node])
+        self.nodes_to_add.append(new_node)
+        self.node_name_to_graph_name[new_node.name] = self.this_graph_name
+
     def fuse(self, reshape_node, input_name_to_nodes, output_name_to_node):
         if reshape_node.input[1] not in output_name_to_node:
             return
 
         concat_node = output_name_to_node[reshape_node.input[1]]
         if concat_node.op_type != 'Concat' or len(concat_node.input) < 3 or len(concat_node.input) > 4:
             return
@@ -113,27 +130,13 @@
         for shape_node in shape_nodes:
             if shape_node.input[0] != root_input:
                 same_shape_input = False
 
         if not same_shape_input:
             return
 
-        shape_value = np.asarray(shape, dtype=np.int64)
+        self.replace_reshape_node(shape, reshape_node, concat_node)
 
-        constant_shape_name = self.model.create_node_name('Constant', 'constant_shape')
-        new_node = helper.make_node('Constant',
-                                    inputs=[],
-                                    outputs=[constant_shape_name],
-                                    value=helper.make_tensor(name='const_tensor',
-                                                             data_type=TensorProto.INT64,
-                                                             dims=shape_value.shape,
-                                                             vals=bytes(shape_value),
-                                                             raw=True))
-        reshape_node.input[1] = constant_shape_name
-        reshape_node.name = self.model.create_node_name('Reshape', 'Reshape_Fuse')
-        self.nodes_to_remove.extend([concat_node])
         self.nodes_to_remove.extend(path0)
         self.nodes_to_remove.extend(path1)
         self.nodes_to_remove.extend(path2)
         self.nodes_to_remove.extend(path3)
-        self.nodes_to_add.append(new_node)
-        self.node_name_to_graph_name[new_node.name] = self.this_graph_name
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_skiplayernorm.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_skiplayernorm.py`

 * *Files 3% similar despite different names*

```diff
@@ -15,15 +15,16 @@
 class FusionSkipLayerNormalization(Fusion):
     """
     Fuse Add + LayerNormalization into one node: SkipLayerNormalization
     Note: This fusion does not check the input shape of Add and LayerNormalization.
     """
     def __init__(self, model: OnnxModel):
         super().__init__(model, "SkipLayerNormalization", "LayerNormalization")
-        self.shape_infer_helper = self.model.infer_runtime_shape({"batch_size": 4, "seq_len": 7})
+        # Update shape inference is needed since other fusions might add new edge which does not have shape info yet.
+        self.shape_infer_helper = self.model.infer_runtime_shape({"batch_size": 4, "seq_len": 7}, update=True)
 
     def fuse(self, node, input_name_to_nodes, output_name_to_node):
         add = self.model.get_parent(node, 0, output_name_to_node)
 
         # In some models there is input_ids->gather->add->LayerNorm and one of input of the
         # add node is initializer with fixed shape which should not be fused into SkipLayerNorm
         if add is None:
@@ -35,14 +36,16 @@
 
         # The number of input node of add should be 2
         if len(self.model.get_parents(add)) != 2:
             return
 
         if self.shape_infer_helper is not None:
             if not self.shape_infer_helper.compare_shape(add.input[0], add.input[1]):
+                logger.debug(
+                    f"skip skiplayernorm fusion since shape of inputs ({add.input[0]}, {add.input[1]}) are not same")
                 return
         else:
             # shape_infer_helper can not handle subgraphs. Current work around is to disable skiplayernorm fusion
             # longterm todo: support subgraph in symbolic_shape_infer or support add broadcasting in skiplayernorm op
             logger.warning(
                 "symbolic shape infer failed. it's safe to ignore this message if there is no issue with optimized model"
             )
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/gpt2_beamsearch_helper.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/gpt2_beamsearch_helper.py`

 * *Files 2% similar despite different names*

```diff
@@ -606,14 +606,26 @@
         if has_position_ids:
             dynamic_axes["prev_step_results"] = {0: "batch_size", 1: "total_seq_len"}
             input_names.append("prev_step_results")
         dynamic_axes["prev_step_scores"] = {0: "batch_size", 1: "total_seq_len"}
         input_names.append("prev_step_scores")
         input_names.extend(past_names)
 
+        # add dynamic output axes
+        present_axes = {1: 'batch_size', 3: 'cur_seq_len'}
+        dynamic_axes["last_state"] = {0: 'batch_size', 1: 'beam_size'}
+        for i in range(num_layer):
+            dynamic_axes["present_" + str(i)] = present_axes
+
+        dynamic_axes["output_selected_indices"] = {0: "batch_size", 1: "'beam_size_or_1'"}
+        dynamic_axes["output_log_probs"] = {0: "batch_size", 1: "'beam_size'"}
+        dynamic_axes["output_unfinished_sents"] = {0: "batch_size", 1: "'beam_size'"}
+        dynamic_axes["current_step_results"] = {0: "beam_size_or_1", 1: "total_seq_len"}
+        dynamic_axes["current_step_scores"] = {0: "beam_size_or_1", 1: "total_seq_len"}
+
         logger.info(
             f"Shapes: input_ids={dummy_inputs.input_ids.shape} past={dummy_inputs.past[0].shape} output={outputs[0].shape} present={outputs[1][0].shape}"
         )
 
         Path(onnx_model_path).parent.mkdir(parents=True, exist_ok=True)
 
         torch.onnx.export(
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/gpt2_beamsearch_tester.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/gpt2_beamsearch_tester.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/gpt2_helper.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/gpt2_helper.py`

 * *Files 13% similar despite different names*

```diff
@@ -3,19 +3,20 @@
 # Licensed under the MIT License.  See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
 # This script helps onnx conversion and validation for GPT2 model with past state.
 import os
 import logging
 import torch
-import onnx
+import shutil
 import random
 import numpy
 import time
 import re
+import pickle
 from pathlib import Path
 from typing import List, Dict, Tuple, Union
 from transformers import GPT2Model, GPT2LMHeadModel, GPT2Config, TFGPT2Model
 from benchmark_helper import Precision
 
 logger = logging.getLogger(__name__)
 
@@ -29,34 +30,37 @@
     """
     def __init__(self, config):
         super().__init__(config)
 
     def forward(self, input_ids):
         return super().forward(input_ids, use_cache=False, return_dict=False)
 
+
 class TFGPT2ModelNoPastState(TFGPT2Model):
     """ Here we wrap a class to disable past state output.
     """
     def __init__(self, config):
         config.use_cache = False
         super().__init__(config)
 
     def forward(self, input_ids):
         return super().call(input_ids, use_cache=False)
 
+
 class MyGPT2Model(GPT2Model):
     """ Here we wrap a class for Onnx model conversion for GPT2Model with past state.
     """
     def __init__(self, config):
         super().__init__(config)
 
     @staticmethod
     def post_process(result, num_layer):
         if isinstance(result[1][0], tuple) or isinstance(result[1][0], list):
-            assert len(result[1]) == num_layer and len(result[1][0]) == 2 #and len(result[1][0][0].shape) == 4 and result[1][0][0].shape == result[1][0][1].shape
+            assert len(result[1]) == num_layer and len(result[1][0]) == 2
+            #assert len(result[1][0][0].shape) == 4 and result[1][0][0].shape == result[1][0][1].shape
             present = []
             for i in range(num_layer):
                 # Since transformers v4.*, past key and values are separated outputs.
                 # Here we concate them into one tensor to be compatible with Attention operator.
                 present.append(torch.cat((result[1][i][0].unsqueeze(0), result[1][i][1].unsqueeze(0)), dim=0))
             return (result[0], tuple(present))
 
@@ -92,15 +96,17 @@
         When you always use batch_size=1 in inference, there is no padding in inputs. In such case, position_ids
         and attention_mask need no be in inputs.
     """
     def __init__(self, config):
         super().__init__(config)
 
     def forward(self, input_ids, *past):
-        return super().forward(input_ids, past_key_values=past)
+        result = super().forward(input_ids, past_key_values=past, return_dict=False)
+
+        return MyGPT2Model.post_process(result, self.config.n_layer)
 
 
 # Maps model class name to a tuple of model class, name of first output and use padding or not
 MODEL_CLASSES = {
     'GPT2LMHeadModel': (MyGPT2LMHeadModel, 'logits', True),
     'GPT2LMHeadModel_NoPadding': (MyGPT2LMHeadModel_NoPadding, 'logits', False),
     'GPT2Model': (MyGPT2Model, 'last_state', True),
@@ -147,15 +153,15 @@
                          has_attention_mask: bool = True) -> Gpt2Inputs:
         """ Create random inputs for GPT2 model.
         Returns torch tensors of input_ids, position_ids, attention_mask and a list of past state tensors.
         """
         float_type = torch.float16 if float16 else torch.float32
         past_shape = [2, batch_size, num_attention_heads, past_sequence_length, int(hidden_size / num_attention_heads)]
 
-        past = [torch.rand(past_shape, dtype=float_type, device=device) for _ in range(num_layer)]
+        past = [(torch.rand(past_shape, dtype=float_type, device=device) * 2.0 - 1.0) for _ in range(num_layer)]
         input_ids = torch.randint(low=0,
                                   high=vocab_size - 1,
                                   size=(batch_size, sequence_length),
                                   dtype=torch.int64,
                                   device=device)
 
         attention_mask = None
@@ -255,14 +261,61 @@
         if not is_all_close:
             max_abs_diff = Gpt2Helper.diff_outputs(torch_outputs, ort_outputs)
             logger.info(f'PyTorch and OnnxRuntime results are not all close: max_abs_diff={max_abs_diff:.5f}')
 
         return is_all_close
 
     @staticmethod
+    def compare_outputs_v2(torch_outputs, ort_outputs, atol=1e-06):
+        """Compare outputs from PyTorch and OnnxRuntime
+
+        Args:
+            torch_outputs (Tuple[Torch.Tensor]): PyTorch model output
+            ort_outputs (List[numpy.ndarray]): OnnxRuntime output
+            atol (float, optional): Absolute tollerance. Defaults to 1e-06.
+
+        Returns:
+            is_all_close(bool): whether all elements are close.
+            max_abs_diff(float): maximum absolute difference.
+            messages(str): a list of debug message for each output
+        """
+        is_all_close = True
+        is_top1_matched = False
+        max_diffs = []
+        messages = []
+        for i in range(len(ort_outputs)):
+            ort_output = ort_outputs[i]
+            torch_output = (torch_outputs[0] if i == 0 else torch_outputs[1][i - 1]).cpu().numpy()
+            is_close = numpy.allclose(ort_output, torch_output, atol=atol, rtol=0)
+            max_diffs.append(numpy.amax(numpy.abs(torch_output - ort_output)))
+            is_all_close = is_all_close and is_close
+
+            if numpy.isnan(torch_output).any():
+                logger.debug(f'PyTorch output {i} has nan')
+            if numpy.isinf(torch_output).any():
+                logger.debug(f'PyTorch output {i} has inf')
+            if numpy.isnan(ort_output).any():
+                logger.debug(f'ORT output {i} has nan')
+            if numpy.isinf(ort_output).any():
+                logger.debug(f'ORT output {i} has inf')
+
+            diff = numpy.fabs(ort_output - torch_output)
+            idx = numpy.unravel_index(diff.argmax(), diff.shape)
+            messages.append(
+                f'diff={diff[idx]:.9f} index={idx} ort={ort_output[idx]:.9f} torch={float(torch_output[idx]):.9f}')
+
+            if i == 0:  # logits
+                ort_max_index = numpy.unravel_index(numpy.argmax(ort_output, axis=None), ort_output.shape)
+                torch_max_index = numpy.unravel_index(numpy.argmax(torch_output, axis=None), torch_output.shape)
+                is_top1_matched = numpy.array_equal(ort_max_index, torch_max_index)
+
+        max_diff_output_index = max_diffs.index(max(max_diffs))
+        return is_all_close, max(max_diffs), max_diff_output_index, messages, is_top1_matched
+
+    @staticmethod
     def export_onnx(model,
                     device,
                     onnx_model_path: str,
                     verbose: bool = False,
                     use_external_data_format: bool = False,
                     has_position_ids: bool = True,
                     has_attention_mask: bool = True):
@@ -317,15 +370,15 @@
         input_names.extend(past_names)
 
         assert len(outputs) == 2 and len(outputs[1]) == num_layer
 
         logger.info(
             f"Shapes: input_ids={dummy_inputs.input_ids.shape} past={dummy_inputs.past[0].shape} output={outputs[0].shape} present={outputs[1][0].shape}"
         )
-    
+
         Path(onnx_model_path).parent.mkdir(parents=True, exist_ok=True)
 
         torch.onnx.export(model,
                           args=tuple(input_list),
                           f=onnx_model_path,
                           input_names=input_names,
                           output_names=output_names,
@@ -338,27 +391,39 @@
 
     @staticmethod
     def optimize_onnx(onnx_model_path,
                       optimized_model_path,
                       is_float16,
                       num_attention_heads,
                       hidden_size,
-                      use_external_data_format=False):
+                      use_external_data_format=False,
+                      **kwargs):
         """ Optimize ONNX model with an option to convert it to use mixed precision.
         """
         from optimizer import optimize_model
+
+        from fusion_options import FusionOptions
+        optimization_options = FusionOptions('gpt2')
+        #optimization_options.enable_gelu = False
+        #optimization_options.enable_layer_norm = False
+        #optimization_options.enable_attention = False
         m = optimize_model(onnx_model_path,
                            model_type='gpt2',
                            num_heads=num_attention_heads,
                            hidden_size=hidden_size,
                            opt_level=0,
-                           optimization_options=None,
+                           optimization_options=optimization_options,
                            use_gpu=False)
+
         if is_float16:
-            m.convert_model_float32_to_float16(cast_input_output=False)
+            op_full_list = set([node.op_type for node in m.nodes()])
+            op_block_list = set(kwargs["op_block_list"]) if "op_block_list" in kwargs else set()
+            op_remain_list = op_full_list.difference(op_block_list)
+            logger.info(f"op_block_list={op_block_list} op_remain_list={op_remain_list}")
+            m.convert_float_to_float16(use_symbolic_shape_infer=True, **kwargs)
 
         m.save_model_to_file(optimized_model_path, use_external_data_format)
 
     @staticmethod
     def pytorch_inference(model, inputs: Gpt2Inputs, total_runs: int = 0):
         """ Run inference of PyTorch model, and returns average latency in ms when total_runs > 0 besides outputs.
         """
@@ -520,73 +585,164 @@
 
         average_latency = sum(latency) * 1000 / len(latency)
         logger.debug("OnnxRuntime with IO binding inference time = {} ms".format(format(average_latency, '.2f')))
 
         return ort_outputs, average_latency
 
     @staticmethod
+    def save_outputs(i, ort_outputs, torch_outputs):
+        with open(f'ort_outputs_{i}.pickle', 'wb') as f:
+            pickle.dump(ort_outputs, f)
+        logger.info(f"ORT output are saved to ort_outputs_{i}.pickle")
+
+        with open(f'torch_outputs_{i}.pickle', 'wb') as f:
+            pickle.dump(torch_outputs, f)
+        logger.info(f"Torch output are saved to torch_outputs_{i}.pickle")
+
+    @staticmethod
+    def save_inputs(i, dummy_inputs, ort_outputs, torch_outputs):
+        with open(f'dummy_inputs_{i}.pickle', 'wb') as f:
+            pickle.dump(dummy_inputs, f)
+        logger.info(f"inputs are saved to dummy_inputs_{i}.pickle")
+
+    @staticmethod
     def test_parity(ort_session,
                     model,
                     device,
                     is_float16=False,
                     rtol=5e-4,
                     atol=5e-4,
                     total_test_cases=100,
                     use_io_binding=True,
                     model_class="GPT2LMHeadModel",
                     has_position_ids=True,
-                    has_attention_mask=True):
+                    has_attention_mask=True,
+                    verbose=False,
+                    enable_pickle_output=False):
         """ Generate random inputs and compare the results of PyTorch and Onnx Runtime.
         """
 
         config: GPT2Config = model.config
 
         logger.info(
-            f"Running parity test (rtol={rtol}, atol={atol}, test_cases={total_test_cases}, use_io_binding={use_io_binding} model_class={model_class} is_float16={is_float16}) ..."
+            f"Running parity test (atol={atol}, test_cases={total_test_cases}, use_io_binding={use_io_binding}, model_class={model_class}, is_float16={is_float16}) ..."
         )
 
         max_batch_size = 8
         max_past_seq_len = 4  # Do not use large number here for higher chance of hitting empty past (past_seq_len=0)
         max_seq_len = 2
 
         output_buffers = None
         if use_io_binding:
             max_output_shapes = Gpt2Helper.get_output_shapes(max_batch_size, max_past_seq_len, max_seq_len, config,
                                                              model_class)
             output_buffers = Gpt2Helper.get_output_buffers(max_output_shapes, device, is_float16)
 
         passed_test_cases = 0
-        for _ in range(total_test_cases):
+        top1_matched_cases = 0
+
+        max_abs_diff_list = []
+        for i in range(total_test_cases):
             sequence_length = random.randint(1, max_seq_len)
             past_sequence_length = random.randint(0, max_past_seq_len)
             batch_size = random.randint(1, max_batch_size)
 
             logger.debug(
                 f"Running parity test for batch_size={batch_size} past_sequence_length={past_sequence_length}...")
             dummy_inputs = Gpt2Helper.get_dummy_inputs(batch_size, past_sequence_length, sequence_length,
                                                        config.num_attention_heads, config.hidden_size, config.n_layer,
                                                        config.vocab_size, device, is_float16, has_position_ids,
                                                        has_attention_mask)
-
             outputs = Gpt2Helper.pytorch_inference(model, dummy_inputs)
             if use_io_binding:
                 ort_outputs = Gpt2Helper.onnxruntime_inference(ort_session, dummy_inputs)
             else:
                 output_shapes = Gpt2Helper.get_output_shapes(batch_size, past_sequence_length, sequence_length, config,
                                                              model_class)
                 ort_outputs = Gpt2Helper.onnxruntime_inference_with_binded_io(ort_session, dummy_inputs, output_buffers,
                                                                               output_shapes)
 
-            is_all_close = Gpt2Helper.compare_outputs(outputs, ort_outputs, rtol=rtol, atol=atol)
+            is_all_close, max_abs_diff, max_diff_output_index, messages, is_top1_matched = Gpt2Helper.compare_outputs_v2(
+                outputs, ort_outputs, atol=atol)
+            if not numpy.isnan(max_abs_diff):
+                max_abs_diff_list.append(max_abs_diff)
             if is_all_close:
                 passed_test_cases += 1
-        logger.info(f"Parity Test Cases={total_test_cases}; Passed={passed_test_cases}")
+            if is_top1_matched:
+                top1_matched_cases += 1
+
+            if verbose and not is_all_close:
+                logger.info(
+                    f"test_case={i} batch_size={batch_size} past_sequence_length={past_sequence_length} sequence_length={sequence_length} MaxDiff={max_abs_diff}"
+                )
+                for i, message in enumerate(messages):
+                    logger.info(f"\t{i}: Name={ort_session.get_outputs()[i].name}, {message}")
+
+            # Collect data for debugging
+            if enable_pickle_output and (numpy.isnan(max_abs_diff) or max_abs_diff > 100 * atol):
+                Gpt2Helper.save_inputs(i, dummy_inputs)
+                Gpt2Helper.save_outputs(i, ort_outputs, outputs)
+
+        if max_abs_diff_list:
+            result = {
+                f"max_diff_percentile_{p}": "{:.5f}".format(numpy.percentile(max_abs_diff_list, p))
+                for p in [50, 90, 95, 99]
+            }
+        else:
+            result = {f"max_diff_percentile_{p}": "nan" for p in [50, 90, 95, 99]}
+
+        result["top1_match_rate"] = top1_matched_cases * 1.0 / total_test_cases
+        result["diff_pass_rate"] = passed_test_cases * 1.0 / total_test_cases
+        result["nan_rate"] = (total_test_cases - len(max_abs_diff_list)) * 1.0 / total_test_cases
+
+        logger.info(
+            f"Parity Test Cases={total_test_cases}; Passed={passed_test_cases}; Nan={total_test_cases-len(max_abs_diff_list)}; Top1_Matched={top1_matched_cases}"
+        )
+
         if passed_test_cases > 0.95 * total_test_cases:
             logger.info(f"Parity is good: passed rate={int(passed_test_cases*100/total_test_cases):.0f}%")
-        return passed_test_cases == total_test_cases
+
+        return result
+
+    @staticmethod
+    def test_performance(ort_session,
+                         model,
+                         device,
+                         is_float16=False,
+                         total_runs=100,
+                         use_io_binding=True,
+                         model_class="GPT2LMHeadModel",
+                         has_position_ids=True,
+                         has_attention_mask=True,
+                         batch_size=8,
+                         sequence_length=1,
+                         past_sequence_length=32):
+        """ Generate random inputs and measure average latency of Onnx Runtime.
+        """
+
+        config: GPT2Config = model.config
+
+        output_buffers = None
+        if use_io_binding:
+            output_shapes = Gpt2Helper.get_output_shapes(batch_size, past_sequence_length, sequence_length, config,
+                                                         model_class)
+            output_buffers = Gpt2Helper.get_output_buffers(output_shapes, device, is_float16)
+
+        dummy_inputs = Gpt2Helper.get_dummy_inputs(batch_size, past_sequence_length, sequence_length,
+                                                   config.num_attention_heads, config.hidden_size, config.n_layer,
+                                                   config.vocab_size, device, is_float16, has_position_ids,
+                                                   has_attention_mask)
+
+        if use_io_binding:
+            _, latency = Gpt2Helper.onnxruntime_inference(ort_session, dummy_inputs, total_runs)
+        else:
+            _, latency = Gpt2Helper.onnxruntime_inference_with_binded_io(ort_session, dummy_inputs, output_buffers,
+                                                                         output_shapes, total_runs)
+
+        return latency
 
     @staticmethod
     def torchscript(model, config, device, has_position_ids=True, has_attention_mask=True):
         """ JIT trace for TorchScript.
         """
         input_list = Gpt2Helper.get_dummy_inputs(batch_size=1,
                                                  past_sequence_length=1,
@@ -617,14 +773,24 @@
         if model_class != 'GPT2LMHeadModel':
             model_name += "_" + model_class
 
         if has_past:
             model_name += "_past"
 
         if new_folder:
+            # Remove the directories if existed.
+            for suffix in ["", "_fp32", "_fp16", "_int8"]:
+                new_dir = os.path.join(output_dir, model_name + suffix)
+                if os.path.exists(new_dir):
+                    try:
+                        shutil.rmtree(new_dir)
+                        logger.info(f"Removed the existed directory: {new_dir}")
+                    except OSError as e:
+                        logger.info(f"Failed to remove the directory {new_dir}: {e.strerror}")
+
             # store each model to its own directory (for external data format).
             return {
                 "raw": os.path.join(os.path.join(output_dir, model_name), model_name + ".onnx"),
                 "fp32": os.path.join(os.path.join(output_dir, model_name + "_fp32"), model_name + "_fp32.onnx"),
                 "fp16": os.path.join(os.path.join(output_dir, model_name + "_fp16"), model_name + "_fp16.onnx"),
                 "int8": os.path.join(os.path.join(output_dir, model_name + "_int8"), model_name + "_int8.onnx")
             }
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/gpt2_tester.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/gpt2_tester.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/huggingface_models.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/huggingface_models.py`

 * *Files 1% similar despite different names*

```diff
@@ -35,17 +35,16 @@
     "openai-gpt": (["input_ids"], 11, False, "gpt2"),
     # GPT-2 (no past state, use benchmark_gpt2.py for past_key_values)
     "gpt2": (["input_ids"], 11, False, "gpt2"),
     "gpt2-medium": (["input_ids"], 11, False, "gpt2"),
     "gpt2-large": (["input_ids"], 11, True, "gpt2"),
     "gpt2-xl": (["input_ids"], 11, True, "gpt2"),
     "distilgpt2": (["input_ids"], 11, False, "gpt2"),
-    # Transformer-XL
-    "transfo-xl-wt103":
-    (["input_ids", "mems"], 12, False, "bert"),  # Models uses Einsum, which need opset version 12 and PyTorch 1.5.0 or above.
+    # Transformer-XL (Models uses Einsum, which need opset version 12 or later.)
+    "transfo-xl-wt103": (["input_ids", "mems"], 12, False, "bert"),
     # XLNet
     "xlnet-base-cased": (["input_ids"], 12, False, "bert"),
     "xlnet-large-cased": (["input_ids"], 12, False, "bert"),
     # XLM
     "xlm-mlm-en-2048": (["input_ids"], 11, True, "bert"),
     "xlm-mlm-ende-1024": (["input_ids"], 11, False, "bert"),
     "xlm-mlm-enfr-1024": (["input_ids"], 11, False, "bert"),
@@ -84,18 +83,18 @@
     "xlm-roberta-large": (["input_ids"], 11, True, "bert"),
     # FlauBERT
     "flaubert/flaubert_small_cased": (["input_ids"], 11, False, "bert"),
     #"flaubert/flaubert_base_uncased": (["input_ids"], 11, False, "bert"),
     "flaubert/flaubert_base_cased": (["input_ids"], 11, False, "bert"),
     #"flaubert/flaubert_large_cased": (["input_ids"], 11, False, "bert"),
     # Bart
-    "facebook/bart-large": (["input_ids"], 11, False, "bert"),
-    "facebook/bart-base": (["input_ids"], 11, False, "bert"),
-    "facebook/bart-large-mnli": (["input_ids"], 11, False, "bert"),
-    "facebook/bart-large-cnn": (["input_ids"], 11, False, "bert"),
+    "facebook/bart-large": (["input_ids", "attention_mask"], 11, False, "bart"),
+    "facebook/bart-base": (["input_ids", "attention_mask"], 11, False, "bart"),
+    "facebook/bart-large-mnli": (["input_ids", "attention_mask"], 11, False, "bart"),
+    "facebook/bart-large-cnn": (["input_ids", "attention_mask"], 11, False, "bart"),
 
     # DialoGPT
     "microsoft/DialoGPT-small": (["input_ids"], 11, False, "gpt2"),
     "microsoft/DialoGPT-medium": (["input_ids"], 11, False, "gpt2"),
     #"microsoft/DialoGPT-large": (["input_ids"], 11, True, "gpt2"),
     # Reformer
     #"google/reformer-enwik8": (["input_ids"], 11, False, "bert"),
@@ -117,15 +116,15 @@
     # "funnel-transformer/medium": (["input_ids"], 12, False, "bert"),
     # "funnel-transformer/medium-base": (["input_ids"], 12, False, "bert"),
     # "funnel-transformer/intermediate": (["input_ids"], 12, False, "bert"),
     # "funnel-transformer/intermediate-base": (["input_ids"], 12, False, "bert"),
     # "funnel-transformer/large": (["input_ids"], 12, True, "bert"),
     # "funnel-transformer/large-base": (["input_ids"], 12, True, "bert"),
     # "funnel-transformer/xlarge": (["input_ids"], 12, True, "bert"),
-    # "funnel-transformer/xlarge-base": (["input_ids"], 12, True, "bert"),    
+    # "funnel-transformer/xlarge-base": (["input_ids"], 12, True, "bert"),
     # Layoutlm
     "microsoft/layoutlm-base-uncased": (["input_ids"], 11, False, "bert"),
     "microsoft/layoutlm-large-uncased": (["input_ids"], 11, False, "bert"),
     # Squeezebert
     "squeezebert/squeezebert-uncased": (["input_ids"], 11, False, "bert"),
     "squeezebert/squeezebert-mnli": (["input_ids"], 11, False, "bert"),
     "squeezebert/squeezebert-mnli-headless": (["input_ids"], 11, False, "bert"),
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/machine_info.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/machine_info.py`

 * *Files 9% similar despite different names*

```diff
@@ -41,32 +41,35 @@
         gpu_info = self.get_gpu_info_by_nvml()
         cpu_info = cpuinfo.get_cpu_info()
 
         machine_info = {
             "gpu": gpu_info,
             "cpu": self.get_cpu_info(),
             "memory": self.get_memory_info(),
-            "python": self._try_get(cpu_info, ["python_version"]),
             "os": platform.platform(),
+            "python": self._try_get(cpu_info, ["python_version"]),
+            "packages": self.get_related_packages(),
             "onnxruntime": self.get_onnxruntime_info(),
-            "onnxruntime_tools": self.get_onnxruntime_tools_info(),
             "pytorch": self.get_pytorch_info(),
             "tensorflow": self.get_tensorflow_info()
         }
         return machine_info
 
     def get_memory_info(self) -> Dict:
         """Get memory info"""
         mem = psutil.virtual_memory()
         return {"total": mem.total, "available": mem.available}
 
     def _try_get(self, cpu_info: Dict, names: List) -> str:
         for name in names:
             if name in cpu_info:
-                return cpu_info[name]
+                value = cpu_info[name]
+                if isinstance(value, (list, tuple)):
+                    return ",".join([str(i) for i in value])
+                return value
         return ""
 
     def get_cpu_info(self) -> Dict:
         """Get CPU info"""
         cpu_info = cpuinfo.get_cpu_info()
 
         return {
@@ -103,41 +106,38 @@
 
         result = {"driver_version": driver_version, "devices": gpu_info_list}
 
         if 'CUDA_VISIBLE_DEVICES' in environ:
             result["cuda_visible"] = environ['CUDA_VISIBLE_DEVICES']
         return result
 
+    def get_related_packages(self) -> List[str]:
+        import pkg_resources
+        installed_packages = pkg_resources.working_set
+        related_packages = [
+            'onnxruntime-gpu', 'onnxruntime', 'ort-nightly-gpu', 'ort-nightly', 'onnx', 'transformers', 'protobuf',
+            'sympy', 'torch', 'tensorflow', 'flatbuffers', 'numpy', 'onnxconverter-common'
+        ]
+        related_packages_list = {i.key: i.version for i in installed_packages if i.key in related_packages}
+        return related_packages_list
+
     def get_onnxruntime_info(self) -> Dict:
         try:
             import onnxruntime
             return {
                 "version": onnxruntime.__version__,
                 "support_gpu": 'CUDAExecutionProvider' in onnxruntime.get_available_providers()
             }
         except ImportError as error:
             if not self.silent:
                 self.logger.exception(error)
             return None
         except Exception as exception:
             if not self.silent:
                 self.logger.exception(exception, False)
-            return None
-
-    def get_onnxruntime_tools_info(self) -> Dict:
-        try:
-            import onnxruntime_tools
-            return {"version": onnxruntime_tools.__version__}
-        except ImportError as error:
-            if not self.silent:
-                self.logger.exception(error)
-            return None
-        except Exception as exception:
-            if not self.silent:
-                self.logger.exception(exception, False)
             return None
 
     def get_pytorch_info(self) -> Dict:
         try:
             import torch
             return {"version": torch.__version__, "support_gpu": torch.cuda.is_available(), "cuda": torch.version.cuda}
         except ImportError as error:
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/onnx_exporter.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/onnx_exporter.py`

 * *Files 1% similar despite different names*

```diff
@@ -5,19 +5,21 @@
 # --------------------------------------------------------------------------
 
 import logging
 import numpy
 import os
 import torch
 from pathlib import Path
-from transformers import AutoConfig, AutoTokenizer, AutoModel, LxmertConfig, TransfoXLConfig
+from transformers import AutoConfig, AutoTokenizer, LxmertConfig, TransfoXLConfig
+from affinity_helper import AffinitySetting
 from benchmark_helper import create_onnxruntime_session, Precision
 from gpt2_helper import GPT2ModelNoPastState, PRETRAINED_GPT2_MODELS, TFGPT2ModelNoPastState
 from quantize_helper import QuantizeHelper
 from huggingface_models import MODEL_CLASSES
+
 os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
 
 logger = logging.getLogger(__name__)
 
 # Walkaround by replacing torch.triu using self-defined op
 # Since torch.triu cannot be exported to ONNX. See https://github.com/pytorch/pytorch/issues/32968
 torch_func = {"triu": torch.triu}
@@ -183,16 +185,16 @@
 def optimize_onnx_model(model_name, onnx_model_path, optimized_model_path, model_type, num_attention_heads, hidden_size,
                         use_gpu, precision, use_raw_attention_mask, overwrite, model_fusion_statistics,
                         use_external_data_format):
     if overwrite or not os.path.exists(optimized_model_path):
         Path(optimized_model_path).parent.mkdir(parents=True, exist_ok=True)
 
         from optimizer import optimize_model
-        from onnx_model_bert import BertOptimizationOptions
-        optimization_options = BertOptimizationOptions(model_type)
+        from fusion_options import FusionOptions
+        optimization_options = FusionOptions(model_type)
         optimization_options.use_raw_attention_mask(use_raw_attention_mask)
         if Precision.FLOAT16 == precision:
             optimization_options.enable_gelu_approximation = True
         if Precision.INT8 == precision:
             optimization_options.enable_embed_layer_norm = False
 
         # Use script to optimize model.
@@ -208,15 +210,15 @@
                                    only_onnxruntime=False)
         if model_type == 'bert_keras' or model_type == "bert_tf":
             opt_model.use_dynamic_axes()
 
         model_fusion_statistics[optimized_model_path] = opt_model.get_fused_operator_statistics()
 
         if Precision.FLOAT16 == precision:
-            opt_model.convert_model_float32_to_float16()
+            opt_model.convert_float_to_float16(keep_io_types=True)
 
         opt_model.save_model_to_file(optimized_model_path, use_external_data_format)
     else:
         logger.info(f"Skip optimization since model existed: {optimized_model_path}")
 
 
 def modelclass_dispatcher(model_name, custom_model_class):
@@ -267,19 +269,24 @@
 
     return config, model
 
 
 def load_tf_model(model_name, model_class, cache_dir):
     config = AutoConfig.from_pretrained(model_name, cache_dir=cache_dir)
 
+    # Loading tf model from transformers limits the cpu affinity to {0} when KMP_AFFINITY is set
+    # Restore the affinity after model loading for expected ORT performance
+    affi_helper = AffinitySetting()
+    affi_helper.get_affinity()
     model = load_pretrained_model(model_name,
                                   config=config,
                                   cache_dir=cache_dir,
                                   custom_model_class=model_class,
                                   is_tf_model=True)
+    affi_helper.set_affinity()
 
     return config, model
 
 
 # For test only
 def load_pt_model_from_tf(model_name):
     # Note that we could get pt model from tf, but model source and its structure in this case is different from directly using
@@ -432,21 +439,21 @@
         if config.use_cache:
             config.use_cache = False
     except:
         pass
 
     example_outputs = model(example_inputs, training=False)
     output_names = None
-    
-    # For xlnet models, only compare the last_hidden_state output. 
+
+    # For xlnet models, only compare the last_hidden_state output.
     if model_name == "xlnet-base-cased" or model_name == "xlnet-large-cased":
         output_names = ["last_hidden_state"]
         example_outputs = example_outputs["last_hidden_state"]
 
-    # Flatten is needed for gpt2 and distilgpt2. Output name sorting is needed for tf2onnx outputs to match onnx outputs. 
+    # Flatten is needed for gpt2 and distilgpt2. Output name sorting is needed for tf2onnx outputs to match onnx outputs.
     from tensorflow.python.util import nest
     example_outputs_flatten = nest.flatten(example_outputs)
 
     onnx_model_path = get_onnx_file_path(onnx_dir, model_name, len(input_names), False, use_gpu, precision, False,
                                          use_external_data_format)
     tf_internal_model_path = onnx_model_path[:-5] if use_external_data_format else onnx_model_path
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/onnx_model.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/onnx_model.py`

 * *Files 7% similar despite different names*

```diff
@@ -6,39 +6,37 @@
 from typing import List, Tuple, Dict
 import logging
 import os
 import sys
 from pathlib import Path
 import numpy as np
 from collections import deque
-from onnx import onnx_pb, AttributeProto, ModelProto, TensorProto, numpy_helper, helper, external_data_helper, save_model
+from onnx import onnx_pb, AttributeProto, ModelProto, TensorProto, NodeProto, numpy_helper, helper, external_data_helper, save_model
 from shape_infer_helper import SymbolicShapeInferenceHelper
 
 logger = logging.getLogger(__name__)
 
 
 class OnnxModel:
     def __init__(self, model):
+        self.initialize(model)
+
+    def initialize(self, model):
         self.model = model
         self._node_name_suffix: Dict[str, int] = {}  # key is node name prefix, value is the last suffix generated
         self.shape_infer_helper = None
         self.all_graphs = None
 
-    def infer_runtime_shape(self, dynamic_axis_mapping, update=False):
-        shape_infer_helper = None
-        if update:
-            shape_infer_helper = SymbolicShapeInferenceHelper(self.model)
-            self.shape_infer_helper = shape_infer_helper
-        else:
-            if self.shape_infer_helper is None:
-                self.shape_infer_helper = SymbolicShapeInferenceHelper(self.model)
-            shape_infer_helper = self.shape_infer_helper
+    def infer_runtime_shape(self, dynamic_axis_mapping={}, update=False):
+        if self.shape_infer_helper is None or update:
+            self.shape_infer_helper = SymbolicShapeInferenceHelper(self.model)
+
         try:
-            if shape_infer_helper.infer(dynamic_axis_mapping):
-                return shape_infer_helper
+            if self.shape_infer_helper.infer(dynamic_axis_mapping):
+                return self.shape_infer_helper
         except:
             print("failed in shape inference", sys.exc_info()[0])
 
         return None
 
     def input_name_to_nodes(self):
         input_name_to_nodes = {}
@@ -82,14 +80,28 @@
                         graph_queue.append(attr.g)
                     if attr.type == AttributeProto.AttributeType.GRAPHS:
                         for g in attr.graphs:
                             assert (isinstance(g, onnx_pb.GraphProto))
                             graph_queue.append(g)
         return self.all_graphs
 
+    def get_graphs_input_names(self):
+        input_names = []
+        for graph in self.graphs():
+            for input in graph.input:
+                input_names.append(input.name)
+        return input_names
+
+    def get_graphs_output_names(self):
+        output_names = []
+        for graph in self.graphs():
+            for output in graph.output:
+                output_names.append(output.name)
+        return output_names
+
     def get_graph_by_node(self, node):
         for graph in self.graphs():
             if node in graph.node:
                 return graph
         return None
 
     def get_graph_by_name(self, graph_name):
@@ -451,111 +463,127 @@
                 shape_list.append(d.dim_value)  # known dimension
             elif (d.HasField("dim_param")):
                 shape_list.append(d.dim_param)  # unknown dimension with symbolic name
             else:
                 shape_list.append("?")  # shall not happen
         return shape_list
 
-    def change_input_output_float32_to_float16(self):
-        """ Change graph input and output data type from FLOAT to FLOAT16
-        """
-        original_opset_version = self.model.opset_import[0].version
-        graph = self.graph()
-
-        new_graph_inputs = []
-        for input in graph.input:
-            if input.type.tensor_type.elem_type == TensorProto.FLOAT:
-                new_graph_inputs.append(
-                    helper.make_tensor_value_info(input.name, TensorProto.FLOAT16,
-                                                  self.tensor_shape_to_list(input.type.tensor_type)))
-            else:
-                new_graph_inputs.append(input)
+    def get_dtype(self, input_or_output: str):
+        """Try get data type given a name (could be initializer, graph input or output)."""
+        tensor_type_map = {obj.name: obj.type for obj in self.model.graph.value_info}
+
+        if input_or_output in tensor_type_map:
+            return tensor_type_map[input_or_output].tensor_type.elem_type
+
+        graph_input = self.find_graph_input(input_or_output)
+        if graph_input:
+            return graph_input.type.tensor_type.elem_type
+
+        graph_output = self.find_graph_output(input_or_output)
+        if graph_output:
+            return graph_output.type.tensor_type.elem_type
 
-        new_graph_outputs = []
-        for output in graph.output:
-            if output.type.tensor_type.elem_type == TensorProto.FLOAT:
-                new_graph_outputs.append(
-                    helper.make_tensor_value_info(output.name, TensorProto.FLOAT16,
-                                                  self.tensor_shape_to_list(output.type.tensor_type)))
-            else:
-                new_graph_outputs.append(output)
-
-        graph_def = helper.make_graph(graph.node,
-                                      'float16 inputs and outputs',
-                                      new_graph_inputs,
-                                      new_graph_outputs,
-                                      initializer=graph.initializer,
-                                      value_info=graph.value_info)
-
-        self.model = helper.make_model(graph_def, producer_name='onnxruntime-tools')
+        return None
 
-        # restore opset version
-        self.model.opset_import[0].version = original_opset_version
+    @staticmethod
+    def get_node_attribute(node: NodeProto, attribute_name: str):
+        for attr in node.attribute:
+            if attr.name == attribute_name:
+                value = helper.get_attribute_value(attr)
+                return value
+        return None
 
     def convert_model_float32_to_float16(self, cast_input_output=True):
-        """Convert a graph to FLOAT16. By default, we will keep data types of inputs and outputs.
-           For decoder model with past_key_values, it is recommended to set cast_input_output=False for better performance.
+        logger.warn(
+            'The function convert_model_float32_to_float16 is deprecated. Use convert_float_to_float16 instead!')
+        self.convert_float_to_float16(use_symbolic_shape_infer=True, keep_io_types=cast_input_output)
+
+    def convert_float_to_float16(self, use_symbolic_shape_infer=True, **kwargs):
+        """Convert a model to half (default) or mixed precision.
+           To use mixed precision, user need specify which graph inputs, outputs, operator type or list of nodes shall keep in float32.
+           By default, we use symbolic shape inference to get shape and type information. If not, ONNX shape inference will be used.
+           Note that symbolic/ONNX shape inference might fail, and the conversion might not proceed without shape and type information.
+
         Args:
-            cast_input_output (bool, optional): keep data type of inputs and outputs, and add Cast nodes to convert float32 inputs to float16, and float16 to float32 for outputs. Defaults to True.
+            use_symbolic_shape_infer (bool, optional): use symbolic shape inference instead of onnx shape inference. Defaults to True.
+            keep_io_types (Union[bool, List[str]], optional): It could be boolean or a list of float32 input/output names. 
+                                                              If True, model inputs/outputs should be left as float32. Defaults to False.
+            op_block_list (List[str], optional): List of operator types to leave as float32.
+                                                 Defaults to None, which will use `float16.DEFAULT_OP_BLOCK_LIST` as default.
+            node_block_list (List[str], optional): List of node names to leave as float32. Defaults to None.
+            force_fp16_initializers(bool): force converting all float initializers to float16.
+                                           Default to false, which will convert only the one needed to avoid precision loss.
+            min_positive_val (float, optional): minimal positive value. Defaults to 1e-7.
+            max_finite_val (float, optional): maximal finite value. Defaults to 1e4.
         """
-        from packaging.version import Version
-        import onnxconverter_common as oc
-        if Version(oc.__version__) > Version("1.7.0"):
-            self.model = oc.float16.convert_float_to_float16(self.model, keep_io_types=cast_input_output)
-            return
+        if "keep_io_types" not in kwargs:
+            kwargs["keep_io_types"] = True
 
-        graph = self.model.graph
-        initializers = graph.initializer
+        def float_to_float16_func():
+            # TODO: import from onnxconverter_common when it is stable
+            #try:
+            #    import onnxconverter_common as oc
+            #    from packaging.version import Version
+            #    if Version(oc.__version__) > Version("1.9.0"):
+            #        from onnxconverter_common.float16 import convert_float_to_float16
+            #        return convert_float_to_float16
+            #except ImportError:
+            #    pass
+
+            from float16 import convert_float_to_float16
+            return convert_float_to_float16
+
+        convert_float_to_float16 = float_to_float16_func()
+
+        model = self.model
+        if use_symbolic_shape_infer:
+            # Use symbolic shape inference since custom operators (like Gelu, SkipLayerNormalization etc) are not recognized by onnx shape inference.
+            shape_infer_helper = SymbolicShapeInferenceHelper(model)
+            model = shape_infer_helper.infer_shapes(model, auto_merge=True, guess_output_rank=False)
+
+        parameters = {'disable_shape_infer': use_symbolic_shape_infer}
+        parameters.update({
+            key: kwargs[key]
+            for key in [
+                'keep_io_types', 'min_positive_val', 'max_finite_val', 'op_block_list', 'node_block_list',
+                'force_fp16_initializers'
+            ] if key in kwargs
+        })
+
+        fp16_model = convert_float_to_float16(model, **parameters)
+        self.initialize(fp16_model)
+
+        # Convert_float_to_float16 might add Cast(to=10) --> Cast(to=1) when two consequent nodes are computed in FP32.
+        # Below are post-processing that removes those Cast nodes.
+        # Remove first Cast nodes in path like  --> Cast --> Cast -->
+        nodes_to_remove = []
+        for node in self.nodes():
+            if node.op_type == "Cast":
+                parent = self.get_parent(node, 0)
+                if parent and parent.op_type == "Cast":
+                    if self.get_children(parent) == 1:  # cannot be removed if its output is used by multiple nodes
+                        self.replace_input_of_all_nodes(parent.output[0], parent.input[0])
+                        nodes_to_remove.append(parent)
 
-        for initializer in initializers:
-            if initializer.data_type == 1:
-                initializer.CopyFrom(
-                    numpy_helper.from_array(numpy_helper.to_array(initializer).astype(np.float16), initializer.name))
+        # Remove the second cast node.
+        for node in self.nodes():
+            if node.op_type == "Cast" and OnnxModel.get_node_attribute(node, "to") == int(TensorProto.FLOAT) and \
+                self.get_dtype(node.input[0])  == int(TensorProto.FLOAT):
 
-        for node in graph.node:
-            if node.op_type in ['Constant', 'ConstantOfShape']:
-                for att in node.attribute:
-                    if att.name == 'value' and att.t.data_type == 1:
-                        att.CopyFrom(
-                            helper.make_attribute(
-                                "value", numpy_helper.from_array(numpy_helper.to_array(att.t).astype(np.float16))))
-            if node.op_type == 'Cast':
-                for att in node.attribute:
-                    if att.name == 'to' and att.i == 1:
-                        att.CopyFrom(helper.make_attribute("to", int(TensorProto.FLOAT16)))
+                if self.find_graph_output(node.output[0]):
+                    self.replace_output_of_all_nodes(node.input[0], node.output[0])
+                else:
+                    self.replace_input_of_all_nodes(node.output[0], node.input[0])
+                nodes_to_remove.append(node)
 
-        if not cast_input_output:
-            self.change_input_output_float32_to_float16()
-            return
+        self.remove_nodes(nodes_to_remove)
 
-        # Below assumes that we keep input and output data types.
-        # Add Cast node to convert input from float32 to float16.
-        for input_value_info in graph.input:
-            if input_value_info.type.tensor_type.elem_type == TensorProto.FLOAT:
-                initializer = self.get_initializer(input_value_info.name)
-                if initializer is not None:  # for compatibility for old converter/exporter
-                    input_value_info.type.tensor_type.elem_type = TensorProto.FLOAT16
-                else:
-                    cast_input = input_value_info.name
-                    cast_output = input_value_info.name + '_float16'
-                    self.replace_input_of_all_nodes(cast_input, cast_output)
-                    cast_node = helper.make_node('Cast', inputs=[cast_input], outputs=[cast_output])
-                    cast_node.attribute.extend([helper.make_attribute("to", int(TensorProto.FLOAT16))])
-                    self.add_node(cast_node)
-
-        # Add Cast node to convert output from float16 back to float32.
-        for output_value_info in graph.output:
-            if output_value_info.type.tensor_type.elem_type == TensorProto.FLOAT:
-                cast_input = output_value_info.name + '_float16'
-                cast_output = output_value_info.name
-                self.replace_output_of_all_nodes(cast_output, cast_input)
-                self.replace_input_of_all_nodes(cast_output, cast_input)
-                cast_node = helper.make_node('Cast', inputs=[cast_input], outputs=[cast_output])
-                cast_node.attribute.extend([helper.make_attribute("to", int(TensorProto.FLOAT))])
-                self.add_node(cast_node)
+        if nodes_to_remove:
+            self.prune_graph()
+            print(f"removed {len(nodes_to_remove)} Cast nodes from float16 model")
 
     def create_node_name(self, op_type, name_prefix=None):
         """Create a unique node name that starts with a prefix (default is operator type).
            The name will not be duplicated with any name that generated or existed in current graphs.
         Args:
             op_type (str): operator type
             name_prefix (str, optional): prefix of node name. Defaults to None.
@@ -666,21 +694,17 @@
         """
         Prune graph to keep only required outputs. It removes unnecessary inputs and nodes.
         Nodes are not linked (directly or indirectly) to any required output will be removed.
 
         Args:
             outputs (list): a list of graph outputs to retain. If it is None, all graph outputs will be kept.
         """
-
-        for node in self.model.graph.node:
-            # Some operators with inner graph in attributes like 'body' 'else_branch' or 'then_branch'
-            if node.op_type in ['Loop', 'Scan', 'If']:
-                # TODO: handle inner graph
-                logger.debug(f"Skip prune_graph since graph has operator: {node.op_type}")
-                return
+        if len(self.graphs()) > 1:
+            logger.debug(f"Skip prune_graph since graph has subgraph")
+            return
 
         if outputs is None:
             outputs = [output.name for output in self.model.graph.output]
 
         output_name_to_node = self.output_name_to_node()
         all_nodes = []
         for output in outputs:
@@ -779,30 +803,31 @@
                                 f"it is not safe to remove nodes since output {output_to_remove} is used by {impacted_node}"
                             )
                             return False
         return True
 
     @staticmethod
     def graph_topological_sort(graph):
-        deps_count = [0]*len(graph.node) # dependency count of each node
-        deps_to_nodes = {} # input to node indice
+        deps_count = [0] * len(graph.node)  # dependency count of each node
+        deps_to_nodes = {}  # input to node indice
         sorted_nodes = []  # initialize sorted_nodes
         for node_idx, node in enumerate(graph.node):
             # CANNOT use len(node.input) directly because input can be optional
-            deps_count[node_idx] = sum(1 for _ in node.input if _ )
-            if deps_count[node_idx] == 0: # Constant doesn't depend on any inputs
+            deps_count[node_idx] = sum(1 for _ in node.input if _)
+            if deps_count[node_idx] == 0:  # Constant doesn't depend on any inputs
                 sorted_nodes.append(graph.node[node_idx])
                 continue
 
             for input_name in node.input:
                 if input_name not in deps_to_nodes:
                     deps_to_nodes[input_name] = [node_idx]
                 else:
                     deps_to_nodes[input_name].append(node_idx)
 
+        # Note: this logic only applies to top level graph since a sub graph could use intializer from parent graph
         initializer_names = [init.name for init in graph.initializer]
         graph_input_names = [input.name for input in graph.input]
         input_names = initializer_names + graph_input_names
         input_names.sort()
         prev_input_name = None
         for input_name in input_names:
             if prev_input_name == input_name:
@@ -824,15 +849,15 @@
                     for node_idx in deps_to_nodes[output]:
                         deps_count[node_idx] = deps_count[node_idx] - 1
                         if deps_count[node_idx] == 0:
                             sorted_nodes.append(graph.node[node_idx])
                             end = end + 1
             start = start + 1
 
-        assert(end == len(graph.node)), "Graph is not a DAG"
+        assert (end == len(graph.node)), "Graph is not a DAG"
         graph.ClearField('node')
         graph.node.extend(sorted_nodes)
 
     def topological_sort(self):
         #TODO: support graph_topological_sort() in subgraphs
         #for graph in self.graphs():
         #    self.graph_topological_sort(graph)
@@ -866,7 +891,21 @@
         Returns real graph inputs (excluding initializers from older onnx model).
         """
         graph_inputs = []
         for input in self.model.graph.input:
             if self.get_initializer(input.name) is None:
                 graph_inputs.append(input)
         return graph_inputs
+
+    def get_opset_version(self):
+        """Get opset version of onnx domain
+
+        Raises:
+            RuntimeError: ONNX model has no opset for default domain.
+
+        Returns:
+            int: opset version of onnx domain.
+        """
+        for opset in self.model.opset_import:
+            if opset.domain in ["", "ai.onnx"]:
+                return opset.version
+        raise RuntimeError("ONNX model has no opset for default domain")
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/onnx_model_bert.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/onnx_model_bert.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,53 +1,38 @@
 #-------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
 #--------------------------------------------------------------------------
 
 from logging import getLogger
 from typing import List
-from onnx import ModelProto, TensorProto, helper
+from onnx import GraphProto, ModelProto, TensorProto, ValueInfoProto, helper
 from onnx_model import OnnxModel
 from fusion_reshape import FusionReshape
+from fusion_shape import FusionShape
 from fusion_layernorm import FusionLayerNormalization, FusionLayerNormalizationTF
 from fusion_skiplayernorm import FusionSkipLayerNormalization, FusionBiasSkipLayerNormalization
 from fusion_embedlayer import FusionEmbedLayerNormalization
-from fusion_attention import FusionAttention, AttentionMask, AttentionMaskFormat
+from fusion_attention import FusionAttention, AttentionMask
 from fusion_gelu import FusionGelu
 from fusion_fastgelu import FusionFastGelu
 from fusion_biasgelu import FusionBiasGelu
 from fusion_gelu_approximation import FusionGeluApproximation
 from fusion_utils import FusionUtils
+from fusion_options import FusionOptions
 
 logger = getLogger(__name__)
 
 
-class BertOptimizationOptions:
+class BertOptimizationOptions(FusionOptions):
+    """ This class is deprecated
+    """
     def __init__(self, model_type):
-        self.enable_gelu = True
-        self.enable_layer_norm = True
-        self.enable_attention = True
-        self.enable_skip_layer_norm = True
-        self.enable_embed_layer_norm = True
-        self.enable_bias_skip_layer_norm = True
-        self.enable_bias_gelu = True
-        self.enable_gelu_approximation = False
-        self.attention_mask_format = AttentionMaskFormat.AttentionMask
-
-        if model_type == 'gpt2':
-            self.enable_skip_layer_norm = False
-
-    def use_raw_attention_mask(self, use_raw_mask=True):
-        if use_raw_mask:
-            self.attention_mask_format = AttentionMaskFormat.AttentionMask
-        else:
-            self.attention_mask_format = AttentionMaskFormat.MaskIndexEnd
-
-    def disable_attention_mask(self):
-        self.attention_mask_format = AttentionMaskFormat.NoMask
+        logger.warning(f"BertOptimizationOptions is depreciated. Please use FusionOptions instead.")
+        super().__init__(model_type)
 
 
 class BertOnnxModel(OnnxModel):
     def __init__(self, model: ModelProto, num_heads: int = 0, hidden_size: int = 0):
         """Initialize BERT ONNX Model.
 
         Args:
@@ -86,14 +71,18 @@
         fusion = FusionBiasSkipLayerNormalization(self)
         fusion.apply()
 
     def fuse_reshape(self):
         fusion = FusionReshape(self)
         fusion.apply()
 
+    def fuse_shape(self):
+        fusion = FusionShape(self)
+        fusion.apply()
+
     def fuse_embed_layer(self):
         fusion = FusionEmbedLayerNormalization(self)
         fusion.apply()
 
     def fuse_layer_norm(self):
         fusion = FusionLayerNormalization(self)
         fusion.apply()
@@ -128,41 +117,87 @@
         return graph_inputs
 
     def get_graph_inputs_from_fused_nodes(self, casted: bool):
         inputs = self.get_graph_inputs_from_node_type('EmbedLayerNormalization', [0, 1, 7], casted)
         inputs += self.get_graph_inputs_from_node_type('Attention', [3], casted)
         return inputs
 
-    def change_input_to_int32(self):
-        original_opset_version = self.model.opset_import[0].version
-        graph = self.graph()
+    def change_graph_input_type(self,
+                                graph: GraphProto,
+                                graph_input: ValueInfoProto,
+                                new_type: int = TensorProto.INT32):
+        """Change graph input type, and add Cast node if needed.
 
-        new_graph_inputs = []
-        casted_bert_graph_inputs = self.get_graph_inputs_from_fused_nodes(casted=True)
+        Args:
+            graph (GraphProto): graph
+            graph_input (TensorProto): input of the graph
+            new_type (int, optional): new data type. Defaults to TensorProto.INT32.
+
+        Returns:
+            NodeProto: a new Cast node that added. None if Cast node is not added.
+            List[NodeProto]: Cast nodes that have been removed.
+        """
+        assert isinstance(graph, GraphProto)
+        assert isinstance(graph_input, ValueInfoProto)
+        assert self.find_graph_input(graph_input.name)
 
-        for input in graph.input:
-            if input.name in casted_bert_graph_inputs:
-                self.utils.remove_cast_int32(input.name)
-                int32_input = helper.make_tensor_value_info(input.name, TensorProto.INT32,
-                                                            self.tensor_shape_to_list(input.type.tensor_type))
-                new_graph_inputs.append(int32_input)
-            else:
-                new_graph_inputs.append(input)
-
-        graph_def = helper.make_graph(graph.node,
-                                      'int32 inputs',
-                                      new_graph_inputs,
-                                      graph.output,
-                                      initializer=graph.initializer,
-                                      value_info=graph.value_info)
+        if graph_input.type.tensor_type.elem_type == int(new_type):
+            return None, []
+
+        new_cast_node = None
+        nodes_to_remove = []
 
-        self.model = helper.make_model(graph_def, producer_name='onnxruntime-tools')
+        input_name_to_nodes = self.input_name_to_nodes()
+        if graph_input.name in input_name_to_nodes:
+            nodes = input_name_to_nodes[graph_input.name]
+
+            # For children that is not Cast node, insert a Cast node to convert int32 to original data type.
+            nodes_not_cast = [node for node in nodes if node.op_type != 'Cast']
+            if nodes_not_cast:
+                node_name = self.create_node_name('Cast')
+                output_name = node_name + '_' + graph_input.name
+                new_value_info = graph.value_info.add()
+                new_value_info.CopyFrom(graph_input)
+                new_value_info.name = output_name
+                new_cast_node = helper.make_node('Cast', [graph_input.name], [output_name],
+                                                 to=int(graph_input.type.tensor_type.elem_type),
+                                                 name=node_name)
+                graph.node.extend([new_cast_node])
+
+                for node in nodes_not_cast:
+                    OnnxModel.replace_node_input(node, graph_input.name, output_name)
+
+            # For children that is Cast node, no need to insert Cast.
+            # When the children is Cast to int32, we can remove that Cast node since input type is int32 now.
+            nodes_cast = [node for node in nodes if node.op_type == 'Cast']
+            for node in nodes_cast:
+                if OnnxModel.get_node_attribute(node, "to") == int(new_type):
+                    self.replace_input_of_all_nodes(node.output[0], graph_input.name)
+                if not self.find_graph_output(node.output[0]):
+                    nodes_to_remove.append(node)
+            if nodes_to_remove:
+                self.remove_nodes(nodes_to_remove)
 
-        # restore opset version
-        self.model.opset_import[0].version = original_opset_version
+        graph_input.type.tensor_type.elem_type = int(new_type)
+        return new_cast_node, nodes_to_remove
+
+    def change_graph_inputs_to_int32(self):
+        """Change data type of all graph inputs to int32 type, and add Cast node if needed.
+        """
+        graph = self.graph()
+        add_cast_count = 0
+        remove_cast_count = 0
+        for graph_input in graph.input:
+            new_node, removed_nodes = self.change_graph_input_type(graph, graph_input, TensorProto.INT32)
+            if new_node:
+                add_cast_count += 1
+            remove_cast_count += len(removed_nodes)
+        logger.info(
+            f"Graph inputs are changed to int32. Added {add_cast_count} Cast nodes, and removed {remove_cast_count} Cast nodes."
+        )
 
     def use_dynamic_axes(self, dynamic_batch_dim='batch_size', dynamic_seq_len='max_seq_len'):
         """
         Update input and output shape to use dynamic axes.
         """
         bert_graph_inputs = self.get_graph_inputs_from_fused_nodes(
             casted=True) + self.get_graph_inputs_from_fused_nodes(casted=False)
@@ -205,19 +240,21 @@
                     expand_shape_value = self.get_constant_value(expand_node.input[1])
 
                     reshape_before_expand = reshape_path[-2]
                     shape_value = self.get_constant_value(reshape_before_expand.input[1])
 
                     slice_node = reshape_path[-1]
                     if expand_shape_value is not None and shape_value is not None and len(
-                            expand_shape_value) is 2 and len(
-                                shape_value) is 1 and expand_shape_value[1] == shape_value[0]:
+                            expand_shape_value) == 2 and len(
+                                shape_value) == 1 and expand_shape_value[1] == shape_value[0]:
                         node.input[0] = slice_node.output[0]
-        self.remove_nodes(nodes_to_remove)
-        logger.info(f"Removed Reshape and Expand count: {len(nodes_to_remove)}")
+
+        if nodes_to_remove:
+            self.remove_nodes(nodes_to_remove)
+            logger.info(f"Removed Reshape and Expand count: {len(nodes_to_remove)}")
 
     def clean_graph(self):
         output_name_to_node = self.output_name_to_node()
         nodes_to_remove = []
         for node in self.nodes():
             # Before:
             #  input_ids --> Shape --> Gather(indices=0) --> Unsqueeze ------+
@@ -258,15 +295,15 @@
                         nodes_to_remove.append(node)
         self.remove_nodes(nodes_to_remove)
 
     def postprocess(self):
         self.clean_graph()
         self.prune_graph()
 
-    def optimize(self, options: BertOptimizationOptions = None, add_dynamic_axes=False):
+    def optimize(self, options: FusionOptions = None, add_dynamic_axes=False):
         if (options is None) or options.enable_layer_norm:
             self.fuse_layer_norm()
 
         if (options is None) or options.enable_gelu:
             self.fuse_gelu()
 
         self.preprocess()
@@ -277,18 +314,22 @@
             self.fuse_skip_layer_norm()
 
         if (options is None) or options.enable_attention:
             if options is not None:
                 self.attention_mask.set_mask_format(options.attention_mask_format)
             self.fuse_attention()
 
+        self.fuse_shape()
+
         if (options is None) or options.enable_embed_layer_norm:
             self.fuse_embed_layer()
 
-        # Post-processing like removing extra reshape nodes.
+        # Remove reshape nodes that having same shape of input and output based on symbolic shape inference.
+        FusionUtils.remove_useless_reshape_nodes(self)
+
         self.postprocess()
 
         # Bias fusion is done after postprocess to avoid extra Reshape between bias and Gelu/FastGelu/SkipLayerNormalization
         if (options is None) or options.enable_bias_gelu:
             # Fuse Gelu and Add Bias before it.
             self.fuse_bias_gelu(is_fastgelu=True)
             self.fuse_bias_gelu(is_fastgelu=False)
@@ -302,15 +343,15 @@
 
         self.remove_unused_constant()
 
         # Use symbolic batch dimension in input and output.
         if add_dynamic_axes:
             self.use_dynamic_axes()
 
-        logger.info(f"opset verion: {self.model.opset_import[0].version}")
+        logger.info(f"opset verion: {self.get_opset_version()}")
 
     def get_fused_operator_statistics(self):
         """
         Returns node count of fused operators.
         """
         op_count = {}
         ops = [
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/onnx_model_bert_keras.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/onnx_model_bert_keras.py`

 * *Files 0% similar despite different names*

```diff
@@ -7,19 +7,20 @@
 import onnx
 import sys
 import argparse
 import numpy as np
 from collections import deque
 from onnx import ModelProto, TensorProto, numpy_helper
 from onnx_model_bert_tf import BertOnnxModelTF
+
 logger = logging.getLogger(__name__)
 
 
 class BertOnnxModelKeras(BertOnnxModelTF):
-    def __init(self, model, num_heads, hidden_size):
+    def __init__(self, model, num_heads, hidden_size):
         super().__init__(model, num_heads, hidden_size)
 
     def match_mask_path(self, add_or_sub_before_softmax):
         mask_nodes = self.match_parent_path(add_or_sub_before_softmax, ['Mul', 'Sub', 'Reshape', 'Cast'],
                                             [1, None, 1, 0])
         if mask_nodes is not None:
             return mask_nodes
@@ -136,15 +137,15 @@
                                                                      output_name_to_node)
             if is_same_root:
                 mask_index = self.attention_mask.process_mask(mask_nodes[-1].input[0])
                 logger.debug("Create an Attention node.")
                 attention_node = self.attention_fusion.create_attention_node(mask_index, matmul_q, matmul_k, matmul_v,
                                                                              add_q, add_k, add_v, self.num_heads,
                                                                              self.hidden_size, parent.output[0],
-                                                                             reshape_qkv.output[0])
+                                                                             reshape_qkv.output[0], None)
                 if attention_node is None:
                     continue
 
                 self.add_node(attention_node)
                 attention_count += 1
 
                 nodes_to_remove.extend([reshape_qkv, transpose_qkv, matmul_qkv])
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/onnx_model_bert_tf.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/onnx_model_bert_tf.py`

 * *Files 0% similar despite different names*

```diff
@@ -12,15 +12,15 @@
 from onnx import ModelProto, TensorProto, numpy_helper, helper
 from onnx_model_bert import BertOnnxModel
 
 logger = logging.getLogger(__name__)
 
 
 class BertOnnxModelTF(BertOnnxModel):
-    def __init(self, model, num_heads, hidden_size):
+    def __init__(self, model, num_heads, hidden_size):
         super().__init__(model, num_heads, hidden_size)
 
     def remove_identity(self):
         nodes_to_remove = []
         for node in self.nodes():
             if node.op_type == 'Identity':
                 if not self.find_graph_output(node.output[0]):
@@ -284,17 +284,16 @@
         skip_layer_norm_nodes = self.get_nodes_by_op_type("SkipLayerNormalization")
         layer_norm_nodes = self.get_nodes_by_op_type("LayerNormalization")
         # Sometimes we can not fuse skiplayernormalization since the add before layernorm has an output that used by nodes outside skiplayernorm
         # Conceptually we treat add before layernorm as skiplayernorm node since they share the same pattern
         start_nodes.extend(skip_layer_norm_nodes)
         start_nodes.extend(layer_norm_nodes)
 
-        graph_name = self.get_graph_by_node(start_nodes[0]).name
-
         for normalize_node in start_nodes:
+            graph_name = self.get_graph_by_node(normalize_node).name
             # SkipLayerNormalization has two inputs, and one of them is the root input for attention.
             if normalize_node.op_type == 'LayerNormalization':
                 add_before_layernorm = self.match_parent(normalize_node, 'Add', 0)
                 if add_before_layernorm is not None:
                     normalize_node = add_before_layernorm
                 else:
                     continue
@@ -380,15 +379,15 @@
                 mask_index = self.attention_mask.process_mask(mask_nodes[-1].input[0])
                 logger.debug("Create an Attention node.")
 
                 # For tf models, q and v are flipped.
                 attention_node = self.attention_fusion.create_attention_node(mask_index, matmul_k, matmul_q, matmul_v,
                                                                              add_k, add_q, add_v, self.num_heads,
                                                                              self.hidden_size, parent.output[0],
-                                                                             qkv_nodes[2].output[0])
+                                                                             qkv_nodes[2].output[0], None)
                 if attention_node is None:
                     continue
 
                 if qkv_nodes[1].op_type == 'Einsum':
                     # add reshape before einsum
                     tensor = helper.make_tensor(name=qkv_nodes[1].name + "_newshape",
                                                 data_type=TensorProto.INT64,
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/onnx_model_gpt2.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/onnx_model_gpt2.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,27 +1,22 @@
 #-------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
 #--------------------------------------------------------------------------
 import logging
 import onnx
-import sys
-import argparse
-import numpy as np
-from collections import deque
-from onnx import ModelProto, TensorProto, numpy_helper
 from onnx_model_bert import BertOnnxModel
 from fusion_gpt_attention_no_past import FusionGptAttentionNoPast
 from fusion_gpt_attention import FusionGptAttention
 
 logger = logging.getLogger(__name__)
 
 
 class Gpt2OnnxModel(BertOnnxModel):
-    def __init(self, model, num_heads, hidden_size):
+    def __init__(self, model, num_heads, hidden_size):
         super().__init__(model, num_heads, hidden_size)
 
     def fuse_attention(self):
         if len(self.model.graph.input) == 1 or len(self.model.graph.output) == 1:
             fusion = FusionGptAttentionNoPast(self, self.num_heads)
             fusion.apply()
         else:
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/optimizer.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/optimizer.py`

 * *Files 14% similar despite different names*

```diff
@@ -15,80 +15,87 @@
 # This script is retained for experiment purpose. Useful senarios like the following:
 #  (1) Change model from fp32 to fp16 for mixed precision inference in GPU with Tensor Core.
 #  (2) Change input data type from int64 to int32.
 #  (3) Some model cannot be handled by OnnxRuntime, and you can modify this script to get optimized model.
 
 import logging
 import coloredlogs
-import onnx
 import os
-import sys
 import argparse
-import numpy as np
 from typing import Dict
-from collections import deque
-from onnx import ModelProto, TensorProto, numpy_helper, load_model
-from onnx_model_bert import BertOnnxModel, BertOptimizationOptions
+from onnx import load_model
+from onnx_model_bart import BartOnnxModel
+from onnx_model_bert import BertOnnxModel
 from onnx_model_bert_tf import BertOnnxModelTF
 from onnx_model_bert_keras import BertOnnxModelKeras
 from onnx_model_gpt2 import Gpt2OnnxModel
+from fusion_options import FusionOptions
 
 logger = logging.getLogger(__name__)
 
-# Map model type to tuple: optimizer class, export tools (pytorch, tf2onnx, keras2onnx) and whether OnnxRuntime has the optimization.
-MODEL_CLASSES = {
-    "bert": (BertOnnxModel, "pytorch", True),
-    "bert_tf": (BertOnnxModelTF, "tf2onnx", False),
-    "bert_keras": (BertOnnxModelKeras, "keras2onnx", False),
-    "gpt2": (Gpt2OnnxModel, "pytorch", True),
-    "gpt2_tf": (Gpt2OnnxModel, 'tf2onnx', False) # might add a class for GPT2OnnxModel for TF later. 
+# Map model type to tuple: optimizer class, export tools (pytorch, tf2onnx, keras2onnx), and default opt_level
+MODEL_TYPES = {
+    "bart": (BartOnnxModel, "pytorch", 1),
+    "bert": (BertOnnxModel, "pytorch", 1),
+    "bert_tf": (BertOnnxModelTF, "tf2onnx", 0),
+    "bert_keras": (BertOnnxModelKeras, "keras2onnx", 0),
+    "gpt2": (Gpt2OnnxModel, "pytorch", 1),
+    "gpt2_tf": (Gpt2OnnxModel, 'tf2onnx', 0)  # might add a class for GPT2OnnxModel for TF later.
 }
 
 
 def optimize_by_onnxruntime(onnx_model_path: str,
                             use_gpu: bool = False,
                             optimized_model_path: str = None,
-                            opt_level: int = 99) -> str:
+                            opt_level: int = 99,
+                            disabled_optimizers=[]) -> str:
     """
     Use onnxruntime to optimize model.
 
     Args:
         onnx_model_path (str): the path of input onnx model.
         use_gpu (bool): whether the optimized model is targeted to run in GPU.
         optimized_model_path (str or None): the path of optimized model.
         opt_level (int): graph optimization level.
-
+        disabled_optimizers (List[str]): a list of names of disabled optimizers
     Returns:
         optimized_model_path (str): the path of optimized model
     """
+    assert opt_level in [1, 2, 99]
     import onnxruntime
 
     if use_gpu and 'CUDAExecutionProvider' not in onnxruntime.get_available_providers():
         logger.error("There is no gpu for onnxruntime to do optimization.")
         return onnx_model_path
 
     sess_options = onnxruntime.SessionOptions()
     if opt_level == 1:
         sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_BASIC
     elif opt_level == 2:
         sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_EXTENDED
     else:
-        assert opt_level == 99
         sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL
 
     if optimized_model_path is None:
         path_prefix = onnx_model_path[:-5]  #remove .onnx suffix
         optimized_model_path = "{}_o{}_{}.onnx".format(path_prefix, opt_level, "gpu" if use_gpu else "cpu")
 
     sess_options.optimized_model_filepath = optimized_model_path
 
+    kwargs = {}
+    if disabled_optimizers:
+        kwargs["disabled_optimizers"] = disabled_optimizers
+
     if not use_gpu:
-        session = onnxruntime.InferenceSession(onnx_model_path, sess_options, providers=['CPUExecutionProvider'])
+        session = onnxruntime.InferenceSession(onnx_model_path,
+                                               sess_options,
+                                               providers=['CPUExecutionProvider'],
+                                               **kwargs)
     else:
-        session = onnxruntime.InferenceSession(onnx_model_path, sess_options)
+        session = onnxruntime.InferenceSession(onnx_model_path, sess_options, **kwargs)
         assert 'CUDAExecutionProvider' in session.get_providers()  # Make sure there is GPU
 
     assert os.path.exists(optimized_model_path) and os.path.isfile(optimized_model_path)
     logger.debug("Save optimized model by onnxruntime to {}".format(optimized_model_path))
     return optimized_model_path
 
 
@@ -104,25 +111,28 @@
     """
     model = load_model(optimized_model_path, format=None, load_external_data=True)
     optimizer = BertOnnxModel(model, num_heads=12, hidden_size=768)
     return optimizer.get_fused_operator_statistics()
 
 
 def _parse_arguments():
-    parser = argparse.ArgumentParser()
+    parser = argparse.ArgumentParser(
+        description=
+        'Graph optimization tool for ONNX Runtime. It transforms ONNX graph to use optimized operators for Transformer models.'
+    )
     parser.add_argument('--input', required=True, type=str, help="input onnx model path")
 
     parser.add_argument('--output', required=True, type=str, help="optimized onnx model path")
 
     parser.add_argument('--model_type',
                         required=False,
                         type=str.lower,
                         default="bert",
-                        choices=list(MODEL_CLASSES.keys()),
-                        help="Model type selected in the list: " + ", ".join(MODEL_CLASSES.keys()))
+                        choices=list(MODEL_TYPES.keys()),
+                        help="Model type selected in the list: " + ", ".join(MODEL_TYPES.keys()))
 
     parser.add_argument(
         '--num_heads',
         required=False,
         type=int,
         default=12,
         help=
@@ -137,178 +147,147 @@
         help=
         "bert model hidden size. 768 for bert-base model and 1024 for bert-large. For BERT, set it to 0 to detect automatically."
     )
 
     parser.add_argument('--input_int32',
                         required=False,
                         action='store_true',
-                        help="Use int32 (instead of int64) tensor as input to avoid unnecessary data cast")
+                        help="Use int32 (instead of int64) tensor as input to avoid unnecessary data cast.")
     parser.set_defaults(input_int32=False)
 
     parser.add_argument(
         '--float16',
         required=False,
         action='store_true',
-        help="If your target device is V100 or T4 GPU, use this to convert float32 to float16 for best performance")
+        help=
+        "If your target device is V100 or T4 GPU, try this to convert float32 to float16 for best performance (with potential loss in precision)."
+    )
     parser.set_defaults(float16=False)
 
-    parser.add_argument('--disable_attention', required=False, action='store_true', help="disable Attention fusion")
-    parser.set_defaults(disable_attention=False)
-
-    parser.add_argument('--disable_skip_layer_norm',
-                        required=False,
-                        action='store_true',
-                        help="disable SkipLayerNormalization fusion")
-    parser.set_defaults(disable_skip_layer_norm=False)
-
-    parser.add_argument('--disable_embed_layer_norm',
-                        required=False,
-                        action='store_true',
-                        help="disable EmbedLayerNormalization fusion")
-    parser.set_defaults(disable_embed_layer_norm=False)
-
-    parser.add_argument('--disable_bias_skip_layer_norm',
-                        required=False,
-                        action='store_true',
-                        help="disable Add Bias and SkipLayerNormalization fusion")
-    parser.set_defaults(disable_bias_skip_layer_norm=False)
-
-    parser.add_argument('--disable_bias_gelu',
-                        required=False,
-                        action='store_true',
-                        help="disable Add Bias and Gelu/FastGelu fusion")
-    parser.set_defaults(disable_bias_gelu=False)
-
-    parser.add_argument('--disable_layer_norm',
-                        required=False,
-                        action='store_true',
-                        help="disable LayerNormalization fusion")
-    parser.set_defaults(disable_layer_norm=False)
-
-    parser.add_argument('--disable_gelu', required=False, action='store_true', help="disable Gelu fusion")
-    parser.set_defaults(disable_gelu=False)
-
-    parser.add_argument('--enable_gelu_approximation',
-                        required=False,
-                        action='store_true',
-                        help="enable Gelu/BiasGelu to FastGelu conversion")
-    parser.set_defaults(enable_gelu_approximation=False)
-
-    parser.add_argument('--use_mask_index',
-                        required=False,
-                        action='store_true',
-                        help="use mask index instead of raw attention mask in attention operator")
-    parser.set_defaults(use_mask_index=False)
-
-    parser.add_argument('--no_attention_mask',
-                        required=False,
-                        action='store_true',
-                        help="no attention mask. Only works for model_type=bert")
-    parser.set_defaults(no_attention_mask=False)
+    FusionOptions.add_arguments(parser)
 
-    parser.add_argument('--verbose', required=False, action='store_true')
+    parser.add_argument('--verbose', required=False, action='store_true', help="show debug information.")
     parser.set_defaults(verbose=False)
 
-    parser.add_argument('--use_gpu', required=False, action='store_true', help="use GPU inference")
+    parser.add_argument(
+        '--use_gpu',
+        required=False,
+        action='store_true',
+        help="use GPU for inference. Set this flag if your model is intended for GPU and opt_level > 1.")
     parser.set_defaults(use_gpu=False)
 
-    parser.add_argument('--only_onnxruntime', required=False, action='store_true', help="optimized by onnxruntime only")
+    parser.add_argument('--only_onnxruntime',
+                        required=False,
+                        action='store_true',
+                        help="optimized by onnxruntime only, and no graph fusion in Python")
     parser.set_defaults(only_onnxruntime=False)
 
-    parser.add_argument('--opt_level',
-                        required=False,
-                        type=int,
-                        choices=[0, 1, 2, 99],
-                        default=0,
-                        help="onnxruntime optimization level. 0 will disable onnxruntime.")
+    parser.add_argument(
+        '--opt_level',
+        required=False,
+        type=int,
+        choices=[0, 1, 2, 99],
+        default=None,
+        help=
+        "onnxruntime optimization level. 0 will disable onnxruntime graph optimization. Graph fusion in Python is not impacted by setting."
+    )
 
     parser.add_argument('--use_external_data_format',
                         required=False,
                         action='store_true',
                         help="use external data format")
     parser.set_defaults(use_external_data_format=False)
 
     args = parser.parse_args()
 
     return args
 
 
-def _get_optimization_options(args):
-    optimization_options = BertOptimizationOptions(args.model_type)
-    if args.disable_gelu:
-        optimization_options.enable_gelu = False
-    if args.disable_layer_norm:
-        optimization_options.enable_layer_norm = False
-    if args.disable_attention:
-        optimization_options.enable_attention = False
-    if args.disable_skip_layer_norm:
-        optimization_options.enable_skip_layer_norm = False
-    if args.disable_embed_layer_norm:
-        optimization_options.enable_embed_layer_norm = False
-    if args.disable_bias_skip_layer_norm:
-        optimization_options.enable_bias_skip_layer_norm = False
-    if args.disable_bias_gelu:
-        optimization_options.enable_bias_gelu = False
-    if args.enable_gelu_approximation:
-        optimization_options.enable_gelu_approximation = True
-    if args.use_mask_index:
-        optimization_options.use_raw_attention_mask(False)
-    if args.no_attention_mask:
-        optimization_options.disable_attention_mask()
-
-    return optimization_options
-
-
 def optimize_model(input,
                    model_type='bert',
                    num_heads=0,
                    hidden_size=0,
                    optimization_options=None,
-                   opt_level=0,
+                   opt_level=None,
                    use_gpu=False,
                    only_onnxruntime=False):
-    """ Optimize Model by OnnxRuntime and/or offline fusion logic.
+    """ Optimize Model by OnnxRuntime and/or python fusion logic.
+
+    ONNX Runtime has graph optimizations (https://onnxruntime.ai/docs/resources/graph-optimizations.html). 
+    However, the coverage is limited. We also have graph fusions that implemented in Python to improve the coverage.
+    They can combined: ONNX Runtime will run first when opt_level > 0, then graph fusions in Python will be applied.
 
-    The following optimizes model by OnnxRuntime only, and no offline fusion logic:
+    To use ONNX Runtime only and no Python fusion logic, use only_onnxruntime flag and a positive opt_level like
         optimize_model(input, opt_level=1, use_gpu=False, only_onnxruntime=True)
-    If you want to optimize model by offline fusion logic.
-        optimize_model(input, model_type, num_heads=12, hidden_size=768, optimization_options=your_options)
+
+    When opt_level is None, we will choose default optimization level according to model type.
+
+    When opt_level is 0 and only_onnxruntime is False, only python fusion logic is used and onnxruntime is disabled.
+
+    When opt_level > 1, use_gpu shall set properly since the optimized graph might contain operators for GPU or CPU only. 
+    If your model is intended for GPU inference only (especially float16 or mixed precision model), it is recommended to 
+    set use_gpu to be True, otherwise the model is not optimized for GPU inference.
+
+    For BERT model, num_heads and hidden_size are optional. For other model types, you need specify these parameters.
 
     Args:
         input (str): input model path.
-        model_type (str): model type - like bert, bert_tf, bert_keras or gpt2.
-        num_heads (int): number of attention heads. Default is 0 to allow detect the parameter from graph automatically (for model_type "bert" only).
-        hidden_size (int): hidden size. Default is 0 to allow detect the parameter from graph automatically (for model_type "bert" only).
-        optimization_options (OptimizationOptions or None): optimization options that can use to turn on/off some fusions.
-        opt_level (int): onnxruntime graph optimization level (0, 1, 2 or 99). When the level > 0, onnxruntime will be used to optimize model first.
-        use_gpu (bool): use gpu or not for onnxruntime.
-        only_onnxruntime (bool): only use onnxruntime to optimize model, and no offline fusion logic is used.
+        model_type (str, optional): model type - like bert, bert_tf, bert_keras or gpt2. Defaults to 'bert'.
+        num_heads (int, optional): number of attention heads. Defaults to 0.
+                                   0 allows detect the parameter from graph automatically (for model_type "bert" only). 
+        hidden_size (int, optional): hidden size. Defaults to 0.
+                                     0 allows detect the parameter from graph automatically (for model_type "bert" only). 
+        optimization_options (FusionOptions, optional): optimization options that turn on/off some fusions. Defaults to None.
+        opt_level (int, optional): onnxruntime graph optimization level (0, 1, 2 or 99) or None. Defaults to None.
+                                   When the value is None, default value (1 for bert and gpt2, 0 for other model types) will be used.
+                                   When the level > 0, onnxruntime will be used to optimize model first.
+        use_gpu (bool, optional): use gpu or not for onnxruntime. Defaults to False.
+        only_onnxruntime (bool, optional): only use onnxruntime to optimize model, and no python fusion. Defaults to False.
 
      Returns:
         object of an optimizer class.
     """
-    (optimizer_class, producer, run_onnxruntime) = MODEL_CLASSES[model_type]
+    assert opt_level is None or opt_level in [0, 1, 2, 99]
+
+    if model_type != "bert" and (num_heads == 0 or hidden_size == 0):
+        logger.warning("Please specify parameters of num_heads and hidden_size when model_type is not 'bert'")
+
+    (optimizer_class, producer, default_opt_level) = MODEL_TYPES[model_type]
+
+    if opt_level is None:
+        opt_level = default_opt_level
 
     temp_model_path = None
-    if opt_level > 1:  # Optimization specified for an execution provider.
-        temp_model_path = optimize_by_onnxruntime(input, use_gpu=use_gpu, opt_level=opt_level)
-    elif run_onnxruntime:
-        # Use Onnxruntime to do optimizations (like constant folding and cast elimation) that is not specified to exection provider.
+    if opt_level > 1:
+        # Disable some optimizers that might cause failure in symbolic shape inference or attention fusion.
+        disabled_optimizers = [] if only_onnxruntime else [
+            'MatMulScaleFusion', 'MatMulAddFusion'
+            'SimplifiedLayerNormFusion', 'GemmActivationFusion', 'BiasSoftmaxFusion'
+        ]
+        temp_model_path = optimize_by_onnxruntime(input,
+                                                  use_gpu=use_gpu,
+                                                  opt_level=opt_level,
+                                                  disabled_optimizers=disabled_optimizers)
+    elif opt_level == 1:
+        # basic optimizations (like constant folding and cast elimation) are not specified to exection provider.
         # CPU provider is used here so that there is no extra node for GPU memory copy.
         temp_model_path = optimize_by_onnxruntime(input, use_gpu=False, opt_level=1)
 
+    if only_onnxruntime and not temp_model_path:
+        logger.warning("Please specify a positive value for opt_level when only_onnxruntime is True")
+
     model = load_model(temp_model_path or input, format=None, load_external_data=True)
 
     if model.producer_name and producer != model.producer_name:
         logger.warning(
-            f"Model producer not matched: Expect {producer},  Got {model.producer_name} {model.producer_version}. Please specify correct --model_type parameter."
+            f"Model producer not matched: Expect {producer}, Got {model.producer_name} {model.producer_version}. Please specify correct --model_type parameter."
         )
 
     if optimization_options is None:
-        optimization_options = BertOptimizationOptions(model_type)
+        optimization_options = FusionOptions(model_type)
 
     optimizer = optimizer_class(model, num_heads, hidden_size)
 
     if not only_onnxruntime:
         optimizer.optimize(optimization_options)
 
     # Remove the temporary model.
@@ -331,33 +310,35 @@
 
 
 def main():
     args = _parse_arguments()
 
     _setup_logger(args.verbose)
 
+    logger.debug(f"arguments:{args}")
+
     if os.path.realpath(args.input) == os.path.realpath(args.output):
         logger.warning(f"Specified the same input and output path. Note that this may overwrite the original model")
 
-    optimization_options = _get_optimization_options(args)
+    optimization_options = FusionOptions.parse(args)
 
     optimizer = optimize_model(args.input,
                                args.model_type,
                                args.num_heads,
                                args.hidden_size,
                                opt_level=args.opt_level,
                                optimization_options=optimization_options,
                                use_gpu=args.use_gpu,
                                only_onnxruntime=args.only_onnxruntime)
 
     if args.float16:
-        optimizer.convert_model_float32_to_float16()
+        optimizer.convert_float_to_float16(keep_io_types=True)
 
     if args.input_int32:
-        optimizer.change_input_to_int32()
+        optimizer.change_graph_inputs_to_int32()
 
     optimizer.save_model_to_file(args.output, args.use_external_data_format)
 
     if optimizer.is_fully_optimized():
         logger.info("The model has been fully optimized.")
     else:
         logger.info("The model has been optimized.")
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/parity_check_helper.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/parity_check_helper.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/profiler.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/profiler.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/quantize_helper.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/quantize_helper.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/shape_infer_helper.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/shape_infer_helper.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,22 +1,24 @@
 #-------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
 #--------------------------------------------------------------------------
 
 import os
 import sys
+import onnx
 
 # In ORT Package the symbolic_shape_infer.py is in ../tools
 file_path = os.path.dirname(__file__)
 if os.path.exists(os.path.join(file_path, "../tools/symbolic_shape_infer.py")):
     sys.path.append(os.path.join(file_path, '../tools'))
 else:
     sys.path.append(os.path.join(file_path, '..'))
-from symbolic_shape_infer import *
+
+from symbolic_shape_infer import SymbolicShapeInference, get_shape_from_type_proto, sympy
 
 
 class SymbolicShapeInferenceHelper(SymbolicShapeInference):
     def __init__(self, model, verbose=0, int_max=2**31 - 1, auto_merge=True, guess_output_rank=False):
         super().__init__(int_max, auto_merge, guess_output_rank, verbose)
         self.model_ = onnx.ModelProto()
         self.model_.CopyFrom(model)
@@ -36,18 +38,19 @@
 
         self.inferred_ = True
         return self.all_shapes_inferred_
 
     # override _preprocess() to avoid unnecessary model copy since ctor copies the model
     def _preprocess(self, in_mp):
         self.out_mp_ = in_mp
+        self.graph_inputs_ = dict([(i.name, i) for i in list(self.out_mp_.graph.input)])
         self.initializers_ = dict([(i.name, i) for i in self.out_mp_.graph.initializer])
         self.known_vi_ = dict([(i.name, i) for i in list(self.out_mp_.graph.input)])
         self.known_vi_.update(
-            dict([(i.name, helper.make_tensor_value_info(i.name, i.data_type, list(i.dims)))
+            dict([(i.name, onnx.helper.make_tensor_value_info(i.name, i.data_type, list(i.dims)))
                   for i in self.out_mp_.graph.initializer]))
 
     # Override _get_sympy_shape() in symbolic_shape_infer.py to ensure shape inference by giving the actual value of dynamic axis
     def _get_sympy_shape(self, node, idx):
         sympy_shape = []
         for d in self._get_shape(node, idx):
             if type(d) == str:
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/shape_optimizer.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/shape_optimizer.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/longformer/benchmark_longformer.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/longformer/benchmark_longformer.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/longformer/convert_longformer_to_onnx.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/longformer/convert_longformer_to_onnx.py`

 * *Files 0% similar despite different names*

```diff
@@ -257,28 +257,26 @@
 
     # Restore original implementaiton:
     LongformerSelfAttention.forward = original_forward
 
 
 def optimize_longformer(onnx_model_path, fp32_model_path, fp16_model_path=None):
     from onnx import load_model
-    from onnxruntime.transformers.onnx_model_bert import BertOnnxModel, BertOptimizationOptions
+    from onnxruntime.transformers.onnx_model_bert import BertOnnxModel
     model = load_model(onnx_model_path, format=None, load_external_data=True)
-    optimization_options = BertOptimizationOptions('bert')
-    optimizer = BertOnnxModel(model, num_heads=16,
-                              hidden_size=768)  # paramters does not matter since attention fusion is not needed.
-    optimizer.optimize(optimization_options)
+    optimizer = BertOnnxModel(model)
+    optimizer.optimize()
 
     use_external_data_format = False
     if fp32_model_path:
         optimizer.save_model_to_file(fp32_model_path, use_external_data_format)
         print(f"optimized fp32 model saved to {fp32_model_path}")
 
     if fp16_model_path:
-        optimizer.convert_model_float32_to_float16(cast_input_output=True)
+        optimizer.convert_float_to_float16(keep_io_types=True)
         optimizer.save_model_to_file(fp16_model_path, use_external_data_format)
         print(f"optimized fp16 model saved to {fp16_model_path}")
 
 
 def main(args):
     model_name = args.model
     onnx_model_path = model_name + ".onnx"
```

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/longformer/generate_test_data.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/longformer/generate_test_data.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/longformer/longformer_helper.py` & `onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/longformer/longformer_helper.py`

 * *Files identical despite different names*

## Comparing `onnxruntime-1.8.1.dist-info/METADATA` & `onnxruntime-1.9.0.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,50 +1,60 @@
 Metadata-Version: 2.1
 Name: onnxruntime
-Version: 1.8.1
+Version: 1.9.0
 Summary: ONNX Runtime is a runtime accelerator for Machine Learning models
 Home-page: https://onnxruntime.ai
 Author: Microsoft Corporation
 Author-email: onnxruntime@microsoft.com
 License: MIT License
 Download-URL: https://github.com/microsoft/onnxruntime/tags
 Keywords: onnx machine learning
 Platform: UNKNOWN
 Classifier: Development Status :: 5 - Production/Stable
 Classifier: Intended Audience :: Developers
 Classifier: License :: OSI Approved :: MIT License
 Classifier: Operating System :: POSIX :: Linux
-Classifier: Operating System :: Microsoft :: Windows
-Classifier: Operating System :: MacOS
 Classifier: Topic :: Scientific/Engineering
 Classifier: Topic :: Scientific/Engineering :: Mathematics
 Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
 Classifier: Topic :: Software Development
 Classifier: Topic :: Software Development :: Libraries
 Classifier: Topic :: Software Development :: Libraries :: Python Modules
 Classifier: Programming Language :: Python
 Classifier: Programming Language :: Python :: 3 :: Only
 Classifier: Programming Language :: Python :: 3.6
 Classifier: Programming Language :: Python :: 3.7
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
+Classifier: Operating System :: Microsoft :: Windows
+Classifier: Operating System :: MacOS
 Requires-Dist: numpy (>=1.16.6)
 Requires-Dist: protobuf
 Requires-Dist: flatbuffers
 
 ONNX Runtime
 ============
 
 ONNX Runtime is a performance-focused scoring engine for Open Neural Network Exchange (ONNX) models.
 For more information on ONNX Runtime, please see `aka.ms/onnxruntime <https://aka.ms/onnxruntime/>`_ or the `Github project <https://github.com/microsoft/onnxruntime/>`_.
 
 
 Changes
 -------
 
+1.9.0
+^^^^^
+
+Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.9.0
+
+1.8.2
+^^^^^
+
+Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.8.2
+
 1.8.1
 ^^^^^
 
 Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.8.1
 
 1.8.0
 ^^^^^
```

## Comparing `onnxruntime-1.8.1.dist-info/RECORD` & `onnxruntime-1.9.0.dist-info/RECORD`

 * *Files 14% similar despite different names*

```diff
@@ -1,140 +1,148 @@
-onnxruntime-1.8.1.data/purelib/onnxruntime/LICENSE,sha256=wlDWJ48LR6ZDn7dZKwi1ilXrn1NapJodtjIRw_mCtnQ,1094
-onnxruntime-1.8.1.data/purelib/onnxruntime/Privacy.md,sha256=v7dxKwdfPwfj6-5dwqKW0d4y2_ca0oZj9z0VOMtsOwg,2490
-onnxruntime-1.8.1.data/purelib/onnxruntime/ThirdPartyNotices.txt,sha256=WW97fKTMkS2foV-rzWgk1dwQB7Cm_9UNmIDuGy1BZZM,243881
-onnxruntime-1.8.1.data/purelib/onnxruntime/__init__.py,sha256=w9gFudhOp4YROq2J8RL0BoSxh-jVh56Inu6eYRrcH8M,2400
-onnxruntime-1.8.1.data/purelib/onnxruntime/backend/__init__.py,sha256=SQO4TOpHR3W4ZBU-3a9AqvXV5jKLdirR9erp51IPS2Y,320
-onnxruntime-1.8.1.data/purelib/onnxruntime/backend/backend.py,sha256=l6xhy_iA7G3-HXzmbDj46jxA4XxI61E2gbYId_iiAHE,6957
-onnxruntime-1.8.1.data/purelib/onnxruntime/backend/backend_rep.py,sha256=uzlpB5yrXRrh2PSCy6g8l5Auw9nOrE6S0P4UVxEZXCo,1812
-onnxruntime-1.8.1.data/purelib/onnxruntime/capi/__init__.py,sha256=uRp4pMtfoayBhZgEsiFqFCD13Y6LUo82FdZsQX8X8LI,251
-onnxruntime-1.8.1.data/purelib/onnxruntime/capi/_ld_preload.py,sha256=EiZaddZNGouUY_s0DbBoHsikv6pLYzDgpoVN9mA505I,487
-onnxruntime-1.8.1.data/purelib/onnxruntime/capi/_pybind_state.py,sha256=9bBs6IALw3AqKXJtLFCCC839CZi5awwrURCQ6OmlgZ4,823
-onnxruntime-1.8.1.data/purelib/onnxruntime/capi/onnxruntime_collect_build_info.py,sha256=gFi1fs3vOuSUjYrwedcSO-4lnz16Xi2kGocxumoqTXw,3795
-onnxruntime-1.8.1.data/purelib/onnxruntime/capi/onnxruntime_inference_collection.py,sha256=m-60UHVF6Fsm4abyaHl1NgSJrzoSfkFuL381QOPjJDI,23843
-onnxruntime-1.8.1.data/purelib/onnxruntime/capi/onnxruntime_providers_shared.dll,sha256=O54TWRAG3IWJOTBWZHjMwJ75DFro9rWH-WmvIclIq1o,19848
-onnxruntime-1.8.1.data/purelib/onnxruntime/capi/onnxruntime_pybind11_state.pyd,sha256=qSoWNurWz2Vx9-M4C2OTva_hu9xPNg6puCGELIGgHI4,13496696
-onnxruntime-1.8.1.data/purelib/onnxruntime/capi/onnxruntime_validation.py,sha256=so6e-nWghMcoYXig8sU2OGVfcTuttuU0eLOjV5_lG78,5975
-onnxruntime-1.8.1.data/purelib/onnxruntime/capi/version_info.py,sha256=g2pKZXlThlkMbp7JytLW6rOC3M9Scbx44P9WBLULNaQ,33
-onnxruntime-1.8.1.data/purelib/onnxruntime/capi/training/__init__.py,sha256=V63zeaS2MljWyqwkvAq6Elo8phOZdPGZNa_1fbZrIig,326
-onnxruntime-1.8.1.data/purelib/onnxruntime/datasets/__init__.py,sha256=nVYftV07eIY7anQBOdZ0RZOvqMSUdIQzK2hrdkLmydI,480
-onnxruntime-1.8.1.data/purelib/onnxruntime/datasets/logreg_iris.onnx,sha256=giR4TJjXNBLZ_ZmrzVejhWi9WQmA0PvlkWRkUxxS6Pw,670
-onnxruntime-1.8.1.data/purelib/onnxruntime/datasets/mul_1.onnx,sha256=cfQxxOkyHsb76xWNAu0kBFmn3MmGc_p5pPQ5zkLvrxA,130
-onnxruntime-1.8.1.data/purelib/onnxruntime/datasets/sigmoid.onnx,sha256=U0Crpnp-NHUWKteUN4r1XxcY9V-aXXS0r2Dsx_emJLY,103
-onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/__init__.py,sha256=MJQh09ou6UruKUFcZfmbswMEp_YNyduyGSuG9JsKsC8,313
-onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/calibrate.py,sha256=q9Oziff-Zh76Yis_3OA7YkySSr3ir8tK74m7k9bzoKg,22358
-onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/onnx_model.py,sha256=jIt15L0uRuKQughmnjxUQ-MjCq1M2m6tOVTbe0NVhCc,12554
-onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/onnx_quantizer.py,sha256=uvzukgJlXJt2-Tl5bPyYzrRjxjFnSvvDnKwqzHckgtY,43846
-onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/qdq_quantizer.py,sha256=YSsSr6izRlAGCF59-Q1V_X3WDse2R8DDF3W55L6uOig,9295
-onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/quant_utils.py,sha256=4rg_A0sXPVBrVpN4XfB49K_qUHHUj0_aOgeBeGiBzMg,13351
-onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/quantize.py,sha256=OsCF_44RPLc_Li-3vQ96NoGWjyB8aJEwZ2QeLj1LkC4,15181
-onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/registry.py,sha256=leAbaI9t5Q-5WUEpIwtxT4fXsqmU_v4Wb-H4-rKcEyg,2965
-onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/CalTableFlatBuffers/KeyValue.py,sha256=UxQjbxBK-JPACwUuapGqiXE2O22zNt19bpB6ZzX9WPs,1413
-onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/CalTableFlatBuffers/TrtTable.py,sha256=EdBdrhabPJ--Xy9AR6kz0Pdu_ic3GxNp6C2EjTvkISA,1765
-onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/CalTableFlatBuffers/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/__init__.py,sha256=w5Sho0iB_2D2eDEy_pkUrACvCdtshbxepiccDpghoY4,83
-onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/activation.py,sha256=ooKS4oI8HwkPNmOv3OdcomozqYxJw_C_Zt6y-fgiF2g,3674
-onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/attention.py,sha256=NBWz9BFq4azFox3fwFj2_XsT0WuJj_9cE1APyXvQNb4,1647
-onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/base_operator.py,sha256=AVm0UQ0zUiEdDkUP0tGle_jG9bglcizMkMkjCQwpSFw,980
-onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/binary_op.py,sha256=zikdjLLRJOOWcA4Yy5Pswud2U8vAjfsshch8XojJsBA,2449
-onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/concat.py,sha256=vekMQ0pgrxIQyV17WWJmov3Q0rgRF_e4pqunKhN-gcY,2253
-onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/conv.py,sha256=7E-t0YYL4mqAMoF-Nn17ubphcFlxwBzVCZAnhk5E9Gc,6932
-onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/direct_q8.py,sha256=m6bVCtR7WqWxxeU5CVQBqQWyhATTPy-D17CuNlrTyNA,1634
-onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/embed_layernorm.py,sha256=o8FCXcAltNgPg5e1uPhC4yBP_iYnR7_NM1fVf6goQyo,632
-onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/gather.py,sha256=s29AuGEOvSKGl0lhFCVVO72yV5vCXzoY6V-Kg9oepEI,1266
-onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/gavgpool.py,sha256=Ad3eYrq7DGFY26viCzkmV4SZ9BG4rafHXY1UoE38TF0,2178
-onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/lstm.py,sha256=qOSTU7EjDxA4SMqOfMyK7KfLIj3uux0T7nPvZZyLRxk,4836
-onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/matmul.py,sha256=zksBsZdo9ZUhDml_MjqAtSWnithm33Nk7RsCMCcUP5s,4698
-onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/maxpool.py,sha256=a2qlIlxXnZ4DwGXkinnBAxpN9fklCU0p8Dlx0lhrZkU,965
-onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/pad.py,sha256=4pAIooc1c_fmbQC1epmmmLG_ZZfx7vkw0YwGpR_UNTQ,4055
-onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/pooling.py,sha256=pCEYXXd46dl__opnCExNvp1lusKO-oBw7wnCp17onK0,1907
-onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/qdq_base_operator.py,sha256=HCF2LIDAxvIAeBrZEF-zhMknTgt3qH21p6vGqlipg2I,481
-onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/resize.py,sha256=NmEnkudFO-R1RTSjCf58hwcLEPUI2x3YRKAS9RWKOvY,966
-onnxruntime-1.8.1.data/purelib/onnxruntime/quantization/operators/split.py,sha256=blBsb0c_aZ3mWJnYqhn9l4w3ou1t6cTHFB8WXvc3uw4,1632
-onnxruntime-1.8.1.data/purelib/onnxruntime/tools/__init__.py,sha256=uRp4pMtfoayBhZgEsiFqFCD13Y6LUo82FdZsQX8X8LI,251
-onnxruntime-1.8.1.data/purelib/onnxruntime/tools/convert_onnx_models_to_ort.py,sha256=VniDwRIx5mSkfvVXcfnLPeZMogRFiwjvvT6iP7PHY0w,10517
-onnxruntime-1.8.1.data/purelib/onnxruntime/tools/logger.py,sha256=BNpnEYTgo6BHO9IKwaSOAJ_mM_6r693axlTkjtC9zFY,305
-onnxruntime-1.8.1.data/purelib/onnxruntime/tools/onnxruntime_test.py,sha256=6oAwQD99Ev2cCT4WWas0VHqN1Wf3xpVul6kB-rbKTjQ,5576
-onnxruntime-1.8.1.data/purelib/onnxruntime/tools/symbolic_shape_infer.py,sha256=JCNG2luVRpo04L71awa5P21xWRTK9__wMFPyI2kx9Cs,80422
-onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/__init__.py,sha256=3CuZeAO0_tiAy1XjOEbPZkiUZlfSnmZvW6dLvFeVmpc,1232
-onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/operator_type_usage_processors.py,sha256=Gm31EIIJJFaouCyQM4e1TiEv6Hbo0JLGOqTSID8Fd8Q,26813
-onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_model_processor.py,sha256=ln3o3tn2ZYW9aZN9Q53RSqY2eQM5T1-LbNrYesbEnkA,4483
-onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/types.py,sha256=20ixNivgxNc-B0NKBoZ0iNlgDhQtR4uGUqNQ0KWdwIA,4184
-onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/utils.py,sha256=6JYMGJMVRWnSienmmiBM0msEoK8osj6EPR87CF7hLiM,3721
-onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Attribute.py,sha256=Hf2eCCVNgx5y4_hXjrbaKDbT2kkiUZQ3eagIzXiPBY0,9362
-onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/AttributeType.py,sha256=1_69-oW8C6M_Wq2_1TXwXL_ryHXUpxIC42CU_QPYFss,348
-onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Dimension.py,sha256=v9CvPeXsaCeQposjhTAQ8M2yRE9RM3r74-EMk8EKY_Y,1805
-onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/DimensionValue.py,sha256=fqWUU7xUeEPZRCbjx1-T2XzSXCKhnSUWys1mngK5tyM,1988
-onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/DimensionValueType.py,sha256=MvwDY8Du8E08-7-lpr2YrB5eRtSbnruGacgQu8zZwBo,176
-onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/EdgeEnd.py,sha256=TH60nYg8iKcHZKO8-YpA2UMOLkNIFr79JdKoWXzbxTY,1076
-onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Graph.py,sha256=tbbnLF01TFWXP5kh8iy_Z0HDl6upZAvU_FLwdFToZk4,8459
-onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/InferenceSession.py,sha256=Tvyv8KqJvVrmrjhpwHBuMzbb9eJBIUZuIwswMNzy4No,2439
-onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/KernelCreateInfos.py,sha256=FZph33bM3telstU4ObWP5ga2_YNcjWCCqan_KMoyM7A,3515
-onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/MapType.py,sha256=CYxXDkdRMgDoJJRBblxXk7-d2LiBG9xzdoP2TGYSnmM,1741
-onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Model.py,sha256=01BOcXIVSad_Ysjz_63-yvefVCjlQoB95dA47MOr5XU,5037
-onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Node.py,sha256=_zKMfBlgdElb1MXY2vLW76Au0UMr7S6F2anaZo3K7Hk,8648
-onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/NodeEdge.py,sha256=eALfkrc2JHLqvTz3JcOaG7v3AYHDLEAubVixqpxodac,3365
-onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/NodeType.py,sha256=quSM-cnMeSAHZRPJdBXSCjC8Wnj30wc_1mKl9obfW4c,153
-onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/OperatorSetId.py,sha256=bZayVbeZdKnKi46Dkg1N4M70oHYaRdl2V87WHZPWIQE,1621
-onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/SequenceType.py,sha256=Lsu7uD59348AD2WUNPuIz5eW5bXb2VGlJOITLm0xSDs,1450
-onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/SessionState.py,sha256=wa8EvVZ6xkwarGDZ2avtF5SS_l_X1cCNZ8wSTWmQ8G4,2716
-onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Shape.py,sha256=zs2Q3H2tUK3nzMWeHkedZvU27POqBJHGYMr0Lghd2Tk,1902
-onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/SparseTensor.py,sha256=inawWBAe4CSO9euvwNicty6VCPe5u28D-JCkY-pH6BU,3159
-onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/SubGraphSessionState.py,sha256=Y0cXmvnEBc0upijbZ9_OvkrnWInrR2bbQkKp3FypjM8,1936
-onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Tensor.py,sha256=KYzRQPUQ3INAPl7SoTZhALOVWDyQGdXg-Gm5qjfUvHE,5144
-onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/TensorDataType.py,sha256=Q45onOhkRAMjauV4eytYveheZsnItgPOjKUKUI-dgzA,408
-onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/TensorTypeAndShape.py,sha256=Zy060idEx5jQXd8jASCRD9-t9qOcGC9l9rQBAuyQydU,1841
-onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/TypeInfo.py,sha256=BozLwO5lX5f___cutnpJDD2oJWOnJqDDaeJPJaqD_WQ,2039
-onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/TypeInfoValue.py,sha256=vLFbH5F9iaEEF5d0OSnt1bvlAzQ9dddPc7pDu5uhvLA,200
-onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/ValueInfo.py,sha256=eq1a2ETP2qN-rqLDEfsdGIMEuqeUwKGkmyx0O18aLmE,2131
-onnxruntime-1.8.1.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/__init__.py,sha256=Q1xYqjyNFFo57GhsZg6sr4EiYhUuLuFjAFCkINYC3tk,259
-onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/__init__.py,sha256=sKMNzX3MnOkXUK-JKQ9RHYMYj_kneEJqLTXLkVVMid4,69
-onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/benchmark.py,sha256=y22811eG-e3OV1DbfQZDsTDRT2_O7FpwZaZlOkV6QuU,26267
-onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/benchmark_gpt2.py,sha256=MdDFPgafNBAzGbeH8XekXr6M9cLCXWJk8KsAf1TMFnc,20886
-onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/benchmark_helper.py,sha256=OAiYXkuR9UOx4NdqwpJ7rJAiG62rbKtSUts9cnUwj4M,14205
-onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/bert_perf_test.py,sha256=_Hs0KrdNMq_Eo9w4qfokd7uMMMSl5AjvUI2a0cBODjo,13638
-onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/bert_test_data.py,sha256=5UlQxSy-PSGL5iB7l0AQ-GPEFiVxppRx1KNs4V0RF2k,18567
-onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/compare_bert_results.py,sha256=D_lAWhtccIxObViSmRK7ACAuVpiskwBq7Yl_IfBeUwY,8014
-onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/convert_tf_models_to_pytorch.py,sha256=FBnx3iZO3s5F_Ab8h68ys7C1zOCPS5I2tNb1YDsdokM,6317
-onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/convert_to_onnx.py,sha256=K_cVLhjf45IledryJV_AwNPVDWYYZ52V69bLm72lcoU,15866
-onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_attention.py,sha256=B8ISzEAYUbgHqyKU5-TOhBE2PdNoLeLwg8gontVSVWc,19141
-onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_base.py,sha256=qf4YEo--_pUlB5Pvmq3exZe3aSgoQ7Jr_ThihvwrG-k,2366
-onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_biasgelu.py,sha256=46iiGOzX-9kVL3PUzyWn-OJrDjWQpWKS92-OejvWWKM,2374
-onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_embedlayer.py,sha256=mhIl7oYcUnsrsCeMz4EyqwYjA33jg0PzPxMufRTkEEU,15782
-onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_fastgelu.py,sha256=_Swo7uZNAJ8f689nVALj5Q_XaUhonneO_lbUTsYM_18,13069
-onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_gelu.py,sha256=3rfj0weesqxrnA7qs-T7J-2kAWbgHK7givYmLNDa2vQ,10211
-onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_gelu_approximation.py,sha256=Lo7k30nCsTypl1MxaWHuIpOs0pas4C3Uf3T19mf_jHw,1105
-onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_gpt_attention.py,sha256=qfgHLyT_28k-AL2d20rdJs53DhBkD-RPV-VmDd4m_bM,12554
-onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_gpt_attention_no_past.py,sha256=o1tVqK5yqNcQGl7CZRl-_3oxtV--MIZOTuGycmK_E9M,8325
-onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_layernorm.py,sha256=9XalyMYEqvEAgw_0fn6Drz-0eenYLd4vm81lagp3JTE,11529
-onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_reshape.py,sha256=ALgrJj73ZIYIWPiIjnqu5zKbP_lUla5qxcdTgNU-TMc,6210
-onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_skiplayernorm.py,sha256=Lg2Kza2TeVPdVO-3TZgFxPA_ylW65gRZzZDYOjN5tZY,6494
-onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/fusion_utils.py,sha256=o9iY9xAuGFqa_JzAu0HFohe9Z6-n3GLvSdT0qzNbqzc,3060
-onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/gpt2_beamsearch_helper.py,sha256=ueWL-iHB9zpuMJXu-cpTCUl53KeClVEdfsXiDIvx60o,45031
-onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/gpt2_beamsearch_tester.py,sha256=A35ArdSnY8x7p01YauE0hanmWauLaXdM2SH0j5IcTV4,17517
-onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/gpt2_helper.py,sha256=LCR64iSpCSxgh8uteUY4KkQD4R2dkAd4NhSmv5Gee-s,30075
-onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/gpt2_tester.py,sha256=AlSlLG_0zNchkZX5gbB2SfPjUd186anSGTQgE80oEpU,19875
-onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/huggingface_models.py,sha256=SXu93xmJsltGXpRtCosWZjxrqunrk8ZSS6FQEwN-anU,8458
-onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/machine_info.py,sha256=bQWv2jrilm2Y9BhtAzDDhVoWCMcmnuCF45lB03tp9cU,6758
-onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/onnx_exporter.py,sha256=_8OhSKe0ZojVy6EX33AuUAD3gHvoZaiZwgPRajJ0Hp0,23352
-onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/onnx_model.py,sha256=mA8qZcduurvArQ0sd9bWRUWkd_RT_77Q86sHclfo1MU,36626
-onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/onnx_model_bert.py,sha256=_8hfooQPkS6ExVpph-7Ff_d2Mwktsrlz96C1X1O7s_8,15669
-onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/onnx_model_bert_keras.py,sha256=9V3mcxrz67m_vud_olk46ikyXcO2iwP3UGI5tqyhKL8,18301
-onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/onnx_model_bert_tf.py,sha256=QSPsgWDPVlCXSt9pXy9hVK2GKIRwL0kiyaxaaydfgjc,23486
-onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/onnx_model_gpt2.py,sha256=RsYA2R7KbkBlBCU_oWZyRO2TIAZVw-E5ffeBmuOohQg,3658
-onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/optimizer.py,sha256=UCdI9d0VoJugOy_UeGZUNMolqbYh9D8_KGvBaOdTVxM,15789
-onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/parity_check_helper.py,sha256=ASBqE6OI9ED2mdQ2DLxLTJ3OqN2mwOsflFk_WUKQAYs,6102
-onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/profiler.py,sha256=q5BPEFwca694EFpaz-NntbQKa8Ti5T4fO2hFXZddvr0,19306
-onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/quantize_helper.py,sha256=1kBX-8qCIQBxHkQS32I4mAYxwama6jx2a1OifKGs7ao,3409
-onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/shape_infer_helper.py,sha256=XW4DUE9_PIID1cHipizv9o9E1wc588SHrbLyhLNfeSA,3649
-onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/shape_optimizer.py,sha256=uiRVaznssGf9rte828cUCXjSvI5dxC9AXrOQWDs6GRM,15456
-onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/longformer/__init__.py,sha256=0-Q_xFLgrLR1MIgE6JNDjvoeKNnEQmUWd0jLGDs54Uk,388
-onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/longformer/benchmark_longformer.py,sha256=O8pOcgbmjjjTVuOLcKvjHonDAS2fBC3GH0B06IZCHfY,21397
-onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/longformer/convert_longformer_to_onnx.py,sha256=zdjDWhg__HEU4UCOJ4wkBrOzzXOYNXLOmFmd4YgCesw,14159
-onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/longformer/generate_test_data.py,sha256=a0-fregvps45-6oC-5cumMHyFs8CLN4hsnKYlPkTgaI,9294
-onnxruntime-1.8.1.data/purelib/onnxruntime/transformers/longformer/longformer_helper.py,sha256=_51oSaI_hgtIlNIC6-wy1Y5FA-sd8KaiSz7ml4fPqbY,3359
-onnxruntime-1.8.1.dist-info/METADATA,sha256=-ULUS84icUZkIeH2CV80dHt1B-2-Y09SDWlHJKtTW7Q,3066
-onnxruntime-1.8.1.dist-info/WHEEL,sha256=jr7ubY0Lkz_yXH9FfFe9PTtLhGOsf62dZkNvTYrJINE,100
-onnxruntime-1.8.1.dist-info/entry_points.txt,sha256=ziQA922fkGW-RIvlaEdwpnXp2SU5VxBrmcXUvL1g8iI,78
-onnxruntime-1.8.1.dist-info/top_level.txt,sha256=zk_fJEekrTm9DLxX2LwGegokVqP6blqPhFoMIuh0Nv8,12
-onnxruntime-1.8.1.dist-info/RECORD,,
+onnxruntime-1.9.0.data/purelib/onnxruntime/LICENSE,sha256=wlDWJ48LR6ZDn7dZKwi1ilXrn1NapJodtjIRw_mCtnQ,1094
+onnxruntime-1.9.0.data/purelib/onnxruntime/Privacy.md,sha256=v7dxKwdfPwfj6-5dwqKW0d4y2_ca0oZj9z0VOMtsOwg,2490
+onnxruntime-1.9.0.data/purelib/onnxruntime/ThirdPartyNotices.txt,sha256=bSNM7RXRpbaMn27EDqwSOc6DTN87HOYRZLFBGcSyEb4,246970
+onnxruntime-1.9.0.data/purelib/onnxruntime/__init__.py,sha256=ShjzUAgGtnwHZV-4CVvt4ddiYdJ7dwHnMOFboZ70VRY,2439
+onnxruntime-1.9.0.data/purelib/onnxruntime/backend/__init__.py,sha256=SQO4TOpHR3W4ZBU-3a9AqvXV5jKLdirR9erp51IPS2Y,320
+onnxruntime-1.9.0.data/purelib/onnxruntime/backend/backend.py,sha256=UIS7VZt8ayLN7jHwapD84v0ISPUWnQp-lVQ7wDY4oMg,7015
+onnxruntime-1.9.0.data/purelib/onnxruntime/backend/backend_rep.py,sha256=uzlpB5yrXRrh2PSCy6g8l5Auw9nOrE6S0P4UVxEZXCo,1812
+onnxruntime-1.9.0.data/purelib/onnxruntime/capi/__init__.py,sha256=uRp4pMtfoayBhZgEsiFqFCD13Y6LUo82FdZsQX8X8LI,251
+onnxruntime-1.9.0.data/purelib/onnxruntime/capi/_ld_preload.py,sha256=li6cbZ64hDfUndat4mprUWzowLa3RQdw0q2E56sXFwE,413
+onnxruntime-1.9.0.data/purelib/onnxruntime/capi/_pybind_state.py,sha256=CwQUH68zJstiGZFi9b_6k3L7x7R-RbX56M37Q-5VTZQ,837
+onnxruntime-1.9.0.data/purelib/onnxruntime/capi/onnxruntime_collect_build_info.py,sha256=gFi1fs3vOuSUjYrwedcSO-4lnz16Xi2kGocxumoqTXw,3795
+onnxruntime-1.9.0.data/purelib/onnxruntime/capi/onnxruntime_inference_collection.py,sha256=B7zWQnFrm4iQyFUwEf59KchPzq7sZAfy342-bL0hCtc,35718
+onnxruntime-1.9.0.data/purelib/onnxruntime/capi/onnxruntime_providers_shared.dll,sha256=mdot5vTqEZixeMM_8XgNNIbmc05iXxugviIZdv6owfA,19848
+onnxruntime-1.9.0.data/purelib/onnxruntime/capi/onnxruntime_pybind11_state.pyd,sha256=1neS-KJb5CkEbGb8e1hx3W1KiDlnLeu7pEY6ZAQnIJw,14809480
+onnxruntime-1.9.0.data/purelib/onnxruntime/capi/onnxruntime_validation.py,sha256=yLzBi-7v3cQ4oI9XYSssXF0oT79PZDCcZmPlGATc5rc,6299
+onnxruntime-1.9.0.data/purelib/onnxruntime/capi/version_info.py,sha256=g2pKZXlThlkMbp7JytLW6rOC3M9Scbx44P9WBLULNaQ,33
+onnxruntime-1.9.0.data/purelib/onnxruntime/capi/training/__init__.py,sha256=V63zeaS2MljWyqwkvAq6Elo8phOZdPGZNa_1fbZrIig,326
+onnxruntime-1.9.0.data/purelib/onnxruntime/datasets/__init__.py,sha256=nVYftV07eIY7anQBOdZ0RZOvqMSUdIQzK2hrdkLmydI,480
+onnxruntime-1.9.0.data/purelib/onnxruntime/datasets/logreg_iris.onnx,sha256=giR4TJjXNBLZ_ZmrzVejhWi9WQmA0PvlkWRkUxxS6Pw,670
+onnxruntime-1.9.0.data/purelib/onnxruntime/datasets/mul_1.onnx,sha256=cfQxxOkyHsb76xWNAu0kBFmn3MmGc_p5pPQ5zkLvrxA,130
+onnxruntime-1.9.0.data/purelib/onnxruntime/datasets/sigmoid.onnx,sha256=U0Crpnp-NHUWKteUN4r1XxcY9V-aXXS0r2Dsx_emJLY,103
+onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/__init__.py,sha256=MJQh09ou6UruKUFcZfmbswMEp_YNyduyGSuG9JsKsC8,313
+onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/calibrate.py,sha256=q9Oziff-Zh76Yis_3OA7YkySSr3ir8tK74m7k9bzoKg,22358
+onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/onnx_model.py,sha256=IiT1iZofuUGOCfif_YgwE1E6E5Idn1kZcvvr_i7bRLM,14885
+onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/onnx_quantizer.py,sha256=5I886eiWenM2IMZJ0B6vssvAhEd5Hxwfi_iend8avAg,45468
+onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/qdq_quantizer.py,sha256=YSsSr6izRlAGCF59-Q1V_X3WDse2R8DDF3W55L6uOig,9295
+onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/quant_utils.py,sha256=Bmjx-G-aEXBd6MbEkiZAdwZWrtZ_amCqZDwNBCuEBE8,14265
+onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/quantize.py,sha256=1W0yWWv4tYxqj_zAbBM53NKLYWEZ7vHPvhKJrQ8_wak,15686
+onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/registry.py,sha256=leAbaI9t5Q-5WUEpIwtxT4fXsqmU_v4Wb-H4-rKcEyg,2965
+onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/CalTableFlatBuffers/KeyValue.py,sha256=UxQjbxBK-JPACwUuapGqiXE2O22zNt19bpB6ZzX9WPs,1413
+onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/CalTableFlatBuffers/TrtTable.py,sha256=sSc14EOcbrcU8Zk3N8qX_43saFqIaeTuuD5NgMaSMZk,1790
+onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/CalTableFlatBuffers/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/__init__.py,sha256=w5Sho0iB_2D2eDEy_pkUrACvCdtshbxepiccDpghoY4,83
+onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/activation.py,sha256=QIthJtrQiwfUBJlFduBxcGKMMyYn49it_DKaWY53zKU,3692
+onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/attention.py,sha256=x5JEgUZJJ6RxKWH-EcBPgE1JGGxnV51_LrQTTkoq96E,1729
+onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/base_operator.py,sha256=ABzwsO0yAzwvbajhveDvgF4AaqA4uZomcgCTSAZLsI8,947
+onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/binary_op.py,sha256=UDw6ecGkdUqiyrnZ2pxzrCwKqyTguKT0U5ejNijLynw,2412
+onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/concat.py,sha256=PVHWEG8S-AKNvc6jYvAp_d8mdVagatnysktqBixutx4,2185
+onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/conv.py,sha256=C0ATfkJacb2vmBf40VAPile1_OsditYP4R5XDSiUn4I,8196
+onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/direct_q8.py,sha256=m6bVCtR7WqWxxeU5CVQBqQWyhATTPy-D17CuNlrTyNA,1634
+onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/embed_layernorm.py,sha256=SzU9uGBPIGuAm4Kjkpul-h4EOWNSzuwj-zd_54irhbQ,3682
+onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/gather.py,sha256=bF87YzynH1HOl8vGrgjOlrEFsVZp5n3r_C0UyE8K5Sw,1348
+onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/gavgpool.py,sha256=QrmwXZCOy88WzRgGfZ7SCeU1bHQNQyeYcRojQASVIMs,2180
+onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/lstm.py,sha256=qOSTU7EjDxA4SMqOfMyK7KfLIj3uux0T7nPvZZyLRxk,4836
+onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/matmul.py,sha256=I5aNGgVyA8oI-6kok59_UoxgtMjrwlMjZqjpORoAXio,4610
+onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/maxpool.py,sha256=a2qlIlxXnZ4DwGXkinnBAxpN9fklCU0p8Dlx0lhrZkU,965
+onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/pad.py,sha256=kPixUdS7EF9sioxlG5wDKOpWQ8_X_A2_5j4iU7jRX2c,4135
+onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/pooling.py,sha256=KuT1Iw5Osm9b1flnpnn3GMJXQR0nGqpjZ9XrllW1lyQ,1940
+onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/qdq_base_operator.py,sha256=HCF2LIDAxvIAeBrZEF-zhMknTgt3qH21p6vGqlipg2I,481
+onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/resize.py,sha256=NmEnkudFO-R1RTSjCf58hwcLEPUI2x3YRKAS9RWKOvY,966
+onnxruntime-1.9.0.data/purelib/onnxruntime/quantization/operators/split.py,sha256=OISWo9uWnPu7LlGfPts4MemgjhuViaUBEuqa38-XLKs,1716
+onnxruntime-1.9.0.data/purelib/onnxruntime/tools/__init__.py,sha256=uRp4pMtfoayBhZgEsiFqFCD13Y6LUo82FdZsQX8X8LI,251
+onnxruntime-1.9.0.data/purelib/onnxruntime/tools/convert_onnx_models_to_ort.py,sha256=UE_sNGRd4vja-7JYl0hoybFiQpucv4fq8lp7W4ZKvrk,12751
+onnxruntime-1.9.0.data/purelib/onnxruntime/tools/logger.py,sha256=BNpnEYTgo6BHO9IKwaSOAJ_mM_6r693axlTkjtC9zFY,305
+onnxruntime-1.9.0.data/purelib/onnxruntime/tools/onnxruntime_test.py,sha256=6oAwQD99Ev2cCT4WWas0VHqN1Wf3xpVul6kB-rbKTjQ,5576
+onnxruntime-1.9.0.data/purelib/onnxruntime/tools/pytorch_export_contrib_ops.py,sha256=EAjXe9hpJZTmhmabuGFpL6Ng5T_weVzmxfPz8KEn8Zc,3415
+onnxruntime-1.9.0.data/purelib/onnxruntime/tools/symbolic_shape_infer.py,sha256=y37mesNKw3A7j8x2XXWjcZNY90nlXaWUQ-3h8Uzmpgg,99985
+onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/__init__.py,sha256=3CuZeAO0_tiAy1XjOEbPZkiUZlfSnmZvW6dLvFeVmpc,1232
+onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/operator_type_usage_processors.py,sha256=DWvGRiM_oiwanm-EACkudO17FoC9EzeYlT8IAR-UHd8,27619
+onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_model_processor.py,sha256=ln3o3tn2ZYW9aZN9Q53RSqY2eQM5T1-LbNrYesbEnkA,4483
+onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/types.py,sha256=20ixNivgxNc-B0NKBoZ0iNlgDhQtR4uGUqNQ0KWdwIA,4184
+onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/utils.py,sha256=6JYMGJMVRWnSienmmiBM0msEoK8osj6EPR87CF7hLiM,3721
+onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Attribute.py,sha256=Hf2eCCVNgx5y4_hXjrbaKDbT2kkiUZQ3eagIzXiPBY0,9362
+onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/AttributeType.py,sha256=1_69-oW8C6M_Wq2_1TXwXL_ryHXUpxIC42CU_QPYFss,348
+onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Dimension.py,sha256=v9CvPeXsaCeQposjhTAQ8M2yRE9RM3r74-EMk8EKY_Y,1805
+onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/DimensionValue.py,sha256=fqWUU7xUeEPZRCbjx1-T2XzSXCKhnSUWys1mngK5tyM,1988
+onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/DimensionValueType.py,sha256=MvwDY8Du8E08-7-lpr2YrB5eRtSbnruGacgQu8zZwBo,176
+onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/EdgeEnd.py,sha256=TH60nYg8iKcHZKO8-YpA2UMOLkNIFr79JdKoWXzbxTY,1076
+onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Graph.py,sha256=tbbnLF01TFWXP5kh8iy_Z0HDl6upZAvU_FLwdFToZk4,8459
+onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/InferenceSession.py,sha256=Tvyv8KqJvVrmrjhpwHBuMzbb9eJBIUZuIwswMNzy4No,2439
+onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/KernelCreateInfos.py,sha256=FZph33bM3telstU4ObWP5ga2_YNcjWCCqan_KMoyM7A,3515
+onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/MapType.py,sha256=CYxXDkdRMgDoJJRBblxXk7-d2LiBG9xzdoP2TGYSnmM,1741
+onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Model.py,sha256=TcL5sFPWboQSSjQ6TtS1-JiWlrzSbTdq_8O5hkTQkzc,6184
+onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Node.py,sha256=_zKMfBlgdElb1MXY2vLW76Au0UMr7S6F2anaZo3K7Hk,8648
+onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/NodeEdge.py,sha256=eALfkrc2JHLqvTz3JcOaG7v3AYHDLEAubVixqpxodac,3365
+onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/NodeType.py,sha256=quSM-cnMeSAHZRPJdBXSCjC8Wnj30wc_1mKl9obfW4c,153
+onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/OperatorSetId.py,sha256=bZayVbeZdKnKi46Dkg1N4M70oHYaRdl2V87WHZPWIQE,1621
+onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/SequenceType.py,sha256=Lsu7uD59348AD2WUNPuIz5eW5bXb2VGlJOITLm0xSDs,1450
+onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/SessionState.py,sha256=wa8EvVZ6xkwarGDZ2avtF5SS_l_X1cCNZ8wSTWmQ8G4,2716
+onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Shape.py,sha256=zs2Q3H2tUK3nzMWeHkedZvU27POqBJHGYMr0Lghd2Tk,1902
+onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/SparseTensor.py,sha256=inawWBAe4CSO9euvwNicty6VCPe5u28D-JCkY-pH6BU,3159
+onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/StringStringEntry.py,sha256=AB9-zZAM9bJjrWNbe8QIOR37i92jPeBGlgd1uRp0vD0,1673
+onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/SubGraphSessionState.py,sha256=Y0cXmvnEBc0upijbZ9_OvkrnWInrR2bbQkKp3FypjM8,1936
+onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/Tensor.py,sha256=KYzRQPUQ3INAPl7SoTZhALOVWDyQGdXg-Gm5qjfUvHE,5144
+onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/TensorDataType.py,sha256=Q45onOhkRAMjauV4eytYveheZsnItgPOjKUKUI-dgzA,408
+onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/TensorTypeAndShape.py,sha256=Zy060idEx5jQXd8jASCRD9-t9qOcGC9l9rQBAuyQydU,1841
+onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/TypeInfo.py,sha256=BozLwO5lX5f___cutnpJDD2oJWOnJqDDaeJPJaqD_WQ,2039
+onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/TypeInfoValue.py,sha256=vLFbH5F9iaEEF5d0OSnt1bvlAzQ9dddPc7pDu5uhvLA,200
+onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/ValueInfo.py,sha256=eq1a2ETP2qN-rqLDEfsdGIMEuqeUwKGkmyx0O18aLmE,2131
+onnxruntime-1.9.0.data/purelib/onnxruntime/tools/ort_format_model/ort_flatbuffers_py/experimental/fbs/__init__.py,sha256=Q1xYqjyNFFo57GhsZg6sr4EiYhUuLuFjAFCkINYC3tk,259
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/__init__.py,sha256=sKMNzX3MnOkXUK-JKQ9RHYMYj_kneEJqLTXLkVVMid4,69
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/affinity_helper.py,sha256=7GJjcWxRgu5HeUImSRRbfSIPdGEqCn7PJWgca-HlxcM,1361
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/benchmark.py,sha256=d3EYKXROESCtDbxrAuTOii6lK7qxKwudV7jZNJ1KLdY,26247
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/benchmark_gpt2.py,sha256=K7eURk2LP0hjQ5yzwTPH_sHaU4eDiBFmRhB4zueP4AM,20898
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/benchmark_helper.py,sha256=5P1nngpmDPin4hsqJRymzqipZVD50ammIsxy4J-cO18,14224
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/bert_perf_test.py,sha256=aHagGYzldtDvOk8GnagZ2di6NqLA5QYLRsK5eBmrbkg,13722
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/bert_test_data.py,sha256=5UlQxSy-PSGL5iB7l0AQ-GPEFiVxppRx1KNs4V0RF2k,18567
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/compare_bert_results.py,sha256=D_lAWhtccIxObViSmRK7ACAuVpiskwBq7Yl_IfBeUwY,8014
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/convert_tf_models_to_pytorch.py,sha256=zlHsOFvm1l5O4uV1ygYKOH8fPJx2eOE_mRj9Iq01WGs,6313
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/convert_to_onnx.py,sha256=uFECtOoYTdF_6MqSAcuOW9cMBKAr7Lrd2XTDVpGgYiI,22365
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/float16.py,sha256=vF9CWUlGqwFHFPT76xIdLFwlkA_NqJppWRhbjaHd8-w,16730
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_attention.py,sha256=zOnQYn5C7lfVQE6kPhRjju6m8B-Zjs0d8PxXXZUCzl4,22424
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_base.py,sha256=J5GBC2DVDgYLGAIhUVSxNQfoWvnsWtFCPKVaEoKsAV8,2538
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_biasgelu.py,sha256=46iiGOzX-9kVL3PUzyWn-OJrDjWQpWKS92-OejvWWKM,2374
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_embedlayer.py,sha256=zCi85H5h_utbAaWmSqzsteVQwzlsJaika0wHnpuGcrY,26736
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_fastgelu.py,sha256=xUQf5zqRMRM2H6GmA7i0NEvjuW0g08wT0PEKtjXbwqM,13363
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_gelu.py,sha256=3rfj0weesqxrnA7qs-T7J-2kAWbgHK7givYmLNDa2vQ,10211
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_gelu_approximation.py,sha256=Lo7k30nCsTypl1MxaWHuIpOs0pas4C3Uf3T19mf_jHw,1105
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_gpt_attention.py,sha256=yZ0G-xP-Ox7KGRn4SpA_KloXcNDXfM2DZzbLd5rben8,18383
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_gpt_attention_no_past.py,sha256=o1tVqK5yqNcQGl7CZRl-_3oxtV--MIZOTuGycmK_E9M,8325
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_layernorm.py,sha256=ERv1IU9Oq2_wxywnKsMrC_XB7Sdz5aht7xrx7WQ7FV4,11381
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_options.py,sha256=qySBScuat4v52K7JgulZn_maVgiW4-cgkjyrJQc4WCQ,5128
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_reshape.py,sha256=SfP9lPKbicuxBmYhpyLOpS2NcF6-_FgCnTF8xa_IC04,6352
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_shape.py,sha256=m7NC0XcFTZLw3opkB9EqSQIGDoBteZUVG-SXg38d8qc,3788
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_skiplayernorm.py,sha256=YogzQYq9v93HRqEMWd5xyWtLFvp5ziK9fnzhACpQrwg,6776
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/fusion_utils.py,sha256=Oh9TG3YT-YMSH7hBSesylQsuI7CJ1h7JpXK-LFpGels,6407
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/gpt2_beamsearch_helper.py,sha256=KXCrFZ-V9CTPrUb3niMaQwG-mKqT8ZkDSoEKVc5CANQ,45738
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/gpt2_beamsearch_tester.py,sha256=A35ArdSnY8x7p01YauE0hanmWauLaXdM2SH0j5IcTV4,17517
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/gpt2_helper.py,sha256=ytKAONCUmLpOVrDeByqRTr8VJawNWYlnE_hXkHIjP-4,37958
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/gpt2_parity.py,sha256=XAN1PXuShZzOXfrKWY_jUaZAetaCCEiaTX5tHTqJ5wQ,6539
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/gpt2_tester.py,sha256=AlSlLG_0zNchkZX5gbB2SfPjUd186anSGTQgE80oEpU,19875
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/huggingface_models.py,sha256=r5N30mg-uZyUyLCki_M5G-GmCHFdI5CbaQ75xtDNlqo,8502
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/machine_info.py,sha256=dbP87GIJX8nvADFO8l2vO21J5_LcslFYzN_TpI5ndTY,6960
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/onnx_exporter.py,sha256=Q56K-ZFDp1aqU4hx2RXs0qjYfcfhQisl2e36BabWcNM,23647
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/onnx_model.py,sha256=5NHMW31-1X_uphHz1hqtLzAiOkmmXhbgcVKBcHEEobc,38053
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/onnx_model_bart.py,sha256=go2gMJVNZ-WZEiStvFX_6ViuPvrscr6sojrwzo4uc9M,11567
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/onnx_model_bert.py,sha256=JBEyw2IbFFvySfLNucLiY0TubfnLsKTBl7XhXY5KOzA,17628
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/onnx_model_bert_keras.py,sha256=8PzWT5irnhKOt9cmye2CIGPJO7zMw7kuBJtyg8czFPs,18311
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/onnx_model_bert_tf.py,sha256=hUnOEEA8t80Dqxx8YtP2Nq0zhYHfkzpInTfUSXFEuEw,23496
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/onnx_model_gpt2.py,sha256=eGblgcEEgm-g6NbobTrf3oahDKFtwuRUFHNVz93XO1o,3524
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/optimizer.py,sha256=-bs9GXSYDiIghdIGGfrIBQ2rBpR0fLzXMAfkKjs6zNk,15290
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/parity_check_helper.py,sha256=ASBqE6OI9ED2mdQ2DLxLTJ3OqN2mwOsflFk_WUKQAYs,6102
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/profiler.py,sha256=q5BPEFwca694EFpaz-NntbQKa8Ti5T4fO2hFXZddvr0,19306
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/quantize_helper.py,sha256=1kBX-8qCIQBxHkQS32I4mAYxwama6jx2a1OifKGs7ao,3409
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/shape_infer_helper.py,sha256=G9wm5H98U4mA1AgOfgYKBPX6lJ2rrnyX0SYKMydD8Xs,3814
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/shape_optimizer.py,sha256=uiRVaznssGf9rte828cUCXjSvI5dxC9AXrOQWDs6GRM,15456
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/longformer/__init__.py,sha256=0-Q_xFLgrLR1MIgE6JNDjvoeKNnEQmUWd0jLGDs54Uk,388
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/longformer/benchmark_longformer.py,sha256=O8pOcgbmjjjTVuOLcKvjHonDAS2fBC3GH0B06IZCHfY,21397
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/longformer/convert_longformer_to_onnx.py,sha256=RpT1uGKGCQvMV2wqDcXOfogEZT4yQNbp1lQPCj9Jm8w,13913
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/longformer/generate_test_data.py,sha256=a0-fregvps45-6oC-5cumMHyFs8CLN4hsnKYlPkTgaI,9294
+onnxruntime-1.9.0.data/purelib/onnxruntime/transformers/longformer/longformer_helper.py,sha256=_51oSaI_hgtIlNIC6-wy1Y5FA-sd8KaiSz7ml4fPqbY,3359
+onnxruntime-1.9.0.dist-info/METADATA,sha256=sldVdffnEz4OOZpRcZBKEPPA2VmbbL5rq0HeU40jWrg,3248
+onnxruntime-1.9.0.dist-info/WHEEL,sha256=MRZ_p7RU4olp1XL4U2EYLkuYikriaVRqXBoKVLH_OSE,100
+onnxruntime-1.9.0.dist-info/entry_points.txt,sha256=ziQA922fkGW-RIvlaEdwpnXp2SU5VxBrmcXUvL1g8iI,78
+onnxruntime-1.9.0.dist-info/top_level.txt,sha256=zk_fJEekrTm9DLxX2LwGegokVqP6blqPhFoMIuh0Nv8,12
+onnxruntime-1.9.0.dist-info/RECORD,,
```

