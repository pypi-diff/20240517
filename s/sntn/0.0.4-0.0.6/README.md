# Comparing `tmp/sntn-0.0.4-py2.py3-none-any.whl.zip` & `tmp/sntn-0.0.6-py2.py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,18 +1,29 @@
-Zip file size: 37377 bytes, number of entries: 16
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-05 14:13 sntn/__init__.py
--rw-r--r--  2.0 unx     1223 b- defN 23-May-20 21:21 sntn/__main__.py
--rw-r--r--  2.0 unx     6413 b- defN 23-May-20 20:56 sntn/_bvn.py
--rw-r--r--  2.0 unx    15434 b- defN 23-May-20 20:56 sntn/_lasso.py
--rw-r--r--  2.0 unx    19228 b- defN 23-May-15 21:09 sntn/_nts.py
--rw-r--r--  2.0 unx    11918 b- defN 23-May-20 20:56 sntn/_screening.py
--rw-r--r--  2.0 unx    25028 b- defN 23-May-13 02:59 sntn/_solvers.py
--rw-r--r--  2.0 unx     3336 b- defN 23-May-20 20:56 sntn/_split.py
--rw-r--r--  2.0 unx    11996 b- defN 23-May-15 21:09 sntn/_tnorm.py
--rw-r--r--  2.0 unx     1182 b- defN 23-May-08 19:07 sntn/dists.py
--rw-r--r--  2.0 unx      632 b- defN 23-May-15 21:09 sntn/posi.py
--rw-r--r--  2.0 unx     1869 b- defN 23-May-20 20:56 sntn/trialML.py
--rw-r--r--  2.0 unx    10563 b- defN 23-May-20 21:21 sntn-0.0.4.dist-info/METADATA
--rw-r--r--  2.0 unx      110 b- defN 23-May-20 21:21 sntn-0.0.4.dist-info/WHEEL
--rw-r--r--  2.0 unx        5 b- defN 23-May-20 21:21 sntn-0.0.4.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     1144 b- defN 23-May-20 21:21 sntn-0.0.4.dist-info/RECORD
-16 files, 110081 bytes uncompressed, 35563 bytes compressed:  67.7%
+Zip file size: 68056 bytes, number of entries: 27
+-rw-r--r--  2.0 unx        0 b- defN 24-May-08 17:40 sntn/__init__.py
+-rw-r--r--  2.0 unx     1139 b- defN 24-May-08 19:10 sntn/__main__.py
+-rw-r--r--  2.0 unx     6325 b- defN 24-May-08 19:27 sntn/_bvn.py
+-rw-r--r--  2.0 unx    15431 b- defN 24-May-08 18:42 sntn/_lasso.py
+-rw-r--r--  2.0 unx    19181 b- defN 24-May-08 18:42 sntn/_nts.py
+-rw-r--r--  2.0 unx    11864 b- defN 24-May-08 18:41 sntn/_screening.py
+-rw-r--r--  2.0 unx    25008 b- defN 24-May-08 18:50 sntn/_solvers.py
+-rw-r--r--  2.0 unx     3333 b- defN 24-May-08 18:39 sntn/_split.py
+-rw-r--r--  2.0 unx    11988 b- defN 24-May-08 18:39 sntn/_tnorm.py
+-rw-r--r--  2.0 unx     1161 b- defN 24-May-08 18:43 sntn/dists.py
+-rw-r--r--  2.0 unx      632 b- defN 24-May-08 17:40 sntn/posi.py
+-rw-r--r--  2.0 unx     1869 b- defN 24-May-08 17:40 sntn/trialML.py
+-rw-r--r--  2.0 unx        0 b- defN 24-May-08 17:40 sntn/_cdf_bvn/__init__.py
+-rw-r--r--  2.0 unx     8466 b- defN 24-May-08 19:27 sntn/_cdf_bvn/_approx.py
+-rw-r--r--  2.0 unx     7472 b- defN 24-May-08 19:27 sntn/_cdf_bvn/_brute.py
+-rw-r--r--  2.0 unx     2074 b- defN 24-May-08 17:40 sntn/_cdf_bvn/_utils.py
+-rw-r--r--  2.0 unx       94 b- defN 24-May-08 17:40 sntn/benchmark/readme.md
+-rw-r--r--  2.0 unx      192 b- defN 24-May-08 17:40 sntn/benchmark/runtime.py
+-rw-r--r--  2.0 unx        0 b- defN 24-May-08 17:40 sntn/utilities/__init__.py
+-rw-r--r--  2.0 unx     3170 b- defN 24-May-08 17:40 sntn/utilities/grad.py
+-rw-r--r--  2.0 unx     5962 b- defN 24-May-08 17:40 sntn/utilities/linear.py
+-rw-r--r--  2.0 unx    22811 b- defN 24-May-08 17:40 sntn/utilities/utils.py
+-rw-r--r--  2.0 unx    35150 b- defN 24-May-08 19:31 sntn-0.0.6.dist-info/LICENSE.txt
+-rw-r--r--  2.0 unx    11051 b- defN 24-May-08 19:31 sntn-0.0.6.dist-info/METADATA
+-rw-r--r--  2.0 unx      110 b- defN 24-May-08 19:31 sntn-0.0.6.dist-info/WHEEL
+-rw-r--r--  2.0 unx        5 b- defN 24-May-08 19:31 sntn-0.0.6.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     2035 b- defN 24-May-08 19:31 sntn-0.0.6.dist-info/RECORD
+27 files, 196523 bytes uncompressed, 64864 bytes compressed:  67.0%
```

## zipnote {}

```diff
@@ -30,20 +30,53 @@
 
 Filename: sntn/posi.py
 Comment: 
 
 Filename: sntn/trialML.py
 Comment: 
 
-Filename: sntn-0.0.4.dist-info/METADATA
+Filename: sntn/_cdf_bvn/__init__.py
 Comment: 
 
-Filename: sntn-0.0.4.dist-info/WHEEL
+Filename: sntn/_cdf_bvn/_approx.py
 Comment: 
 
-Filename: sntn-0.0.4.dist-info/top_level.txt
+Filename: sntn/_cdf_bvn/_brute.py
 Comment: 
 
-Filename: sntn-0.0.4.dist-info/RECORD
+Filename: sntn/_cdf_bvn/_utils.py
+Comment: 
+
+Filename: sntn/benchmark/readme.md
+Comment: 
+
+Filename: sntn/benchmark/runtime.py
+Comment: 
+
+Filename: sntn/utilities/__init__.py
+Comment: 
+
+Filename: sntn/utilities/grad.py
+Comment: 
+
+Filename: sntn/utilities/linear.py
+Comment: 
+
+Filename: sntn/utilities/utils.py
+Comment: 
+
+Filename: sntn-0.0.6.dist-info/LICENSE.txt
+Comment: 
+
+Filename: sntn-0.0.6.dist-info/METADATA
+Comment: 
+
+Filename: sntn-0.0.6.dist-info/WHEEL
+Comment: 
+
+Filename: sntn-0.0.6.dist-info/top_level.txt
+Comment: 
+
+Filename: sntn-0.0.6.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## sntn/__main__.py

```diff
@@ -1,41 +1,36 @@
 """
 Checks that package has configured properly, `python3 -m sntn`, will run this script. Runs the scripts contained in the readme.md.
 
 For package updates:
 # Clean up old wheels
-1) rm dist/* 
+1) rm -r dist/
 2) python setup.py bdist_wheel --universal
 # On some test conda env
 1) twine upload --repository-url https://test.pypi.org/legacy/ dist/sntn*
 2) pip uninstall sntn
-3) pip install -i https://test.pypi.org/simple/ sntn==0.0.3 
-# Upload to PYPI
-1) https://pypi.org/project/sntn/
-2) twine upload dist/sntn*
-3) pip uninstall sntn
-4) pip install sntn
+3) pip install -i https://test.pypi.org/simple/ sntn==X.X.X
+# Upload to PYPI https://pypi.org/project/sntn/
+1) twine upload dist/sntn*
+2) pip uninstall sntn
+3) pip install sntn
 
 """
 
 # Load dependencies
 import numpy as np
-import pandas as pd
-from glmnet import ElasticNet
-from scipy.stats import norm
 from sntn.dists import nts
 
 def fun_main() -> None:
     # Check that classic 1964 technometrics query works
     mu1, tau21 = 100, 6**2
     mu2, tau22 = 50, 3**2
     a, b = 44, np.inf
     w = 138
     dist_1964 = nts(mu1, tau21, mu2, tau22, a, b)
     expected_1964 = 0.03276
     cdf_1964 = dist_1964.cdf(w)[0]
-    assert np.round(cdf_1964,5) == expected_1964, F'Expected CDF to be: {expected_1964} not {cdf_1964}, nts package did not compile properly!!' 
-
+    assert np.round(cdf_1964,5) == expected_1964, F'Expected CDF to be: {expected_1964} not {cdf_1964}, nts package did not compile properly!!'
 
 if __name__ == '__main__':
     fun_main()
     print('~~~ The sntn package was successfully compiled ~~~')
```

## sntn/_bvn.py

```diff
@@ -2,26 +2,25 @@
 Main bivariate normal class
 """
 
 # External
 import numpy as np
 from scipy.linalg import cholesky
 # Internal
-from sntn._cdf_bvn._utils import cdf_to_orthant
 from sntn.utilities.utils import process_x_x1_x2
-from sntn._cdf_bvn._approx import _bvn_cox, valid_cox_approach
-from sntn._cdf_bvn._brute import _bvn_scipy, valid_quad_approach, _bvn_quad
-from sntn.utilities.utils import broastcast_max_shape, try2array, broadcast_to_k, reverse_broadcast_from_k
+from sntn._cdf_bvn._approx import bvn_cox, valid_cox_approach
+from sntn._cdf_bvn._brute import bvn_scipy, valid_quad_approach, _bvn_quad
+from sntn.utilities.utils import broastcast_max_shape, reverse_broadcast_from_k
 
 # Accepted CDF methods
 valid_cdf_approach = ['scipy'] + valid_cox_approach + valid_quad_approach
 
 
 class _bvn():
-    def __init__(self, mu1:float or np.ndarray, sigma21:float or np.ndarray, mu2:float or np.ndarray, sigma22:float or np.ndarray, rho:float or np.ndarray, cdf_approach:str='owen', **kwargs) -> None:
+    def __init__(self, mu1:float | np.ndarray, sigma21:float | np.ndarray, mu2:float | np.ndarray, sigma22:float | np.ndarray, rho:float | np.ndarray, cdf_approach:str='owen', **kwargs) -> None:
         """
         Main workhorse class for a bivariate normal distribution:
 
         X1 ~ N(mu1, sigma21), X2 ~ N(mu2, sigma22)
         corr(X1, X2) = rho
         [X1; X2] ~ BVN( [mu1, mu2], [[sigma21, rho], [rho, sigma22]] )
         
@@ -60,19 +59,19 @@
         assert isinstance(cdf_approach, str), 'cdf_approach needs to be a string'
         assert cdf_approach in valid_cdf_approach, f'cdf approach must be one of: {valid_cdf_approach}'
         # Capture the original shape for later transformations
         self.param_shape = mu1.shape
         # Prepare cdf method (important to assign before we flatten as well)
         self.cdf_approach = cdf_approach
         if self.cdf_approach == 'cox1':
-            self.cdf_method = _bvn_cox(mu1, mu2, sigma21, sigma22, rho, monte_carlo=True, **kwargs)
+            self.cdf_method = bvn_cox(mu1, mu2, sigma21, sigma22, rho, monte_carlo=True, **kwargs)
         if self.cdf_approach == 'cox2':
-            self.cdf_method = _bvn_cox(mu1, mu2, sigma21, sigma22, rho, monte_carlo=False, **kwargs)
+            self.cdf_method = bvn_cox(mu1, mu2, sigma21, sigma22, rho, monte_carlo=False, **kwargs)
         if self.cdf_approach == 'scipy':
-            self.cdf_method = _bvn_scipy(mu1, mu2, sigma21, sigma22, rho)
+            self.cdf_method = bvn_scipy(mu1, mu2, sigma21, sigma22, rho)
         if self.cdf_approach == 'owen':
             self.cdf_method = _bvn_quad(mu1, mu2, sigma21, sigma22, rho, 'owen')
         if self.cdf_approach == 'drezner1':
             self.cdf_method = _bvn_quad(mu1, mu2, sigma21, sigma22, rho, 'drezner1')
         if self.cdf_approach == 'drezner2':
             self.cdf_method = _bvn_quad(mu1, mu2, sigma21, sigma22, rho, 'drezner2')
         
@@ -92,15 +91,15 @@
         self.Sigma = np.stack([self.sigma21, self.sigma1*self.sigma2*self.rho, self.sigma1*self.sigma2*self.rho, self.sigma22],axis=1)
         # Cholesky decomp used for for drawing data        
         self.A = np.zeros(self.Sigma.shape)
         for i in range(self.k):
             self.A[i] = cholesky(self.Sigma[i].reshape(2,2)).flatten()
 
 
-    def cdf(self, x:np.ndarray or None=None, x1:np.ndarray or None=None, x2:np.ndarray or None=None, **kwargs) -> np.ndarray:
+    def cdf(self, x:np.ndarray | None=None, x1:np.ndarray | None=None, x2:np.ndarray | None=None, **kwargs) -> np.ndarray:
         """
         Calculates the CDF for an array with two dimensions (i.e. bivariate normal)
 
         Parameters
         ----------
         x (np.ndarray):
             A (n1,..,nj,d1,..,dk,2) array
```

## sntn/_lasso.py

```diff
@@ -161,15 +161,15 @@
 
         # (iv) Combine matrices and return    
         A = np.vstack((A1, A0))
         b = np.vstack((b1, b0))
         return A, b
 
 
-    def _inference_on_screened(self, sigma2:float or int, partial:bool=True) -> tuple:
+    def _inference_on_screened(self, sigma2:float | int, partial:bool=True) -> tuple:
         """
         Runs post selection inference for the screened variables
 
         Parameters
         ==========
         sigma2:             Variance of the error term
         partial:            Whether we want to test the |M| variables independently (and only use hte active constraints)
@@ -222,15 +222,15 @@
         mask = sign_vars == -1
         V = np.c_[v_neg, v_pos]
         V[mask] = -V[mask][:,[1,0]]
         # Return terms
         return eta2var, V[:,0], V[:,1]
 
 
-    def run_inference(self, alpha:float, null_beta:float or np.ndarray=0, sigma2:float or None=None, run_screen:bool=True, run_split:bool=True, run_carve:bool=True, run_ci:bool=True, **kwargs) -> None:
+    def run_inference(self, alpha:float, null_beta:float | np.ndarray=0, sigma2:float | None=None, run_screen:bool=True, run_split:bool=True, run_carve:bool=True, run_ci:bool=True, **kwargs) -> None:
         """
         Carries out classical and PoSI inference (including data carving)
         
         Parameters
         ==========
         alpha:                  The type-I error rate
         null_beta:              The null hypothesis (default=0)
```

## sntn/_nts.py

```diff
@@ -1,20 +1,20 @@
 """
 Fully specified SNTN distribution
 """
 
 # External
 import numpy as np
 from time import time
+from scipy.optimize import root
 from scipy.stats import truncnorm, norm
-from scipy.optimize import root, root_scalar
 # Internal
 from sntn._bvn import _bvn
+from sntn._solvers import conf_inf_solver
 from sntn.utilities.grad import _log_gauss_approx
-from sntn._solvers import conf_inf_solver, _process_args_kwargs_flatten
 from sntn.utilities.utils import broastcast_max_shape, try2array, broadcast_to_k, reverse_broadcast_from_k, pass_kwargs_to_classes, get_valid_kwargs_cls, get_valid_kwargs_method
 
 
 @staticmethod
 def _mus_are_equal(mu1, mu2) -> tuple:
     """When we desire to fix mu1/mu2 to the same value, will return the value of both when one of them is None"""
     # Determine which are None (if any)
@@ -36,15 +36,15 @@
         return mu1, mu1
     else:
         assert np.all(mu1 == mu2), 'if mu is fixed, mu1 != mu2'
         return mu1, mu2
 
 
 class _nts():
-    def __init__(self, mu1:float or np.ndarray or None, tau21:float or np.ndarray, mu2:float or np.ndarray or None, tau22:float or np.ndarray, a:float or np.ndarray, b:float or np.ndarray, c1:float or np.ndarray=1, c2:float or np.ndarray=1, fix_mu:bool=False, **kwargs) -> None:
+    def __init__(self, mu1:float | np.ndarray | None, tau21:float | np.ndarray, mu2:float | np.ndarray | None, tau22:float | np.ndarray, a:float | np.ndarray, b:float | np.ndarray, c1:float | np.ndarray=1, c2:float | np.ndarray=1, fix_mu:bool=False, **kwargs) -> None:
         """
         The "normal and truncated sum": workhorse class for the sum of a normal and truncated normal. Carries out standard inferece using scipy.dist syntax with added conf_int method
 
         W = c1*Z1 + c2*Z2,  Z1 ~ N(mu1, tau21^2), Z2 ~ TN(mu2, tau22^2, a, b)
         W ~ NTS(theta(mu(c)), Sigma(tau(c)), a, b)
         mu(c) =             [c1*mu1, c2*mu2]
         tau(c) =            [c1**2 * tau21, c2**2 * tau22]
@@ -279,15 +279,15 @@
             assert merr < tol, f'Error! Root finding had a max error {merr} which exceeded tolerance {tol}'
             w = solution.x.reshape(w0.shape) # Reshape
             # Put to original param shape
         w = reverse_broadcast_from_k(w, self.param_shape)
         return w
 
 
-    def _find_dist_kwargs_CI(**kwargs) -> tuple:
+    def _find_dist_kwargs_CI(self, **kwargs) -> tuple:
         """
         Looks for valid truncated normal distribution keywords that are needed for generating a CI
 
         Returns
         -------
         {mu1,mu2}, tau21, tau22, c1, c2, fix_mu, kwargs
         """
```

## sntn/_screening.py

```diff
@@ -1,16 +1,14 @@
 """
 Workhorse class for doing marginal screening 
 """
 
 # External
 import numpy as np
 import pandas as pd
-from math import isclose
-from warnings import warn
 from sklearn.model_selection import KFold
 # Internal
 from sntn.dists import tnorm, nts
 from sntn._split import _split_yx
 from sntn._cdf_bvn._utils import Phi
 from sntn.utilities.linear import ols
 from sntn.utilities.utils import get_valid_kwargs_cls, cvec, rvec
@@ -105,15 +103,15 @@
             partial_cp = partial.copy()
             partial_cp[:,cidx] = -self.s[cidx]
             start, stop = int(i*n_partial), int((i+1)*n_partial)
             mat_A[start:stop] = np.dot(partial_cp, self.x_screen.T)
         return mat_A
         
 
-    def _inference_on_screened(self, sigma2:float or int) -> tuple:
+    def _inference_on_screened(self, sigma2:float | int) -> tuple:
         """
         Runs post selection inference for the screened variables
 
         Returns
         =======
         A tuple containing (alph_den, v_neg, v_pos) which is equivalent to the tnorm(; sigma2, a, b) terms
         """
@@ -138,15 +136,15 @@
         # Indexes are w.r.t to alph's
         v_neg = np.max(np.where(alph < 0, ratio, -np.inf), axis=0)
         v_pos = np.min(np.where(alph > 0, ratio, +np.inf), axis=0)
         assert np.all(v_pos > v_neg), 'expected pos to be larger than neg'
         return alph_den, v_neg, v_pos
 
 
-    def run_inference(self, alpha:float, null_beta:float or np.ndarray=0, sigma2:float or None=None, run_screen:bool=True, run_split:bool=True, run_carve:bool=True, run_ci:bool=True, **kwargs) -> None:
+    def run_inference(self, alpha:float, null_beta:float | np.ndarray=0, sigma2:float | None=None, run_screen:bool=True, run_split:bool=True, run_carve:bool=True, run_ci:bool=True, **kwargs) -> None:
         """
         Carries out classical and PoSI inference (including data carving)
         
         Parameters
         ==========
         alpha:                  The type-I error rate
         null_beta:              The null hypothesis (default=0)
```

## sntn/_solvers.py

```diff
@@ -27,20 +27,19 @@
 
 """
 
 # External 
 import numpy as np
 from time import time
 from warnings import warn
-from copy import deepcopy
 from inspect import getfullargspec
-from scipy.stats import norm
+from typing import Callable, Optional
 from scipy.optimize import root, minimize_scalar, minimize, root_scalar
 # Internal modules
-from sntn.utilities.utils import broastcast_max_shape, str2list, try2list, no_diff, vprint
+from sntn.utilities.utils import broastcast_max_shape, str2list, no_diff
 
 # Hard-coded scipy approaches and methods
 valid_approaches = ['root', 'minimize_scalar', 'minimize', 'root_scalar']
 # First item is "recommended"
 di_default_methods = {'root':['hybr', 'lm'],
                       'minimize_scalar':['Golden', 'Bounded', 'Brent'],
                       'minimize':['Powell','COBYLA','L-BFGS-B'],
@@ -85,15 +84,15 @@
             # Should zip fine
             kwargs = dict(zip(di_names, args[1:]))
     # Return the flatten variable along with the kwargs
     return _check_flatten(**kwargs)
 
 
 class conf_inf_solver():
-    def __init__(self, dist:callable, param_theta:str, dF_dtheta:None or callable=None, alpha:float=0.05, verbose:bool=False, verbose_iter:int=50) -> None:
+    def __init__(self, dist:callable, param_theta:str, dF_dtheta : Optional[Callable] = None, alpha:float=0.05, verbose:bool=False, verbose_iter:int=50) -> None:
         """
         The conf_inf_solver class can generate confidence intervals for a single parameter of a distribution. Specifically for a target parameter "theta":
 
         CI_ub: inf_theta: F(theta;phi1,...,phik).cdf(x)-alpha/2=0
         CI_lb: sup_theta: F(theta;phi1,...,phik).cdf(x)-1+alpha/2=0
         whhere phi1,...,phik are other parameters of the distribution
 
@@ -189,15 +188,15 @@
         # res = 2 * term1 * term2
         res = (2 * term1 * term2) / ( term1**2 + 1)
         if flatten:
             res = res.flatten()
         return res
 
     @staticmethod
-    def _process_fun_x0_x1(x:float or np.ndarray, fun_x0:None or callable=None, fun_x1:None or callable=None) -> tuple or np.ndarray or None:
+    def _process_fun_x0_x1(x:float | np.ndarray, fun_x0 : Optional[Callable] = None, fun_x1:Optional[Callable] = None) -> tuple | np.ndarray | None:
         """
         If we have a vector/float of observation values (x), and the user passes a fun_x{01} which maps x to some starting point(s), then we return those
         """
         x0, x1 = None, None
         if fun_x0 is not None:
             x0 = fun_x0(x)
         if fun_x1 is not None:
@@ -270,15 +269,15 @@
                 theta_recover[j] = sol_j.root
                 di_fail_j['args'] = list(di_fail_j['args'])
             # Update the failed vector
             theta[idx_fail] = theta_recover
         return theta
 
 
-    def _conf_int(self, x:np.ndarray, approach:str='root', di_dist_args:dict or None=None, di_scipy:dict or None=None, mu_lb:float or int=-100000, mu_ub:float or int=100000, fun_x0:None or callable=None, fun_x1:None or callable=None, fun_x01_type:str='nudge', x0:np.ndarray or None=None, x1:np.ndarray or None=None) -> np.ndarray:
+    def _conf_int(self, x:np.ndarray, approach:str='root', di_dist_args:dict | None=None, di_scipy:dict | None=None, mu_lb:float | int=-100000, mu_ub:float | int=100000, fun_x0:Optional[Callable] = None, fun_x1:Optional[Callable] = None, fun_x01_type:str='nudge', x0:np.ndarray | None=None, x1:np.ndarray | None=None) -> np.ndarray:
         """
         Parameters
         ----------
         x:                  An array-like object of points that corresponds to dimensions of estimated means
         approach:           Which scipy method to use (see scipy.optimize.{root, minimize_scalar, minimize, root_scalar}), default='root'
         di_scipy:           Dictionary to be passed into scipy optimization (e.g. root(**di_scipy), di_scipy={'method':'secant'}), default=None
         di_dist_args:       A dictionary that contains the named paramaters which are fixed for CDF calculation (e.g. {'scale':2}), default=None
```

## sntn/_split.py

```diff
@@ -7,15 +7,15 @@
 import pandas as pd
 from sklearn.model_selection import train_test_split
 # Internal
 from sntn.utilities.utils import cvec
 
 
 class _split_yx():
-    def __init__(self, y:np.ndarray, x:np.ndarray, frac_split:float=0.5, seed:int or None=None, normalize:bool=True, has_int:bool=True) -> None:
+    def __init__(self, y:np.ndarray, x:np.ndarray, frac_split:float=0.5, seed:int | None=None, normalize:bool=True, has_int:bool=True) -> None:
         """
         Class which
 
         Parameters
         ==========
         y:                  (n,) array of responses
         x:                  (n,p) array of covariates
@@ -67,15 +67,15 @@
             self.x_screen, mu_screen, se_screen = self.normalize_mat(self.x_screen, return_mu_se=True)
             if self.has_split:
                 mu_split = self.x_split.mean(0)
                 self.x_split = (self.x_split - mu_split) / se_screen
 
 
     @staticmethod
-    def normalize_mat(mat:np.ndarray, mu:None or np.ndarray=None, se:None or np.ndarray=None, return_mu_se:bool = False) -> np.ndarray:
+    def normalize_mat(mat:np.ndarray, mu:None | np.ndarray=None, se:None | np.ndarray=None, return_mu_se:bool = False) -> np.ndarray:
         if mu is None:
             mu = mat.mean(axis=0)
         if se is None:
             se = mat.std(axis=0, ddof=1)
         res = (mat - mu) / se
         if return_mu_se:
             return res, mu, se
```

## sntn/_tnorm.py

```diff
@@ -37,15 +37,15 @@
         self.alpha = (self.a - self.mu) / self.sigma
         self.beta = (self.b - self.mu) / self.sigma
         # Initialize the distribution
         self.dist = truncnorm(loc=self.mu, scale=self.sigma, a=self.alpha, b=self.beta)
 
 
 class _tnorm():
-    def __init__(self, mu:float or np.ndarray or int, sigma2:float or np.ndarray or int, a:float or np.ndarray or int, b:float or np.ndarray or int, **kwargs) -> None:
+    def __init__(self, mu:float | np.ndarray | int, sigma2:float | np.ndarray | int, a:float | np.ndarray | int, b:float | np.ndarray | int, **kwargs) -> None:
         """
         Main model class for the truncated normal distribution
 
         Parameters
         ----------
         mu:                 Means of the unconditional normal (can be array)
         sigma2:             Variance of the truncated normal (can be array)
```

## sntn/dists.py

```diff
@@ -8,20 +8,20 @@
 from sntn._bvn import _bvn
 from sntn._nts import _nts
 from sntn._tnorm import _tnorm
 
 
 """tnorm (truncated normal) inherits the _tnorm class"""
 class tnorm(_tnorm):
-    def __init__(self, mu:float or np.ndarray or int, sigma2:float or np.ndarray or int, a:float or np.ndarray or int, b:float or np.ndarray or int) -> None:
+    def __init__(self, mu:float | np.ndarray | int, sigma2:float | np.ndarray | int, a:float | np.ndarray | int, b:float | np.ndarray | int) -> None:
         super().__init__(mu, sigma2, a, b)
 
 
 """Normal & Truncated Sum (NTS) inherits the _nts class"""
 class nts(_nts):
-    def __init__(self,  mu1:float or np.ndarray, tau21:float or np.ndarray, mu2:float or np.ndarray, tau22:float or np.ndarray, a:float or np.ndarray, b:float or np.ndarray, c1:float or np.ndarray=1, c2:float or np.ndarray=1, **kwargs) -> None:
+    def __init__(self,  mu1:float | np.ndarray, tau21:float | np.ndarray, mu2:float | np.ndarray, tau22:float | np.ndarray, a:float | np.ndarray, b:float | np.ndarray, c1:float | np.ndarray=1, c2:float | np.ndarray=1, **kwargs) -> None:
         super().__init__(mu1, tau21, mu2, tau22, a, b, c1, c2, **kwargs)
 
 """Bivariate normal (BVN) inherits the _bvn class"""
 class bvn(_bvn):
-    def __init__(self, mu1:float or np.ndarray, sigma21:float or np.ndarray, mu2:float or np.ndarray, sigma22:float or np.ndarray, rho:float or np.ndarray, **kwargs) -> None:
+    def __init__(self, mu1:float | np.ndarray, sigma21:float | np.ndarray, mu2:float | np.ndarray, sigma22:float | np.ndarray, rho:float | np.ndarray, **kwargs) -> None:
         super().__init__(mu1, sigma21, mu2, sigma22, rho, **kwargs)
```

## Comparing `sntn-0.0.4.dist-info/METADATA` & `sntn-0.0.6.dist-info/METADATA`

 * *Files 6% similar despite different names*

```diff
@@ -1,28 +1,36 @@
 Metadata-Version: 2.1
 Name: sntn
-Version: 0.0.4
+Version: 0.0.6
 Summary: Sum of a normal and a truncated Normal (SNTN)
 Home-page: https://github.com/ErikinBC/SNTN
 Author: Erik Drysdale
 Author-email: erikinwest@gmail.com
 License: GPLv3
 Classifier: Development Status :: 3 - Alpha
 Classifier: Intended Audience :: Science/Research
 Classifier: License :: OSI Approved :: MIT License
 Classifier: Operating System :: POSIX :: Linux
 Classifier: Programming Language :: Python :: 3
+Requires-Python: >=3.10.0
 Description-Content-Type: text/markdown
+License-File: LICENSE.txt
+Requires-Dist: plotnine >=0.12.1
+Requires-Dist: numpy >=1.25.0
+Requires-Dist: pandas >=1.5.3
+Requires-Dist: scikit-learn >=1.2.2
+Requires-Dist: scipy >=1.10.1
+Requires-Dist: statsmodels >=0.14.0
 
 ## `sntn`: The sum of a normal and truncated normal distribution package
 
 
 This repo is for the `sntn` package, which implements a scipy-like class for doing inference on a sum of a normal and a trunctated normal (SNTN) distribution (see [Kim (2006)](https://www.kss.or.kr/jounalDown.php?IDX=831) and [Arnold (1993)](https://link.springer.com/article/10.1007/BF02294652)). 
 
-The SNTN distribution can be used in a variety of situations including [data carving](https://arxiv.org/abs/) (a type of post-selection inference), situations where a filtering process is applied, as well as [two-stage hypothesis testing](http://www.erikdrysdale.com/regression_trial/#1-two-stage-testing-approach). The [arXiv paper](https://arxiv.org/abs/), "A parametric distribution for exact post-selection inference with data carving" makes extensive use of this package. Please see the [notebook](examples/data_carving.ipynb) in the examples folder for a more thorough walk through of how these methods can be used for the post-selection inference lasso and marginal screening algorithms.
+The SNTN distribution can be used in a variety of situations including [data carving](https://arxiv.org/pdf/2305.12581.pdf) (a type of post-selection inference), situations where a filtering process is applied, as well as [two-stage hypothesis testing](http://www.erikdrysdale.com/regression_trial/#1-two-stage-testing-approach). The [arXiv paper](https://arxiv.org/pdf/2305.12581.pdf), "A parametric distribution for exact post-selection inference with data carving" makes extensive use of this package. Please see the [notebook](examples/data_carving.ipynb) in the examples folder for a more thorough walk through of how these methods can be used for the post-selection inference lasso and marginal screening algorithms.
 
 
 Formally, if $X_1 \sim N(\mu_1, \tau_1^2)$ and $X_2 \sim \text{TN}(\mu_2, \tau_2^2, a, b)$, then $Z = c_1 X_1 + c_2 X_2$, $c_i \in \mathbb{R}$, is said to follow an SNTN distribution denoted as either $\text{SNTN}(\mu_1, \tau_1^2, \mu_2, \tau_2^2, a, b, c_1, c_2)\overset{d}{=}\text{SNTN}(\theta_1, \sigma_1^2, \theta_2, \sigma_2^2, \omega, \delta)$ where $\theta_1=\sum_i c_i\mu_i$, $\sigma_1^2=\sum_i c_i^2\tau_i^2$, $\theta_2=\mu_2$, $\sigma_2^2=\tau_2^2$, $m_j(x)=(x-\theta_x)/\sigma_x$, $\omega=m_2(a)=(a-\theta_2)/\sigma_2$, and $\delta=m_2(b)=(b-\theta_2)/\sigma_2$.
 
 Inference for the SNTN distributions ends up being computationally scalable since the cdf ($f$) has a closed form solution, and the CDF ($F$) can be calculated using the CDF of two bivariate normal distributions with correlation $\rho$.
 
 $$
@@ -35,17 +43,24 @@
 \end{align*}
 $$
 
 <br>
 
 # Installation
 
-see pypi.
+See [pypi](https://pypi.org/project/sntn/). 
 
-To check that the package compiled properly, please run `python3 -m sntn`.
+**Please make sure you have atleast python>=3.11!**
+
+1. numpy<=1.24.2
+2. pandas<=2.0.0
+3. scipy<=1.9.3
+4. glmnet<=2.2.1
+
+I recommend using `conda install -c conda-forge glmnet`. To check that the package compiled properly, please run `python3 -m sntn`.
 
 <br>
 
 # main classes
 
 There are six classes from this package that are likely to be used by practioneers. Functions or classes that start with an underscore "_" are designed for internal use, but some of the their optional arguments may be of interest.
```

