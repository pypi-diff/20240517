# Comparing `tmp/sonar_tools-2.9-py3-none-any.whl.zip` & `tmp/sonar_tools-3.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,81 +1,84 @@
-Zip file size: 209027 bytes, number of entries: 79
--rw-r--r--  2.0 unx      828 b- defN 24-Mar-20 14:07 sonar/__init__.py
--rw-r--r--  2.0 unx     4052 b- defN 24-Mar-20 14:36 sonar/aggregations.py
--rw-r--r--  2.0 unx    20257 b- defN 24-Mar-24 18:19 sonar/applications.py
--rw-r--r--  2.0 unx     7001 b- defN 24-Mar-20 14:07 sonar/components.py
--rw-r--r--  2.0 unx     2921 b- defN 24-Mar-20 14:07 sonar/custom_measures.py
--rw-r--r--  2.0 unx    10862 b- defN 24-Mar-20 14:07 sonar/devops.py
--rw-r--r--  2.0 unx     1493 b- defN 24-Mar-20 14:07 sonar/exceptions.py
--rw-r--r--  2.0 unx    12296 b- defN 24-Mar-22 08:49 sonar/groups.py
--rw-r--r--  2.0 unx     3750 b- defN 24-Mar-20 14:07 sonar/languages.py
--rw-r--r--  2.0 unx     9455 b- defN 24-Mar-20 14:07 sonar/measures.py
--rw-r--r--  2.0 unx     6281 b- defN 24-Mar-20 14:07 sonar/metrics.py
--rw-r--r--  2.0 unx     2207 b- defN 24-Mar-20 14:36 sonar/options.py
--rw-r--r--  2.0 unx    33955 b- defN 24-Mar-24 18:19 sonar/platform.py
--rw-r--r--  2.0 unx    29924 b- defN 24-Mar-25 12:59 sonar/portfolios.py
--rw-r--r--  2.0 unx    17660 b- defN 24-Mar-20 14:07 sonar/qualitygates.py
--rw-r--r--  2.0 unx    29108 b- defN 24-Mar-22 08:49 sonar/qualityprofiles.py
--rw-r--r--  2.0 unx    11288 b- defN 24-Mar-21 13:49 sonar/rules.py
--rw-r--r--  2.0 unx    16190 b- defN 24-Mar-20 14:07 sonar/settings.py
--rw-r--r--  2.0 unx    19125 b- defN 24-Mar-24 18:19 sonar/sif.py
--rw-r--r--  2.0 unx     6010 b- defN 24-Mar-20 14:36 sonar/sqobject.py
--rw-r--r--  2.0 unx     8443 b- defN 24-Mar-20 14:07 sonar/syncer.py
--rw-r--r--  2.0 unx    21269 b- defN 24-Mar-24 18:19 sonar/tasks.py
--rw-r--r--  2.0 unx     3486 b- defN 24-Mar-20 14:07 sonar/tokens.py
--rw-r--r--  2.0 unx    17339 b- defN 24-Mar-22 08:49 sonar/users.py
--rw-r--r--  2.0 unx    16352 b- defN 24-Mar-22 08:31 sonar/utilities.py
--rw-r--r--  2.0 unx      896 b- defN 24-Mar-20 14:36 sonar/version.py
--rw-r--r--  2.0 unx     5315 b- defN 24-Mar-20 14:07 sonar/webhooks.py
--rw-r--r--  2.0 unx     1034 b- defN 24-Mar-20 14:07 sonar/audit/__init__.py
--rw-r--r--  2.0 unx     2851 b- defN 24-Mar-20 14:07 sonar/audit/config.py
--rw-r--r--  2.0 unx     2526 b- defN 24-Mar-20 14:07 sonar/audit/problem.py
--rw-r--r--  2.0 unx    20583 b- defN 24-Mar-24 18:19 sonar/audit/rules.json
--rw-r--r--  2.0 unx     6153 b- defN 24-Mar-24 18:19 sonar/audit/rules.py
--rw-r--r--  2.0 unx     1130 b- defN 24-Mar-20 14:07 sonar/audit/severities.py
--rw-r--r--  2.0 unx    13229 b- defN 24-Mar-20 14:36 sonar/audit/sonar-audit.properties
--rw-r--r--  2.0 unx     1179 b- defN 24-Mar-20 14:07 sonar/audit/types.py
--rw-r--r--  2.0 unx      828 b- defN 24-Mar-20 14:07 sonar/dce/__init__.py
--rw-r--r--  2.0 unx    12510 b- defN 24-Mar-22 08:31 sonar/dce/app_nodes.py
--rw-r--r--  2.0 unx     1086 b- defN 24-Mar-20 14:07 sonar/dce/nodes.py
--rw-r--r--  2.0 unx     4586 b- defN 24-Mar-22 08:31 sonar/dce/search_nodes.py
--rw-r--r--  2.0 unx      828 b- defN 24-Mar-20 14:07 sonar/findings/__init__.py
--rw-r--r--  2.0 unx     6900 b- defN 24-Mar-20 14:07 sonar/findings/changelog.py
--rw-r--r--  2.0 unx    12542 b- defN 24-Mar-20 14:07 sonar/findings/findings.py
--rw-r--r--  2.0 unx    15687 b- defN 24-Mar-20 14:07 sonar/findings/hotspots.py
--rw-r--r--  2.0 unx    31311 b- defN 24-Mar-20 14:07 sonar/findings/issues.py
--rw-r--r--  2.0 unx      828 b- defN 24-Mar-20 14:07 sonar/permissions/__init__.py
--rw-r--r--  2.0 unx     1820 b- defN 24-Mar-20 14:07 sonar/permissions/aggregation_permissions.py
--rw-r--r--  2.0 unx     1021 b- defN 24-Mar-20 14:07 sonar/permissions/application_permissions.py
--rw-r--r--  2.0 unx     3341 b- defN 24-Mar-20 14:07 sonar/permissions/global_permissions.py
--rw-r--r--  2.0 unx     9576 b- defN 24-Mar-20 14:07 sonar/permissions/permission_templates.py
--rw-r--r--  2.0 unx    11397 b- defN 24-Mar-24 18:19 sonar/permissions/permissions.py
--rw-r--r--  2.0 unx     1018 b- defN 24-Mar-20 14:07 sonar/permissions/portfolio_permissions.py
--rw-r--r--  2.0 unx     8252 b- defN 24-Mar-22 08:49 sonar/permissions/project_permissions.py
--rw-r--r--  2.0 unx     4406 b- defN 24-Mar-20 14:07 sonar/permissions/quality_permissions.py
--rw-r--r--  2.0 unx     2196 b- defN 24-Mar-20 14:07 sonar/permissions/qualitygate_permissions.py
--rw-r--r--  2.0 unx     2349 b- defN 24-Mar-20 14:07 sonar/permissions/qualityprofile_permissions.py
--rw-r--r--  2.0 unx     2955 b- defN 24-Mar-20 14:07 sonar/permissions/template_permissions.py
--rw-r--r--  2.0 unx      828 b- defN 24-Mar-20 14:07 sonar/projects/__init__.py
--rw-r--r--  2.0 unx    17542 b- defN 24-Mar-22 08:49 sonar/projects/branches.py
--rw-r--r--  2.0 unx    56961 b- defN 24-Mar-25 12:59 sonar/projects/projects.py
--rw-r--r--  2.0 unx     4484 b- defN 24-Mar-22 08:49 sonar/projects/pull_requests.py
--rwxr-xr-x  2.0 unx     2223 b- defN 24-Mar-25 13:52 sonar_tools-2.9.data/scripts/sonar-tools
--rw-r--r--  2.0 unx      828 b- defN 24-Mar-20 14:07 tools/__init__.py
--rw-r--r--  2.0 unx     6700 b- defN 24-Mar-20 14:07 tools/audit.py
--rw-r--r--  2.0 unx     7932 b- defN 24-Mar-20 14:07 tools/config.py
--rw-r--r--  2.0 unx     2539 b- defN 24-Mar-20 14:07 tools/cust_measures.py
--rw-r--r--  2.0 unx    14736 b- defN 24-Mar-25 12:59 tools/findings_export.py
--rw-r--r--  2.0 unx     9195 b- defN 24-Mar-20 14:07 tools/findings_sync.py
--rw-r--r--  2.0 unx     8908 b- defN 24-Mar-20 14:07 tools/housekeeper.py
--rw-r--r--  2.0 unx     5818 b- defN 24-Mar-20 14:07 tools/loc.py
--rw-r--r--  2.0 unx     8803 b- defN 24-Mar-25 12:59 tools/measures_export.py
--rw-r--r--  2.0 unx     2508 b- defN 24-Mar-20 14:07 tools/projects_export.py
--rw-r--r--  2.0 unx     3750 b- defN 24-Mar-20 14:07 tools/projects_import.py
--rw-r--r--  2.0 unx     6829 b- defN 24-Mar-20 14:07 tools/support.py
--rw-r--r--  2.0 unx     7652 b- defN 24-Mar-25 13:52 sonar_tools-2.9.dist-info/LICENSE
--rw-r--r--  2.0 unx    20176 b- defN 24-Mar-25 13:52 sonar_tools-2.9.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Mar-25 13:52 sonar_tools-2.9.dist-info/WHEEL
--rw-r--r--  2.0 unx      582 b- defN 24-Mar-25 13:52 sonar_tools-2.9.dist-info/entry_points.txt
--rw-r--r--  2.0 unx       12 b- defN 24-Mar-25 13:52 sonar_tools-2.9.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     6408 b- defN 24-Mar-25 13:52 sonar_tools-2.9.dist-info/RECORD
-79 files, 697391 bytes uncompressed, 199107 bytes compressed:  71.4%
+Zip file size: 221743 bytes, number of entries: 82
+-rw-r--r--  2.0 unx      828 b- defN 24-May-10 15:18 sonar/__init__.py
+-rw-r--r--  2.0 unx     4026 b- defN 24-May-14 10:43 sonar/aggregations.py
+-rw-r--r--  2.0 unx    20531 b- defN 24-May-14 10:47 sonar/applications.py
+-rw-r--r--  2.0 unx     7503 b- defN 24-May-15 19:25 sonar/components.py
+-rw-r--r--  2.0 unx     2921 b- defN 24-May-10 15:18 sonar/custom_measures.py
+-rw-r--r--  2.0 unx    10862 b- defN 24-May-14 10:43 sonar/devops.py
+-rw-r--r--  2.0 unx     1607 b- defN 24-May-10 15:28 sonar/exceptions.py
+-rw-r--r--  2.0 unx    12292 b- defN 24-May-14 10:43 sonar/groups.py
+-rw-r--r--  2.0 unx     3750 b- defN 24-May-10 15:18 sonar/languages.py
+-rw-r--r--  2.0 unx    11632 b- defN 24-May-15 18:27 sonar/measures.py
+-rw-r--r--  2.0 unx     6281 b- defN 24-May-10 15:18 sonar/metrics.py
+-rw-r--r--  2.0 unx     2272 b- defN 24-May-15 18:27 sonar/options.py
+-rw-r--r--  2.0 unx     7170 b- defN 24-May-14 11:05 sonar/organizations.py
+-rw-r--r--  2.0 unx    35548 b- defN 24-May-14 12:20 sonar/platform.py
+-rw-r--r--  2.0 unx    31776 b- defN 24-May-14 11:44 sonar/portfolios.py
+-rw-r--r--  2.0 unx    17676 b- defN 24-May-14 10:43 sonar/qualitygates.py
+-rw-r--r--  2.0 unx    30758 b- defN 24-May-14 10:43 sonar/qualityprofiles.py
+-rw-r--r--  2.0 unx    12560 b- defN 24-May-14 10:43 sonar/rules.py
+-rw-r--r--  2.0 unx    19068 b- defN 24-May-14 10:43 sonar/settings.py
+-rw-r--r--  2.0 unx    12698 b- defN 24-May-10 15:20 sonar/sif.py
+-rw-r--r--  2.0 unx    13938 b- defN 24-May-14 10:43 sonar/sif_node.py
+-rw-r--r--  2.0 unx     6010 b- defN 24-May-10 15:19 sonar/sqobject.py
+-rw-r--r--  2.0 unx     8443 b- defN 24-May-14 10:43 sonar/syncer.py
+-rw-r--r--  2.0 unx    22317 b- defN 24-May-15 19:49 sonar/tasks.py
+-rw-r--r--  2.0 unx     3486 b- defN 24-May-14 10:43 sonar/tokens.py
+-rw-r--r--  2.0 unx    18375 b- defN 24-May-16 12:52 sonar/users.py
+-rw-r--r--  2.0 unx    19668 b- defN 24-May-14 12:06 sonar/utilities.py
+-rw-r--r--  2.0 unx      896 b- defN 24-May-16 13:23 sonar/version.py
+-rw-r--r--  2.0 unx     5311 b- defN 24-May-14 10:43 sonar/webhooks.py
+-rw-r--r--  2.0 unx     1034 b- defN 24-May-10 15:18 sonar/audit/__init__.py
+-rw-r--r--  2.0 unx     2851 b- defN 24-May-10 15:18 sonar/audit/config.py
+-rw-r--r--  2.0 unx     3974 b- defN 24-May-14 10:43 sonar/audit/problem.py
+-rw-r--r--  2.0 unx    22032 b- defN 24-May-14 10:35 sonar/audit/rules.json
+-rw-r--r--  2.0 unx     6401 b- defN 24-May-14 10:35 sonar/audit/rules.py
+-rw-r--r--  2.0 unx     1130 b- defN 24-May-10 15:18 sonar/audit/severities.py
+-rw-r--r--  2.0 unx    13172 b- defN 24-May-15 19:25 sonar/audit/sonar-audit.properties
+-rw-r--r--  2.0 unx     1179 b- defN 24-May-10 15:18 sonar/audit/types.py
+-rw-r--r--  2.0 unx      828 b- defN 24-May-10 15:18 sonar/dce/__init__.py
+-rw-r--r--  2.0 unx     4814 b- defN 24-May-10 15:20 sonar/dce/app_nodes.py
+-rw-r--r--  2.0 unx     1086 b- defN 24-May-10 15:18 sonar/dce/nodes.py
+-rw-r--r--  2.0 unx     6340 b- defN 24-May-14 10:43 sonar/dce/search_nodes.py
+-rw-r--r--  2.0 unx      828 b- defN 24-May-10 15:18 sonar/findings/__init__.py
+-rw-r--r--  2.0 unx     6900 b- defN 24-May-10 15:18 sonar/findings/changelog.py
+-rw-r--r--  2.0 unx    13744 b- defN 24-May-15 18:27 sonar/findings/findings.py
+-rw-r--r--  2.0 unx    15727 b- defN 24-May-17 14:55 sonar/findings/hotspots.py
+-rw-r--r--  2.0 unx    31351 b- defN 24-May-15 18:27 sonar/findings/issues.py
+-rw-r--r--  2.0 unx      828 b- defN 24-May-10 15:18 sonar/permissions/__init__.py
+-rw-r--r--  2.0 unx     1820 b- defN 24-May-10 15:18 sonar/permissions/aggregation_permissions.py
+-rw-r--r--  2.0 unx     1021 b- defN 24-May-10 15:18 sonar/permissions/application_permissions.py
+-rw-r--r--  2.0 unx     3341 b- defN 24-May-14 10:43 sonar/permissions/global_permissions.py
+-rw-r--r--  2.0 unx     9576 b- defN 24-May-10 15:18 sonar/permissions/permission_templates.py
+-rw-r--r--  2.0 unx    11397 b- defN 24-May-10 15:19 sonar/permissions/permissions.py
+-rw-r--r--  2.0 unx     1018 b- defN 24-May-10 15:18 sonar/permissions/portfolio_permissions.py
+-rw-r--r--  2.0 unx     8216 b- defN 24-May-14 10:43 sonar/permissions/project_permissions.py
+-rw-r--r--  2.0 unx     4406 b- defN 24-May-10 15:18 sonar/permissions/quality_permissions.py
+-rw-r--r--  2.0 unx     2196 b- defN 24-May-14 10:43 sonar/permissions/qualitygate_permissions.py
+-rw-r--r--  2.0 unx     2349 b- defN 24-May-10 15:18 sonar/permissions/qualityprofile_permissions.py
+-rw-r--r--  2.0 unx     2955 b- defN 24-May-14 10:43 sonar/permissions/template_permissions.py
+-rw-r--r--  2.0 unx      828 b- defN 24-May-10 15:18 sonar/projects/__init__.py
+-rw-r--r--  2.0 unx    17684 b- defN 24-May-10 15:28 sonar/projects/branches.py
+-rw-r--r--  2.0 unx    57214 b- defN 24-May-15 19:25 sonar/projects/projects.py
+-rw-r--r--  2.0 unx     4480 b- defN 24-May-10 15:20 sonar/projects/pull_requests.py
+-rwxr-xr-x  2.0 unx     2337 b- defN 24-May-17 16:37 sonar_tools-3.0.data/scripts/sonar-tools
+-rw-r--r--  2.0 unx      828 b- defN 24-May-10 15:18 tools/__init__.py
+-rw-r--r--  2.0 unx     6631 b- defN 24-May-14 10:43 tools/audit.py
+-rw-r--r--  2.0 unx     8168 b- defN 24-May-14 10:43 tools/config.py
+-rw-r--r--  2.0 unx     2596 b- defN 24-May-14 10:43 tools/cust_measures.py
+-rw-r--r--  2.0 unx    17510 b- defN 24-May-16 13:28 tools/findings_export.py
+-rw-r--r--  2.0 unx     9192 b- defN 24-May-14 10:43 tools/findings_sync.py
+-rw-r--r--  2.0 unx     9076 b- defN 24-May-16 12:52 tools/housekeeper.py
+-rw-r--r--  2.0 unx     5727 b- defN 24-May-14 10:43 tools/loc.py
+-rw-r--r--  2.0 unx    12386 b- defN 24-May-15 19:49 tools/measures_export.py
+-rw-r--r--  2.0 unx     2365 b- defN 24-May-14 10:43 tools/projects_export.py
+-rw-r--r--  2.0 unx     3622 b- defN 24-May-10 15:23 tools/projects_import.py
+-rw-r--r--  2.0 unx     3146 b- defN 24-May-14 10:43 tools/rules_cli.py
+-rw-r--r--  2.0 unx     6761 b- defN 24-May-14 10:43 tools/support.py
+-rw-r--r--  2.0 unx     7652 b- defN 24-May-17 16:37 sonar_tools-3.0.dist-info/LICENSE
+-rw-r--r--  2.0 unx    20950 b- defN 24-May-17 16:37 sonar_tools-3.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-May-17 16:37 sonar_tools-3.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx      617 b- defN 24-May-17 16:37 sonar_tools-3.0.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx       12 b- defN 24-May-17 16:37 sonar_tools-3.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     6638 b- defN 24-May-17 16:37 sonar_tools-3.0.dist-info/RECORD
+82 files, 739229 bytes uncompressed, 211481 bytes compressed:  71.4%
```

## zipnote {}

```diff
@@ -30,14 +30,17 @@
 
 Filename: sonar/metrics.py
 Comment: 
 
 Filename: sonar/options.py
 Comment: 
 
+Filename: sonar/organizations.py
+Comment: 
+
 Filename: sonar/platform.py
 Comment: 
 
 Filename: sonar/portfolios.py
 Comment: 
 
 Filename: sonar/qualitygates.py
@@ -51,14 +54,17 @@
 
 Filename: sonar/settings.py
 Comment: 
 
 Filename: sonar/sif.py
 Comment: 
 
+Filename: sonar/sif_node.py
+Comment: 
+
 Filename: sonar/sqobject.py
 Comment: 
 
 Filename: sonar/syncer.py
 Comment: 
 
 Filename: sonar/tasks.py
@@ -174,15 +180,15 @@
 
 Filename: sonar/projects/projects.py
 Comment: 
 
 Filename: sonar/projects/pull_requests.py
 Comment: 
 
-Filename: sonar_tools-2.9.data/scripts/sonar-tools
+Filename: sonar_tools-3.0.data/scripts/sonar-tools
 Comment: 
 
 Filename: tools/__init__.py
 Comment: 
 
 Filename: tools/audit.py
 Comment: 
@@ -210,29 +216,32 @@
 
 Filename: tools/projects_export.py
 Comment: 
 
 Filename: tools/projects_import.py
 Comment: 
 
+Filename: tools/rules_cli.py
+Comment: 
+
 Filename: tools/support.py
 Comment: 
 
-Filename: sonar_tools-2.9.dist-info/LICENSE
+Filename: sonar_tools-3.0.dist-info/LICENSE
 Comment: 
 
-Filename: sonar_tools-2.9.dist-info/METADATA
+Filename: sonar_tools-3.0.dist-info/METADATA
 Comment: 
 
-Filename: sonar_tools-2.9.dist-info/WHEEL
+Filename: sonar_tools-3.0.dist-info/WHEEL
 Comment: 
 
-Filename: sonar_tools-2.9.dist-info/entry_points.txt
+Filename: sonar_tools-3.0.dist-info/entry_points.txt
 Comment: 
 
-Filename: sonar_tools-2.9.dist-info/top_level.txt
+Filename: sonar_tools-3.0.dist-info/top_level.txt
 Comment: 
 
-Filename: sonar_tools-2.9.dist-info/RECORD
+Filename: sonar_tools-3.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## sonar/aggregations.py

```diff
@@ -80,16 +80,15 @@
         return m
 
     def _audit_aggregation_cardinality(self, sizes: tuple[int], broken_rule: object) -> list[problem.Problem]:
         problems = []
         n = self.nbr_projects()
         if n in sizes:
             rule = rules.get_rule(broken_rule)
-            msg = rule.msg.format(str(self))
-            problems.append(problem.Problem(rule.type, rule.severity, msg, concerned_object=self))
+            problems.append(problem.Problem(broken_rule=rule, msg=rule.msg.format(str(self)), concerned_object=self))
         else:
             utilities.logger.debug("%s has %d projects", str(self), n)
         return problems
 
     def _audit_empty_aggregation(self, broken_rule: object) -> list[problem.Problem]:
         return self._audit_aggregation_cardinality((0, None), broken_rule)
```

## sonar/applications.py

```diff
@@ -407,40 +407,44 @@
         api=APIS["search"], params=new_params, returned_field="components", key_field="key", object_class=Application, endpoint=endpoint
     )
 
 
 def get_list(endpoint, key_list=None, use_cache=True):
     """
     :return: List of Applications (all of them if key_list is None or empty)
-    :param key_list: List of app keys to get, if None or empty all portfolios are returned
+    :param key_list: List of app keys to get, if None or empty all applications are returned
     :param use_cache: Whether to use local cache or query SonarQube, default True (use cache)
     :type use_cache: bool
     :rtype: dict{<branchName>: <Branch>}
     """
     with _CLASS_LOCK:
         if key_list is None or len(key_list) == 0 or not use_cache:
             util.logger.info("Listing applications")
             return search(endpoint=endpoint)
         object_list = {}
         for key in util.csv_to_list(key_list):
             object_list[key] = Application.get_object(endpoint, key)
     return object_list
 
 
-def export(endpoint, key_list=None, full=False):
+def export(endpoint: object, key_list: list[str] = None, full: bool = False) -> dict[str, str]:
     """Exports applications as JSON
 
     :param Platform endpoint: Reference to the SonarQube platform
     :param key_list: list of Application keys to export, defaults to all if None
     :type key_list: list, optional
     :param full: Whether to export all attributes, including those that can't be set, defaults to False
     :type full: bool
     :return: Dict of applications settings
     :rtype: dict
     """
+    if endpoint.is_sonarcloud():
+        # util.logger.info("Applications do not exist in SonarCloud, export skipped")
+        raise exceptions.UnsupportedOperation("Applications do not exist in SonarCloud, export skipped")
+
     apps_settings = {k: app.export(full) for k, app in get_list(endpoint, key_list).items()}
     for k in apps_settings:
         # remove key from JSON value, it's already the dict key
         apps_settings[k].pop("key")
     return apps_settings
```

## sonar/components.py

```diff
@@ -167,15 +167,24 @@
             self._visibility = settings.get_visibility(self.endpoint, component=self).value
         return self._visibility
 
     def set_visibility(self, visibility):
         settings.set_visibility(self.endpoint, visibility=visibility, component=self)
         self._visibility = visibility
 
-    def _audit_bg_task(self, audit_settings):
+    def _audit_bg_task(self, audit_settings: dict[str, str]):
+        """Audits project background tasks"""
+        if (
+            not audit_settings.get("audit.projects.exclusions", True)
+            and not audit_settings.get("audit.projects.analysisWarnings", True)
+            and not audit_settings.get("audit.projects.failedTasks", True)
+            and not audit_settings.get("audit.project.scm.disabled", True)
+        ):
+            util.logger.debug("%s: Background task audit disabled, audit skipped", str(self))
+            return []
         util.logger.debug("Auditing last background task of %s", str(self))
         last_task = tasks.search_last(component_key=self.key, endpoint=self.endpoint)
         if last_task:
             last_task.concerned_object = self
             return last_task.audit(audit_settings)
         return []
```

## sonar/exceptions.py

```diff
@@ -14,20 +14,29 @@
 # Lesser General Public License for more details.
 #
 # You should have received a copy of the GNU Lesser General Public License
 # along with this program; if not, write to the Free Software Foundation,
 # Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
 #
 
+"""
+
+Exceptions raised but the sonar python APIs
+
+"""
+
 
 class SonarException(Exception):
     def __init__(self, message):
         super().__init__()
         self.message = message
 
+    def __str__(self) -> str:
+        return self.message
+
 
 class ObjectNotFound(SonarException):
     """
     Object not found during a SonarQube search
     """
 
     def __init__(self, key, message):
```

## sonar/groups.py

```diff
@@ -164,15 +164,15 @@
         :return: List of problems found, or empty list
         :rtype: list[Problem]
         """
         util.logger.debug("Auditing %s", str(self))
         problems = []
         if audit_settings.get("audit.groups.empty", True) and self.__members_count == 0:
             rule = rules.get_rule(rules.RuleId.GROUP_EMPTY)
-            problems = [problem.Problem(rule.type, rule.severity, rule.msg.format(str(self)), concerned_object=self)]
+            problems = [problem.Problem(broken_rule=rule, msg=rule.msg.format(str(self)), concerned_object=self)]
         return problems
 
     def to_json(self, full_specs=False):
         """Returns the group properties (name, description, default) as dict
 
         :param full_specs: Also include properties that are not modifiable, default to False
         :type full_specs: bool, optional
```

## sonar/measures.py

```diff
@@ -16,15 +16,17 @@
 # You should have received a copy of the GNU Lesser General Public License
 # along with this program; if not, write to the Free Software Foundation,
 # Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
 #
 
 import json
 import re
-from sonar import metrics
+from http import HTTPStatus
+from requests.exceptions import HTTPError
+from sonar import metrics, exceptions
 import sonar.utilities as util
 import sonar.sqobject as sq
 
 DATETIME_METRICS = ("last_analysis", "createdAt", "updatedAt", "creation_date", "modification_date")
 
 
 class Measure(sq.SqObject):
@@ -118,21 +120,63 @@
     :return: Dict of found measures
     :rtype: dict{<metric>: <value>}
     """
     params = util.replace_keys(("project", "application", "portfolio"), "component", concerned_object.search_params())
     params["metricKeys"] = util.list_to_csv(metrics_list)
     util.logger.debug("Getting measures with %s", str(params))
 
-    data = json.loads(concerned_object.endpoint.get(Measure.API_READ, params={**kwargs, **params}).text)
+    try:
+        data = json.loads(concerned_object.endpoint.get(Measure.API_READ, params={**kwargs, **params}).text)
+    except HTTPError as e:
+        if e.response.status_code == HTTPStatus.NOT_FOUND:
+            raise exceptions.ObjectNotFound(concerned_object.key, f"{str(concerned_object)} not found")
     m_dict = {m: None for m in metrics_list}
     for m in data["component"]["measures"]:
         m_dict[m["metric"]] = Measure.load(data=m, concerned_object=concerned_object)
+    util.logger.debug("Returning measures %s", str(m_dict))
     return m_dict
 
 
+def get_history(concerned_object: object, metrics_list: list[str], **kwargs) -> list[str, str, str]:
+    """Reads the history of measures of a component (project, branch, application or portfolio)
+
+    :param concerned_object: Concerned object (project, branch, pull request, application or portfolio)
+    :type concerned_object: Project, Branch, PullRequest, Application or Portfolio
+    :param metrics_list: List of metrics to read
+    :type metrics_list: list
+    :param kwargs: List of filters to search for the measures history, defaults to None
+    :type kwargs: dict, optional
+    :return: List of found history of measures
+    :rtype: list[<date>, <metricKey>, <value>]
+    """
+    # http://localhost:9999/api/measures/search_history?component=okorach_sonar-tools&metrics=ncloc&p=1&ps=1000
+
+    params = util.replace_keys(("project", "application", "portfolio"), "component", concerned_object.search_params())
+    params["metrics"] = util.list_to_csv(metrics_list)
+    util.logger.debug("Getting measures history with %s", str(params))
+
+    try:
+        data = json.loads(concerned_object.endpoint.get(Measure.API_HISTORY, params={**kwargs, **params}).text)
+    except HTTPError as e:
+        if e.response.status_code == HTTPStatus.NOT_FOUND:
+            raise exceptions.ObjectNotFound(concerned_object.key, f"{str(concerned_object)} not found")
+    res_list = []
+    last_metric, last_date = "", ""
+    for m in reversed(data["measures"]):
+        m_key = m["metric"]
+        for dt in m["history"]:
+            if "value" in dt:
+                cur_date = dt["date"].split("T")[0]
+                if cur_date != last_date or last_metric != m_key:
+                    res_list.append([dt["date"], m_key, dt["value"]])
+                    last_date = cur_date
+                    last_metric = m_key
+    return res_list
+
+
 def get_rating_letter(rating):
     """
     :params rating:
     :type rating: int
     :return: The rating converted from number to letter, if number between 1 and 5, else the unchanged rating
     :rtype: str
     """
```

## sonar/options.py

```diff
@@ -23,15 +23,17 @@
 
 """
 
 WITH_URL = "withURL"
 WITH_NAME = "withName"
 WITH_LAST_ANALYSIS = "withLastAnalysis"
 WITH_BRANCHES = "withBranches"
+WITH_HISTORY = "history"
 NBR_THREADS = "threads"
+DATES_WITHOUT_TIME = "datesWithoutTime"
 
 WHAT_SETTINGS = "settings"
 WHAT_USERS = "users"
 WHAT_GROUPS = "groups"
 WHAT_GATES = "qualitygates"
 WHAT_RULES = "rules"
 WHAT_PROFILES = "qualityprofiles"
```

## sonar/platform.py

```diff
@@ -49,43 +49,48 @@
 
 _NON_EXISTING_SETTING_SKIPPED = "Setting %s does not exist, skipping..."
 _HTTP_ERROR = "%s Error: %s HTTP status code %d"
 
 _SONAR_TOOLS_AGENT = {"user-agent": f"sonar-tools {version.PACKAGE_VERSION}"}
 _UPDATE_CENTER = "https://raw.githubusercontent.com/SonarSource/sonar-update-center-properties/master/update-center-source.properties"
 
-LTS = None
+LTA = None
 LATEST = None
-_HARDCODED_LTS = (9, 9, 4)
-_HARDCODED_LATEST = (10, 4, 1)
+_HARDCODED_LTA = (9, 9, 5)
+_HARDCODED_LATEST = (10, 5, 1)
+
+_SERVER_ID_KEY = "Server ID"
 
 
 class Platform:
     """Abstraction of the SonarQube "platform" concept"""
 
-    def __init__(self, some_url, some_token, cert_file=None):
+    def __init__(self, some_url: str, some_token: str, org: str = None, cert_file: str = None, http_timeout: int = 10) -> None:
         """Creates a SonarQube platform object
 
         :param some_url: base URL of the SonarQube platform
         :type some_url: str
         :param some_token: token to connect to the platform
         :type some_token: str
         :param cert_file: Client certificate, if any needed, defaults to None
         :type cert_file: str, optional
         :return: the SonarQube object
         :rtype: Platform
         """
-        self.url = some_url.rstrip("/")  #: SonarQube URL
+        self.url = some_url.rstrip("/").lower()  #: SonarQube URL
         self.__token = some_token
         self.__cert_file = cert_file
         self._version = None
         self.__sys_info = None
         self.__global_nav = None
         self._server_id = None
         self._permissions = None
+        self.http_timeout = http_timeout
+        self.organization = org
+        self.__is_sonarcloud = util.is_sonarcloud_url(self.url)
 
     def __str__(self):
         """
         :return: string representation of the SonarQube connection, with the token recognizable but largely redacted
         :rtype: str
         """
         return f"{util.redacted_token(self.__token)}@{self.url}"
@@ -96,17 +101,19 @@
     def version(self, digits=3, as_string=False):
         """Returns the SonarQube platform version
 
         :param digits: Number of digits to include in the version, defaults to 3
         :type digits: int, optional
         :param as_string: Whether to return the version as string or tuple, default to False (ie returns a tuple)
         :type as_string: bool, optional
-        :return: the SonarQube platform version
+        :return: the SonarQube platform version, or 0.0.0 for SonarCloud
         :rtype: tuple or str
         """
+        if self.is_sonarcloud():
+            return "sonarcloud" if as_string else tuple(int(n) for n in [0, 0, 0][0:digits])
         if digits < 1 or digits > 3:
             digits = 3
         if self._version is None:
             self._version = self.get("/api/server/version").text.split(".")
             util.logger.debug("Version = %s", self._version)
         if as_string:
             return ".".join(self._version[0:digits])
@@ -114,79 +121,71 @@
             return tuple(int(n) for n in self._version[0:digits])
 
     def edition(self):
         """
         :return: the SonarQube platform edition
         :rtype: str ("community", "developer", "enterprise" or "datacenter")
         """
+        if self.is_sonarcloud():
+            return "sonarcloud"
         if "edition" in self.global_nav():
             return util.edition_normalize(self.global_nav()["edition"])
         else:
             return util.edition_normalize(self.sys_info()["Statistics"]["edition"])
 
     def server_id(self):
         """
         :return: the SonarQube platform server id
         :rtype: str
         """
         if self._server_id is not None:
             return self._server_id
-        if self.__sys_info is not None and "Server ID" in self.__sys_info["System"]:
-            self._server_id = self.__sys_info["System"]["Server ID"]
+        if self.__sys_info is not None and _SERVER_ID_KEY in self.__sys_info["System"]:
+            self._server_id = self.__sys_info["System"][_SERVER_ID_KEY]
         else:
             self._server_id = json.loads(self.get("system/status").text)["id"]
         return self._server_id
 
+    def is_sonarcloud(self) -> bool:
+        """
+        :return: whether the target platform is SonarCloud
+        :rtype: bool
+        """
+        return self.__is_sonarcloud
+
     def basics(self):
         """
         :return: the 3 basic information of the platform: ServerId, Edition and Version
         :rtype: dict{"serverId": <id>, "edition": <edition>, "version": <version>}
         """
+        if self.is_sonarcloud():
+            return {"edition": self.edition()}
+
         return {
             "version": self.version(as_string=True),
             "edition": self.edition(),
             "serverId": self.server_id(),
         }
 
-    def get(self, api, params=None, exit_on_error=False, mute=()):
+    def get(self, api: str, params: dict[str, str] = None, exit_on_error: bool = False, mute: tuple[HTTPStatus] = ()) -> requests.Response:
         """Makes an HTTP GET request to SonarQube
 
         :param api: API to invoke (without the platform base URL)
         :type api: str
         :param params: params to pass in the HTTP request, defaults to None
         :type params: dict, optional
         :param exit_on_error: When to fail fast and exit if the HTTP status code is not 2XX, defaults to True
         :type exit_on_error: bool, optional
         :param mute: Tuple of HTTP Error codes to mute (ie not write an error log for), defaults to None.
                      Typically, Error 404 Not found may be expected sometimes so this can avoid logging an error for 404
         :type mute: tuple, optional
         :return: the result of the HTTP request
         :rtype: request.Response
         """
-        api = _normalize_api(api)
-        util.logger.debug("GET: %s", self.__urlstring(api, params))
-        try:
-            r = requests.get(
-                url=self.url + api, auth=self.__credentials(), verify=self.__cert_file, headers=_SONAR_TOOLS_AGENT, params=params, timeout=10
-            )
-            r.raise_for_status()
-        except requests.exceptions.HTTPError as e:
-            if exit_on_error or (r.status_code not in mute and r.status_code in (HTTPStatus.UNAUTHORIZED, HTTPStatus.FORBIDDEN)):
-                util.log_and_exit(r)
-            else:
-                if r.status_code in mute:
-                    util.logger.debug(_HTTP_ERROR, "GET", self.__urlstring(api, params), r.status_code)
-                else:
-                    util.logger.error(_HTTP_ERROR, "GET", self.__urlstring(api, params), r.status_code)
-                raise e
-        except requests.exceptions.Timeout as e:
-            util.exit_fatal(str(e), options.ERR_REQUEST_TIMEOUT)
-        except requests.RequestException as e:
-            util.exit_fatal(str(e), options.ERR_SONAR_API)
-        return r
+        return self.__run_request(requests.get, api, params, exit_on_error, mute)
 
     def post(self, api, params=None, exit_on_error=False, mute=()):
         """Makes an HTTP POST request to SonarQube
 
         :param api: API to invoke (without the platform base URL)
         :type api: str
         :param params: params to pass in the HTTP request, defaults to None
@@ -195,35 +194,15 @@
         :type exit_on_error: bool, optional
         :param mute: HTTP Error codes to mute (ie not write an error log for), defaults to None
                      Typically, Error 404 Not found may be expected sometimes so this can avoid logging an error for 404
         :type mute: tuple, optional
         :return: the result of the HTTP request
         :rtype: request.Response
         """
-        api = _normalize_api(api)
-        util.logger.debug("POST: %s", self.__urlstring(api, params))
-        try:
-            r = requests.post(
-                url=self.url + api, auth=self.__credentials(), verify=self.__cert_file, headers=_SONAR_TOOLS_AGENT, data=params, timeout=10
-            )
-            r.raise_for_status()
-        except requests.exceptions.HTTPError:
-            if exit_on_error or r.status_code in (HTTPStatus.UNAUTHORIZED, HTTPStatus.FORBIDDEN):
-                util.log_and_exit(r)
-            else:
-                if r.status_code in mute:
-                    util.logger.debug(_HTTP_ERROR, "POST", self.__urlstring(api, params), r.status_code)
-                else:
-                    util.logger.error(_HTTP_ERROR, "POST", self.__urlstring(api, params), r.status_code)
-                raise
-        except requests.exceptions.Timeout as e:
-            util.exit_fatal(str(e), options.ERR_REQUEST_TIMEOUT)
-        except requests.RequestException as e:
-            util.exit_fatal(str(e), options.ERR_SONAR_API)
-        return r
+        return self.__run_request(requests.post, api, params, exit_on_error, mute)
 
     def delete(self, api, params=None, exit_on_error=False, mute=()):
         """Makes an HTTP DELETE request to SonarQube
 
         :param api: API to invoke (without the platform base URL)
         :type api: str
         :param params: params to pass in the HTTP request, defaults to None
@@ -232,34 +211,58 @@
         :type exit_on_error: bool, optional
         :param mute: HTTP Error codes to mute (ie not write an error log for), defaults to None
                      Typically, Error 404 Not found may be expected sometimes so this can avoid logging an error for 404
         :type mute: tuple, optional
         :return: the result of the HTTP request
         :rtype: request.Response
         """
+        return self.__run_request(requests.delete, api, params, exit_on_error, mute)
+
+    def __run_request(
+        self, request: callable, api: str, params: dict[str, str] = None, exit_on_error: bool = False, mute: tuple[HTTPStatus] = ()
+    ) -> requests.Response:
+        """Makes an HTTP request to SonarQube"""
         api = _normalize_api(api)
-        util.logger.debug("DELETE: %s", self.__urlstring(api, params))
+        headers = _SONAR_TOOLS_AGENT
+        if params is None:
+            params = {}
+        if self.is_sonarcloud():
+            headers["Authorization"] = f"Bearer {self.__token}"
+            params["organization"] = self.organization
+        util.logger.debug("%s: %s", getattr(request, "__name__", repr(request)).upper(), self.__urlstring(api, params))
+
         try:
-            r = requests.delete(
-                url=self.url + api, auth=self.__credentials(), verify=self.__cert_file, params=params, headers=_SONAR_TOOLS_AGENT, timeout=10
-            )
+            retry = True
+            while retry:
+                r = request(
+                    url=self.url + api,
+                    auth=self.__credentials(),
+                    verify=self.__cert_file,
+                    headers=headers,
+                    params=params,
+                    timeout=self.http_timeout,
+                )
+                (retry, new_url) = _check_for_retry(r)
+                if retry:
+                    self.url = new_url
             r.raise_for_status()
-        except requests.exceptions.HTTPError:
-            if exit_on_error:
+        except requests.exceptions.HTTPError as e:
+            if exit_on_error or (r.status_code not in mute and r.status_code in (HTTPStatus.UNAUTHORIZED, HTTPStatus.FORBIDDEN)):
                 util.log_and_exit(r)
             else:
                 if r.status_code in mute:
-                    util.logger.debug(_HTTP_ERROR, "DELETE", self.__urlstring(api, params), r.status_code)
+                    util.logger.debug(_HTTP_ERROR, "GET", self.__urlstring(api, params), r.status_code)
                 else:
-                    util.logger.error(_HTTP_ERROR, "DELETE", self.__urlstring(api, params), r.status_code)
-                raise
+                    util.logger.error(_HTTP_ERROR, "GET", self.__urlstring(api, params), r.status_code)
+                raise e
         except requests.exceptions.Timeout as e:
             util.exit_fatal(str(e), options.ERR_REQUEST_TIMEOUT)
         except requests.RequestException as e:
             util.exit_fatal(str(e), options.ERR_SONAR_API)
+        return r
 
     def global_permissions(self):
         """Returns the SonarQube platform global permissions
 
         :return: dict{"users": {<login>: <permissions comma separated>, ...}, "groups"; {<name>: <permissions comma separated>, ...}}}
         :rtype: dict
         """
@@ -268,14 +271,16 @@
         return self._permissions
 
     def sys_info(self):
         """
         :return: the SonarQube platform system info file
         :rtype: dict
         """
+        if self.is_sonarcloud():
+            return {"System": {_SERVER_ID_KEY: "sonarcloud"}}
         if self.__sys_info is None:
             success, counter = False, 0
             while not success:
                 try:
                     resp = self.get("system/info", mute=(HTTPStatus.INTERNAL_SERVER_ERROR,))
                     success = True
                 except HTTPError as e:
@@ -301,28 +306,30 @@
         return self.__global_nav
 
     def database(self):
         """
         :return: the SonarQube platform backend database
         :rtype: str
         """
+        if self.is_sonarcloud():
+            return "postgres"
         if self.version() < (9, 7, 0):
             return self.sys_info()["Statistics"]["database"]["name"]
-        else:
-            return self.sys_info()["Database"]["Database"]
+        return self.sys_info()["Database"]["Database"]
 
     def plugins(self):
         """
         :return: the SonarQube platform plugins
         :rtype: dict
         """
+        if self.is_sonarcloud():
+            return {}
         if self.version() < (9, 7, 0):
             return self.sys_info()["Statistics"]["plugins"]
-        else:
-            return self.sys_info()["Plugins"]
+        return self.sys_info()["Plugins"]
 
     def get_settings(self, settings_list=None):
         """Returns a list of (or all) platform global settings value from their key
 
         :param key: settings_list
         :type key: list or str (comma separated)
         :return: the list of settings values
@@ -407,21 +414,29 @@
         :type full: bool, optional
         :return: dict of all properties with their values
         :rtype: dict
         """
         util.logger.info("Exporting platform global settings")
         json_data = {}
         for s in self.__settings(include_not_set=True).values():
+            if s.is_internal():
+                continue
             (categ, subcateg) = s.category()
+            if self.is_sonarcloud() and categ == settings.THIRD_PARTY_SETTINGS:
+                # What is reported as 3rd part are SonarCloud internal settings
+                continue
             util.update_json(json_data, categ, subcateg, s.to_json())
 
-        json_data[settings.GENERAL_SETTINGS].update({"webhooks": webhooks.export(self, full=full)})
+        hooks = webhooks.export(self, full=full)
+        if hooks is not None:
+            json_data[settings.GENERAL_SETTINGS].update({"webhooks": hooks})
         json_data["permissions"] = self.global_permissions().export()
         json_data["permissionTemplates"] = permission_templates.export(self, full=full)
-        json_data[settings.DEVOPS_INTEGRATION] = devops.export(self, full=full)
+        if not self.is_sonarcloud():
+            json_data[settings.DEVOPS_INTEGRATION] = devops.export(self, full=full)
         return json_data
 
     def set_webhooks(self, webhooks_data):
         """Sets global webhooks with a list of webhooks represented as JSON
 
         :param webhooks_data: the webhooks representation
         :type webhooks_data: dict
@@ -491,26 +506,32 @@
             elif key.startswith("audit.globalSettings.value"):
                 problems += _audit_setting_value(key, platform_settings, audit_settings, settings_url)
             elif key.startswith("audit.globalSettings.isSet"):
                 problems += _audit_setting_set(key, True, platform_settings, audit_settings, settings_url)
             elif key.startswith("audit.globalSettings.isNotSet"):
                 problems += _audit_setting_set(key, False, platform_settings, audit_settings, settings_url)
 
+        problems += (
+            self._audit_project_default_visibility()
+            + self._audit_global_permissions()
+            + webhooks.audit(self)
+            + permission_templates.audit(self, audit_settings)
+        )
+        if self.is_sonarcloud():
+            return problems
+
         pf_sif = self.sys_info()
         if self.version() >= (9, 7, 0):
             # Hack: Manually add edition in SIF (it's removed starting from 9.7 :-()
             pf_sif["edition"] = self.edition()
         problems += (
             _audit_maintainability_rating_grid(platform_settings, audit_settings, settings_url)
-            + self._audit_project_default_visibility()
             + self._audit_admin_password()
-            + self._audit_global_permissions()
-            + self._audit_lts_latest()
+            + self._audit_lta_latest()
             + sif.Sif(pf_sif, self).audit(audit_settings)
-            + webhooks.audit(self)
             + permission_templates.audit(self, audit_settings)
         )
         return problems
 
     def _audit_project_default_visibility(self):
         util.logger.info("Auditing project default visibility")
         problems = []
@@ -522,95 +543,101 @@
             visi = json.loads(resp.text)["organization"]["projectVisibility"]
         else:
             resp = self.get("settings/values", params={"keys": "projects.default.visibility"})
             visi = json.loads(resp.text)["settings"][0]["value"]
         util.logger.info("Project default visibility is '%s'", visi)
         if config.get_property("checkDefaultProjectVisibility") and visi != "private":
             rule = rules.get_rule(rules.RuleId.SETTING_PROJ_DEFAULT_VISIBILITY)
-            problems.append(pb.Problem(rule.type, rule.severity, rule.msq.format(visi), concerned_object=f"{self.url}/admin/projects_management"))
+            problems.append(pb.Problem(broken_rule=rule, msg=rule.msg.format(visi), concerned_object=f"{self.url}/admin/projects_management"))
         return problems
 
     def _audit_admin_password(self):
         util.logger.info("Auditing admin password")
         problems = []
         try:
-            r = requests.get(url=self.url + "/api/authentication/validate", auth=("admin", "admin"), timeout=10)
+            r = requests.get(url=self.url + "/api/authentication/validate", auth=("admin", "admin"), timeout=self.http_timeout)
             data = json.loads(r.text)
             if data.get("valid", False):
                 rule = rules.get_rule(rules.RuleId.DEFAULT_ADMIN_PASSWORD)
-                problems.append(pb.Problem(rule.type, rule.severity, rule.msg, concerned_object=self.url))
+                problems.append(pb.Problem(broken_rule=rule, msg=rule.msg, concerned_object=self.url))
             else:
                 util.logger.info("User 'admin' default password has been changed")
         except requests.RequestException as e:
             util.exit_fatal(str(e), options.ERR_SONAR_API)
         return problems
 
     def __audit_group_permissions(self):
         util.logger.info("Auditing group global permissions")
         problems = []
         perms_url = f"{self.url}/admin/permissions"
         groups = self.global_permissions().groups()
         if len(groups) > 10:
+            rule = rules.get_rule(rule_id=rules.RuleId.RISKY_GLOBAL_PERMISSIONS)
             msg = f"Too many ({len(groups)}) groups with global permissions"
-            problems.append(pb.Problem(typ.Type.BAD_PRACTICE, sev.Severity.MEDIUM, msg, concerned_object=perms_url))
+            problems.append(pb.Problem(broken_rule=rule, msg=msg, concerned_object=perms_url))
 
         for gr_name, gr_perms in groups.items():
             if gr_name == "Anyone":
                 rule = rules.get_rule(rules.RuleId.ANYONE_WITH_GLOBAL_PERMS)
-                problems.append(pb.Problem(rule.type, rule.severity, rule.msg, concerned_object=perms_url))
+                problems.append(pb.Problem(broken_rule=rule, msg=rule.msg, concerned_object=perms_url))
             if gr_name == "sonar-users" and (
                 "admin" in gr_perms or "gateadmin" in gr_perms or "profileadmin" in gr_perms or "provisioning" in gr_perms
             ):
                 rule = rules.get_rule(rules.RuleId.SONAR_USERS_WITH_ELEVATED_PERMS)
-                problems.append(pb.Problem(rule.type, rule.severity, rule.msg, concerned_object=perms_url))
+                problems.append(pb.Problem(broken_rule=rule, msg=rule.msg, concerned_object=perms_url))
 
         maxis = {"admin": 2, "gateadmin": 2, "profileadmin": 2, "scan": 2, "provisioning": 3}
         for key, name in permissions.ENTERPRISE_GLOBAL_PERMISSIONS.items():
             counter = self.global_permissions().count(perm_type="groups", perm_filter=(key,))
             if key in maxis and counter > maxis[key]:
+                rule = rules.get_rule(rule_id=rules.RuleId.RISKY_GLOBAL_PERMISSIONS)
                 msg = f"Too many ({counter}) groups with permission '{name}', {maxis[key]} max recommended"
-                problems.append(pb.Problem(typ.Type.BAD_PRACTICE, sev.Severity.MEDIUM, msg, concerned_object=perms_url))
+                problems.append(pb.Problem(broken_rule=rule, msg=msg, concerned_object=perms_url))
         return problems
 
     def __audit_user_permissions(self):
         util.logger.info("Auditing users global permissions")
         problems = []
         perms_url = f"{self.url}/admin/permissions"
         users = self.global_permissions().users()
         if len(users) > 10:
+            rule = rules.get_rule(rule_id=rules.RuleId.RISKY_GLOBAL_PERMISSIONS)
             msg = f"Too many ({len(users)}) users with direct global permissions, use groups instead"
-            problems.append(pb.Problem(typ.Type.BAD_PRACTICE, sev.Severity.MEDIUM, msg, concerned_object=perms_url))
+            problems.append(pb.Problem(broken_rule=rule, msg=msg, concerned_object=perms_url))
 
         maxis = {"admin": 3, "gateadmin": 3, "profileadmin": 3, "scan": 3, "provisioning": 3}
         for key, name in permissions.ENTERPRISE_GLOBAL_PERMISSIONS.items():
             counter = self.global_permissions().count(perm_type="users", perm_filter=(key,))
             if key in maxis and counter > maxis[key]:
+                rule = rules.get_rule(rule_id=rules.RuleId.RISKY_GLOBAL_PERMISSIONS)
                 msg = f"Too many ({counter}) users with permission '{name}', use groups instead"
-                problems.append(pb.Problem(typ.Type.BAD_PRACTICE, sev.Severity.MEDIUM, msg, concerned_object=perms_url))
+                problems.append(pb.Problem(broken_rule=rule, msg=msg, concerned_object=perms_url))
         return problems
 
     def _audit_global_permissions(self):
         util.logger.info("--- Auditing global permissions ---")
         return self.__audit_user_permissions() + self.__audit_group_permissions()
 
-    def _audit_lts_latest(self):
+    def _audit_lta_latest(self) -> list[pb.Problem]:
+        if self.is_sonarcloud():
+            return []
         sq_vers, v = self.version(3), None
-        if sq_vers < lts(2):
-            rule = rules.get_rule(rules.RuleId.BELOW_LTS)
-            v = lts()
-        elif sq_vers < lts(3):
-            rule = rules.get_rule(rules.RuleId.LTS_PATCH_MISSING)
-            v = lts()
-        elif sq_vers[:2] > lts(2) and sq_vers < latest(2):
+        if sq_vers < lta(2):
+            rule = rules.get_rule(rules.RuleId.BELOW_LTA)
+            v = lta()
+        elif sq_vers < lta(3):
+            rule = rules.get_rule(rules.RuleId.LTA_PATCH_MISSING)
+            v = lta()
+        elif sq_vers[:2] > lta(2) and sq_vers < latest(2):
             rule = rules.get_rule(rules.RuleId.BELOW_LATEST)
             v = latest()
         if not v:
             return []
         msg = rule.msg.format(_version_as_string(sq_vers), _version_as_string(v))
-        return [pb.Problem(rule.type, rule.severity, msg, concerned_object=self.url)]
+        return [pb.Problem(broken_rule=rule, msg=msg, concerned_object=self.url)]
 
 
 # --------------------- Static methods -----------------
 # this is a pointer to the module object instance itself.
 this = sys.modules[__name__]
 this.context = Platform(os.getenv("SONAR_HOST_URL", "http://localhost:9000"), os.getenv("SONAR_TOKEN", ""))
 
@@ -636,15 +663,17 @@
     if v[0] not in platform_settings:
         util.logger.warning(_NON_EXISTING_SETTING_SKIPPED, v[0])
         return []
     util.logger.info("Auditing that setting %s has common/recommended value '%s'", v[0], v[1])
     s = platform_settings.get(v[0], "")
     if s == v[1]:
         return []
-    return [pb.Problem(v[2], v[3], f"Setting {v[0]} has potentially incorrect or unsafe value '{s}'", concerned_object=url)]
+    rule = rules.get_rule(rules.RuleId.DUBIOUS_GLOBAL_SETTING)
+    msg = f"Setting {v[0]} has potentially incorrect or unsafe value '{s}'"
+    return [pb.Problem(broken_rule=rule, msg=msg, concerned_object=url)]
 
 
 def _audit_setting_in_range(key, platform_settings, audit_settings, sq_version, url):
     v = _get_multiple_values(5, audit_settings[key], "MEDIUM", "CONFIGURATION")
     if v is None:
         util.logger.error(WRONG_CONFIG_MSG, key, audit_settings[key])
         return []
@@ -659,74 +688,69 @@
         "Auditing that setting %s is within recommended range [%.2f-%.2f]",
         v[0],
         min_v,
         max_v,
     )
     if min_v <= value <= max_v:
         return []
-    return [
-        pb.Problem(v[4], v[3], f"Setting '{v[0]}' value {platform_settings[v[0]]} is outside recommended range [{v[1]}-{v[2]}]", concerned_object=url)
-    ]
+    rule = rules.get_rule(rules.RuleId.DUBIOUS_GLOBAL_SETTING)
+    msg = f"Setting '{v[0]}' value {platform_settings[v[0]]} is outside recommended range [{v[1]}-{v[2]}]"
+    return [pb.Problem(broken_rule=rule, msg=msg, concerned_object=url)]
 
 
 def _audit_setting_set(key, check_is_set, platform_settings, audit_settings, url):
     v = _get_multiple_values(3, audit_settings[key], "MEDIUM", "CONFIGURATION")
     if v is None:
         util.logger.error(WRONG_CONFIG_MSG, key, audit_settings[key])
         return []
     util.logger.info("Auditing whether setting %s is set or not", v[0])
     if platform_settings.get(v[0], "") == "":  # Setting is not set
         if check_is_set:
             rule = rules.get_rule(rules.RuleId.SETTING_NOT_SET)
-            return [pb.Problem(rule.type, rule.severity, rule.msg.format(v[0]), concerned_object=url)]
+            return [pb.Problem(broken_rule=rule, msg=rule.msg.format(v[0]), concerned_object=url)]
         util.logger.info("Setting %s is not set", v[0])
     else:
         if not check_is_set:
-            return [pb.Problem(v[1], v[2], f"Setting {v[0]} is set, although it should probably not", concerned_object=url)]
+            rule = rules.get_rule(rules.RuleId.SETTING_SET)
+            return [pb.Problem(broken_rule=rule, msg=rule.msg, concerned_object=url)]
         util.logger.info("Setting %s is set with value %s", v[0], platform_settings[v[0]])
     return []
 
 
-def _audit_maintainability_rating_range(value, range, rating_letter, severity, domain, url):
-    util.logger.debug(
+def _audit_maintainability_rating_range(value: float, range: tuple[float, float], rating_letter: str, url: str):
+    util.logger.info(
         "Checking that maintainability rating threshold %.1f%% for '%s' is within recommended range [%.1f%%-%.1f%%]",
         value * 100,
         rating_letter,
         range[0] * 100,
         range[1] * 100,
     )
     if range[0] <= value <= range[1]:
         return []
-    return [
-        pb.Problem(
-            domain,
-            severity,
-            f"Maintainability rating threshold {value * 100}% for {rating_letter} "
-            f"is NOT within recommended range [{range[0] * 100:.1f}%-{range[1] * 100:.1f}%]",
-            concerned_object=url,
-        )
-    ]
+    rule = rules.get_rule(rules.RuleId.SETTING_MAINT_GRID)
+    msg = rule.msg.format(f"{value * 100:.1f}", rating_letter, f"{range[0] * 100:.1f}", f"{range[1] * 100:.1f}")
+    return [pb.Problem(broken_rule=rule, msg=msg, concerned_object=url)]
 
 
 def _audit_maintainability_rating_grid(platform_settings, audit_settings, url):
     thresholds = util.csv_to_list(platform_settings["sonar.technicalDebt.ratingGrid"])
     problems = []
-    util.logger.debug("Auditing maintainabillity rating grid")
+    util.logger.info("Auditing maintainability rating grid")
     for key in audit_settings:
         if not key.startswith("audit.globalSettings.maintainabilityRating"):
             continue
         (_, _, _, letter, _, _) = key.split(".")
         if letter not in ["A", "B", "C", "D"]:
             util.logger.error("Incorrect audit configuration setting %s, skipping audit", key)
             continue
         value = float(thresholds[ord(letter.upper()) - 65])
         v = _get_multiple_values(4, audit_settings[key], sev.Severity.MEDIUM, typ.Type.CONFIGURATION)
         if v is None:
             continue
-        problems += _audit_maintainability_rating_range(value, (float(v[0]), float(v[1])), letter, v[2], v[3], url)
+        problems += _audit_maintainability_rating_range(value, (float(v[0]), float(v[1])), letter, url)
     return problems
 
 
 def _get_multiple_values(n, setting, severity, domain):
     values = util.csv_to_list(setting)
     if len(values) < (n - 2):
         return None
@@ -740,60 +764,71 @@
     return values
 
 
 def _version_as_string(a_version):
     return ".".join([str(n) for n in a_version])
 
 
-def __lts_and_latest():
-    global LTS
+def __lta_and_latest() -> tuple[tuple[int], tuple[int]]:
+    """Returns the current version of LTA and LATEST, if possible querying the update center,
+    using hardcoded values as fallback"""
+    global LTA
     global LATEST
-    if LTS is None:
+    if LTA is None:
         util.logger.debug("Attempting to reach Sonar update center")
         _, tmpfile = tempfile.mkstemp(prefix="sonar-tools", suffix=".txt", text=True)
         try:
             with open(tmpfile, "w", encoding="utf-8") as fp:
                 print(requests.get(_UPDATE_CENTER, headers=_SONAR_TOOLS_AGENT, timeout=10).text, file=fp)
             with open(tmpfile, "r", encoding="utf-8") as fp:
                 upd_center_props = jprops.load_properties(fp)
             v = upd_center_props.get("ltsVersion", "9.9.0").split(".")
             if len(v) == 2:
                 v.append("0")
-            LTS = tuple(int(n) for n in v)
+            LTA = tuple(int(n) for n in v)
             v = upd_center_props.get("publicVersions", "10.4").split(",")[-1].split(".")
             if len(v) == 2:
                 v.append("0")
             LATEST = tuple(int(n) for n in v)
-            util.logger.debug("Sonar update center says LTS = %s, LATEST = %s", str(LTS), str(LATEST))
+            util.logger.debug("Sonar update center says LTA (ex-LTS) = %s, LATEST = %s", str(LTA), str(LATEST))
         except (EnvironmentError, requests.exceptions.HTTPError):
-            LTS = _HARDCODED_LTS
+            LTA = _HARDCODED_LTA
             LATEST = _HARDCODED_LATEST
-            util.logger.debug("Sonar update center read failed, hardcoding LTS = %s, LATEST = %s", str(LTS), str(LATEST))
+            util.logger.debug("Sonar update center read failed, hardcoding LTA (ex-LTS) = %s, LATEST = %s", str(LTA), str(LATEST))
         try:
             os.remove(tmpfile)
         except EnvironmentError:
             pass
-    return (LTS, LATEST)
+    return LTA, LATEST
 
 
-def lts(digits=3):
+def lta(digits=3) -> tuple[int]:
     """
-    :return: the current SonarQube LTS version
+    :return: the current SonarQube LTA (ex-LTS) version
     :params digits: number of digits to consider in the version (min 1, max 3), defaults to 3
     :type digits: int, optional
     :rtype: tuple (x, y, z)
     """
     if digits < 1 or digits > 3:
         digits = 3
-    return __lts_and_latest()[0][0:digits]
+    return __lta_and_latest()[0][0:digits]
 
 
 def latest(digits=3):
     """
     :return: the current SonarQube LATEST version
     :params digits: number of digits to consider in the version (min 1, max 3), defaults to 3
     :type digits: int, optional
     :rtype: tuple (x, y, z)
     """
     if digits < 1 or digits > 3:
         digits = 3
-    return __lts_and_latest()[1][0:digits]
+    return __lta_and_latest()[1][0:digits]
+
+
+def _check_for_retry(response: requests.models.Response) -> tuple[bool, str]:
+    """Verifies if a response had a 301 Moved permanently and if so provide the new location"""
+    if len(response.history) > 0 and response.history[0].status_code == HTTPStatus.MOVED_PERMANENTLY:
+        new_url = "/".join(response.history[0].headers["Location"].split("/")[0:3])
+        util.logger.debug("Moved permanently to URL %s", new_url)
+        return True, new_url
+    return False, None
```

## sonar/portfolios.py

```diff
@@ -173,15 +173,15 @@
     def url(self):
         return f"{self.endpoint.url}/portfolio?id={self.key}"
 
     def selection_mode(self):
         return self._selection_mode
 
     def root_portfolio(self):
-        if self.parent is None:
+        if self.parent is None or self.parent.key == self.key:
             util.logger.debug("Found root for %s, parent = %s", self.key, str(self.parent))
             self._root_portfolio = self
         else:
             util.logger.debug("recursing root for %s, parent = %s", self.key, str(self.parent))
             self._root_portfolio = self.parent.root_portfolio()
         return self._root_portfolio
 
@@ -212,37 +212,61 @@
 
     def sub_portfolios(self, full=False):
         self.refresh()
         # self._sub_portfolios = _sub_portfolios(self._json, self.endpoint.version(), full=full)
         self.create_sub_portfolios()
         return self._sub_portfolios
 
+    def to_json(self) -> dict[str, str]:
+        """Returns the portfolio representation as JSON"""
+        data = {
+            "key": self.key,
+            "name": self.name,
+            "description": None if self._description == "" else self._description,
+            _PROJECT_SELECTION_MODE: self.selection_mode(),
+            "visibility": self._visibility,
+            _PROJECT_SELECTION_REGEXP: self.regexp(),
+            _PROJECT_SELECTION_BRANCH: self._selection_branch,
+            _PROJECT_SELECTION_TAGS: util.list_to_csv(self.tags(), separator=", "),
+        }
+        if not self.is_sub_portfolio:
+            data["permissions"] = self.permissions().export()
+            data["visibility"] = self._visibility
+
+        if self._sub_portfolios:
+            for key, subp in self._sub_portfolios:
+                data["subPortfolios"][key] = subp.to_json()
+                if not subp.is_subportfolio:
+                    data["subPortfolios"][key]["byReference"] = True
+        return util.remove_nones(data)
+
     def create_sub_portfolios(self):
-        if "subViews" in self._json and len(self._json["subViews"]) > 0:
-            util.logger.debug("Inspecting %s subportfolios data = %s", str(self), util.json_dump(self._json["subViews"]))
-            self._sub_portfolios = {}
-            for oldp in self._json["subViews"]:
-                p = oldp.copy()
-                util.logger.debug("Found subport data = %s", util.json_dump(p))
-                if p["qualifier"] == _PORTFOLIO_QUALIFIER:
-                    key = p.get("originalKey", None)
-                    if key is None:
-                        key = p.pop("key").split(":")[-1]
-                else:
-                    key = p.pop("key")
-                try:
-                    subp = Portfolio.get_object(self.endpoint, key)
+        util.logger.debug("Creating subportfolios for %s with JSON %s", str(self), str(self._json))
+        if "subViews" not in self._json or len(self._json["subViews"]) == 0:
+            return
+
+        util.logger.debug("Inspecting %s subportfolios data = %s", str(self), util.json_dump(self._json["subViews"]))
+        self._sub_portfolios = {}
+        for oldp in self._json["subViews"]:
+            p = oldp.copy()
+            util.logger.debug("Found subport data = %s", util.json_dump(p))
+            key = p.pop("key")
+            if p["qualifier"] == _PORTFOLIO_QUALIFIER:
+                key = p.get("originalKey", key.split(":")[-1])
+            try:
+                subp = Portfolio.get_object(self.endpoint, key)
+                if p["qualifier"] == _SUBPORTFOLIO_QUALIFIER:
                     subp.set_parent(self)
-                except exceptions.ObjectNotFound:
-                    subp = Portfolio.create(self.endpoint, name=p.pop("name"), key=key, parent=self.key, description=p.pop("desc", None), **p)
-                util.logger.debug("Subp = %s", str(subp))
-                subp.reload(oldp)
-                self._sub_portfolios[subp.key] = subp
-                subp.create_sub_portfolios()
-                subp.projects()
+            except exceptions.ObjectNotFound:
+                subp = Portfolio.create(self.endpoint, name=p.pop("name"), key=key, parent=self.key, description=p.pop("desc", None), **p)
+            util.logger.debug("%s Subp = %s", str(self), str(subp))
+            subp.reload(oldp)
+            self._sub_portfolios[subp.key] = subp
+            subp.create_sub_portfolios()
+            subp.projects()
 
     def regexp(self):
         if self.selection_mode() != SELECTION_MODE_REGEXP:
             self._regexp = None
         elif self._regexp is None:
             self._regexp = self._json["regexp"]
         return self._regexp
@@ -290,17 +314,19 @@
         util.logger.info("Auditing %s", str(self))
         return self._audit_empty(audit_settings) + self._audit_singleton(audit_settings) + self._audit_bg_task(audit_settings)
 
     def export(self, full=False):
         util.logger.info("Exporting %s", str(self))
         self.refresh()
         json_data = self._json
-        subp = self.sub_portfolios(full=full)
-        if subp:
-            json_data.update(subp)
+        subportfolios = self.sub_portfolios(full=full)
+        if subportfolios:
+            json_data["subPortfolios"] = {}
+            for s in subportfolios.values():
+                json_data["subPortfolios"][s.key] = s.to_json()
         json_data.update(
             {
                 "key": self.key,
                 "name": self.name,
                 "description": None if self._description == "" else self._description,
                 _PROJECT_SELECTION_MODE: self.selection_mode(),
                 "visibility": self._visibility,
@@ -653,18 +679,30 @@
     return util.search_by_name(endpoint, name, _SEARCH_API, "components")
 
 
 def search_by_key(endpoint, key):
     return util.search_by_key(endpoint, key, _SEARCH_API, "components")
 
 
-def export(endpoint, key_list=None, full=False):
+def export(endpoint: object, key_list: list[str] = None, full: bool = False) -> dict[str, str]:
+    """Exports portfolios as JSON
+
+    :param Platform endpoint: Reference to the SonarQube platform
+    :param key_list: list of portfoliios keys to export as csv or list, defaults to all if None
+    :type key_list: list, optional
+    :param full: Whether to export all attributes, including those that can't be set, defaults to False
+    :type full: bool
+    :return: Dict of applications settings
+    :rtype: dict
+    """
     if endpoint.edition() in ("community", "developer"):
-        util.logger.info("No portfolios in community and developer editions")
-        return None
+        raise exceptions.UnsupportedOperation("Portfolios do not exist in community and developer edition, export skipped")
+    if endpoint.is_sonarcloud():
+        raise exceptions.UnsupportedOperation("Portfolios do not exist in SonarCloud, export skipped")
+
     util.logger.info("Exporting portfolios")
     if key_list:
         nb_portfolios = len(key_list)
     else:
         nb_portfolios = count(endpoint=endpoint)
     i = 0
     exported_portfolios = {}
```

## sonar/qualitygates.py

```diff
@@ -28,15 +28,15 @@
 from requests.exceptions import HTTPError
 import sonar.sqobject as sq
 from sonar import measures, exceptions
 import sonar.permissions.qualitygate_permissions as permissions
 from sonar.projects import projects
 import sonar.utilities as util
 
-from sonar.audit import rules, severities, types
+from sonar.audit import rules
 import sonar.audit.problem as pb
 
 
 _OBJECTS = {}
 _MAP = {}
 
 #: Quality gates APIs
@@ -278,22 +278,23 @@
     def __audit_conditions(self):
         problems = []
         for c in self.conditions():
             m = c["metric"]
             if m not in GOOD_QG_CONDITIONS:
                 rule = rules.get_rule(rules.RuleId.QG_WRONG_METRIC)
                 msg = rule.msg.format(str(self), m)
-                problems.append(pb.Problem(rule.type, rule.severity, msg, concerned_object=self))
+                problems.append(pb.Problem(broken_rule=rule, msg=msg, concerned_object=self))
                 continue
             val = int(c["error"])
-            (mini, maxi, msg) = GOOD_QG_CONDITIONS[m]
-            util.logger.debug("Condition on metric '%s': Check that %d in range [%d - %d]", m, val, mini, maxi)
+            (mini, maxi, precise_msg) = GOOD_QG_CONDITIONS[m]
+            util.logger.info("Condition on metric '%s': Check that %d in range [%d - %d]", m, val, mini, maxi)
             if val < mini or val > maxi:
-                msg = f"{str(self)} condition on metric '{m}': {msg}".format(self.name, m, msg)
-                problems.append(pb.Problem(types.Type.BAD_PRACTICE, severities.Severity.HIGH, msg, concerned_object=self))
+                rule = rules.get_rule(rules.RuleId.QG_WRONG_THRESHOLD)
+                msg = rule.msg.format(str(self), str(val), str(m), str(mini), str(maxi), precise_msg)
+                problems.append(pb.Problem(broken_rule=rule, msg=msg, concerned_object=self))
         return problems
 
     def audit(self, audit_settings=None):
         """
         :meta private:
         """
         my_name = str(self)
@@ -303,25 +304,25 @@
             return problems
         max_cond = int(util.get_setting(audit_settings, "audit.qualitygates.maxConditions", 8))
         nb_conditions = len(self.conditions())
         util.logger.debug("Auditing %s number of conditions (%d) is OK", my_name, nb_conditions)
         if nb_conditions == 0:
             rule = rules.get_rule(rules.RuleId.QG_NO_COND)
             msg = rule.msg.format(my_name)
-            problems.append(pb.Problem(rule.type, rule.severity, msg, concerned_object=self))
+            problems.append(pb.Problem(broken_rule=rule, msg=msg, concerned_object=self))
         elif nb_conditions > max_cond:
             rule = rules.get_rule(rules.RuleId.QG_TOO_MANY_COND)
             msg = rule.msg.format(my_name, nb_conditions, max_cond)
-            problems.append(pb.Problem(rule.type, rule.severity, msg, concerned_object=self))
+            problems.append(pb.Problem(broken_rule=rule, msg=msg, concerned_object=self))
         problems += self.__audit_conditions()
         util.logger.debug("Auditing that %s has some assigned projects", my_name)
         if not self.is_default and len(self.projects()) == 0:
             rule = rules.get_rule(rules.RuleId.QG_NOT_USED)
             msg = rule.msg.format(my_name)
-            problems.append(pb.Problem(rule.type, rule.severity, msg, concerned_object=self))
+            problems.append(pb.Problem(broken_rule=rule, msg=msg, concerned_object=self))
         return problems
 
     def to_json(self, full=False):
         json_data = self._json
         if not self.is_default and not full:
             json_data.pop("isDefault")
         if self.is_built_in:
@@ -343,15 +344,15 @@
     problems = []
     quality_gates_list = get_list(endpoint)
     max_qg = util.get_setting(audit_settings, "audit.qualitygates.maxNumber", 5)
     nb_qg = len(quality_gates_list)
     util.logger.debug("Auditing that there are no more than %s quality gates", str(max_qg))
     if nb_qg > max_qg:
         rule = rules.get_rule(rules.RuleId.QG_TOO_MANY_GATES)
-        problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format(nb_qg, 5), concerned_object=f"{endpoint.url}/quality_gates"))
+        problems.append(pb.Problem(broken_rule=rule, msg=rule.msg.format(nb_qg, 5), concerned_object=f"{endpoint.url}/quality_gates"))
     for qg in quality_gates_list.values():
         problems += qg.audit(audit_settings)
     return problems
 
 
 def get_list(endpoint):
     """
```

## sonar/qualityprofiles.py

```diff
@@ -14,14 +14,16 @@
 # Lesser General Public License for more details.
 #
 # You should have received a copy of the GNU Lesser General Public License
 # along with this program; if not, write to the Free Software Foundation,
 # Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
 #
 
+from __future__ import annotations
+from typing import Union
 import json
 from http import HTTPStatus
 from queue import Queue
 from threading import Thread, Lock
 from requests import HTTPError
 import requests.utils
 from sonar import rules, languages
@@ -331,27 +333,86 @@
         """
         data = json.loads(self.get("qualityprofiles/compare", params={"leftKey": self.key, "rightKey": another_qp.key}).text)
         for r in data["inLeft"] + data["same"] + data["inRight"] + data["modified"]:
             for k in ("name", "pluginKey", "pluginName", "languageKey", "languageName"):
                 r.pop(k, None)
         return data
 
-    def diff(self, another_qp):
+    def _treat_added_rules(self, added_rules: dict[str:str], added_flag: bool = True) -> dict[str:str]:
+        diff_rules = {}
+        my_rules = self.rules()
+        for r in added_rules:
+            r_key = r.pop("key")
+            diff_rules[r_key] = r
+            if (added_flag and r_key in my_rules) or (not added_flag and r_key not in my_rules):
+                rule_obj = rules.get_object(r_key, self.endpoint)
+                diff_rules[r_key] = rules.convert_for_export(rule_obj.to_json(), rule_obj.language)
+            if "severity" in r:
+                if isinstance(diff_rules[r_key], str):
+                    diff_rules[r_key] = r["severity"]
+                else:
+                    diff_rules[r_key]["severity"] = r["severity"]
+        return diff_rules
+
+    def _treat_removed_rules(self, removed_rules: dict[str:str]) -> dict[str:str]:
+        return self._treat_added_rules(removed_rules, added_flag=False)
+
+    def _treat_modified_rules(self, modified_rules: dict[str:str]) -> dict[str:str]:
+        diff_rules = {}
+        my_rules = self.rules()
+        for r in modified_rules:
+            r_key, r_left, r_right = r["key"], r["left"], r["right"]
+            diff_rules[r_key] = {"modified": True}
+            parms = None
+            if r_left["severity"] != r_right["severity"]:
+                diff_rules[r_key]["severity"] = r_left["severity"]
+            if len(r_left.get("params", {})) > 0:
+                diff_rules[r_key]["params"] = r_left["params"]
+                parms = r_left["params"]
+            if r_key not in my_rules:
+                continue
+            data = rules.convert_for_export(my_rules[r_key].to_json(), my_rules[r_key].language)
+            if "templateKey" in data:
+                diff_rules[r_key]["templateKey"] = data["templateKey"]
+                diff_rules[r_key]["params"] = data["params"]
+                if parms:
+                    diff_rules[r_key]["params"].update(parms)
+        return diff_rules
+
+    def diff(self, another_qp: QualityProfile, qp_json_data: dict[str:str] = None) -> tuple[dict[str:str], dict[str:str]]:
         """Returns the list of rules added or modified in self compared to another_qp (for inheritance)
         :param another_qp: The second quality profile to diff
         :type another_qp: QualityProfile
         :return: dict result of the diff ("inLeft", "modified")
         :rtype: dict
         """
         util.logger.debug("Comparing %s and %s", str(self), str(another_qp))
         compare_result = self.compare(another_qp)
-        my_rules = self.rules()
-        diff_rules = _treat_added_rules(my_rules, compare_result["inLeft"])
-        diff_rules.update(_treat_modified_rules(my_rules, compare_result["modified"]))
-        return diff_rules
+        diff_rules = {"addedRules": {}, "modifiedRules": {}}
+        if len(compare_result["inLeft"]) > 0:
+            diff_rules["addedRules"] = self._treat_added_rules(compare_result["inLeft"])
+        if len(compare_result["modified"]) > 0:
+            diff_rules["modifiedRules"] = self._treat_modified_rules(compare_result["modified"])
+        if len(compare_result["inRight"]) > 0:
+            diff_rules["removedRules"] = self._treat_removed_rules(compare_result["inRight"])
+        elif self.endpoint.version() >= (10, 3, 0):
+            diff_rules["removedRules"] = {}
+
+        util.logger.debug("Returning QP diff %s", str(diff_rules))
+        if qp_json_data is None:
+            return (diff_rules, qp_json_data)
+        for index in ("addedRules", "modifiedRules", "removedRules"):
+            if index not in diff_rules:
+                continue
+            if index not in qp_json_data:
+                qp_json_data[index] = {}
+            for k, v in diff_rules[index].items():
+                qp_json_data[index][k] = v if isinstance(v, str) or "templateKey" not in v else v["severity"]
+
+        return (diff_rules, qp_json_data)
 
     def projects(self):
         """Returns the list of projects keys using this quality profile
         :return: dict result of the diff ("inLeft", "modified")
         :rtype: List[project_key]
         """
 
@@ -417,45 +478,45 @@
 
         util.logger.debug("Auditing %s (key '%s')", str(self), self.key)
         problems = []
         age = util.age(self.last_update(), rounded=True)
         if age > audit_settings.get("audit.qualityProfiles.maxLastChangeAge", 180):
             rule = arules.get_rule(arules.RuleId.QP_LAST_CHANGE_DATE)
             msg = rule.msg.format(str(self), age)
-            problems.append(pb.Problem(rule.type, rule.severity, msg, concerned_object=self))
+            problems.append(pb.Problem(broken_rule=rule, msg=msg, concerned_object=self))
 
         total_rules = rules.count(endpoint=self.endpoint, languages=self.language)
         if self.nbr_rules < int(total_rules * audit_settings.get("audit.qualityProfiles.minNumberOfRules", 0.5)):
             rule = arules.get_rule(arules.RuleId.QP_TOO_FEW_RULES)
             msg = rule.msg.format(str(self), self.nbr_rules, total_rules)
-            problems.append(pb.Problem(rule.type, rule.severity, msg, concerned_object=self))
+            problems.append(pb.Problem(broken_rule=rule, msg=msg, concerned_object=self))
 
         age = util.age(self.last_use(), rounded=True)
         if self.project_count == 0 or age is None:
             rule = arules.get_rule(arules.RuleId.QP_NOT_USED)
             msg = rule.msg.format(str(self))
-            problems.append(pb.Problem(rule.type, rule.severity, msg, concerned_object=self))
+            problems.append(pb.Problem(broken_rule=rule, msg=msg, concerned_object=self))
         elif age > audit_settings.get("audit.qualityProfiles.maxUnusedAge", 60):
             rule = arules.get_rule(arules.RuleId.QP_LAST_USED_DATE)
             msg = rule.msg.format(str(self), age)
-            problems.append(pb.Problem(rule.type, rule.severity, msg, concerned_object=self))
+            problems.append(pb.Problem(broken_rule=rule, msg=msg, concerned_object=self))
         if audit_settings.get("audit.qualityProfiles.checkDeprecatedRules", True):
             max_deprecated_rules = 0
             parent_qp = self.built_in_parent()
             if parent_qp is not None:
                 max_deprecated_rules = parent_qp.nbr_deprecated_rules
             if self.nbr_deprecated_rules > max_deprecated_rules:
                 rule = arules.get_rule(arules.RuleId.QP_USE_DEPRECATED_RULES)
                 msg = rule.msg.format(str(self), self.nbr_deprecated_rules)
-                problems.append(pb.Problem(rule.type, rule.severity, msg, concerned_object=self))
+                problems.append(pb.Problem(broken_rule=rule, msg=msg, concerned_object=self))
 
         return problems
 
 
-def search(endpoint, params=None):
+def search(endpoint: object, params: dict[str:str] = None) -> dict[str:QualityProfile]:
     """Searches projects in SonarQube
 
     :param params: list of parameters to filter quality profiles to search
     :type params: dict
     :return: list of quality profiles
     :rtype: dict{key: QualityProfile}
     """
@@ -497,46 +558,44 @@
     for qp in search(endpoint).values():
         problems += qp.audit(audit_settings)
         langs[qp.language] = langs.get(qp.language, 0) + 1
     for lang, nb_qp in langs.items():
         if nb_qp > 5:
             rule = arules.get_rule(arules.RuleId.QP_TOO_MANY_QP)
             problems.append(
-                pb.Problem(rule.type, rule.severity, rule.msg.format(nb_qp, lang, 5), concerned_object=f"{endpoint.url}/profiles?language={lang}")
+                pb.Problem(broken_rule=rule, msg=rule.msg.format(nb_qp, lang, 5), concerned_object=f"{endpoint.url}/profiles?language={lang}")
             )
     return problems
 
 
 def hierarchize(qp_list):
     """Organize a flat list of QP in hierarchical (inheritance) fashion
 
     :param qp_list: List of quality profiles
     :type qp_list: {<language>: {<qp_name>: <qd_data>}}
     :return: Same list with child profiles nested in their parent
     :rtype: {<language>: {<qp_name>: {"children": <qp_list>; <qp_data>}}}
     """
     util.logger.info("Organizing quality profiles in hierarchy")
     for lang, qpl in qp_list.copy().items():
-        for qp_name, qp_value in qpl.copy().items():
+        for qp_name, qp_json_data in qpl.copy().items():
             util.logger.debug("Treating %s:%s", lang, qp_name)
-            if "parentName" not in qp_value:
+            if "parentName" not in qp_json_data:
                 continue
-            util.logger.debug("QP name '%s:%s' has parent '%s'", lang, qp_name, qp_value["parentName"])
-            if _CHILDREN_KEY not in qp_list[lang][qp_value["parentName"]]:
-                qp_list[lang][qp_value["parentName"]][_CHILDREN_KEY] = {}
+            parent_qp_name = qp_json_data["parentName"]
+            qp_json_data.pop("rules", None)
+            util.logger.debug("QP name '%s:%s' has parent '%s'", lang, qp_name, qp_json_data["parentName"])
+            if _CHILDREN_KEY not in qp_list[lang][qp_json_data["parentName"]]:
+                qp_list[lang][qp_json_data["parentName"]][_CHILDREN_KEY] = {}
 
-            parent_qp = get_object(qp_value["parentName"], lang)
             this_qp = get_object(name=qp_name, language=lang)
-            qp_value["rules"] = {}
-            for k, v in this_qp.diff(parent_qp).items():
-                qp_value["rules"][k] = v if isinstance(v, str) or "templateKey" not in v else v["severity"]
-
-            qp_list[lang][qp_value["parentName"]][_CHILDREN_KEY][qp_name] = qp_value
+            (_, qp_json_data) = this_qp.diff(get_object(parent_qp_name, lang), qp_json_data)
+            qp_list[lang][parent_qp_name][_CHILDREN_KEY][qp_name] = qp_json_data
             qp_list[lang].pop(qp_name)
-            qp_value.pop("parentName")
+            qp_json_data.pop("parentName")
     return qp_list
 
 
 def export(endpoint, in_hierarchy=True, full=False):
     """Exports all quality profiles configuration as dict
 
     :param endpoint: reference to the SonarQube platform
@@ -559,15 +618,15 @@
             qp_list[lang] = {}
         qp_list[lang][name] = json_data
     if in_hierarchy:
         qp_list = hierarchize(qp_list)
     return qp_list
 
 
-def get_object(name, language, endpoint=None):
+def get_object(name: str, language: str, endpoint: object = None) -> Union[QualityProfile, None]:
     """Returns a quality profile Object from its name and language
 
     :param name: Quality profile name
     :type name: str
     :param language: Quality profile language
     :type language: str
     :param endpoint: Reference to the SonarQube platform
@@ -661,44 +720,7 @@
     :type name: str
     :param language: Quality profile language
     :type language: str
     :return: whether the project exists
     :rtype: bool
     """
     return get_object(name=name, language=language, endpoint=endpoint) is not None
-
-
-def _treat_added_rules(my_rules, added_rules):
-    diff_rules = {}
-    for r in added_rules:
-        r_key = r.pop("key")
-        diff_rules[r_key] = r
-        if r_key in my_rules:
-            diff_rules[r_key] = rules.convert_for_export(my_rules[r_key].to_json(), my_rules[r_key].language)
-        if "severity" in r:
-            if isinstance(diff_rules[r_key], str):
-                diff_rules[r_key] = r["severity"]
-            else:
-                diff_rules[r_key]["severity"] = r["severity"]
-    return diff_rules
-
-
-def _treat_modified_rules(my_rules, modified_rules):
-    diff_rules = {}
-    for r in modified_rules:
-        r_key, r_left, r_right = r["key"], r["left"], r["right"]
-        diff_rules[r_key] = {"modified": True}
-        parms = None
-        if r_left["severity"] != r_right["severity"]:
-            diff_rules[r_key]["severity"] = r_left["severity"]
-        if len(r_left.get("params", {})) > 0:
-            diff_rules[r_key]["params"] = r_left["params"]
-            parms = r_left["params"]
-        if r_key not in my_rules:
-            continue
-        data = rules.convert_for_export(my_rules[r_key].to_json(), my_rules[r_key].language)
-        if "templateKey" in data:
-            diff_rules[r_key]["templateKey"] = data["templateKey"]
-            diff_rules[r_key]["params"] = data["params"]
-            if parms:
-                diff_rules[r_key]["params"].update(parms)
-    return diff_rules
```

## sonar/rules.py

```diff
@@ -19,14 +19,15 @@
 #
 """
 
     Abstraction of the SonarQube "rule" concept
 
 """
 import json
+from typing import Union
 from http import HTTPStatus
 from requests.exceptions import HTTPError
 import sonar.sqobject as sq
 from sonar import utilities, exceptions
 
 _OBJECTS = {}
 SEARCH_API = "rules/search"
@@ -91,14 +92,15 @@
         super().__init__(key, endpoint)
         utilities.logger.debug("Creating rule object '%s'", key)  # utilities.json_dump(data))
         self._json = data
         self.severity = data.get("severity", None)
         self.repo = data.get("repo", None)
         self.type = data.get("type", None)
         self.tags = None if len(data.get("tags", [])) == 0 else data["tags"]
+        self.systags = data.get("sysTags", [])
         self.name = data.get("name", None)
         self.language = data.get("lang", None)
         self.custom_desc = data.get("mdNote", None)
         self.created_at = data["createdAt"]
         self.is_template = data.get("isTemplate", False)
         self.template_key = data.get("templateKey", None)
         self._impacts = data.get("impacts", None)
@@ -110,24 +112,36 @@
 
     def __str__(self):
         return f"rule key '{self.key}'"
 
     def to_json(self):
         return self._json
 
+    def to_csv(self) -> list[str]:
+        tags = self.systags
+        if self.tags:
+            tags += self.tags
+        rule_type = "STANDARD"
+        if self.is_template:
+            rule_type = "TEMPLATE"
+        elif self.template_key:
+            rule_type = "INSTANTIATED"
+        return [self.key, self.language, self.repo, self.type, self.name, rule_type, ",".join(tags)]
+
     def export(self, full=False):
         return convert_for_export(self.to_json(), self.language, full=full)
 
-    def set_tags(self, tags):
+    def set_tags(self, tags: Union[str, list[str]]) -> None:
         if tags is None:
             return
         if isinstance(tags, list):
             tags = utilities.list_to_csv(tags)
         utilities.logger.debug("Settings custom tags '%s' to %s", tags, str(self))
         self.post("rules/update", params={"key": self.key, "tags": tags})
+        self.tags = tags
 
     def set_description(self, description):
         if description is None:
             return
         utilities.logger.debug("Settings custom description '%s' to %s", description, str(self))
         self.post("rules/update", params={"key": self.key, "markdown_note": description})
 
@@ -154,15 +168,20 @@
     return json.loads(endpoint.get(SEARCH_API, params={**params, "ps": 1}).text)["total"]
 
 
 def get_list(endpoint, **params):
     return search(endpoint, include_external="false", **params)
 
 
-def get_object(key, endpoint):
+def get_object(key: str, endpoint: object) -> Union[Rule, None]:
+    """Returns a Rule object from its key
+    :return: The Rule object corresponding to the input rule key, or None if not found
+    :param str key: The rule key
+    :rtype: Rule or None
+    """
     if key in _OBJECTS:
         return _OBJECTS[key]
     try:
         return Rule.get_object(key, endpoint)
     except exceptions.ObjectNotFound:
         return None
 
@@ -223,15 +242,24 @@
     if instantiated:
         rule_list["instantiated"] = export_instantiated(endpoint, full)
     if extended:
         rule_list["extended"] = export_customized(endpoint, full)
     return utilities.remove_nones(rule_list)
 
 
-def export(endpoint, instantiated=True, extended=True, standard=False, full=False):
+def export(endpoint: object, instantiated: bool = True, extended: bool = True, standard: bool = False, full: bool = False) -> dict[str:Rule]:
+    """Returns a dict of rules for export
+    :return: a dict of rule onbjects indexed with rule key
+    :param object endpoint: The SonarQube Platform object to connect to
+    :param bool instantiated: Include instantiated rules in the list
+    :param bool extended: Include extended rules in the list
+    :param bool standard: Include standard rules in the list
+    :param full standard: Include full rule information in the export
+    :rtype: dict{ruleKey: Rule}
+    """
     utilities.logger.info("Exporting rules")
     if standard:
         return export_all(endpoint, full)
     else:
         return export_needed(endpoint, instantiated, extended, full)
```

## sonar/settings.py

```diff
@@ -19,15 +19,15 @@
 #
 """
     Abstraction of the SonarQube setting concept
 """
 
 import re
 import json
-from sonar import sqobject
+from sonar import sqobject, exceptions
 import sonar.utilities as util
 
 DEVOPS_INTEGRATION = "devopsIntegration"
 GENERAL_SETTINGS = "generalSettings"
 LANGUAGES_SETTINGS = "languages"
 AUTH_SETTINGS = "authentication"
 LINTER_SETTINGS = "linters"
@@ -52,35 +52,62 @@
 COMPONENT_VISIBILITY = "visibility"
 PROJECT_DEFAULT_VISIBILITY = "projects.default.visibility"
 
 DEFAULT_SETTING = "__default__"
 
 _OBJECTS = {}
 
-_PRIVATE_SETTINGS = (
+_SQ_INTERNAL_SETTINGS = (
     "sonaranalyzer",
     "sonar.updatecenter",
     "sonar.plugins.risk.consent",
     "sonar.core.id",
     "sonar.core.startTime",
     "sonar.plsql.jdbc.driver.class",
 )
 
+_SC_INTERNAL_SETTINGS = (
+    "sonaranalyzer",
+    "sonar.updatecenter",
+    "sonar.plugins.risk.consent",
+    "sonar.core.id",
+    "sonar.core.startTime",
+    "sonar.plsql.jdbc.driver.class",
+    "sonar.dbcleaner",
+    "sonar.core.serverBaseURL",
+    "email.",
+    "sonar.builtIn",
+    "sonar.issues.defaultAssigneeLogin",
+    "sonar.filesize.limit",
+    "sonar.kubernetes.activate",
+    "sonar.lf",
+    "sonar.notifications",
+    "sonar.plugins.loadAll",
+    "sonar.plugins.loadAll",
+    "sonar.qualityProfiles.allowDisableInheritedRules",
+    "sonar.scm.disabled",
+    "sonar.technicalDebt",
+    "sonar.issue.",
+    "sonar.global",
+    "sonar.forceAuthentication",
+)
+
 _INLINE_SETTINGS = (
     r"^.*\.file\.suffixes$",
     r"^.*\.reportPaths$",
-    r"^sonar(\.[a-z]+)?\.exclusions$",
+    r"^sonar(\.[a-z]+)?\.(in|ex)clusions$",
     r"^sonar\.javascript\.(globals|environments)$",
     r"^sonar\.dbcleaner\.branchesToKeepWhenInactive$",
     r"^sonar\.rpg\.suffixes$",
     r"^sonar\.cs\.roslyn\.(bug|codeSmell|vulnerability)Categories$",
     r"^sonar\.governance\.report\.view\.recipients$",
     r"^sonar\.portfolios\.recompute\.hours$",
     r"^sonar\.cobol\.copy\.(directories|exclusions)$",
     r"^sonar\.cobol\.sql\.catalog\.defaultSchema$",
+    r"^sonar\.docker\.file\.patterns$",
 )
 
 _API_SET = "settings/set"
 _CREATE_API = "settings/set"
 _API_GET = "settings/values"
 _API_LIST = "settings/list_definitions"
 API_NEW_CODE_GET = "new_code_periods/show"
@@ -92,21 +119,27 @@
 class Setting(sqobject.SqObject):
     @classmethod
     def read(cls, key, endpoint, component=None):
         util.logger.debug("Reading setting '%s' for %s", key, str(component))
         uu = _uuid_p(key, component)
         if uu in _OBJECTS:
             return _OBJECTS[uu]
-        if key == NEW_CODE_PERIOD:
+        if key == NEW_CODE_PERIOD and not endpoint.is_sonarcloud():
             params = get_component_params(component, name="project")
             data = json.loads(endpoint.get(API_NEW_CODE_GET, params=params).text)
         else:
+            if key == NEW_CODE_PERIOD:
+                key = "sonar.leak.period.type"
             params = get_component_params(component)
             params.update({"keys": key})
-            data = json.loads(endpoint.get(_API_GET, params=params).text)["settings"][0]
+            data = json.loads(endpoint.get(_API_GET, params=params).text)["settings"]
+            if not endpoint.is_sonarcloud() and len(data) > 0:
+                data = data[0]
+            else:
+                data = {"inherited": True}
         return Setting.load(key=key, endpoint=endpoint, data=data, component=component)
 
     @classmethod
     def create(cls, key, endpoint, value=None, component=None):
         util.logger.debug("Creating setting '%s' of component '%s' value '%s'", key, str(component), str(value))
         r = endpoint.post(_CREATE_API, params={"key": key, "component": component})
         if not r.ok:
@@ -133,15 +166,15 @@
 
     def reload(self, data):
         if not data:
             return
         if self.key == NEW_CODE_PERIOD:
             self.value = new_code_to_string(data)
         elif self.key == COMPONENT_VISIBILITY:
-            self.value = data["visibility"]
+            self.value = data.get("visibility", None)
         elif self.key.startswith("sonar.issue."):
             self.value = data.get("fieldValues", None)
         else:
             self.value = util.convert_string(data.get("value", data.get("values", data.get("defaultValue", ""))))
 
         if "inherited" in data:
             self.inherited = data["inherited"]
@@ -165,16 +198,16 @@
         if self.component is None:
             return f"setting '{self.key}'"
         else:
             return f"setting '{self.key}' of {str(self.component)}"
 
     def set(self, value):
         util.logger.debug("%s set to '%s'", str(self), str(value))
-        if not is_valid(self.key, self.endpoint):
-            util.logger.error("Setting '%s' does not seem to be a valid setting, trying to set anyway...", str(self))
+        if not self.is_settable():
+            util.logger.error("Setting '%s' does not seem to be a settable setting, trying to set anyway...", str(self))
         if value is None or value == "":
             # TODO: return endpoint.reset_setting(key)
             return True
         if self.key in (COMPONENT_VISIBILITY, PROJECT_DEFAULT_VISIBILITY):
             return set_visibility(endpoint=self.endpoint, component=self.component, visibility=value)
 
         # Hack: Up to 9.4 cobol settings are comma separated mono-valued, in 9.5+ they are multi-valued
@@ -190,28 +223,59 @@
                 params["fieldValues"] = [util.json.dumps(v) for v in value]
         else:
             if isinstance(value, bool):
                 value = "true" if value else "false"
             params["value"] = value
         return self.post(_API_SET, params=params).ok
 
-    def to_json(self):
+    def to_json(self) -> dict[str, str]:
         return {self.key: encode(self.key, self.value)}
 
+    def is_global(self) -> bool:
+        """Returns whether a setting global or specific for one component (project, branch, application, portfolio)"""
+        return self.component is None
+
+    def is_internal(self) -> bool:
+        """Returns whether a setting is internal to the platform and is useless to expose externally"""
+        internal_settings = _SQ_INTERNAL_SETTINGS
+        if self.endpoint.is_sonarcloud():
+            internal_settings = _SC_INTERNAL_SETTINGS
+            if self.is_global():
+                (categ, _) = self.category()
+                if categ in ("languages", "analysisScope", "tests", "authentication"):
+                    return True
+
+        for prefix in internal_settings:
+            if self.key.startswith(prefix):
+                return True
+        return False
+
+    def is_settable(self) -> bool:
+        """Returns whether a setting can be set"""
+        if len(VALID_SETTINGS) == 0:
+            get_bulk(endpoint=self.endpoint, include_not_set=True)
+        if self.key not in VALID_SETTINGS:
+            return False
+        return not self.is_internal()
+
     def category(self):
         m = re.match(
-            r"^sonar\.(cpd\.)?(abap|apex|cloudformation|c|cpp|cfamily|cobol|cs|css|flex|go|html|java|"
-            r"javascript|json|jsp|kotlin|objc|php|pli|plsql|python|rpg|ruby|scala|swift|terraform|tsql|"
-            r"typescript|vb|vbnet|xml|yaml)\.",
+            r"^sonar\.(cpd\.)?(abap|androidLint|apex|azureresourcemanager|cloudformation|c|cpp|cfamily|cobol|cs|css|docker|"
+            r"eslint|flex|go|html|java|javascript|jcl|json|jsp|kotlin|objc|php|pli|plsql|python|rpg|ruby|scala|swift|"
+            r"terraform|text|tsql|typescript|vb|vbnet|xml|yaml)\.",
             self.key,
         )
         if m:
             lang = m.group(2)
             if lang in ("c", "cpp", "objc", "cfamily"):
                 lang = "cfamily"
+            elif lang in ("androidLint"):
+                lang = "kotlin"
+            elif lang in ("eslint"):
+                lang = "javascript"
             return (LANGUAGES_SETTINGS, lang)
         if re.match(
             r"^.*([lL]int|govet|flake8|checkstyle|pmd|spotbugs|phpstan|psalm|detekt|bandit|rubocop|scalastyle|scapegoat).*$",
             self.key,
         ):
             return (LINTER_SETTINGS, None)
         if re.match(r"^sonar\.security\.config\..+$", self.key):
@@ -226,64 +290,76 @@
             return (AUTH_SETTINGS, None)
         m = re.match(r"^sonar\.forceAuthentication$", self.key)
         if m:
             return (AUTH_SETTINGS, None)
         if self.key not in (NEW_CODE_PERIOD, PROJECT_DEFAULT_VISIBILITY, COMPONENT_VISIBILITY) and not re.match(
             r"^(email|sonar\.core|sonar\.allowPermission|sonar\.builtInQualityProfiles|sonar\.core|"
             r"sonar\.cpd|sonar\.dbcleaner|sonar\.developerAggregatedInfo|sonar\.governance|sonar\.issues|sonar\.lf|sonar\.notifications|"
-            r"sonar\.portfolios|sonar\.qualitygate|sonar\.scm\.disabled|sonar\.scm\.provider|sonar\.technicalDebt|sonar\.validateWebhooks).*$",
+            r"sonar\.portfolios|sonar\.qualitygate|sonar\.scm\.disabled|sonar\.scm\.provider|sonar\.technicalDebt|sonar\.validateWebhooks|"
+            r"sonar\.docker|sonar\.login|sonar\.kubernetes|sonar\.plugins|sonar\.documentation|sonar\.projectCreation|"
+            r"sonar\.qualityProfiles|sonar\.announcement|provisioning\.git|sonar\.ce|sonar\.azureresourcemanager|sonar\.filesize\.limit).*$",
             self.key,
         ):
             return ("thirdParty", None)
         return (GENERAL_SETTINGS, None)
 
 
 def get_object(key, component=None):
     return _OBJECTS.get(_uuid_p(key, component), None)
 
 
+def __get_settings(endpoint: object, data: dict[str, str], component: object = None) -> dict[str, Setting]:
+    """Returns settings of the global platform or a specific component object (Project, Branch, App, Portfolio)"""
+    settings = {}
+    settings_type_list = ["settings"]
+    # Hack: Sonar API also return setSecureSettings for projects although it's irrelevant
+    if component is None:
+        settings_type_list += ["setSecuredSettings"]
+
+    for setting_type in settings_type_list:
+        util.logger.debug("Looking at %s", setting_type)
+        for s in data.get(setting_type, {}):
+            (key, sdata) = (s, {}) if isinstance(s, str) else (s["key"], s)
+            o = Setting(key=key, endpoint=endpoint, component=component, data=None)
+            if o.is_internal():
+                util.logger.debug("Skipping internal setting %s", s["key"])
+                continue
+            o = Setting.load(key=key, endpoint=endpoint, component=component, data=sdata)
+            settings[o.key] = o
+    return settings
+
+
 def get_bulk(endpoint, settings_list=None, component=None, include_not_set=False):
     """Gets several settings as bulk (returns a dict)"""
     settings_dict = {}
     params = get_component_params(component)
     if include_not_set:
         data = json.loads(endpoint.get(_API_LIST, params=params).text)
         for s in data["definitions"]:
             if s["key"].endswith("coverage.reportPath") or s["key"] == "languageSpecificParameters":
                 continue
             o = Setting.load(key=s["key"], endpoint=endpoint, data=s, component=component)
             settings_dict[o.key] = o
-    if settings_list is None:
-        pass
-    elif isinstance(settings_list, list):
+
+    if settings_list is not None:
         params["keys"] = util.list_to_csv(settings_list)
-    else:
-        params["keys"] = util.csv_normalize(settings_list)
+
     data = json.loads(endpoint.get(_API_GET, params=params).text)
-    settings_type_list = ["settings"]
-    # Hack: Sonar API also return setSecureSettings for projects although it's irrelevant
-    if component is None:
-        settings_type_list = ["setSecuredSettings"]
-    settings_type_list += ["settings"]
-    for setting_type in settings_type_list:
-        util.logger.debug("Looking at %s", setting_type)
-        for s in data.get(setting_type, {}):
-            (key, sdata) = (s, {}) if isinstance(s, str) else (s["key"], s)
-            if is_private(key) > 0:
-                util.logger.debug("Skipping private setting %s", s["key"])
-                continue
-            o = Setting.load(key=key, endpoint=endpoint, component=component, data=sdata)
-            settings_dict[o.key] = o
+    settings_dict |= __get_settings(endpoint, data, component)
 
     # Hack since projects.default.visibility is not returned by settings/list_definitions
-    o = get_visibility(endpoint, component)
-    settings_dict[o.key] = o
-
-    o = get_new_code_period(endpoint, component)
-    settings_dict[o.key] = o
+    try:
+        o = get_visibility(endpoint, component)
+        settings_dict[o.key] = o
+    except exceptions.UnsupportedOperation as e:
+        util.logger.info("%s", str(e))
+
+    if not endpoint.is_sonarcloud():
+        o = get_new_code_period(endpoint, component)
+        settings_dict[o.key] = o
     VALID_SETTINGS.update(set(settings_dict.keys()))
     VALID_SETTINGS.update({"sonar.scm.provider"})
     return settings_dict
 
 
 def get_all(endpoint, project=None):
     return get_bulk(endpoint, component=project, include_not_set=True)
@@ -312,15 +388,16 @@
         return data["type"]
     elif data["type"] == "SPECIFIC_ANALYSIS":
         return f"{data['type']} = {data['effectiveValue']}"
     else:
         return f"{data['type']} = {data['value']}"
 
 
-def get_new_code_period(endpoint, project_or_branch):
+def get_new_code_period(endpoint: object, project_or_branch: object) -> Setting:
+    """returns the new code period, either the default global setting, or specific to a project/branch"""
     return Setting.read(key=NEW_CODE_PERIOD, endpoint=endpoint, component=project_or_branch)
 
 
 def string_to_new_code(value):
     return re.split(r"\s*=\s*", value)
 
 
@@ -335,14 +412,16 @@
     uu = uuid(COMPONENT_VISIBILITY, component.key) if component else uuid(PROJECT_DEFAULT_VISIBILITY)
     if uu in _OBJECTS:
         return _OBJECTS[uu]
     if component:
         data = json.loads(endpoint.get("components/show", params={"component": component.key}).text)
         return Setting.load(key=COMPONENT_VISIBILITY, endpoint=endpoint, component=component, data=data["component"])
     else:
+        if endpoint.is_sonarcloud():
+            raise exceptions.UnsupportedOperation("Project default visibility does not exist in SonarCloud")
         data = json.loads(endpoint.get(_API_GET, params={"keys": PROJECT_DEFAULT_VISIBILITY}).text)
         return Setting.load(key=PROJECT_DEFAULT_VISIBILITY, endpoint=endpoint, component=None, data=data["settings"][0])
 
 
 def set_visibility(endpoint, visibility, component=None):
     if component:
         util.logger.debug("Setting setting '%s' of %s to value '%s'", COMPONENT_VISIBILITY, str(component), visibility)
@@ -405,22 +484,7 @@
 def get_component_params(component, name="component"):
     if not component:
         return {}
     elif type(component).__name__ == "Branch":
         return {name: component.project.key, "branch": component.key}
     else:
         return {name: component.key}
-
-
-def is_valid(setting_key, endpoint):
-    if len(VALID_SETTINGS) == 0:
-        get_bulk(endpoint=endpoint, include_not_set=True)
-    if setting_key not in VALID_SETTINGS:
-        return False
-    return not is_private(setting_key)
-
-
-def is_private(setting_key):
-    for prefix in _PRIVATE_SETTINGS:
-        if setting_key.startswith(prefix):
-            return True
-    return False
```

## sonar/sif.py

```diff
@@ -21,19 +21,21 @@
 
     Abstraction of the SonarQube System Info File (or Support Info File) concept
 
 """
 
 import datetime
 import re
+from typing import Union
 from dateutil.relativedelta import relativedelta
 import sonar.utilities as util
 
-from sonar.audit import rules, types, severities
+from sonar.audit import rules
 import sonar.audit.problem as pb
+import sonar.sif_node as sifn
 
 import sonar.dce.app_nodes as appnodes
 import sonar.dce.search_nodes as searchnodes
 
 _RELEASE_DATE_6_7 = datetime.datetime(2017, 11, 8) + relativedelta(months=+6)
 _RELEASE_DATE_7_9 = datetime.datetime(2019, 7, 1) + relativedelta(months=+6)
 _RELEASE_DATE_8_9 = datetime.datetime(2021, 5, 4) + relativedelta(months=+6)
@@ -62,34 +64,40 @@
         if not is_sysinfo(json_sif):
             util.logger.critical("Provided JSON does not seem to be a system info")
             raise NotSystemInfo("JSON is not a system info nor a support info")
         self.json = json_sif
         self.concerned_object = concerned_object
         self._url = None
 
+    def __str__(self) -> str:
+        return str(self.concerned_object)
+
     def url(self):
         if not self._url:
             if self.concerned_object:
                 self._url = self.concerned_object.url
             else:
                 self._url = self.json.get("Settings", {}).get("sonar.core.serverBaseURL", "")
         return self._url
 
-    def edition(self):
-        try:
-            ed = self.json[_STATS]["edition"]
-        except KeyError:
-            try:
-                ed = self.json["License"]["edition"]
-            except KeyError:
+    def edition(self) -> Union[str, None]:
+        ed = None
+        for section in (_STATS, _SYSTEM, "License"):
+            for subsection in ("edition", "Edition"):
                 try:
-                    # FIXME: Can't get edition in SIF of SonarQube 9.7+, this is an unsolvable problem
-                    ed = self.json["edition"]
+                    ed = self.json[section][subsection]
                 except KeyError:
-                    return None
+                    pass
+        if "Application Nodes" in self.json:
+            util.logger.debug("DCE edition detected from the presence in SIF of the 'Application Nodes' key")
+            ed = "datacenter"
+        if ed is None:
+            util.logger.warning("Could not find edition in SIF")
+            return None
+
         # Old SIFs could return "Enterprise Edition"
         return util.edition_normalize(ed)
 
     def database(self):
         if self.version() < (9, 7, 0):
             return self.json[_STATS]["database"]["name"]
         else:
@@ -105,15 +113,18 @@
         if "License" not in self.json:
             return None
         elif "type" in self.json["License"]:
             return self.json["License"]["type"]
         return None
 
     def version(self, digits=3, as_string=False):
-        return util.string_to_version(self.__get_field("Version"), digits=digits, as_string=as_string)
+        try:
+            return util.string_to_version(self.json["System"]["Version"], digits=digits, as_string=as_string)
+        except KeyError:
+            return None
 
     def server_id(self):
         return self.__get_field("Server ID")
 
     def start_time(self):
         try:
             return util.string_to_date(self.json[_SETTINGS]["sonar.core.startTime"]).replace(tzinfo=None)
@@ -145,35 +156,32 @@
         problems = self.__audit_jdbc_url()
         util.logger.debug("Edition = %s", self.edition())
         if self.edition() == "datacenter":
             util.logger.info("DCE SIF audit")
             problems += self.__audit_dce_settings()
         else:
             problems += (
-                self.__audit_ce_settings()
-                + self.__audit_background_tasks()
+                sifn.audit_web(self, "Web process", self.json)
+                + sifn.audit_ce(self, "CE process", self.json)
                 + self.__audit_es_settings()
-                + self.__audit_log_level()
-                + self.__audit_version()
                 + self.__audit_branch_use()
                 + self.__audit_undetected_scm()
-                + self.__audit_web_settings(audit_settings)
             )
         return problems
 
     def __audit_branch_use(self):
         if self.edition() == "community":
             return []
         util.logger.info("Auditing usage of branch analysis")
         try:
             use_br = self.json[_STATS]["usingBranches"]
             if use_br:
                 return []
             rule = rules.get_rule(rules.RuleId.NOT_USING_BRANCH_ANALYSIS)
-            return [pb.Problem(rule.type, rule.severity, rule.msg, concerned_object=self)]
+            return [pb.Problem(broken_rule=rule, msg=rule.msg, concerned_object=self)]
         except KeyError:
             util.logger.info("Branch usage information not in SIF, ignoring audit...")
             return []
 
     def __audit_undetected_scm(self):
         util.logger.info("Auditing SCM integration")
         try:
@@ -181,15 +189,15 @@
             for scm in self.json[_STATS]["projectCountByScm"]:
                 scm_count += scm["count"]
                 if scm["scm"] == "undetected":
                     undetected_scm_count = scm["count"]
             if undetected_scm_count == 0:
                 return []
             rule = rules.get_rule(rules.RuleId.SIF_UNDETECTED_SCM)
-            return [pb.Problem(rule.type, rule.severity, rule.msg.format(undetected_scm_count), concerned_object=self)]
+            return [pb.Problem(broken_rule=rule, msg=rule.msg.format(undetected_scm_count), concerned_object=self)]
         except KeyError:
             util.logger.info("SCM information not in SIF, ignoring audit...")
             return []
 
     def __get_field(self, name, node_type=_APP_NODES):
         if _SYSTEM in self.json and name in self.json[_SYSTEM]:
             return self.json[_SYSTEM][name]
@@ -226,45 +234,47 @@
         return st_time > _MIN_DATE_LOG4SHELL
 
     def __audit_log4shell(self, jvm_settings, broken_rule):
         # If SIF is older than 2022 don't audit for log4shell to avoid noise
         if not self.__eligible_to_log4shell_check():
             return []
 
-        util.logger.debug("Auditing log4shell vulnerability fix")
+        util.logger.info("Auditing log4shell vulnerability fix")
         sq_version = self.version()
         if sq_version < (8, 9, 6) or ((9, 0, 0) <= sq_version < (9, 2, 4)):
             for s in jvm_settings.split(" "):
                 if s == "-Dlog4j2.formatMsgNoLookups=true":
                     return []
             rule = rules.get_rule(broken_rule)
-            return [pb.Problem(rule.type, rule.severity, rule.msg, concerned_object=self)]
+            return [pb.Problem(broken_rule=rule, msg=rule.msg, concerned_object=self)]
         return []
 
-    def __audit_jdbc_url(self):
+    def __audit_jdbc_url(self) -> list[pb.Problem]:
         util.logger.info("Auditing JDBC settings")
-        problems = []
         stats = self.json.get(_SETTINGS)
         if stats is None:
             util.logger.error("Can't verify Database settings in System Info File, was it corrupted or redacted ?")
-            return problems
+            return []
         jdbc_url = stats.get("sonar.jdbc.url", None)
-        util.logger.debug("JDBC URL = %s", str(jdbc_url))
         if jdbc_url is None:
             rule = rules.get_rule(rules.RuleId.SETTING_JDBC_URL_NOT_SET)
-            problems.append(pb.Problem(rule.type, rule.severity, rule.msg, concerned_object=self))
-        elif re.search(
+            return [pb.Problem(broken_rule=rule, msg=rule.msg, concerned_object=self)]
+        if re.search(
             r":(postgresql://|sqlserver://|oracle:thin:@)(localhost|127\.0+\.0+\.1)[:;/]",
             jdbc_url,
         ):
             lic = self.license_type()
             if lic == "PRODUCTION":
-                rule = rules.get_rule(rules.RuleId.SETTING_DB_ON_SAME_HOST)
-                problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format(jdbc_url), concerned_object=self))
-        return problems
+                rule = rules.get_rule(rules.RuleId.DB_ON_SAME_HOST)
+                return [pb.Problem(broken_rule=rule, msg=rule.msg.format(jdbc_url), concerned_object=self)]
+            else:
+                util.logger.info("JDBC URL %s is on localhost but this is not a production license. So be it!", jdbc_url)
+        else:
+            util.logger.info("JDBC URL %s does not use localhost, all good!", jdbc_url)
+        return []
 
     def __audit_dce_settings(self):
         util.logger.info("Auditing DCE settings for version %s", str(self.version()))
         problems = []
         sq_edition = self.edition()
         if sq_edition is None:
             util.logger.error("Can't verify edition in System Info File (2_), was it corrupted or redacted ?")
@@ -279,183 +289,35 @@
 
         if _ES_NODES in self.json:
             problems += searchnodes.audit(self.json[_ES_NODES], self)
         else:
             util.logger.info("Sys Info too old (pre-8.9), can't check plugins")
         return problems
 
-    def __audit_log_level(self):
-        util.logger.debug("Auditing log levels")
-        log_level = self.__get_field("Web Logging")
-        if log_level is None:
-            return []
-        log_level = log_level["Logs Level"]
-        if log_level not in ("DEBUG", "TRACE"):
-            return []
-        if log_level == "TRACE":
-            return [
-                pb.Problem(
-                    types.Type.PERFORMANCE,
-                    severities.Severity.CRITICAL,
-                    "Log level set to TRACE, this does very negatively affect platform performance, reverting to INFO is required",
-                    concerned_object=self,
-                )
-            ]
-        if log_level == "DEBUG":
-            return [
-                pb.Problem(
-                    types.Type.PERFORMANCE,
-                    severities.Severity.HIGH,
-                    "Log level is set to DEBUG, this may affect platform performance, reverting to INFO is recommended",
-                    concerned_object=self,
-                )
-            ]
-        return []
-
-    def __audit_version(self):
-        st_time = self.start_time()
-        if st_time is None:
-            util.logger.warning("SIF date is not available, skipping audit on SonarQube version (aligned with LTS)...")
-            return []
-        sq_version = self.version()
-        if (
-            (st_time > _RELEASE_DATE_6_7 and sq_version < (6, 7, 0))
-            or (st_time > _RELEASE_DATE_7_9 and sq_version < (7, 9, 0))
-            or (st_time > _RELEASE_DATE_8_9 and sq_version < (8, 9, 0))
-        ):
-            rule = rules.get_rule(rules.RuleId.BELOW_LTS)
-            return [pb.Problem(rule.type, rule.severity, rule.msg, concerned_object=self)]
-        return []
-
-    def __audit_web_settings(self, audit_settings):
-        util.logger.debug("Auditing Web settings")
-        problems = []
-        if self.version() < (9, 0, 0):
-            jvm_cmdline = self.web_jvm_cmdline()
-            if jvm_cmdline is None:
-                util.logger.warning("Can't retrieve web JVM command line, skipping heap and log4shell audits...")
-                return []
-            problems += self.__audit_log4shell(jvm_cmdline, rules.RuleId.LOG4SHELL_WEB)
-            heap_size = util.jvm_heap(jvm_cmdline)
-        else:
-            try:
-                heap_size = self.json["Web JVM State"]["Heap Max (MB)"]
-            except KeyError:
-                util.logger.warning("Can't retrieve web JVM heap")
-                return []
-        min_heap = audit_settings.get("audit.web.heapMin", 1024)
-        max_heap = audit_settings.get("audit.web.heapMax", 2048)
-        if heap_size is None:
-            rule = rules.get_rule(rules.RuleId.SETTING_WEB_NO_HEAP)
-            problems.append(pb.Problem(rule.type, rule.severity, rule.msg, concerned_object=self))
-        elif heap_size < min_heap or heap_size > max_heap:
-            rule = rules.get_rule(rules.RuleId.SETTING_WEB_HEAP)
-            problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format(heap_size, min_heap, max_heap), concerned_object=self))
-        else:
-            util.logger.debug("Sonar Web process heap value is %d MB, within the recommended range [%d-%d]", heap_size, min_heap, max_heap)
-
-        return problems
-
-    def __audit_ce_settings(self):
-        util.logger.info("Auditing CE settings")
-        problems = []
-        if self.version() < (9, 0, 0):
-            jvm_cmdline = self.ce_jvm_cmdline()
-            if jvm_cmdline is None:
-                util.logger.warning("Can't retrieve CE JVM command line, heap and logshell checks skipped")
-                return []
-            problems += self.__audit_log4shell(jvm_cmdline, rules.RuleId.LOG4SHELL_CE)
-            ce_ram = util.jvm_heap(jvm_cmdline)
-        else:
-            try:
-                ce_ram = self.json["Compute Engine JVM State"]["Heap Max (MB)"]
-            except KeyError:
-                util.logger.warning("Can't retrieve CE JVM heap")
-                return []
-        ce_tasks = self.__get_field("Compute Engine Tasks")
-        if ce_tasks is None:
-            return []
-        ce_workers = ce_tasks["Worker Count"]
-        MAX_WORKERS = 4
-        if ce_workers > MAX_WORKERS:
-            rule = rules.get_rule(rules.RuleId.SETTING_CE_TOO_MANY_WORKERS)
-            problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format(ce_workers, MAX_WORKERS), concerned_object=self))
-        else:
-            util.logger.debug(
-                "%d CE workers configured, correct compared to the max %d recommended",
-                ce_workers,
-                MAX_WORKERS,
-            )
-
-        if ce_ram is None:
-            rule = rules.get_rule(rules.RuleId.SETTING_CE_NO_HEAP)
-            problems.append(pb.Problem(rule.type, rule.severity, rule.msg, concerned_object=self))
-        elif ce_ram < 512 * ce_workers or ce_ram > 2048 * ce_workers:
-            rule = rules.get_rule(rules.RuleId.SETTING_CE_HEAP)
-            problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format(ce_ram, 512, 2048, ce_workers), concerned_object=self))
-        else:
-            util.logger.debug(
-                "SonarQube CE memory setting value is %d MB, within recommended range ([512-2048] x %d workers)",
-                ce_ram,
-                ce_workers,
-            )
-
-        return problems
-
-    def __audit_background_tasks(self):
-        util.logger.debug("Auditing CE background tasks")
-        problems = []
-        ce_tasks = self.__get_field("Compute Engine Tasks")
-        if ce_tasks is None:
-            return []
-        ce_success = ce_tasks["Processed With Success"]
-        ce_error = ce_tasks["Processed With Error"]
-        if ce_success == 0 and ce_error == 0:
-            failure_rate = 0
-        else:
-            failure_rate = ce_error / (ce_success + ce_error)
-        if ce_error > 10 and failure_rate > 0.01:
-            rule = rules.get_rule(rules.RuleId.BACKGROUND_TASKS_FAILURE_RATE_HIGH)
-            problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format(int(failure_rate * 100)), concerned_object=self))
-        else:
-            util.logger.debug(
-                "Number of failed background tasks (%d), and failure rate %d%% is OK",
-                ce_error,
-                int(failure_rate * 100),
-            )
-
-        ce_pending = ce_tasks["Pending"]
-        if ce_pending > 100:
-            rule = rules.get_rule(rules.RuleId.BACKGROUND_TASKS_PENDING_QUEUE_VERY_LONG)
-            problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format(ce_pending), concerned_object=self))
-        elif ce_pending > 20 and ce_pending > (10 * ce_tasks["Worker Count"]):
-            rule = rules.get_rule(rules.RuleId.BACKGROUND_TASKS_PENDING_QUEUE_LONG)
-            problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format(ce_pending), concerned_object=self))
-        else:
-            util.logger.debug("Number of pending background tasks (%d) is OK", ce_pending)
-        return problems
-
     def __audit_es_settings(self):
         util.logger.info("Auditing Search Server settings")
         problems = []
         jvm_cmdline = self.search_jvm_cmdline()
         if jvm_cmdline is None:
             util.logger.warning("Can't retrieve search JVM command line, heap and logshell checks skipped")
             return []
         es_ram = util.jvm_heap(jvm_cmdline)
         index_size = self.store_size()
 
         if index_size is None:
             util.logger.warning("Search server index size is missing. Audit of ES heap vs index size is skipped...")
         elif es_ram is None:
             rule = rules.get_rule(rules.RuleId.SETTING_ES_NO_HEAP)
-            problems.append(pb.Problem(rule.type, rule.severity, rule.msg, concerned_object=self))
+            problems.append(pb.Problem(broken_rule=rule, msg=rule.msg, concerned_object=self))
         elif es_ram < 2 * index_size and es_ram < index_size + 1000:
-            rule = rules.get_rule(rules.RuleId.SETTING_ES_HEAP)
-            problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format(es_ram, index_size), concerned_object=self))
+            rule = rules.get_rule(rules.RuleId.ES_HEAP_TOO_LOW)
+            problems.append(pb.Problem(broken_rule=rule, msg=rule.msg.format("ES", es_ram, index_size), concerned_object=self))
+        elif es_ram > 32 * 1024:
+            rule = rules.get_rule(rules.RuleId.ES_HEAP_TOO_HIGH)
+            problems.append(pb.Problem(broken_rule=rule, msg=rule.msg.format("ES", es_ram, 32 * 1024), concerned_object=self))
         else:
             util.logger.debug(
                 "Search server memory %d MB is correct wrt to index size of %d MB",
                 es_ram,
                 index_size,
             )
         problems += self.__audit_log4shell(jvm_cmdline, rules.RuleId.LOG4SHELL_ES)
```

## sonar/tasks.py

```diff
@@ -37,27 +37,32 @@
 STATUSES = (SUCCESS, PENDING, IN_PROGRESS, FAILED, CANCELED)
 
 __SUSPICIOUS_EXCLUSIONS = None
 __SUSPICIOUS_EXCEPTIONS = None
 
 SCANNER_VERSIONS = {
     "ScannerCLI": {
+        "5.0.1": datetime.datetime(2023, 8, 4),
+        "5.0.0": datetime.datetime(2023, 7, 31),
+        "4.8.1": datetime.datetime(2023, 8, 14),
         "4.8.0": datetime.datetime(2022, 2, 6),
         "4.7.0": datetime.datetime(2022, 2, 2),
         "4.6.2": datetime.datetime(2021, 5, 7),
         "4.6.1": datetime.datetime(2021, 4, 30),
         "4.6.0": datetime.datetime(2021, 1, 13),
         "4.5.0": datetime.datetime(2020, 10, 5),
         "4.4.0": datetime.datetime(2020, 7, 3),
         "4.3.0": datetime.datetime(2019, 3, 9),
         "4.2.0": datetime.datetime(2019, 10, 1),
         "4.1.0": datetime.datetime(2019, 9, 9),
     },
     "ScannerMaven": {
-        "3.9.1": datetime.datetime(2021, 11, 1),
+        "3.11.0": datetime.datetime(2024, 3, 13),
+        "3.10.0": datetime.datetime(2023, 9, 15),
+        "3.9.1": datetime.datetime(2021, 12, 1),
         "3.9.0": datetime.datetime(2021, 4, 1),
         "3.8.0": datetime.datetime(2021, 1, 1),
         "3.7.0": datetime.datetime(2019, 10, 1),
         "3.6.1": datetime.datetime(2019, 8, 1),
         "3.6.0": datetime.datetime(2019, 1, 1),
         "3.5.0": datetime.datetime(2018, 9, 1),
         "3.4.1": datetime.datetime(2018, 6, 1),
@@ -67,25 +72,36 @@
         "3.1.1": datetime.datetime(2016, 9, 1),
         "3.1.0": datetime.datetime(2016, 9, 1),
         "3.0.2": datetime.datetime(2016, 4, 1),
         "3.0.1": datetime.datetime(2016, 1, 1),
         "3.0.0": datetime.datetime(2016, 1, 1),
     },
     "ScannerGradle": {
+        "5.0.0": datetime.datetime(2024, 3, 26),
+        "4.4.1": datetime.datetime(2023, 10, 3),
+        "4.3.1": datetime.datetime(2023, 9, 1),
+        "4.2.1": datetime.datetime(2023, 6, 12),
         "4.0.0": datetime.datetime(2023, 2, 17),
         "3.5.0": datetime.datetime(2022, 10, 27),
         "3.4.0": datetime.datetime(2022, 6, 8),
         "3.3.0": datetime.datetime(2021, 6, 10),
         "3.2.0": datetime.datetime(2021, 4, 30),
         "3.1.1": datetime.datetime(2021, 1, 25),
         "3.1.0": datetime.datetime(2021, 1, 13),
         "3.0.0": datetime.datetime(2020, 6, 20),
         "2.8.0": datetime.datetime(2019, 10, 1),
     },
     "ScannerMSBuild": {
+        "6.2.0": datetime.datetime(2024, 2, 16),
+        "6.1.0": datetime.datetime(2024, 1, 29),
+        "6.0.0": datetime.datetime(2023, 12, 4),
+        "5.15.1": datetime.datetime(2024, 3, 26),
+        "5.15.0": datetime.datetime(2023, 11, 20),
+        "5.14.0": datetime.datetime(2023, 10, 2),
+        "5.13.1": datetime.datetime(2023, 8, 14),
         "5.13.0": datetime.datetime(2023, 4, 5),
         "5.12.0": datetime.datetime(2023, 3, 17),
         "5.11.0": datetime.datetime(2023, 1, 27),
         "5.10.0": datetime.datetime(2023, 1, 13),
         "5.9.2": datetime.datetime(2022, 12, 14),
         "5.9.1": datetime.datetime(2022, 12, 6),
         "5.9.0": datetime.datetime(2022, 12, 1),
@@ -112,14 +128,19 @@
         "4.10.0": datetime.datetime(2020, 6, 29),
         "4.9.0": datetime.datetime(2020, 5, 5),
         "4.8.0": datetime.datetime(2019, 11, 6),
         "4.7.1": datetime.datetime(2019, 9, 10),
         "4.7.0": datetime.datetime(2019, 9, 3),
     },
     "ScannerNpm": {
+        "3.5.0": datetime.datetime(2024, 5, 1),
+        "3.4.0": datetime.datetime(2024, 3, 27),
+        "3.3.0": datetime.datetime(2023, 11, 16),
+        "3.2.0": datetime.datetime(2023, 11, 6),
+        "3.1.0": datetime.datetime(2023, 8, 31),
         "3.0.1": datetime.datetime(2023, 2, 10),
         "3.0.0": datetime.datetime(2023, 1, 4),
         "2.9.0": datetime.datetime(2022, 12, 4),
         "2.8.9": datetime.datetime(2022, 12, 4),
         "2.8.2": datetime.datetime(2022, 9, 25),
         "2.8.1": datetime.datetime(2021, 6, 10),
         "2.8.0": datetime.datetime(2020, 11, 10),
@@ -332,59 +353,57 @@
                 if re.search(rf"{exception}", exclusion_pattern):
                     util.logger.debug("Exclusion %s matches exception %s, no audit problem will be raised", exclusion_pattern, exception)
                     is_exception = True
                     break
             if not is_exception:
                 rule = rules.get_rule(rules.RuleId.PROJ_SUSPICIOUS_EXCLUSION)
                 msg = rule.msg.format(str(self.concerned_object), exclusion_pattern)
-                problems.append(problem.Problem(rule.type, rule.severity, msg, concerned_object=self.concerned_object))
+                problems.append(problem.Problem(broken_rule=rule, msg=msg, concerned_object=self.concerned_object))
                 break  # Report only on the 1st suspicious match
         return problems
 
     def __audit_disabled_scm(self, audit_settings, scan_context):
         if not audit_settings.get("audit.project.scm.disabled", True):
             util.logger.info("Auditing disabled SCM integration is turned off, skipping...")
             return []
 
         if scan_context.get("sonar.scm.disabled", "false") == "false":
             return []
         rule = rules.get_rule(rules.RuleId.PROJ_SCM_DISABLED)
-        return [problem.Problem(rule.type, rule.severity, rule.msg.format(str(self.concerned_object)), concerned_object=self)]
+        return [problem.Problem(broken_rule=rule, msg=rule.msg.format(str(self.concerned_object)), concerned_object=self)]
 
     def __audit_warnings(self, audit_settings):
         if not audit_settings.get("audit.projects.analysisWarnings", True):
             util.logger.info("Project analysis warnings auditing disabled, skipping...")
             return []
         pbs = []
         warnings = self.warnings()
         warnings_left = []
         for w in warnings:
             if w.find("SCM provider autodetection failed") >= 0:
                 rule = rules.get_rule(rules.RuleId.PROJ_SCM_UNDETECTED)
-                pbs.append(
-                    problem.Problem(rule.type, rule.severity, rule.msg.format(str(self.concerned_object)), concerned_object=self.concerned_object)
-                )
+                pbs.append(problem.Problem(broken_rule=rule, msg=rule.msg.format(str(self.concerned_object)), concerned_object=self.concerned_object))
             else:
                 warnings_left.append(w)
         if len(warnings_left) > 0:
             rule = rules.get_rule(rules.RuleId.PROJ_ANALYSIS_WARNING)
             msg = rule.msg.format(str(self.concerned_object), " --- ".join(warnings_left))
-            pbs.append(problem.Problem(rule.type, rule.severity, msg, concerned_object=self))
+            pbs.append(problem.Problem(broken_rule=rule, msg=msg, concerned_object=self))
         return pbs
 
     def __audit_failed_task(self, audit_settings):
         if not audit_settings.get("audit.projects.failedTasks", True):
             util.logger.debug("Project failed background tasks auditing disabled, skipping...")
             return []
         if self._json["status"] != "FAILED":
             util.logger.debug("Last bg task of %s has status %s...", str(self.concerned_object), self._json["status"])
             return []
         rule = rules.get_rule(rules.RuleId.BG_TASK_FAILED)
         msg = rule.msg.format(str(self.concerned_object))
-        return [problem.Problem(rule.type, rule.severity, msg, concerned_object=self)]
+        return [problem.Problem(broken_rule=rule, msg=msg, concerned_object=self)]
 
     def __audit_scanner_version(self, audit_settings):
         if not self.has_scanner_context():
             return []
         context = self.scanner_context()
         scanner_type = context.get("sonar.scanner.app", None)
         scanner_version = context.get("sonar.scanner.appVersion", None)
@@ -403,15 +422,15 @@
                 scanner_type,
             )
             return []
 
         if scanner_type == "Ant":
             rule = rules.get_rule(rules.RuleId.ANT_SCANNER_DEPRECATED)
             msg = rule.msg.format(str(self.concerned_object))
-            return [problem.Problem(rule.type, rule.severity, msg, concerned_object=self.concerned_object)]
+            return [problem.Problem(broken_rule=rule, msg=msg, concerned_object=self.concerned_object)]
 
         if scanner_type in ("ScannerGradle", "ScannerMaven"):
             (scanner_version, build_tool_version) = scanner_version.split("/")
             scanner_version = scanner_version.replace("-SNAPSHOT", "")
         scanner_version = [int(n) for n in scanner_version.split(".")]
         if len(scanner_version) == 2:
             scanner_version.append(0)
@@ -438,37 +457,37 @@
 
         delta_days = (datetime.datetime.today() - release_date).days
         index = tuple_version_list.index(scanner_version)
         util.logger.debug("Scanner used is %d versions old", index)
         if delta_days > audit_settings.get("audit.projects.scannerMaxAge", 730):
             rule = rules.get_rule(rules.RuleId.OBSOLETE_SCANNER) if index >= 3 else rules.get_rule(rules.RuleId.NOT_LATEST_SCANNER)
             msg = rule.msg.format(str(self.concerned_object), scanner_type, str_version, util.date_to_string(release_date, with_time=False))
-            return [problem.Problem(rule.type, rule.severity, msg, concerned_object=self.concerned_object)]
+            return [problem.Problem(broken_rule=rule, msg=msg, concerned_object=self.concerned_object)]
         return []
 
     def audit(self, audit_settings):
         """
         :meta private:
         """
         if not audit_settings.get("audit.projects.exclusions", True):
             util.logger.debug("Project exclusions auditing disabled, skipping...")
             return []
         util.logger.debug("Auditing %s", str(self))
         problems = []
         if self.has_scanner_context():
-            problems = []
-            context = self.scanner_context()
-            susp_exclusions = _get_suspicious_exclusions(audit_settings.get("audit.projects.suspiciousExclusionsPatterns", ""))
-            susp_exceptions = _get_suspicious_exceptions(audit_settings.get("audit.projects.suspiciousExclusionsExceptions", ""))
-            for prop in ("sonar.exclusions", "sonar.global.exclusions"):
-                if context.get(prop, None) is None:
-                    continue
-                for excl in util.csv_to_list(context[prop]):
-                    util.logger.debug("Pattern = '%s'", excl)
-                    problems += self.__audit_exclusions(excl, susp_exclusions, susp_exceptions)
+            if audit_settings.get("audit.projects.exclusions", True):
+                context = self.scanner_context()
+                susp_exclusions = _get_suspicious_exclusions(audit_settings.get("audit.projects.suspiciousExclusionsPatterns", ""))
+                susp_exceptions = _get_suspicious_exceptions(audit_settings.get("audit.projects.suspiciousExclusionsExceptions", ""))
+                for prop in ("sonar.exclusions", "sonar.global.exclusions"):
+                    if context.get(prop, None) is None:
+                        continue
+                    for excl in util.csv_to_list(context[prop]):
+                        util.logger.debug("Pattern = '%s'", excl)
+                        problems += self.__audit_exclusions(excl, susp_exclusions, susp_exceptions)
             problems += self.__audit_disabled_scm(audit_settings, context)
         elif type(self.concerned_object).__name__ == "Project":
             util.logger.debug("Last background task of %s has no scanner context, can't audit it", str(self.concerned_object))
 
         problems += self.__audit_warnings(audit_settings)
         problems += self.__audit_failed_task(audit_settings)
         problems += self.__audit_scanner_version(audit_settings)
```

## sonar/users.py

```diff
@@ -15,26 +15,29 @@
 #
 # You should have received a copy of the GNU Lesser General Public License
 # along with this program; if not, write to the Free Software Foundation,
 # Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
 #
 
 import datetime as dt
+import json
 from sonar import groups, sqobject, tokens, exceptions
 import sonar.utilities as util
 from sonar.audit import rules, problem
 
 
 _OBJECTS = {}
 
-SEARCH_API = "users/search"
+_SEARCH_API_SQ = "users/search"
+_SEARCH_API_SC = "organizations/search_members"
 CREATE_API = "users/create"
 UPDATE_API = "users/update"
 DEACTIVATE_API = "users/deactivate"
 UPDATE_LOGIN_API = "users/update_login"
+_GROUPS_API_SC = "users/groups"
 
 SETTABLE_PROPERTIES = ("login", "name", "scmAccounts", "email", "groups", "local")
 
 
 class User(sqobject.SqObject):
     """
     Abstraction of the SonarQube "user" concept
@@ -42,15 +45,15 @@
     """
 
     def __init__(self, login, endpoint, data):
         """Do not use to create users, use on of the constructor class methods"""
         super().__init__(login, endpoint)
         self.login = login  #: User login (str)
         self.name = None  #: User name (str)
-        self.groups = None  #: User groups (list)
+        self._groups = None  #: User groups (list)
         self.scm_accounts = None  #: User SCM accounts (list)
         self.email = None  #: User email (str)
         self.is_local = None  #: Whether user is local (bool) - read-only
         self.last_login = None  #: User last login (datetime) - read-only
         self.nb_tokens = None  #: Nbr of tokens (int) - read-only
         self.__tokens = None
         self.__load(data)
@@ -116,29 +119,43 @@
         :return: String formatting of the object
         :rtype: str
         """
         return f"user '{self.login}'"
 
     def __load(self, data):
         self.name = data["name"]  #: User name
-        self.groups = data.get("groups", [])  #: User groups
         self.scm_accounts = data.pop("scmAccounts", None)  #: User SCM accounts
         self.email = data.get("email", None)  #: User email
         self.is_local = data.get("local", False)  #: User is local - read-only
         self.last_login = util.string_to_date(data.get("lastConnectionDate", None))  #: User last login - read-only
         self.nb_tokens = data.get("tokenCount", None)  #: Nbr of tokens - read-only
         self.__tokens = None
+        self._groups = self.groups(data)  #: User groups
         self._json = data
 
+    def groups(self, data: dict[str, str] = None) -> list[str]:
+        """Returns the list of groups of a user"""
+        if self._groups is not None:
+            return self._groups
+        if self.endpoint.is_sonarcloud():
+            data = json.loads(self.get(_GROUPS_API_SC, {"login": self.key}).text)["groups"]
+            self._groups = [g["name"] for g in data]
+        else:
+            self._groups = data.get("groups", [])  #: User groups
+        return self._groups
+
     def refresh(self):
         """Refreshes a User object from SonarQube data
 
         :return:  Nothing
         """
-        data = self.get(SEARCH_API, params={"q": self.login})
+        api = _SEARCH_API_SQ
+        if self.endpoint.is_sonarcloud():
+            api = _SEARCH_API_SC
+        data = self.get(api, params={"q": self.login})
         for d in data["users"]:
             if d["login"] == self.login:
                 self.__load(d)
                 break
 
     def url(self):
         """
@@ -234,22 +251,22 @@
 
         :param group_list: List of groups to set membership
         :type group_list: list[str]
         :return: Whether all group membership were OK
         :rtype: bool
         """
         ok = True
-        for g in list(set(group_list) - set(self.groups)):
+        for g in list(set(group_list) - set(self.groups())):
             if g != "sonar-users":
                 ok = ok and self.add_to_group(g)
-        for g in list(set(self.groups) - set(group_list)):
+        for g in list(set(self.groups()) - set(group_list)):
             if g != "sonar-users":
                 ok = ok and self.remove_from_group(g)
         if ok:
-            self.groups = group_list
+            self._groups = group_list
         else:
             self.refresh()
         return ok
 
     def add_scm_accounts(self, accounts_list):
         """Adds SCM accounts to the user (on top of existing ones)
 
@@ -299,64 +316,73 @@
         today = dt.datetime.today().replace(tzinfo=dt.timezone.utc)
         problems = []
         for t in self.tokens():
             age = abs((today - t.created_at).days)
             if age > settings.get("audit.tokens.maxAge", 90):
                 rule = rules.get_rule(rules.RuleId.TOKEN_TOO_OLD)
                 msg = rule.msg.format(str(t), age)
-                problems.append(problem.Problem(rule.type, rule.severity, msg, concerned_object=self))
+                problems.append(problem.Problem(broken_rule=rule, msg=msg, concerned_object=t))
             if t.last_connection_date is None and age > settings.get("audit.tokens.maxUnusedAge", 30):
                 rule = rules.get_rule(rules.RuleId.TOKEN_NEVER_USED)
                 msg = rule.msg.format(str(t), age)
-                problems.append(problem.Problem(rule.type, rule.severity, msg, concerned_object=self))
+                problems.append(problem.Problem(broken_rule=rule, msg=msg, concerned_object=t))
             if t.last_connection_date is None:
                 continue
             last_cnx_age = abs((today - t.last_connection_date).days)
             if last_cnx_age > settings.get("audit.tokens.maxUnusedAge", 30):
                 rule = rules.get_rule(rules.RuleId.TOKEN_UNUSED)
                 msg = rule.msg.format(str(t), last_cnx_age)
-                problems.append(problem.Problem(rule.type, rule.severity, msg, concerned_object=self))
+                problems.append(problem.Problem(broken_rule=rule, msg=msg, concerned_object=t))
 
         if self.last_login:
             age = abs((today - self.last_login).days)
             if age > settings.get("audit.users.maxLoginAge", 180):
                 rule = rules.get_rule(rules.RuleId.USER_UNUSED)
                 msg = rule.msg.format(str(self), age)
-                problems.append(problem.Problem(rule.type, rule.severity, msg, concerned_object=self))
+                problems.append(problem.Problem(broken_rule=rule, msg=msg, concerned_object=self))
         return problems
 
     def to_json(self, full=False):
         """Exports the user data (login, email, groups, SCM accounts local or not) as dict
 
         :return: User data
         :rtype: dict
         """
         json_data = self._json.copy()
         scm = self.scm_accounts
         json_data["scmAccounts"] = util.list_to_csv(scm) if scm else None
-        my_groups = self.groups.copy()
-        my_groups.remove("sonar-users")
+        my_groups = self.groups().copy()
+        if "sonar-users" in my_groups:
+            my_groups.remove("sonar-users")
         json_data["groups"] = util.list_to_csv(my_groups, ", ", True)
-        if not full and not json_data["local"]:
+        if not self.endpoint.is_sonarcloud() and not full and not json_data["local"]:
             json_data.pop("local")
         return util.remove_nones(util.filter_export(json_data, SETTABLE_PROPERTIES, full))
 
 
-def search(endpoint, params=None):
-    """Searches users in SonarQube
+def search(endpoint: object, params: dict[str, str] = None) -> dict[str, object]:
+    """Searches users in SonarQube or SonarCloud
 
     :param endpoint: Reference to the SonarQube platform
     :type endpoint: Platform
     :param params: list of parameters to narrow down the search
     :type params: dict
     :return: list of projects
     :rtype: dict{login: User}
     """
     util.logger.debug("Searching users with params %s", str(params))
-    return sqobject.search_objects(api=SEARCH_API, params=params, returned_field="users", key_field="login", object_class=User, endpoint=endpoint)
+    api = _SEARCH_API_SQ
+    if endpoint.is_sonarcloud():
+        api = _SEARCH_API_SC
+        if params is None:
+            params = {"organization": endpoint.organization}
+        else:
+            params["organization"] = endpoint.organization
+
+    return sqobject.search_objects(api=api, params=params, returned_field="users", key_field="login", object_class=User, endpoint=endpoint)
 
 
 def export(endpoint, full=False):
     """Exports all users as dict
 
     :param endpoint: reference to the SonarQube platform
     :type endpoint: Platform
```

## sonar/utilities.py

```diff
@@ -15,31 +15,36 @@
 #
 # You should have received a copy of the GNU Lesser General Public License
 # along with this program; if not, write to the Free Software Foundation,
 # Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
 #
 """
 
-    Utilities for SonarQube API
+    Utilities for sonar-tools
 
 """
 from http import HTTPStatus
 import sys
 import os
 import contextlib
 import re
 import logging
 import argparse
 import json
 import datetime
+from datetime import timezone
+import random
 from typing import Union
-import pytz
-from sonar import options
+import requests
+from sonar import options, version
 
+OPT_URL = "url"
 OPT_VERBOSE = "verbosity"
+OPT_SKIP_VERSION_CHECK = "skipVersionCheck"
+OPT_ORGANIZATION = "organization"
 OPT_MODE = "mode"
 DRY_RUN = "dryrun"
 CONFIRM = "confirm"
 BATCH = "batch"
 RUN_MODE = DRY_RUN
 ISO_DATE_FORMAT = "%04d-%02d-%02d"
 SQ_DATETIME_FORMAT = "%Y-%m-%dT%H:%M:%S%z"
@@ -78,35 +83,54 @@
         required=False,
         default=os.getenv("SONAR_TOKEN", None),
         help="""Token to authenticate to the source SonarQube, default is environment variable $SONAR_TOKEN
         - Unauthenticated usage is not possible""",
     )
     parser.add_argument(
         "-u",
-        "--url",
+        f"--{OPT_URL}",
         required=False,
         default=os.getenv("SONAR_HOST_URL", "http://localhost:9000"),
-        help="""Root URL of the source SonarQube server,
+        help="""Root URL of the source SonarQube or SonarCloud server,
         default is environment variable $SONAR_HOST_URL or http://localhost:9000 if not set""",
     )
     parser.add_argument(
+        "-o",
+        f"--{OPT_ORGANIZATION}",
+        required=False,
+        help="SonarCloud organization when using sonar-tools with SonarCloud",
+    )
+    parser.add_argument(
         "-v",
         "--" + OPT_VERBOSE,
         required=False,
         choices=["WARN", "INFO", "DEBUG"],
         default="INFO",
         help="Logging verbosity level",
     )
     parser.add_argument(
         "-c",
         "--clientCert",
         required=False,
         default=None,
         help="Optional client certificate file (as .pem file)",
     )
+    parser.add_argument(
+        "--httpTimeout",
+        required=False,
+        default=10,
+        help="HTTP timeout for requests to SonarQube, 10s by default",
+    )
+    parser.add_argument(
+        f"--{OPT_SKIP_VERSION_CHECK}",
+        required=False,
+        default=False,
+        action="store_true",
+        help="Prevents sonar-tools to occasionnally check from more recent version",
+    )
     return parser
 
 
 def set_key_arg(parser):
     parser.add_argument(
         "-k",
         "--projectKeys",
@@ -130,26 +154,40 @@
         "--tokenTarget",
         required=False,
         help="Token to authenticate to target SonarQube - Unauthenticated usage is not possible",
     )
     return parser
 
 
-def set_output_file_args(parser, json_fmt=True, csv_fmt=True):
+def set_output_file_args(parser, json_fmt: bool = True, csv_fmt: bool = True, sarif_fmt: bool = False):
     parser.add_argument(
         "-f",
         "--file",
         required=False,
         help="Output file for the report, stdout by default",
     )
+    fmt_choice = []
+    default_format = None
+    if csv_fmt:
+        fmt_choice.append("csv")
+        default_format = "csv"
+    if json_fmt:
+        fmt_choice.append("json")
+        if default_format is None:
+            default_format = "json"
+    if sarif_fmt:
+        fmt_choice.append("sarif")
+        if default_format is None:
+            default_format = "sarif"
     if json_fmt and csv_fmt:
         parser.add_argument(
             "--" + options.FORMAT,
-            choices=["csv", "json"],
+            choices=fmt_choice,
             required=False,
+            default=default_format,
             help="Output format for generated report.\nIf not specified, it is the output file extension if json or csv, then csv by default",
         )
     if csv_fmt:
         parser.add_argument(
             "--" + options.CSV_SEPARATOR,
             required=False,
             default=CSV_SEPARATOR,
@@ -185,39 +223,61 @@
 
 
 def set_debug_level(level):
     logger.setLevel(get_logging_level(level))
     logger.info("Set debug level to %s", level)
 
 
-def check_environment(kwargs):
-    set_debug_level(kwargs.pop(OPT_VERBOSE))
+def check_last_sonar_tools_version() -> None:
+    """Checks last version of sonar-tools on pypi and displays a warning if the currently used version is older"""
+    logger.info("Checking latest sonar-version on pypi.org")
+    try:
+        r = requests.get(url="https://pypi.org/simple/sonar-tools", headers={"Accept": "application/vnd.pypi.simple.v1+json"}, timeout=10)
+        r.raise_for_status()
+    except (requests.RequestException, requests.exceptions.HTTPError, requests.exceptions.Timeout) as e:
+        logger.info("Can't access pypi.org, error %s", str(e))
+    txt_version = json.loads(r.text)["versions"][-1]
+    logger.info("Latest sonar-tools version is %s", txt_version)
+    if tuple(".".split(txt_version)) > tuple(".".split(version.PACKAGE_VERSION)):
+        logger.warning("A more recent version of sonar-tools (%s) is available, your are advised to upgrade", txt_version)
 
 
-def parse_and_check_token(parser):
+def parse_and_check(parser: argparse.ArgumentParser, verify_token: bool = True) -> object:
+    """Parses arguments and perform common environment checks"""
     args = parser.parse_args()
-    if args.token is None:
-        exit_fatal(
-            "Token is missing (Argument -t/--token)",
-            options.ERR_SONAR_API_AUTHENTICATION,
-        )
+    kwargs = vars(args)
+    set_debug_level(kwargs[OPT_VERBOSE])
+    logger.info("sonar-tools version %s", version.PACKAGE_VERSION)
+
+    # Verify version randomly once every 10 runs
+    if not kwargs[OPT_SKIP_VERSION_CHECK] and random.randrange(10) == 0:
+        check_last_sonar_tools_version()
+
+    if verify_token:
+        check_token(args.token, is_sonarcloud_url(kwargs[OPT_URL]))
     return args
 
 
 def token_type(token):
     if token[0:4] == "sqa_":
         return "global-analysis"
     elif token[0:4] == "sqp_":
         return "project-analysis"
     else:
         return "user"
 
 
-def check_token(token):
-    if token_type(token) != "user":
+def check_token(token: str, is_sonarcloud: bool = False) -> None:
+    """Verifies if a proper user token has been provided"""
+    if token is None:
+        exit_fatal(
+            "Token is missing (Argument -t/--token)",
+            options.ERR_SONAR_API_AUTHENTICATION,
+        )
+    if not is_sonarcloud and token_type(token) != "user":
         exit_fatal(
             f"The provided token {redacted_token(token)} is a {token_type(token)} token, a user token is required for sonar-tools",
             options.ERR_TOKEN_NOT_SUITED,
         )
 
 
 def json_dump_debug(json_data, pre_string=""):
@@ -251,15 +311,15 @@
     :param rounded: Whether to rounddown to nearest day
     :type rounded: bool
     :return: The age in days, or by the second if not rounded
     :rtype: timedelta or int if rounded
     """
     if not some_date:
         return None
-    delta = datetime.datetime.today().replace(tzinfo=pytz.UTC) - some_date
+    delta = datetime.datetime.today().replace(tzinfo=timezone.utc) - some_date
     return delta.days if rounded else delta
 
 
 def get_setting(settings, key, default):
     if settings is None:
         return default
     return settings.get(key, default)
@@ -377,27 +437,34 @@
         except ValueError:
             logger.warning("JVM -Xmx heap specified seems invalid in '%s'", cmdline)
             return None
     logger.warning("No JVM heap memory settings specified in '%s'", cmdline)
     return None
 
 
-def int_memory(string):
+def int_memory(string) -> Union[int, None]:
     (val, unit) = string.split(" ")
     # For decimal separator in some countries
     val = float(val.replace(",", "."))
+    int_val = None
     if unit == "MB":
-        return int(val)
+        int_val = int(val)
     elif unit == "GB":
-        return int(val * 1024)
+        int_val = int(val * 1024)
+    elif unit == "TB":
+        int_val = int(val * 1024 * 1024)
+    elif unit == "PB":
+        int_val = int(val * 1024 * 1024 * 1024)
+    elif unit == "EB":
+        int_val = int(val * 1024 * 1024 * 1024 * 1024)
     elif unit == "KB":
-        return val / 1024
+        int_val = val / 1024
     elif unit == "bytes":
-        return val / 1024 / 1024
-    return None
+        int_val = val / 1024 / 1024
+    return int_val
 
 
 def dict_add(dict1, dict2):
     for k in dict2:
         if k not in dict1:
             dict1[k] = 0
         dict1[k] += dict2[k]
@@ -587,12 +654,28 @@
     :param str edition: The original non normalized edition string
     :return: The normalized edition string
     :rtype: str
     """
     if sif_v is None:
         return None
 
-    split_version = sif_v.split(".")
-    if as_string:
-        return ".".join(split_version[0:digits])
-    else:
-        return tuple(int(n) for n in split_version[0:digits])
+    try:
+        split_version = sif_v.split(".")
+    except KeyError:
+        return None
+    try:
+        if as_string:
+            return ".".join(split_version[0:digits])
+        else:
+            return tuple(int(n) for n in split_version[0:digits])
+    except ValueError:
+        return None
+
+
+def is_sonarcloud_url(url: str) -> bool:
+    """Returns whether an URL is the SonarCloud URL
+
+    :param str url: The URL to examine
+    :return: Whether the URL is the SonarCloud URL (in any form)
+    :rtype: str
+    """
+    return url.rstrip("/").lower().endswith("sonarcloud.io")
```

## sonar/version.py

```diff
@@ -20,8 +20,8 @@
 
 """
 
     sonar-tools project version
 
 """
 
-PACKAGE_VERSION = "2.9"
+PACKAGE_VERSION = "3.0"
```

## sonar/webhooks.py

```diff
@@ -73,15 +73,15 @@
     def audit(self):
         """
         :meta private:
         """
         if self._json["latestDelivery"]["success"]:
             return []
         rule = rules.get_rule(rules.RuleId.FAILED_WEBHOOK)
-        return [problem.Problem(rule.type, rule.severity, rule.msg.format(str(self)), concerned_object=self)]
+        return [problem.Problem(broken_rule=rule, msg=rule.msg.format(str(self)), concerned_object=self)]
 
     def to_json(self, full=False):
         """Exports a Webhook configuration in JSON format
 
         :param full: Whether to export all properties, including those that can't be set, or not, defaults to False
         :type full: bool, optional
         :return: The configuration of the DevOps platform (except secrets)
```

## sonar/audit/problem.py

```diff
@@ -19,50 +19,88 @@
 #
 
 import csv
 from sonar import utilities, options
 
 
 class Problem:
-    def __init__(self, problem_type, severity, msg, concerned_object=None):
+    def __init__(self, broken_rule: object, msg: str = "", problem_type=None, severity=None, concerned_object: object = None) -> None:
         # dict.__init__(type=problem_type, severity=severity, message=msg)
         self.concerned_object = concerned_object
-        self.type = problem_type
-        self.severity = severity
-        self.message = msg
+        self.rule_id = "UNDEF"
+        if broken_rule is None:
+            self.type = problem_type
+            self.severity = severity
+            self.message = msg
+        else:
+            self.type = broken_rule.type
+            self.severity = broken_rule.severity
+            self.rule_id = broken_rule.id
+            self.message = msg
+
         utilities.logger.warning(msg)
 
     def __str__(self):
         return f"Type: {self.type} - Severity: {self.severity} - Description: {self.message}"
 
     def to_json(self, with_url=False):
         d = vars(self).copy()
         d.pop("concerned_object")
-        for k in ("severity", "type"):
+
+        for k in ("severity", "type", "rule_id"):
             d[k] = str(d[k])
         if with_url:
             try:
                 d["url"] = self.concerned_object.url()
             except AttributeError:
                 d["url"] = str(self.concerned_object)
         return d
 
 
-def dump_report(problems, file, **kwargs):
+def dump_report(problems: list[Problem], file: str, server_id: str = None, **kwargs) -> None:
+    """Dumps to file a report about a list of problems
+
+    :param list[Problems] problems: List of problems to dump
+    :param str file: Filename to write the problems
+    :param str server_id: ServerId of the platform having the problems
+    :return: Nothing
+    :rtype: None
+    """
     utilities.logger.info("Writing report to %s", f"file '{file}'" if file else "stdout")
     if kwargs.get("format", "csv") == "json":
-        __dump_json(problems=problems, file=file, **kwargs)
+        __dump_json(problems=problems, file=file, server_id=server_id, **kwargs)
     else:
-        __dump_csv(problems=problems, file=file, **kwargs)
+        __dump_csv(problems=problems, file=file, server_id=server_id, **kwargs)
 
 
-def __dump_csv(problems, file, **kwargs):
+def __dump_csv(problems: list[Problem], file: str, server_id: str = None, **kwargs) -> None:
+    """Writes a list of problems in CSV format
+
+    :param list[Problems] problems: List of problems to dump
+    :param str file: Filename to write the problems
+    :return: Nothing
+    :rtype: None
+    """
     with utilities.open_file(file, "w") as fd:
         csvwriter = csv.writer(fd, delimiter=kwargs.get("separator", ","))
         for p in problems:
-            csvwriter.writerow(list(p.to_json(kwargs.get(options.WITH_URL, False)).values()))
-
-
-def __dump_json(problems, file, **kwargs):
-    json = [p.to_json(kwargs.get(options.WITH_URL, False)) for p in problems]
+            data = []
+            if server_id is not None:
+                data = [server_id]
+            data += list(p.to_json(kwargs.get(options.WITH_URL, False)).values())
+            csvwriter.writerow(data)
+
+
+def __dump_json(problems: list[Problem], file: str, server_id: str = None, **kwargs) -> None:
+    """Writes a list of problems in JSON format
+
+    :param list[Problems] problems: List of problems to dump
+    :param str file: Filename to write the problems
+    :return: Nothing
+    :rtype: None
+    """
+    sid_dict = {}
+    if server_id is not None:
+        sid_dict = {"server_id": server_id}
+    json = [{**p.to_json(kwargs.get(options.WITH_URL, False)), **sid_dict} for p in problems]
     with utilities.open_file(file) as fd:
         print(utilities.json_dump(json), file=fd)
```

## sonar/audit/rules.json

### Pretty-printed

 * *Similarity: 0.7751068376068376%*

 * *Differences: {"'BACKGROUND_TASKS_FAILURE_RATE_VERY_HIGH'": "OrderedDict([('severity', 'CRITICAL'), ('type', "*

 * *                                              "'OPERATIONS'), ('message', 'Background task failure "*

 * *                                              'rate ({}%) is very high, verify failed background '*

 * *                                              "tasks')])",*

 * * "'BELOW_LATEST'": "{'message': 'SonarQube version ({}) is greater than the LTA (ex-LTS) version "*

 * *                   "but not on LATEST {}'}",*

 * * "'BELOW_LTA […]*

```diff
@@ -22,31 +22,36 @@
         "type": "OPERATIONS"
     },
     "BACKGROUND_TASKS_FAILURE_RATE_HIGH": {
         "message": "Background task failure rate ({}%) is high, verify failed background tasks",
         "severity": "HIGH",
         "type": "OPERATIONS"
     },
+    "BACKGROUND_TASKS_FAILURE_RATE_VERY_HIGH": {
+        "message": "Background task failure rate ({}%) is very high, verify failed background tasks",
+        "severity": "CRITICAL",
+        "type": "OPERATIONS"
+    },
     "BACKGROUND_TASKS_PENDING_QUEUE_LONG": {
         "message": "Number of pending background tasks ({}) is high, verify CE dimensioning",
         "severity": "HIGH",
         "type": "PERFORMANCE"
     },
     "BACKGROUND_TASKS_PENDING_QUEUE_VERY_LONG": {
         "message": "Number of pending background tasks ({}) is very high, verify CE dimensioning",
         "severity": "CRITICAL",
         "type": "PERFORMANCE"
     },
     "BELOW_LATEST": {
-        "message": "SonarQube version ({}) is greater than the LTS version but not on LATEST {}",
+        "message": "SonarQube version ({}) is greater than the LTA (ex-LTS) version but not on LATEST {}",
         "severity": "LOW",
         "type": "OPERATIONS"
     },
-    "BELOW_LTS": {
-        "message": "SonarQube version ({}) is lower than the Long Term Support version {}",
+    "BELOW_LTA": {
+        "message": "SonarQube version ({}) is lower than the Long Term Active version {}",
         "severity": "CRITICAL",
         "type": "OPERATIONS"
     },
     "BG_TASK_FAILED": {
         "message": "{} last background task FAILED",
         "severity": "MEDIUM",
         "type": "OPERATIONS"
@@ -59,14 +64,29 @@
     },
     "BRANCH_NEVER_ANALYZED": {
         "message": "{} has never been analyzed and is kept even if inactive, this is suspicious",
         "object": "Branch",
         "severity": "LOW",
         "type": "PERFORMANCE"
     },
+    "CE_HEAP_TOO_HIGH": {
+        "message": "{} heap memory setting value is {} MB, above the maximum suggested value of {} MB",
+        "severity": "LOW",
+        "type": "PERFORMANCE"
+    },
+    "CE_HEAP_TOO_LOW": {
+        "message": "{} heap memory setting value is {} MB, below the recommended value of {} MB",
+        "severity": "HIGH",
+        "type": "PERFORMANCE"
+    },
+    "DB_ON_SAME_HOST": {
+        "message": "JDBC URL '{}' is pointing to local host which is highly discouraged for production",
+        "severity": "HIGH",
+        "type": "PERFORMANCE"
+    },
     "DCE_APP_CLUSTER_NOT_HA": {
         "message": "Only 1 app node is running, cluster will go down if this app node goes down",
         "severity": "CRITICAL",
         "type": "OPERATIONS"
     },
     "DCE_APP_NODE_NOT_GREEN": {
         "message": "{} health is {}, it should be GREEN",
@@ -114,14 +134,29 @@
         "type": "OPERATIONS"
     },
     "DEFAULT_ADMIN_PASSWORD": {
         "message": "Default admin password has not been changed, this is a critical security risk",
         "severity": "CRITICAL",
         "type": "SECURITY"
     },
+    "DUBIOUS_GLOBAL_SETTING": {
+        "message": "{}",
+        "severity": "HIGH",
+        "type": "BAD_PRACTICE"
+    },
+    "ES_HEAP_TOO_HIGH": {
+        "message": "{} heap ('sonar.search.javaOpts' -Xmx) memory setting value is {} MB, above the max {} MB recommended",
+        "severity": "HIGH",
+        "type": "PERFORMANCE"
+    },
+    "ES_HEAP_TOO_LOW": {
+        "message": "{} ('sonar.search.javaOpts' -Xmx) memory setting value is {} MB, too low for index size of {} MB",
+        "severity": "HIGH",
+        "type": "PERFORMANCE"
+    },
     "FAILED_WEBHOOK": {
         "message": "{} last delivery was failed",
         "object": "Webhook",
         "severity": "HIGH",
         "type": "OPERATIONS"
     },
     "GROUP_EMPTY": {
@@ -141,16 +176,36 @@
         "type": "SECURITY"
     },
     "LOG4SHELL_WEB": {
         "message": "Web server is not protected against the log4shell vulnerability",
         "severity": "CRITICAL",
         "type": "SECURITY"
     },
-    "LTS_PATCH_MISSING": {
-        "message": "SonarQube version ({}) missed a Long Term Support patch ({})",
+    "LOGS_IN_DEBUG_MODE": {
+        "message": "Log level of {} set to DEBUG, this very negatively affects platform performance, reverting to INFO is required",
+        "severity": "HIGH",
+        "type": "PERFORMANCE"
+    },
+    "LOGS_IN_TRACE_MODE": {
+        "message": "Log level of {} set to TRACE, this very negatively affects platform performance, reverting to INFO is urgent",
+        "severity": "CRITICAL",
+        "type": "PERFORMANCE"
+    },
+    "LOW_FREE_DISK_SPACE_1": {
+        "message": "Disk free space for '{}' ({} GB) is too low for a store size of {} GB",
+        "severity": "HIGH",
+        "type": "OPERATIONS"
+    },
+    "LOW_FREE_DISK_SPACE_2": {
+        "message": "Disk free space for '{}' ({} GB) is too low, please ensure at least 10 GB of free space",
+        "severity": "HIGH",
+        "type": "OPERATIONS"
+    },
+    "LTA_PATCH_MISSING": {
+        "message": "SonarQube version ({}) missed a Long Term Active patch ({})",
         "severity": "LOW",
         "type": "OPERATIONS"
     },
     "NOT_LATEST_SCANNER": {
         "message": "{} was last analyzed with {} version {} that is not the most recent version, consider updating the scanner",
         "severity": "MEDIUM",
         "type": "OPERATIONS"
@@ -360,15 +415,15 @@
     "QG_WRONG_METRIC": {
         "message": "{} has a condition on metric '{}', this is not recommended",
         "object": "QualityGate",
         "severity": "HIGH",
         "type": "GOVERNANCE"
     },
     "QG_WRONG_THRESHOLD": {
-        "message": "{} threshold {} on metric '{}', is outside of the recommended range [{}-{}]",
+        "message": "{} threshold {} on metric '{}', is outside of the recommended range [{}-{}]. {}",
         "object": "QualityGate",
         "severity": "HIGH",
         "type": "GOVERNANCE"
     },
     "QP_LAST_CHANGE_DATE": {
         "message": "{} has not been updated since {} days, it should be updated",
         "object": "QualityProfile",
@@ -401,54 +456,34 @@
     },
     "QP_USE_DEPRECATED_RULES": {
         "message": "{} uses {} deprecated rules, it should be updated",
         "object": "QualityProfile",
         "severity": "MEDIUM",
         "type": "GOVERNANCE"
     },
+    "RISKY_GLOBAL_PERMISSIONS": {
+        "message": "{}",
+        "severity": "HIGH",
+        "type": "BAD_PRACTICE"
+    },
     "SETTING_BASE_URL": {
         "message": "'sonar.core.serverBaseURL' is not set, this may break some features",
         "severity": "HIGH",
         "type": "OPERATIONS"
     },
-    "SETTING_CE_HEAP": {
-        "message": "CE process heap ('sonar.ce.javaOpts' -Xmx) memory setting value is {} MB, outside of recommended range [{}-{}] x {} workers",
-        "severity": "HIGH",
-        "type": "PERFORMANCE"
-    },
-    "SETTING_CE_NO_HEAP": {
-        "message": "CE process heap ('sonar.ce.javaOpts' -Xmx) memory setting is not specified",
-        "severity": "HIGH",
-        "type": "PERFORMANCE"
-    },
-    "SETTING_CE_TOO_MANY_WORKERS": {
-        "message": "CE has {} workers configured, more than the max {} recommended",
-        "severity": "HIGH",
-        "type": "PERFORMANCE"
-    },
     "SETTING_CPD_CROSS_PROJECT": {
         "message": "Cross project duplication is set to 'true', this can have a negative performance impact",
         "severity": "MEDIUM",
         "type": "PERFORMANCE"
     },
     "SETTING_DB_CLEANER": {
         "message": "DB Cleaner setting '{}' is outside of recommended range [{}-{}]",
         "severity": "MEDIUM",
         "type": "OPERATIONS"
     },
-    "SETTING_DB_ON_SAME_HOST": {
-        "message": "JDBC URL '{}' is pointing to local host which is highly discouraged for production",
-        "severity": "HIGH",
-        "type": "PERFORMANCE"
-    },
-    "SETTING_ES_HEAP": {
-        "message": "Search process heap ('sonar.search.javaOpts' -Xmx) memory setting value is {} MB, too low for index size of {} MB",
-        "severity": "HIGH",
-        "type": "PERFORMANCE"
-    },
     "SETTING_ES_NO_HEAP": {
         "message": "Search process heap ('sonar.search.javaOpts' -Xmx) memory setting is not specified",
         "severity": "HIGH",
         "type": "PERFORMANCE"
     },
     "SETTING_FORCE_AUTH": {
         "message": "'sonar.forceAuthentation' is set to 'false', this is a security risk",
@@ -457,15 +492,15 @@
     },
     "SETTING_JDBC_URL_NOT_SET": {
         "message": "JDBC URL is not set, most probably SonarQube runs on internal H2 DB that is not supported for production. You will not be able to upgrade",
         "severity": "CRITICAL",
         "type": "PERFORMANCE"
     },
     "SETTING_MAINT_GRID": {
-        "message": "Maintainability rating threshold of '{}' for {} rating is outside of recommended range [{}-{}]",
+        "message": "Maintainability rating threshold of {}% for {} rating is outside of recommended range [{}%-{}%]",
         "severity": "MEDIUM",
         "type": "GOVERNANCE"
     },
     "SETTING_NOT_SET": {
         "message": "'{}' is not set, this may break some features",
         "severity": "MEDIUM",
         "type": "OPERATIONS"
@@ -496,26 +531,16 @@
         "type": "OPERATIONS"
     },
     "SETTING_VALUE_OUT_OF_RANGE": {
         "message": "Setting '{}', is set to value '{}' which is outside of recommended range [{}-{}]",
         "severity": "MEDIUM",
         "type": "OPERATIONS"
     },
-    "SETTING_WEB_HEAP": {
-        "message": "Web process heap ('sonar.web.javaOpts' -Xmx) memory setting value is {} MB, outside of recommended range [{}-{}]",
-        "severity": "HIGH",
-        "type": "PERFORMANCE"
-    },
-    "SETTING_WEB_NO_HEAP": {
-        "message": "Web process heap ('sonar.web.javaOpts' -Xmx) memory setting is not specified",
-        "severity": "HIGH",
-        "type": "PERFORMANCE"
-    },
     "SETTING_WEB_WRONG_JAVA_VERSION": {
-        "message": "SonarQube {} is running on Java {} which is not a supported Java version",
+        "message": "{}: SonarQube {} is running on Java {} which is not a supported Java version for this release",
         "severity": "HIGH",
         "type": "OPERATIONS"
     },
     "SIF_UNDETECTED_SCM": {
         "message": "{} projects with undetected SCM, issue timestamping may not be accurate for those projects",
         "severity": "LOW",
         "type": "GOVERNANCE"
@@ -539,14 +564,29 @@
     },
     "TOKEN_UNUSED": {
         "message": "{} is not used since {} days, it should be revoked",
         "object": "Token",
         "severity": "MEDIUM",
         "type": "SECURITY"
     },
+    "TOO_MANY_CE_WORKERS": {
+        "message": "CE has {} workers configured, more than the max {} recommended",
+        "severity": "HIGH",
+        "type": "PERFORMANCE"
+    },
     "USER_UNUSED": {
         "message": "{} did not connect since {} days, it should be deactivated",
         "object": "User",
         "severity": "MEDIUM",
         "type": "SECURITY"
+    },
+    "WEB_HEAP_TOO_HIGH": {
+        "message": "{} heap memory setting value is {} MB, above the maximum suggested value of {} MB",
+        "severity": "LOW",
+        "type": "PERFORMANCE"
+    },
+    "WEB_HEAP_TOO_LOW": {
+        "message": "{} heap memory setting value is {} MB, below the recommended value of {} MB",
+        "severity": "HIGH",
+        "type": "PERFORMANCE"
     }
 }
```

## sonar/audit/rules.py

```diff
@@ -23,66 +23,75 @@
 import sonar.utilities as util
 
 __RULES = {}
 
 
 class RuleId(enum.Enum):
     DEFAULT_ADMIN_PASSWORD = 1
-    BELOW_LTS = 2
+    BELOW_LTA = 2
     LOG4SHELL_WEB = 3
     LOG4SHELL_CE = 4
     LOG4SHELL_ES = 5
     BELOW_LATEST = 6
-    LTS_PATCH_MISSING = 7
+    LTA_PATCH_MISSING = 7
 
     SETTING_FORCE_AUTH = 100
     SETTING_PROJ_DEFAULT_VISIBILITY = 101
     SETTING_CPD_CROSS_PROJECT = 102
 
     SETTING_NOT_SET = 110
     SETTING_SET = 111
     SETTING_VALUE_INCORRECT = 112
     SETTING_VALUE_OUT_OF_RANGE = 113
+    DUBIOUS_GLOBAL_SETTING = 114
 
     SETTING_BASE_URL = 120
     SETTING_DB_CLEANER = 121
     SETTING_MAINT_GRID = 122
     SETTING_SLB_RETENTION = 123
     SETTING_TD_LOC_COST = 124
 
-    SETTING_WEB_HEAP = 130
-    SETTING_ES_HEAP = 131
-    SETTING_CE_HEAP = 132
-    SETTING_CE_TOO_MANY_WORKERS = 133
+    ES_HEAP_TOO_HIGH = 131
+    ES_HEAP_TOO_LOW = 132
+    TOO_MANY_CE_WORKERS = 133
     SETTING_JDBC_URL_NOT_SET = 134
-    SETTING_DB_ON_SAME_HOST = 135
-
-    SETTING_WEB_NO_HEAP = 140
-    SETTING_ES_NO_HEAP = 141
-    SETTING_CE_NO_HEAP = 142
-    SETTING_WEB_WRONG_JAVA_VERSION = 143
-
-    ANYONE_WITH_GLOBAL_PERMS = 150
-    SONAR_USERS_WITH_ELEVATED_PERMS = 151
-    FAILED_WEBHOOK = 152
+    DB_ON_SAME_HOST = 135
+    WEB_HEAP_TOO_LOW = 136
+    WEB_HEAP_TOO_HIGH = 137
+    CE_HEAP_TOO_LOW = 138
+    CE_HEAP_TOO_HIGH = 139
+
+    SETTING_WEB_WRONG_JAVA_VERSION = 141
+    SETTING_ES_NO_HEAP = 142
+    LOW_FREE_DISK_SPACE_1 = 143
+    LOW_FREE_DISK_SPACE_2 = 144
+
+    RISKY_GLOBAL_PERMISSIONS = 150
+    ANYONE_WITH_GLOBAL_PERMS = 151
+    SONAR_USERS_WITH_ELEVATED_PERMS = 152
+    FAILED_WEBHOOK = 153
 
     DCE_DIFFERENT_APP_NODES_VERSIONS = 160
     DCE_DIFFERENT_APP_NODES_PLUGINS = 161
     DCE_APP_NODE_UNOFFICIAL_DISTRO = 162
     DCE_APP_NODE_NOT_GREEN = 163
     DCE_APP_CLUSTER_NOT_HA = 164
     DCE_ES_CLUSTER_NOT_HA = 170
     DCE_ES_UNBALANCED_INDEX = 171
     DCE_ES_INDEX_EMPTY = 172
     DCE_ES_CLUSTER_WRONG_NUMBER_OF_NODES = 173
     DCE_ES_CLUSTER_EVEN_NUMBER_OF_NODES = 174
 
+    LOGS_IN_TRACE_MODE = 180
+    LOGS_IN_DEBUG_MODE = 181
+
     BACKGROUND_TASKS_FAILURE_RATE_HIGH = 200
     BACKGROUND_TASKS_PENDING_QUEUE_LONG = 201
     BACKGROUND_TASKS_PENDING_QUEUE_VERY_LONG = 202
+    BACKGROUND_TASKS_FAILURE_RATE_VERY_HIGH = 203
 
     PROJ_LAST_ANALYSIS = 1000
     PROJ_NOT_ANALYZED = 1001
     PROJ_VISIBILITY = 1002
     PROJ_DUPLICATE = 1003
 
     BRANCH_LAST_ANALYSIS = 1020
```

## sonar/audit/sonar-audit.properties

```diff
@@ -197,17 +197,14 @@
 
 # Audits projects with last background task failed
 audit.projects.failedTasks = yes
 
 # Audits projects analyzed with too old scanner version, 2 years by default
 audit.projects.scannerMaxAge = 730
 
-# Audits projects branches
-audit.project.branches = yes
-
 #====================== QUALITY GATES AUDIT CONFIGURATION =====================
 
 # Audit that there are not too many quality gates, this defeats company common governance
 audit.qualitygates.maxNumber = 5
 
 # Audit that quality gates don't have too many criterias, it's too complex and
 # may prevent passing QG because of incorrect QG criteria
```

## sonar/dce/app_nodes.py

```diff
@@ -22,25 +22,24 @@
     Abstraction of the App Node concept
 
 """
 
 import datetime
 from dateutil.relativedelta import relativedelta
 import sonar.utilities as util
-from sonar.audit import rules, severities, types
+from sonar.audit import rules
+import sonar.sif_node as sifn
 import sonar.audit.problem as pb
 import sonar.dce.nodes as dce_nodes
 
 _RELEASE_DATE_6_7 = datetime.datetime(2017, 11, 8) + relativedelta(months=+6)
 _RELEASE_DATE_7_9 = datetime.datetime(2019, 7, 1) + relativedelta(months=+6)
 _RELEASE_DATE_8_9 = datetime.datetime(2021, 5, 4) + relativedelta(months=+6)
 
 _SYSTEM = "System"
-_SETTINGS = "Settings"
-_VERSION = "Version"
 
 
 class AppNode(dce_nodes.DceNode):
     def __str__(self):
         return f"App Node '{self.name()}'"
 
     def plugins(self):
@@ -48,257 +47,85 @@
 
     def health(self):
         return self.json.get("Health", "RED")
 
     def node_type(self):
         return "APPLICATION"
 
+    def start_time(self) -> datetime.datetime:
+        return self.sif.start_time()
+
     def version(self, digits=3, as_string=False):
-        if _SETTINGS in self.json:
-            split_version = self.json[_SETTINGS][_VERSION].split(".")
-        elif _SYSTEM in self.json and _VERSION in self.json[_SYSTEM]:
-            split_version = self.json[_SYSTEM][_VERSION].split(".")
-        else:
-            return None
         try:
-            if as_string:
-                return ".".join(split_version[0:digits])
-            else:
-                return tuple(int(n) for n in split_version[0:digits])
-        except ValueError:
+            return util.string_to_version(self.json[_SYSTEM]["Version"], digits, as_string)
+        except KeyError:
             return None
 
-    def log_level(self):
-        if "Web Logging" in self.json:
-            return self.json["Web Logging"]["Logs Level"]
-        else:
-            return None
+    def edition(self) -> str:
+        self.sif.edition()
 
     def name(self):
         return self.json["Name"]
 
     def audit(self, audit_settings: dict[str, str] = None):
         util.logger.info("Auditing %s", str(self))
         return (
-            self.__audit_log_level()
-            + self.__audit_official()
+            self.__audit_official()
             + self.__audit_health()
-            + self.__audit_version()
-            + self.__audit_web_settings(audit_settings)
-            + self.__audit_ce_settings()
-            + self.__audit_background_tasks()
+            + sifn.audit_web(self, f"{str(self)} Web process", self.json)
+            + sifn.audit_ce(self, f"{str(self)} CE process", self.json)
         )
 
-    def __audit_log_level(self):
-        util.logger.debug("Auditing log level")
-        log_level = self.log_level()
-        if log_level is None:
-            util.logger.warning("%s: log level is missing, audit of log level is skipped...", str(self))
-            return []
-        if log_level not in ("DEBUG", "TRACE"):
-            util.logger.info("Log level of '%s' is '%s', all good...", str(self), log_level)
-            return []
-        if log_level == "TRACE":
-            return [
-                pb.Problem(
-                    types.Type.PERFORMANCE,
-                    severities.Severity.CRITICAL,
-                    f"Log level of {str(self)} set to TRACE, this does very negatively affect platform performance, " "reverting to INFO is required",
-                )
-            ]
-        if log_level == "DEBUG":
-            return [
-                pb.Problem(
-                    types.Type.PERFORMANCE,
-                    severities.Severity.HIGH,
-                    f"Log level of {str(self)} is set to DEBUG, this may affect platform performance, " "reverting to INFO is recommended",
-                )
-            ]
-        util.logger.debug("%s: Node log level is %s", str(self), log_level)
-        return []
-
     def __audit_health(self):
+        util.logger.info("%s: Auditing node health", str(self))
         if self.health() != dce_nodes.HEALTH_GREEN:
             rule = rules.get_rule(rules.RuleId.DCE_APP_NODE_NOT_GREEN)
-            return [pb.Problem(rule.type, rule.severity, rule.msg.format(str(self), self.health()))]
-        else:
-            util.logger.debug("%s: Node health is %s", str(self), dce_nodes.HEALTH_GREEN)
-            return []
+            return [pb.Problem(broken_rule=rule, msg=rule.msg.format(str(self), self.health()))]
+
+        util.logger.info("%s: Node health is %s", str(self), dce_nodes.HEALTH_GREEN)
+        return []
 
     def __audit_official(self):
         if _SYSTEM not in self.json:
             util.logger.warning(
                 "%s: Official distribution information missing, audit skipped...",
                 str(self),
             )
             return []
         elif not self.json[_SYSTEM]["Official Distribution"]:
             rule = rules.get_rule(rules.RuleId.DCE_APP_NODE_UNOFFICIAL_DISTRO)
-            return [pb.Problem(rule.type, rule.severity, rule.msg.format(str(self)))]
+            return [pb.Problem(broken_rule=rule, msg=rule.msg.format(str(self)))]
         else:
             util.logger.debug("%s: Node is official distribution", str(self))
             return []
 
-    def __audit_version(self):
-        sq_version = self.version()
-        if sq_version is None:
-            util.logger.warning("%s: Version information is missing, audit on node vresion is skipped...")
-            return []
-        st_time = self.sif.start_time()
-        if (
-            (st_time > _RELEASE_DATE_6_7 and sq_version < (6, 7, 0))
-            or (st_time > _RELEASE_DATE_7_9 and sq_version < (7, 9, 0))
-            or (st_time > _RELEASE_DATE_8_9 and sq_version < (8, 9, 0))
-        ):
-            rule = rules.get_rule(rules.RuleId.BELOW_LTS)
-            return [pb.Problem(rule.type, rule.severity, rule.msg)]
-        else:
-            util.logger.debug(
-                "%s: Version %s is correct wrt LTS",
-                str(self),
-                self.version(as_string=True),
-            )
-            return []
-
-    def __audit_jvm_version(self) -> list[pb.Problem]:
-        try:
-            java_version = int(self.json["Web JVM Properties"]["java.specification.version"])
-        except KeyError:
-            util.logger.warning("Can't find Java version for %s in SIF, auditing this part is skipped", str(self))
-            return []
-        try:
-            sq_version = util.string_to_version(self.json["System"]["Version"])
-        except KeyError:
-            util.logger.warning("Can't find SonarQube version for %s in SIF, auditing this part is skipped", str(self))
-            return []
-        if sq_version >= (9, 9, 0) and java_version != 17:
-            rule = rules.get_rule(rules.RuleId.SETTING_WEB_WRONG_JAVA_VERSION)
-            return [pb.Problem(rule.type, rule.severity, rule.msg.format(str(self), java_version), concerned_object=self)]
-        else:
-            util.logger.debug("%s is running on the required java version (java %d)", str(self), java_version)
-        return []
-
-    def __audit_jvm_ram(self, audit_settings: dict[str, str]) -> list[pb.Problem]:
-        # On DCE we expect between 2 and 4 GB of RAM per App Node Web JVM
-        min_heap = audit_settings.get("audit.web.heapMin", 2024)
-        max_heap = audit_settings.get("audit.web.heapMax", 4096)
-        try:
-            web_heap = self.json["Web JVM State"]["Heap Max (MB)"]
-        except KeyError:
-            util.logger.warning("Can't find JVM Heap for %s in SIF, auditing this part is skipped", str(self))
-            return []
-        if web_heap is None:
-            rule = rules.get_rule(rules.RuleId.SETTING_WEB_NO_HEAP)
-            return [pb.Problem(rule.type, rule.severity, rule.msg, concerned_object=self)]
-        elif web_heap < min_heap or web_heap > max_heap:
-            rule = rules.get_rule(rules.RuleId.SETTING_WEB_HEAP)
-            return [pb.Problem(rule.type, rule.severity, rule.msg.format(web_heap, min_heap, max_heap), concerned_object=self)]
-        else:
-            util.logger.debug("%s web heap of %d MB is within recommended range [%d-%d]", str(self), web_heap, min_heap, max_heap)
-
-    def __audit_web_settings(self, audit_settings: dict[str, str]) -> list[pb.Problem]:
-        return self.__audit_jvm_version() + self.__audit_jvm_ram(audit_settings)
-
-    def __audit_ce_settings(self):
-        util.logger.info("Auditing CE settings")
-        try:
-            ce_workers = self.json["Compute Engine Tasks"]["Worker Count"]
-        except KeyError:
-            util.logger.warning(
-                "%s: CE section missing from SIF, CE workers audit skipped...",
-                str(self),
-            )
-            return []
-        MAX_WORKERS = 2
-        if ce_workers > MAX_WORKERS:
-            rule = rules.get_rule(rules.RuleId.SETTING_CE_TOO_MANY_WORKERS)
-            return [pb.Problem(rule.type, rule.severity, rule.msg.format(ce_workers, MAX_WORKERS))]
-        else:
-            util.logger.debug(
-                "%s: %d CE workers configured, correct compared to the max %d recommended",
-                str(self),
-                ce_workers,
-                MAX_WORKERS,
-            )
-            return []
-
-    def __audit_background_tasks(self):
-        util.logger.debug("Auditing CE background tasks")
-        problems = []
-        try:
-            ce_tasks = self.json["Compute Engine Tasks"]
-        except KeyError:
-            util.logger.warning(
-                "%s: CE section missing from SIF, background tasks audit skipped...",
-                str(self),
-            )
-            return []
-
-        ce_success = ce_tasks["Processed With Success"]
-        ce_error = ce_tasks["Processed With Error"]
-        failure_rate = 0
-        if ce_success != 0 or ce_error != 0:
-            failure_rate = ce_error / (ce_success + ce_error)
-        if ce_error > 10 and failure_rate > 0.01:
-            rule = rules.get_rule(rules.RuleId.BACKGROUND_TASKS_FAILURE_RATE_HIGH)
-            problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format(int(failure_rate * 100))))
-        else:
-            util.logger.debug(
-                "Number of failed background tasks (%d), and failure rate %d%% is OK",
-                ce_error,
-                int(failure_rate * 100),
-            )
-
-        ce_pending = ce_tasks["Pending"]
-        if ce_pending > 100:
-            rule = rules.get_rule(rules.RuleId.BACKGROUND_TASKS_PENDING_QUEUE_VERY_LONG)
-            problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format(ce_pending)))
-        elif ce_pending > 20 and ce_pending > (10 * ce_tasks["Worker Count"]):
-            rule = rules.get_rule(rules.RuleId.BACKGROUND_TASKS_PENDING_QUEUE_LONG)
-            problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format(ce_pending)))
-        else:
-            util.logger.debug("Number of pending background tasks (%d) is OK", ce_pending)
-        return problems
-
 
-def audit(sub_sif: dict[str, str], sif: object, audit_settings: dict[str, str] = None) -> list[pb.Problem]:
+def audit(sub_sif: dict[str, str], sif_object: object, audit_settings: dict[str, str] = None) -> list[pb.Problem]:
     """Audits application nodes of a DCE instance
 
     :param dict sub_sif: The JSON subsection of the SIF pertaining to the App Nodes
     :param Sif sif: The Sif object
     :param dict audit_settings: Config settings for audit
     :return: List of Problems
     :rtype: list
     """
     if audit_settings is None:
         audit_settings = {}
     nodes = []
     problems = []
     for n in sub_sif:
-        nodes.append(AppNode(n, sif))
+        nodes.append(AppNode(n, sif_object))
     if len(nodes) == 1:
         rule = rules.get_rule(rules.RuleId.DCE_APP_CLUSTER_NOT_HA)
-        return [pb.Problem(rule.type, rule.severity, rule.msg)]
-    for i in range(len(nodes)):
-        problems += nodes[i].audit(audit_settings)
-        for j in range(i, len(nodes)):
-            v1 = nodes[i].version()
-            v2 = nodes[j].version()
+        return [pb.Problem(broken_rule=rule, msg=rule.msg)]
+    for node_1 in nodes:
+        problems += node_1.audit(audit_settings)
+        for node_2 in nodes:
+            v1 = node_1.version()
+            v2 = node_2.version()
             if v1 is not None and v2 is not None and v1 != v2:
                 rule = rules.get_rule(rules.RuleId.DCE_DIFFERENT_APP_NODES_VERSIONS)
-                problems.append(
-                    pb.Problem(
-                        rule.type,
-                        rule.severity,
-                        rule.msg.format(str(nodes[i]), str(nodes[j])),
-                    )
-                )
-            if nodes[i].plugins() != nodes[j].plugins():
+                problems.append(pb.Problem(broken_rule=rule, msg=rule.msg.format(str(node_1), str(node_2))))
+            if node_1.plugins() != node_2.plugins():
                 rule = rules.get_rule(rules.RuleId.DCE_DIFFERENT_APP_NODES_PLUGINS)
-                problems.append(
-                    pb.Problem(
-                        rule.type,
-                        rule.severity,
-                        rule.msg.format(str(nodes[i]), str(nodes[j])),
-                    )
-                )
+                problems.append(pb.Problem(broken_rule=rule, msg=rule.msg.format(str(node_1), str(node_2))))
     return problems
```

## sonar/dce/search_nodes.py

```diff
@@ -44,84 +44,115 @@
     def name(self):
         return self.json["Name"]
 
     def node_type(self):
         return "SEARCH"
 
     def audit(self):
-        util.logger.info("Auditing %s", str(self))
-        return self.__audit_store_size()
+        util.logger.info("%s: Auditing...", str(self))
+        return self.__audit_store_size() + self.__audit_available_disk()
 
     def max_heap(self) -> Union[int, None]:
-        if self.sif.version() < (9, 0, 0):
+        if self.sif.edition() != "datacenter" and self.sif.version() < (9, 0, 0):
             return util.jvm_heap(self.sif.search_jvm_cmdline())
         try:
-            sz = self.json["Search State"]["JVM Heap Max"]
+            sz = self.json[_ES_STATE]["JVM Heap Max"]
         except KeyError:
-            util.logger.warning("Can't retrieve heap allocated to %s", str(self))
+            util.logger.warning("%s: Can't retrieve heap allocated, skipping this check", str(self))
             return None
         return int(float(sz.split(" ")[0]) * 1024)
 
     def __audit_store_size(self):
+        util.logger.info("%s: Auditing store size", str(self))
         es_heap = self.max_heap()
         if es_heap is None:
-            util.logger.warning("No ES heap found for %s, audit of ES head is skipped", str(self))
+            util.logger.warning("%s: No ES heap found, audit of ES head is skipped", str(self))
             rule = rules.get_rule(rules.RuleId.SETTING_ES_NO_HEAP)
-            return [pb.Problem(rule.type, rule.severity, rule.msg)]
+            return [pb.Problem(broken_rule=rule, msg=rule.msg)]
 
         index_size = self.store_size()
 
+        es_min = min(2 * index_size, es_heap < index_size + 1000)
+        es_max = 32 * 1024
+        es_pb = []
         if index_size is None:
-            util.logger.debug("Search server index size missing, audit of ES index vs heap skipped...")
-            return []
+            util.logger.warning("%s: Search server store size missing, audit of ES index vs heap skipped...", str(self))
         elif index_size == 0:
             rule = rules.get_rule(rules.RuleId.DCE_ES_INDEX_EMPTY)
-            return [pb.Problem(rule.type, rule.severity, rule.msg.format(str(self)))]
-        elif es_heap < 2 * index_size and es_heap < index_size + 1000:
-            rule = rules.get_rule(rules.RuleId.SETTING_ES_HEAP)
-            return [pb.Problem(rule.type, rule.severity, rule.msg.format(es_heap, index_size))]
+            es_pb = [pb.Problem(broken_rule=rule, msg=rule.msg.format(str(self)))]
+        elif es_heap < es_min:
+            rule = rules.get_rule(rules.RuleId.ES_HEAP_TOO_LOW)
+            es_pb = [pb.Problem(broken_rule=rule, msg=rule.msg.format(str(self), es_heap, index_size))]
+        elif es_heap > es_max:
+            rule = rules.get_rule(rules.RuleId.ES_HEAP_TOO_HIGH)
+            es_pb = [pb.Problem(broken_rule=rule, msg=rule.msg.format(str(self), es_heap, 32 * 1024))]
         else:
-            util.logger.debug(
-                "Search server memory %d MB is correct wrt to index size of %d MB",
-                es_heap,
-                index_size,
-            )
+            util.logger.info("%s: Search server memory %d MB is correct wrt to store size of %d MB", str(self), es_heap, index_size)
+        return es_pb
+
+    def __audit_available_disk(self) -> list[pb.Problem]:
+        util.logger.info("%s: Auditing available disk space", str(self))
+        try:
+            space_avail = util.int_memory(self.json[_ES_STATE]["Disk Available"])
+        except ValueError:
+            util.logger.warning("%s: disk space available not found in SIF, skipping this check", str(self))
             return []
+        store_size = self.store_size()
+        util.logger.info(
+            "%s: Search server available disk size of %d MB and store size is %d MB",
+            str(self),
+            space_avail,
+            store_size,
+        )
+        if space_avail < 10000:
+            rule = rules.get_rule(rules.RuleId.LOW_FREE_DISK_SPACE_2)
+            return [pb.Problem(broken_rule=rule, msg=rule.msg.format(str(self), space_avail // 1024))]
+        elif store_size * 2 > space_avail:
+            rule = rules.get_rule(rules.RuleId.LOW_FREE_DISK_SPACE_1)
+            return [pb.Problem(broken_rule=rule, msg=rule.msg.format(str(self), space_avail // 1024, store_size // 1024))]
+
+        return []
 
 
 def __audit_index_balance(searchnodes):
+    util.logger.info("Auditing search nodes store size balance")
     nbr_search_nodes = len(searchnodes)
     for i in range(nbr_search_nodes):
         size_i = searchnodes[i].store_size()
         if size_i is None:
             continue
         for j in range(i + 1, nbr_search_nodes):
             size_j = searchnodes[j].store_size()
             if size_j is None or size_j == 0:
                 continue
             store_ratio = size_i / size_j
             if store_ratio >= 0.5 or store_ratio <= 2:
                 continue
             rule = rules.get_rule(rules.RuleId.DCE_ES_UNBALANCED_INDEX)
-            return [pb.Problem(rule.type, rule.severity, rule.msg.format())]
+            return [pb.Problem(broken_rule=rule, msg=rule.msg.format())]
+    util.logger.info("Search nodes store size balance acceptable")
     return []
 
 
 def audit(sub_sif, sif):
+    util.logger.info("Auditing search node(s)")
     searchnodes = []
     problems = []
     for n in sub_sif:
         searchnodes.append(SearchNode(n, sif))
     nbr_search_nodes = len(searchnodes)
+    util.logger.info("Auditing number of search nodes")
     if nbr_search_nodes < 3:
         rule = rules.get_rule(rules.RuleId.DCE_ES_CLUSTER_NOT_HA)
-        problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format()))
+        problems.append(pb.Problem(broken_rule=rule, msg=rule.msg.format()))
     elif nbr_search_nodes > 3:
         if nbr_search_nodes % 2 == 0:
             rule = rules.get_rule(rules.RuleId.DCE_ES_CLUSTER_EVEN_NUMBER_OF_NODES)
         else:
             rule = rules.get_rule(rules.RuleId.DCE_ES_CLUSTER_WRONG_NUMBER_OF_NODES)
-        problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format(nbr_search_nodes)))
+        problems.append(pb.Problem(broken_rule=rule, msg=rule.msg.format(nbr_search_nodes)))
+    else:
+        util.logger.info("%d search nodes found, all OK", nbr_search_nodes)
     for i in range(nbr_search_nodes):
         problems += searchnodes[i].audit()
     problems += __audit_index_balance(searchnodes)
     return problems
```

## sonar/findings/findings.py

```diff
@@ -157,49 +157,81 @@
             return comp.split(":")[-1]
         elif "path" in self._json:
             return self._json["path"]
         else:
             util.logger.warning("Can't find file name for %s", str(self))
             return None
 
-    def to_csv(self, separator=","):
+    def to_csv(self, separator: str = ",", without_time: bool = False):
         """
         :param separator: CSV separator, defaults to ","
         :type separator: str, optional
         :return: The finding as CSV
         :rtype: str
         """
-        data = self.to_json()
+        data = self.to_json(without_time)
         for field in _CSV_FIELDS:
             if data.get(field, None) is None:
                 data[field] = ""
         data["branch"] = util.quote(data["branch"], separator)
         data["message"] = util.quote(data["message"], separator)
         data["projectName"] = projects.Project.get_object(key=self.projectKey, endpoint=self.endpoint).name
         return separator.join([str(data[field]) for field in _CSV_FIELDS])
 
-    def to_json(self):
+    def to_json(self, without_time: bool = False):
         """
         :return: The finding as dict
         :rtype: dict
         """
+        fmt = util.SQ_DATETIME_FORMAT
+        if without_time:
+            fmt = util.SQ_DATE_FORMAT
         data = vars(self).copy()
         for old_name, new_name in _JSON_FIELDS_REMAPPED:
             data[new_name] = data.pop(old_name, None)
         data["effort"] = ""
         data["file"] = self.file()
-        data["creationDate"] = self.creation_date.strftime(util.SQ_DATETIME_FORMAT)
-        data["updateDate"] = self.modification_date.strftime(util.SQ_DATETIME_FORMAT)
+        data["creationDate"] = self.creation_date.strftime(fmt)
+        data["updateDate"] = self.modification_date.strftime(fmt)
         for field in _JSON_FIELDS_PRIVATE:
             data.pop(field, None)
         for k in data.copy():
             if data[k] is None or data[k] == "":
                 data.pop(k)
         return data
 
+    def to_sarif(self) -> dict[str, str]:
+        """
+        :return: The finding in SARIF format
+        :rtype: dict
+        """
+        data = {}
+        data["level"] = "warning"
+        if self.is_bug() or self.is_vulnerability() or self.severity in ("CRITICAL", "BLOCKER"):
+            data["level"] = "error"
+        data["ruleId"] = self.rule
+        data["message"] = {"text": self.message}
+        data["properties"] = self.to_json()
+        data["properties"]["url"] = self.url()
+        rg = self._json["textRange"]
+        data["locations"] = [
+            {
+                "physicalLocation": {
+                    "artifactLocation": {"uri": f"file:///{self.file()}", "index": 0},
+                    "region": {
+                        "startLine": max(int(rg["startLine"]), 1),
+                        "startColumn": max(int(rg["startOffset"]), 1),
+                        "endLine": max(int(rg["endLine"]), 1),
+                        "endColumn": max(int(rg["endOffset"]), 1),
+                    },
+                }
+            }
+        ]
+        return data
+
     def is_vulnerability(self):
         return self.type == "VULNERABILITY"
 
     def is_hotspot(self):
         return self.type == "SECURITY_HOTSPOT"
 
     def is_bug(self):
@@ -245,15 +277,15 @@
         return set([c.author() for c in self.changelog().values()])
 
     def commenters(self):
         """
         :return: the set of users that commented the finding
         :rtype: set(str)
         """
-        return set([v.get("user", None) for v in self.comments() if v.get("user", None)])
+        return set([v["user"] for v in self.comments() if "user" in v])
 
     def can_be_synced(self, user_list):
         """
         :meta private:
         """
         util.logger.debug(
             "Issue %s: Checking if modifiers %s are different from user %s",
```

## sonar/findings/hotspots.py

```diff
@@ -99,20 +99,20 @@
         branch = ""
         if self.branch is not None:
             branch = f"branch={requests.utils.quote(self.branch)}&"
         elif self.pull_request is not None:
             branch = f"pullRequest={requests.utils.quote(self.pull_request)}&"
         return f"{self.endpoint.url}/security_hotspots?{branch}id={self.projectKey}&hotspots={self.key}"
 
-    def to_json(self):
+    def to_json(self, without_time: bool = False):
         """
         :return: JSON representation of the hotspot
         :rtype: dict
         """
-        data = super().to_json()
+        data = super().to_json(without_time)
         data["url"] = self.url()
         return data
 
     def refresh(self):
         """Refreshes and reads hotspots details in SonarQube
         :return: The hotspot details
         :rtype: Whether ther operation succeeded
```

## sonar/findings/issues.py

```diff
@@ -189,20 +189,20 @@
             self._debt = ((kdays * 1000 + days) * 24 + hours) * 60 + minutes
         elif "effort" in self._json:
             self._debt = 0
             if self._json["effort"] != "null":
                 self._debt = int(self._json["effort"])
         return self._debt
 
-    def to_json(self):
+    def to_json(self, without_time: bool = False):
         """
         :return: The issue attributes as JSON
         :rtype: dict
         """
-        data = super().to_json()
+        data = super().to_json(without_time)
         data["url"] = self.url()
         data["effort"] = self.debt()
         return data
 
     def refresh(self):
         """Refreshes an issue from the SonarQube platform live data
         :return: whether the refresh was successful
```

## sonar/permissions/project_permissions.py

```diff
@@ -98,68 +98,68 @@
     def __audit_user_permissions(self, audit_settings):
         problems = []
         user_count = self.count("users")
         max_users = audit_settings.get("audit.projects.permissions.maxUsers", 5)
         if user_count > max_users:
             rule = rules.get_rule(rules.RuleId.PROJ_PERM_MAX_USERS)
             msg = rule.msg.format(str(self.concerned_object), user_count)
-            problems.append(problem.Problem(rule.type, rule.severity, msg, concerned_object=self))
+            problems.append(problem.Problem(broken_rule=rule, msg=msg, concerned_object=self))
 
         max_admins = audit_settings.get("audit.projects.permissions.maxAdminUsers", 2)
         admin_count = self.count("users", ("admin"))
         if admin_count > max_admins:
             rule = rules.get_rule(rules.RuleId.PROJ_PERM_MAX_ADM_USERS)
             msg = rule.msg.format(str(self.concerned_object), admin_count, max_admins)
-            problems.append(problem.Problem(rule.type, rule.severity, msg, concerned_object=self))
+            problems.append(problem.Problem(broken_rule=rule, msg=msg, concerned_object=self))
 
         return problems
 
     def __audit_group_permissions(self, audit_settings):
         problems = []
         groups = self.read().to_json(perm_type="groups")
         for gr_name, gr_perms in groups.items():
             if gr_name == "Anyone":
                 rule = rules.get_rule(rules.RuleId.PROJ_PERM_ANYONE)
-                problems.append(problem.Problem(rule.type, rule.severity, rule.msg.format(str(self.concerned_object)), concerned_object=self))
+                problems.append(problem.Problem(broken_rule=rule, msg=rule.msg.format(str(self.concerned_object)), concerned_object=self))
             if gr_name == "sonar-users" and (
                 "issueadmin" in gr_perms or "scan" in gr_perms or "securityhotspotadmin" in gr_perms or "admin" in gr_perms
             ):
                 rule = rules.get_rule(rules.RuleId.PROJ_PERM_SONAR_USERS_ELEVATED_PERMS)
                 problems.append(
-                    problem.Problem(rule.type, rule.severity, rule.msg.format(str(self.concerned_object)), concerned_object=self.concerned_object)
+                    problem.Problem(broken_rule=rule, msg=rule.msg.format(str(self.concerned_object)), concerned_object=self.concerned_object)
                 )
 
         max_perms = audit_settings.get("audit.projects.permissions.maxGroups", 5)
         counter = self.count(perm_type="groups", perm_filter=permissions.PROJECT_PERMISSIONS)
         if counter > max_perms:
             rule = rules.get_rule(rules.RuleId.PROJ_PERM_MAX_GROUPS)
             msg = rule.msg.format(str(self.concerned_object), counter, max_perms)
-            problems.append(problem.Problem(rule.type, rule.severity, msg, concerned_object=self))
+            problems.append(problem.Problem(broken_rule=rule, msg=msg, concerned_object=self))
 
         max_scan = audit_settings.get("audit.projects.permissions.maxScanGroups", 1)
         counter = self.count(perm_type="groups", perm_filter=("scan"))
         if counter > max_scan:
             rule = rules.get_rule(rules.RuleId.PROJ_PERM_MAX_SCAN_GROUPS)
             msg = rule.msg.format(str(self.concerned_object), counter, max_scan)
-            problems.append(problem.Problem(rule.type, rule.severity, msg, concerned_object=self))
+            problems.append(problem.Problem(broken_rule=rule, msg=msg, concerned_object=self))
 
         max_issue_adm = audit_settings.get("audit.projects.permissions.maxIssueAdminGroups", 2)
         counter = self.count(perm_type="groups", perm_filter=("issueadmin"))
         if counter > max_issue_adm:
             rule = rules.get_rule(rules.RuleId.PROJ_PERM_MAX_ISSUE_ADM_GROUPS)
             msg = rule.msg.format(str(self.concerned_object), counter, max_issue_adm)
-            problems.append(problem.Problem(rule.type, rule.severity, msg, concerned_object=self))
+            problems.append(problem.Problem(broken_rule=rule, msg=msg, concerned_object=self))
 
         max_spots_adm = audit_settings.get("audit.projects.permissions.maxHotspotAdminGroups", 2)
         counter = self.count(perm_type="groups", perm_filter=("securityhotspotadmin"))
         if counter > max_spots_adm:
             rule = rules.get_rule(rules.RuleId.PROJ_PERM_MAX_HOTSPOT_ADM_GROUPS)
             msg = rule.msg.format(str(self.concerned_object), counter, max_spots_adm)
-            problems.append(problem.Problem(rule.type, rule.severity, msg, concerned_object=self))
+            problems.append(problem.Problem(broken_rule=rule, msg=msg, concerned_object=self))
 
         max_admins = audit_settings.get("audit.projects.permissions.maxAdminGroups", 2)
         counter = self.count(perm_type="groups", perm_filter=("admin"))
         if counter > max_admins:
             rule = rules.get_rule(rules.RuleId.PROJ_PERM_MAX_ADM_GROUPS)
             msg = rule.msg.format(str(self.concerned_object), counter, max_admins)
-            problems.append(problem.Problem(rule.type, rule.severity, msg, concerned_object=self))
+            problems.append(problem.Problem(broken_rule=rule, msg=msg, concerned_object=self))
         return problems
```

## sonar/projects/branches.py

```diff
@@ -68,15 +68,15 @@
         try:
             data = json.loads(concerned_object.endpoint.get(APIS["list"], params={"project": concerned_object.key}).text)
         except HTTPError as e:
             if e.response.status_code == HTTPStatus.NOT_FOUND:
                 raise exceptions.ObjectNotFound(concerned_object.key, f"Project '{concerned_object.key}' not found")
         for br in data.get("branches", []):
             if br["name"] == branch_name:
-                return cls.load(concerned_object, branch_name, data)
+                return cls.load(concerned_object, branch_name, br)
         raise exceptions.ObjectNotFound(branch_name, f"Branch '{branch_name}' of project '{concerned_object.key}' not found")
 
     @classmethod
     def load(cls, concerned_object, branch_name, data):
         """Gets a Branch object from JSON data gotten from a list API call
 
         :param concerned_object: Object concerned by the branch (Project or Application)
@@ -191,20 +191,22 @@
         try:
             return sq.delete_object(self, APIS["delete"], {"branch": self.name, "project": self.concerned_object.key}, _OBJECTS)
         except HTTPError as e:
             if e.response.status_code == HTTPStatus.BAD_REQUEST:
                 util.logger.warning("Can't delete %s, it's the main branch", str(self))
             return False
 
-    def new_code(self):
+    def new_code(self) -> str:
         """
         :return: The branch new code period definition
         :rtype: str
         """
-        if self._new_code is None:
+        if self._new_code is None and self.endpoint.is_sonarcloud():
+            self._new_code = settings.new_code_to_string({"inherited": True})
+        elif self._new_code is None:
             try:
                 data = json.loads(self.get(api=APIS["get_new_code"], params={"project": self.concerned_object.key}).text)
             except HTTPError as e:
                 if e.response.status_code == HTTPStatus.NOT_FOUND:
                     raise exceptions.ObjectNotFound(self.concerned_object.key, f"str{self.concerned_object} not found")
             for b in data["newCodePeriods"]:
                 new_code = settings.new_code_to_string(b)
@@ -269,22 +271,22 @@
         self.name = new_name
         _OBJECTS[uuid(self.concerned_object.key, self.name)] = self
         return True
 
     def __audit_zero_loc(self):
         if self.last_analysis() and self.loc() == 0:
             rule = rules.get_rule(rules.RuleId.PROJ_ZERO_LOC)
-            return [problem.Problem(rule.type, rule.severity, rule.msg.format(str(self)), concerned_object=self)]
+            return [problem.Problem(broken_rule=rule, msg=rule.msg.format(str(self)), concerned_object=self)]
         return []
 
     def __audit_never_analyzed(self) -> list[problem.Problem]:
         """Detects branches that have never been analyzed are are kept when inactive"""
         if not self.last_analysis() and self.is_kept_when_inactive():
             rule = rules.get_rule(rules.RuleId.BRANCH_NEVER_ANALYZED)
-            return [problem.Problem(rule.type, rule.severity, rule.msg.format(str(self)), concerned_object=self)]
+            return [problem.Problem(broken_rule=rule, msg=rule.msg.format(str(self)), concerned_object=self)]
         return []
 
     def get_measures(self, metrics_list):
         """Retrieves a branch list of measures
 
         :param metrics_list: List of metrics to return
         :type metrics_list: str (comma separated)
@@ -370,15 +372,15 @@
         if self.is_main():
             util.logger.debug("%s is main (not purgeable)", str(self))
         elif self.is_kept_when_inactive():
             util.logger.debug("%s is kept when inactive (not purgeable)", str(self))
         elif age > max_age:
             rule = rules.get_rule(rules.RuleId.BRANCH_LAST_ANALYSIS)
             msg = rule.msg.format(str(self), age)
-            problems.append(problem.Problem(rule.type, rule.severity, msg, concerned_object=self))
+            problems.append(problem.Problem(broken_rule=rule, msg=msg, concerned_object=self))
         else:
             util.logger.debug("%s age is %d days", str(self), age)
         return problems
 
     def audit(self, audit_settings):
         """Audits a branch and return list of problems found
```

## sonar/projects/projects.py

```diff
@@ -247,18 +247,22 @@
         """Retrieves a project list of measures
 
         :param list metrics_list: List of metrics to return
         :return: List of measures of a projects
         :rtype: dict
         """
         m = measures.get(self, metrics_list)
-        if "ncloc" in m:
+        if "ncloc" in m and m["ncloc"]:
             self.ncloc = 0 if not m["ncloc"].value else int(m["ncloc"].value)
         return m
 
+    def get_measures_history(self, metrics_list: list[str]) -> dict[str, str]:
+        """Returns the history of a project metrics"""
+        return measures.get_history(self, metrics_list)
+
     def branches(self, use_cache: bool = True) -> dict[str, branches.Branch]:
         """
         :return: Dict of branches of the project
         :param use_cache: Whether to use local cache or query SonarQube, default True (use cache)
         :type use_cache: bool
         :rtype: dict{<branchName>: <Branch>}
         """
@@ -372,26 +376,26 @@
         age = util.age(self.last_analysis(include_branches=True), True)
         if age is None:
             if not audit_settings.get("audit.projects.neverAnalyzed", True):
                 util.logger.debug("Auditing of never analyzed projects is disabled, skipping")
             else:
                 rule = rules.get_rule(rules.RuleId.PROJ_NOT_ANALYZED)
                 msg = rule.msg.format(str(self))
-                problems.append(pb.Problem(rule.type, rule.severity, msg, concerned_object=self))
+                problems.append(pb.Problem(broken_rule=rule, msg=msg, concerned_object=self))
             return problems
 
         max_age = audit_settings.get("audit.projects.maxLastAnalysisAge", 180)
         if max_age == 0:
             util.logger.debug("Auditing of projects with old analysis date is disabled, skipping")
         elif age > max_age:
             rule = rules.get_rule(rules.RuleId.PROJ_LAST_ANALYSIS)
             severity = severities.Severity.HIGH if age > 365 else rule.severity
             loc = self.get_measure("ncloc", fallback="0")
             msg = rule.msg.format(str(self), loc, age)
-            problems.append(pb.Problem(rule.type, severity, msg, concerned_object=self))
+            problems.append(pb.Problem(broken_rule=rule, severity=severity, msg=msg, concerned_object=self))
 
         util.logger.debug("%s last analysis is %d days old", str(self), age)
         return problems
 
     def __audit_branches(self, audit_settings):
         """Audits project branches
 
@@ -408,15 +412,15 @@
         main_br_count = 0
         for branch in self.branches().values():
             problems += branch.audit(audit_settings)
             if branch.name in ("main", "master"):
                 main_br_count += 1
                 if main_br_count > 1:
                     rule = rules.get_rule(rules.RuleId.PROJ_MAIN_AND_MASTER)
-                    problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format(str(self)), concerned_object=self))
+                    problems.append(pb.Problem(broken_rule=rule, msg=rule.msg.format(str(self)), concerned_object=self))
         return problems
 
     def __audit_pull_requests(self, audit_settings):
         """Audits project pul requests
 
         :param audit_settings: Settings (thresholds) to raise problems
         :type audit_settings: dict
@@ -443,15 +447,15 @@
         if not audit_settings.get("audit.projects.visibility", True):
             util.logger.debug("Project visibility audit is disabled by configuration, skipping...")
             return []
         util.logger.debug("Auditing %s visibility", str(self))
         visi = self.visibility()
         if visi != "private":
             rule = rules.get_rule(rules.RuleId.PROJ_VISIBILITY)
-            return [pb.Problem(rule.type, rule.severity, rule.msg.format(str(self), visi), concerned_object=self)]
+            return [pb.Problem(broken_rule=rule, msg=rule.msg.format(str(self), visi), concerned_object=self)]
         util.logger.debug("%s visibility is 'private'", str(self))
         return []
 
     def __audit_languages(self, audit_settings):
         """Audits project utility languages and returns problems if too many LoCs of these
 
         :param audit_settings: Settings (thresholds) to raise problems
@@ -472,15 +476,15 @@
         for lang in self.get_measure("ncloc_language_distribution").split(";"):
             (lang, ncloc) = lang.split("=")
             languages[lang] = int(ncloc)
             total_locs += int(ncloc)
         utility_locs = sum(lcount for lang, lcount in languages.items() if lang in ("xml", "json"))
         if total_locs > 100000 and (utility_locs / total_locs) > 0.5:
             rule = rules.get_rule(rules.RuleId.PROJ_UTILITY_LOCS)
-            return [pb.Problem(rule.type, rule.severity, rule.msg.format(str(self), utility_locs), concerned_object=self)]
+            return [pb.Problem(broken_rule=rule, msg=rule.msg.format(str(self), utility_locs), concerned_object=self)]
         util.logger.debug("%s utility LoCs count (%d) seems reasonable", str(self), utility_locs)
         return []
 
     def __audit_zero_loc(self, audit_settings):
         """Audits project utility projects with 0 LoCs
 
         :param audit_settings: Settings (thresholds) to raise problems
@@ -490,15 +494,15 @@
         """
         if (
             (not audit_settings.get(_AUDIT_BRANCHES_PARAM, True) or self.endpoint.edition() == "community")
             and self.last_analysis() is not None
             and self.loc() == 0
         ):
             rule = rules.get_rule(rules.RuleId.PROJ_ZERO_LOC)
-            return [pb.Problem(rule.type, rule.severity, rule.msg.format(str(self)), concerned_object=self)]
+            return [pb.Problem(broken_rule=rule, msg=rule.msg.format(str(self)), concerned_object=self)]
         return []
 
     def __audit_binding_valid(self, audit_settings):
         if self.endpoint.edition() == "community":
             util.logger.info("Community edition, skipping binding validation...")
             return []
         elif not audit_settings.get("audit.projects.bindings", True):
@@ -516,15 +520,15 @@
             _ = self.get("alm_settings/validate_binding", params={"project": self.key})
             util.logger.debug("%s binding is valid", str(self))
             return []
         except HTTPError as e:
             # Hack: 8.9 returns 404, 9.x returns 400
             if e.response.status_code in (HTTPStatus.BAD_REQUEST, HTTPStatus.NOT_FOUND):
                 rule = rules.get_rule(rules.RuleId.PROJ_INVALID_BINDING)
-                return [pb.Problem(rule.type, rule.severity, rule.msg.format(str(self)), concerned_object=self)]
+                return [pb.Problem(broken_rule=rule, msg=rule.msg.format(str(self)), concerned_object=self)]
             else:
                 util.exit_fatal(f"alm_settings/validate_binding returning status code {e.response.status_code}, exiting", options.ERR_SONAR_API)
 
     def audit(self, audit_settings):
         """Audits a project and returns the list of problems found
 
         :param dict audit_settings: Options of what to audit and thresholds to raise problems
@@ -806,17 +810,19 @@
         json_data["branches"] = self.__get_branch_export()
         json_data["tags"] = util.list_to_csv(self.tags(), separator=", ")
         json_data["visibility"] = self.visibility()
         (json_data["qualityGate"], qg_is_default) = self.quality_gate()
         if qg_is_default:
             json_data.pop("qualityGate")
 
-        json_data["webhooks"] = webhooks.export(self.endpoint, self.key)
+        hooks = webhooks.export(self.endpoint, self.key)
+        if hooks is not None:
+            json_data["webhooks"] = hooks
         json_data = util.filter_export(json_data, _IMPORTABLE_PROPERTIES, full)
-        settings_dict = settings.get_bulk(endpoint=self, component=self, settings_list=settings_list, include_not_set=False)
+        settings_dict = settings.get_bulk(endpoint=self.endpoint, component=self, settings_list=settings_list, include_not_set=False)
         # json_data.update({s.to_json() for s in settings_dict.values() if include_inherited or not s.inherited})
         for s in settings_dict.values():
             if not include_inherited and s.inherited:
                 continue
             json_data.update(s.to_json())
         return util.remove_nones(json_data)
 
@@ -1131,15 +1137,15 @@
     :type params: dict
     :return: list of projects
     :rtype: dict{key: Project}
     """
     new_params = {} if params is None else params.copy()
     new_params["qualifiers"] = "TRK"
     return sqobject.search_objects(
-        api="projects/search",
+        api=_SEARCH_API,
         params=new_params,
         key_field="key",
         returned_field="components",
         endpoint=endpoint,
         object_class=Project,
     )
 
@@ -1170,15 +1176,15 @@
         if project.endpoint.edition() == "community" or not audit_bindings or project.is_part_of_monorepo():
             queue.task_done()
             util.logger.debug("%s audit done", str(project))
             continue
         bindkey = project.binding_key()
         if bindkey and bindkey in bindings:
             rule = rules.get_rule(rules.RuleId.PROJ_DUPLICATE_BINDING)
-            results.append(pb.Problem(rule.type, rule.severity, rule.msg.format(str(project), str(bindings[bindkey])), concerned_object=project))
+            results.append(pb.Problem(broken_rule=rule, msg=rule.msg.format(str(project), str(bindings[bindkey])), concerned_object=project))
         else:
             bindings[bindkey] = project
         queue.task_done()
         util.logger.debug("%s audit done", str(project))
     util.logger.debug("Queue empty, exiting thread")
 
 
@@ -1212,15 +1218,15 @@
         util.logger.info("Project duplicates auditing was disabled by configuration")
         return problems
     for key, p in plist.items():
         util.logger.debug("Auditing for potential duplicate projects")
         for key2 in plist:
             if key2 != key and re.match(key2, key):
                 rule = rules.get_rule(rules.RuleId.PROJ_DUPLICATE)
-                problems.append(pb.Problem(rule.type, rule.severity, rule.msg.format(str(p), key2), concerned_object=p))
+                problems.append(pb.Problem(broken_rule=rule, msg=rule.msg.format(str(p), key2), concerned_object=p))
     return problems
 
 
 def __export_thread(queue, results, full):
     while not queue.empty():
         project = queue.get()
         results[project.key] = project.export(full=full)
```

## sonar/projects/pull_requests.py

```diff
@@ -74,15 +74,15 @@
         age = util.age(self.last_analysis())
         if age is None:  # Main branch not analyzed yet
             return []
         max_age = audit_settings.get("audit.projects.pullRequests.maxLastAnalysisAge", 30)
         problems = []
         if age > max_age:
             rule = rules.get_rule(rules.RuleId.PULL_REQUEST_LAST_ANALYSIS)
-            problems.append(problem.Problem(rule.type, rule.severity, rule.msg.format(str(self), age), concerned_object=self))
+            problems.append(problem.Problem(broken_rule=rule, msg=rule.msg.format(str(self), age), concerned_object=self))
         else:
             util.logger.debug("%s age is %d days", str(self), age)
         return problems
 
     def search_params(self):
         """Return params used to search for that object
```

## tools/audit.py

```diff
@@ -23,15 +23,15 @@
     Audits a SonarQube platform
 
 """
 import sys
 import datetime
 import json
 
-from sonar import platform, users, groups, version, qualityprofiles, qualitygates, sif, options, portfolios, applications, exceptions
+from sonar import platform, users, groups, qualityprofiles, qualitygates, sif, options, portfolios, applications, exceptions
 from sonar.projects import projects
 import sonar.utilities as util
 from sonar.audit import problem, config
 
 _ALL_AUDITABLE = [
     options.WHAT_SETTINGS,
     options.WHAT_USERS,
@@ -64,15 +64,17 @@
         raise
     except FileNotFoundError:
         util.logger.critical("File %s does not exist", sysinfo)
         raise
     except PermissionError:
         util.logger.critical("No permission to open file %s", sysinfo)
         raise
-    return sif.Sif(sysinfo).audit(audit_settings)
+    sif_obj = sif.Sif(sysinfo)
+    server_id = sif_obj.server_id()
+    return (server_id, sif_obj.audit(audit_settings))
 
 
 def _audit_sq(sq, settings, what_to_audit=None, key_list=None):
     problems = []
     if options.WHAT_PROJECTS in what_to_audit:
         problems += projects.audit(endpoint=sq, audit_settings=settings, key_list=key_list)
     if options.WHAT_PROFILES in what_to_audit:
@@ -103,64 +105,60 @@
     parser.add_argument(
         "--config",
         required=False,
         dest="config",
         action="store_true",
         help="Creates the $HOME/.sonar-audit.properties configuration file, if not already present or outputs to stdout if it already exist",
     )
-    args = parser.parse_args()
-    if args.sif is None and args.config is None and args.token is None:
-        util.exit_fatal(
-            "Token is missing (Argument -t/--token) when not analyzing local SIF",
-            options.ERR_TOKEN_MISSING,
-        )
+    args = util.parse_and_check(parser, verify_token=False)
+    if args.sif is None and args.config is None:
+        util.check_token(args.token)
     return args
 
 
 def main():
     args = __parser_args("Audits a SonarQube platform or a SIF (Support Info File or System Info File)")
     kwargs = vars(args)
-    sq = platform.Platform(some_url=args.url, some_token=args.token, cert_file=args.clientCert)
-    util.check_environment(kwargs)
-    util.logger.info("sonar-tools version %s", version.PACKAGE_VERSION)
+    sq = platform.Platform(some_url=args.url, some_token=args.token, cert_file=args.clientCert, http_timeout=args.httpTimeout)
     start_time = datetime.datetime.today()
 
     settings = config.load("sonar-audit")
     settings["threads"] = kwargs["threads"]
     if kwargs.get("config", False):
         config.configure()
         sys.exit(0)
 
     if kwargs.get("sif", None) is not None:
         err = options.ERR_SIF_AUDIT_ERROR
         try:
-            problems = _audit_sif(kwargs["sif"], settings)
+            (server_id, problems) = _audit_sif(kwargs["sif"], settings)
         except json.decoder.JSONDecodeError:
             util.exit_fatal(f"File {kwargs['sif']} does not seem to be a legit JSON file, aborting...", err)
         except FileNotFoundError:
             util.exit_fatal(f"File {kwargs['sif']} does not exist, aborting...", err)
         except PermissionError:
             util.exit_fatal(f"No permissiont to open file {kwargs['sif']}, aborting...", err)
         except sif.NotSystemInfo:
             util.exit_fatal(f"File {kwargs['sif']} does not seem to be a system info or support info file, aborting...", err)
     else:
+        server_id = sq.server_id()
         util.check_token(args.token)
         key_list = util.csv_to_list(args.projectKeys)
         if len(key_list) > 0 and "projects" in util.csv_to_list(args.what):
             for key in key_list:
                 if not projects.exists(key, sq):
                     util.exit_fatal(f"Project key '{key}' does not exist", options.ERR_NO_SUCH_KEY)
         try:
             problems = _audit_sq(sq, settings, what_to_audit=util.check_what(args.what, _ALL_AUDITABLE, "audited"), key_list=key_list)
         except exceptions.ObjectNotFound as e:
             util.exit_fatal(e.message, options.ERR_NO_SUCH_KEY)
 
     kwargs["format"] = __deduct_format__(args.format, args.file)
     ofile = kwargs.pop("file", None)
-    problem.dump_report(problems, ofile, **kwargs)
+    problem.dump_report(problems, ofile, server_id, **kwargs)
 
     util.logger.info("Total audit execution time: %s", str(datetime.datetime.today() - start_time))
     if problems:
         util.logger.warning("%d issues found during audit", len(problems))
     else:
         util.logger.info("%d issues found during audit", len(problems))
     sys.exit(0)
```

## tools/config.py

```diff
@@ -19,15 +19,15 @@
 # Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
 #
 """
     Exports SonarQube platform configuration as JSON
 """
 import sys
 import datetime
-from sonar import platform, version, rules, qualityprofiles, qualitygates, portfolios, applications, users, groups, options, utilities, exceptions
+from sonar import platform, rules, qualityprofiles, qualitygates, portfolios, applications, users, groups, options, utilities, exceptions
 from sonar.projects import projects
 
 _EVERYTHING = [
     options.WHAT_SETTINGS,
     options.WHAT_USERS,
     options.WHAT_GROUPS,
     options.WHAT_GATES,
@@ -94,40 +94,45 @@
         "--fullExport",
         required=False,
         default=False,
         action="store_true",
         help="Also exports informative data that would be ignored as part of an import. Informative field are prefixed with _."
         "This option is ignored in case of import",
     )
-    args = utilities.parse_and_check_token(parser)
-    utilities.check_environment(vars(args))
-    utilities.check_token(args.token)
-    utilities.logger.info("sonar-tools version %s", version.PACKAGE_VERSION)
+    args = utilities.parse_and_check(parser)
     return args
 
 
-def __export_config(endpoint, what, args):
-    key_list = utilities.csv_to_list(args.projectKeys)
-    if len(key_list) > 0 and "projects" in utilities.csv_to_list(args.what):
-        for key in key_list:
-            if not projects.exists(key, endpoint):
-                utilities.exit_fatal(f"Project key '{key}' does not exist", options.ERR_NO_SUCH_KEY)
+def __check_projects_existence(endpoint: object, key_list: list[str]) -> None:
+    for key in key_list:
+        if not projects.exists(key, endpoint):
+            utilities.exit_fatal(f"Project key '{key}' does not exist", options.ERR_NO_SUCH_KEY)
+
+
+def __export_config(endpoint: object, what: list[str], args: object) -> None:
+    """Exports a platform configuration in a JSON file"""
+    if "projects" in args.what:
+        __check_projects_existence(endpoint, args.projectKeys)
+
     utilities.logger.info("Exporting configuration from %s", args.url)
     sq_settings = {}
     sq_settings[__JSON_KEY_PLATFORM] = endpoint.basics()
     if options.WHAT_SETTINGS in what:
         sq_settings[__JSON_KEY_SETTINGS] = endpoint.export(full=args.fullExport)
     if options.WHAT_RULES in what:
         sq_settings[__JSON_KEY_RULES] = rules.export(endpoint, full=args.fullExport)
     if options.WHAT_PROFILES in what:
         if options.WHAT_RULES not in what:
             sq_settings[__JSON_KEY_RULES] = rules.export(endpoint, full=args.fullExport)
         sq_settings[__JSON_KEY_PROFILES] = qualityprofiles.export(endpoint, full=args.fullExport)
     if options.WHAT_GATES in what:
-        sq_settings[__JSON_KEY_GATES] = qualitygates.export(endpoint, full=args.fullExport)
+        if not endpoint.is_sonarcloud():
+            sq_settings[__JSON_KEY_GATES] = qualitygates.export(endpoint, full=args.fullExport)
+        else:
+            utilities.logger.warning("Quality gates export not yet supported for SonarCloud")
     if options.WHAT_PROJECTS in what:
         sq_settings[__JSON_KEY_PROJECTS] = projects.export(endpoint, key_list=args.projectKeys, full=args.fullExport, threads=args.threads)
     if options.WHAT_APPS in what:
         try:
             sq_settings[__JSON_KEY_APPS] = applications.export(endpoint, key_list=args.projectKeys, full=args.fullExport)
         except exceptions.UnsupportedOperation as e:
             utilities.logger.info("%s", e.message)
@@ -176,16 +181,19 @@
 def main():
     args = __parse_args("Extract SonarQube platform configuration")
     kwargs = vars(args)
     if not kwargs["export"] and not kwargs["import"]:
         utilities.exit_fatal("One of --export or --import option must be chosen", exit_code=options.ERR_ARGS_ERROR)
 
     start_time = datetime.datetime.today()
-    endpoint = platform.Platform(some_url=args.url, some_token=args.token, cert_file=args.clientCert)
+    endpoint = platform.Platform(
+        some_url=args.url, some_token=args.token, org=args.organization, cert_file=args.clientCert, http_timeout=args.httpTimeout
+    )
     what = utilities.check_what(args.what, _EVERYTHING, "exported or imported")
+    args.projectKeys = utilities.csv_to_list(args.projectKeys)
     if kwargs["export"]:
         try:
             __export_config(endpoint, what, args)
         except exceptions.ObjectNotFound as e:
             utilities.exit_fatal(e.message, options.ERR_NO_SUCH_KEY)
     if kwargs["import"]:
         __import_config(endpoint, what, args)
```

## tools/cust_measures.py

```diff
@@ -30,22 +30,22 @@
 
 def parse_args(desc):
     parser = utilities.set_common_args(desc)
     parser = utilities.set_key_arg(parser)
     parser.add_argument("-m", "--metricKey", required=True, help="What custom metric to work on")
     parser.add_argument("--value", required=False, help="Updates the value of the metric")
     parser.add_argument("--description", required=False, help="Updates the description of the metric")
-    return utilities.parse_and_check_token(parser)
+    return utilities.parse_and_check(parser)
 
 
 def main():
     args = parse_args("Manipulate custom metrics")
-    sqenv = platform.Platform(some_url=args.url, some_token=args.token)
+    sqenv = platform.Platform(some_url=args.url, some_token=args.token, cert_file=args.clientCert, http_timeout=args.httpTimeout)
     if sqenv.version() >= (9, 0, 0):
-        utilities.exit_fatal("Custom measures are no longer supported after 8.9.x", options.UnsupportedOperation)
+        utilities.exit_fatal("Custom measures are no longer supported after 8.9.x", options.ERR_UNSUPPORTED_OPERATION)
     else:
         utilities.logger.warning("Custom measures are are deprecated in 8.9 and lower and are dropped starting from SonarQube 9.0")
     # Remove unset params from the dict
     params = vars(args)
     for key in params.copy():
         if params[key] is None:
             del params[key]
```

## tools/findings_export.py

```diff
@@ -35,29 +35,34 @@
 """
 
 import sys
 import os
 import time
 import datetime
 from queue import Queue
+import threading
 from threading import Thread
 
-from sonar import platform, version, options, exceptions
+from sonar import platform, options, exceptions
 from sonar.projects import projects
 import sonar.utilities as util
 from sonar.findings import findings, issues, hotspots
 
 WRITE_END = object()
 TOTAL_FINDINGS = 0
+IS_FIRST = True
+TOTAL_SEM = threading.Semaphore()
+FIRST_SEM = threading.Semaphore()
+DATES_WITHOUT_TIME = False
 
 
 def parse_args(desc):
     parser = util.set_common_args(desc)
     parser = util.set_key_arg(parser)
-    parser = util.set_output_file_args(parser)
+    parser = util.set_output_file_args(parser, sarif_fmt=True)
     parser = options.add_thread_arg(parser, "findings search")
     parser.add_argument(
         "-b",
         "--branches",
         required=False,
         default=None,
         help="Comma separated list of branches to export. Use * to export findings from all branches. "
@@ -112,73 +117,142 @@
     parser.add_argument(
         "--" + options.WITH_URL,
         required=False,
         default=False,
         action="store_true",
         help="Generate finding URL in the report, false by default",
     )
-    args = util.parse_and_check_token(parser)
-    util.check_token(args.token)
+    parser.add_argument(
+        f"--{options.DATES_WITHOUT_TIME}",
+        action="store_true",
+        default=False,
+        required=False,
+        help="Reports timestamps only with date, not time",
+    )
+    args = util.parse_and_check(parser)
     return args
 
 
 def __write_header(file, format):
     util.logger.info("Dumping report to %s", f"file '{file}'" if file else "stdout")
     with util.open_file(file) as f:
-        print("[" if format == "json" else findings.to_csv_header(), file=f)
+        if format == "json":
+            print("[", file=f)
+        elif format == "sarif":
+            print(
+                """{
+   "version": "2.1.0",
+   "$schema": "https://schemastore.azurewebsites.net/schemas/json/sarif-2.1.0-rtm.4.json",
+   "runs": [
+       {
+          "tool": {
+            "driver": {
+                "name": "SonarQube",
+                "informationUri": "https://www.sonarsource.com/products/sonarqube/"
+            }
+          },
+          "results": [
+""",
+                file=f,
+            )
+        else:
+            print(findings.to_csv_header(), file=f)
 
 
 def __write_footer(file, format):
-    if format != "json":
+    if format == "csv":
         return
+    closing_sequence = ""
+    if format == "sarif":
+        closing_sequence = "\n]\n}\n]\n}"
+    elif format == "json":
+        closing_sequence = "\n]"
+    # Add closing sequence
     with util.open_file(file, mode="a") as f:
-        print("]\n", file=f)
+        print(f"{closing_sequence}", file=f)
+
 
+def __dump_findings(findings_list: list[object], file: str, file_format: str, **kwargs) -> None:
+    """Dumps a list of findings in a file. The findings are appended at the end of the file
 
-def __dump_findings(findings_list, file, file_format, is_last=False, **kwargs):
+    :param findings_list: List of findings
+    :type findings_list: Array
+    :param file: Filename to dump the findings
+    :type file: str
+    :param file_format: Format to dump (can be "csv", "json" or "sarif")
+    :type file_format: str
+    :return: Nothing
+    """
     i = len(findings_list)
-    util.logger.info("Writing %d more findings to %s", i, f"file '{file}'" if file else "stdout")
+    util.logger.info("Writing %d more findings to %s in format %s", i, f"file '{file}'" if file else "stdout", file_format)
     with util.open_file(file, mode="a") as f:
         url = ""
         sep = kwargs.get(options.CSV_SEPARATOR, ",")
         comma = ","
         for _, finding in findings_list.items():
             i -= 1
+            if i == 0:
+                comma = ""
             if file_format == "json":
-                finding_json = finding.to_json()
+                finding_json = finding.to_json(DATES_WITHOUT_TIME)
                 if not kwargs[options.WITH_URL]:
                     finding_json.pop("url", None)
-                if is_last and i == 0:
-                    comma = ""
                 print(f"{util.json_dump(finding_json, indent=1)}{comma}\n", file=f, end="")
+            elif file_format == "sarif":
+                finding_sarif = finding.to_sarif()
+                print(f"{util.json_dump(finding_sarif, indent=1)}{comma}\n", file=f, end="")
             else:
                 if kwargs[options.WITH_URL]:
                     url = f'{sep}"{finding.url()}"'
-                print(f"{finding.to_csv(sep)}{url}", file=f)
+                print(f"{finding.to_csv(sep, DATES_WITHOUT_TIME)}{url}", file=f)
+    util.logger.debug("File written")
 
 
 def __write_findings(queue, file_to_write, file_format, with_url, separator):
+    global IS_FIRST
+    global TOTAL_FINDINGS
     while True:
         while queue.empty():
             time.sleep(0.5)
-        (data, is_last) = queue.get()
+        (data, _) = queue.get()
         if data == WRITE_END:
+            util.logger.debug("End of write queue reached")
             queue.task_done()
             break
 
-        global TOTAL_FINDINGS
-        TOTAL_FINDINGS += len(data)
-        __dump_findings(data, file_to_write, file_format, is_last, withURL=with_url, csvSeparator=separator)
+        util.logger.debug("Processing write queue for project")
+        if len(data) == 0:
+            queue.task_done()
+            continue
+
+        if file_format in (None, "csv"):
+            __dump_findings(data, file_to_write, file_format, withURL=with_url, csvSeparator=separator)
+            queue.task_done()
+            with TOTAL_SEM:
+                TOTAL_FINDINGS += len(data)
+            continue
+
+        with FIRST_SEM:
+            if not IS_FIRST:
+                with util.open_file(file_to_write, mode="a") as f:
+                    print(",", file=f)
+            IS_FIRST = False
+
+        with TOTAL_SEM:
+            TOTAL_FINDINGS += len(data)
+
+        __dump_findings(data, file_to_write, file_format, withURL=with_url, csvSeparator=separator)
         queue.task_done()
+    util.logger.debug("End of write findings")
 
 
 def __dump_compact(finding_list, file, **kwargs):
     new_dict = {}
     for finding in finding_list.values():
-        f_json = finding.to_json()
+        f_json = finding.to_json(DATES_WITHOUT_TIME)
         if not kwargs[options.WITH_URL]:
             f_json.pop("url", None)
         pkey = f_json.pop("projectKey")
         ftype = f_json.pop("type")
         if pkey in new_dict:
             if ftype in new_dict[pkey]:
                 new_dict[pkey][ftype].append(f_json)
@@ -240,15 +314,15 @@
         if status_list or resol_list or type_list or sev_list:
             search_findings = False
 
         util.logger.debug("WriteQueue %s task %s put", str(write_queue), key)
         if search_findings:
             findings_list = findings.export_findings(endpoint, key, branch=params.get("branch", None), pull_request=params.get("pullRequest", None))
 
-            write_queue.put([findings_list, queue.empty()])
+            write_queue.put([findings_list, False])
         else:
             new_params = issues.get_search_criteria(params)
             new_params.update({"branch": params.get("branch", None), "pullRequest": params.get("pullRequest", None)})
             findings_list = {}
             if (i_statuses or not status_list) and (i_resols or not resol_list) and (i_types or not type_list) and (i_sevs or not sev_list):
                 findings_list = issues.search_by_project(key, params=new_params, endpoint=endpoint)
             else:
@@ -258,15 +332,15 @@
             if (h_statuses or not status_list) and (h_resols or not resol_list) and (h_types or not type_list) and (h_sevs or not sev_list):
                 new_params = hotspots.get_search_criteria(params)
                 new_params.update({"branch": params.get("branch", None), "pullRequest": params.get("pullRequest", None)})
                 findings_list.update(hotspots.search_by_project(key, endpoint=endpoint, params=new_params))
             else:
                 util.logger.debug("Status = %s, Types = %s, Resol = %s, Sev = %s", str(h_statuses), str(h_types), str(h_resols), str(h_sevs))
                 util.logger.info("Selected types, severities, resolutions or statuses disables issue search")
-            write_queue.put([findings_list, queue.empty()])
+            write_queue.put([findings_list, False])
         util.logger.debug("Queue %s task %s done", str(queue), key)
         queue.task_done()
 
 
 def store_findings(project_list, params, endpoint, file, format, threads=4, with_url=False, csv_separator=","):
     my_queue = Queue(maxsize=0)
     write_queue = Queue(maxsize=0)
@@ -298,42 +372,53 @@
     write_worker = Thread(target=__write_findings, args=[write_queue, file, format, with_url, csv_separator])
     write_worker.setDaemon(True)
     write_worker.setName("findingWriter")
     write_worker.start()
 
     my_queue.join()
     # Tell the writer thread that writing is complete
-    util.logger.debug("WriteQueue %s task %s put", str(write_queue), str(WRITE_END))
+    util.logger.debug("WriteQueue %s task WRITE_END put", str(write_queue))
     write_queue.put((WRITE_END, True))
     write_queue.join()
+    util.logger.debug("WriteQueue joined")
 
 
 def main():
-    kwargs = vars(parse_args("Sonar findings extractor"))
-    sqenv = platform.Platform(some_url=kwargs["url"], some_token=kwargs["token"], cert_file=kwargs["clientCert"])
+    global DATES_WITHOUT_TIME
+    kwargs = vars(parse_args("Sonar findings export"))
+    DATES_WITHOUT_TIME = kwargs[options.DATES_WITHOUT_TIME]
+    sqenv = platform.Platform(
+        some_url=kwargs["url"],
+        some_token=kwargs["token"],
+        org=kwargs["organization"],
+        cert_file=kwargs["clientCert"],
+        http_timeout=kwargs["httpTimeout"],
+    )
     del kwargs["token"]
-    util.check_environment(kwargs)
-    util.logger.info("sonar-tools version %s", version.PACKAGE_VERSION)
     start_time = datetime.datetime.today()
     params = util.remove_nones(kwargs.copy())
     __verify_inputs(params)
 
+    if util.is_sonarcloud_url(params["url"]) and params["useFindings"]:
+        util.logger.warning("--useFindings option is not available with SonarCloud, disabling the option to proceed")
+        params["useFindings"] = False
+
     for p in ("statuses", "createdAfter", "createdBefore", "resolutions", "severities", "types", "tags"):
         if params.get(p, None) is not None:
             if params["useFindings"]:
                 util.logger.warning("Selected search criteria %s will disable --useFindings", params[p])
             params["useFindings"] = False
             break
     try:
         project_list = projects.get_list(endpoint=sqenv, key_list=util.csv_to_list(kwargs.get("projectKeys", None)))
     except exceptions.ObjectNotFound as e:
         util.exit_fatal(e.message, options.ERR_NO_SUCH_KEY)
     fmt = kwargs.pop("format", None)
     fname = kwargs.pop("file", None)
-    if fname is not None:
+    if fmt is None and fname is not None:
         ext = fname.split(".")[-1].lower()
         if os.path.exists(fname):
             os.remove(fname)
         if ext in ("csv", "json"):
             fmt = ext
 
     util.logger.info("Exporting findings for %d projects with params %s", len(project_list), str(params))
```

## tools/findings_sync.py

```diff
@@ -24,15 +24,15 @@
     - One project to another (normally on different platforms but not necessarily).
       The 2 platform don't need to be identical in version, edition or plugins
     - One branch of a project to another branch of the same project (normally LLBs)
 
     Only issues with a 100% match are synchronized. When there's a doubt, nothing is done
 """
 
-from sonar import platform, version, syncer, options, exceptions
+from sonar import platform, syncer, options, exceptions
 from sonar.projects import projects
 from sonar.projects.branches import Branch
 import sonar.utilities as util
 
 _WITH_COMMENTS = {"additionalFields": "comments"}
 
 
@@ -75,16 +75,15 @@
         "--nolink",
         required=False,
         default=False,
         action="store_true",
         help="If specified, will not add a link to source issue in the target issue comments",
     )
 
-    args = util.parse_and_check_token(parser)
-    util.check_token(args.token)
+    args = util.parse_and_check(parser)
     return args
 
 
 def __dump_report(report, file):
     txt = util.json_dump(report)
     if file is None:
         util.logger.info("Dumping report to stdout")
@@ -97,18 +96,18 @@
 
 def main():
     args = __parse_args(
         "Synchronizes issues changelog of different branches of same or different projects, "
         "see: https://pypi.org/project/sonar-tools/#sonar-issues-sync"
     )
 
-    util.logger.info("sonar-tools version %s", version.PACKAGE_VERSION)
-    source_env = platform.Platform(some_url=args.url, some_token=args.token, cert_file=args.clientCert)
+    source_env = platform.Platform(
+        some_url=args.url, some_token=args.token, org=args.organization, cert_file=args.clientCert, http_timeout=args.httpTimeout
+    )
     params = vars(args)
-    util.check_environment(params)
     source_key = params["projectKeys"]
     target_key = params.get("targetProjectKey", None)
     source_branch = params.get("sourceBranch", None)
     target_branch = params.get("targetBranch", None)
     target_url = params.get("urlTarget", None)
 
     settings = {
@@ -141,15 +140,17 @@
             settings[syncer.SYNC_IGNORE_COMPONENTS] = target_key != source_key
             src_branch = Branch.get_object(projects.Project.get_object(key=source_key, endpoint=source_env), source_branch)
             tgt_branch = Branch.get_object(projects.Project.get_object(key=target_key, endpoint=source_env), target_branch)
             (report, counters) = src_branch.sync(tgt_branch, sync_settings=settings)
 
         elif target_url is not None and target_key is not None:
             util.check_token(args.tokenTarget)
-            target_env = platform.Platform(some_url=args.urlTarget, some_token=args.tokenTarget, cert_file=args.clientCert)
+            target_env = platform.Platform(
+                some_url=args.urlTarget, some_token=args.tokenTarget, org=args.organization, cert_file=args.clientCert, http_timeout=args.httpTimeout
+            )
             if not projects.exists(target_key, endpoint=target_env):
                 raise exceptions.ObjectNotFound(target_key, f"Project key '{target_key}' does not exist")
             settings[syncer.SYNC_IGNORE_COMPONENTS] = target_key != source_key
             if source_branch is not None or target_branch is not None:
                 util.logger.info("Syncing findings between main branch of 2 projects of different platforms")
                 src_branch = Branch.get_object(projects.Project.get_object(key=source_key, endpoint=source_env), source_branch)
                 tgt_branch = Branch.get_object(projects.Project.get_object(key=target_key, endpoint=target_env), target_branch)
```

## tools/housekeeper.py

```diff
@@ -23,15 +23,15 @@
     Removes obsolete data from SonarQube platform
     Currently:
     - projects, branches, PR not analyzed since a given number of days
     - Tokens not renewed since a given number of days
 
 """
 import sys
-from sonar import platform, tokens, users, groups, version, options
+from sonar import platform, tokens, users, groups, options
 from sonar.projects import projects, branches, pull_requests
 import sonar.utilities as util
 import sonar.exceptions as ex
 from sonar.audit import config, problem
 
 
 def get_project_problems(max_days_proj, max_days_branch, max_days_pr, nb_threads, endpoint):
@@ -44,22 +44,29 @@
         "audit.projects.maxLastAnalysisAge": max_days_proj,
         "audit.projects.branches.maxLastAnalysisAge": max_days_branch,
         "audit.projects.pullRequests.maxLastAnalysisAge": max_days_pr,
         "audit.projects.neverAnalyzed": False,
         "audit.projects.duplicates": False,
         "audit.projects.visibility": False,
         "audit.projects.permissions": False,
+        "audit.projects.failedTasks": False,
+        "audit.projects.exclusions": False,
+        "audit.project.scm.disabled": False,
+        "audit.projects.analysisWarnings": False,
     }
     settings = config.load(config_name="sonar-audit", settings=settings)
     settings["threads"] = nb_threads
     problems = projects.audit(endpoint=endpoint, audit_settings=settings)
     nb_proj = 0
     total_loc = 0
+    project_list = []
     for p in problems:
-        if p.concerned_object is not None and isinstance(p.concerned_object, projects.Project):
+        key = p.concerned_object.key if p.concerned_object is not None else None
+        if key not in project_list and isinstance(p.concerned_object, projects.Project):
+            project_list.append(key)
             nb_proj += 1
             total_loc += int(p.concerned_object.get_measure("ncloc", fallback="0"))
 
     if nb_proj == 0:
         util.logger.info("%d projects older than %d days found during audit", nb_proj, max_days_proj)
     else:
         util.logger.warning(
@@ -103,15 +110,15 @@
 
 
 def _parse_arguments():
     _DEFAULT_PROJECT_OBSOLESCENCE = 365
     _DEFAULT_BRANCH_OBSOLESCENCE = 90
     _DEFAULT_PR_OBSOLESCENCE = 30
     _DEFAULT_TOKEN_OBSOLESCENCE = 365
-    parser = util.set_common_args("Deletes projects not analyzed since a given numbr of days")
+    parser = util.set_common_args("Deletes projects, branches, PR, user tokens not used since a given number of days")
     parser = options.add_thread_arg(parser, "auditing before housekeeping")
     parser.add_argument(
         "--mode",
         required=False,
         choices=["dry-run", "delete"],
         default="dry-run",
         help="""
@@ -147,16 +154,15 @@
         "-T",
         "--tokens",
         required=False,
         type=int,
         default=_DEFAULT_TOKEN_OBSOLESCENCE,
         help=f"Deletes user tokens older than a certain number of days, by default {_DEFAULT_TOKEN_OBSOLESCENCE} days",
     )
-    args = util.parse_and_check_token(parser)
-    util.check_token(args.token)
+    args = util.parse_and_check(parser)
     return args
 
 
 def _delete_objects(problems, mode):
     revoked_token_count = 0
     deleted_projects = {}
     deleted_branch_count = 0
@@ -199,20 +205,16 @@
         revoked_token_count,
     )
 
 
 def main():
     args = _parse_arguments()
 
-    sq = platform.Platform(some_url=args.url, some_token=args.token, cert_file=args.clientCert)
-    kwargs = vars(args)
+    sq = platform.Platform(some_url=args.url, some_token=args.token, cert_file=args.clientCert, http_timeout=args.httpTimeout, org=args.organization)
     mode = args.mode
-    util.check_environment(kwargs)
-    util.logger.debug("Args = %s", str(kwargs))
-    util.logger.info("sonar-tools version %s", version.PACKAGE_VERSION)
     problems = []
     if args.projects > 0 or args.branches > 0 or args.pullrequests > 0:
         problems = get_project_problems(args.projects, args.branches, args.pullrequests, args.threads, sq)
 
     if args.tokens:
         problems += get_user_problems(args.tokens, sq)
```

## tools/loc.py

```diff
@@ -20,15 +20,15 @@
 #
 """
     Exports LoC per projects
 """
 import sys
 import csv
 
-from sonar import platform, portfolios, version, options
+from sonar import platform, portfolios, options
 from sonar.projects import projects
 import sonar.utilities as util
 
 
 def __deduct_format(fmt, file):
     if fmt is not None:
         return fmt
@@ -145,24 +145,23 @@
     parser.add_argument(
         "--topLevelOnly",
         required=False,
         default=False,
         action="store_true",
         help="Extracts only toplevel portfolios LoCs, not sub-portfolios",
     )
-    args = util.parse_and_check_token(parser)
-    util.check_environment(vars(args))
-    util.check_token(args.token)
-    util.logger.info("sonar-tools version %s", version.PACKAGE_VERSION)
+    args = util.parse_and_check(parser)
     return args
 
 
 def main():
     args = __parse_args("Extract projects or portfolios lines of code, as computed for the licence")
-    endpoint = platform.Platform(some_url=args.url, some_token=args.token, cert_file=args.clientCert)
+    endpoint = platform.Platform(
+        some_url=args.url, some_token=args.token, org=args.organization, cert_file=args.clientCert, http_timeout=args.httpTimeout
+    )
     kwargs = vars(args)
     ofile = kwargs.pop("file", None)
     args.format = __deduct_format(args.format, ofile)
 
     if args.portfolios:
         params = {}
         if args.topLevelOnly:
```

## tools/measures_export.py

```diff
@@ -21,15 +21,15 @@
 """
     Exports some measures of all projects
     - Either all measures (-m _all)
     - Or the main measures (-m _main)
     - Or a custom selection of measures (-m <measure1,measure2,measure3...>)
 """
 import sys
-from sonar import measures, metrics, platform, version, options, exceptions
+from sonar import measures, metrics, platform, options, exceptions
 from sonar.projects import projects
 import sonar.utilities as util
 
 RATINGS = "letters"
 PERCENTS = "float"
 DATEFMT = "datetime"
 CONVERT_OPTIONS = {"ratings": "letters", "percents": "float", "dates": "datetime"}
@@ -43,35 +43,68 @@
     if last_analysis is None:
         last_analysis = "Never"
     else:
         last_analysis = util.date_to_string(last_analysis, with_time)
     return last_analysis
 
 
-def __get_csv_header(wanted_metrics, edition, **kwargs):
-    sep = kwargs["csvSeparator"]
-    if edition == "community" or not kwargs[options.WITH_BRANCHES]:
-        header = f"# Project Key:1{sep}Project Name:2{sep}Last Analysis:3"
-        i = 4
-    else:
-        header = f"# Project Key:1{sep}Project Name:2{sep}Branch:3{sep}Last Analysis:4"
-        i = 5
-    for m in util.csv_to_list(wanted_metrics):
-        header += f"{sep}{m}:{i}"
-        i += 1
-    if kwargs[options.WITH_URL]:
-        header += f"{sep}URL:{i}"
-    return header
+def __get_object_measures_history(obj: object, wanted_metrics: list[str], **kwargs) -> dict[str, str]:
+    """Returns the measure history of an object (project, branch, application, portfolio)"""
+    data = {}
+    data["history"] = obj.get_measures_history(wanted_metrics)
+    if kwargs[options.DATES_WITHOUT_TIME]:
+        for item in data["history"]:
+            item[0] = item[0].split("T")[0]
+    data["url"] = obj.url()
+    proj = obj
+    if not isinstance(obj, projects.Project):
+        proj = obj.concerned_object
+        data["branch"] = obj.name
+    data["projectKey"] = proj.key
+    data["projectName"] = proj.name
+    return data
+
+
+def __get_json_measures_history(obj: object, wanted_metrics: list[str], **kwargs) -> dict[str, str]:
+    """Returns the measure history of an object (project, branch, application, portfolio) as JSON"""
+    d = __get_object_measures_history(obj, wanted_metrics, **kwargs)
+    if not kwargs[options.WITH_URL]:
+        d.pop("url", None)
+    if not kwargs[options.WITH_BRANCHES]:
+        d.pop("branch", None)
+    return d
+
+
+def __get_csv_measures_history(obj: object, wanted_metrics: list[str], **kwargs) -> str:
+    """Returns a CSV list of measures history of an object, as CSV string"""
+    data = __get_json_measures_history(obj, wanted_metrics, **kwargs)
+    sep = kwargs[options.CSV_SEPARATOR]
+
+    line = ""
+    projkey = data["projectKey"]
+    projname = data["projectName"]
+    branch = data.get("branch", "")
+    for m in data["history"]:
+        if m[1] == "quality_gate_details":
+            continue
+        if CONVERT_OPTIONS["dates"] == "dateonly":
+            m[0] = m[0].split("T")[0]
+        line += projkey + sep + projname + sep
+        if kwargs[options.WITH_BRANCHES]:
+            line += branch + sep
+        line += sep.join(m) + "\n"
+    return line[:-1]
 
 
 def __get_object_measures(obj, wanted_metrics):
     util.logger.info("Getting measures for %s", str(obj))
-    measures_d = {k: v.value if v else "" for k, v in obj.get_measures(wanted_metrics).items()}
+    measures_d = {k: v.value if v else None for k, v in obj.get_measures(wanted_metrics).items()}
     measures_d["lastAnalysis"] = __last_analysis(obj)
     measures_d["url"] = obj.url()
+    measures_d.pop("quality_gate_details", None)
     proj = obj
     if not isinstance(obj, projects.Project):
         proj = obj.concerned_object
         measures_d["branch"] = obj.name
     measures_d["projectKey"] = proj.key
     measures_d["projectName"] = proj.name
     return measures_d
@@ -88,15 +121,15 @@
 
 def __get_csv_measures(obj, wanted_metrics, **kwargs):
     measures_d = __get_object_measures(obj, wanted_metrics)
     sep = kwargs[options.CSV_SEPARATOR]
     overall_metrics = "projectKey" + sep + "projectName"
     if kwargs[options.WITH_BRANCHES]:
         overall_metrics += sep + "branch"
-    overall_metrics += sep + "lastAnalysis" + sep + wanted_metrics
+    overall_metrics += sep + "lastAnalysis" + sep + util.list_to_csv(wanted_metrics)
     if kwargs[options.WITH_BRANCHES]:
         overall_metrics += sep + "url"
     line = ""
     for metric in util.csv_to_list(overall_metrics):
         val = ""
         if metric in measures_d and measures_d[metric] is not None:
             if isinstance(measures_d[metric], str) and sep in measures_d[metric]:
@@ -115,15 +148,15 @@
         # Hack: With SonarQube 7.9 and below new_development_cost measure can't be retrieved
         if endpoint.version() < (8, 0, 0):
             all_metrics.pop("new_development_cost")
         util.logger.info("Exporting %s metrics", len(all_metrics))
         wanted_metrics = main_metrics + "," + util.list_to_csv(set(all_metrics) - set(metrics.MAIN_METRICS))
     elif wanted_metrics == "_main" or wanted_metrics is None:
         wanted_metrics = main_metrics
-    return wanted_metrics
+    return util.csv_to_list(wanted_metrics)
 
 
 def __get_fmt_and_file(args):
     kwargs = vars(args)
     fmt = kwargs["format"]
     fname = kwargs.get("file", None)
     if fname is not None:
@@ -171,93 +204,148 @@
         action="store_true",
         default=False,
         required=False,
         help="Reports percentages as string xy.z%% instead of float values 0.xyz",
     )
     parser.add_argument(
         "-d",
-        "--datesWithoutTime",
+        f"--{options.DATES_WITHOUT_TIME}",
         action="store_true",
         default=False,
         required=False,
         help="Reports timestamps only with date, not time",
     )
     parser.add_argument(
+        f"--{options.WITH_HISTORY}",
+        action="store_true",
+        default=False,
+        required=False,
+        help="Reports measures history not just last value",
+    )
+    parser.add_argument(
         "--" + options.WITH_URL,
         action="store_true",
         default=False,
         required=False,
         help="Add projects/branches URLs in report",
     )
-
-    args = util.parse_and_check_token(parser)
-    util.check_environment(vars(args))
-    util.check_token(args.token)
-    util.logger.info("sonar-tools version %s", version.PACKAGE_VERSION)
+    args = util.parse_and_check(parser)
     if args.ratingsAsNumbers:
         CONVERT_OPTIONS["ratings"] = "numbers"
     if args.percentsAsString:
         CONVERT_OPTIONS["percents"] = "percents"
     if args.datesWithoutTime:
         CONVERT_OPTIONS["dates"] = "dateonly"
 
     return args
 
 
+def __write_measures_history_csv(file: str, args: object, obj_list: list[object], wanted_metrics: list[str]) -> None:
+    """Writes measures history of object list in CSV format"""
+    kwargs = vars(args)
+    sep = kwargs[options.CSV_SEPARATOR]
+    if not kwargs[options.WITH_BRANCHES]:
+        header = f"# Project Key:1{sep}Project Name:2{sep}Last Analysis:3"
+        i = 4
+    else:
+        header = "# Project Key:1{sep}Project Name:2{sep}Branch:3{sep}Last Analysis:4"
+        i = 5
+    for m in util.csv_to_list(wanted_metrics):
+        header += f"{sep}{m}:{i}"
+        i += 1
+    if kwargs[options.WITH_URL]:
+        header += f"{sep}URL:{i}"
+    with util.open_file(file) as fd:
+        print(header, file=fd)
+        for obj in obj_list:
+            print(__get_csv_measures_history(obj, wanted_metrics, **vars(args)), file=fd)
+
+
+def __write_measures_history_json(file: str, args: object, obj_list: list[object], wanted_metrics: list[str]) -> None:
+    """Bl"""
+    is_first = True
+    with util.open_file(file) as fd:
+        print("[", end="", file=fd)
+        for obj in obj_list:
+            if not is_first:
+                print(",", end="", file=fd)
+            values = __get_json_measures_history(obj, wanted_metrics, **vars(args))
+            json_str = util.json_dump(values)
+            print(json_str, file=fd)
+            is_first = False
+        print("\n]\n", file=fd)
+
+
+def __write_measures_json(file: str, args: object, obj_list: list[object], wanted_metrics: list[str]) -> None:
+    """writes measures"""
+    is_first = True
+    with util.open_file(file) as fd:
+        print("[", end="", file=fd)
+
+        for obj in obj_list:
+            if not is_first:
+                print(",", end="", file=fd)
+            json_str = util.json_dump(__get_json_measures(obj, wanted_metrics, **vars(args)))
+            print(json_str, file=fd)
+            is_first = False
+        print("\n]\n", file=fd)
+
+
+def __write_measures_csv(file: str, args: object, obj_list: list[object], wanted_metrics: list[str]) -> None:
+    """writes measures in CSV"""
+    kwargs = vars(args)
+    sep = kwargs[options.CSV_SEPARATOR]
+    with_br = kwargs[options.WITH_BRANCHES]
+    base = f"# Project Key:1{sep}Project Name:2{sep}Date:3{sep}Metric:4{sep}Value:5"
+    with util.open_file(file) as fd:
+        if with_br:
+            print(f"{base}{sep}Branch:6", file=fd)
+        else:
+            print(base, file=fd)
+        for obj in obj_list:
+            print(__get_csv_measures(obj, wanted_metrics, **vars(args)), file=fd)
+
+
 def main():
     args = __parse_args("Extract measures of projects")
-    endpoint = platform.Platform(some_url=args.url, some_token=args.token, cert_file=args.clientCert)
+    endpoint = platform.Platform(
+        some_url=args.url, some_token=args.token, org=args.organization, cert_file=args.clientCert, http_timeout=args.httpTimeout
+    )
 
     with_branches = args.withBranches
     if endpoint.edition() == "community":
         with_branches = False
 
     wanted_metrics = __get_wanted_metrics(args, endpoint)
     (fmt, file) = __get_fmt_and_file(args)
 
     try:
         project_list = projects.get_list(endpoint=endpoint, key_list=args.projectKeys)
     except exceptions.ObjectNotFound as e:
         util.exit_fatal(e.message, options.ERR_NO_SUCH_KEY)
-    is_first = True
     obj_list = []
     if with_branches:
         for project in project_list.values():
             obj_list += project.branches().values()
     else:
         obj_list = project_list.values()
     nb_branches = len(obj_list)
 
-    with util.open_file(file) as fd:
+    if endpoint.edition() == "community":
+        args.withBranches = False
+    if args.history:
         if fmt == "json":
-            print("[", end="", file=fd)
+            __write_measures_history_json(file, args, obj_list, wanted_metrics)
         else:
-            print(
-                __get_csv_header(wanted_metrics, endpoint.edition(), **vars(args)),
-                file=fd,
-            )
-
-        for obj in obj_list:
-            if fmt == "json":
-                if not is_first:
-                    print(",", end="", file=fd)
-                values = __get_json_measures(obj, wanted_metrics, **vars(args))
-                json_str = util.json_dump(values)
-                print(json_str, file=fd)
-                is_first = False
-            else:
-                print(__get_csv_measures(obj, wanted_metrics, **vars(args)), file=fd)
-
+            __write_measures_history_csv(file, args, obj_list, wanted_metrics)
+    else:
         if fmt == "json":
-            print("\n]\n", file=fd)
-
-    util.logger.info("Computing LoCs")
-    nb_loc = 0
-    for project in project_list.values():
-        nb_loc += project.loc()
+            __write_measures_json(file, args, obj_list, wanted_metrics)
+        else:
+            __write_measures_csv(file, args, obj_list, wanted_metrics)
 
-    util.logger.info("%d PROJECTS %d branches %d LoCs", len(project_list), nb_branches, nb_loc)
-    sys.exit(0)
+        util.logger.info("%d PROJECTS %d branches", len(project_list), nb_branches)
+        sys.exit(0)
 
 
 if __name__ == "__main__":
     main()
```

## tools/projects_export.py

```diff
@@ -21,15 +21,15 @@
 """
 
     Exports all projects of a SonarQube platform
 
 """
 import sys
 import datetime
-from sonar import options, platform, utilities, version
+from sonar import options, platform, utilities
 from sonar.projects import projects
 
 
 def main():
     parser = utilities.set_common_args("Exports all projects of a SonarQube platform")
     parser = utilities.set_key_arg(parser)
     parser = utilities.set_output_file_args(parser, json_fmt=True, csv_fmt=False)
@@ -37,20 +37,17 @@
     parser.add_argument(
         "--exportTimeout",
         required=False,
         type=int,
         default=180,
         help="Maximum wait time for export",
     )
-    args = utilities.parse_and_check_token(parser)
-    utilities.check_environment(vars(args))
-    utilities.check_token(args.token)
-    utilities.logger.info("sonar-tools version %s", version.PACKAGE_VERSION)
+    args = utilities.parse_and_check(parser)
     start_time = datetime.datetime.today()
-    sq = platform.Platform(some_url=args.url, some_token=args.token, cert_file=args.clientCert)
+    sq = platform.Platform(some_url=args.url, some_token=args.token, cert_file=args.clientCert, http_timeout=args.httpTimeout)
 
     if sq.edition() in ("community", "developer") and sq.version(digits=2) < (9, 2):
         utilities.exit_fatal(
             "Can't export projects on Community and Developer Edition before 9.2, aborting...",
             options.ERR_UNSUPPORTED_OPERATION,
         )
```

## tools/projects_import.py

```diff
@@ -21,15 +21,15 @@
 """
 
     Imports a list of projects to a SonarQube platform
 
 """
 import sys
 import json
-from sonar import options, platform, version, exceptions
+from sonar import options, platform, exceptions
 from sonar.projects import projects
 import sonar.utilities as util
 
 
 def _check_sq_environments(import_sq, export_sq):
     imp_version = import_sq.version(digits=2, as_string=True)
     if imp_version != export_sq["version"]:
@@ -51,19 +51,16 @@
                 options.ERR_UNSUPPORTED_OPERATION,
             )
 
 
 def main():
     parser = util.set_common_args("Imports a list of projects in a SonarQube platform")
     parser.add_argument("-f", "--projectsFile", required=True, help="File with the list of projects")
-    args = util.parse_and_check_token(parser)
-    util.check_environment(vars(args))
-    util.check_token(args.token)
-    util.logger.info("sonar-tools version %s", version.PACKAGE_VERSION)
-    sq = platform.Platform(some_url=args.url, some_token=args.token, cert_file=args.clientCert)
+    args = util.parse_and_check(parser)
+    sq = platform.Platform(some_url=args.url, some_token=args.token, cert_file=args.clientCert, http_timeout=args.httpTimeout)
 
     with open(args.projectsFile, "r", encoding="utf-8") as file:
         data = json.load(file)
     project_list = data["project_exports"]
     _check_sq_environments(sq, data["sonarqube_environment"])
 
     nb_projects = len(project_list)
```

## tools/support.py

```diff
@@ -25,15 +25,15 @@
 """
 from http import HTTPStatus
 import sys
 import os
 import json
 import argparse
 import requests
-from sonar import version, sif, options
+from sonar import sif, options
 from sonar.audit import severities
 import sonar.utilities as util
 from sonar.audit import problem, config
 
 PRIVATE_COMMENT = [{"key": "sd.public.comment", "value": {"internal": "true"}}]
 
 
@@ -80,33 +80,33 @@
     return args
 
 
 def __get_issue_id(**kwargs):
     """Converts a ticket number into issue id needed to post on the issue"""
     tix = kwargs["ticket"]
     url = f'{kwargs["url"]}/rest/servicedeskapi/request/{tix}'
-    r = requests.get(url, auth=kwargs["creds"])
+    r = requests.get(url, auth=kwargs["creds"], timeout=10)
     if not r.ok:
         if r.status_code == HTTPStatus.NOT_FOUND:
             return None
         else:
             util.exit_fatal(f"Ticket {tix}: URL '{url}' status code {r.status_code}", options.ERR_SONAR_API)
     return json.loads(r.text)["issueId"]
 
 
 def __add_comment(comment, **kwargs):
     url = f'{kwargs["url"]}/rest/api/2/issue/{__get_issue_id(**kwargs)}/comment'
-    requests.post(url, auth=kwargs["creds"], json={"body": comment, "properties": PRIVATE_COMMENT})
+    requests.post(url, auth=kwargs["creds"], json={"body": comment, "properties": PRIVATE_COMMENT}, timeout=10)
 
 
 def __get_sysinfo_from_ticket(**kwargs):
     tix = kwargs["ticket"]
     url = f"{kwargs['url']}/rest/servicedeskapi/request/{tix}"
     util.logger.debug("Check %s - URL %s", kwargs["ticket"], url)
-    r = requests.get(url, auth=kwargs["creds"])
+    r = requests.get(url, auth=kwargs["creds"], timeout=10)
     if not r.ok:
         if r.status_code == HTTPStatus.NOT_FOUND:
             print(f"Ticket {tix} not found")
             sys.exit(3)
         else:
             util.exit_fatal(f"Ticket {tix}: URL '{url}' status code {r.status_code}", options.ERR_SONAR_API)
 
@@ -119,29 +119,27 @@
         for v in d["value"]:
             file_type = v["filename"].split(".")[-1].lower()
             if file_type not in ("json", "txt"):
                 continue
             attachment_url = v["content"]
             attachment_file = attachment_url.split("/")[-1]
             util.logger.info("Ticket %s: Verifying attachment '%s' found", tix, attachment_file)
-            r = requests.get(attachment_url, auth=kwargs["creds"])
+            r = requests.get(attachment_url, auth=kwargs["creds"], timeout=10)
             if not r.ok:
                 util.exit_fatal(f"ERROR: Ticket {tix} get attachment status code {r.status_code}", options.ERR_SONAR_API)
             try:
                 sif_list[attachment_file] = json.loads(r.text)
             except json.decoder.JSONDecodeError:
                 util.logger.info("Ticket %s: Attachment '%s' is not a JSON file, skipping", tix, attachment_file)
                 continue
     return sif_list
 
 
 def main():
     kwargs = vars(__get_args("Audits a Sonar ServiceDesk ticket (Searches for SIF attachment and audits SIF)"))
-    util.check_environment(kwargs)
-    util.logger.info("sonar-tools version %s", version.PACKAGE_VERSION)
     kwargs["creds"] = (kwargs.pop("login"), kwargs.pop("password"))
     if not kwargs["ticket"].startswith("SUPPORT-"):
         kwargs["ticket"] = f'SUPPORT-{kwargs["ticket"]}'
     sif_list = __get_sysinfo_from_ticket(**kwargs)
     if len(sif_list) == 0:
         print(f"No SIF found in ticket {kwargs['ticket']}")
         sys.exit(2)
```

## Comparing `sonar_tools-2.9.data/scripts/sonar-tools` & `sonar_tools-3.0.data/scripts/sonar-tools`

 * *Files 8% similar despite different names*

```diff
@@ -14,27 +14,32 @@
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
 # Lesser General Public License for more details.
 #
 # You should have received a copy of the GNU Lesser General Public License
 # along with this program; if not, write to the Free Software Foundation,
 # Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
 #
+
+"""Main entry point for sonar-tools"""
+
 from sonar import version
 
 print(f'''
 sonar-tools version {version.PACKAGE_VERSION}
 Collections of utilities for SonarQube:
 - sonar-audit: Audits a SonarQube platform for bad practices, performance, configuration problems
 - sonar-housekeeper: Deletes projects that have not been analyzed since a given number of days
 - sonar-loc: Produces a list of projects with their LoC count as computed by the SonarQube
   commercial licenses (ie taking the largest branch or PR)
 - sonar-measures-export: Exports measures/metrics of one, several or all projects of the platform in CSV or JSON
+  (Can also export measures history)
 - sonar-findings-export: Exports findings (potentially filtered) from the platform in CSV or JSON
   (also available as sonar-issues-export for backward compatibility, but deprecated)
 - sonar-findings-sync: Synchronizes issues between 2 branches of a same project, a whole project
   branches of 2 different projects (potentially on different platforms).
   (also available as sonar-issues-sync for backward compatibility, but deprecated)
 - sonar-projects-export: Exports all projects from a platform (All editions)
 - sonar-projects-import: Imports a list of projects into a platform (EE and higher)
 - sonar-config: Exports and imports an entire (or subsets of a) SonarQube platform configuration as code (JSON)
+- sonar-rules: Exports Sonar rules
 See tools built-in -h help and https://github.com/okorach/sonar-tools for more documentation
-''')
+''')
```

## Comparing `sonar_tools-2.9.dist-info/LICENSE` & `sonar_tools-3.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `sonar_tools-2.9.dist-info/METADATA` & `sonar_tools-3.0.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: sonar-tools
-Version: 2.9
+Version: 3.0
 Summary: A collection of utility scripts for SonarQube
 Home-page: https://github.com/okorach/sonar-tools
 Author: Olivier Korach
 Author-email: olivier.korach@gmail.com
 License: UNKNOWN
 Project-URL: Bug Tracker, https://github.com/okorach/sonar-tools/issues
 Project-URL: Documentation, https://github.com/okorach/sonar-tools/README.md
@@ -12,15 +12,14 @@
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3
 Classifier: License :: OSI Approved :: GNU Lesser General Public License v3 or later (LGPLv3+)
 Classifier: Operating System :: OS Independent
 Requires-Python: >=3.6
 Description-Content-Type: text/markdown
 License-File: LICENSE
-Requires-Dist: pytz
 Requires-Dist: argparse
 Requires-Dist: datetime
 Requires-Dist: python-dateutil
 Requires-Dist: requests
 Requires-Dist: jprops
 
 # sonar-tools
@@ -39,23 +38,24 @@
 
 **DISCLAIMER**: This software is community software. None of the tools it contains are neither supported nor endorsed by SonarSource S.A. Switzerland, the company publishing the [SonarQube](https://www.sonarqube.org/), [SonarCloud](https://sonarcloud.io) and [SonarLint](https://sonarlint.org) products
 
 The following utilities are available:
 - [sonar-audit](#sonar-audit): Audits a SonarQube instance, and reports all the problems
 - [sonar-housekeeper](#sonar-housekeeper): Deletes projects, branches, PR  that have not been analyzed since a certain number of days, or
 deletes tokens created since more than a certain number of days
-- [sonar-loc](#sonar-loc): Computes lines of code per project and in total, as they would be coputed by the license
+- [sonar-loc](#sonar-loc): Computes lines of code per project and in total, as they would be computed by SonarQube (and the licensing system on commercial editions)
 - [sonar-measures-export](#sonar-measures-export): Exports measures/metrics of one, several or all projects of the instance in CSV
 - [sonar-findings-export](#sonar-findings-export) (Also available as **sonar-issues-export** (deprecated) for backward compatibility): Exports issues and hotspots (potentially filtered) from the instance in CSV
 - [sonar-findings-sync](#sonar-findings-sync): Synchronizes issues and hotspots changelog between branches, projects or even SonarQube instances (formerly **sonar-issues-sync**, now deprecated)
 - [sonar-projects-export](#sonar-projects-export): Exports all projects from a SonarQube instance (EE and higher)
 - [sonar-projects-import](#sonar-projects-import): Imports a list of projects into a SonarQube instance (EE and higher)
 - [sonar-config](#sonar-config): Exports or Imports a SonarQube platform configuration to/from configuration as code file (JSON file).
 
-:information_source: Although they are likely to work with many versions, the offered tools are **only tested against SonarQube LTS (Long Term Support, currently 9.9.x) and LATEST versions**
+:information_source: Although they are likely to work with many versions, the offered tools are **only tested against SonarQube LTA (Long Term Active, currently 9.9.x) and LATEST versions**
+
 :warning: **sonar-tools** 2.7 or higher is required for compatibility with SonarQube 10
 
 # What's New - Release notes
 - [What's new](https://github.com/okorach/sonar-tools/blob/master/doc/what-is-new.md)
 - [Release notes](https://github.com/okorach/sonar-tools/releases)
 
 # Requirements and Installation
@@ -64,28 +64,32 @@
 - Online installation.
   - Run: `python3 -m pip install sonar-tools` (or `python3 -m pip upgrade sonar-tools`)
   If install does not behave as expected you can try the **pip** `--force-reinstall` option (see **pip** documentation)
 - Offline installation: If you have no access to the internet on the install machine, you can:
   - Download the `.whl` file from https://pypi.org/project/sonar-tools or attached to the release at https://github.com/okorach/sonar-tools/releases. The file should be something like. **sonar_tools-\<VERSION\>-py3-none-any.whl**
   - Copy the downloaded file on the install machine
   - On the install machine, run `python3 -m pip install sonar_tools-<VERSION>-py3-none-any.whl`
-  - Note: The package is dependent upon `pytz`, `argparse`, `datetime`, `python-dateutil`, `requests` and `jprops` python packages that are automatically installed when installing `sonar-tools`
+  - Note: The package is dependent upon `argparse`, `datetime`, `python-dateutil`, `requests` and `jprops` python packages that are automatically installed when installing `sonar-tools`
 
 # Common command line parameters
 
 All tools accept the following common parameters:
 - `-h` : Displays a help and exits
 - `-u` : URL of the SonarQube server. The default is environment variable `$SONAR_HOST_URL`
 or `http://localhost:9000` by default if the environment variable is not set
 - `-t` : User token to invoke the SonarQube APIs, like `squ_83356c9b2db891d45da2a119a29cdc4d03fe654e`.
 The default is environment variable `$SONAR_TOKEN`.
 Using login/password is not possible.
 The user corresponding to the token must have enough permissions to achieve the tool tasks
+- `-o` : Organization, for SonarCloud - Ignored if running against a SonarQube instance
 - `-v` : Logging verbosity level (`WARN`, `ÌNFO` or `DEBUG`). The default is `INFO`.
 `ERROR` and above is always active.
+- `-c` or `--clientCert` : Allows to specify an optional client certificate file (as .pem file)
+- `--httpTimeout` : Sets the timeout for HTTP(S) requests to the SonarQube platform
+- `--skipVersionCheck` : Starting with **sonar-tools** 2.11, by default all sonar tools occasionnally check on pypi.org if there is a new version of **sonar-tools** available, and output a warning log if that is the case. You can skip this check with this option.
 
 See common [error exit codes](#exit-codes) at the bottom of this page
 
 # <a name="sonar-audit"></a>sonar-audit
 
 `sonar-audit` allows to audit a SonarQube instance and output warning logs for all anomalies found.
 See [complete documentation](https://github.com/okorach/sonar-tools/blob/master/doc/sonar-audit.md) for details
@@ -132,74 +136,76 @@
 ### Example
 ```
 sonar-housekeeper -u https://sonar.acme-corp.com -t 15ee09df11fb9b8234b7a1f1ac5fce2e4e93d75d
 ```
 
 # <a name="sonar-loc"></a>sonar-loc
 
-Exports all projects lines of code as they would be counted by the commercial licences.  
+Exports all projects lines of code as they would be counted by the commercial licences.
 See `sonar-loc -h` for details
 
-Basic Usage: `sonar-loc [-f <file>] [--format json|csv] [-a] [-n] [--withURL] [--portfolios] [--topLevelOnly]`  
+Basic Usage: `sonar-loc [-f <file>] [--format json|csv] [-a] [-n] [--withURL] [--portfolios] [--topLevelOnly]`
 - `-f`: Define file for output (default stdout). File extension is used to deduct expected format (json if file.json, csv otherwise)
 - `--format`: Choose export format between csv (default) and json
 - `--portfolios`: Output the LOC of portfolios instead of projects (Enterprise Edition only)
 - `--topLevelOnly`: For portfolios, only output LoCs for top level portfolios (Enterprise Edition only)
 - `-n | --withName`: Outputs the project or portfolio name in addition to the key
 - `-a | --withLastAnalysis`: Output the last analysis date (all branches and PR taken into account) in addition to the LOCs
 - `--withURL`: Outputs the URL of the project or portfolio for each record
 
 ## Required Permissions
 
 `sonar-loc` needs `Browse` permission on all projects of the SonarQube instance
 
 # <a name="sonar-measures-export"></a>sonar-measures-export
 
-Exports one or all projects with all (or some selected) measures in a CSV file.  
-The CSV is sent to standard output.  
+Exports one or all projects with all (or some selected) measures in a CSV file.
+The CSV is sent to standard output.
 Plenty of issue filters can be specified from the command line, type `sonar-measures-export -h` for details
 
-Basic Usage: `sonar-measures-export -m _main [-f <file>] [--format json|csv] [-b] [-r] [-p] [-d] [-d] [-n] [-a] [--withURL]`  
+Basic Usage: `sonar-measures-export -m _main [-f <file>] [--format json|csv] [-b] [-r] [-p] [-d] [-d] [-n] [-a] [--withURL]`
 - `-m | --metricKeys`: comma separated list of metrics to export
-  - `-m _main` is a shortcut to list all main metrics. It's the recommended option  
+  - `-m _main` is a shortcut to list all main metrics. It's the recommended option
   - `-m _all` is a shortcut to list all metrics, including the most obscure ones
 - `-f`: Define file for output (default stdout). File extension is used to deduct expected format (json if file.json, csv otherwise)
 - `--format`: Choose export format between csv (default) and json
 - `-b | --withBranches`: Exports measures for all project branches (by default only export measures of the main branch)
 - `-r | --ratingsAsNumbers`: Converts ratings as numbers (by default ratings are exported as letters between A and E)
 - `-p | --percentsAsString`: Converts percentages as strings "xy.z%" (by default percentages are exported as floats between 0 and 1)
 - `-d | --datesWithoutTime`: Outputs dates without time
 - `-n | --withName`: Outputs the project or portfolio name in addition to the key
 - `-a | --withLastAnalysis`: Output the last analysis date (all branches and PR taken into account) in addition to the LOCs
 - `--withURL`: Outputs the URL of the project or portfolio for each record
+- `--history`: Export measures history instead of only the last value
+
 
 ## Required Permissions
 
 `sonar-measures-export` needs `Browse` permission on all projects of the SonarQube instance
 
 ## Examples
 ```
 export SONAR_HOST_URL=https://sonar.acme-corp.com
 export SONAR_TOKEN=squ_83356c9b2db891d45da2a119a29cdc4d03fe654e
 
 # Exports LoCs, nbr of bugs and number of vulnerabilities of all projects main branch
 sonar-measures-export -m ncloc,bugs,vulnerabilities >measures.csv
 
 # Exports main metrics of all projects and all their branches
-sonar-measures-export -m _main -b -o measures.json
+sonar-measures-export -m _main -b -f measures.json
 
 # Exports all metrics of projects myProjectKey1 and myOtherProjectKey main branch. Convert ratings to letters
-sonar-measures-export -k myProjectKey1,myOtherProjectKey -m _all -r -o all_measures.csv
+sonar-measures-export -k myProjectKey1,myOtherProjectKey -m _all -r -f all_measures.csv
 ```
 
 # <a name="sonar-findings-export"></a>sonar-findings-export
 (Also available as `sonar-issues-export` for backward compatibility, but **deprecated**)
 
-Exports a list of issues as CSV  or JSON. The export is sent to standard output or into a file
-Plenty of issue filters can be specified from the command line, type `sonar-findings-export -h` for details.  
+Exports a list of issues as CSV, JSON or SARIF format. The export is sent to standard output or into a file
+Plenty of issue filters can be specified from the command line, type `sonar-findings-export -h` for details.
 :warning: On large SonarQube instances with a lot of issues, it can be stressful for the instance (many API calls) and very long to export all issues. It's recommended to define filters that will only export a subset of all issues (see examples below).
 
 ## Required Permissions
 
 `sonar-findings-export` needs `Browse` permission on all projects for which findings are exported
 
 ## Examples
@@ -207,34 +213,37 @@
 export SONAR_HOST_URL=https://sonar.acme-corp.com
 export SONAR_TOKEN=squ_83356c9b2db891d45da2a119a29cdc4d03fe654e
 
 # Exports all issues (main branch). This can be very long and stressful for SonarQube APIs
 sonar-findings-export >all_issues.csv
 
 # Exports all issues of project myProjectKey
-sonar-findings-export -k myProjectKey -o project_issues.csv
+sonar-findings-export -k myProjectKey -f project_issues.csv
 
 # Exports all false positive and won't fix issues across all projects
-sonar-findings-export -r FALSE-POSITIVE,WONTFIX -o fp_wf.json
+sonar-findings-export -r FALSE-POSITIVE,WONTFIX -f fp_wf.json
 
 # Exports all issues created in 2020
-sonar-findings-export -a 2020-01-01 -b 2020-12-31 -o issues_created_in_2020.csv
+sonar-findings-export -a 2020-01-01 -b 2020-12-31 -f issues_created_in_2020.csv
 
 # Exports all vulnerabilities and bugs
 sonar-findings-export -types VULNERABILITY,BUG -f json >bugs_and_vulnerabilities.json
+
+# Exports all vulnerabilities and bugs in SARIF format
+sonar-findings-export -types VULNERABILITY,BUG -f json --format sarif >bugs_and_vulnerabilities.sarif.json
 ```
 
 # <a name="sonar-projects-export"></a>sonar-projects-export
 
-Exports all projects of a given SonarQube instance.  
+Exports all projects of a given SonarQube instance.
 It sends to the output a CSV or JSON with the list of project keys, the export result (`SUCCESS` or `FAIL`), and:
 - If the export was successful, the generated zip file
 - If the export was failed, the failure reason
 
-Basic Usage: `sonar-projects-export [--exportTimeout <timeout>] >exported_projects.csv`  
+Basic Usage: `sonar-projects-export [--exportTimeout <timeout>] >exported_projects.csv`
 - `--exportTimeout`: Defines timeout to export a single project in seconds,
                      by default 180 s (large projects can take time to export)
 - `-f`: Define file for output (default stdout). File extension is used to deduct expected format (json if file.json, csv otherwise)
 
 :information_source: All zip files are generated in the SonarQube instance standard location (under `data/governance/project_dumps/export`). On a DCE, the export may be distributed over all the Application Nodes
 :warning: **sonar-tools** 2.7 or higher is required for compatibility with SonarQube 10
 
@@ -254,20 +263,20 @@
 sonar-projects-export -k myProjectKey1,myOtherProjectKey -f exports.csv
 # Exports all projects, with results of export in JSON file exported_projects.json
 sonar-projects-export -f exported_projects.json
 ```
 
 # <a name="sonar-projects-import"></a>sonar-projects-import
 
-Imports a list of projects previously exported with `sonar-projects-export`.  
-:warning: Unlike projects export that is available on all editions, project import requires a SonarQube Enterprise or Data Center Edition.  
+Imports a list of projects previously exported with `sonar-projects-export`.
+:warning: Unlike projects export that is available on all editions, project import requires a SonarQube Enterprise or Data Center Edition.
 It takes as input a CSV or JSON file produced by `sonar-projects-export`
 :warning: **sonar-tools** 2.7 or higher is required for compatibility with SonarQube 10
 
-Basic Usage: `sonar-projects-import -f <file.csv>`  
+Basic Usage: `sonar-projects-import -f <file.csv>`
 - `-f`: Define input file for project import, result of a `sonar-projects-export` command
 
 :information_source: All exported zip files must be first copied to the right location on the target SonarQube instance for the import to be successful (In `data/governance/project_dumps/import`)
 
 ## Required Permissions
 
 `sonar-projects-import` needs the global `Create Projects` permission
@@ -284,15 +293,15 @@
 ```
 
 # <a name="sonar-config"></a>sonar-config
 
 Exports or imports all or part of a SonarQube platform configuration.
 `sonar-config` is expected to export/import everything that is configurable in a SonarQube platform, except secrets
 
-Basic Usage: `sonar-config --export -f <file.json>`  
+Basic Usage: `sonar-config --export -f <file.json>`
 - `-f`: Define the output file, if not specified `stdout` is used
 - `-e` or `--export`: Specify the export operation
 - `-w` or `--what`: Specify what to export (everything by default)
 - `k "<key1>,<key2>,...,<keyn>"`: Will only import/export projects, apps or portfolios with matching keys
 - `--fullExport`: Will also export object properties that are not used for an import by may be of interest anyway
 See [sonar-config complete doc](https://github.com/okorach/sonar-tools/blob/master/doc/sonar-config.md)
```

## Comparing `sonar_tools-2.9.dist-info/entry_points.txt` & `sonar_tools-3.0.dist-info/entry_points.txt`

 * *Files 8% similar despite different names*

```diff
@@ -7,9 +7,10 @@
 sonar-housekeeper = tools.housekeeper:main
 sonar-issues-export = tools.findings_export:main
 sonar-issues-sync = tools.findings_sync:main
 sonar-loc = tools.loc:main
 sonar-measures-export = tools.measures_export:main
 sonar-projects-export = tools.projects_export:main
 sonar-projects-import = tools.projects_import:main
+sonar-rules = tools.rules_cli:main
 support-audit = tools.support:main
```

## Comparing `sonar_tools-2.9.dist-info/RECORD` & `sonar_tools-3.0.dist-info/RECORD`

 * *Files 10% similar despite different names*

```diff
@@ -1,79 +1,82 @@
 sonar/__init__.py,sha256=yItDqma8FgLXKkN-Ju9N-kXCA9VXEp3ZmXdGm6Peaus,828
-sonar/aggregations.py,sha256=IlPJO7mS3Wrrr5b4klhX8IE4omzuKDF82CKSRzZR0OY,4052
-sonar/applications.py,sha256=aOcTg02_dsEoFxYznwTJDsgwVFmqTbj60psN7Sw-UCc,20257
-sonar/components.py,sha256=biMZBRaIcCeeaUUUhp1xVAgkakreAAeyXUGXc9AMCv8,7001
+sonar/aggregations.py,sha256=lci9g2i8psIfzS7YfB8TzgmwYE6qE9Y0XFopovtAR8k,4026
+sonar/applications.py,sha256=YLswwTa1P9ZM9FAAIF77pSwdg-i6JmzsLk2_syqZc2k,20531
+sonar/components.py,sha256=ujwTtglwOmApPVvkvQd_qcXZQ8a5GPhVNxFBqsbhJBI,7503
 sonar/custom_measures.py,sha256=CCl_Q1WHWNAxsvF8hOh8NvIk5HeNrMga-9xcwmwlKmk,2921
 sonar/devops.py,sha256=ErGZkmnqXroSVhP81NfOMj14460ztBXPEqaMDIJG6no,10862
-sonar/exceptions.py,sha256=Z8mUZ8FONc14j_wg5Y9MFf3AkNiKKqpE9jCqsMnKMpw,1493
-sonar/groups.py,sha256=NkL-RCbJ4yx97tQQ6ISH27NWY871pvQMRsLjAntstYs,12296
+sonar/exceptions.py,sha256=O4DIeKnYvIMFan5iFayd9LEYV9cqIGjZHfD2QYIeWMU,1607
+sonar/groups.py,sha256=n4izWYtvOglSJhE5hN9nEix3-F3BZeCbTo0nUow2MBY,12292
 sonar/languages.py,sha256=T2eYC5l3UX1FtMBcDaOYZuB4m-V9T-snlt1I7bh1FX4,3750
-sonar/measures.py,sha256=5WI3yrfLvlStcZtG6oYXE8IY0Ez8ddvThJQ_9gPZcUw,9455
+sonar/measures.py,sha256=zozmXXqWoh6MJ7EyoTdYSuD3EqO5o0vgShRJtaE57pg,11632
 sonar/metrics.py,sha256=cYg96089Tysywvvdmpd-mG562agRCM5K0cE0eSzCyl8,6281
-sonar/options.py,sha256=xpdtXP58aq8tW3dQuyNGqHxc8Y2yACTtAAhXgAx4CXs,2207
-sonar/platform.py,sha256=vIv9bKSm27Ofh9oD8SbE9GjsQBEH4Lqts5NJKIsE3Fs,33955
-sonar/portfolios.py,sha256=quJ3IKOWOBQ8XuoBOFS5RQ5tFnL7ElwSkmmoUHW6k4w,29924
-sonar/qualitygates.py,sha256=pi0CdqGfckb3UAhcymzdJaDazy0bVFlEEfty5_8XaAc,17660
-sonar/qualityprofiles.py,sha256=RXXYfT2rv7x35XsEp-EFrtlC1PLQZFLy6LUV9EBbFMk,29108
-sonar/rules.py,sha256=pTZ7wNdBUV23ovE9sn2sB-KRQeHupxQXKsntS1wcdVc,11288
-sonar/settings.py,sha256=F-piA13EITyeLbw3M14DKF_9mVWwwgOFESP21F-YgBE,16190
-sonar/sif.py,sha256=5G-hIg5sy1SHXMRZSQN1jCXXjlzBYZkKthoB1EHhcEs,19125
+sonar/options.py,sha256=pXmrDJWfiyrjt3nkuNrv2Hwy-nSgt8kTTadZ3b728Ug,2272
+sonar/organizations.py,sha256=YCw8P406K9G2mJfXbNT0HjvlIYA6CynFLV3nOlyMsE4,7170
+sonar/platform.py,sha256=3BeJPKKYECDCidvPGzKaKAY9GYMlHZ-1D_7bB6RoG2Q,35548
+sonar/portfolios.py,sha256=r-4RZqHfsrry75PzYlZgrrc3_KrPjMxeRz7xvdyVhGM,31776
+sonar/qualitygates.py,sha256=0UjC48TJaUWqvwDDrLtoIU6ajMb2lrEYGsGMeCogT00,17676
+sonar/qualityprofiles.py,sha256=KjMiJwj2XGjQ3YnSczJKMiYthLTly21DH8JQOfdw2uo,30758
+sonar/rules.py,sha256=XiyC-kbsNbzZ9j7rUMzy2uRr97d1IlLjRFcWIJu7ra0,12560
+sonar/settings.py,sha256=iDFyZrAt1YZQbtFE6AL5Qmdt2KBuzsC6JrzfeIPXAmY,19068
+sonar/sif.py,sha256=mvNklI0BOo0-u9DXul0NeQOOLd0cV-RX1GlgSiEkpGk,12698
+sonar/sif_node.py,sha256=5A9Pt01mEx-1zqi83B1qHjk8g1VuSq-aijrAxFjEyPg,13938
 sonar/sqobject.py,sha256=10VHp3gidC3Han5weHr0jUTajBYfZZpMTEErN2CfClY,6010
 sonar/syncer.py,sha256=OHfuCuQZBBvTeRj28qnhetDdMvAerPL0QV2inAMiyCA,8443
-sonar/tasks.py,sha256=BnX5Z9LRA4D9bFeX9dqxWngwNBRuXRzE7_8AP5mqoeI,21269
+sonar/tasks.py,sha256=uxkrrDR5io-6KTbfszYdQBKK66_-_ZIAxEFw71Ujs_8,22317
 sonar/tokens.py,sha256=xNZJozcFnD2m-57TTFTxafoHMsOsuHUBYXBEOTuswo4,3486
-sonar/users.py,sha256=Yl1yxurTPs_u_5fR7rIvP-p-04lmx5dAOBnrf5qyRPE,17339
-sonar/utilities.py,sha256=yZZZxMscwgWgyrC2Hy6va0yTashKRjCPDOmH7bhS7aQ,16352
-sonar/version.py,sha256=Ur9wleSFHlm0kTa1wvOz4iLRc3obZoHzYPnLuZWKFB0,896
-sonar/webhooks.py,sha256=Sh7I72fYjCtf8-M1bQbh-74Umesq9T9eljH5k3nDpCU,5315
+sonar/users.py,sha256=50bd8opMT69h3Yr7Ff6Ow3GcBYlw6McSBPTLPKaORSI,18375
+sonar/utilities.py,sha256=XT0XMOBFbGu0ZS6hut5KJ5mlWMMBLtRvMuILkcl88hY,19668
+sonar/version.py,sha256=AEnZgtZo6nlbwqWH43_PWZ9iDD18Ks6vnONnJqCqq2g,896
+sonar/webhooks.py,sha256=HVVk4uxwTXJLPEaHlSxZt7C9taRDJOIq07-i5zU6SYY,5311
 sonar/audit/__init__.py,sha256=lI74dctpwxuPbsRe8d0SRwHGqHuCzfif4v0P5pikrHc,1034
 sonar/audit/config.py,sha256=Sg8IEomDNR4u1JeT4IojO2nzwVvIJi81wTwVNlIZtrc,2851
-sonar/audit/problem.py,sha256=tAeOgi9fYBeVuLbwZB8OrZDuXo-GV43qdd8dKHXPVlM,2526
-sonar/audit/rules.json,sha256=0SS6e7Ut_2Y1eHzKdqn2rxE3jnGCGZleFuEPN00VCeg,20583
-sonar/audit/rules.py,sha256=lU_u29NE1h8QFA2DSZJrRHMcXPVTdPR2c7LEyjlm0iI,6153
+sonar/audit/problem.py,sha256=6jhlRumg1zdmtN26nq3giWIs3geFYPwa36Aet8Q_-30,3974
+sonar/audit/rules.json,sha256=z1b6sEjpegO5eub3JvY9e6WQnwhLkSfBeHjTQMiSqSI,22032
+sonar/audit/rules.py,sha256=eT6fkhuxoStkfzbY47YMcpidw2hz2I8izMq621rYYOU,6401
 sonar/audit/severities.py,sha256=zR3UzGTloWH0c-0k4FCHnnvm6bc9Ys7oSM4GFj__EFg,1130
-sonar/audit/sonar-audit.properties,sha256=mviFERQ2tdjWd0dQbO5l2XJW_QV-G4hZ6Cj-0eTEdOw,13229
+sonar/audit/sonar-audit.properties,sha256=YbORxeWt0SVTloj3uR_DNPv1d5d0_bNXQ3KhJi05Qi8,13172
 sonar/audit/types.py,sha256=St7UUg_nGBc6Hhc_YvlTuWZtOipXWxSGsXNzfwc54mI,1179
 sonar/dce/__init__.py,sha256=ipA9RjscR6Qgt8-mWm-MqYVdQx0YVas0S7msFus8Djo,828
-sonar/dce/app_nodes.py,sha256=z4o3Zf_yMRZBHn4-IiP4EiXbpyABrl0e_4ApFOsIE70,12510
+sonar/dce/app_nodes.py,sha256=hPe4fBx_5oR2ComTFTtFAZc9J_F4IUk1Pbf62_84S3Q,4814
 sonar/dce/nodes.py,sha256=-o6vqrYhawTXrGSDYQAQ0LbyQWconYZkrKcmycP76QY,1086
-sonar/dce/search_nodes.py,sha256=5pWWh1ppJPZ5QxNYmcAM-EKafIN_1eBYyoPiyCuPFrI,4586
+sonar/dce/search_nodes.py,sha256=eoqgDO5yaGtFzORkgrsVpi3Wlt5vjQ2m4vpYj4906SY,6340
 sonar/findings/__init__.py,sha256=ipA9RjscR6Qgt8-mWm-MqYVdQx0YVas0S7msFus8Djo,828
 sonar/findings/changelog.py,sha256=kHuOlm63vWmsj4tmAZ6l15OxxM-3UDyBkDYAY17lB-8,6900
-sonar/findings/findings.py,sha256=6XnVa5FNxRZgN8v4NBRKASpz2JG5Xh9-x9fN7ATONo8,12542
-sonar/findings/hotspots.py,sha256=3PxbA2DFWDMjrRv27sB9U-3KuNFgQTN7edu12pI6Kkk,15687
-sonar/findings/issues.py,sha256=82INKOtDsnYH4avSxsUzAp7O-IiJON6h2ahXkLYEQ88,31311
+sonar/findings/findings.py,sha256=g6Hj0GHWF0RgsjvEM8UUgmjZYjmPu6RIDmnD5-cXpfk,13744
+sonar/findings/hotspots.py,sha256=y3wWIAYjs1T8CUHwtfcDNLxWDkPXSsG-ylLeW8G8XeA,15727
+sonar/findings/issues.py,sha256=lQiqiKdHFVXMSf6ljLgMgjyMZaS5DaxC25s7E8YJb18,31351
 sonar/permissions/__init__.py,sha256=ipA9RjscR6Qgt8-mWm-MqYVdQx0YVas0S7msFus8Djo,828
 sonar/permissions/aggregation_permissions.py,sha256=6V_-xzNSsyB1QuIelqkszzkxbWa1YbptZhcr5vQOMZM,1820
 sonar/permissions/application_permissions.py,sha256=jHozT6vVgqhxPwjFdhllQIYzgG89rOtAQYYyaewi6D0,1021
 sonar/permissions/global_permissions.py,sha256=yxrhmVWRzhggt4T9g2eGs2FB1z5NT6ahbQr6oZ9zU3g,3341
 sonar/permissions/permission_templates.py,sha256=mUbCGkBVc7nzsJRJL-d9lVN3709XoDHlQy9fOOoLcc8,9576
 sonar/permissions/permissions.py,sha256=LweMzA4zO9XNbhIEGJrcs8NUtpMiVPUbia5m641dUr8,11397
 sonar/permissions/portfolio_permissions.py,sha256=jpp4iiIjBgBNe-BcQd16BF6uKS04sEUKsgXE_RSszEo,1018
-sonar/permissions/project_permissions.py,sha256=eesjkmuNJeCED1RGz4ku7wgwSwOMA-hnyjsV5oBzidw,8252
+sonar/permissions/project_permissions.py,sha256=yMr9S0Zuh6kcjBsOFrW5TNfuVLJAAraHWN5QKJzMqHY,8216
 sonar/permissions/quality_permissions.py,sha256=nSViz1ei5MtzMaPMYA6CvrYjcZ-5OdcSHOywlhJSkc0,4406
 sonar/permissions/qualitygate_permissions.py,sha256=4o84cK7RSU8_Pgpg3aOSKnWUgqjKqyhgMQAGxpPeHDo,2196
 sonar/permissions/qualityprofile_permissions.py,sha256=yGl420BTH3V--D0j6nLsid8d4GDBVtcxrlJPS53wlfw,2349
 sonar/permissions/template_permissions.py,sha256=kYH4NcX8YxC6fCOm1N4IEdDFYkSbQBSQHiME2mKMkqY,2955
 sonar/projects/__init__.py,sha256=ipA9RjscR6Qgt8-mWm-MqYVdQx0YVas0S7msFus8Djo,828
-sonar/projects/branches.py,sha256=uNc1rgdqAKnBKxcgivRZP0f4nkzFep3JZCnz_XtL2zo,17542
-sonar/projects/projects.py,sha256=M7S8a8GLgSOWFWT6J5oKGxasYqkAZYKUlpi7iIA8WrY,56961
-sonar/projects/pull_requests.py,sha256=ODlKiRihhow-qIR1ZhnYZvS8UcrNgwPfq-qOZFrEUzI,4484
-sonar_tools-2.9.data/scripts/sonar-tools,sha256=hWGI3S-RldL4jHXYrL9yrv5cgpCscQq5a2lz6sCgFxQ,2223
+sonar/projects/branches.py,sha256=uuB7XoKl0jQbWsmbIck7yxW_8dmsVFGpJ4e8NcqzF8g,17684
+sonar/projects/projects.py,sha256=qlaqOLdPv8f4euNYEfWEa9MfIUFESOSEoyUAaTTswcE,57214
+sonar/projects/pull_requests.py,sha256=X2hcoDHW3UQ2Fvu_6yz_jGzT9XKxmPe2Y14QdXtr-NU,4480
+sonar_tools-3.0.data/scripts/sonar-tools,sha256=U_ej6MguqD1EOyTXjKrudRCoGSdY4MRRpmO4ySqNUSU,2337
 tools/__init__.py,sha256=ipA9RjscR6Qgt8-mWm-MqYVdQx0YVas0S7msFus8Djo,828
-tools/audit.py,sha256=wo0MrgQxRGkLo6RRH3S3wnFXhKfTxhCuoC7H2NAaGPQ,6700
-tools/config.py,sha256=zJBw55tW-Y4xK-xoh04ijC2UvfN5IpBUSGKUXJZpAe4,7932
-tools/cust_measures.py,sha256=EvQypeRL3dOckkAHOoVf6aaJgVNXZ4nnm5113wENBSI,2539
-tools/findings_export.py,sha256=kRqLGTAU9hK0uORVS_OAEQGwi1Yf3HzO6EKZidKEFlk,14736
-tools/findings_sync.py,sha256=VKAJ0RtcK5u_J5gabqbwJeJYJT73IepiOp0EOsKesoY,9195
-tools/housekeeper.py,sha256=Kd_rqonBjBOofUxXVYuJLYa-lPlBYDO7F3-n3EnewJU,8908
-tools/loc.py,sha256=TQYgYE6KkJs1G-LpGRje7PJSE3QRDEkygM5a1YvtAGo,5818
-tools/measures_export.py,sha256=iSiLrhrMXeR_vp-tOsSRdEMyhDvEiLdawK9QkYDk9Ds,8803
-tools/projects_export.py,sha256=XmCOlmrxF8NiGe7DOTNjWY7X3wWaZPtg7adGmIrP3SY,2508
-tools/projects_import.py,sha256=30SxetQogq3XloiKADTyQKdyx-Gx22-mfP6VLhpOlnE,3750
-tools/support.py,sha256=Z6yrdb8vq4RG8Az5OkJPC9R0TM9xCygSXVoRrAboNIQ,6829
-sonar_tools-2.9.dist-info/LICENSE,sha256=46mU2C5kSwOnkqkw9XQAJlhBL2JAf1_uCD8lVcXyMRg,7652
-sonar_tools-2.9.dist-info/METADATA,sha256=TVLrp265o1p8MhhpgAuMwPDKq8nbu_PzAmtqxBkYD5Y,20176
-sonar_tools-2.9.dist-info/WHEEL,sha256=oiQVh_5PnQM0E3gPdiz09WCNmwiHDMaGer_elqB3coM,92
-sonar_tools-2.9.dist-info/entry_points.txt,sha256=tqqjNP53MeqgNKUEH1-U07ANYBT67ef-sfWBkgvlON0,582
-sonar_tools-2.9.dist-info/top_level.txt,sha256=4UurrhJYnkA9Gtw-URkzRpfPjRIOIqSCQFJGrwE8Nvo,12
-sonar_tools-2.9.dist-info/RECORD,,
+tools/audit.py,sha256=fW9zA5j4xU6_EDgvcWLo4Un9-tKMC0dABZaaIis07o8,6631
+tools/config.py,sha256=BM8SLTvCXkmS1lM5TRlCl78Dxkhu2fPYqZuyTOxqdmU,8168
+tools/cust_measures.py,sha256=1khglZCAJ0elv7BwUq_YFgwsOIk3xrLEipXA-tytHXQ,2596
+tools/findings_export.py,sha256=kTKw_Bg0eu7bnt3FPCGZw1_9lWqBU2woLfIQUwpp7Rc,17510
+tools/findings_sync.py,sha256=CvuvX-OKJHjYJUAbbCQINZMjZkyGZ3JZSYB01YRYf-g,9192
+tools/housekeeper.py,sha256=HUOJ75wkbyqs1JHHA4KVbZwOt0A449lO8Ntvn3IQiF4,9076
+tools/loc.py,sha256=Rqnt0arZYMFdsfhNrbh_aAOt4lx8l1IkmHVpUiZbJpI,5727
+tools/measures_export.py,sha256=WFto6-RPNEyp4i4l21g_llZgx4fqFmArhvmB6vB8zvs,12386
+tools/projects_export.py,sha256=d7ZdBilEygZPJn0bjYhnV0mdUXBcM6y-O3miVuIjyig,2365
+tools/projects_import.py,sha256=Pzt7yZIfCftXUvlsv7MGoU8hZoQIiURt2PeMohme_jk,3622
+tools/rules_cli.py,sha256=swwoQuZFUC3Oayh7Rc_yH3jzcKJwN65dgP-s-Hp_e90,3146
+tools/support.py,sha256=diPjPvBB-GXjFo0Z1_jDnEnEsBN5NMp7D5G8aTdmZ3k,6761
+sonar_tools-3.0.dist-info/LICENSE,sha256=46mU2C5kSwOnkqkw9XQAJlhBL2JAf1_uCD8lVcXyMRg,7652
+sonar_tools-3.0.dist-info/METADATA,sha256=eu8ZIvM1DPNic6aKJONB6Yu8cCZ5bwBntgXOGYuYQzM,20950
+sonar_tools-3.0.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+sonar_tools-3.0.dist-info/entry_points.txt,sha256=UELEndtzCF9bPx_IPG7pj02NGB3nUWkbZBwYFVtmpV8,617
+sonar_tools-3.0.dist-info/top_level.txt,sha256=4UurrhJYnkA9Gtw-URkzRpfPjRIOIqSCQFJGrwE8Nvo,12
+sonar_tools-3.0.dist-info/RECORD,,
```

